[{"number": 23948, "title": "UpSampling1D takes long. UpSampling2D is fast.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): virtual env, pip\r\n- TensorFlow version (use command below): **('v1.11.0-0-gc19e29306c', '1.11.0')**\r\n- Python version: 2.7\r\n- CUDA/cuDNN version: Yes, 9\r\n- GPU model and memory: Yes, 2GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n**('v1.11.0-0-gc19e29306c', '1.11.0')**\r\n\r\n**Describe the current behavior**\r\nWhen I run this code, it is super slow. it takes minutes to generate the model\r\n```\r\ninput1 = tf.keras.layers.Input(shape=(input_n,1))\r\n x_a = input1\r\nfor i in range(6):\r\n    x_a = tf.keras.layers.Conv1D(8 * (2 ** i), (3), padding='same')(x_a)\r\n    x_a = Activation('relu')(x_a)\r\n    x_a = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x_a)\r\nlatent = x_a\r\nx_d = latent\r\nfor i in range(5,-1,-1):\r\n    x_d = tf.keras.layers.Conv1D(8 * (2 ** i), (3), padding='same')(x_d)\r\n    x_d = Activation('relu')(x_d)\r\n    x_d = tf.keras.layers.UpSampling1D(size=2)(x_d)\r\ndecoded = x_d\r\nmodel = tf.keras.models.Model(inputs=input1, outputs=decoded)\r\n```\r\n\r\n**Describe the expected behavior**\r\nBut the UpSampling2D is much faster\r\n```\r\ninput1 = tf.keras.layers.Input(shape=(input_n,1))\r\n x_a = input1\r\nfor i in range(6):\r\n    x_a = tf.keras.layers.Conv2D(8 * (2 ** i), (3), padding='same')(x_a)\r\n    x_a = Activation('relu')(x_a)\r\n    x_a = tf.keras.layers.MaxPooling2D(pool_size=2, padding='same')(x_a)\r\nlatent = x_a\r\nx_d = latent\r\nfor i in range(5,-1,-1):\r\n    x_d = tf.keras.layers.Conv2D(8 * (2 ** i), (3), padding='same')(x_d)\r\n    x_d = Activation('relu')(x_d)\r\n    x_d = tf.keras.layers.UpSampling2D(size=2)(x_d)\r\ndecoded = x_d\r\nmodel = tf.keras.models.Model(inputs=input1, outputs=decoded)\r\n```\r\n\r\nI tested every component. The reason is UpSampling1D\r\n", "comments": ["Please test with the latest version of TF and see if the issue still persists. Also the provided code snippet looks incomplete to reproduce the issue reported. Closing this issue for now. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23948\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23948\">No</a>\n"]}, {"number": 23946, "title": "[BUG] `accumulate_n` fatal  in `InteractiveSession`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary, Anaconda\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.2/7.2.1\r\n- GPU model and memory: GTX 1050 mobile, 4GB\r\n\r\n**Describe the current behavior**\r\nFailure when using `accumulate_n` (`add_n` works) with tensors (also with `tf.constant(array_here)`) with an `InteractiveSession` (works for normal session).\r\n\r\n**Describe the expected behavior**\r\nSame result as `add_n` (and no error)\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nt1 = tf.constant(1.)\r\nt2 = tf.constant(42.)\r\ntensor_list = [t1, t2]\r\nadd_n_op = tf.add_n(tensor_list)   # for comparison only\r\naccumulate_n_op = tf.accumulate_n(tensor_list) \r\nsess = tf.InteractiveSession()\r\n# sess = tf.Session()  # this would work\r\nprint(sess.run(add_n_op))  # works (print to show it works)\r\nprint(sess.run(accumulate_n_op))  # does not work\r\n```\r\n**Other info / logs**\r\nError message:  \r\n2018-11-24 11:11:27.877871: F tensorflow/compiler/jit/deadness_analysis.cc:639] Check failed: it != predicate_map_.end() AccumulateNV2/Internal/_3\r\n", "comments": ["Hello @josh11b , user points out that _accumulate_n_ op does not work in interactive session mode, but _add_n_ op works. Can you please advise. Thanks.", "I'm not familiar with the specifics of that code. Interactive session should not be significantly different from regular session. @alextp likely knows more.", "@sanjoy do you know why this jit pass is failing here?", "Usually this means the graph has an illegal cycle.  I've since changes this piece of code to print a friendlier error message:  https://github.com/tensorflow/tensorflow/blob/1ee193a2563d51ee45b401f2cff91f6e480e21db/tensorflow/compiler/jit/deadness_analysis.cc#L639\r\n\r\nMaybe you could try running on HEAD to see if that helps?", "I've run it again using the most recent master branch (CPU-only, default compilation flags: XLA on, others off).\r\n\r\nThe problem persists, the message changed though. This is the error message:\r\n\r\n```\r\nInternalError: Could not find input [id=7 AccumulateNV2_2/Internal/_2:0 -> \r\nAccumulateNV2_2/Internal/_3:0] to AccumulateNV2_2/Internal/_3 when visiting the \r\ngraph in post-order.  Most likely indicates a bug in deadness analysis.\r\n```\r\n\r\nFor completeness, the stacktrace\r\n```\r\nInternalError                             Traceback (most recent call last)\r\n~/anaconda3/envs/zfit36master1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1334     try:\r\n-> 1335       return fn(*args)\r\n   1336     except errors.OpError as e:\r\n\r\n~/anaconda3/envs/zfit36master1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1319       return self._call_tf_sessionrun(\r\n-> 1320           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1321 \r\n\r\n~/anaconda3/envs/zfit36master1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1407         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1408         run_metadata)\r\n   1409 \r\n\r\nInternalError: Could not find input [id=7 AccumulateNV2_2/Internal/_2:0 -> AccumulateNV2_2/Internal/_3:0] to AccumulateNV2_2/Internal/_3 when visiting the graph in post-order.  Most likely indicates a bug in deadness analysis.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-11-1792673f89f6> in <module>\r\n      9 # sess = tf.Session()  # this would work\r\n     10 #print(sess.run(add_n_op))  # works (print to show it works)\r\n---> 11 print(sess.run(accumulate_n_op))  # does not work\r\n\r\n~/anaconda3/envs/zfit36master1/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    928     try:\r\n    929       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 930                          run_metadata_ptr)\r\n    931       if run_metadata:\r\n    932         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~/anaconda3/envs/zfit36master1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1151     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1152       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1153                              feed_dict_tensor, options, run_metadata)\r\n   1154     else:\r\n   1155       results = []\r\n\r\n~/anaconda3/envs/zfit36master1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1327     if handle is None:\r\n   1328       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1329                            run_metadata)\r\n   1330     else:\r\n   1331       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n~/anaconda3/envs/zfit36master1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1347           pass\r\n   1348       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1349       raise type(e)(node_def, op, message)\r\n   1350 \r\n   1351   def _extend_graph(self):\r\n\r\nInternalError: Could not find input [id=7 AccumulateNV2_2/Internal/_2:0 -> AccumulateNV2_2/Internal/_3:0] to AccumulateNV2_2/Internal/_3 when visiting the graph in post-order.  Most likely indicates a bug in deadness analysis.\r\n```\r\n", "Should be fixed by https://github.com/tensorflow/tensorflow/commit/0b3c3c55e177b35d38ba33170ebe2baa3f5badff\r\n\r\nPlease re-open if you this is still broken.", "Thank you @sanjoy."]}, {"number": 23945, "title": "Automatic new type of layer creation mechanism for TensorFlow", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIDEA: I want to offer you a method that put production of a new type of layers on an industrial scale.\r\nSTATUS: Idea came to my mind I am thinking about implementation and I want to know your opinion.\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nAPI may change as to provide compatibility to a new feature.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAPPLICATION:\r\n1. Creation of new type of layers suited for particular industries and application.\r\n2. Research work can be enhanced. We are turning a neural network into the classic mathematical language.\r\n\r\n**Any Other info.**\r\n\r\nPROBLEM: How to turn a neural network in mathematical formula suited for a creation of an absolutely new type of layer for TensorFlow. It might be used in TensorFlow, Keras,  PyTorch or anything. \r\n\r\nHOW: \r\n1, You trained your network for particular industry application and invested a lot of computational power to make it work. And this neural network work for solving a particular problem in a particular industry very well.\r\n2. You freeze all layers in your neural network.\r\n3. Let's suppose we have an MSE loss for a simplicity.\r\n4. Then you start to training target(label) looking for INTEGRAL of a function of your neural network. It will produce a mathematical formula describing a neural network. Let's call ANTI GRAD algorithm.\r\n5. After it's done you have a mathematical formula that can easily be turned into a new type of layer for particular industry application with a simple(you can tune simplicity) formula for Chemistry, Economics, Medicine.\r\n\r\nCOMMENTS:\r\nIt would require a team of programmers to make it work. So, I am looking for new opportunities and the team to make it happen.\r\n\r\n\r\n", "comments": []}, {"number": 23944, "title": "Installation with python 2.7 on MacOS,  attribute __doc__ not writable", "body": "As shown in title, I was compiling from source with python 2.7 on MacOS, and found failure described as \"AttributeError: attribute '__doc__' of 'type' objects is not writable\"\r\n\r\n\"* `ONLY_FIRST_TOWER`: Deprecated alias for `ONLY_FIRST_REPLICA'.\\n\"\r\n\r\nThen I changed this file \"variables.py\" and recompile only to found the same bug in other files.", "comments": ["@RickLee26  Request you to fill [this ](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md)template which helps us to look into this issue. Thanks !", "@RickLee26 I got the same error on Ubuntu 16.04. For me it was related to the enum / enum34 packages.\r\nMaybe have a look at #12491", "@RickLee26  Any update on this ? Please fill the template and keep us posted.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 23943, "title": "Build Options for Older CPU w/o AVX", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version:\r\ntensorflow-1.12.0rc0\r\n- Python version:\r\n3.6\r\n- Installed using virtualenv? pip? conda?:\r\nconda\r\n- Bazel version (if compiling from source):\r\n0.17.2\r\n- GCC/Compiler version (if compiling from source):\r\ngcc version 5.4.0\r\n- CUDA/cuDNN version:\r\nno\r\n- GPU model and memory:\r\nno\r\n\r\n**Describe the problem**\r\nI need to turn off AVX during build. However, the tf_configure.bazelrc doesn't have an explicit AVX flag. Since the build takes hours to complete, could you tell me what the flag is? Here is my tf_configure.bazelrc:\r\n\r\nimport %workspace%/tools/bazel.rc\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/me/anaconda3/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/me/anaconda3/lib/python3.6/site-packages\"\r\nbuild --python_path=\"/home/me/anaconda3/bin/python\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --action_env TF_DOWNLOAD_CLANG=\"0\"\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n2018-11-23 16:35:33.993987: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.\r\nAborted (core dumped)\r\n\r\n\r\n", "comments": ["What\u2019s your bazel build command?", "bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nAfter ./configure, as in https://www.tensorflow.org/install/source", "./configure\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: **-march=core2**\r\n\r\nOr change \"core2\" to the \u03bcarch which you are using.", "What are the permitted \"uarch\" names?\n\n I am building now  by sticking -mno-avx somewhere. It has been building\nfor a couple of hours.....\n\nOn Fri, Nov 23, 2018 at 9:37 PM Iamanorange <notifications@github.com>\nwrote:\n\n> ./configure\n> Please specify optimization flags to use during compilation when bazel\n> option \"--config=opt\" is specified [Default is -march=native]:\n> *-march=core2*\n>\n> Or change \"core2\" to the \u03bcarch which you are using.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23943#issuecomment-441345193>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AX3ZAqJ8onghxQHESpBFNG7UEGdIZswOks5uyNsjgaJpZM4YxNk5>\n> .\n>\n", "https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html", "I have two laptops both showed x86_64 when I type gcc -dumpmachine.\nHowever, only one of them gives the core dump message as shown above and\nthe other does not.  So the -march=xxx solution may not work.\n\nOn Fri, Nov 23, 2018 at 10:05 PM Iamanorange <notifications@github.com>\nwrote:\n\n> https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23943#issuecomment-441346148>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AX3ZAu71uu2d79guxRISVNW-JMY9eX2Eks5uyOHEgaJpZM4YxNk5>\n> .\n>\n", "The proper way to find native \u03bcarch is\r\n`gcc -Q -march=native --help=target | grep march`", "Thanks. I specified -march=sandybridge but still get the same message:\n\n2018-11-24 22:13:30.127860: F\ntensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library\nwas compiled to use AVX instructions, but these aren't available on your\nmachine.\n\nProcess finished with exit code 134 (interrupted by signal 6: SIGABRT)\n\nOn Fri, Nov 23, 2018 at 11:21 PM Iamanorange <notifications@github.com>\nwrote:\n\n> The proper way to find native \u03bcarch is\n> gcc -Q -march=native --help=target | grep march\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23943#issuecomment-441349093>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AX3ZAmESzkSZ4pSTBDJibcIC29di4G4Gks5uyPNcgaJpZM4YxNk5>\n> .\n>\n", "Why did you build with -march=sandybridge? Sandybridge is the first generation which support AVX.", "Oh, I found out my architecture is sandybridge,using the command above. The\nerror message, however, says I don't have AVX. Is there a simple way of\nturning off avx, regardless if the CPU supports it or not? I tried adding\n-mno-avx in the configuration file, but it doesn't seem to have an effect,\nstill getting the same message.\n\n\nOn Sat, Nov 24, 2018 at 11:24 PM Iamanorange <notifications@github.com>\nwrote:\n\n> Why did you build with -march=sandybridge? Sandybridge is the first\n> generation which support AVX.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23943#issuecomment-441420918>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AX3ZAnRJ9YOK9ERzqpX3UcdiD99-8ybnks5uykWigaJpZM4YxNk5>\n> .\n>\n", "Try `-march=core2`.", "Just tried, still no luck. The message is the same.\n\nOn Sun, Nov 25, 2018 at 8:45 AM Iamanorange <notifications@github.com>\nwrote:\n\n> Try -march=core2.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23943#issuecomment-441454007>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AX3ZApW5ha9tbtuJhOZlUPML-e0q6OjQks5uysksgaJpZM4YxNk5>\n> .\n>\n", "Did you \u2018bazel clean\u2019 between your trials?", "Hello @pauldelmusica , please try building with bazel 0.15.0 and gcc 4.8 , and let us know. Thanks.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Just chipping in here that I have the exact same problem also relating to the use of Linux-Fake-Background-Webcam on an HP Pavilion 14-ce3510sa "]}, {"number": 23942, "title": "Question about example programs", "body": "Hi, I just started using the regression portion of tensorflow. However, I can not find a program that creates a multi variable regression MODEL and a programs that uses that MODEL to make a prediction.\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Window 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:NONE\r\n- **TensorFlow installed from (source or binary)**:Binary(installed with pip, non gpu)\r\n- **TensorFlow version (use command below)**: 1.12\r\n- **Python version**:3.5.2\r\n- **Bazel version (if compiling from source)**:none\r\n- **GCC/Compiler version (if compiling from source)**:none\r\n- **CUDA/cuDNN version**:none\r\n- **GPU model and memory**: intel hd(can not be used with tensorflow, irrevelant)\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please refer to this [article](https://medium.com/themlblog/multivariate-regression-using-deep-neural-networks-in-tensorflow-f94f42a148b3) for multi variable regression. It should help you to get started. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23941, "title": "Tensorflow Linux Crash", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Both, have tried using stock code and non-stock code.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): I believe it was binary, installed it a while ago\r\n- TensorFlow version (use command below):  ('v1.7.0-3-g024aecf414', '1.7.0')\r\n- Python version: Python 2.7.12\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA = CUDA Version 9.0.176, cuDNN = Not sure, I do not see cudnn.h in the \"/usr/local/cuda/include/cudnn.h\" directory\r\n- GPU model and memory: Titan Xp, 12GB\r\n\r\n**Describe the current behavior**\r\nA while back (~6 months ago) we got a new computer with a Titan Xp GPU.  I installed Tensorflow and CUDA/cuDNN per the instructions on the Tensorflow website.  I was running into some strange behavior where running code with Tensorflow would sometimes work OK with no issue, but sometimes it would cause the entire computer to crash, where the only way to recover was through a hard reset.  I am not so experienced with Tensorflow/Linux/GPU computing, this is all relatively new to me.  So I tried wiping the computer and starting over from scratch to repeat the procedure from the beginning, and was still seeing the same behavior.  There is another GPU computer that I've been able to use that has not had this issue, so I put troubleshooting this issue onto the \"backburner\" and have not looked at it for a couple months - so I am trying to get back to resolving this.\r\n\r\nNote that there seems to be on correlation to what code causes this, it will happen with either a stock example or a custom code written for our data.  There is code that will run OK on a different machine with GPU, but will sometimes cause this behavior on this machine without changing the code.  Note that this seems to only occur when running Python+Tensorflow on the GPU, it makes me think somehow related to the GPU - maybe some driver was not configured correctly?\r\n\r\n**Describe the expected behavior**\r\nExpecting the machine to run without crashing, would expect it to give an error and stop running code rather than the entire machine crashing.\r\n\r\n**Code to reproduce the issue**\r\nSeems like running almost any code using Tensorflow will cause this behavior, does not correlated with certain code.\r\n\r\n**Other info / logs**\r\nPlease let me know what other information would be helpful.\r\n", "comments": ["I suspect misconfiguration in your NV driver. Could you post the output of \u2018sudo dmesg\u2019? A fresh reinstall of your NV driver is also recommended. If the problem remains there, please post your installation method and log.", "Thank you for the reply.  Below is the output.\r\n\r\nI will be back on the computer tomorrow.  I will try doing a fresh re-install of my NV driver and report back afterwards.\r\n\r\nsudo dmesg\r\n[    0.000000] microcode: microcode updated early to revision 0x20, date = 2018-04-10\r\n[    0.000000] Linux version 4.15.0-33-generic (buildd@lgw01-amd64-010) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)) #36~16.04.1-Ubuntu SMP Wed Aug 15 17:21:05 UTC 2018 (Ubuntu 4.15.0-33.36~16.04.1-generic 4.15.18)\r\n[    0.000000] Command line: BOOT_IMAGE=/boot/vmlinuz-4.15.0-33-generic root=UUID=054eb44b-74d4-4332-9e20-16806f870677 ro quiet splash\r\n[    0.000000] KERNEL supported cpus:\r\n[    0.000000]   Intel GenuineIntel\r\n[    0.000000]   AMD AuthenticAMD\r\n[    0.000000]   Centaur CentaurHauls\r\n[    0.000000] x86/fpu: Supporting XSAVE feature 0x001: 'x87 floating point registers'\r\n[    0.000000] x86/fpu: Supporting XSAVE feature 0x002: 'SSE registers'\r\n[    0.000000] x86/fpu: Supporting XSAVE feature 0x004: 'AVX registers'\r\n[    0.000000] x86/fpu: xstate_offset[2]:  576, xstate_sizes[2]:  256\r\n[    0.000000] x86/fpu: Enabled xstate features 0x7, context size is 832 bytes, using 'standard' format.\r\n[    0.000000] e820: BIOS-provided physical RAM map:\r\n[    0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009d7ff] usable\r\n[    0.000000] BIOS-e820: [mem 0x000000000009d800-0x000000000009ffff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000000e0000-0x00000000000fffff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x0000000000100000-0x00000000dd0dffff] usable\r\n[    0.000000] BIOS-e820: [mem 0x00000000dd0e0000-0x00000000dda90fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000dda91000-0x00000000ddaa0fff] ACPI data\r\n[    0.000000] BIOS-e820: [mem 0x00000000ddaa1000-0x00000000ddbcafff] ACPI NVS\r\n[    0.000000] BIOS-e820: [mem 0x00000000ddbcb000-0x00000000de7fbfff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000de7fc000-0x00000000de7fcfff] usable\r\n[    0.000000] BIOS-e820: [mem 0x00000000de7fd000-0x00000000de83ffff] ACPI NVS\r\n[    0.000000] BIOS-e820: [mem 0x00000000de840000-0x00000000dec61fff] usable\r\n[    0.000000] BIOS-e820: [mem 0x00000000dec62000-0x00000000deff3fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000deff4000-0x00000000deffffff] usable\r\n[    0.000000] BIOS-e820: [mem 0x00000000f8000000-0x00000000fbffffff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000fec00000-0x00000000fec00fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000fed00000-0x00000000fed03fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000fed1c000-0x00000000fed1ffff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000fee00000-0x00000000fee00fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000ff000000-0x00000000ffffffff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x0000000100000000-0x000000081effffff] usable\r\n[    0.000000] NX (Execute Disable) protection: active\r\n[    0.000000] SMBIOS 2.7 present.\r\n[    0.000000] DMI: System manufacturer System Product Name/P8Z77-V PRO, BIOS 1805 12/19/2012\r\n[    0.000000] e820: update [mem 0x00000000-0x00000fff] usable ==> reserved\r\n[    0.000000] e820: remove [mem 0x000a0000-0x000fffff] usable\r\n[    0.000000] e820: last_pfn = 0x81f000 max_arch_pfn = 0x400000000\r\n[    0.000000] MTRR default type: uncachable\r\n[    0.000000] MTRR fixed ranges enabled:\r\n[    0.000000]   00000-9FFFF write-back\r\n[    0.000000]   A0000-BFFFF uncachable\r\n[    0.000000]   C0000-CFFFF write-protect\r\n[    0.000000]   D0000-E7FFF uncachable\r\n[    0.000000]   E8000-FFFFF write-protect\r\n[    0.000000] MTRR variable ranges enabled:\r\n[    0.000000]   0 base 000000000 mask 800000000 write-back\r\n[    0.000000]   1 base 800000000 mask FF0000000 write-back\r\n[    0.000000]   2 base 810000000 mask FF8000000 write-back\r\n[    0.000000]   3 base 818000000 mask FFC000000 write-back\r\n[    0.000000]   4 base 81C000000 mask FFE000000 write-back\r\n[    0.000000]   5 base 81E000000 mask FFF000000 write-back\r\n[    0.000000]   6 base 0E0000000 mask FE0000000 uncachable\r\n[    0.000000]   7 disabled\r\n[    0.000000]   8 disabled\r\n[    0.000000]   9 disabled\r\n[    0.000000] x86/PAT: Configuration [0-7]: WB  WC  UC- UC  WB  WP  UC- WT  \r\n[    0.000000] total RAM covered: 32752M\r\n[    0.000000] Found optimal setting for mtrr clean up\r\n[    0.000000]  gran_size: 64K \tchunk_size: 32M \tnum_reg: 8  \tlose cover RAM: 0G\r\n[    0.000000] e820: update [mem 0xe0000000-0xffffffff] usable ==> reserved\r\n[    0.000000] e820: last_pfn = 0xdf000 max_arch_pfn = 0x400000000\r\n[    0.000000] found SMP MP-table at [mem 0x000fd8f0-0x000fd8ff] mapped at [        (ptrval)]\r\n[    0.000000] Scanning 1 areas for low memory corruption\r\n[    0.000000] Base memory trampoline at [        (ptrval)] 97000 size 24576\r\n[    0.000000] BRK [0x1093d000, 0x1093dfff] PGTABLE\r\n[    0.000000] BRK [0x1093e000, 0x1093efff] PGTABLE\r\n[    0.000000] BRK [0x1093f000, 0x1093ffff] PGTABLE\r\n[    0.000000] BRK [0x10940000, 0x10940fff] PGTABLE\r\n[    0.000000] BRK [0x10941000, 0x10941fff] PGTABLE\r\n[    0.000000] BRK [0x10942000, 0x10942fff] PGTABLE\r\n[    0.000000] BRK [0x10943000, 0x10943fff] PGTABLE\r\n[    0.000000] BRK [0x10944000, 0x10944fff] PGTABLE\r\n[    0.000000] BRK [0x10945000, 0x10945fff] PGTABLE\r\n[    0.000000] BRK [0x10946000, 0x10946fff] PGTABLE\r\n[    0.000000] BRK [0x10947000, 0x10947fff] PGTABLE\r\n[    0.000000] BRK [0x10948000, 0x10948fff] PGTABLE\r\n[    0.000000] RAMDISK: [mem 0x30a1a000-0x34504fff]\r\n[    0.000000] ACPI: Early table checksum verification disabled\r\n[    0.000000] ACPI: RSDP 0x00000000000F0490 000024 (v02 ALASKA)\r\n[    0.000000] ACPI: XSDT 0x00000000DDA94078 00006C (v01 ALASKA A M I    01072009 AMI  00010013)\r\n[    0.000000] ACPI: FACP 0x00000000DDA9EF48 00010C (v05 ALASKA A M I    01072009 AMI  00010013)\r\n[    0.000000] ACPI: DSDT 0x00000000DDA94180 00ADC1 (v02 ALASKA A M I    00000022 INTL 20051117)\r\n[    0.000000] ACPI: FACS 0x00000000DDBC9080 000040\r\n[    0.000000] ACPI: APIC 0x00000000DDA9F058 000092 (v03 ALASKA A M I    01072009 AMI  00010013)\r\n[    0.000000] ACPI: FPDT 0x00000000DDA9F0F0 000044 (v01 ALASKA A M I    01072009 AMI  00010013)\r\n[    0.000000] ACPI: MCFG 0x00000000DDA9F138 00003C (v01 ALASKA A M I    01072009 MSFT 00000097)\r\n[    0.000000] ACPI: HPET 0x00000000DDA9F178 000038 (v01 ALASKA A M I    01072009 AMI. 00000005)\r\n[    0.000000] ACPI: SSDT 0x00000000DDA9F1B0 00036D (v01 SataRe SataTabl 00001000 INTL 20091112)\r\n[    0.000000] ACPI: DMAR 0x00000000DDAA09C0 000080 (v01 INTEL  SNB      00000001 INTL 00000001)\r\n[    0.000000] ACPI: SSDT 0x00000000DDA9F578 0009AA (v01 PmRef  Cpu0Ist  00003000 INTL 20051117)\r\n[    0.000000] ACPI: SSDT 0x00000000DDA9FF28 000A92 (v01 PmRef  CpuPm    00003000 INTL 20051117)\r\n[    0.000000] ACPI: Local APIC address 0xfee00000\r\n[    0.000000] No NUMA configuration found\r\n[    0.000000] Faking a node at [mem 0x0000000000000000-0x000000081effffff]\r\n[    0.000000] NODE_DATA(0) allocated [mem 0x81efb8000-0x81efe2fff]\r\n[    0.000000] tsc: Fast TSC calibration using PIT\r\n[    0.000000] Zone ranges:\r\n[    0.000000]   DMA      [mem 0x0000000000001000-0x0000000000ffffff]\r\n[    0.000000]   DMA32    [mem 0x0000000001000000-0x00000000ffffffff]\r\n[    0.000000]   Normal   [mem 0x0000000100000000-0x000000081effffff]\r\n[    0.000000]   Device   empty\r\n[    0.000000] Movable zone start for each node\r\n[    0.000000] Early memory node ranges\r\n[    0.000000]   node   0: [mem 0x0000000000001000-0x000000000009cfff]\r\n[    0.000000]   node   0: [mem 0x0000000000100000-0x00000000dd0dffff]\r\n[    0.000000]   node   0: [mem 0x00000000de7fc000-0x00000000de7fcfff]\r\n[    0.000000]   node   0: [mem 0x00000000de840000-0x00000000dec61fff]\r\n[    0.000000]   node   0: [mem 0x00000000deff4000-0x00000000deffffff]\r\n[    0.000000]   node   0: [mem 0x0000000100000000-0x000000081effffff]\r\n[    0.000000] Initmem setup node 0 [mem 0x0000000000001000-0x000000081effffff]\r\n[    0.000000] On node 0 totalpages: 8373419\r\n[    0.000000]   DMA zone: 64 pages used for memmap\r\n[    0.000000]   DMA zone: 21 pages reserved\r\n[    0.000000]   DMA zone: 3996 pages, LIFO batch:0\r\n[    0.000000]   DMA32 zone: 14101 pages used for memmap\r\n[    0.000000]   DMA32 zone: 902415 pages, LIFO batch:31\r\n[    0.000000]   Normal zone: 116672 pages used for memmap\r\n[    0.000000]   Normal zone: 7467008 pages, LIFO batch:31\r\n[    0.000000] Reserved but unavailable: 100 pages\r\n[    0.000000] ACPI: PM-Timer IO Port: 0x408\r\n[    0.000000] ACPI: Local APIC address 0xfee00000\r\n[    0.000000] ACPI: LAPIC_NMI (acpi_id[0xff] high edge lint[0x1])\r\n[    0.000000] IOAPIC[0]: apic_id 2, version 32, address 0xfec00000, GSI 0-23\r\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 0 global_irq 2 dfl dfl)\r\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 9 global_irq 9 high level)\r\n[    0.000000] ACPI: IRQ0 used by override.\r\n[    0.000000] ACPI: IRQ9 used by override.\r\n[    0.000000] Using ACPI (MADT) for SMP configuration information\r\n[    0.000000] ACPI: HPET id: 0x8086a701 base: 0xfed00000\r\n[    0.000000] smpboot: Allowing 8 CPUs, 0 hotplug CPUs\r\n[    0.000000] PM: Registered nosave memory: [mem 0x00000000-0x00000fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0x0009d000-0x0009dfff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0x0009e000-0x0009ffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0x000a0000-0x000dffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0x000e0000-0x000fffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xdd0e0000-0xdda90fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xdda91000-0xddaa0fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xddaa1000-0xddbcafff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xddbcb000-0xde7fbfff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xde7fd000-0xde83ffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xdec62000-0xdeff3fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xdf000000-0xf7ffffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xf8000000-0xfbffffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfc000000-0xfebfffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfec00000-0xfec00fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfec01000-0xfecfffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfed00000-0xfed03fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfed04000-0xfed1bfff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfed1c000-0xfed1ffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfed20000-0xfedfffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfee00000-0xfee00fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfee01000-0xfeffffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xff000000-0xffffffff]\r\n[    0.000000] e820: [mem 0xdf000000-0xf7ffffff] available for PCI devices\r\n[    0.000000] Booting paravirtualized kernel on bare hardware\r\n[    0.000000] clocksource: refined-jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 7645519600211568 ns\r\n[    0.000000] random: get_random_bytes called from start_kernel+0x99/0x51b with crng_init=0\r\n[    0.000000] setup_percpu: NR_CPUS:8192 nr_cpumask_bits:8 nr_cpu_ids:8 nr_node_ids:1\r\n[    0.000000] percpu: Embedded 46 pages/cpu @        (ptrval) s151552 r8192 d28672 u262144\r\n[    0.000000] pcpu-alloc: s151552 r8192 d28672 u262144 alloc=1*2097152\r\n[    0.000000] pcpu-alloc: [0] 0 1 2 3 4 5 6 7 \r\n[    0.000000] Built 1 zonelists, mobility grouping on.  Total pages: 8242561\r\n[    0.000000] Policy zone: Normal\r\n[    0.000000] Kernel command line: BOOT_IMAGE=/boot/vmlinuz-4.15.0-33-generic root=UUID=054eb44b-74d4-4332-9e20-16806f870677 ro quiet splash\r\n[    0.000000] Calgary: detecting Calgary via BIOS EBDA area\r\n[    0.000000] Calgary: Unable to locate Rio Grande table in EBDA - bailing!\r\n[    0.000000] Memory: 32813252K/33493676K available (12300K kernel code, 2469K rwdata, 4252K rodata, 2404K init, 2416K bss, 680424K reserved, 0K cma-reserved)\r\n[    0.000000] SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=8, Nodes=1\r\n[    0.000000] Kernel/User page tables isolation: enabled\r\n[    0.000000] ftrace: allocating 39127 entries in 153 pages\r\n[    0.000000] Hierarchical RCU implementation.\r\n[    0.000000] \tRCU restricting CPUs from NR_CPUS=8192 to nr_cpu_ids=8.\r\n[    0.000000] \tTasks RCU enabled.\r\n[    0.000000] RCU: Adjusting geometry for rcu_fanout_leaf=16, nr_cpu_ids=8\r\n[    0.000000] NR_IRQS: 524544, nr_irqs: 488, preallocated irqs: 16\r\n[    0.000000] Console: colour VGA+ 80x25\r\n[    0.000000] console [tty0] enabled\r\n[    0.000000] ACPI: Core revision 20170831\r\n[    0.000000] ACPI: 4 ACPI AML tables successfully acquired and loaded\r\n[    0.000000] clocksource: hpet: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 133484882848 ns\r\n[    0.000000] hpet clockevent registered\r\n[    0.000000] APIC: Switch to symmetric I/O mode setup\r\n[    0.000000] DMAR: Host address width 36\r\n[    0.000000] DMAR: DRHD base: 0x000000fed90000 flags: 0x1\r\n[    0.000000] DMAR: dmar0: reg_base_addr fed90000 ver 1:0 cap c9008020660262 ecap f0105a\r\n[    0.000000] DMAR: RMRR base: 0x000000dda13000 end: 0x000000dda24fff\r\n[    0.000000] DMAR-IR: IOAPIC id 2 under DRHD base  0xfed90000 IOMMU 0\r\n[    0.000000] DMAR-IR: HPET id 0 under DRHD base 0xfed90000\r\n[    0.000000] DMAR-IR: Queued invalidation will be enabled to support x2apic and Intr-remapping.\r\n[    0.000000] DMAR-IR: Enabled IRQ remapping in x2apic mode\r\n[    0.000000] x2apic enabled\r\n[    0.000000] Switched APIC routing to cluster x2apic.\r\n[    0.000000] ..TIMER: vector=0x30 apic1=0 pin1=2 apic2=-1 pin2=-1\r\n[    0.020000] tsc: Fast TSC calibration using PIT\r\n[    0.024000] tsc: Detected 3410.088 MHz processor\r\n[    0.024000] Calibrating delay loop (skipped), value calculated using timer frequency.. 6820.17 BogoMIPS (lpj=13640352)\r\n[    0.024000] pid_max: default: 32768 minimum: 301\r\n[    0.024000] Security Framework initialized\r\n[    0.024000] Yama: becoming mindful.\r\n[    0.024000] AppArmor: AppArmor initialized\r\n[    0.031347] Dentry cache hash table entries: 4194304 (order: 13, 33554432 bytes)\r\n[    0.033408] Inode-cache hash table entries: 2097152 (order: 12, 16777216 bytes)\r\n[    0.033486] Mount-cache hash table entries: 65536 (order: 7, 524288 bytes)\r\n[    0.033551] Mountpoint-cache hash table entries: 65536 (order: 7, 524288 bytes)\r\n[    0.033719] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'\r\n[    0.033720] ENERGY_PERF_BIAS: View and update with x86_energy_perf_policy(8)\r\n[    0.033723] mce: CPU supports 9 MCE banks\r\n[    0.033729] CPU0: Thermal monitoring enabled (TM1)\r\n[    0.033739] process: using mwait in idle threads\r\n[    0.033741] Last level iTLB entries: 4KB 512, 2MB 8, 4MB 8\r\n[    0.033741] Last level dTLB entries: 4KB 512, 2MB 32, 4MB 32, 1GB 0\r\n[    0.033743] Spectre V2 : Mitigation: Full generic retpoline\r\n[    0.033743] Spectre V2 : Spectre v2 mitigation: Enabling Indirect Branch Prediction Barrier\r\n[    0.033743] Spectre V2 : Enabling Restricted Speculation for firmware calls\r\n[    0.033744] Speculative Store Bypass: Mitigation: Speculative Store Bypass disabled via prctl and seccomp\r\n[    0.033745] L1TF: System has more than MAX_PA/2 memory. L1TF mitigation not effective.\r\n[    0.033824] Freeing SMP alternatives memory: 36K\r\n[    0.036055] TSC deadline timer enabled\r\n[    0.036057] smpboot: CPU0: Intel(R) Core(TM) i7-3770 CPU @ 3.40GHz (family: 0x6, model: 0x3a, stepping: 0x9)\r\n[    0.036111] Performance Events: PEBS fmt1+, IvyBridge events, 16-deep LBR, full-width counters, Intel PMU driver.\r\n[    0.036128] ... version:                3\r\n[    0.036128] ... bit width:              48\r\n[    0.036128] ... generic registers:      4\r\n[    0.036129] ... value mask:             0000ffffffffffff\r\n[    0.036129] ... max period:             00007fffffffffff\r\n[    0.036130] ... fixed-purpose events:   3\r\n[    0.036130] ... event mask:             000000070000000f\r\n[    0.036157] Hierarchical SRCU implementation.\r\n[    0.036962] NMI watchdog: Enabled. Permanently consumes one hw-PMU counter.\r\n[    0.036974] smp: Bringing up secondary CPUs ...\r\n[    0.037026] x86: Booting SMP configuration:\r\n[    0.037027] .... node  #0, CPUs:      #1 #2 #3 #4 #5 #6 #7\r\n[    0.052049] smp: Brought up 1 node, 8 CPUs\r\n[    0.052049] smpboot: Max logical packages: 1\r\n[    0.052049] smpboot: Total of 8 processors activated (54561.40 BogoMIPS)\r\n[    0.056628] devtmpfs: initialized\r\n[    0.056628] x86/mm: Memory block size: 128MB\r\n[    0.057698] evm: security.selinux\r\n[    0.057699] evm: security.SMACK64\r\n[    0.057699] evm: security.SMACK64EXEC\r\n[    0.057699] evm: security.SMACK64TRANSMUTE\r\n[    0.057700] evm: security.SMACK64MMAP\r\n[    0.057700] evm: security.apparmor\r\n[    0.057700] evm: security.ima\r\n[    0.057701] evm: security.capability\r\n[    0.057712] PM: Registering ACPI NVS region [mem 0xddaa1000-0xddbcafff] (1220608 bytes)\r\n[    0.057712] PM: Registering ACPI NVS region [mem 0xde7fd000-0xde83ffff] (274432 bytes)\r\n[    0.057712] clocksource: jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 7645041785100000 ns\r\n[    0.057712] futex hash table entries: 2048 (order: 5, 131072 bytes)\r\n[    0.057712] pinctrl core: initialized pinctrl subsystem\r\n[    0.057712] RTC time: 16:18:52, date: 11/23/18\r\n[    0.057712] NET: Registered protocol family 16\r\n[    0.057712] audit: initializing netlink subsys (disabled)\r\n[    0.057712] audit: type=2000 audit(1542989932.056:1): state=initialized audit_enabled=0 res=1\r\n[    0.057712] cpuidle: using governor ladder\r\n[    0.057712] cpuidle: using governor menu\r\n[    0.057712] ACPI FADT declares the system doesn't support PCIe ASPM, so disable it\r\n[    0.057712] ACPI: bus type PCI registered\r\n[    0.057712] acpiphp: ACPI Hot Plug PCI Controller Driver version: 0.5\r\n[    0.057712] PCI: MMCONFIG for domain 0000 [bus 00-3f] at [mem 0xf8000000-0xfbffffff] (base 0xf8000000)\r\n[    0.057712] PCI: MMCONFIG at [mem 0xf8000000-0xfbffffff] reserved in E820\r\n[    0.057712] PCI: Using configuration type 1 for base access\r\n[    0.057712] core: PMU erratum BJ122, BV98, HSD29 worked around, HT is on\r\n[    0.060157] HugeTLB registered 2.00 MiB page size, pre-allocated 0 pages\r\n[    0.060159] ACPI: Added _OSI(Module Device)\r\n[    0.060159] ACPI: Added _OSI(Processor Device)\r\n[    0.060159] ACPI: Added _OSI(3.0 _SCP Extensions)\r\n[    0.060159] ACPI: Added _OSI(Processor Aggregator Device)\r\n[    0.060159] ACPI: Added _OSI(Linux-Dell-Video)\r\n[    0.060210] ACPI: Executed 1 blocks of module-level executable AML code\r\n[    0.064512] ACPI: Dynamic OEM Table Load:\r\n[    0.064517] ACPI: SSDT 0xFFFF8F32FA4E4000 00083B (v01 PmRef  Cpu0Cst  00003001 INTL 20051117)\r\n[    0.064648] ACPI: Dynamic OEM Table Load:\r\n[    0.064648] ACPI: SSDT 0xFFFF8F32FA74A400 000303 (v01 PmRef  ApIst    00003000 INTL 20051117)\r\n[    0.064648] ACPI: Dynamic OEM Table Load:\r\n[    0.064648] ACPI: SSDT 0xFFFF8F32FA745800 000119 (v01 PmRef  ApCst    00003000 INTL 20051117)\r\n[    0.065567] ACPI: EC: EC started\r\n[    0.065567] ACPI: EC: interrupt blocked\r\n[    0.065590] ACPI: \\_SB_.PCI0.LPCB.EC0_: Used as first EC\r\n[    0.065592] ACPI: \\_SB_.PCI0.LPCB.EC0_: GPE=0x18, EC_CMD/EC_SC=0x66, EC_DATA=0x62\r\n[    0.065593] ACPI: \\_SB_.PCI0.LPCB.EC0_: Used as boot DSDT EC to handle transactions\r\n[    0.065593] ACPI: Interpreter enabled\r\n[    0.065612] ACPI: (supports S0 S3 S4 S5)\r\n[    0.065612] ACPI: Using IOAPIC for interrupt routing\r\n[    0.065639] PCI: Using host bridge windows from ACPI; if necessary, use \"pci=nocrs\" and report a bug\r\n[    0.065841] ACPI: Enabled 9 GPEs in block 00 to 3F\r\n[    0.074686] ACPI: Power Resource [FN00] (off)\r\n[    0.074764] ACPI: Power Resource [FN01] (off)\r\n[    0.074841] ACPI: Power Resource [FN02] (off)\r\n[    0.074918] ACPI: Power Resource [FN03] (off)\r\n[    0.074994] ACPI: Power Resource [FN04] (off)\r\n[    0.075632] ACPI: PCI Root Bridge [PCI0] (domain 0000 [bus 00-3e])\r\n[    0.075636] acpi PNP0A08:00: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]\r\n[    0.075912] acpi PNP0A08:00: _OSC: platform does not support [PCIeHotplug PME]\r\n[    0.076089] acpi PNP0A08:00: _OSC: OS now controls [AER PCIeCapability]\r\n[    0.076090] acpi PNP0A08:00: FADT indicates ASPM is unsupported, using BIOS configuration\r\n[    0.076604] PCI host bridge to bus 0000:00\r\n[    0.076606] pci_bus 0000:00: root bus resource [io  0x0000-0x0cf7 window]\r\n[    0.076607] pci_bus 0000:00: root bus resource [io  0x0d00-0xffff window]\r\n[    0.076608] pci_bus 0000:00: root bus resource [mem 0x000a0000-0x000bffff window]\r\n[    0.076609] pci_bus 0000:00: root bus resource [mem 0x000d0000-0x000d3fff window]\r\n[    0.076610] pci_bus 0000:00: root bus resource [mem 0x000d4000-0x000d7fff window]\r\n[    0.076611] pci_bus 0000:00: root bus resource [mem 0x000d8000-0x000dbfff window]\r\n[    0.076612] pci_bus 0000:00: root bus resource [mem 0x000dc000-0x000dffff window]\r\n[    0.076613] pci_bus 0000:00: root bus resource [mem 0x000e0000-0x000e3fff window]\r\n[    0.076614] pci_bus 0000:00: root bus resource [mem 0x000e4000-0x000e7fff window]\r\n[    0.076615] pci_bus 0000:00: root bus resource [mem 0xe0000000-0xfeafffff window]\r\n[    0.076616] pci_bus 0000:00: root bus resource [bus 00-3e]\r\n[    0.076622] pci 0000:00:00.0: [8086:0150] type 00 class 0x060000\r\n[    0.076707] pci 0000:00:01.0: [8086:0151] type 01 class 0x060400\r\n[    0.076735] pci 0000:00:01.0: PME# supported from D0 D3hot D3cold\r\n[    0.076830] pci 0000:00:14.0: [8086:1e31] type 00 class 0x0c0330\r\n[    0.076852] pci 0000:00:14.0: reg 0x10: [mem 0xf7520000-0xf752ffff 64bit]\r\n[    0.076917] pci 0000:00:14.0: PME# supported from D3hot D3cold\r\n[    0.076992] pci 0000:00:16.0: [8086:1e3a] type 00 class 0x078000\r\n[    0.077014] pci 0000:00:16.0: reg 0x10: [mem 0xf753b000-0xf753b00f 64bit]\r\n[    0.077082] pci 0000:00:16.0: PME# supported from D0 D3hot D3cold\r\n[    0.077159] pci 0000:00:19.0: [8086:1503] type 00 class 0x020000\r\n[    0.077176] pci 0000:00:19.0: reg 0x10: [mem 0xf7500000-0xf751ffff]\r\n[    0.077183] pci 0000:00:19.0: reg 0x14: [mem 0xf7539000-0xf7539fff]\r\n[    0.077191] pci 0000:00:19.0: reg 0x18: [io  0xf040-0xf05f]\r\n[    0.077246] pci 0000:00:19.0: PME# supported from D0 D3hot D3cold\r\n[    0.077319] pci 0000:00:1a.0: [8086:1e2d] type 00 class 0x0c0320\r\n[    0.077339] pci 0000:00:1a.0: reg 0x10: [mem 0xf7538000-0xf75383ff]\r\n[    0.077418] pci 0000:00:1a.0: PME# supported from D0 D3hot D3cold\r\n[    0.077495] pci 0000:00:1b.0: [8086:1e20] type 00 class 0x040300\r\n[    0.077514] pci 0000:00:1b.0: reg 0x10: [mem 0xf7530000-0xf7533fff 64bit]\r\n[    0.077583] pci 0000:00:1b.0: PME# supported from D0 D3hot D3cold\r\n[    0.077664] pci 0000:00:1c.0: [8086:1e10] type 01 class 0x060400\r\n[    0.077742] pci 0000:00:1c.0: PME# supported from D0 D3hot D3cold\r\n[    0.077824] pci 0000:00:1c.1: [8086:1e12] type 01 class 0x060400\r\n[    0.077902] pci 0000:00:1c.1: PME# supported from D0 D3hot D3cold\r\n[    0.077982] pci 0000:00:1c.2: [8086:1e14] type 01 class 0x060400\r\n[    0.078060] pci 0000:00:1c.2: PME# supported from D0 D3hot D3cold\r\n[    0.078139] pci 0000:00:1c.3: [8086:1e16] type 01 class 0x060400\r\n[    0.078217] pci 0000:00:1c.3: PME# supported from D0 D3hot D3cold\r\n[    0.078295] pci 0000:00:1c.4: [8086:244e] type 01 class 0x060401\r\n[    0.078374] pci 0000:00:1c.4: PME# supported from D0 D3hot D3cold\r\n[    0.078455] pci 0000:00:1c.7: [8086:1e1e] type 01 class 0x060400\r\n[    0.078533] pci 0000:00:1c.7: PME# supported from D0 D3hot D3cold\r\n[    0.078612] pci 0000:00:1d.0: [8086:1e26] type 00 class 0x0c0320\r\n[    0.078632] pci 0000:00:1d.0: reg 0x10: [mem 0xf7537000-0xf75373ff]\r\n[    0.078710] pci 0000:00:1d.0: PME# supported from D0 D3hot D3cold\r\n[    0.078787] pci 0000:00:1f.0: [8086:1e44] type 00 class 0x060100\r\n[    0.078956] pci 0000:00:1f.2: [8086:1e02] type 00 class 0x010601\r\n[    0.078972] pci 0000:00:1f.2: reg 0x10: [io  0xf090-0xf097]\r\n[    0.078979] pci 0000:00:1f.2: reg 0x14: [io  0xf080-0xf083]\r\n[    0.078985] pci 0000:00:1f.2: reg 0x18: [io  0xf070-0xf077]\r\n[    0.078991] pci 0000:00:1f.2: reg 0x1c: [io  0xf060-0xf063]\r\n[    0.078998] pci 0000:00:1f.2: reg 0x20: [io  0xf020-0xf03f]\r\n[    0.079004] pci 0000:00:1f.2: reg 0x24: [mem 0xf7536000-0xf75367ff]\r\n[    0.079041] pci 0000:00:1f.2: PME# supported from D3hot\r\n[    0.079110] pci 0000:00:1f.3: [8086:1e22] type 00 class 0x0c0500\r\n[    0.079127] pci 0000:00:1f.3: reg 0x10: [mem 0xf7535000-0xf75350ff 64bit]\r\n[    0.079146] pci 0000:00:1f.3: reg 0x20: [io  0xf000-0xf01f]\r\n[    0.079266] pci 0000:01:00.0: [10de:1b02] type 00 class 0x030000\r\n[    0.079281] pci 0000:01:00.0: reg 0x10: [mem 0xf6000000-0xf6ffffff]\r\n[    0.079290] pci 0000:01:00.0: reg 0x14: [mem 0xe0000000-0xefffffff 64bit pref]\r\n[    0.079298] pci 0000:01:00.0: reg 0x1c: [mem 0xf0000000-0xf1ffffff 64bit pref]\r\n[    0.079304] pci 0000:01:00.0: reg 0x24: [io  0xe000-0xe07f]\r\n[    0.079309] pci 0000:01:00.0: reg 0x30: [mem 0xf7000000-0xf707ffff pref]\r\n[    0.079314] pci 0000:01:00.0: enabling Extended Tags\r\n[    0.079397] pci 0000:01:00.1: [10de:10ef] type 00 class 0x040300\r\n[    0.079409] pci 0000:01:00.1: reg 0x10: [mem 0xf7080000-0xf7083fff]\r\n[    0.079440] pci 0000:01:00.1: enabling Extended Tags\r\n[    0.092022] pci 0000:00:01.0: PCI bridge to [bus 01]\r\n[    0.092026] pci 0000:00:01.0:   bridge window [io  0xe000-0xefff]\r\n[    0.092029] pci 0000:00:01.0:   bridge window [mem 0xf6000000-0xf70fffff]\r\n[    0.092033] pci 0000:00:01.0:   bridge window [mem 0xe0000000-0xf1ffffff 64bit pref]\r\n[    0.092113] pci 0000:00:1c.0: PCI bridge to [bus 02]\r\n[    0.092185] pci 0000:03:00.0: [10ec:8178] type 00 class 0x028000\r\n[    0.092222] pci 0000:03:00.0: reg 0x10: [io  0xd000-0xd0ff]\r\n[    0.092258] pci 0000:03:00.0: reg 0x18: [mem 0xf7400000-0xf7403fff 64bit]\r\n[    0.092405] pci 0000:03:00.0: supports D1 D2\r\n[    0.092406] pci 0000:03:00.0: PME# supported from D0 D1 D2 D3hot D3cold\r\n[    0.104029] pci 0000:00:1c.1: PCI bridge to [bus 03]\r\n[    0.104034] pci 0000:00:1c.1:   bridge window [io  0xd000-0xdfff]\r\n[    0.104038] pci 0000:00:1c.1:   bridge window [mem 0xf7400000-0xf74fffff]\r\n[    0.104133] pci 0000:04:00.0: [1b21:1042] type 00 class 0x0c0330\r\n[    0.104172] pci 0000:04:00.0: reg 0x10: [mem 0xf7300000-0xf7307fff 64bit]\r\n[    0.104336] pci 0000:04:00.0: PME# supported from D3hot D3cold\r\n[    0.116023] pci 0000:00:1c.2: PCI bridge to [bus 04]\r\n[    0.116030] pci 0000:00:1c.2:   bridge window [mem 0xf7300000-0xf73fffff]\r\n[    0.116124] pci 0000:05:00.0: [1b21:0612] type 00 class 0x010601\r\n[    0.116152] pci 0000:05:00.0: reg 0x10: [io  0xc050-0xc057]\r\n[    0.116165] pci 0000:05:00.0: reg 0x14: [io  0xc040-0xc043]\r\n[    0.116177] pci 0000:05:00.0: reg 0x18: [io  0xc030-0xc037]\r\n[    0.116189] pci 0000:05:00.0: reg 0x1c: [io  0xc020-0xc023]\r\n[    0.116201] pci 0000:05:00.0: reg 0x20: [io  0xc000-0xc01f]\r\n[    0.116214] pci 0000:05:00.0: reg 0x24: [mem 0xf7200000-0xf72001ff]\r\n[    0.128022] pci 0000:00:1c.3: PCI bridge to [bus 05]\r\n[    0.128027] pci 0000:00:1c.3:   bridge window [io  0xc000-0xcfff]\r\n[    0.128031] pci 0000:00:1c.3:   bridge window [mem 0xf7200000-0xf72fffff]\r\n[    0.128120] pci 0000:06:00.0: [1b21:1080] type 01 class 0x060401\r\n[    0.128273] pci 0000:00:1c.4: PCI bridge to [bus 06-07] (subtractive decode)\r\n[    0.128282] pci 0000:00:1c.4:   bridge window [io  0x0000-0x0cf7 window] (subtractive decode)\r\n[    0.128283] pci 0000:00:1c.4:   bridge window [io  0x0d00-0xffff window] (subtractive decode)\r\n[    0.128284] pci 0000:00:1c.4:   bridge window [mem 0x000a0000-0x000bffff window] (subtractive decode)\r\n[    0.128285] pci 0000:00:1c.4:   bridge window [mem 0x000d0000-0x000d3fff window] (subtractive decode)\r\n[    0.128286] pci 0000:00:1c.4:   bridge window [mem 0x000d4000-0x000d7fff window] (subtractive decode)\r\n[    0.128287] pci 0000:00:1c.4:   bridge window [mem 0x000d8000-0x000dbfff window] (subtractive decode)\r\n[    0.128288] pci 0000:00:1c.4:   bridge window [mem 0x000dc000-0x000dffff window] (subtractive decode)\r\n[    0.128289] pci 0000:00:1c.4:   bridge window [mem 0x000e0000-0x000e3fff window] (subtractive decode)\r\n[    0.128290] pci 0000:00:1c.4:   bridge window [mem 0x000e4000-0x000e7fff window] (subtractive decode)\r\n[    0.128291] pci 0000:00:1c.4:   bridge window [mem 0xe0000000-0xfeafffff window] (subtractive decode)\r\n[    0.128395] pci 0000:06:00.0: PCI bridge to [bus 07] (subtractive decode)\r\n[    0.128415] pci 0000:06:00.0:   bridge window [io  0x0000-0x0cf7 window] (subtractive decode)\r\n[    0.128416] pci 0000:06:00.0:   bridge window [io  0x0d00-0xffff window] (subtractive decode)\r\n[    0.128417] pci 0000:06:00.0:   bridge window [mem 0x000a0000-0x000bffff window] (subtractive decode)\r\n[    0.128418] pci 0000:06:00.0:   bridge window [mem 0x000d0000-0x000d3fff window] (subtractive decode)\r\n[    0.128419] pci 0000:06:00.0:   bridge window [mem 0x000d4000-0x000d7fff window] (subtractive decode)\r\n[    0.128420] pci 0000:06:00.0:   bridge window [mem 0x000d8000-0x000dbfff window] (subtractive decode)\r\n[    0.128421] pci 0000:06:00.0:   bridge window [mem 0x000dc000-0x000dffff window] (subtractive decode)\r\n[    0.128422] pci 0000:06:00.0:   bridge window [mem 0x000e0000-0x000e3fff window] (subtractive decode)\r\n[    0.128423] pci 0000:06:00.0:   bridge window [mem 0x000e4000-0x000e7fff window] (subtractive decode)\r\n[    0.128424] pci 0000:06:00.0:   bridge window [mem 0xe0000000-0xfeafffff window] (subtractive decode)\r\n[    0.128498] pci 0000:08:00.0: [1b21:1042] type 00 class 0x0c0330\r\n[    0.128538] pci 0000:08:00.0: reg 0x10: [mem 0xf7100000-0xf7107fff 64bit]\r\n[    0.128702] pci 0000:08:00.0: PME# supported from D3hot D3cold\r\n[    0.140023] pci 0000:00:1c.7: PCI bridge to [bus 08]\r\n[    0.140030] pci 0000:00:1c.7:   bridge window [mem 0xf7100000-0xf71fffff]\r\n[    0.140551] ACPI: PCI Interrupt Link [LNKA] (IRQs 3 4 5 6 10 *11 12 14 15)\r\n[    0.140613] ACPI: PCI Interrupt Link [LNKB] (IRQs 3 4 5 6 *10 11 12 14 15)\r\n[    0.140673] ACPI: PCI Interrupt Link [LNKC] (IRQs 3 4 5 6 10 *11 12 14 15)\r\n[    0.140733] ACPI: PCI Interrupt Link [LNKD] (IRQs 3 4 *5 6 10 11 12 14 15)\r\n[    0.140793] ACPI: PCI Interrupt Link [LNKE] (IRQs *3 4 5 6 10 11 12 14 15)\r\n[    0.140853] ACPI: PCI Interrupt Link [LNKF] (IRQs 3 4 5 6 10 11 12 14 15) *0, disabled.\r\n[    0.140913] ACPI: PCI Interrupt Link [LNKG] (IRQs 3 *4 5 6 10 11 12 14 15)\r\n[    0.140973] ACPI: PCI Interrupt Link [LNKH] (IRQs 3 4 5 6 *10 11 12 14 15)\r\n[    0.141222] ACPI: EC: interrupt unblocked\r\n[    0.141227] ACPI: EC: event unblocked\r\n[    0.141232] ACPI: \\_SB_.PCI0.LPCB.EC0_: GPE=0x18, EC_CMD/EC_SC=0x66, EC_DATA=0x62\r\n[    0.141233] ACPI: \\_SB_.PCI0.LPCB.EC0_: Used as boot DSDT EC to handle transactions and events\r\n[    0.141386] SCSI subsystem initialized\r\n[    0.141396] libata version 3.00 loaded.\r\n[    0.141396] pci 0000:01:00.0: vgaarb: setting as boot VGA device\r\n[    0.141396] pci 0000:01:00.0: vgaarb: VGA device added: decodes=io+mem,owns=io+mem,locks=none\r\n[    0.141396] pci 0000:01:00.0: vgaarb: bridge control possible\r\n[    0.141396] vgaarb: loaded\r\n[    0.141396] ACPI: bus type USB registered\r\n[    0.141396] usbcore: registered new interface driver usbfs\r\n[    0.141396] usbcore: registered new interface driver hub\r\n[    0.141396] usbcore: registered new device driver usb\r\n[    0.141396] EDAC MC: Ver: 3.0.0\r\n[    0.141396] PCI: Using ACPI for IRQ routing\r\n[    0.141396] PCI: pci_cache_line_size set to 64 bytes\r\n[    0.141396] e820: reserve RAM buffer [mem 0x0009d800-0x0009ffff]\r\n[    0.141396] e820: reserve RAM buffer [mem 0xdd0e0000-0xdfffffff]\r\n[    0.141396] e820: reserve RAM buffer [mem 0xde7fd000-0xdfffffff]\r\n[    0.141396] e820: reserve RAM buffer [mem 0xdec62000-0xdfffffff]\r\n[    0.141396] e820: reserve RAM buffer [mem 0xdf000000-0xdfffffff]\r\n[    0.141396] e820: reserve RAM buffer [mem 0x81f000000-0x81fffffff]\r\n[    0.141445] NetLabel: Initializing\r\n[    0.141445] NetLabel:  domain hash size = 128\r\n[    0.141445] NetLabel:  protocols = UNLABELED CIPSOv4 CALIPSO\r\n[    0.141456] NetLabel:  unlabeled traffic allowed by default\r\n[    0.141466] hpet0: at MMIO 0xfed00000, IRQs 2, 8, 0, 0, 0, 0, 0, 0\r\n[    0.141466] hpet0: 8 comparators, 64-bit 14.318180 MHz counter\r\n[    0.145016] clocksource: Switched to clocksource hpet\r\n[    0.151617] VFS: Disk quotas dquot_6.6.0\r\n[    0.151630] VFS: Dquot-cache hash table entries: 512 (order 0, 4096 bytes)\r\n[    0.151699] AppArmor: AppArmor Filesystem Enabled\r\n[    0.151716] pnp: PnP ACPI init\r\n[    0.151804] system 00:00: [mem 0xfed40000-0xfed44fff] has been reserved\r\n[    0.151808] system 00:00: Plug and Play ACPI device, IDs PNP0c01 (active)\r\n[    0.151881] system 00:01: [io  0x0680-0x069f] has been reserved\r\n[    0.151882] system 00:01: [io  0x1000-0x100f] has been reserved\r\n[    0.151883] system 00:01: [io  0xffff] has been reserved\r\n[    0.151884] system 00:01: [io  0xffff] has been reserved\r\n[    0.151885] system 00:01: [io  0x0400-0x0453] has been reserved\r\n[    0.151887] system 00:01: [io  0x0458-0x047f] has been reserved\r\n[    0.151888] system 00:01: [io  0x0500-0x057f] has been reserved\r\n[    0.151889] system 00:01: [io  0x164e-0x164f] has been reserved\r\n[    0.151892] system 00:01: Plug and Play ACPI device, IDs PNP0c02 (active)\r\n[    0.151910] pnp 00:02: Plug and Play ACPI device, IDs PNP0b00 (active)\r\n[    0.151955] system 00:03: [io  0x0454-0x0457] has been reserved\r\n[    0.151958] system 00:03: Plug and Play ACPI device, IDs INT3f0d PNP0c02 (active)\r\n[    0.152061] system 00:04: [io  0x0290-0x029f] has been reserved\r\n[    0.152064] system 00:04: Plug and Play ACPI device, IDs PNP0c02 (active)\r\n[    0.152115] system 00:05: [io  0x04d0-0x04d1] has been reserved\r\n[    0.152118] system 00:05: Plug and Play ACPI device, IDs PNP0c02 (active)\r\n[    0.152352] system 00:06: [mem 0xfed1c000-0xfed1ffff] has been reserved\r\n[    0.152354] system 00:06: [mem 0xfed10000-0xfed17fff] has been reserved\r\n[    0.152355] system 00:06: [mem 0xfed18000-0xfed18fff] has been reserved\r\n[    0.152356] system 00:06: [mem 0xfed19000-0xfed19fff] has been reserved\r\n[    0.152357] system 00:06: [mem 0xf8000000-0xfbffffff] has been reserved\r\n[    0.152358] system 00:06: [mem 0xfed20000-0xfed3ffff] has been reserved\r\n[    0.152360] system 00:06: [mem 0xfed90000-0xfed93fff] could not be reserved\r\n[    0.152361] system 00:06: [mem 0xfed45000-0xfed8ffff] has been reserved\r\n[    0.152362] system 00:06: [mem 0xff000000-0xffffffff] has been reserved\r\n[    0.152363] system 00:06: [mem 0xfee00000-0xfeefffff] could not be reserved\r\n[    0.152364] system 00:06: [mem 0xf2000000-0xf2000fff] has been reserved\r\n[    0.152367] system 00:06: Plug and Play ACPI device, IDs PNP0c02 (active)\r\n[    0.152498] pnp: PnP ACPI: found 7 devices\r\n[    0.158218] clocksource: acpi_pm: mask: 0xffffff max_cycles: 0xffffff, max_idle_ns: 2085701024 ns\r\n[    0.158232] pci 0000:00:1c.0: bridge window [io  0x1000-0x0fff] to [bus 02] add_size 1000\r\n[    0.158234] pci 0000:00:1c.0: bridge window [mem 0x00100000-0x000fffff 64bit pref] to [bus 02] add_size 200000 add_align 100000\r\n[    0.158235] pci 0000:00:1c.0: bridge window [mem 0x00100000-0x000fffff] to [bus 02] add_size 200000 add_align 100000\r\n[    0.158283] pci 0000:00:1c.0: BAR 14: assigned [mem 0xf2100000-0xf22fffff]\r\n[    0.158288] pci 0000:00:1c.0: BAR 15: assigned [mem 0xf2300000-0xf24fffff 64bit pref]\r\n[    0.158290] pci 0000:00:1c.0: BAR 13: assigned [io  0x2000-0x2fff]\r\n[    0.158292] pci 0000:00:01.0: PCI bridge to [bus 01]\r\n[    0.158293] pci 0000:00:01.0:   bridge window [io  0xe000-0xefff]\r\n[    0.158295] pci 0000:00:01.0:   bridge window [mem 0xf6000000-0xf70fffff]\r\n[    0.158297] pci 0000:00:01.0:   bridge window [mem 0xe0000000-0xf1ffffff 64bit pref]\r\n[    0.158299] pci 0000:00:1c.0: PCI bridge to [bus 02]\r\n[    0.158301] pci 0000:00:1c.0:   bridge window [io  0x2000-0x2fff]\r\n[    0.158305] pci 0000:00:1c.0:   bridge window [mem 0xf2100000-0xf22fffff]\r\n[    0.158308] pci 0000:00:1c.0:   bridge window [mem 0xf2300000-0xf24fffff 64bit pref]\r\n[    0.158312] pci 0000:00:1c.1: PCI bridge to [bus 03]\r\n[    0.158314] pci 0000:00:1c.1:   bridge window [io  0xd000-0xdfff]\r\n[    0.158318] pci 0000:00:1c.1:   bridge window [mem 0xf7400000-0xf74fffff]\r\n[    0.158325] pci 0000:00:1c.2: PCI bridge to [bus 04]\r\n[    0.158329] pci 0000:00:1c.2:   bridge window [mem 0xf7300000-0xf73fffff]\r\n[    0.158335] pci 0000:00:1c.3: PCI bridge to [bus 05]\r\n[    0.158337] pci 0000:00:1c.3:   bridge window [io  0xc000-0xcfff]\r\n[    0.158341] pci 0000:00:1c.3:   bridge window [mem 0xf7200000-0xf72fffff]\r\n[    0.158348] pci 0000:06:00.0: PCI bridge to [bus 07]\r\n[    0.158366] pci 0000:00:1c.4: PCI bridge to [bus 06-07]\r\n[    0.158376] pci 0000:00:1c.7: PCI bridge to [bus 08]\r\n[    0.158379] pci 0000:00:1c.7:   bridge window [mem 0xf7100000-0xf71fffff]\r\n[    0.158387] pci_bus 0000:00: resource 4 [io  0x0000-0x0cf7 window]\r\n[    0.158388] pci_bus 0000:00: resource 5 [io  0x0d00-0xffff window]\r\n[    0.158389] pci_bus 0000:00: resource 6 [mem 0x000a0000-0x000bffff window]\r\n[    0.158390] pci_bus 0000:00: resource 7 [mem 0x000d0000-0x000d3fff window]\r\n[    0.158391] pci_bus 0000:00: resource 8 [mem 0x000d4000-0x000d7fff window]\r\n[    0.158392] pci_bus 0000:00: resource 9 [mem 0x000d8000-0x000dbfff window]\r\n[    0.158393] pci_bus 0000:00: resource 10 [mem 0x000dc000-0x000dffff window]\r\n[    0.158394] pci_bus 0000:00: resource 11 [mem 0x000e0000-0x000e3fff window]\r\n[    0.158395] pci_bus 0000:00: resource 12 [mem 0x000e4000-0x000e7fff window]\r\n[    0.158396] pci_bus 0000:00: resource 13 [mem 0xe0000000-0xfeafffff window]\r\n[    0.158397] pci_bus 0000:01: resource 0 [io  0xe000-0xefff]\r\n[    0.158398] pci_bus 0000:01: resource 1 [mem 0xf6000000-0xf70fffff]\r\n[    0.158399] pci_bus 0000:01: resource 2 [mem 0xe0000000-0xf1ffffff 64bit pref]\r\n[    0.158400] pci_bus 0000:02: resource 0 [io  0x2000-0x2fff]\r\n[    0.158401] pci_bus 0000:02: resource 1 [mem 0xf2100000-0xf22fffff]\r\n[    0.158402] pci_bus 0000:02: resource 2 [mem 0xf2300000-0xf24fffff 64bit pref]\r\n[    0.158403] pci_bus 0000:03: resource 0 [io  0xd000-0xdfff]\r\n[    0.158404] pci_bus 0000:03: resource 1 [mem 0xf7400000-0xf74fffff]\r\n[    0.158405] pci_bus 0000:04: resource 1 [mem 0xf7300000-0xf73fffff]\r\n[    0.158406] pci_bus 0000:05: resource 0 [io  0xc000-0xcfff]\r\n[    0.158407] pci_bus 0000:05: resource 1 [mem 0xf7200000-0xf72fffff]\r\n[    0.158408] pci_bus 0000:06: resource 4 [io  0x0000-0x0cf7 window]\r\n[    0.158409] pci_bus 0000:06: resource 5 [io  0x0d00-0xffff window]\r\n[    0.158410] pci_bus 0000:06: resource 6 [mem 0x000a0000-0x000bffff window]\r\n[    0.158411] pci_bus 0000:06: resource 7 [mem 0x000d0000-0x000d3fff window]\r\n[    0.158412] pci_bus 0000:06: resource 8 [mem 0x000d4000-0x000d7fff window]\r\n[    0.158413] pci_bus 0000:06: resource 9 [mem 0x000d8000-0x000dbfff window]\r\n[    0.158414] pci_bus 0000:06: resource 10 [mem 0x000dc000-0x000dffff window]\r\n[    0.158415] pci_bus 0000:06: resource 11 [mem 0x000e0000-0x000e3fff window]\r\n[    0.158416] pci_bus 0000:06: resource 12 [mem 0x000e4000-0x000e7fff window]\r\n[    0.158417] pci_bus 0000:06: resource 13 [mem 0xe0000000-0xfeafffff window]\r\n[    0.158418] pci_bus 0000:07: resource 4 [io  0x0000-0x0cf7 window]\r\n[    0.158419] pci_bus 0000:07: resource 5 [io  0x0d00-0xffff window]\r\n[    0.158420] pci_bus 0000:07: resource 6 [mem 0x000a0000-0x000bffff window]\r\n[    0.158421] pci_bus 0000:07: resource 7 [mem 0x000d0000-0x000d3fff window]\r\n[    0.158422] pci_bus 0000:07: resource 8 [mem 0x000d4000-0x000d7fff window]\r\n[    0.158423] pci_bus 0000:07: resource 9 [mem 0x000d8000-0x000dbfff window]\r\n[    0.158424] pci_bus 0000:07: resource 10 [mem 0x000dc000-0x000dffff window]\r\n[    0.158425] pci_bus 0000:07: resource 11 [mem 0x000e0000-0x000e3fff window]\r\n[    0.158426] pci_bus 0000:07: resource 12 [mem 0x000e4000-0x000e7fff window]\r\n[    0.158427] pci_bus 0000:07: resource 13 [mem 0xe0000000-0xfeafffff window]\r\n[    0.158428] pci_bus 0000:08: resource 1 [mem 0xf7100000-0xf71fffff]\r\n[    0.158515] NET: Registered protocol family 2\r\n[    0.158679] TCP established hash table entries: 262144 (order: 9, 2097152 bytes)\r\n[    0.158938] TCP bind hash table entries: 65536 (order: 8, 1048576 bytes)\r\n[    0.159044] TCP: Hash tables configured (established 262144 bind 65536)\r\n[    0.159078] UDP hash table entries: 16384 (order: 7, 524288 bytes)\r\n[    0.159144] UDP-Lite hash table entries: 16384 (order: 7, 524288 bytes)\r\n[    0.159230] NET: Registered protocol family 1\r\n[    0.208142] pci 0000:01:00.0: Video device with shadowed ROM at [mem 0x000c0000-0x000dffff]\r\n[    0.208151] PCI: CLS mismatch (64 != 32), using 64 bytes\r\n[    0.208470] Unpacking initramfs...\r\n[    0.825459] Freeing initrd memory: 60332K\r\n[    0.825489] PCI-DMA: Using software bounce buffering for IO (SWIOTLB)\r\n[    0.825491] software IO TLB [mem 0xd90e0000-0xdd0e0000] (64MB) mapped at [        (ptrval)-        (ptrval)]\r\n[    0.825767] Scanning for low memory corruption every 60 seconds\r\n[    0.826263] Initialise system trusted keyrings\r\n[    0.826271] Key type blacklist registered\r\n[    0.826298] workingset: timestamp_bits=36 max_order=23 bucket_order=0\r\n[    0.827055] zbud: loaded\r\n[    0.827400] squashfs: version 4.0 (2009/01/31) Phillip Lougher\r\n[    0.827492] fuse init (API version 7.26)\r\n[    0.828426] Key type asymmetric registered\r\n[    0.828426] Asymmetric key parser 'x509' registered\r\n[    0.828447] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 246)\r\n[    0.828484] io scheduler noop registered\r\n[    0.828484] io scheduler deadline registered\r\n[    0.828512] io scheduler cfq registered (default)\r\n[    0.829220] intel_idle: MWAIT substates: 0x1120\r\n[    0.829220] intel_idle: v0.4.1 model 0x3A\r\n[    0.829405] intel_idle: lapic_timer_reliable_states 0xffffffff\r\n[    0.829475] input: Power Button as /devices/LNXSYSTM:00/LNXSYBUS:00/PNP0C0C:00/input/input0\r\n[    0.829495] ACPI: Power Button [PWRB]\r\n[    0.829520] input: Power Button as /devices/LNXSYSTM:00/LNXPWRBN:00/input/input1\r\n[    0.829534] ACPI: Power Button [PWRF]\r\n[    0.830275] (NULL device *): hwmon_device_register() is deprecated. Please convert the driver to use hwmon_device_register_with_info().\r\n[    0.830420] thermal LNXTHERM:00: registered as thermal_zone0\r\n[    0.830421] ACPI: Thermal Zone [TZ00] (28 C)\r\n[    0.830685] thermal LNXTHERM:01: registered as thermal_zone1\r\n[    0.830685] ACPI: Thermal Zone [TZ01] (30 C)\r\n[    0.830780] Serial: 8250/16550 driver, 32 ports, IRQ sharing enabled\r\n[    0.832109] Linux agpgart interface v0.103\r\n[    0.833478] loop: module loaded\r\n[    0.833588] libphy: Fixed MDIO Bus: probed\r\n[    0.833589] tun: Universal TUN/TAP device driver, 1.6\r\n[    0.833612] PPP generic driver version 2.4.2\r\n[    0.833640] ehci_hcd: USB 2.0 'Enhanced' Host Controller (EHCI) Driver\r\n[    0.833641] ehci-pci: EHCI PCI platform driver\r\n[    0.833733] ehci-pci 0000:00:1a.0: EHCI Host Controller\r\n[    0.833737] ehci-pci 0000:00:1a.0: new USB bus registered, assigned bus number 1\r\n[    0.833747] ehci-pci 0000:00:1a.0: debug port 2\r\n[    0.837641] ehci-pci 0000:00:1a.0: cache line size of 64 is not supported\r\n[    0.837658] ehci-pci 0000:00:1a.0: irq 16, io mem 0xf7538000\r\n[    0.852019] ehci-pci 0000:00:1a.0: USB 2.0 started, EHCI 1.00\r\n[    0.852054] usb usb1: New USB device found, idVendor=1d6b, idProduct=0002\r\n[    0.852056] usb usb1: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.852057] usb usb1: Product: EHCI Host Controller\r\n[    0.852058] usb usb1: Manufacturer: Linux 4.15.0-33-generic ehci_hcd\r\n[    0.852059] usb usb1: SerialNumber: 0000:00:1a.0\r\n[    0.852191] hub 1-0:1.0: USB hub found\r\n[    0.852197] hub 1-0:1.0: 2 ports detected\r\n[    0.852394] ehci-pci 0000:00:1d.0: EHCI Host Controller\r\n[    0.852397] ehci-pci 0000:00:1d.0: new USB bus registered, assigned bus number 2\r\n[    0.852406] ehci-pci 0000:00:1d.0: debug port 2\r\n[    0.856286] ehci-pci 0000:00:1d.0: cache line size of 64 is not supported\r\n[    0.856296] ehci-pci 0000:00:1d.0: irq 23, io mem 0xf7537000\r\n[    0.872021] ehci-pci 0000:00:1d.0: USB 2.0 started, EHCI 1.00\r\n[    0.872049] usb usb2: New USB device found, idVendor=1d6b, idProduct=0002\r\n[    0.872050] usb usb2: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.872051] usb usb2: Product: EHCI Host Controller\r\n[    0.872052] usb usb2: Manufacturer: Linux 4.15.0-33-generic ehci_hcd\r\n[    0.872053] usb usb2: SerialNumber: 0000:00:1d.0\r\n[    0.872190] hub 2-0:1.0: USB hub found\r\n[    0.872195] hub 2-0:1.0: 2 ports detected\r\n[    0.872301] ehci-platform: EHCI generic platform driver\r\n[    0.872308] ohci_hcd: USB 1.1 'Open' Host Controller (OHCI) Driver\r\n[    0.872310] ohci-pci: OHCI PCI platform driver\r\n[    0.872316] ohci-platform: OHCI generic platform driver\r\n[    0.872320] uhci_hcd: USB Universal Host Controller Interface driver\r\n[    0.872412] xhci_hcd 0000:00:14.0: xHCI Host Controller\r\n[    0.872416] xhci_hcd 0000:00:14.0: new USB bus registered, assigned bus number 3\r\n[    0.873487] xhci_hcd 0000:00:14.0: hcc params 0x20007181 hci version 0x100 quirks 0x0000b930\r\n[    0.873491] xhci_hcd 0000:00:14.0: cache line size of 64 is not supported\r\n[    0.873591] usb usb3: New USB device found, idVendor=1d6b, idProduct=0002\r\n[    0.873592] usb usb3: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.873593] usb usb3: Product: xHCI Host Controller\r\n[    0.873594] usb usb3: Manufacturer: Linux 4.15.0-33-generic xhci-hcd\r\n[    0.873595] usb usb3: SerialNumber: 0000:00:14.0\r\n[    0.873717] hub 3-0:1.0: USB hub found\r\n[    0.873726] hub 3-0:1.0: 4 ports detected\r\n[    0.874030] xhci_hcd 0000:00:14.0: xHCI Host Controller\r\n[    0.874032] xhci_hcd 0000:00:14.0: new USB bus registered, assigned bus number 4\r\n[    0.874035] xhci_hcd 0000:00:14.0: Host supports USB 3.0  SuperSpeed\r\n[    0.874056] usb usb4: New USB device found, idVendor=1d6b, idProduct=0003\r\n[    0.874058] usb usb4: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.874058] usb usb4: Product: xHCI Host Controller\r\n[    0.874059] usb usb4: Manufacturer: Linux 4.15.0-33-generic xhci-hcd\r\n[    0.874060] usb usb4: SerialNumber: 0000:00:14.0\r\n[    0.874184] hub 4-0:1.0: USB hub found\r\n[    0.874192] hub 4-0:1.0: 4 ports detected\r\n[    0.874568] xhci_hcd 0000:04:00.0: xHCI Host Controller\r\n[    0.874571] xhci_hcd 0000:04:00.0: new USB bus registered, assigned bus number 5\r\n[    0.933951] xhci_hcd 0000:04:00.0: hcc params 0x0200f180 hci version 0x96 quirks 0x00080000\r\n[    0.934196] usb usb5: New USB device found, idVendor=1d6b, idProduct=0002\r\n[    0.934197] usb usb5: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.934198] usb usb5: Product: xHCI Host Controller\r\n[    0.934199] usb usb5: Manufacturer: Linux 4.15.0-33-generic xhci-hcd\r\n[    0.934200] usb usb5: SerialNumber: 0000:04:00.0\r\n[    0.934342] hub 5-0:1.0: USB hub found\r\n[    0.934350] hub 5-0:1.0: 2 ports detected\r\n[    0.934415] xhci_hcd 0000:04:00.0: xHCI Host Controller\r\n[    0.934417] xhci_hcd 0000:04:00.0: new USB bus registered, assigned bus number 6\r\n[    0.934419] xhci_hcd 0000:04:00.0: Host supports USB 3.0  SuperSpeed\r\n[    0.934436] usb usb6: We don't know the algorithms for LPM for this host, disabling LPM.\r\n[    0.934447] usb usb6: New USB device found, idVendor=1d6b, idProduct=0003\r\n[    0.934448] usb usb6: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.934449] usb usb6: Product: xHCI Host Controller\r\n[    0.934450] usb usb6: Manufacturer: Linux 4.15.0-33-generic xhci-hcd\r\n[    0.934451] usb usb6: SerialNumber: 0000:04:00.0\r\n[    0.934595] hub 6-0:1.0: USB hub found\r\n[    0.934604] hub 6-0:1.0: 2 ports detected\r\n[    0.934726] xhci_hcd 0000:08:00.0: xHCI Host Controller\r\n[    0.934729] xhci_hcd 0000:08:00.0: new USB bus registered, assigned bus number 7\r\n[    0.994106] xhci_hcd 0000:08:00.0: hcc params 0x0200f180 hci version 0x96 quirks 0x00080000\r\n[    0.994345] usb usb7: New USB device found, idVendor=1d6b, idProduct=0002\r\n[    0.994346] usb usb7: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.994347] usb usb7: Product: xHCI Host Controller\r\n[    0.994348] usb usb7: Manufacturer: Linux 4.15.0-33-generic xhci-hcd\r\n[    0.994349] usb usb7: SerialNumber: 0000:08:00.0\r\n[    0.994460] hub 7-0:1.0: USB hub found\r\n[    0.994466] hub 7-0:1.0: 2 ports detected\r\n[    0.994526] xhci_hcd 0000:08:00.0: xHCI Host Controller\r\n[    0.994529] xhci_hcd 0000:08:00.0: new USB bus registered, assigned bus number 8\r\n[    0.994531] xhci_hcd 0000:08:00.0: Host supports USB 3.0  SuperSpeed\r\n[    0.994548] usb usb8: We don't know the algorithms for LPM for this host, disabling LPM.\r\n[    0.994558] usb usb8: New USB device found, idVendor=1d6b, idProduct=0003\r\n[    0.994560] usb usb8: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.994560] usb usb8: Product: xHCI Host Controller\r\n[    0.994561] usb usb8: Manufacturer: Linux 4.15.0-33-generic xhci-hcd\r\n[    0.994562] usb usb8: SerialNumber: 0000:08:00.0\r\n[    0.994674] hub 8-0:1.0: USB hub found\r\n[    0.994680] hub 8-0:1.0: 2 ports detected\r\n[    0.994762] i8042: PNP: No PS/2 controller found.\r\n[    0.994906] mousedev: PS/2 mouse device common for all mice\r\n[    0.995062] rtc_cmos 00:02: RTC can wake from S4\r\n[    0.995172] rtc_cmos 00:02: rtc core: registered rtc_cmos as rtc0\r\n[    0.995196] rtc_cmos 00:02: alarms up to one month, y3k, 242 bytes nvram, hpet irqs\r\n[    0.995201] i2c /dev entries driver\r\n[    0.995229] device-mapper: uevent: version 1.0.3\r\n[    0.995269] device-mapper: ioctl: 4.37.0-ioctl (2017-09-20) initialised: dm-devel@redhat.com\r\n[    0.995273] intel_pstate: Intel P-state driver initializing\r\n[    0.995712] ledtrig-cpu: registered to indicate activity on CPUs\r\n[    0.996120] NET: Registered protocol family 10\r\n[    1.000700] Segment Routing with IPv6\r\n[    1.000713] NET: Registered protocol family 17\r\n[    1.000756] Key type dns_resolver registered\r\n[    1.001210] RAS: Correctable Errors collector initialized.\r\n[    1.001227] microcode: sig=0x306a9, pf=0x2, revision=0x20\r\n[    1.001377] microcode: Microcode Update Driver: v2.2.\r\n[    1.001383] sched_clock: Marking stable (1001373622, 0)->(1108854930, -107481308)\r\n[    1.001625] registered taskstats version 1\r\n[    1.001631] Loading compiled-in X.509 certificates\r\n[    1.003832] Loaded X.509 cert 'Build time autogenerated kernel key: d918b280ed158d77154089242222928ec1ab43e6'\r\n[    1.003844] zswap: loaded using pool lzo/zbud\r\n[    1.007151] Key type big_key registered\r\n[    1.007154] Key type trusted registered\r\n[    1.008773] Key type encrypted registered\r\n[    1.008774] AppArmor: AppArmor sha1 policy hashing enabled\r\n[    1.008776] ima: No TPM chip found, activating TPM-bypass! (rc=-19)\r\n[    1.008790] evm: HMAC attrs: 0x1\r\n[    1.009056]   Magic number: 10:727:338\r\n[    1.009066] thermal thermal_zone1: hash matches\r\n[    1.009072] tty tty59: hash matches\r\n[    1.009110] memory memory188: hash matches\r\n[    1.009168] rtc_cmos 00:02: setting system clock to 2018-11-23 16:18:53 UTC (1542989933)\r\n[    1.009237] BIOS EDD facility v0.16 2004-Jun-25, 0 devices found\r\n[    1.009237] EDD information not available.\r\n[    1.010655] Freeing unused kernel memory: 2404K\r\n[    1.060045] Write protecting the kernel read-only data: 20480k\r\n[    1.060512] Freeing unused kernel memory: 2008K\r\n[    1.063209] Freeing unused kernel memory: 1892K\r\n[    1.068156] x86/mm: Checked W+X mappings: passed, no W+X pages found.\r\n[    1.068156] x86/mm: Checking user space page tables\r\n[    1.072847] x86/mm: Checked W+X mappings: passed, no W+X pages found.\r\n[    1.080604] random: systemd-udevd: uninitialized urandom read (16 bytes read)\r\n[    1.080667] random: systemd-udevd: uninitialized urandom read (16 bytes read)\r\n[    1.080676] random: systemd-udevd: uninitialized urandom read (16 bytes read)\r\n[    1.124534] pps_core: LinuxPPS API ver. 1 registered\r\n[    1.124535] pps_core: Software ver. 5.3.6 - Copyright 2005-2007 Rodolfo Giometti <giometti@linux.it>\r\n[    1.125575] PTP clock support registered\r\n[    1.127616] ahci 0000:00:1f.2: version 3.0\r\n[    1.129515] e1000e: Intel(R) PRO/1000 Network Driver - 3.2.6-k\r\n[    1.129515] e1000e: Copyright(c) 1999 - 2015 Intel Corporation.\r\n[    1.137920] ahci 0000:00:1f.2: AHCI 0001.0300 32 slots 6 ports 6 Gbps 0x24 impl SATA mode\r\n[    1.137923] ahci 0000:00:1f.2: flags: 64bit ncq led clo pio slum part ems apst \r\n[    1.138506] nvidia: loading out-of-tree module taints kernel.\r\n[    1.138511] nvidia: module license 'NVIDIA' taints kernel.\r\n[    1.138512] Disabling lock debugging due to kernel taint\r\n[    1.142105] nvidia: module verification failed: signature and/or required key missing - tainting kernel\r\n[    1.147896] nvidia-nvlink: Nvlink Core is being initialized, major device number 242\r\n[    1.148170] nvidia 0000:01:00.0: vgaarb: changed VGA decodes: olddecodes=io+mem,decodes=none:owns=io+mem\r\n[    1.148228] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  384.130  Wed Mar 21 03:37:26 PDT 2018 (using threaded interrupts)\r\n[    1.148604] scsi host0: ahci\r\n[    1.149285] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms  384.130  Wed Mar 21 02:59:49 PDT 2018\r\n[    1.149754] [drm] [nvidia-drm] [GPU ID 0x00000100] Loading driver\r\n[    1.149755] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:01:00.0 on minor 0\r\n[    1.152030] scsi host1: ahci\r\n[    1.156210] scsi host2: ahci\r\n[    1.156371] scsi host3: ahci\r\n[    1.156472] scsi host4: ahci\r\n[    1.156534] scsi host5: ahci\r\n[    1.156572] ata1: DUMMY\r\n[    1.156573] ata2: DUMMY\r\n[    1.156576] ata3: SATA max UDMA/133 abar m2048@0xf7536000 port 0xf7536200 irq 42\r\n[    1.156577] ata4: DUMMY\r\n[    1.156577] ata5: DUMMY\r\n[    1.156580] ata6: SATA max UDMA/133 abar m2048@0xf7536000 port 0xf7536380 irq 42\r\n[    1.156790] ahci 0000:05:00.0: SSS flag set, parallel bus scan disabled\r\n[    1.156834] ahci 0000:05:00.0: AHCI 0001.0200 32 slots 2 ports 6 Gbps 0x3 impl SATA mode\r\n[    1.156835] ahci 0000:05:00.0: flags: 64bit ncq sntf stag led clo pmp pio slum part ccc sxs \r\n[    1.157003] scsi host6: ahci\r\n[    1.157065] scsi host7: ahci\r\n[    1.157093] ata7: SATA max UDMA/133 abar m512@0xf7200000 port 0xf7200100 irq 43\r\n[    1.157096] ata8: SATA max UDMA/133 abar m512@0xf7200000 port 0xf7200180 irq 43\r\n[    1.157122] e1000e 0000:00:19.0: Interrupt Throttling Rate (ints/sec) set to dynamic conservative mode\r\n[    1.192013] usb 1-1: new high-speed USB device number 2 using ehci-pci\r\n[    1.212032] usb 2-1: new high-speed USB device number 2 using ehci-pci\r\n[    1.266758] e1000e 0000:00:19.0 0000:00:19.0 (uninitialized): registered PHC clock\r\n[    1.328009] usb 7-2: new high-speed USB device number 2 using xhci_hcd\r\n[    1.348352] usb 1-1: New USB device found, idVendor=8087, idProduct=0024\r\n[    1.348356] usb 1-1: New USB device strings: Mfr=0, Product=0, SerialNumber=0\r\n[    1.348606] hub 1-1:1.0: USB hub found\r\n[    1.348712] hub 1-1:1.0: 6 ports detected\r\n[    1.368720] usb 2-1: New USB device found, idVendor=8087, idProduct=0024\r\n[    1.368723] usb 2-1: New USB device strings: Mfr=0, Product=0, SerialNumber=0\r\n[    1.368981] hub 2-1:1.0: USB hub found\r\n[    1.369080] hub 2-1:1.0: 8 ports detected\r\n[    1.369425] e1000e 0000:00:19.0 eth0: (PCI Express:2.5GT/s:Width x1) 60:a4:4c:b0:cf:9d\r\n[    1.369426] e1000e 0000:00:19.0 eth0: Intel(R) PRO/1000 Network Connection\r\n[    1.369461] e1000e 0000:00:19.0 eth0: MAC: 10, PHY: 11, PBA No: FFFFFF-0FF\r\n[    1.370205] e1000e 0000:00:19.0 eno1: renamed from eth0\r\n[    1.470484] ata7: SATA link down (SStatus 0 SControl 300)\r\n[    1.470487] ata3: SATA link up 3.0 Gbps (SStatus 123 SControl 300)\r\n[    1.470624] ata6: SATA link up 1.5 Gbps (SStatus 113 SControl 300)\r\n[    1.470762] ACPI Error: [DSSP] Namespace lookup failure, AE_NOT_FOUND (20170831/psargs-364)\r\n[    1.470823] No Local Variables are initialized for Method [_GTF]\r\n[    1.470824] No Arguments are initialized for method [_GTF]\r\n[    1.470825] ACPI Error: Method parse/execution failed \\_SB.PCI0.SAT0.SPT2._GTF, AE_NOT_FOUND (20170831/psparse-550)\r\n[    1.471298] ata3.00: NCQ Send/Recv Log not supported\r\n[    1.471301] ata3.00: ATA-9: Samsung SSD 840 EVO 250GB, EXT0BB0Q, max UDMA/133\r\n[    1.471303] ata3.00: 488397168 sectors, multi 16: LBA48 NCQ (depth 31/32), AA\r\n[    1.471531] ACPI Error: [DSSP] Namespace lookup failure, AE_NOT_FOUND (20170831/psargs-364)\r\n[    1.471606] No Local Variables are initialized for Method [_GTF]\r\n[    1.471607] No Arguments are initialized for method [_GTF]\r\n[    1.471608] ACPI Error: Method parse/execution failed \\_SB.PCI0.SAT0.SPT2._GTF, AE_NOT_FOUND (20170831/psparse-550)\r\n[    1.472138] ata3.00: NCQ Send/Recv Log not supported\r\n[    1.472142] ata3.00: configured for UDMA/133\r\n[    1.472481] ACPI Error: \r\n[    1.472482] scsi 2:0:0:0: Direct-Access     ATA      Samsung SSD 840  BB0Q PQ: 0 ANSI: 5\r\n[    1.472482] [DSSP] Namespace lookup failure, AE_NOT_FOUND (20170831/psargs-364)\r\n[    1.472487] No Local Variables are initialized for Method [_GTF]\r\n[    1.472487] No Arguments are initialized for method [_GTF]\r\n[    1.472489] ACPI Error: Method parse/execution failed \\_SB.PCI0.SAT0.SPT5._GTF, AE_NOT_FOUND (20170831/psparse-550)\r\n[    1.472495] ata6.00: ATAPI: ATAPI   iHAS124   W, HL0F, max UDMA/100\r\n[    1.472901] sd 2:0:0:0: Attached scsi generic sg0 type 0\r\n[    1.473058] sd 2:0:0:0: [sda] 488397168 512-byte logical blocks: (250 GB/233 GiB)\r\n[    1.473098] sd 2:0:0:0: [sda] Write Protect is off\r\n[    1.473099] sd 2:0:0:0: [sda] Mode Sense: 00 3a 00 00\r\n[    1.473186] sd 2:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\r\n[    1.474375] ACPI Error: [DSSP] Namespace lookup failure, AE_NOT_FOUND (20170831/psargs-364)\r\n[    1.474434] No Local Variables are initialized for Method [_GTF]\r\n[    1.474435] No Arguments are initialized for method [_GTF]\r\n[    1.474436] ACPI Error: Method parse/execution failed \\_SB.PCI0.SAT0.SPT5._GTF, AE_NOT_FOUND (20170831/psparse-550)\r\n[    1.474498] ata6.00: configured for UDMA/100\r\n[    1.475460] scsi 5:0:0:0: CD-ROM            ATAPI    iHAS124   W      HL0F PQ: 0 ANSI: 5\r\n[    1.479396]  sda: sda1 sda2 < sda5 >\r\n[    1.479745] sd 2:0:0:0: [sda] Attached SCSI disk\r\n[    1.550593] sr 5:0:0:0: [sr0] scsi3-mmc drive: 188x/125x writer dvd-ram cd/rw xa/form2 cdda tray\r\n[    1.550595] cdrom: Uniform CD-ROM driver Revision: 3.20\r\n[    1.550726] sr 5:0:0:0: Attached scsi CD-ROM sr0\r\n[    1.550799] sr 5:0:0:0: Attached scsi generic sg1 type 5\r\n[    1.589944] usb 7-2: New USB device found, idVendor=0b95, idProduct=772b\r\n[    1.589945] usb 7-2: New USB device strings: Mfr=1, Product=2, SerialNumber=3\r\n[    1.589946] usb 7-2: SerialNumber: 0008B1\r\n[    1.640026] usb 1-1.5: new low-speed USB device number 3 using ehci-pci\r\n[    1.752215] usb 1-1.5: New USB device found, idVendor=03f0, idProduct=094a\r\n[    1.752219] usb 1-1.5: New USB device strings: Mfr=1, Product=2, SerialNumber=0\r\n[    1.752221] usb 1-1.5: Product: HP USB Optical Mouse\r\n[    1.752223] usb 1-1.5: Manufacturer: PixArt\r\n[    1.755508] hidraw: raw HID events driver (C) Jiri Kosina\r\n[    1.758230] usbcore: registered new interface driver usbhid\r\n[    1.758231] usbhid: USB HID core driver\r\n[    1.759319] input: PixArt HP USB Optical Mouse as /devices/pci0000:00/0000:00:1a.0/usb1/1-1/1-1.5/1-1.5:1.0/0003:03F0:094A.0001/input/input2\r\n[    1.759383] hid-generic 0003:03F0:094A.0001: input,hidraw0: USB HID v1.11 Mouse [PixArt HP USB Optical Mouse] on usb-0000:00:1a.0-1.5/input0\r\n[    1.835992] usb 1-1.6: new low-speed USB device number 4 using ehci-pci\r\n[    1.852003] tsc: Refined TSC clocksource calibration: 3410.014 MHz\r\n[    1.852017] clocksource: tsc: mask: 0xffffffffffffffff max_cycles: 0x312742bc088, max_idle_ns: 440795245657 ns\r\n[    1.866313] ata8: SATA link down (SStatus 0 SControl 300)\r\n[    1.951687] usb 1-1.6: New USB device found, idVendor=03f0, idProduct=2f4a\r\n[    1.951689] usb 1-1.6: New USB device strings: Mfr=1, Product=2, SerialNumber=0\r\n[    1.951690] usb 1-1.6: Product: HP Business Slim Keyboard\r\n[    1.951691] usb 1-1.6: Manufacturer: Chicony\r\n[    1.955091] input: Chicony HP Business Slim Keyboard as /devices/pci0000:00/0000:00:1a.0/usb1/1-1/1-1.6/1-1.6:1.0/0003:03F0:2F4A.0002/input/input3\r\n[    2.012204] hid-generic 0003:03F0:2F4A.0002: input,hidraw1: USB HID v1.10 Keyboard [Chicony HP Business Slim Keyboard] on usb-0000:00:1a.0-1.6/input0\r\n[    2.016321] input: Chicony HP Business Slim Keyboard as /devices/pci0000:00/0000:00:1a.0/usb1/1-1/1-1.6/1-1.6:1.1/0003:03F0:2F4A.0003/input/input4\r\n[    2.076252] hid-generic 0003:03F0:2F4A.0003: input,hiddev0,hidraw2: USB HID v1.10 Device [Chicony HP Business Slim Keyboard] on usb-0000:00:1a.0-1.6/input1\r\n[    2.117484] random: fast init done\r\n[    2.762238] EXT4-fs (sda1): mounted filesystem with ordered data mode. Opts: (null)\r\n[    2.876126] clocksource: Switched to clocksource tsc\r\n[    4.177573] systemd[1]: systemd 229 running in system mode. (+PAM +AUDIT +SELINUX +IMA +APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ -LZ4 +SECCOMP +BLKID +ELFUTILS +KMOD -IDN)\r\n[    4.196124] systemd[1]: Detected architecture x86-64.\r\n[    4.196271] systemd[1]: Set hostname to <titangpu>.\r\n[    4.466324] systemd[1]: Listening on /dev/initctl Compatibility Named Pipe.\r\n[    4.466368] systemd[1]: Reached target Remote File Systems (Pre).\r\n[    4.466401] systemd[1]: Listening on Journal Socket.\r\n[    4.466466] systemd[1]: Created slice User and Session Slice.\r\n[    4.466473] systemd[1]: Reached target Remote File Systems.\r\n[    4.466512] systemd[1]: Created slice System Slice.\r\n[    4.466801] systemd[1]: Starting Create list of required static device nodes for the current kernel...\r\n[    4.515782] lp: driver loaded but no devices found\r\n[    4.519535] ppdev: user-space parallel port driver\r\n[    5.802906] EXT4-fs (sda1): re-mounted. Opts: errors=remount-ro\r\n[    6.126005] systemd-journald[309]: Received request to flush runtime journal from PID 1\r\n[    6.372719] ACPI Warning: SystemIO range 0x0000000000000428-0x000000000000042F conflicts with OpRegion 0x0000000000000400-0x000000000000047F (\\PMIO) (20170831/utaddress-247)\r\n[    6.372728] ACPI: If an ACPI driver is available for this device, you should use it instead of the native driver\r\n[    6.372732] ACPI Warning: SystemIO range 0x0000000000000540-0x000000000000054F conflicts with OpRegion 0x0000000000000500-0x0000000000000563 (\\GPIO) (20170831/utaddress-247)\r\n[    6.372738] ACPI Warning: SystemIO range 0x0000000000000540-0x000000000000054F conflicts with OpRegion 0x0000000000000500-0x000000000000057F (\\_SB.PCI0.LPCB.GPBX) (20170831/utaddress-247)\r\n[    6.372743] ACPI: If an ACPI driver is available for this device, you should use it instead of the native driver\r\n[    6.372744] ACPI Warning: SystemIO range 0x0000000000000530-0x000000000000053F conflicts with OpRegion 0x0000000000000500-0x0000000000000563 (\\GPIO) (20170831/utaddress-247)\r\n[    6.372748] ACPI Warning: SystemIO range 0x0000000000000530-0x000000000000053F conflicts with OpRegion 0x0000000000000500-0x000000000000057F (\\_SB.PCI0.LPCB.GPBX) (20170831/utaddress-247)\r\n[    6.372752] ACPI: If an ACPI driver is available for this device, you should use it instead of the native driver\r\n[    6.372753] ACPI Warning: SystemIO range 0x0000000000000500-0x000000000000052F conflicts with OpRegion 0x0000000000000500-0x0000000000000563 (\\GPIO) (20170831/utaddress-247)\r\n[    6.372757] ACPI Warning: SystemIO range 0x0000000000000500-0x000000000000052F conflicts with OpRegion 0x0000000000000500-0x000000000000057F (\\_SB.PCI0.LPCB.GPBX) (20170831/utaddress-247)\r\n[    6.372760] ACPI: If an ACPI driver is available for this device, you should use it instead of the native driver\r\n[    6.372761] lpc_ich: Resource conflict(s) found affecting gpio_ich\r\n[    6.440502] shpchp: Standard Hot Plug PCI Controller Driver version: 0.4\r\n[    6.455448] audit: type=1400 audit(1542989938.938:2): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/sbin/cups-browsed\" pid=570 comm=\"apparmor_parser\"\r\n[    6.456077] audit: type=1400 audit(1542989938.942:3): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/sbin/dhclient\" pid=564 comm=\"apparmor_parser\"\r\n[    6.456081] audit: type=1400 audit(1542989938.942:4): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/lib/NetworkManager/nm-dhcp-client.action\" pid=564 comm=\"apparmor_parser\"\r\n[    6.456083] audit: type=1400 audit(1542989938.942:5): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/lib/NetworkManager/nm-dhcp-helper\" pid=564 comm=\"apparmor_parser\"\r\n[    6.456084] audit: type=1400 audit(1542989938.942:6): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/lib/connman/scripts/dhclient-script\" pid=564 comm=\"apparmor_parser\"\r\n[    6.456110] audit: type=1400 audit(1542989938.942:7): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/lib/lightdm/lightdm-guest-session\" pid=563 comm=\"apparmor_parser\"\r\n[    6.456112] audit: type=1400 audit(1542989938.942:8): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/lib/lightdm/lightdm-guest-session//chromium\" pid=563 comm=\"apparmor_parser\"\r\n[    6.456269] audit: type=1400 audit(1542989938.942:9): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/lib/cups/backend/cups-pdf\" pid=571 comm=\"apparmor_parser\"\r\n[    6.456272] audit: type=1400 audit(1542989938.942:10): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/sbin/cupsd\" pid=571 comm=\"apparmor_parser\"\r\n[    6.456274] audit: type=1400 audit(1542989938.942:11): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/sbin/cupsd//third_party\" pid=571 comm=\"apparmor_parser\"\r\n[    6.485568] RAPL PMU: API unit is 2^-32 Joules, 3 fixed counters, 163840 ms ovfl timer\r\n[    6.485569] RAPL PMU: hw unit of domain pp0-core 2^-16 Joules\r\n[    6.485570] RAPL PMU: hw unit of domain package 2^-16 Joules\r\n[    6.485571] RAPL PMU: hw unit of domain pp1-gpu 2^-16 Joules\r\n[    6.507759] nvidia-uvm: Loaded the UVM driver in 8 mode, major device number 239\r\n[    6.536824] cfg80211: Loading compiled-in X.509 certificates for regulatory database\r\n[    6.543712] cfg80211: Loaded X.509 cert 'sforshee: 00b28ddf47aef9cea7'\r\n[    6.554325] AVX version of gcm_enc/dec engaged.\r\n[    6.554326] AES CTR mode by8 optimization enabled\r\n[    6.603223] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2\r\n[    6.603225] cfg80211: failed to load regulatory.db\r\n[    6.649682] resource sanity check: requesting [mem 0x000e0000-0x000fffff], which spans more than PCI Bus 0000:00 [mem 0x000e0000-0x000e3fff window]\r\n[    6.649825] caller os_map_kernel_space.part.2+0x6d/0x80 [nvidia] mapping multiple BARs\r\n[    6.651112] rtl8192ce: Chip Version ID: B_CHIP_92C\r\n[    6.651808] snd_hda_intel 0000:01:00.1: Disabling MSI\r\n[    6.651811] snd_hda_intel 0000:01:00.1: Handle vga_switcheroo audio client\r\n[    6.664197] rtl8192ce: Using firmware rtlwifi/rtl8192cfw.bin\r\n[    6.682146] snd_hda_codec_realtek hdaudioC0D0: autoconfig for ALC892: line_outs=4 (0x14/0x15/0x16/0x17/0x0) type:line\r\n[    6.682149] snd_hda_codec_realtek hdaudioC0D0:    speaker_outs=0 (0x0/0x0/0x0/0x0/0x0)\r\n[    6.682151] snd_hda_codec_realtek hdaudioC0D0:    hp_outs=1 (0x1b/0x0/0x0/0x0/0x0)\r\n[    6.682152] snd_hda_codec_realtek hdaudioC0D0:    mono: mono_out=0x0\r\n[    6.682153] snd_hda_codec_realtek hdaudioC0D0:    dig-out=0x11/0x1e\r\n[    6.682154] snd_hda_codec_realtek hdaudioC0D0:    inputs:\r\n[    6.682156] snd_hda_codec_realtek hdaudioC0D0:      Front Mic=0x19\r\n[    6.682158] snd_hda_codec_realtek hdaudioC0D0:      Rear Mic=0x18\r\n[    6.682159] snd_hda_codec_realtek hdaudioC0D0:      Line=0x1a\r\n[    6.685491] ieee80211 phy0: Selected rate control algorithm 'rtl_rc'\r\n[    6.685721] rtlwifi: rtlwifi: wireless switch is on\r\n[    6.695604] input: HDA Intel PCH Front Mic as /devices/pci0000:00/0000:00:1b.0/sound/card0/input5\r\n[    6.695656] input: HDA Intel PCH Rear Mic as /devices/pci0000:00/0000:00:1b.0/sound/card0/input6\r\n[    6.695701] input: HDA Intel PCH Line as /devices/pci0000:00/0000:00:1b.0/sound/card0/input7\r\n[    6.695746] input: HDA Intel PCH Line Out Front as /devices/pci0000:00/0000:00:1b.0/sound/card0/input8\r\n[    6.695796] input: HDA Intel PCH Line Out Surround as /devices/pci0000:00/0000:00:1b.0/sound/card0/input9\r\n[    6.695844] input: HDA Intel PCH Line Out CLFE as /devices/pci0000:00/0000:00:1b.0/sound/card0/input10\r\n[    6.695889] input: HDA Intel PCH Line Out Side as /devices/pci0000:00/0000:00:1b.0/sound/card0/input11\r\n[    6.695935] input: HDA Intel PCH Front Headphone as /devices/pci0000:00/0000:00:1b.0/sound/card0/input12\r\n[    6.781141] asus_wmi: ASUS WMI generic driver loaded\r\n[    6.808836] asus_wmi: Initialization: 0x0\r\n[    6.808863] asus_wmi: BIOS WMI version: 0.9\r\n[    6.808911] asus_wmi: SFUN value: 0x0\r\n[    6.809339] input: Eee PC WMI hotkeys as /devices/platform/eeepc-wmi/input/input13\r\n[    6.809424] asus_wmi: Number of fans: 1\r\n[    6.822692] rtl8192ce 0000:03:00.0 wlp3s0: renamed from wlan0\r\n[    7.063792] resource sanity check: requesting [mem 0x000c0000-0x000fffff], which spans more than PCI Bus 0000:00 [mem 0x000d0000-0x000d3fff window]\r\n[    7.063922] caller os_map_kernel_space.part.2+0x6d/0x80 [nvidia] mapping multiple BARs\r\n[    7.105237] intel_rapl: Found RAPL domain package\r\n[    7.105238] intel_rapl: Found RAPL domain core\r\n[    7.105243] intel_rapl: RAPL package 0 domain package locked by BIOS\r\n[    7.704601] input: HDA NVidia HDMI/DP,pcm=3 as /devices/pci0000:00/0000:00:01.0/0000:01:00.1/sound/card1/input14\r\n[    7.704647] input: HDA NVidia HDMI/DP,pcm=7 as /devices/pci0000:00/0000:00:01.0/0000:01:00.1/sound/card1/input15\r\n[    7.704688] input: HDA NVidia HDMI/DP,pcm=8 as /devices/pci0000:00/0000:00:01.0/0000:01:00.1/sound/card1/input16\r\n[    7.704730] input: HDA NVidia HDMI/DP,pcm=9 as /devices/pci0000:00/0000:00:01.0/0000:01:00.1/sound/card1/input17\r\n[    8.271478] asix 7-2:1.0 eth0: register 'asix' at usb-0000:08:00.0-2, ASIX AX88772B USB 2.0 Ethernet, a0:ce:c8:03:67:ea\r\n[    8.271508] usbcore: registered new interface driver asix\r\n[    8.302516] asix 7-2:1.0 enxa0cec80367ea: renamed from eth0\r\n[    8.444855] Adding 33491964k swap on /dev/sda5.  Priority:-2 extents:1 across:33491964k SSFS\r\n[    9.394964] IPv6: ADDRCONF(NETDEV_UP): enxa0cec80367ea: link is not ready\r\n[    9.395144] IPv6: ADDRCONF(NETDEV_UP): enxa0cec80367ea: link is not ready\r\n[    9.397337] IPv6: ADDRCONF(NETDEV_UP): wlp3s0: link is not ready\r\n[    9.770136] IPv6: ADDRCONF(NETDEV_UP): wlp3s0: link is not ready\r\n[    9.774647] IPv6: ADDRCONF(NETDEV_UP): eno1: link is not ready\r\n[    9.961257] asix 7-2:1.0 enxa0cec80367ea: link up, 100Mbps, full-duplex, lpa 0xC1E1\r\n[   10.004124] IPv6: ADDRCONF(NETDEV_UP): eno1: link is not ready\r\n[   10.004154] IPv6: ADDRCONF(NETDEV_CHANGE): enxa0cec80367ea: link becomes ready\r\n[   10.475843] resource sanity check: requesting [mem 0x000c0000-0x000fffff], which spans more than PCI Bus 0000:00 [mem 0x000d0000-0x000d3fff window]\r\n[   10.475983] caller os_map_kernel_space.part.2+0x6d/0x80 [nvidia] mapping multiple BARs\r\n[   10.578104] IPv6: ADDRCONF(NETDEV_UP): wlp3s0: link is not ready\r\n[   10.718050] nvidia-modeset: Allocated GPU:0 (GPU-1873795c-080f-649e-e5bc-1e634e5bdcfd) @ PCI:0000:01:00.0\r\n[   18.746669] random: crng init done\r\n[   18.746680] random: 7 urandom warning(s) missed due to ratelimiting\r\n[   34.411989] asix 7-2:1.0 enxa0cec80367ea: link up, 100Mbps, full-duplex, lpa 0xC1E1\r\n[  186.495631] asix 7-2:1.0 enxa0cec80367ea: link up, 100Mbps, full-duplex, lpa 0xC1E1\r\n[  924.389193] kauditd_printk_skb: 21 callbacks suppressed\r\n[  924.389194] audit: type=1400 audit(1542990787.350:33): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/snap/core/5897/usr/lib/snapd/snap-confine\" pid=4897 comm=\"apparmor_parser\"\r\n[  924.389357] audit: type=1400 audit(1542990787.350:34): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/snap/core/5897/usr/lib/snapd/snap-confine//mount-namespace-capture-helper\" pid=4897 comm=\"apparmor_parser\"\r\n[  924.469471] audit: type=1400 audit(1542990787.430:35): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"snap.core.hook.configure\" pid=4900 comm=\"apparmor_parser\"\r\n[  924.472928] audit: type=1400 audit(1542990787.434:36): apparmor=\"STATUS\" operation=\"profile_replace\" info=\"same as current profile, skipping\" profile=\"unconfined\" name=\"snap-update-ns.core\" pid=4902 comm=\"apparmor_parser\"\r\n[  924.562907] audit: type=1400 audit(1542990787.522:37): apparmor=\"STATUS\" operation=\"profile_replace\" info=\"same as current profile, skipping\" profile=\"unconfined\" name=\"snap-update-ns.notepadqq\" pid=4909 comm=\"apparmor_parser\"\r\n[  924.570323] audit: type=1400 audit(1542990787.530:38): apparmor=\"STATUS\" operation=\"profile_replace\" info=\"same as current profile, skipping\" profile=\"unconfined\" name=\"snap.notepadqq.notepadqq\" pid=4911 comm=\"apparmor_parser\"\r\n[  926.499781] audit: type=1400 audit(1542990789.458:39): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"snap-update-ns.notepadqq\" pid=5087 comm=\"apparmor_parser\"\r\n[  926.743732] audit: type=1400 audit(1542990789.702:40): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"snap.notepadqq.notepadqq\" pid=5088 comm=\"apparmor_parser\"\r\n[  926.809797] audit: type=1400 audit(1542990789.770:41): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/snap/core/5897/usr/lib/snapd/snap-confine\" pid=5097 comm=\"apparmor_parser\"\r\n[  926.831925] audit: type=1400 audit(1542990789.794:42): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/snap/core/5897/usr/lib/snapd/snap-confine//mount-namespace-capture-helper\" pid=5097 comm=\"apparmor_parser\"\r\n[  950.699841] kauditd_printk_skb: 2 callbacks suppressed\r\n[  950.699842] audit: type=1400 audit(1542990813.662:45): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"snap.notepadqq.notepadqq\" pid=5315 comm=\"apparmor_parser\"\r\n[  950.703493] audit: type=1400 audit(1542990813.662:46): apparmor=\"STATUS\" operation=\"profile_replace\" info=\"same as current profile, skipping\" profile=\"unconfined\" name=\"snap-update-ns.notepadqq\" pid=5319 comm=\"apparmor_parser\"\r\n[  950.737435] audit: type=1400 audit(1542990813.698:47): apparmor=\"STATUS\" operation=\"profile_replace\" info=\"same as current profile, skipping\" profile=\"unconfined\" name=\"/snap/core/5897/usr/lib/snapd/snap-confine\" pid=5326 comm=\"apparmor_parser\"\r\n[  950.737438] audit: type=1400 audit(1542990813.698:48): apparmor=\"STATUS\" operation=\"profile_replace\" info=\"same as current profile, skipping\" profile=\"unconfined\" name=\"/snap/core/5897/usr/lib/snapd/snap-confine//mount-namespace-capture-helper\" pid=5326 comm=\"apparmor_parser\"\r\n[  950.741915] audit: type=1400 audit(1542990813.702:49): apparmor=\"STATUS\" operation=\"profile_replace\" info=\"same as current profile, skipping\" profile=\"unconfined\" name=\"snap-update-ns.core\" pid=5328 comm=\"apparmor_parser\"\r\n[  950.743109] audit: type=1400 audit(1542990813.702:50): apparmor=\"STATUS\" operation=\"profile_replace\" info=\"same as current profile, skipping\" profile=\"unconfined\" name=\"snap.core.hook.configure\" pid=5329 comm=\"apparmor_parser\"\r\n[70435.682480] systemd: 35 output lines suppressed due to ratelimiting\r\n[70512.984492] audit: type=1400 audit(1543060376.541:51): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/usr/sbin/cups-browsed\" pid=21049 comm=\"apparmor_parser\"\r\n[70513.031541] audit: type=1400 audit(1543060376.589:52): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/usr/sbin/ippusbxd\" pid=21053 comm=\"apparmor_parser\"\r\n[70513.099735] audit: type=1400 audit(1543060376.657:53): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/sbin/dhclient\" pid=21040 comm=\"apparmor_parser\"\r\n[70513.100225] audit: type=1400 audit(1543060376.657:54): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/usr/lib/NetworkManager/nm-dhcp-client.action\" pid=21040 comm=\"apparmor_parser\"\r\n[70513.100631] audit: type=1400 audit(1543060376.657:55): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/usr/lib/NetworkManager/nm-dhcp-helper\" pid=21040 comm=\"apparmor_parser\"\r\n[70513.101024] audit: type=1400 audit(1543060376.657:56): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/usr/lib/connman/scripts/dhclient-script\" pid=21040 comm=\"apparmor_parser\"\r\n[70513.117209] audit: type=1400 audit(1543060376.673:57): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/usr/lib/snapd/snap-confine\" pid=21047 comm=\"apparmor_parser\"\r\n[70513.166150] audit: type=1400 audit(1543060376.725:58): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/usr/sbin/tcpdump\" pid=21057 comm=\"apparmor_parser\"\r\n[70513.166161] audit: type=1400 audit(1543060376.725:59): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/usr/lib/lightdm/lightdm-guest-session\" pid=21051 comm=\"apparmor_parser\"\r\n[70513.198117] audit: type=1400 audit(1543060376.757:60): apparmor=\"STATUS\" operation=\"profile_replace\" profile=\"unconfined\" name=\"/snap/core/5145/usr/lib/snapd/snap-confine\" pid=21041 comm=\"apparmor_parser\"\r\n[70785.967518] SGI XFS with ACLs, security attributes, realtime, no debug enabled\r\n[70785.972565] JFS: nTxBlock = 8192, nTxLock = 65536\r\n[70785.981269] ntfs: driver 2.1.32 [Flags: R/O MODULE].\r\n[70785.994032] QNX4 filesystem 0.2.3 registered.\r\n[70786.047711] raid6: sse2x1   gen()  9897 MB/s\r\n[70786.095708] raid6: sse2x1   xor()  8232 MB/s\r\n[70786.143709] raid6: sse2x2   gen() 12896 MB/s\r\n[70786.191712] raid6: sse2x2   xor()  9034 MB/s\r\n[70786.239709] raid6: sse2x4   gen() 15045 MB/s\r\n[70786.287710] raid6: sse2x4   xor() 11131 MB/s\r\n[70786.287711] raid6: using algorithm sse2x4 gen() 15045 MB/s\r\n[70786.287712] raid6: .... xor() 11131 MB/s, rmw enabled\r\n[70786.287712] raid6: using ssse3x2 recovery algorithm\r\n[70786.288746] xor: automatically using best checksumming function   avx       \r\n[70786.304526] Btrfs loaded, crc32c=crc32c-intel", "Is this still an issue? Did you get a chance to re-install your nvidia driver?", "It looks like this is still an issue, I let some code run overnight and I was seeing the same behavior.\r\n\r\nBefore running, I re-installed the nvidia driver by using the following command:\r\n\r\nsudo apt-get install nvidia-384", "Apologies for the delay in response. Can you please build latest TF 1.13.0rc0. Note that TF 1.13.0.rc0 comes with prebuilt cuda 10 binaries. So make sure you update the cuda drivers too."]}, {"number": 23940, "title": "tflite - Tensorflow lite : Output tensor dimension are not changeable - Adding/removing labels(classes) gives a  crash like Cannot copy between a TensorFlowLite tensor with shape [1, 1001] and a Java object with shape [1, 1000].", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["\r\nDemo is available by Tensorflow in following link : \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo\r\n\r\nPlease change labels.txt file in above file by adding or removing a text(class) , It'll crash the application.\r\nIt's making the given solution unable to change output's size.\r\nPlease address the solution, how can we change the label.txt file with some constants like INPUT_SIZE or OUTPUT_SIZE that makes this bug solved.\r\n\r\n\r\n**Note**\r\n>Application works fine as it is, but crash upon changing label.txt file, e.g. adding or removing names in the file.\r\n\r\n**Crash :** \r\n - _Cannot copy between a TensorFlowLite tensor with shape [1, 1001] and a Java object with shape [1, 1000]._ \r\n\r\n\r\n**Above demo is build by using already present gradle configuration setting.\r\n\r\n**Feature request:** \r\nChanging label file size & work accordingly through parameter is available in IOS demo with this \r\n output_size = 1000;\r\nThis feature should be in Android version as well.\r\n\r\n\r\nThanks.", "@jdduke could you reassign this to the right person?", "Tracking the solution, but not found any yet, please let me know how to address it ! Thanks", "I think it's not possible to do it, but if it's done on IOS side it should be available in Android as well.\r\n", "The labels length needs to match the output tensor length for your trained model. You cannot simply resize your labels file and expect it to work with the same source model; the two are coupled. Of course, you can hardcode an override in your own code if you wish, but that's not generally desirable. ", "\r\n> The labels length needs to match the output tensor length for your trained model. You cannot simply resize your labels file and expect it to work with the same source model; the two are coupled. Of course, you can hardcode an override in your own code if you wish, but that's not generally desirable.\r\n\r\n@jdduke I h'v resolved the problem myself:\r\n\r\n**Problem :** \r\n\r\nCannot copy between a TensorFlowLite tensor with shape ... \r\n\r\n**Solution** : \r\n\r\n Changes to TFLiteImageClassifier file in Tensoflow/lite/examples/android are following:\r\n\r\nIt's great to resize the input tensors, as this functionality is provided by Tensorflow-lite but\r\nIt should be used carefully, Like there would be a lot of problems coming on the way, but you got to look into buffers(imgData) and\r\n\r\nChanging labels types to following line.\r\n\r\nprivate List<String> labels  = new ArrayList<String>();\r\n\r\nBiggest change in TFLiteImageClassifier is: \r\n\r\n  c.tfLite = new Interpreter(c.loadModelFile(assetManager, modelFilename));\r\n\r\n      int[] dimensions = new int[4];\r\n      dimensions[0] = 1; // Batch_size // No of frames at a time\r\n      dimensions[1] = 224; // Image Width required by model\r\n      dimensions[2] = 224; // Image Height required by model\r\n      dimensions[3] = 3; // No of Pixels\r\n      Tensor tensor = c.tfLite.getInputTensor(0);\r\n      c.tfLite.resizeInput(0, dimensions);\r\n      Tensor tensor1 = c.tfLite.getInputTensor(0);\r\n\r\nPlacing above code  in TFLiteImageClassifier would resize your input tensors, so your model can take images of any size, pixels or batches.\r\n\r\nSo even if your model is accepting 150*150 images, or h'v other different parameters, you can make your model accept this size of images, to improve your model accuracy.\r\n\r\n**Request :**\r\nSo far, I h'v came to know that output tensors are also resize-able in IOS demo version, but In case of Android demo version it's not mentioned anywhere in given demo in Tensorflow lite Android version, Hopefully I would find a way to do it as well soon.\r\n\r\n**Suggestion :** \r\nTensorflow lite Android version resizing input tensors link were not given anywhere properly or in demo code, took me a little longer to resolve this issue, Please update Read me & code as well.\r\n\r\n\r\nThanks All.", "Thanks for the report, @shahid14. We do have an example which resizes the input tensor, but it's not at all prominent. We'll also be publishing proper API documentation in the coming month, so look out for that as well. Cheers!", "> So far, I h'v came to know that output tensors a\r\n\r\nHie shahid14 ,\r\n\r\nYour suggestion looks good.\r\nI have trained my custom object detector using tensorflow 1.8.0 and 1.13.1.\r\nI converted the model to tflite using tflite_convert and then trying to deploy in the android project. It is throwing the error \"java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.\"\r\nCan you please suggest me something doing which I will be able to build my detector in android.\r\n\r\n\r\nThank you\r\nRashmi", "Hi Rashmi, \r\nThe issue you are facing can be solved by playing with nuts & bolts in your code.\r\nYou can also get help from the following issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/14719\r\n\r\nava.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.\"", "I'm not sure if I have a similar or related problem. I am interested in modifying the batch size and I can resize the input tensors, but when I call to allocate the tensors I get an error message \r\n\r\n`tensorflow/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (6 != 30)Node number 228`\r\n\r\nI am unable to find which operation (built-in or custom) has the batch size as constant", "same problem here !!!!", "@hamlatzis can you file a separate bug for that issue, ideally with a link to your  converted model (and/or the source model, including how you converted it)? Thanks.", "@jdduke regarding https://github.com/tensorflow/tensorflow/issues/23940#issuecomment-541168279 I've asked my manager regarding your request and I'm waiting for his reply. \r\n\r\nIn the mean time I see there is a similar open issue https://github.com/tensorflow/tensorflow/issues/22377\r\n\r\nAlso I've found at stackoveflow (https://stackoverflow.com/questions/50735705/tensorflow-litetflite-invoke-error-after-resize-the-input-dimension) ", "I solved the same problem by doing as this https://github.com/tensorflow/tensorflow/issues/22106#issuecomment-428409506"]}, {"number": 23939, "title": " fatal error: cuda/include/cublas_v2.h: No such file or directory", "body": "Following instructions building tensorflow from source by using bazel, I got error when \r\n \r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  --define framework_shared_object=false tensorflow:libtensorflow_cc.so --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n\r\n\r\n**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed from source\r\n- TensorFlow version: r1.8\r\n- Python version: 3.5\r\n- Installed using virtualenv :\r\n- Bazel version (if compiling from source): 0.18.1\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- CUDA/cuDNN version: 9.0/7.0\r\n- GPU model and memory: TITAN XP \r\n\r\n\r\nmy configuration file:\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/kx/segmappyenv/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/kx/segmappyenv/lib/python3.5/site-packages\"\r\nbuild --force_python=py3\r\nbuild --host_force_python=py3\r\nbuild --python_path=\"/home/kx/segmappyenv/bin/python\"\r\nbuild --define with_jemalloc=true\r\nbuild --define with_gcp_support=true\r\nbuild --define with_hdfs_support=true\r\nbuild --define with_s3_support=true\r\nbuild --define with_kafka_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=true\r\nbuild:verbs --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-9.0\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/local/cuda-9.0\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_NCCL_VERSION=\"1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/opt/ros/kinetic/lib:/opt/ros/kinetic/lib/x86_64-linux-gnu:/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\n\r\n**Any other info / logs**\r\nbuild logs:\r\n\r\n\r\nINFO: From Compiling tensorflow/core/distributed_runtime/master.cc [for host]:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/framework/variant.h:29,\r\n                 from ./tensorflow/core/framework/allocator.h:26,\r\n                 from ./tensorflow/core/common_runtime/device.h:35,\r\n                 from ./tensorflow/core/distributed_runtime/master.h:21,\r\n                 from tensorflow/core/distributed_runtime/master.cc:32:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:372:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:340:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:82:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:340:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:381:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:355:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:82:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:355:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nERROR: /home/kx/segmappyenv/tensorflow/tensorflow/core/kernels/BUILD:2547:1: C++ compilation of rule '//tensorflow/core/kernels:self_adjoint_eig_v2_op' failed (Exit 1)\r\nIn file included from tensorflow/core/kernels/self_adjoint_eig_v2_op_gpu.cc:29:0:\r\n./tensorflow/core/kernels/cuda_solvers.h:26:36: fatal error: cuda/include/cublas_v2.h: No such file or directory\r\ncompilation terminated.\r\nINFO: Elapsed time: 383.691s, Critical Path: 52.31s\r\nINFO: 2759 processes: 2759 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Could you try `bazel clean --expunge` and build again?", "It seems like the same error? @byronyi \r\n\r\n\r\nINFO: From Compiling tensorflow/core/distributed_runtime/master.cc [for host]:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/framework/variant.h:29,\r\n                 from ./tensorflow/core/framework/allocator.h:26,\r\n                 from ./tensorflow/core/common_runtime/device.h:35,\r\n                 from ./tensorflow/core/distributed_runtime/master.h:21,\r\n                 from tensorflow/core/distributed_runtime/master.cc:32:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:372:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:340:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:82:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:340:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:381:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:355:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:82:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:355:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nINFO: From Compiling tensorflow/core/kernels/data/skip_dataset_op.cc [for host]:\r\ntensorflow/core/kernels/data/skip_dataset_op.cc: In member function 'virtual void tensorflow::{anonymous}::SkipDatasetOp::MakeDataset(tensorflow::OpKernelContext*, tensorflow::DatasetBase*, tensorflow::DatasetBase**)':\r\ntensorflow/core/kernels/data/skip_dataset_op.cc:44:61: warning: 'count' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n         : GraphDatasetBase(ctx), count_(count), input_(input) {\r\n                                                             ^\r\ntensorflow/core/kernels/data/skip_dataset_op.cc:34:11: note: 'count' was declared here\r\n     int64 count;\r\n           ^\r\nERROR: /home/kx/segmappyenv/tensorflow/tensorflow/core/kernels/BUILD:915:1: C++ compilation of rule '//tensorflow/core/kernels:where_op' failed (Exit 1)\r\nIn file included from tensorflow/core/kernels/where_op.cc:42:0:\r\n./tensorflow/core/kernels/cuda_solvers.h:26:36: fatal error: cuda/include/cublas_v2.h: No such file or directory\r\ncompilation terminated.\r\nINFO: Elapsed time: 613.655s, Critical Path: 38.70s\r\nINFO: 5331 processes: 5331 local.\r\nFAILED: Build did NOT complete successfully\r\n", "Could you try reinstall your CUDA and bazel clean \u2014expunge and build again?", "@byronyi  After reinstall CUDA9.0, cudnn7.4 and bazel clean --expunge and build again. \r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package  --define framework_shared_object=false tensorflow:libtensorflow_cc.so --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n\r\n\r\nI got another error:\r\n\r\nINFO: From Compiling tensorflow/contrib/lite/toco/dump_graphviz.cc:\r\nIn file included from ./tensorflow/contrib/lite/kernels/internal/common.h:42:0,\r\n                 from ./tensorflow/contrib/lite/toco/runtime/types.h:18,\r\n                 from ./tensorflow/contrib/lite/toco/model.h:26,\r\n                 from ./tensorflow/contrib/lite/toco/dump_graphviz.h:20,\r\n                 from tensorflow/contrib/lite/toco/dump_graphviz.cc:15:\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'void vst1q_lane_f32(float32_t*, float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:9673:31: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     *(ptr) =  *((float*)&ilane);\r\n                               ^\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h: In function 'float32_t vgetq_lane_f32(float32x4_t, int)':\r\nexternal/arm_neon_2_x86_sse/NEON_2_SSE.h:11912:22: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]\r\n     return *(float*)&ilane;\r\n                      ^\r\ntensorflow/contrib/lite/toco/dump_graphviz.cc: In function 'void toco::{anonymous}::AppendArrayVal(std::string*, const toco::Array&, int)':\r\ntensorflow/contrib/lite/toco/dump_graphviz.cc:112:15: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (index >= data.size()) {\r\n               ^\r\ntensorflow/contrib/lite/toco/dump_graphviz.cc:118:15: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (index >= data.size()) {\r\n               ^\r\ntensorflow/contrib/lite/toco/dump_graphviz.cc:124:15: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (index >= data.size()) {\r\n               ^\r\ntensorflow/contrib/lite/toco/dump_graphviz.cc:130:15: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (index >= data.size()) {\r\n               ^\r\ntensorflow/contrib/lite/toco/dump_graphviz.cc: In function 'void toco::DumpGraphviz(const toco::Model&, std::string*)':\r\ntensorflow/contrib/lite/toco/dump_graphviz.cc:323:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int op_index = 0; op_index < ops_to_dump.size(); op_index++) {\r\n                                   ^\r\nERROR: /home/kx/segmappyenv/tensorflow/tensorflow/python/BUILD:3315:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)\r\ngcc: error: pywrap_tensorflow_internal_versionscript.lds: No such file or directory\r\nINFO: Elapsed time: 1911.083s, Critical Path: 183.69s\r\nINFO: 4469 processes: 4469 local.\r\nFAILED: Build did NOT complete successfully\r\n", "Exact Same issue when building a Docker Image:\r\n\r\nNV Driver 410.72\r\nCUDA 10.0\r\nCuDNN 7.4.1\r\nNCCL 2.3.7 \r\n\r\n```Dockerfile\r\nENV CI_BUILD_PYTHON=python \\\r\n    BAZEL_VERSION='0.15.0' \\\r\n    LD_LIBRARY_PATH='/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH' \\\r\n    BLAS_INCLUDE='/usr/local/cuda/targets/x86_64-linux/include' \\\r\n    BLAS_LIB='/usr/local/cuda/targets/x86_64-linux/lib' \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_TENSORRT=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=5.2,6.0,6.1,7.0,7.5 \\\r\n    TF_CUDA_VERSION=10.0 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NCCL_VERSION=2 \\\r\n    TF_BUILD_BRANCH='r1.12'\r\n\r\nRUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 && \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:${LD_LIBRARY_PATH}\r\n\r\n# Set up Bazel.\r\n\r\n# Running bazel inside a `docker build` command causes trouble, cf:\r\n#   https://github.com/bazelbuild/bazel/issues/134\r\n# The easiest solution is to set up a bazelrc file forcing --batch.\r\n\r\n# Similarly, we need to work around sandboxing issues:\r\n#   https://github.com/bazelbuild/bazel/issues/418\r\nRUN rm -f /etc/bazel.bazelrc && \\\r\n    echo \"startup --batch\" >> /etc/bazel.bazelrc && \\\r\n    echo \"build --spawn_strategy=standalone --genrule_strategy=standalone\" \\\r\n        >> /etc/bazel.bazelrc && \\\r\n    rm -rf /bazel &&  mkdir /bazel && cd /bazel && \\\r\n    curl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" \\\r\n        -fSsL -O https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\r\n    curl -H \"User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\" \\\r\n        -fSsL -o /bazel/LICENSE.txt https://raw.githubusercontent.com/bazelbuild/bazel/master/LICENSE && \\\r\n    chmod +x bazel-*.sh && \\\r\n    ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \\\r\n    cd / && \\\r\n    rm -f /bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh\r\n\r\n# Get the TF branch for later build\r\nRUN rm -rf /tensorflow && \\\r\n    git clone --branch=$TF_BUILD_BRANCH --depth=1 https://github.com/tensorflow/tensorflow.git /tensorflow && \\\r\n    cd /tensorflow && \\\r\n    tensorflow/tools/ci_build/builds/configured GPU \\\r\n    bazel build -c opt --copt=-mavx --config=cuda \\\r\n\t    --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\r\n        tensorflow/tools/pip_package:build_pip_package && \\\r\n    rm /usr/local/cuda/lib64/stubs/libcuda.so.1 && \\\r\n    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \\\r\n    pip --no-cache-dir install --upgrade /tmp/pip/tensorflow-*.whl && \\\r\n    rm -rf /tmp/pip && \\\r\n    rm -rf /root/.cache\r\n```\r\n\r\nError:\r\n\r\n```cpp\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:452:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:461:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nERROR: /tensorflow/tensorflow/stream_executor/BUILD:64:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed (Exit 1)\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:16:36: fatal error: cuda/include/cublas_v2.h: No such file or directory\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2463.463s, Critical Path: 111.56s\r\nINFO: 7366 processes: 7366 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "I fixed error `fatal error: cuda/include/cublas_v2.h` by reinstalling CUDA. Finally, I compiled TF1.9 successfully and everything works fine now. I failed executing `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...` with TF1.8 but succeed with TF1.9. So make sure you can [bazel test](https://www.tensorflow.org/install/source) successfully. Maybe you can try another version. My problem is fixed so I will close this issue."]}, {"number": 23938, "title": "docker cannot open web", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nsudo nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:1.12.0-devel-gpu-py3\r\nroot@82bdd6461185:~# exit\r\n\r\nthe chrome is not open jupyter notebook \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please provide the following information:\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\nAlso can you confirm that you have successfully installed and verified your [nvidia-docker setup](https://www.tensorflow.org/install/docker#gpu_support)?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23937, "title": "TF-lite not supported Abs, ResizeNearestNeighbor\uff1f\uff1f", "body": "super@super-virtual-machine:~/Downloads/Onet-TF_to_caffe$ tflite_convert \\\r\n>   --output_file=/home/super/Downloads/Onet-TF_to_caffe/align.tflite \\\r\n>   --graph_def_file=/home/super/Downloads/Onet-TF_to_caffe/frozen_model.pb \\\r\n>   --input_arrays=alignmentnet/inputs \\\r\n>   --output_arrays=alignmentnet/add\r\n2018-11-23 18:02:52.499590: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/home/super/.local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 453, in convert\r\n    **converter_kwargs)\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 342, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2018-11-23 18:02:54.114501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.114680: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.114763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.114819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.114875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.114933: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.115020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.115154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.115213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ResizeNearestNeighbor\r\n2018-11-23 18:02:54.115265: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.115308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ResizeNearestNeighbor\r\n2018-11-23 18:02:54.115341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.115377: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.123507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2018-11-23 18:02:54.125172: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 162 operators, 227 arrays (0 quantized)\r\n2018-11-23 18:02:54.126147: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 162 operators, 227 arrays (0 quantized)\r\n2018-11-23 18:02:54.128320: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 101 operators, 163 arrays (0 quantized)\r\n2018-11-23 18:02:54.149382: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 100 operators, 162 arrays (0 quantized)\r\n2018-11-23 18:02:54.150311: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 99 operators, 160 arrays (0 quantized)\r\n2018-11-23 18:02:54.151169: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 99 operators, 160 arrays (0 quantized)\r\n2018-11-23 18:02:54.152134: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 2032128 bytes, theoretical optimal value: 2032128 bytes.\r\n2018-11-23 18:02:54.152627: F tensorflow/contrib/lite/toco/tflite/export.cc:386] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.TFLiteConverter(). Here is a list of operators for which  you will need custom implementations: Abs, ResizeNearestNeighbor.\r\nAborted (core dumped)\r\n", "comments": ["ResizeNearesetNeighbor neighbor has been added (try the nightly or source). Abs is  currently being worked on internally and should hopefully land in a few days.\r\n\r\nIn the meantime you can try TensorFlow Lite with Select TensorFlow ops\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md\r\n\r\n", "@aselle, Thank you for your reply, Now, I can build .tflite file using select_tf_ops", "> @aselle, Thank you for your reply, Now, I can build .tflite file using select_tf_ops\r\n\r\nhi, I met the same problem, how do you solve it?", "@aselle What do you mean by 'try the nightly or source'? Am getting an error for ResizeNearesetNeighbor. I am using Tensorflow 1.12"]}, {"number": 23936, "title": "Updating ppc64le CPU build status link", "body": "We have changed the CPU build job name from `TensorFlow_Ubuntu_16.04_CPU` to `TensorFlow_PPC64LE_CPU_Build` , hence updating the build status link for the same.\r\n\r\nThanks!\r\nSandip", "comments": ["@drpngx  Can you please review this ?"]}, {"number": 23935, "title": "Can only utilize GPU after being run as root", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10 Buster Fresh Install (Testing)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6/2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 9.0.176 / cuDNN 9.0\r\n- GPU model and memory: GeForce GTX 1060 6GB\r\n\r\n**What is happening**\r\nWhen the system is first booted Tensorflow is not able to recognize the GPU instead returning `CUDA_ERROR_UNKNOWN: unknown error` but after running a version of Tensorflow as root all other instances start working.\r\n\r\n**Describe the expected behavior**\r\nIn the example below the first instance of Tensorflow in the virtualenv should have been able to see the GPU. \r\n\r\n**Code to reproduce the issue**\r\nBelow is edited/commented version of an experiment I ran showcasing this bug. I have removed most of the verbose logging for brevity but the original can be found [here](https://pastebin.com/B7vHcyjn).\r\n```bash\r\n# Activate Virtualenv with Tensorflow 1.12.0\r\nkyle@Debian:~/Code/ML$ source env/bin/activate\r\n# Export the paths to CUDA\r\n(env) kyle@Debian:~/Code/ML$ export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"\r\n(env) kyle@Debian:~/Code/ML$ export CUDA_HOME=/usr/local/cuda\r\n# Launch Python Interpreter\r\n(env) kyle@Debian:~/Code/ML$ python\r\nPython 3.6.7 (default, Oct 21 2018, 08:08:16) \r\n# Import the TensorFlow installed in Virtualenv\r\n>>> import tensorflow as tf\r\n# Test for GPUs\r\n>>> tf.test.is_gpu_available()\r\n# None are found\r\nFalse\r\n# Let's exit the Python interpreter\r\n>>> exit()\r\n# and deactivate the Virtualenv\r\n(env) kyle@Debian:~/Code/ML$ deactivate\r\n\r\n# Login as superuser\r\nkyle@Debian:~/Code/ML$ sudo su\r\n[sudo] password for kyle: \r\n# Export paths to CUDA (again)\r\nroot@Debian:/home/kyle/Code/ML# export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"\r\nroot@Debian:/home/kyle/Code/ML# export CUDA_HOME=/usr/local/cuda\r\n# Launch Global Python Interpreter (as root)\r\nroot@Debian:/home/kyle/Code/ML# python\r\nPython 2.7.15+ (default, Aug 31 2018, 11:56:52) \r\n# Import a global install of TensorFlow 1.12.0\r\n>>> import tensorflow as tf\r\n# Test for GPUs\r\n>>> tf.test.is_gpu_available()\r\n# It found one! :D\r\nTrue\r\n# Exit Python interpreter\r\n>>> exit()\r\n# Exit root\r\nroot@Debian:/home/kyle/Code/ML# exit\r\n\r\n# We are now logged in as user 'kyle'\r\n# Activate the same virtualenv as before\r\nkyle@Debian:~/Code/ML$ source env/bin/activate\r\n# Enter the Virtualenv's Python interpreter\r\n(env) kyle@Debian:~/Code/ML$ python\r\nPython 3.6.7 (default, Oct 21 2018, 08:08:16) \r\n# Import Tensorflow from within Virtualenv\r\n>>> import tensorflow as tf\r\n# Test for GPUs\r\n>>> tf.test.is_gpu_available()\r\n# Now it found one? wtf.\r\nTrue\r\n```\r\n\r\n**Other info / logs**\r\nThe package `nvidia-modprobe` is already installed. \r\nNvidia Driver: 390.87\r\n```bash\r\nkyle@Debian:~$ nvidia-smi\r\nFri Nov 23 02:46:11 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.87                 Driver Version: 390.87                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 106...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  3%   49C    P8     9W / 120W |    275MiB /  6070MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1007      G   /usr/lib/xorg/Xorg                           152MiB |\r\n|    0      1485      G   /usr/bin/kwin_x11                             36MiB |\r\n|    0      1489      G   /usr/bin/krunner                               1MiB |\r\n|    0      1491      G   /usr/bin/plasmashell                          30MiB |\r\n|    0      2729      G   ...quest-channel-token=5720462904883144478    51MiB |\r\n+-----------------------------------------------------------------------------+\r\n```", "comments": ["Please post your `sudo dmesg` output when this issue occurs. It is very difficult to reproduce your issue, so we have to look closer.", "@byronyi Sure thing\r\n```\r\n[    0.000000] microcode: microcode updated early to revision 0x25, date = 2018-04-02\r\n[    0.000000] Linux version 4.18.0-2-amd64 (debian-kernel@lists.debian.org) (gcc version 7.3.0 (Debian 7.3.0-30)) #1 SMP Debian 4.18.10-2 (2018-11-02)\r\n[    0.000000] Command line: BOOT_IMAGE=/boot/vmlinuz-4.18.0-2-amd64 root=UUID=412abef5-6a99-4d7e-abda-c02086b097b7 ro quiet\r\n[    0.000000] x86/fpu: Supporting XSAVE feature 0x001: 'x87 floating point registers'\r\n[    0.000000] x86/fpu: Supporting XSAVE feature 0x002: 'SSE registers'\r\n[    0.000000] x86/fpu: Supporting XSAVE feature 0x004: 'AVX registers'\r\n[    0.000000] x86/fpu: xstate_offset[2]:  576, xstate_sizes[2]:  256\r\n[    0.000000] x86/fpu: Enabled xstate features 0x7, context size is 832 bytes, using 'standard' format.\r\n[    0.000000] BIOS-provided physical RAM map:\r\n[    0.000000] BIOS-e820: [mem 0x0000000000000000-0x0000000000057fff] usable\r\n[    0.000000] BIOS-e820: [mem 0x0000000000058000-0x0000000000058fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x0000000000059000-0x000000000009efff] usable\r\n[    0.000000] BIOS-e820: [mem 0x000000000009f000-0x000000000009ffff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x0000000000100000-0x00000000cc358fff] usable\r\n[    0.000000] BIOS-e820: [mem 0x00000000cc359000-0x00000000cc35ffff] ACPI NVS\r\n[    0.000000] BIOS-e820: [mem 0x00000000cc360000-0x00000000ccd52fff] usable\r\n[    0.000000] BIOS-e820: [mem 0x00000000ccd53000-0x00000000cd00afff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000cd00b000-0x00000000de3c1fff] usable\r\n[    0.000000] BIOS-e820: [mem 0x00000000de3c2000-0x00000000de812fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000de813000-0x00000000de832fff] ACPI data\r\n[    0.000000] BIOS-e820: [mem 0x00000000de833000-0x00000000de99ffff] ACPI NVS\r\n[    0.000000] BIOS-e820: [mem 0x00000000de9a0000-0x00000000def74fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000def75000-0x00000000deffefff] type 20\r\n[    0.000000] BIOS-e820: [mem 0x00000000defff000-0x00000000deffffff] usable\r\n[    0.000000] BIOS-e820: [mem 0x00000000f8000000-0x00000000fbffffff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000fec00000-0x00000000fec00fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000fed00000-0x00000000fed03fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000fed1c000-0x00000000fed1ffff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000fee00000-0x00000000fee00fff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x00000000ff000000-0x00000000ffffffff] reserved\r\n[    0.000000] BIOS-e820: [mem 0x0000000100000000-0x000000081effffff] usable\r\n[    0.000000] NX (Execute Disable) protection: active\r\n[    0.000000] efi: EFI v2.31 by American Megatrends\r\n[    0.000000] efi:  ESRT=0xdef73998  ACPI=0xde81a000  ACPI 2.0=0xde81a000  SMBIOS=0xf04c0  MPS=0xfd4b0 \r\n[    0.000000] secureboot: Secure boot could not be determined (mode 0)\r\n[    0.000000] SMBIOS 2.7 present.\r\n[    0.000000] DMI: Hewlett-Packard 810-135qe/2AF3, BIOS 80.16 12/05/2013\r\n[    0.000000] e820: update [mem 0x00000000-0x00000fff] usable ==> reserved\r\n[    0.000000] e820: remove [mem 0x000a0000-0x000fffff] usable\r\n[    0.000000] last_pfn = 0x81f000 max_arch_pfn = 0x400000000\r\n[    0.000000] MTRR default type: uncachable\r\n[    0.000000] MTRR fixed ranges enabled:\r\n[    0.000000]   00000-9FFFF write-back\r\n[    0.000000]   A0000-BFFFF uncachable\r\n[    0.000000]   C0000-CFFFF write-protect\r\n[    0.000000]   D0000-DFFFF uncachable\r\n[    0.000000]   E0000-FFFFF write-protect\r\n[    0.000000] MTRR variable ranges enabled:\r\n[    0.000000]   0 base 0000000000 mask 7800000000 write-back\r\n[    0.000000]   1 base 0800000000 mask 7FF0000000 write-back\r\n[    0.000000]   2 base 0810000000 mask 7FF8000000 write-back\r\n[    0.000000]   3 base 0818000000 mask 7FFC000000 write-back\r\n[    0.000000]   4 base 081C000000 mask 7FFE000000 write-back\r\n[    0.000000]   5 base 081E000000 mask 7FFF000000 write-back\r\n[    0.000000]   6 base 00E0000000 mask 7FE0000000 uncachable\r\n[    0.000000]   7 disabled\r\n[    0.000000]   8 disabled\r\n[    0.000000]   9 disabled\r\n[    0.000000] x86/PAT: Configuration [0-7]: WB  WC  UC- UC  WB  WP  UC- WT  \r\n[    0.000000] e820: update [mem 0xe0000000-0xffffffff] usable ==> reserved\r\n[    0.000000] last_pfn = 0xdf000 max_arch_pfn = 0x400000000\r\n[    0.000000] found SMP MP-table at [mem 0x000fd760-0x000fd76f] mapped at [(____ptrval____)]\r\n[    0.000000] esrt: Reserving ESRT space from 0x00000000def73998 to 0x00000000def739d0.\r\n[    0.000000] Base memory trampoline at [(____ptrval____)] 97000 size 24576\r\n[    0.000000] Using GB pages for direct mapping\r\n[    0.000000] BRK [0x375a4b000, 0x375a4bfff] PGTABLE\r\n[    0.000000] BRK [0x375a4c000, 0x375a4cfff] PGTABLE\r\n[    0.000000] BRK [0x375a4d000, 0x375a4dfff] PGTABLE\r\n[    0.000000] BRK [0x375a4e000, 0x375a4efff] PGTABLE\r\n[    0.000000] BRK [0x375a4f000, 0x375a4ffff] PGTABLE\r\n[    0.000000] BRK [0x375a50000, 0x375a50fff] PGTABLE\r\n[    0.000000] BRK [0x375a51000, 0x375a51fff] PGTABLE\r\n[    0.000000] BRK [0x375a52000, 0x375a52fff] PGTABLE\r\n[    0.000000] BRK [0x375a53000, 0x375a53fff] PGTABLE\r\n[    0.000000] BRK [0x375a54000, 0x375a54fff] PGTABLE\r\n[    0.000000] BRK [0x375a55000, 0x375a55fff] PGTABLE\r\n[    0.000000] RAMDISK: [mem 0x34029000-0x3600bfff]\r\n[    0.000000] ACPI: Early table checksum verification disabled\r\n[    0.000000] ACPI: RSDP 0x00000000DE81A000 000024 (v02 HPQOEM)\r\n[    0.000000] ACPI: XSDT 0x00000000DE81A088 00008C (v01 HPQOEM SLIC-CPC 01072009 AMI  00010013)\r\n[    0.000000] ACPI: FACP 0x00000000DE82E2F0 00010C (v05 HPQOEM SLIC-CPC 01072009 AMI  00010013)\r\n[    0.000000] ACPI: DSDT 0x00000000DE81A1A0 014149 (v02 HPQOEM SLIC-CPC 00008016 INTL 20120711)\r\n[    0.000000] ACPI: FACS 0x00000000DE99E080 000040\r\n[    0.000000] ACPI: APIC 0x00000000DE82E400 000092 (v03 HPQOEM SLIC-CPC 01072009 AMI  00010013)\r\n[    0.000000] ACPI: FPDT 0x00000000DE82E498 000044 (v01 HPQOEM SLIC-CPC 01072009 AMI  00010013)\r\n[    0.000000] ACPI: ASF! 0x00000000DE82E4E0 0000A5 (v32 HPQOEM SLIC-CPC 00000001 TFSM 000F4240)\r\n[    0.000000] ACPI: SLIC 0x00000000DE82E588 000176 (v01 HPQOEM SLIC-CPC 01072009 AMI  00010013)\r\n[    0.000000] ACPI: SSDT 0x00000000DE82E700 000539 (v01 PmRef  Cpu0Ist  00003000 INTL 20120711)\r\n[    0.000000] ACPI: SSDT 0x00000000DE82EC40 000AD8 (v01 PmRef  CpuPm    00003000 INTL 20120711)\r\n[    0.000000] ACPI: MCFG 0x00000000DE82F718 00003C (v01 HPQOEM SLIC-CPC 01072009 MSFT 00000097)\r\n[    0.000000] ACPI: HPET 0x00000000DE82F758 000038 (v01 HPQOEM SLIC-CPC 01072009 AMI. 00000005)\r\n[    0.000000] ACPI: SSDT 0x00000000DE82F790 00036D (v01 SataRe SataTabl 00001000 INTL 20120711)\r\n[    0.000000] ACPI: SSDT 0x00000000DE82FB00 0033C9 (v01 SaSsdt SaSsdt   00003000 INTL 20091112)\r\n[    0.000000] ACPI: DBGP 0x00000000DE832ED0 000034 (v01 HPQOEM SLIC-CPC 01072009 AMI  00010013)\r\n[    0.000000] ACPI: BGRT 0x00000000DE832F08 000038 (v00 HPQOEM SLIC-CPC 01072009 AMI  00010013)\r\n[    0.000000] ACPI: Local APIC address 0xfee00000\r\n[    0.000000] No NUMA configuration found\r\n[    0.000000] Faking a node at [mem 0x0000000000000000-0x000000081effffff]\r\n[    0.000000] NODE_DATA(0) allocated [mem 0x81effb000-0x81effffff]\r\n[    0.000000] tsc: Fast TSC calibration using PIT\r\n[    0.000000] Zone ranges:\r\n[    0.000000]   DMA      [mem 0x0000000000001000-0x0000000000ffffff]\r\n[    0.000000]   DMA32    [mem 0x0000000001000000-0x00000000ffffffff]\r\n[    0.000000]   Normal   [mem 0x0000000100000000-0x000000081effffff]\r\n[    0.000000]   Device   empty\r\n[    0.000000] Movable zone start for each node\r\n[    0.000000] Early memory node ranges\r\n[    0.000000]   node   0: [mem 0x0000000000001000-0x0000000000057fff]\r\n[    0.000000]   node   0: [mem 0x0000000000059000-0x000000000009efff]\r\n[    0.000000]   node   0: [mem 0x0000000000100000-0x00000000cc358fff]\r\n[    0.000000]   node   0: [mem 0x00000000cc360000-0x00000000ccd52fff]\r\n[    0.000000]   node   0: [mem 0x00000000cd00b000-0x00000000de3c1fff]\r\n[    0.000000]   node   0: [mem 0x00000000defff000-0x00000000deffffff]\r\n[    0.000000]   node   0: [mem 0x0000000100000000-0x000000081effffff]\r\n[    0.000000] Reserved but unavailable: 8031 pages\r\n[    0.000000] Initmem setup node 0 [mem 0x0000000000001000-0x000000081effffff]\r\n[    0.000000] On node 0 totalpages: 8376481\r\n[    0.000000]   DMA zone: 64 pages used for memmap\r\n[    0.000000]   DMA zone: 26 pages reserved\r\n[    0.000000]   DMA zone: 3997 pages, LIFO batch:0\r\n[    0.000000]   DMA32 zone: 14149 pages used for memmap\r\n[    0.000000]   DMA32 zone: 905476 pages, LIFO batch:31\r\n[    0.000000]   Normal zone: 116672 pages used for memmap\r\n[    0.000000]   Normal zone: 7467008 pages, LIFO batch:31\r\n[    0.000000] ACPI: PM-Timer IO Port: 0x1808\r\n[    0.000000] ACPI: Local APIC address 0xfee00000\r\n[    0.000000] ACPI: LAPIC_NMI (acpi_id[0xff] high edge lint[0x1])\r\n[    0.000000] IOAPIC[0]: apic_id 2, version 32, address 0xfec00000, GSI 0-23\r\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 0 global_irq 2 dfl dfl)\r\n[    0.000000] ACPI: INT_SRC_OVR (bus 0 bus_irq 9 global_irq 9 high level)\r\n[    0.000000] ACPI: IRQ0 used by override.\r\n[    0.000000] ACPI: IRQ9 used by override.\r\n[    0.000000] Using ACPI (MADT) for SMP configuration information\r\n[    0.000000] ACPI: HPET id: 0x8086a701 base: 0xfed00000\r\n[    0.000000] smpboot: Allowing 8 CPUs, 0 hotplug CPUs\r\n[    0.000000] PM: Registered nosave memory: [mem 0x00000000-0x00000fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0x00058000-0x00058fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0x0009f000-0x0009ffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0x000a0000-0x000fffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xcc359000-0xcc35ffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xccd53000-0xcd00afff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xde3c2000-0xde812fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xde813000-0xde832fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xde833000-0xde99ffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xde9a0000-0xdef74fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xdef75000-0xdeffefff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xdf000000-0xf7ffffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xf8000000-0xfbffffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfc000000-0xfebfffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfec00000-0xfec00fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfec01000-0xfecfffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfed00000-0xfed03fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfed04000-0xfed1bfff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfed1c000-0xfed1ffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfed20000-0xfedfffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfee00000-0xfee00fff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xfee01000-0xfeffffff]\r\n[    0.000000] PM: Registered nosave memory: [mem 0xff000000-0xffffffff]\r\n[    0.000000] [mem 0xdf000000-0xf7ffffff] available for PCI devices\r\n[    0.000000] Booting paravirtualized kernel on bare hardware\r\n[    0.000000] clocksource: refined-jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 7645519600211568 ns\r\n[    0.000000] random: get_random_bytes called from start_kernel+0x94/0x52e with crng_init=0\r\n[    0.000000] setup_percpu: NR_CPUS:512 nr_cpumask_bits:512 nr_cpu_ids:8 nr_node_ids:1\r\n[    0.000000] percpu: Embedded 44 pages/cpu @(____ptrval____) s142744 r8192 d29288 u262144\r\n[    0.000000] pcpu-alloc: s142744 r8192 d29288 u262144 alloc=1*2097152\r\n[    0.000000] pcpu-alloc: [0] 0 1 2 3 4 5 6 7 \r\n[    0.000000] Built 1 zonelists, mobility grouping on.  Total pages: 8245570\r\n[    0.000000] Policy zone: Normal\r\n[    0.000000] Kernel command line: BOOT_IMAGE=/boot/vmlinuz-4.18.0-2-amd64 root=UUID=412abef5-6a99-4d7e-abda-c02086b097b7 ro quiet\r\n[    0.000000] Calgary: detecting Calgary via BIOS EBDA area\r\n[    0.000000] Calgary: Unable to locate Rio Grande table in EBDA - bailing!\r\n[    0.000000] Memory: 32633704K/33505924K available (10252K kernel code, 1223K rwdata, 3260K rodata, 1548K init, 656K bss, 872220K reserved, 0K cma-reserved)\r\n[    0.000000] SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=8, Nodes=1\r\n[    0.000000] Kernel/User page tables isolation: enabled\r\n[    0.000000] ftrace: allocating 31267 entries in 123 pages\r\n[    0.000000] Hierarchical RCU implementation.\r\n[    0.000000]  RCU restricting CPUs from NR_CPUS=512 to nr_cpu_ids=8.\r\n[    0.000000] RCU: Adjusting geometry for rcu_fanout_leaf=16, nr_cpu_ids=8\r\n[    0.000000] NR_IRQS: 33024, nr_irqs: 488, preallocated irqs: 16\r\n[    0.000000] Console: colour dummy device 80x25\r\n[    0.000000] console [tty0] enabled\r\n[    0.000000] ACPI: Core revision 20180531\r\n[    0.000000] clocksource: hpet: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 133484882848 ns\r\n[    0.000000] hpet clockevent registered\r\n[    0.000000] APIC: Switch to symmetric I/O mode setup\r\n[    0.000000] ..TIMER: vector=0x30 apic1=0 pin1=2 apic2=-1 pin2=-1\r\n[    0.020000] tsc: Fast TSC calibration using PIT\r\n[    0.024000] tsc: Detected 3492.193 MHz processor\r\n[    0.024000] clocksource: tsc-early: mask: 0xffffffffffffffff max_cycles: 0x3256826d48f, max_idle_ns: 440795324722 ns\r\n[    0.024000] Calibrating delay loop (skipped), value calculated using timer frequency.. 6984.38 BogoMIPS (lpj=13968772)\r\n[    0.024000] pid_max: default: 32768 minimum: 301\r\n[    0.024000] Security Framework initialized\r\n[    0.024000] Yama: disabled by default; enable with sysctl kernel.yama.*\r\n[    0.024000] AppArmor: AppArmor initialized\r\n[    0.029312] Dentry cache hash table entries: 4194304 (order: 13, 33554432 bytes)\r\n[    0.031124] Inode-cache hash table entries: 2097152 (order: 12, 16777216 bytes)\r\n[    0.031194] Mount-cache hash table entries: 65536 (order: 7, 524288 bytes)\r\n[    0.031251] Mountpoint-cache hash table entries: 65536 (order: 7, 524288 bytes)\r\n[    0.031402] ENERGY_PERF_BIAS: Set to 'normal', was 'performance'\r\n[    0.031402] ENERGY_PERF_BIAS: View and update with x86_energy_perf_policy(8)\r\n[    0.031406] mce: CPU supports 9 MCE banks\r\n[    0.031414] CPU0: Thermal monitoring enabled (TM1)\r\n[    0.031424] process: using mwait in idle threads\r\n[    0.031426] Last level iTLB entries: 4KB 1024, 2MB 1024, 4MB 1024\r\n[    0.031427] Last level dTLB entries: 4KB 1024, 2MB 1024, 4MB 1024, 1GB 4\r\n[    0.031428] Spectre V2 : Mitigation: Full generic retpoline\r\n[    0.031428] Spectre V2 : Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\r\n[    0.031429] Spectre V2 : Spectre v2 mitigation: Enabling Indirect Branch Prediction Barrier\r\n[    0.031429] Spectre V2 : Enabling Restricted Speculation for firmware calls\r\n[    0.031430] Speculative Store Bypass: Mitigation: Speculative Store Bypass disabled via prctl and seccomp\r\n[    0.034200] Freeing SMP alternatives memory: 24K\r\n[    0.039207] TSC deadline timer enabled\r\n[    0.039210] smpboot: CPU0: Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz (family: 0x6, model: 0x3c, stepping: 0x3)\r\n[    0.039269] Performance Events: PEBS fmt2+, Haswell events, 16-deep LBR, full-width counters, Intel PMU driver.\r\n[    0.039297] ... version:                3\r\n[    0.039297] ... bit width:              48\r\n[    0.039298] ... generic registers:      4\r\n[    0.039298] ... value mask:             0000ffffffffffff\r\n[    0.039299] ... max period:             00007fffffffffff\r\n[    0.039299] ... fixed-purpose events:   3\r\n[    0.039299] ... event mask:             000000070000000f\r\n[    0.039323] Hierarchical SRCU implementation.\r\n[    0.039995] NMI watchdog: Enabled. Permanently consumes one hw-PMU counter.\r\n[    0.040000] smp: Bringing up secondary CPUs ...\r\n[    0.040000] x86: Booting SMP configuration:\r\n[    0.040000] .... node  #0, CPUs:      #1 #2 #3 #4 #5 #6 #7\r\n[    0.041543] smp: Brought up 1 node, 8 CPUs\r\n[    0.041543] smpboot: Max logical packages: 1\r\n[    0.041543] smpboot: Total of 8 processors activated (55875.08 BogoMIPS)\r\n[    0.044613] devtmpfs: initialized\r\n[    0.044613] x86/mm: Memory block size: 128MB\r\n[    0.045453] PM: Registering ACPI NVS region [mem 0xcc359000-0xcc35ffff] (28672 bytes)\r\n[    0.045453] PM: Registering ACPI NVS region [mem 0xde833000-0xde99ffff] (1495040 bytes)\r\n[    0.045453] clocksource: jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 7645041785100000 ns\r\n[    0.045453] futex hash table entries: 2048 (order: 5, 131072 bytes)\r\n[    0.045453] pinctrl core: initialized pinctrl subsystem\r\n[    0.045453] NET: Registered protocol family 16\r\n[    0.045453] audit: initializing netlink subsys (disabled)\r\n[    0.045453] audit: type=2000 audit(1543000327.044:1): state=initialized audit_enabled=0 res=1\r\n[    0.045453] cpuidle: using governor ladder\r\n[    0.045453] cpuidle: using governor menu\r\n[    0.045453] ACPI FADT declares the system doesn't support PCIe ASPM, so disable it\r\n[    0.045453] ACPI: bus type PCI registered\r\n[    0.045453] acpiphp: ACPI Hot Plug PCI Controller Driver version: 0.5\r\n[    0.045453] PCI: MMCONFIG for domain 0000 [bus 00-3f] at [mem 0xf8000000-0xfbffffff] (base 0xf8000000)\r\n[    0.045453] PCI: MMCONFIG at [mem 0xf8000000-0xfbffffff] reserved in E820\r\n[    0.045453] pmd_set_huge: Cannot satisfy [mem 0xf8000000-0xf8200000] with a huge-page mapping due to MTRR override.\r\n[    0.045453] PCI: Using configuration type 1 for base access\r\n[    0.045453] core: PMU erratum BJ122, BV98, HSD29 worked around, HT is on\r\n[    0.045453] mtrr: your CPUs had inconsistent fixed MTRR settings\r\n[    0.045453] mtrr: probably your BIOS does not setup all CPUs.\r\n[    0.045453] mtrr: corrected configuration.\r\n[    0.045453] HugeTLB registered 1.00 GiB page size, pre-allocated 0 pages\r\n[    0.045453] HugeTLB registered 2.00 MiB page size, pre-allocated 0 pages\r\n[    0.048052] ACPI: Added _OSI(Module Device)\r\n[    0.048053] ACPI: Added _OSI(Processor Device)\r\n[    0.048053] ACPI: Added _OSI(3.0 _SCP Extensions)\r\n[    0.048054] ACPI: Added _OSI(Processor Aggregator Device)\r\n[    0.048055] ACPI: Added _OSI(Linux-Dell-Video)\r\n[    0.053156] ACPI: 5 ACPI AML tables successfully acquired and loaded\r\n[    0.055270] ACPI: [Firmware Bug]: BIOS _OSI(Linux) query ignored\r\n[    0.056084] ACPI: Dynamic OEM Table Load:\r\n[    0.056088] ACPI: SSDT 0xFFFF88F7FA58A800 0003D3 (v01 PmRef  Cpu0Cst  00003001 INTL 20120711)\r\n[    0.056202] ACPI: Dynamic OEM Table Load:\r\n[    0.056202] ACPI: SSDT 0xFFFF88F7FA9A5000 0005AA (v01 PmRef  ApIst    00003000 INTL 20120711)\r\n[    0.056414] ACPI: Dynamic OEM Table Load:\r\n[    0.056414] ACPI: SSDT 0xFFFF88F7FA56D200 000119 (v01 PmRef  ApCst    00003000 INTL 20120711)\r\n[    0.056415] ACPI: Interpreter enabled\r\n[    0.056415] ACPI: (supports S0 S3 S4 S5)\r\n[    0.056415] ACPI: Using IOAPIC for interrupt routing\r\n[    0.056427] PCI: Using host bridge windows from ACPI; if necessary, use \"pci=nocrs\" and report a bug\r\n[    0.056578] ACPI: Enabled 9 GPEs in block 00 to 3F\r\n[    0.062089] ACPI: Power Resource [FN00] (off)\r\n[    0.062133] ACPI: Power Resource [FN01] (off)\r\n[    0.062174] ACPI: Power Resource [FN02] (off)\r\n[    0.062215] ACPI: Power Resource [FN03] (off)\r\n[    0.062255] ACPI: Power Resource [FN04] (off)\r\n[    0.062740] ACPI: PCI Root Bridge [PCI0] (domain 0000 [bus 00-3e])\r\n[    0.062744] acpi PNP0A08:00: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]\r\n[    0.062868] acpi PNP0A08:00: _OSC: platform does not support [PCIeHotplug SHPCHotplug PME]\r\n[    0.062952] acpi PNP0A08:00: _OSC: OS now controls [AER PCIeCapability LTR]\r\n[    0.062953] acpi PNP0A08:00: FADT indicates ASPM is unsupported, using BIOS configuration\r\n[    0.063238] PCI host bridge to bus 0000:00\r\n[    0.063239] pci_bus 0000:00: root bus resource [io  0x0000-0x0cf7 window]\r\n[    0.063240] pci_bus 0000:00: root bus resource [io  0x0d00-0xffff window]\r\n[    0.063241] pci_bus 0000:00: root bus resource [mem 0x000a0000-0x000bffff window]\r\n[    0.063242] pci_bus 0000:00: root bus resource [mem 0x000d0000-0x000d3fff window]\r\n[    0.063243] pci_bus 0000:00: root bus resource [mem 0x000d4000-0x000d7fff window]\r\n[    0.063244] pci_bus 0000:00: root bus resource [mem 0x000d8000-0x000dbfff window]\r\n[    0.063245] pci_bus 0000:00: root bus resource [mem 0x000dc000-0x000dffff window]\r\n[    0.063246] pci_bus 0000:00: root bus resource [mem 0xe0000000-0xfeafffff window]\r\n[    0.063247] pci_bus 0000:00: root bus resource [bus 00-3e]\r\n[    0.063252] pci 0000:00:00.0: [8086:0c00] type 00 class 0x060000\r\n[    0.063320] pci 0000:00:01.0: [8086:0c01] type 01 class 0x060400\r\n[    0.063347] pci 0000:00:01.0: PME# supported from D0 D3hot D3cold\r\n[    0.063442] pci 0000:00:14.0: [8086:8c31] type 00 class 0x0c0330\r\n[    0.063461] pci 0000:00:14.0: reg 0x10: [mem 0xf7200000-0xf720ffff 64bit]\r\n[    0.063509] pci 0000:00:14.0: PME# supported from D3hot D3cold\r\n[    0.063565] pci 0000:00:16.0: [8086:8c3a] type 00 class 0x078000\r\n[    0.063584] pci 0000:00:16.0: reg 0x10: [mem 0xf721a000-0xf721a00f 64bit]\r\n[    0.063635] pci 0000:00:16.0: PME# supported from D0 D3hot D3cold\r\n[    0.063696] pci 0000:00:1a.0: [8086:8c2d] type 00 class 0x0c0320\r\n[    0.063714] pci 0000:00:1a.0: reg 0x10: [mem 0xf7218000-0xf72183ff]\r\n[    0.063784] pci 0000:00:1a.0: PME# supported from D0 D3hot D3cold\r\n[    0.063843] pci 0000:00:1b.0: [8086:8c20] type 00 class 0x040300\r\n[    0.063859] pci 0000:00:1b.0: reg 0x10: [mem 0xf7210000-0xf7213fff 64bit]\r\n[    0.063908] pci 0000:00:1b.0: PME# supported from D0 D3hot D3cold\r\n[    0.063963] pci 0000:00:1c.0: [8086:8c10] type 01 class 0x060400\r\n[    0.064024] pci 0000:00:1c.0: PME# supported from D0 D3hot D3cold\r\n[    0.064108] pci 0000:00:1c.2: [8086:8c14] type 01 class 0x060400\r\n[    0.064168] pci 0000:00:1c.2: PME# supported from D0 D3hot D3cold\r\n[    0.064252] pci 0000:00:1d.0: [8086:8c26] type 00 class 0x0c0320\r\n[    0.064271] pci 0000:00:1d.0: reg 0x10: [mem 0xf7217000-0xf72173ff]\r\n[    0.064342] pci 0000:00:1d.0: PME# supported from D0 D3hot D3cold\r\n[    0.064402] pci 0000:00:1f.0: [8086:8c44] type 00 class 0x060100\r\n[    0.064542] pci 0000:00:1f.2: [8086:8c02] type 00 class 0x010601\r\n[    0.064556] pci 0000:00:1f.2: reg 0x10: [io  0xf070-0xf077]\r\n[    0.064562] pci 0000:00:1f.2: reg 0x14: [io  0xf060-0xf063]\r\n[    0.064567] pci 0000:00:1f.2: reg 0x18: [io  0xf050-0xf057]\r\n[    0.064573] pci 0000:00:1f.2: reg 0x1c: [io  0xf040-0xf043]\r\n[    0.064579] pci 0000:00:1f.2: reg 0x20: [io  0xf020-0xf03f]\r\n[    0.064585] pci 0000:00:1f.2: reg 0x24: [mem 0xf7216000-0xf72167ff]\r\n[    0.064614] pci 0000:00:1f.2: PME# supported from D3hot\r\n[    0.064666] pci 0000:00:1f.3: [8086:8c22] type 00 class 0x0c0500\r\n[    0.064681] pci 0000:00:1f.3: reg 0x10: [mem 0xf7215000-0xf72150ff 64bit]\r\n[    0.064697] pci 0000:00:1f.3: reg 0x20: [io  0xf000-0xf01f]\r\n[    0.064783] pci 0000:01:00.0: [10de:1c03] type 00 class 0x030000\r\n[    0.064800] pci 0000:01:00.0: reg 0x10: [mem 0xf6000000-0xf6ffffff]\r\n[    0.064809] pci 0000:01:00.0: reg 0x14: [mem 0xe0000000-0xefffffff 64bit pref]\r\n[    0.064817] pci 0000:01:00.0: reg 0x1c: [mem 0xf0000000-0xf1ffffff 64bit pref]\r\n[    0.064823] pci 0000:01:00.0: reg 0x24: [io  0xe000-0xe07f]\r\n[    0.064829] pci 0000:01:00.0: reg 0x30: [mem 0xf7000000-0xf707ffff pref]\r\n[    0.064840] pci 0000:01:00.0: BAR 3: assigned to efifb\r\n[    0.064916] pci 0000:01:00.1: [10de:10f1] type 00 class 0x040300\r\n[    0.064929] pci 0000:01:00.1: reg 0x10: [mem 0xf7080000-0xf7083fff]\r\n[    0.065038] pci 0000:00:01.0: PCI bridge to [bus 01]\r\n[    0.065039] pci 0000:00:01.0:   bridge window [io  0xe000-0xefff]\r\n[    0.065041] pci 0000:00:01.0:   bridge window [mem 0xf6000000-0xf70fffff]\r\n[    0.065043] pci 0000:00:01.0:   bridge window [mem 0xe0000000-0xf1ffffff 64bit pref]\r\n[    0.065101] acpiphp: Slot [1] registered\r\n[    0.065105] pci 0000:00:1c.0: PCI bridge to [bus 02]\r\n[    0.065167] pci 0000:03:00.0: [10ec:8168] type 00 class 0x020000\r\n[    0.065194] pci 0000:03:00.0: reg 0x10: [io  0xd000-0xd0ff]\r\n[    0.065219] pci 0000:03:00.0: reg 0x18: [mem 0xf7104000-0xf7104fff 64bit]\r\n[    0.065235] pci 0000:03:00.0: reg 0x20: [mem 0xf7100000-0xf7103fff 64bit pref]\r\n[    0.065325] pci 0000:03:00.0: supports D1 D2\r\n[    0.065326] pci 0000:03:00.0: PME# supported from D0 D1 D2 D3hot D3cold\r\n[    0.065421] pci 0000:00:1c.2: PCI bridge to [bus 03]\r\n[    0.065424] pci 0000:00:1c.2:   bridge window [io  0xd000-0xdfff]\r\n[    0.065427] pci 0000:00:1c.2:   bridge window [mem 0xf7100000-0xf71fffff]\r\n[    0.065973] ACPI: PCI Interrupt Link [LNKA] (IRQs 3 4 5 6 10 *11 12 14 15)\r\n[    0.066020] ACPI: PCI Interrupt Link [LNKB] (IRQs 3 4 5 6 *10 11 12 14 15)\r\n[    0.066056] ACPI: PCI Interrupt Link [LNKC] (IRQs 3 *4 5 6 10 11 12 14 15)\r\n[    0.066097] ACPI: PCI Interrupt Link [LNKD] (IRQs 3 4 *5 6 10 11 12 14 15)\r\n[    0.066138] ACPI: PCI Interrupt Link [LNKE] (IRQs 3 4 5 6 10 11 12 14 15) *0, disabled.\r\n[    0.066174] ACPI: PCI Interrupt Link [LNKF] (IRQs 3 4 5 6 10 11 12 14 15) *0, disabled.\r\n[    0.066215] ACPI: PCI Interrupt Link [LNKG] (IRQs *3 4 5 6 10 11 12 14 15)\r\n[    0.066257] ACPI: PCI Interrupt Link [LNKH] (IRQs 3 4 5 6 10 *11 12 14 15)\r\n[    0.068006] pci 0000:01:00.0: vgaarb: setting as boot VGA device\r\n[    0.068007] pci 0000:01:00.0: vgaarb: VGA device added: decodes=io+mem,owns=io+mem,locks=none\r\n[    0.068008] pci 0000:01:00.0: vgaarb: bridge control possible\r\n[    0.068008] vgaarb: loaded\r\n[    0.068040] pps_core: LinuxPPS API ver. 1 registered\r\n[    0.068040] pps_core: Software ver. 5.3.6 - Copyright 2005-2007 Rodolfo Giometti <giometti@linux.it>\r\n[    0.068042] PTP clock support registered\r\n[    0.068045] EDAC MC: Ver: 3.0.0\r\n[    0.068091] Registered efivars operations\r\n[    0.072008] PCI: Using ACPI for IRQ routing\r\n[    0.073006] PCI: pci_cache_line_size set to 64 bytes\r\n[    0.073040] e820: reserve RAM buffer [mem 0x00058000-0x0005ffff]\r\n[    0.073041] e820: reserve RAM buffer [mem 0x0009f000-0x0009ffff]\r\n[    0.073041] e820: reserve RAM buffer [mem 0xcc359000-0xcfffffff]\r\n[    0.073042] e820: reserve RAM buffer [mem 0xccd53000-0xcfffffff]\r\n[    0.073042] e820: reserve RAM buffer [mem 0xde3c2000-0xdfffffff]\r\n[    0.073043] e820: reserve RAM buffer [mem 0xdf000000-0xdfffffff]\r\n[    0.073044] e820: reserve RAM buffer [mem 0x81f000000-0x81fffffff]\r\n[    0.076209] hpet0: at MMIO 0xfed00000, IRQs 2, 8, 0, 0, 0, 0, 0, 0\r\n[    0.076213] hpet0: 8 comparators, 64-bit 14.318180 MHz counter\r\n[    0.078236] clocksource: Switched to clocksource tsc-early\r\n[    0.082498] VFS: Disk quotas dquot_6.6.0\r\n[    0.082507] VFS: Dquot-cache hash table entries: 512 (order 0, 4096 bytes)\r\n[    0.082578] AppArmor: AppArmor Filesystem Enabled\r\n[    0.082587] pnp: PnP ACPI init\r\n[    0.082641] system 00:00: [mem 0xfed40000-0xfed44fff] has been reserved\r\n[    0.082644] system 00:00: Plug and Play ACPI device, IDs PNP0c01 (active)\r\n[    0.082761] system 00:01: [io  0x0680-0x069f] has been reserved\r\n[    0.082763] system 00:01: [io  0xffff] has been reserved\r\n[    0.082764] system 00:01: [io  0xffff] has been reserved\r\n[    0.082765] system 00:01: [io  0xffff] has been reserved\r\n[    0.082766] system 00:01: [io  0x1c00-0x1cfe] has been reserved\r\n[    0.082767] system 00:01: [io  0x1d00-0x1dfe] has been reserved\r\n[    0.082768] system 00:01: [io  0x1e00-0x1efe] has been reserved\r\n[    0.082768] system 00:01: [io  0x1f00-0x1ffe] has been reserved\r\n[    0.082769] system 00:01: [io  0x1800-0x18fe] has been reserved\r\n[    0.082770] system 00:01: [io  0x164e-0x164f] has been reserved\r\n[    0.082773] system 00:01: Plug and Play ACPI device, IDs PNP0c02 (active)\r\n[    0.082786] pnp 00:02: Plug and Play ACPI device, IDs PNP0b00 (active)\r\n[    0.082813] system 00:03: [io  0x1854-0x1857] has been reserved\r\n[    0.082815] system 00:03: Plug and Play ACPI device, IDs INT3f0d PNP0c02 (active)\r\n[    0.082864] system 00:04: [io  0x0290-0x029f] has been reserved\r\n[    0.082866] system 00:04: Plug and Play ACPI device, IDs PNP0c02 (active)\r\n[    0.082910] system 00:05: [io  0x04d0-0x04d1] has been reserved\r\n[    0.082912] system 00:05: Plug and Play ACPI device, IDs PNP0c02 (active)\r\n[    0.083234] system 00:06: [mem 0xfed1c000-0xfed1ffff] has been reserved\r\n[    0.083235] system 00:06: [mem 0xfed10000-0xfed17fff] has been reserved\r\n[    0.083238] system 00:06: [mem 0xfed18000-0xfed18fff] has been reserved\r\n[    0.083239] system 00:06: [mem 0xfed19000-0xfed19fff] has been reserved\r\n[    0.083240] system 00:06: [mem 0xf8000000-0xfbffffff] has been reserved\r\n[    0.083241] system 00:06: [mem 0xfed20000-0xfed3ffff] has been reserved\r\n[    0.083242] system 00:06: [mem 0xfed90000-0xfed93fff] has been reserved\r\n[    0.083243] system 00:06: [mem 0xfed45000-0xfed8ffff] has been reserved\r\n[    0.083244] system 00:06: [mem 0xff000000-0xffffffff] has been reserved\r\n[    0.083245] system 00:06: [mem 0xfee00000-0xfeefffff] could not be reserved\r\n[    0.083246] system 00:06: [mem 0xf7fee000-0xf7feefff] has been reserved\r\n[    0.083247] system 00:06: [mem 0xf7fd0000-0xf7fdffff] has been reserved\r\n[    0.083250] system 00:06: Plug and Play ACPI device, IDs PNP0c02 (active)\r\n[    0.083432] pnp: PnP ACPI: found 7 devices\r\n[    0.088706] clocksource: acpi_pm: mask: 0xffffff max_cycles: 0xffffff, max_idle_ns: 2085701024 ns\r\n[    0.088728] pci 0000:00:01.0: PCI bridge to [bus 01]\r\n[    0.088730] pci 0000:00:01.0:   bridge window [io  0xe000-0xefff]\r\n[    0.088732] pci 0000:00:01.0:   bridge window [mem 0xf6000000-0xf70fffff]\r\n[    0.088734] pci 0000:00:01.0:   bridge window [mem 0xe0000000-0xf1ffffff 64bit pref]\r\n[    0.088736] pci 0000:00:1c.0: PCI bridge to [bus 02]\r\n[    0.088748] pci 0000:00:1c.2: PCI bridge to [bus 03]\r\n[    0.088751] pci 0000:00:1c.2:   bridge window [io  0xd000-0xdfff]\r\n[    0.088754] pci 0000:00:1c.2:   bridge window [mem 0xf7100000-0xf71fffff]\r\n[    0.088761] pci_bus 0000:00: resource 4 [io  0x0000-0x0cf7 window]\r\n[    0.088762] pci_bus 0000:00: resource 5 [io  0x0d00-0xffff window]\r\n[    0.088763] pci_bus 0000:00: resource 6 [mem 0x000a0000-0x000bffff window]\r\n[    0.088764] pci_bus 0000:00: resource 7 [mem 0x000d0000-0x000d3fff window]\r\n[    0.088765] pci_bus 0000:00: resource 8 [mem 0x000d4000-0x000d7fff window]\r\n[    0.088765] pci_bus 0000:00: resource 9 [mem 0x000d8000-0x000dbfff window]\r\n[    0.088766] pci_bus 0000:00: resource 10 [mem 0x000dc000-0x000dffff window]\r\n[    0.088767] pci_bus 0000:00: resource 11 [mem 0xe0000000-0xfeafffff window]\r\n[    0.088768] pci_bus 0000:01: resource 0 [io  0xe000-0xefff]\r\n[    0.088769] pci_bus 0000:01: resource 1 [mem 0xf6000000-0xf70fffff]\r\n[    0.088770] pci_bus 0000:01: resource 2 [mem 0xe0000000-0xf1ffffff 64bit pref]\r\n[    0.088771] pci_bus 0000:03: resource 0 [io  0xd000-0xdfff]\r\n[    0.088772] pci_bus 0000:03: resource 1 [mem 0xf7100000-0xf71fffff]\r\n[    0.088840] NET: Registered protocol family 2\r\n[    0.088928] tcp_listen_portaddr_hash hash table entries: 16384 (order: 6, 262144 bytes)\r\n[    0.088991] TCP established hash table entries: 262144 (order: 9, 2097152 bytes)\r\n[    0.089201] TCP bind hash table entries: 65536 (order: 8, 1048576 bytes)\r\n[    0.089297] TCP: Hash tables configured (established 262144 bind 65536)\r\n[    0.089327] UDP hash table entries: 16384 (order: 7, 524288 bytes)\r\n[    0.089388] UDP-Lite hash table entries: 16384 (order: 7, 524288 bytes)\r\n[    0.089464] NET: Registered protocol family 1\r\n[    0.089810] pci 0000:01:00.0: Video device with shadowed ROM at [mem 0x000c0000-0x000dffff]\r\n[    0.089813] pci 0000:01:00.1: Linked as a consumer to 0000:01:00.0\r\n[    0.089828] PCI: CLS 64 bytes, default 64\r\n[    0.089873] Unpacking initramfs...\r\n[    0.423302] Freeing initrd memory: 32652K\r\n[    0.423353] PCI-DMA: Using software bounce buffering for IO (SWIOTLB)\r\n[    0.423355] software IO TLB [mem 0xc8359000-0xcc359000] (64MB) mapped at [(____ptrval____)-(____ptrval____)]\r\n[    0.423836] Initialise system trusted keyrings\r\n[    0.423874] workingset: timestamp_bits=40 max_order=23 bucket_order=0\r\n[    0.424622] zbud: loaded\r\n[    0.424794] pstore: using deflate compression\r\n[    0.631993] Key type asymmetric registered\r\n[    0.631994] Asymmetric key parser 'x509' registered\r\n[    0.632017] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 248)\r\n[    0.632040] io scheduler noop registered\r\n[    0.632041] io scheduler deadline registered\r\n[    0.632064] io scheduler cfq registered (default)\r\n[    0.632065] io scheduler mq-deadline registered\r\n[    0.632417] shpchp: Standard Hot Plug PCI Controller Driver version: 0.4\r\n[    0.632427] efifb: probing for efifb\r\n[    0.632444] efifb: framebuffer at 0xf1000000, using 8128k, total 8128k\r\n[    0.632445] efifb: mode is 1920x1080x32, linelength=7680, pages=1\r\n[    0.632445] efifb: scrolling: redraw\r\n[    0.632446] efifb: Truecolor: size=8:8:8:8, shift=24:16:8:0\r\n[    0.635342] Console: switching to colour frame buffer device 240x67\r\n[    0.638159] fb0: EFI VGA frame buffer device\r\n[    0.638165] intel_idle: MWAIT substates: 0x42120\r\n[    0.638165] intel_idle: v0.4.1 model 0x3C\r\n[    0.638376] intel_idle: lapic_timer_reliable_states 0xffffffff\r\n[    0.638641] Serial: 8250/16550 driver, 4 ports, IRQ sharing enabled\r\n[    0.638962] Linux agpgart interface v0.103\r\n[    0.638987] AMD IOMMUv2 driver by Joerg Roedel <jroedel@suse.de>\r\n[    0.638988] AMD IOMMUv2 functionality not available on this system\r\n[    0.639306] i8042: PNP: No PS/2 controller found.\r\n[    0.639335] mousedev: PS/2 mouse device common for all mice\r\n[    0.639359] rtc_cmos 00:02: RTC can wake from S4\r\n[    0.639479] rtc_cmos 00:02: registered as rtc0\r\n[    0.639488] rtc_cmos 00:02: alarms up to one month, y3k, 242 bytes nvram, hpet irqs\r\n[    0.639494] intel_pstate: Intel P-state driver initializing\r\n[    0.639703] ledtrig-cpu: registered to indicate activity on CPUs\r\n[    0.640668] NET: Registered protocol family 10\r\n[    0.651512] Segment Routing with IPv6\r\n[    0.651560] mip6: Mobile IPv6\r\n[    0.651564] NET: Registered protocol family 17\r\n[    0.651572] mpls_gso: MPLS GSO support\r\n[    0.652827] microcode: sig=0x306c3, pf=0x2, revision=0x25\r\n[    0.653223] microcode: Microcode Update Driver: v2.2.\r\n[    0.653251] sched_clock: Marking stable (653201523, 0)->(640807103, 12394420)\r\n[    0.653842] registered taskstats version 1\r\n[    0.653843] Loading compiled-in X.509 certificates\r\n[    0.684099] Loaded X.509 cert 'secure-boot-test-key-lfaraone: 97c1b25cddf9873ca78a58f3d73bf727d2cf78ff'\r\n[    0.684112] zswap: loaded using pool lzo/zbud\r\n[    0.684184] AppArmor: AppArmor sha1 policy hashing enabled\r\n[    0.684476] rtc_cmos 00:02: setting system clock to 2018-11-23 19:12:07 UTC (1543000327)\r\n[    0.836255] Freeing unused kernel image memory: 1548K\r\n[    0.856087] Write protecting the kernel read-only data: 16384k\r\n[    0.857237] Freeing unused kernel image memory: 2028K\r\n[    0.857416] Freeing unused kernel image memory: 836K\r\n[    0.865306] x86/mm: Checked W+X mappings: passed, no W+X pages found.\r\n[    0.865306] x86/mm: Checking user space page tables\r\n[    0.872796] x86/mm: Checked W+X mappings: passed, no W+X pages found.\r\n[    0.924460] thermal LNXTHERM:00: registered as thermal_zone0\r\n[    0.924462] ACPI: Thermal Zone [TZ00] (28 C)\r\n[    0.924671] thermal LNXTHERM:01: registered as thermal_zone1\r\n[    0.924671] ACPI: Thermal Zone [TZ01] (30 C)\r\n[    0.924842] r8169 Gigabit Ethernet driver 2.3LK-NAPI loaded\r\n[    0.924846] r8169 0000:03:00.0: can't disable ASPM; OS doesn't have ASPM control\r\n[    0.926564] ACPI: bus type USB registered\r\n[    0.926574] usbcore: registered new interface driver usbfs\r\n[    0.926578] usbcore: registered new interface driver hub\r\n[    0.926596] usbcore: registered new device driver usb\r\n[    0.927559] SCSI subsystem initialized\r\n[    0.927821] ehci_hcd: USB 2.0 'Enhanced' Host Controller (EHCI) Driver\r\n[    0.929056] ehci-pci: EHCI PCI platform driver\r\n[    0.929183] ehci-pci 0000:00:1a.0: EHCI Host Controller\r\n[    0.929188] ehci-pci 0000:00:1a.0: new USB bus registered, assigned bus number 1\r\n[    0.929203] ehci-pci 0000:00:1a.0: debug port 2\r\n[    0.930882] i801_smbus 0000:00:1f.3: SPD Write Disable is set\r\n[    0.930914] i801_smbus 0000:00:1f.3: SMBus using PCI interrupt\r\n[    0.930927] cryptd: max_cpu_qlen set to 1000\r\n[    0.931722] libata version 3.00 loaded.\r\n[    0.932166] AVX2 version of gcm_enc/dec engaged.\r\n[    0.932166] AES CTR mode by8 optimization enabled\r\n[    0.932749] ahci 0000:00:1f.2: version 3.0\r\n[    0.932929] ahci 0000:00:1f.2: AHCI 0001.0300 32 slots 6 ports 6 Gbps 0x7 impl SATA mode\r\n[    0.932931] ahci 0000:00:1f.2: flags: 64bit ncq pm led clo pio slum part ems apst \r\n[    0.933109] ehci-pci 0000:00:1a.0: cache line size of 64 is not supported\r\n[    0.933124] ehci-pci 0000:00:1a.0: irq 16, io mem 0xf7218000\r\n[    0.936959] r8169 0000:03:00.0 eth0: RTL8168g/8111g, 9c:b6:54:f8:b5:1e, XID 4c000800, IRQ 25\r\n[    0.936960] r8169 0000:03:00.0 eth0: jumbo features [frames: 9200 bytes, tx checksumming: ko]\r\n[    0.944058] r8169 0000:03:00.0 eno1: renamed from eth0\r\n[    0.952032] ehci-pci 0000:00:1a.0: USB 2.0 started, EHCI 1.00\r\n[    0.952205] usb usb1: New USB device found, idVendor=1d6b, idProduct=0002, bcdDevice= 4.18\r\n[    0.952207] usb usb1: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.952208] usb usb1: Product: EHCI Host Controller\r\n[    0.952208] usb usb1: Manufacturer: Linux 4.18.0-2-amd64 ehci_hcd\r\n[    0.952209] usb usb1: SerialNumber: 0000:00:1a.0\r\n[    0.952347] hub 1-0:1.0: USB hub found\r\n[    0.952357] hub 1-0:1.0: 2 ports detected\r\n[    0.952520] xhci_hcd 0000:00:14.0: xHCI Host Controller\r\n[    0.952526] xhci_hcd 0000:00:14.0: new USB bus registered, assigned bus number 2\r\n[    0.953589] xhci_hcd 0000:00:14.0: hcc params 0x200077c1 hci version 0x100 quirks 0x0000000000009810\r\n[    0.953593] xhci_hcd 0000:00:14.0: cache line size of 64 is not supported\r\n[    0.953920] usb usb2: New USB device found, idVendor=1d6b, idProduct=0002, bcdDevice= 4.18\r\n[    0.953921] usb usb2: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.953922] usb usb2: Product: xHCI Host Controller\r\n[    0.953923] usb usb2: Manufacturer: Linux 4.18.0-2-amd64 xhci-hcd\r\n[    0.953924] usb usb2: SerialNumber: 0000:00:14.0\r\n[    0.954018] hub 2-0:1.0: USB hub found\r\n[    0.954041] hub 2-0:1.0: 14 ports detected\r\n[    0.955457] ehci-pci 0000:00:1d.0: EHCI Host Controller\r\n[    0.955461] ehci-pci 0000:00:1d.0: new USB bus registered, assigned bus number 3\r\n[    0.955465] xhci_hcd 0000:00:14.0: xHCI Host Controller\r\n[    0.955467] xhci_hcd 0000:00:14.0: new USB bus registered, assigned bus number 4\r\n[    0.955469] xhci_hcd 0000:00:14.0: Host supports USB 3.0  SuperSpeed\r\n[    0.955470] ehci-pci 0000:00:1d.0: debug port 2\r\n[    0.955509] usb usb4: New USB device found, idVendor=1d6b, idProduct=0003, bcdDevice= 4.18\r\n[    0.955510] usb usb4: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.955510] usb usb4: Product: xHCI Host Controller\r\n[    0.955511] usb usb4: Manufacturer: Linux 4.18.0-2-amd64 xhci-hcd\r\n[    0.955512] usb usb4: SerialNumber: 0000:00:14.0\r\n[    0.955584] hub 4-0:1.0: USB hub found\r\n[    0.955599] hub 4-0:1.0: 6 ports detected\r\n[    0.956398] scsi host0: ahci\r\n[    0.956494] scsi host1: ahci\r\n[    0.956569] scsi host2: ahci\r\n[    0.956643] scsi host3: ahci\r\n[    0.956710] scsi host4: ahci\r\n[    0.956774] scsi host5: ahci\r\n[    0.956816] ata1: SATA max UDMA/133 abar m2048@0xf7216000 port 0xf7216100 irq 24\r\n[    0.956817] ata2: SATA max UDMA/133 abar m2048@0xf7216000 port 0xf7216180 irq 24\r\n[    0.956820] ata3: SATA max UDMA/133 abar m2048@0xf7216000 port 0xf7216200 irq 24\r\n[    0.956820] ata4: DUMMY\r\n[    0.956821] ata5: DUMMY\r\n[    0.956821] ata6: DUMMY\r\n[    0.959380] ehci-pci 0000:00:1d.0: cache line size of 64 is not supported\r\n[    0.959388] ehci-pci 0000:00:1d.0: irq 23, io mem 0xf7217000\r\n[    0.972172] ehci-pci 0000:00:1d.0: USB 2.0 started, EHCI 1.00\r\n[    0.972240] usb usb3: New USB device found, idVendor=1d6b, idProduct=0002, bcdDevice= 4.18\r\n[    0.972242] usb usb3: New USB device strings: Mfr=3, Product=2, SerialNumber=1\r\n[    0.972243] usb usb3: Product: EHCI Host Controller\r\n[    0.972244] usb usb3: Manufacturer: Linux 4.18.0-2-amd64 ehci_hcd\r\n[    0.972246] usb usb3: SerialNumber: 0000:00:1d.0\r\n[    0.972469] hub 3-0:1.0: USB hub found\r\n[    0.972475] hub 3-0:1.0: 2 ports detected\r\n[    1.275547] ata2: SATA link up 3.0 Gbps (SStatus 123 SControl 300)\r\n[    1.275587] ata1: SATA link up 6.0 Gbps (SStatus 133 SControl 300)\r\n[    1.275624] ata3: SATA link up 1.5 Gbps (SStatus 113 SControl 300)\r\n[    1.277183] ata2.00: ATA-8: Hitachi HDS721010CLA332, JP4OA3GC, max UDMA/133\r\n[    1.277189] ata2.00: 1953525168 sectors, multi 16: LBA48 NCQ (depth 32), AA\r\n[    1.277662] ata3.00: ATAPI: hp       BDDVDRW CH30L, BC56, max UDMA/100\r\n[    1.277682] ata1.00: supports DRM functions and may not be fully accessible\r\n[    1.278813] ata1.00: disabling queued TRIM support\r\n[    1.278818] ata1.00: ATA-9: Samsung SSD 850 EVO 500GB, EMT02B6Q, max UDMA/133\r\n[    1.278823] ata1.00: 976773168 sectors, multi 1: LBA48 NCQ (depth 32), AA\r\n[    1.279021] ata2.00: configured for UDMA/133\r\n[    1.280576] ata3.00: configured for UDMA/100\r\n[    1.281605] ata1.00: supports DRM functions and may not be fully accessible\r\n[    1.282539] ata1.00: disabling queued TRIM support\r\n[    1.284517] ata1.00: configured for UDMA/133\r\n[    1.284864] scsi 0:0:0:0: Direct-Access     ATA      Samsung SSD 850  2B6Q PQ: 0 ANSI: 5\r\n[    1.285581] scsi 1:0:0:0: Direct-Access     ATA      Hitachi HDS72101 A3GC PQ: 0 ANSI: 5\r\n[    1.288062] usb 2-3: new full-speed USB device number 2 using xhci_hcd\r\n[    1.288067] usb 1-1: new high-speed USB device number 2 using ehci-pci\r\n[    1.289698] scsi 2:0:0:0: CD-ROM            hp       BDDVDRW CH30L    BC56 PQ: 0 ANSI: 5\r\n[    1.308121] usb 3-1: new high-speed USB device number 2 using ehci-pci\r\n[    1.329042] sd 0:0:0:0: [sda] 976773168 512-byte logical blocks: (500 GB/466 GiB)\r\n[    1.329052] sd 0:0:0:0: [sda] Write Protect is off\r\n[    1.329056] sd 0:0:0:0: [sda] Mode Sense: 00 3a 00 00\r\n[    1.329068] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\r\n[    1.329105] sd 1:0:0:0: [sdb] 1953525168 512-byte logical blocks: (1.00 TB/932 GiB)\r\n[    1.329111] sd 1:0:0:0: [sdb] Write Protect is off\r\n[    1.329113] sd 1:0:0:0: [sdb] Mode Sense: 00 3a 00 00\r\n[    1.329122] sd 1:0:0:0: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\r\n[    1.330420]  sda: sda1 sda2 sda3 sda4\r\n[    1.330876] sd 0:0:0:0: [sda] supports TCG Opal\r\n[    1.330877] sd 0:0:0:0: [sda] Attached SCSI disk\r\n[    1.335163]  sdb: sdb1\r\n[    1.335301] sd 1:0:0:0: [sdb] Attached SCSI disk\r\n[    1.372180] sr 2:0:0:0: [sr0] scsi3-mmc drive: 1x/1x writer dvd-ram cd/rw xa/form2 cdda tray\r\n[    1.372183] cdrom: Uniform CD-ROM driver Revision: 3.20\r\n[    1.372441] sr 2:0:0:0: Attached scsi CD-ROM sr0\r\n[    1.438254] usb 2-3: New USB device found, idVendor=062a, idProduct=4102, bcdDevice= 3.12\r\n[    1.438258] usb 2-3: New USB device strings: Mfr=1, Product=2, SerialNumber=0\r\n[    1.438261] usb 2-3: Product: 2.4G Wireless Mouse\r\n[    1.438263] usb 2-3: Manufacturer: MOSART Semi.\r\n[    1.440037] tsc: Refined TSC clocksource calibration: 3491.910 MHz\r\n[    1.440048] clocksource: tsc: mask: 0xffffffffffffffff max_cycles: 0x32557694fed, max_idle_ns: 440795306138 ns\r\n[    1.440078] clocksource: Switched to clocksource tsc\r\n[    1.452758] usb 1-1: New USB device found, idVendor=8087, idProduct=8008, bcdDevice= 0.05\r\n[    1.452763] usb 1-1: New USB device strings: Mfr=0, Product=0, SerialNumber=0\r\n[    1.453280] hub 1-1:1.0: USB hub found\r\n[    1.453413] hub 1-1:1.0: 6 ports detected\r\n[    1.464918] usb 3-1: New USB device found, idVendor=8087, idProduct=8000, bcdDevice= 0.05\r\n[    1.464922] usb 3-1: New USB device strings: Mfr=0, Product=0, SerialNumber=0\r\n[    1.465367] hub 3-1:1.0: USB hub found\r\n[    1.465555] hub 3-1:1.0: 8 ports detected\r\n[    1.564178] usb 4-3: new SuperSpeed Gen 1 USB device number 2 using xhci_hcd\r\n[    1.585450] usb 4-3: New USB device found, idVendor=0bc2, idProduct=a013, bcdDevice= 1.00\r\n[    1.585454] usb 4-3: New USB device strings: Mfr=2, Product=3, SerialNumber=1\r\n[    1.585457] usb 4-3: Product: Backup+ SL\r\n[    1.585460] usb 4-3: Manufacturer: Seagate\r\n[    1.585462] usb 4-3: SerialNumber: NA5C4Y3K\r\n[    1.712126] usb 2-4: new full-speed USB device number 3 using xhci_hcd\r\n[    1.861347] usb 2-4: New USB device found, idVendor=0d8c, idProduct=000c, bcdDevice= 1.00\r\n[    1.861353] usb 2-4: New USB device strings: Mfr=0, Product=1, SerialNumber=0\r\n[    1.861357] usb 2-4: Product: C-Media USB Headphone Set  \r\n[    1.862478] random: fast init done\r\n[    1.988400] usb 4-5: new SuperSpeed Gen 1 USB device number 3 using xhci_hcd\r\n[    2.248569] usb 4-5: New USB device found, idVendor=2109, idProduct=0812, bcdDevice=90.81\r\n[    2.248575] usb 4-5: New USB device strings: Mfr=1, Product=2, SerialNumber=0\r\n[    2.248579] usb 4-5: Product: USB3.0 Hub             \r\n[    2.248582] usb 4-5: Manufacturer: VIA Labs, Inc.         \r\n[    2.252127] hub 4-5:1.0: USB hub found\r\n[    2.253009] hub 4-5:1.0: 4 ports detected\r\n[    2.263816] usbcore: registered new interface driver usb-storage\r\n[    2.270518] scsi host6: uas\r\n[    2.270585] usbcore: registered new interface driver uas\r\n[    2.271877] scsi 6:0:0:0: Direct-Access     Seagate  Backup+ SL       0412 PQ: 0 ANSI: 6\r\n[    2.272650] sd 6:0:0:0: [sdc] 1953525167 512-byte logical blocks: (1.00 TB/932 GiB)\r\n[    2.272652] sd 6:0:0:0: [sdc] 4096-byte physical blocks\r\n[    2.272724] sd 6:0:0:0: [sdc] Write Protect is off\r\n[    2.272725] sd 6:0:0:0: [sdc] Mode Sense: 4f 00 00 00\r\n[    2.272889] sd 6:0:0:0: [sdc] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\r\n[    2.274405]  sdc: sdc1\r\n[    2.275218] sd 6:0:0:0: [sdc] Attached SCSI disk\r\n[    2.372104] usb 2-9: new high-speed USB device number 4 using xhci_hcd\r\n[    2.523293] usb 2-9: New USB device found, idVendor=2109, idProduct=2812, bcdDevice=90.80\r\n[    2.523296] usb 2-9: New USB device strings: Mfr=1, Product=2, SerialNumber=0\r\n[    2.523297] usb 2-9: Product: USB2.0 Hub             \r\n[    2.523299] usb 2-9: Manufacturer: VIA Labs, Inc.         \r\n[    2.524461] hub 2-9:1.0: USB hub found\r\n[    2.524811] hub 2-9:1.0: 4 ports detected\r\n[    2.660102] usb 2-11: new high-speed USB device number 5 using xhci_hcd\r\n[    2.817110] usb 2-11: New USB device found, idVendor=0bda, idProduct=0184, bcdDevice=84.13\r\n[    2.817114] usb 2-11: New USB device strings: Mfr=1, Product=2, SerialNumber=3\r\n[    2.817117] usb 2-11: Product: USB2.0-CRW\r\n[    2.817119] usb 2-11: Manufacturer: Generic\r\n[    2.817122] usb 2-11: SerialNumber: 20100818841300000\r\n[    2.823154] usb-storage 2-11:1.0: USB Mass Storage device detected\r\n[    2.823620] scsi host7: usb-storage 2-11:1.0\r\n[    2.900126] usb 2-9.1: new full-speed USB device number 6 using xhci_hcd\r\n[    3.030918] usb 2-9.1: New USB device found, idVendor=0a12, idProduct=0001, bcdDevice=88.91\r\n[    3.030923] usb 2-9.1: New USB device strings: Mfr=0, Product=2, SerialNumber=0\r\n[    3.030926] usb 2-9.1: Product: CSR8510 A10\r\n[    3.152125] usb 2-14: new high-speed USB device number 7 using xhci_hcd\r\n[    3.302384] usb 2-14: New USB device found, idVendor=7392, idProduct=7811, bcdDevice= 2.00\r\n[    3.302390] usb 2-14: New USB device strings: Mfr=1, Product=2, SerialNumber=3\r\n[    3.302393] usb 2-14: Product: 802.11n WLAN Adapter\r\n[    3.302396] usb 2-14: Manufacturer: Realtek\r\n[    3.302399] usb 2-14: SerialNumber: 00e04c000001\r\n[    3.312360] hidraw: raw HID events driver (C) Jiri Kosina\r\n[    3.315785] usbcore: registered new interface driver usbhid\r\n[    3.315786] usbhid: USB HID core driver\r\n[    3.317407] input: MOSART Semi. 2.4G Wireless Mouse Mouse as /devices/pci0000:00/0000:00:14.0/usb2/2-3/2-3:1.0/0003:062A:4102.0001/input/input0\r\n[    3.317444] input: MOSART Semi. 2.4G Wireless Mouse as /devices/pci0000:00/0000:00:14.0/usb2/2-3/2-3:1.0/0003:062A:4102.0001/input/input1\r\n[    3.317480] hid-generic 0003:062A:4102.0001: input,hiddev0,hidraw0: USB HID v1.10 Mouse [MOSART Semi. 2.4G Wireless Mouse] on usb-0000:00:14.0-3/input0\r\n[    3.317537] input: C-Media USB Headphone Set   as /devices/pci0000:00/0000:00:14.0/usb2/2-4/2-4:1.3/0003:0D8C:000C.0002/input/input2\r\n[    3.376246] hid-generic 0003:0D8C:000C.0002: input,hidraw1: USB HID v1.00 Device [C-Media USB Headphone Set  ] on usb-0000:00:14.0-4/input3\r\n[    3.392109] usb 2-9.2: new full-speed USB device number 8 using xhci_hcd\r\n[    3.507942] usb 2-9.2: New USB device found, idVendor=0d8c, idProduct=0005, bcdDevice= 1.00\r\n[    3.507946] usb 2-9.2: New USB device strings: Mfr=1, Product=2, SerialNumber=3\r\n[    3.507948] usb 2-9.2: Product: Blue Snowball\r\n[    3.507951] usb 2-9.2: Manufacturer: BLUE MICROPHONE\r\n[    3.507953] usb 2-9.2: SerialNumber: 201306\r\n[    3.513289] hid-generic 0003:0D8C:0005.0003: No inputs registered, leaving\r\n[    3.513531] hid-generic 0003:0D8C:0005.0003: hidraw2: USB HID v1.00 Device [BLUE MICROPHONE Blue Snowball] on usb-0000:00:14.0-9.2/input2\r\n[    3.596005] raid6: sse2x1   gen() 13897 MB/s\r\n[    3.664002] raid6: sse2x1   xor() 10213 MB/s\r\n[    3.732006] raid6: sse2x2   gen() 17070 MB/s\r\n[    3.800003] raid6: sse2x2   xor() 11149 MB/s\r\n[    3.842179] scsi 7:0:0:0: Direct-Access     Generic- Compact Flash    1.00 PQ: 0 ANSI: 0 CCS\r\n[    3.844055] scsi 7:0:0:1: Direct-Access     Generic- SM/xD-Picture    1.00 PQ: 0 ANSI: 0 CCS\r\n[    3.846085] scsi 7:0:0:2: Direct-Access     Generic- SD/MMC           1.00 PQ: 0 ANSI: 0 CCS\r\n[    3.848131] scsi 7:0:0:3: Direct-Access     Generic- M.S./M.S.Pro/HG  1.00 PQ: 0 ANSI: 0 CCS\r\n[    3.868006] raid6: sse2x4   gen() 20216 MB/s\r\n[    3.936005] raid6: sse2x4   xor() 12497 MB/s\r\n[    4.004003] raid6: avx2x1   gen() 28060 MB/s\r\n[    4.072003] raid6: avx2x1   xor() 19240 MB/s\r\n[    4.140002] raid6: avx2x2   gen() 32481 MB/s\r\n[    4.208003] raid6: avx2x2   xor() 20033 MB/s\r\n[    4.276003] raid6: avx2x4   gen() 37126 MB/s\r\n[    4.344003] raid6: avx2x4   xor() 23060 MB/s\r\n[    4.344004] raid6: using algorithm avx2x4 gen() 37126 MB/s\r\n[    4.344004] raid6: .... xor() 23060 MB/s, rmw enabled\r\n[    4.344005] raid6: using avx2x2 recovery algorithm\r\n[    4.345601] xor: automatically using best checksumming function   avx       \r\n[    4.359495] Btrfs loaded, crc32c=crc32c-intel\r\n[    4.376025] sd 7:0:0:0: [sdd] Attached SCSI removable disk\r\n[    4.389674] sd 7:0:0:3: [sdg] Attached SCSI removable disk\r\n[    4.393694] sd 7:0:0:2: [sdf] Attached SCSI removable disk\r\n[    4.399463] sd 7:0:0:1: [sde] Attached SCSI removable disk\r\n[    4.579850] PM: Image not found (code -22)\r\n[    4.696238] EXT4-fs (sda2): mounted filesystem with ordered data mode. Opts: (null)\r\n[    4.810800] systemd[1]: systemd 239 running in system mode. (+PAM +AUDIT +SELINUX +IMA +APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +SECCOMP +BLKID +ELFUTILS +KMOD -IDN2 +IDN -PCRE2 default-hierarchy=hybrid)\r\n[    4.828528] systemd[1]: Detected architecture x86-64.\r\n[    4.831450] systemd[1]: Set hostname to <Debian>.\r\n[    4.905998] random: systemd: uninitialized urandom read (16 bytes read)\r\n[    4.906086] systemd[1]: Listening on Journal Socket.\r\n[    4.906132] random: systemd: uninitialized urandom read (16 bytes read)\r\n[    4.907314] systemd[1]: Created slice system-getty.slice.\r\n[    4.907324] random: systemd: uninitialized urandom read (16 bytes read)\r\n[    4.907330] systemd[1]: Reached target Remote File Systems.\r\n[    4.907825] systemd[1]: Mounting Kernel Debug File System...\r\n[    4.908364] systemd[1]: Starting Set the console keyboard layout...\r\n[    4.908491] systemd[1]: Listening on fsck to fsckd communication Socket.\r\n[    4.909036] systemd[1]: Mounting Huge Pages File System...\r\n[    4.921664] ipmi message handler version 39.2\r\n[    4.922473] EXT4-fs (sda2): re-mounted. Opts: errors=remount-ro\r\n[    4.923296] ipmi device interface\r\n[    4.965528] nvidia: loading out-of-tree module taints kernel.\r\n[    4.965532] nvidia: module license 'NVIDIA' taints kernel.\r\n[    4.965532] Disabling lock debugging due to kernel taint\r\n[    4.974976] nvidia-nvlink: Nvlink Core is being initialized, major device number 245\r\n[    4.975186] nvidia 0000:01:00.0: vgaarb: changed VGA decodes: olddecodes=io+mem,decodes=none:owns=io+mem\r\n[    4.975248] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  390.87  Tue Aug 21 12:33:05 PDT 2018 (using threaded interrupts)\r\n[    4.983264] loop: module loaded\r\n[    4.988782] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms  390.87  Tue Aug 21 16:16:14 PDT 2018\r\n[    5.013126] [drm] [nvidia-drm] [GPU ID 0x00000100] Loading driver\r\n[    5.013128] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:01:00.0 on minor 0\r\n[    5.019605] systemd-journald[357]: Received request to flush runtime journal from PID 1\r\n[    5.026492] squashfs: version 4.0 (2009/01/31) Phillip Lougher\r\n[    5.029809] input: Power Button as /devices/LNXSYSTM:00/LNXSYBUS:00/PNP0C0C:00/input/input4\r\n[    5.029824] ACPI: Power Button [PWRB]\r\n[    5.029890] input: Power Button as /devices/LNXSYSTM:00/LNXPWRBN:00/input/input5\r\n[    5.029900] ACPI: Power Button [PWRF]\r\n[    5.050539] iTCO_vendor_support: vendor-support=0\r\n[    5.051447] iTCO_wdt: Intel TCO WatchDog Timer Driver v1.11\r\n[    5.051478] iTCO_wdt: Found a Lynx Point TCO device (Version=2, TCOBASE=0x1860)\r\n[    5.052086] iTCO_wdt: initialized. heartbeat=30 sec (nowayout=0)\r\n[    5.056594] sd 0:0:0:0: Attached scsi generic sg0 type 0\r\n[    5.056628] sd 1:0:0:0: Attached scsi generic sg1 type 0\r\n[    5.056654] sr 2:0:0:0: Attached scsi generic sg2 type 5\r\n[    5.056679] sd 6:0:0:0: Attached scsi generic sg3 type 0\r\n[    5.056703] sd 7:0:0:0: Attached scsi generic sg4 type 0\r\n[    5.056728] sd 7:0:0:1: Attached scsi generic sg5 type 0\r\n[    5.056752] sd 7:0:0:2: Attached scsi generic sg6 type 0\r\n[    5.056777] sd 7:0:0:3: Attached scsi generic sg7 type 0\r\n[    5.080279] EFI Variables Facility v0.08 2004-May-17\r\n[    5.080450] input: PC Speaker as /devices/platform/pcspkr/input/input6\r\n[    5.087197] RAPL PMU: API unit is 2^-32 Joules, 4 fixed counters, 655360 ms ovfl timer\r\n[    5.087198] RAPL PMU: hw unit of domain pp0-core 2^-14 Joules\r\n[    5.087199] RAPL PMU: hw unit of domain package 2^-14 Joules\r\n[    5.087199] RAPL PMU: hw unit of domain dram 2^-14 Joules\r\n[    5.087200] RAPL PMU: hw unit of domain pp1-gpu 2^-14 Joules\r\n[    5.096720] pstore: Registered efi as persistent store backend\r\n[    5.100816] snd_hda_intel 0000:00:1b.0: enabling device (0100 -> 0102)\r\n[    5.101046] snd_hda_intel 0000:01:00.1: Disabling MSI\r\n[    5.101050] snd_hda_intel 0000:01:00.1: Handle vga_switcheroo audio client\r\n[    5.114066] usbcore: registered new interface driver snd-usb-audio\r\n[    5.129339] Bluetooth: Core ver 2.22\r\n[    5.129350] NET: Registered protocol family 31\r\n[    5.129351] Bluetooth: HCI device and connection manager initialized\r\n[    5.129354] Bluetooth: HCI socket layer initialized\r\n[    5.129356] Bluetooth: L2CAP socket layer initialized\r\n[    5.129360] Bluetooth: SCO socket layer initialized\r\n[    5.130666] snd_hda_codec_idt hdaudioC0D0: autoconfig for 92HD89E2: line_outs=4 (0xd/0xf/0x10/0x11/0x0) type:speaker\r\n[    5.130668] snd_hda_codec_idt hdaudioC0D0:    speaker_outs=0 (0x0/0x0/0x0/0x0/0x0)\r\n[    5.130669] snd_hda_codec_idt hdaudioC0D0:    hp_outs=0 (0x0/0x0/0x0/0x0/0x0)\r\n[    5.130671] snd_hda_codec_idt hdaudioC0D0:    mono: mono_out=0x0\r\n[    5.130672] snd_hda_codec_idt hdaudioC0D0:    dig-out=0x22/0x0\r\n[    5.130673] snd_hda_codec_idt hdaudioC0D0:    inputs:\r\n[    5.130674] snd_hda_codec_idt hdaudioC0D0:      Mic=0xe\r\n[    5.130675] snd_hda_codec_idt hdaudioC0D0:      Line=0xc\r\n[    5.135307] usbcore: registered new interface driver btusb\r\n[    5.138454] rtl8192cu: Chip version 0x10\r\n[    5.140967] intel_rapl: Found RAPL domain package\r\n[    5.140968] intel_rapl: Found RAPL domain core\r\n[    5.140969] intel_rapl: Found RAPL domain dram\r\n[    5.163328] ACPI Error: Field [D128] at bit offset/length 128/1024 exceeds size of target Buffer (160 bits) (20180531/dsopcode-201)                                                            \r\n[    5.163379] ACPI Error: Method parse/execution failed \\HWMC, AE_AML_BUFFER_LIMIT (20180531/psparse-516)                                                                                        \r\n[    5.163419] ACPI Error: Method parse/execution failed \\_SB.WMID.WMAA, AE_AML_BUFFER_LIMIT (20180531/psparse-516)                                                                               \r\n[    5.163490] ACPI Error: Field [D128] at bit offset/length 128/1024 exceeds size of target Buffer (160 bits) (20180531/dsopcode-201)                                                            \r\n[    5.163531] ACPI Error: Method parse/execution failed \\HWMC, AE_AML_BUFFER_LIMIT (20180531/psparse-516)                                                                                        \r\n[    5.163568] ACPI Error: Method parse/execution failed \\_SB.WMID.WMAA, AE_AML_BUFFER_LIMIT (20180531/psparse-516)                                                                               \r\n[    5.163632] ACPI Error: Field [D128] at bit offset/length 128/1024 exceeds size of target Buffer (160 bits) (20180531/dsopcode-201)                                                            \r\n[    5.163675] ACPI Error: Method parse/execution failed \\HWMC, AE_AML_BUFFER_LIMIT (20180531/psparse-516)                                                                                        \r\n[    5.163712] ACPI Error: Method parse/execution failed \\_SB.WMID.WMAA, AE_AML_BUFFER_LIMIT (20180531/psparse-516)                                                                               \r\n[    5.163789] input: HP WMI hotkeys as /devices/virtual/input/input8\r\n[    5.168517] ACPI Error: Field [D128] at bit offset/length 128/1024 exceeds size of target Buffer (160 bits) (20180531/dsopcode-201)                                                            \r\n[    5.168565] ACPI Error: Method parse/execution failed \\HWMC, AE_AML_BUFFER_LIMIT (20180531/psparse-516)                                                                                        \r\n[    5.168602] ACPI Error: Method parse/execution failed \\_SB.WMID.WMAA, AE_AML_BUFFER_LIMIT (20180531/psparse-516)                                                                               \r\n[    5.168672] ACPI Error: Field [D128] at bit offset/length 128/1024 exceeds size of target Buffer (160 bits) (20180531/dsopcode-201)                                                            \r\n[    5.168713] ACPI Error: Method parse/execution failed \\HWMC, AE_AML_BUFFER_LIMIT (20180531/psparse-516)                                                                                        \r\n[    5.168747] ACPI Error: Method parse/execution failed \\_SB.WMID.WMAA, AE_AML_BUFFER_LIMIT (20180531/psparse-516)                                                                               \r\n[    5.170848] input: HDA Digital PCBeep as /devices/pci0000:00/0000:00:1b.0/sound/card0/input7\r\n[    5.174316] input: HDA Intel PCH Mic as /devices/pci0000:00/0000:00:1b.0/sound/card0/input9\r\n[    5.174362] input: HDA Intel PCH Line as /devices/pci0000:00/0000:00:1b.0/sound/card0/input10\r\n[    5.174403] input: HDA Intel PCH Speaker Front as /devices/pci0000:00/0000:00:1b.0/sound/card0/input11\r\n[    5.174445] input: HDA Intel PCH Speaker Surround as /devices/pci0000:00/0000:00:1b.0/sound/card0/input12\r\n[    5.174484] input: HDA Intel PCH Speaker CLFE as /devices/pci0000:00/0000:00:1b.0/sound/card0/input13\r\n[    5.174529] input: HDA Intel PCH Speaker Side as /devices/pci0000:00/0000:00:1b.0/sound/card0/input14\r\n[    5.200563] rtl8192cu: Board Type 0\r\n[    5.200643] rtl_usb: rx_max_size 15360, rx_urb_num 8, in_ep 1\r\n[    5.200666] rtl8192cu: Loading firmware rtlwifi/rtl8192cufw_TMSC.bin\r\n[    5.201630] usb 2-14: firmware: direct-loading firmware rtlwifi/rtl8192cufw_TMSC.bin\r\n[    5.212909] ieee80211 phy0: Selected rate control algorithm 'rtl_rc'\r\n[    5.213045] usbcore: registered new interface driver rtl8192cu\r\n[    5.346636] rtl8192cu 2-14:1.0 wlx74da38aee630: renamed from wlan0\r\n[    5.806673] Adding 33506300k swap on /dev/sda3.  Priority:-2 extents:1 across:33506300k SSFS\r\n[    5.811858] EXT4-fs (sda4): mounted filesystem with ordered data mode. Opts: (null)\r\n[    5.845613] audit: type=1400 audit(1543000332.655:2): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"libreoffice-senddoc\" pid=804 comm=\"apparmor_parser\"\r\n[    5.845625] audit: type=1400 audit(1543000332.655:3): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/sbin/haveged\" pid=805 comm=\"apparmor_parser\"\r\n[    5.845976] audit: type=1400 audit(1543000332.655:4): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"libreoffice-oopslash\" pid=799 comm=\"apparmor_parser\"\r\n[    5.846172] audit: type=1400 audit(1543000332.655:5): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/bin/man\" pid=798 comm=\"apparmor_parser\"\r\n[    5.846174] audit: type=1400 audit(1543000332.655:6): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"man_filter\" pid=798 comm=\"apparmor_parser\"\r\n[    5.846175] audit: type=1400 audit(1543000332.655:7): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"man_groff\" pid=798 comm=\"apparmor_parser\"\r\n[    5.846204] audit: type=1400 audit(1543000332.655:8): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/snap/core/5897/usr/lib/snapd/snap-confine\" pid=803 comm=\"apparmor_parser\"\r\n[    5.846206] audit: type=1400 audit(1543000332.655:9): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/snap/core/5897/usr/lib/snapd/snap-confine//mount-namespace-capture-helper\" pid=803 comm=\"apparmor_parser\"\r\n[    5.847886] audit: type=1400 audit(1543000332.655:10): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"libreoffice-xpdfimport\" pid=808 comm=\"apparmor_parser\"\r\n[    5.849068] audit: type=1400 audit(1543000332.659:11): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"/usr/lib/snapd/snap-confine\" pid=807 comm=\"apparmor_parser\"\r\n[    5.997203] input: HDA NVidia HDMI/DP,pcm=3 as /devices/pci0000:00/0000:00:01.0/0000:01:00.1/sound/card2/input15\r\n[    5.997341] input: HDA NVidia HDMI/DP,pcm=7 as /devices/pci0000:00/0000:00:01.0/0000:01:00.1/sound/card2/input16\r\n[    5.997472] input: HDA NVidia HDMI/DP,pcm=8 as /devices/pci0000:00/0000:00:01.0/0000:01:00.1/sound/card2/input17\r\n[    5.997598] input: HDA NVidia HDMI/DP,pcm=9 as /devices/pci0000:00/0000:00:01.0/0000:01:00.1/sound/card2/input18\r\n[    6.126615] Bluetooth: BNEP (Ethernet Emulation) ver 1.3\r\n[    6.126616] Bluetooth: BNEP filters: protocol multicast\r\n[    6.126618] Bluetooth: BNEP socket layer initialized\r\n[    6.238589] random: crng init done\r\n[    6.238591] random: 7 urandom warning(s) missed due to ratelimiting\r\n[    6.339093] IPv6: ADDRCONF(NETDEV_UP): eno1: link is not ready\r\n[    6.339513] r8169 0000:03:00.0: firmware: direct-loading firmware rtl_nic/rtl8168g-2.fw\r\n[    6.418215] r8169 0000:03:00.0 eno1: link down\r\n[    6.418286] IPv6: ADDRCONF(NETDEV_UP): eno1: link is not ready\r\n[    6.420044] IPv6: ADDRCONF(NETDEV_UP): wlx74da38aee630: link is not ready\r\n[    6.421506] rtl8192cu: MAC auto ON okay!\r\n[    6.432305] rtl8192cu: Tx queue select: 0x05\r\n[    7.118171] rtl8192c_common: Polling FW ready fail! REG_MCUFWDL:0x00030006.\r\n[    7.118173] rtl8192c_common: Firmware is not ready to run!\r\n[    7.392063] resource sanity check: requesting [mem 0x000c0000-0x000fffff], which spans more than PCI Bus 0000:00 [mem 0x000d0000-0x000d3fff window]\r\n[    7.392183] caller _nv001169rm+0xe3/0x1d0 [nvidia] mapping multiple BARs\r\n[    7.450740] IPv6: ADDRCONF(NETDEV_UP): wlx74da38aee630: link is not ready\r\n[    7.671573] IPv6: ADDRCONF(NETDEV_UP): wlx74da38aee630: link is not ready\r\n[    8.634476] wlx74da38aee630: authenticate with 10:da:43:ae:33:67\r\n[    8.645555] wlx74da38aee630: send auth to 10:da:43:ae:33:67 (try 1/3)\r\n[    8.649661] wlx74da38aee630: authenticated\r\n[    8.652023] wlx74da38aee630: associate with 10:da:43:ae:33:67 (try 1/3)\r\n[    8.674816] wlx74da38aee630: RX AssocResp from 10:da:43:ae:33:67 (capab=0x1411 status=0 aid=2)\r\n[    8.775061] wlx74da38aee630: associated\r\n[    8.791614] IPv6: ADDRCONF(NETDEV_CHANGE): wlx74da38aee630: link becomes ready\r\n[   16.961730] Bluetooth: HIDP (Human Interface Emulation) ver 1.2\r\n[   16.961736] Bluetooth: HIDP socket layer initialized\r\n[   16.962797] hid-generic 0005:046D:B319.0004: unknown main item tag 0x0\r\n[   16.962952] input: Logitech K810 Keyboard as /devices/pci0000:00/0000:00:14.0/usb2/2-9/2-9.1/2-9.1:1.0/bluetooth/hci0/hci0:70/0005:046D:B319.0004/input/input19\r\n[   16.963188] input: Logitech K810 Consumer Control as /devices/pci0000:00/0000:00:14.0/usb2/2-9/2-9.1/2-9.1:1.0/bluetooth/hci0/hci0:70/0005:046D:B319.0004/input/input20\r\n[   16.963266] input: Logitech K810 System Control as /devices/pci0000:00/0000:00:14.0/usb2/2-9/2-9.1/2-9.1:1.0/bluetooth/hci0/hci0:70/0005:046D:B319.0004/input/input21\r\n[   16.963385] hid-generic 0005:046D:B319.0004: input,hidraw3: BLUETOOTH HID v12.02 Keyboard [Logitech K810] on 00:1a:7d:da:71:13\r\n[   24.735884] Bluetooth: RFCOMM TTY layer initialized\r\n[   24.735889] Bluetooth: RFCOMM socket layer initialized\r\n[   24.735893] Bluetooth: RFCOMM ver 1.11\r\n[  279.721759] nvidia-uvm: Loaded the UVM driver in 8 mode, major device number 242\r\n```", "I did not see the usual NV driver initialization routine here, so I suspect a misconfiguration of your NV driver. Debian testing is also not officially supported by NV. \r\n\r\nTypically I would suggest switching to Ubuntu 16.04/18.04, but you could also do a fresh installation of your NV driver. If the issue remains there, please post your installation method and the log.", "I tried \r\n* reinstalling the Nvidia driver with `sudo apt install nvidia-driver --reinstall` \r\n* installing a older version with `sudo apt install nvidia-legacy-390xx-driver`. \r\n\r\nHowever, neither of those solutions worked. \r\n\r\nSince this is likely an issue specific to Debian's build of the 390.x driver series and hopefully will be fixed in a future update, I've decided to implement a quick hack on my end. \r\n\r\nAt boot systemd will run the following bash script as root. This fixes the issue for all users on the system.\r\n```bash\r\n#!/bin/bash\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"\r\nexport CUDA_HOME=/usr/local/cuda\r\npython -c \"import tensorflow as tf; tf.Session()\"\r\n```"]}, {"number": 23934, "title": "transform_graph build  error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version: Github Master version\r\n- Python version:2.7\r\n- Installed using virtualenv? pip? conda?: None. \r\n- Bazel version (if compiling from source): 0.18.1\r\n- GCC/Compiler version (if compiling from source):7.3.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nInstall transform_graph tools from the source using Bazel on my VMware VM Ubuntu 18.04\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ncd ~/tensorflow\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nAnd i got random error such as this:\r\nERROR: /home/zpluo/tensorflow/tensorflow/core/kernels/BUILD:2829:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_square_root_op' failed (Exit 4)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\n\r\nPS:Sometime it's the other file can't be built.", "comments": ["Apologies for the delay in response. Is this still an issue?", "> Apologies for the delay in response. Is this still an issue?\r\n\r\nI also came across this problem just now. \r\n\r\n**System information**\r\n\r\n* OS Platform and Distribution: Linux Ubuntu 18.04\r\n* TensorFlow installed from: Source\r\n* TensorFlow version: Github Master version\r\n* Python version: 2.7\r\n* Installed using virtualenv? pip? conda?: None.\r\n* Bazel version (if compiling from source): 0.22.0\r\n* GCC/Compiler version (if compiling from source): 7.3.0", "Can you please paste the error? Also what are the steps you followed to install TF?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23933, "title": "38c9b12 stucks gdr in ResNet50_v1.5 official benchmark", "body": "With 38c9b12464b23aac132fbc8005cb74de86eee241, grpc+gdr server protocol gets stuck in ResNet50_v1.5 official benchmark. \r\n\r\nThis issue is a marker to other users of GDR. Hopefully I could find some time to fix it next week.", "comments": []}, {"number": 23932, "title": "lite.TFLiteConverter.from_session not supported constant shape placeholder as input", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\nSystem information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.12.0\r\n- Python version:\r\nPython 3.6.5 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen I tried to convert scalar shape placeholder tensor( shape = () ) as an input, converter triggers \"IndexError: list index( shape list idx ) out of range\"\r\n\r\n**Describe the expected behavior**\r\nI think converter should be support scalar shape placeholder, because TF users usually need scalar shape placeholder when they set dynamic params (hyper params, dropout, ect...) in neural network.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n--------------------------------------\r\n import tensorflow as tf\r\n import tensorflow.contrib.lite as lite\r\n\r\n scalar_input = tf.placeholder(shape=(), name='input', dtype=tf.float32)\r\n b = tf.constant(1.0)\r\n val = scalar_input + b\r\n out = tf.identity(val, name=\"out\")\r\n\r\n with tf.Session() as sess:\r\n     converter = lite.TFLiteConverter.from_session(sess, [scalar_input], [out])\r\n     tflite_model = converter.convert()\r\n     open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n---------------------------------------\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is a known limitation currently. You can workaround it by using a 1d vector of 1 element i.e.\r\n```python\r\ntf.placeholder(shape=(1,), name='input', dtype=tf.float32)\r\n```", "@jason9693 Is the suggested workaround work for your scenario?", "@achowdhery Yes. I usually use dropout or dynamic params like this.", "Closing due to inactivity."]}, {"number": 23931, "title": "Flatten Op is nil when accessed via exported graph", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.12.0\r\n- Python version:\r\nPython 3.6.5 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n\r\n**Describe the current behavior**\r\nMy use case is the build the graph using Python interface for TF and export that graph as a pb file. This graph is then imported in Go environment using Go interface for TF. It seems tf.layers.flatten is inaccessible using the name, when name is provided as part of tf.layers.flatten's argument.\r\n\r\nHowever, assigning name to it using tf.identity works. Pl. see code below.\r\n**Describe the expected behavior**\r\n`tf.layers.flatten(my_input, name=\"flatten\")` should allow access to flatten operation\r\nfrom the exported graph using name `flatten` and should not require naming it separately.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nBelow is python code to export the graph:\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\nmy_input = tf.placeholder(dtype=tf.uint8, shape=[1, None, None, 3], name=\"myInput\")\r\n\r\ntf.layers.flatten(my_input, name=\"flatten1\")\r\n\r\nf = tf.layers.flatten(my_input)\r\ntf.identity(f, name=\"flatten2\")\r\n\r\n# finally save the graph to be used in Go code\r\ngraph = tf.Session().graph_def\r\ntf.io.write_graph(graph, \"./\", \"flatten.pb\", as_text=False)\r\n```\r\nBelow is Go code to access flatten operation:\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"log\"\r\n\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n)\r\n\r\nfunc main() {\r\n\tdef, err := ioutil.ReadFile(\"flatten.pb\")\r\n\tif err != nil {\r\n\t\tlog.Fatal(err)\r\n\t}\r\n\r\n\tgraph := tf.NewGraph()\r\n\tif err := graph.Import(def, \"\"); err != nil {\r\n\t\tlog.Fatal(err)\r\n\t}\r\n\r\n\tfmt.Println(\"Access via name=flatten1 fetched a nil?:\", graph.Operation(\"flatten1\") == nil)\r\n\tfmt.Println(\"Access via name=flatten2 fetched a nil?:\", graph.Operation(\"flatten2\") == nil)\r\n}\r\n```\r\nExecuting above produces following output:\r\n```\r\n$ go run flatten.go\r\nAccess via name=flatten1 fetched a nil?: true\r\nAccess via name=flatten2 fetched a nil?: false\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hello @sdeoras , this Stack Overflow post addresses your query:\r\nhttps://stackoverflow.com/questions/46366954/saving-tf-model-trained-with-keras-and-then-evaluated-in-go\r\nPlease see answer by _nessuno_ ; please let us know. Thanks.", "@msymp, thanks for sharing link. i am able to work with it although restructuring the name internally can be bit confusing to users particularly when there are no other nodes already defined with that name. Here is what I get when I explore the above graph. It seems the correct name to use is `flatten1/Shape`.\r\n\r\n```\r\n{\"Name\":\"myInput\",\"Device\":\"\",\"Type\":\"Placeholder\",\"NumInputs\":0,\"NumOutputs\":1}\r\n{\"Name\":\"flatten1/Shape\",\"Device\":\"\",\"Type\":\"Shape\",\"NumInputs\":1,\"NumOutputs\":1}\r\n{\"Name\":\"flatten1/strided_slice/stack\",\"Device\":\"\",\"Type\":\"Const\",\"NumInputs\":0,\"NumOutputs\":1}\r\n{\"Name\":\"flatten1/strided_slice/stack_1\",\"Device\":\"\",\"Type\":\"Const\",\"NumInputs\":0,\"NumOutputs\":1}\r\n{\"Name\":\"flatten1/strided_slice/stack_2\",\"Device\":\"\",\"Type\":\"Const\",\"NumInputs\":0,\"NumOutputs\":1}\r\n{\"Name\":\"flatten1/strided_slice\",\"Device\":\"\",\"Type\":\"StridedSlice\",\"NumInputs\":4,\"NumOutputs\":1}\r\n{\"Name\":\"flatten1/Reshape/shape/1\",\"Device\":\"\",\"Type\":\"Const\",\"NumInputs\":0,\"NumOutputs\":1}\r\n{\"Name\":\"flatten1/Reshape/shape\",\"Device\":\"\",\"Type\":\"Pack\",\"NumInputs\":2,\"NumOutputs\":1}\r\n{\"Name\":\"flatten1/Reshape\",\"Device\":\"\",\"Type\":\"Reshape\",\"NumInputs\":2,\"NumOutputs\":1}\r\n{\"Name\":\"flatten/Shape\",\"Device\":\"\",\"Type\":\"Shape\",\"NumInputs\":1,\"NumOutputs\":1}\r\n{\"Name\":\"flatten/strided_slice/stack\",\"Device\":\"\",\"Type\":\"Const\",\"NumInputs\":0,\"NumOutputs\":1}\r\n{\"Name\":\"flatten/strided_slice/stack_1\",\"Device\":\"\",\"Type\":\"Const\",\"NumInputs\":0,\"NumOutputs\":1}\r\n{\"Name\":\"flatten/strided_slice/stack_2\",\"Device\":\"\",\"Type\":\"Const\",\"NumInputs\":0,\"NumOutputs\":1}\r\n{\"Name\":\"flatten/strided_slice\",\"Device\":\"\",\"Type\":\"StridedSlice\",\"NumInputs\":4,\"NumOutputs\":1}\r\n{\"Name\":\"flatten/Reshape/shape/1\",\"Device\":\"\",\"Type\":\"Const\",\"NumInputs\":0,\"NumOutputs\":1}\r\n{\"Name\":\"flatten/Reshape/shape\",\"Device\":\"\",\"Type\":\"Pack\",\"NumInputs\":2,\"NumOutputs\":1}\r\n{\"Name\":\"flatten/Reshape\",\"Device\":\"\",\"Type\":\"Reshape\",\"NumInputs\":2,\"NumOutputs\":1}\r\n{\"Name\":\"flatten2\",\"Device\":\"\",\"Type\":\"Identity\",\"NumInputs\":1,\"NumOutputs\":1}\r\n```\r\n"]}, {"number": 23930, "title": "Failed to load the native TensorFlow runtime.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n\r\nHi!\r\nI worked with Tensorflow on without GPU. No I'm trying to run it on a device with following specs.\r\n I have get the follow error message, how I can solve it?\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 Home\r\n- Mobile device : ThinkPad X1 Yoga 1st, Intel(R)Core(TM)i7-6500U CPU@2.50GHz 2.59GHz\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.9.0\r\n- Python version: 3.7.0\r\n\r\n**Describe the problem**\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\y_pas\\python\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\y_pas\\python\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\y_pas\\python\\Anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\y_pas\\python\\Anaconda3\\envs\\tensor\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\y_pas\\python\\Anaconda3\\envs\\tensor\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u6307\u5b9a\u3055\u308c\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u304c\u898b\u3064\u304b\u308a\u307e\u305b\u3093\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n", "comments": ["Could you try to install the binary version from pip/conda? Thanks!", "Thanks @byronyi, this issue was solved by a preinstallation anaconda and tensorflow."]}, {"number": 23929, "title": "Minor change in word2vec_basic tutorial", "body": "Initial `count` with tuple instead of list for consistency, add `global data` in method `generate_batch()` to avoid misunderstanding.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it.", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Reviewer @MarkDaoust: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Hi @CastielWong.\r\n\r\nI removed the `global data` in python you can always read variables without a global declaration.\r\n`data_index` only needs the `global` because it's being written to. So I reverted that line.\r\n\r\nThanks for taking the time to make a PR. \r\n", "Ah~ Got it, thanks! @MarkDaoust :)"]}, {"number": 23927, "title": "Fix MutableHashTableOpTest.testMutableHashTableOfTensors. (Issue #23564)", "body": "np.sort sorts the last axis by default, but this axis was already sorted. It should have been sorting the first axis.", "comments": ["@ysuematsu  Any update please ?", "Nagging Reviewer @ysuematsu: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 104 days with no activity and the `awaiting review` label has been applied.", "@MorganR please resolve conflicts.", "@MorganR Gentle reminder to resolve the conflicts.", "@MorganR Did you get a chance to look on conflicts? Please let us know on the update. Thanks !", "This test was moved to tensorflow/python/kernel_tests/lookup_ops_test.py where it has since been fixed. Closing this pull request."]}, {"number": 23926, "title": "Makefile for TFLite under Linux", "body": "\r\n**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow Lite compilation from source\r\n- TensorFlow version 1.12\r\n- g++ 5.4.0\r\n- GPU / CUDA/cuDNN version - not used\r\n\r\nI came across interesting thing in implementation of Makefile for Linux version of TFLite. In particular:\r\nlist of compiled sources does not include directory 'tensorflow/lite/core', only some sub-directories from it. This make the compiled 'libtensorflow-lite.a' binary unusable - linker always gives error even on the simplest examples, as e.g. give on this page - https://www.tensorflow.org/lite/apis\r\n\r\nBy adding line \r\n$(wildcard tensorflow/lite/core/*.cc) \\\r\nI managed to fix this - newly compiled binary now contained all symbols that were previously absent in the binary.\r\n\r\nMy question is - is this made on purpose for some reason that I've missed or this is just a bug? Could someone from the developers give better explanation? Thanks! \r\n", "comments": ["Can you provide the exact makefile path you are looking at?", "Hi! \r\nthis one : https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/make/Makefile", "Yes, that is an oversight, your fix is correct. Thanks!  If you submit a PR that would be awesome!", "Dear Andrew, I'd love to finish the pull-request, but unfortunately Google Bot does no allow me to get past the 'sign the CLA' stage. I've double checked all my e-mails and etc - still cannot find the reason.", "I'm happy to make the one line fix on your behalf since it is so small if\nyou are ok with that.\n\nAlso, please check the troubleshooting if you haven't\nhttps://opensource.google.com/docs/cla/#troubleshoot\n\n-A\n\n\nOn Wed, Nov 28, 2018 at 5:59 AM pglushkov <notifications@github.com> wrote:\n\n> Dear Andrew, I'd love to finish the pull-request, but unfortunately Google\n> Bot does no allow me to get past the 'sign the CLA' stage. I've double\n> checked all my e-mails and etc - still cannot find the reason.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23926#issuecomment-442455838>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAT52nxj69yoqFbH7LiS8HbzG4Q3LRhgks5uzpamgaJpZM4YvuvT>\n> .\n>\n", "Sure, please do, I'm totally ok."]}, {"number": 23925, "title": "libtensorflow_framework.so: undefined symbol: cuDevicePrimaryCtxGetState???", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I have installed tensorflow-gpu with conda successfully.  When I test doing import tensorflow I have the issue mentioned above. Any idea? I have checked my drivers, the nvidia toolkit and cudnn are intalled correctly and set the values of PATH, LD_LIBRARY_PATH and CUDA_HOME respectively. \r\n\r\nI am using cuDNN 7, CUDA 9.2 and my card is Quadro FX 5600 with driver updated to Driver Version: 340.107 CPU Intel(R) Xeon(R) CPU           E5420  @ 2.50GHz", "TensorFlow is tested against CUDA 9.0. If you want to use CUDA 9.2 if you have to build TF from sources yourself. I would recommend you to switch to CUDA 9.0 and give it a try. ", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "Hi, I am having the same issue. The weird thing is I could import tensorflow successfully just yesterday....I am so confused about this...\r\nAnd I am using tensorflow 1.12.0, CUDA 9.0, cuDNN 7.0.5, I am sure I added relative path to my ~/.bashrc file, and everything went alright yesterday\r\nAny help is appreciated, thanks!\r\n\r\n`Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/libtensorflow_framework.so: undefined symbol: cuDevicePrimaryCtxGetState\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/libtensorflow_framework.so: undefined symbol: cuDevicePrimaryCtxGetState\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n`", "Luckily I solved it myself!\r\nI find that my Nvidia Driver was disabled accidentally and I guess that tensorflow-gpu has problem when import without GPU.\r\nSo what I did was:\r\n1. Reinstall NVIDIA Driver\r\n2. Uninstall all the tensorflow in the list (because I had tf 1.12.0 for cpu and tf 1.6.0 for gpu in the same time, but I not sure whether they will conflict or not, I uninstall them all anyways)\r\n3. Reinstall tensorflow-gpu==1.6.0 using pip\r\nHope this method is helpful to you guys :)"]}, {"number": 23924, "title": "Memory leak when using tf.contrib.data.unbatch()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: Python 3.6.7 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 1080ti\r\n\r\n**Describe the current behavior**\r\nMemory usage continuously increase when using `tf.contrib.data.unbatch()`.\r\n\r\n**Describe the expected behavior**\r\nMemory usage should not increase.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nfrom absl import app\r\nfrom absl import flags\r\nfrom absl import logging\r\nimport tensorflow as tf\r\n\r\n\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_integer('epochs', 1000, '')\r\nflags.DEFINE_boolean('use_unbatch', False, '')\r\n\r\n\r\ndef create_dataset(input_holder):\r\n    dataset = tf.data.Dataset.from_tensor_slices((input_holder,))\r\n\r\n    def generate_random_tensor(size):\r\n        return tf.random_uniform([5, size, size], dtype=tf.float32)\r\n\r\n    dataset = dataset.map(generate_random_tensor)\r\n    if FLAGS.use_unbatch:\r\n        dataset = dataset.apply(tf.contrib.data.unbatch())\r\n    else:\r\n        dataset = dataset.flat_map(\r\n            lambda x: tf.data.Dataset.from_tensor_slices((x,)))\r\n        # The output of the dataset becomes a single-element tuple w/o this.\r\n        dataset = dataset.map(lambda x: x)\r\n    return dataset\r\n\r\n\r\ndef main(_):\r\n    with tf.Session() as sess:\r\n        size_holder = tf.placeholder(tf.int32, shape=[None])\r\n        dataset = create_dataset(size_holder)\r\n\r\n        iterator = dataset.make_initializable_iterator()\r\n        get_next = iterator.get_next()\r\n\r\n        for i in range(FLAGS.epochs):\r\n            logging.info('Epoch #%d', i)\r\n            sess.run(iterator.initializer, feed_dict={\r\n                size_holder: [1000 + (i % 100)],\r\n            })\r\n            try:\r\n                while True:\r\n                    array = sess.run(get_next)\r\n                    logging.info('  Generated: %s', array.shape)\r\n            except tf.errors.OutOfRangeError:\r\n                pass\r\n\r\n\r\nif __name__ == '__main__':\r\n    app.run(main)\r\n```\r\n\r\nMemory usage will increase with `--use_unbatch`, while with `--nouse_unbatch`, memory usage does not increase.\r\n\r\n**Other info / logs**\r\n\r\nIt seems like `input_->Unref()` call is missing in [`UnbatchDatasetOp`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/unbatch_dataset_op.cc#L42).", "comments": ["Possibly related to #23904", "Thanks for tracking down the issue: indeed, the missing `input_->Unref()` seems to be the culprit.\r\n\r\n@artsobolev I think the root cause for this bug is different. I'll comment on the other thread."]}, {"number": 23923, "title": "Inconsistant naming in Estimator documentation", "body": "- Doc Link: https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\r\n\r\nIn the section for `export_savedmodel`, it mentioned `export_to_savedmodel` which seems should be `export_savedmodel`. Maybe worth a fix.", "comments": [" Can I take this up?", "This looks fixed to me: https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_saved_model\r\n\r\nPlease reopen or submit a PR if not fixed."]}, {"number": 23922, "title": "Tensorflow uses only one from eight gpus", "body": "Hey,\r\ni have written an neural chatbot with tensorflow. But my problem is, that TF only uses one GPU.\r\nI am training on an NVIDIA DGX Station with 8x Tesla V100, so I want to use alle memory and cores.\r\nWhere is my mistake in the code? Who could I solve my problem?\r\n\r\nI would be very thankful about an answer! :D\r\n\r\n\r\nMy code:\r\n\r\n\r\nimport time\r\n\r\nimport data\r\nimport click\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorlayer as tl\r\n\r\nfrom tqdm import tqdm\r\nfrom sklearn.utils import shuffle\r\nfrom tensorlayer.layers import DenseLayer, EmbeddingInputlayer, Seq2Seq, retrieve_seq_length_op2\r\n\r\nsessionConfig = tf.ConfigProto(allow_soft_placement = True, log_device_placement = False)\r\n\r\n@click.command()\r\n@click.option('-dc', '--data-corpus', default = 'twitter')\r\n@click.option('-bs', '--batch-size', default = 32)\r\n@click.option('-e', '--epochs', default = 100000)\r\n@click.option('-lr', '--learning-rate', default = 0.001)\r\n@click.option('-m', '--mode', is_flag = True)\r\ndef train(data_corpus, batch_size, epochs, learning_rate, mode):\r\n\r\n    metadata, trainX, trainY, testX, testY, validX, validY = setup(data_corpus)\r\n\r\n    srcLength = len(trainX)\r\n    targetLength = len(trainY)\r\n\r\n    assert srcLength == targetLength\r\n\r\n    nStep = srcLength // batch_size\r\n    srcVocabSize = len(metadata['index2Word'])\r\n    embeddingDim = 1024\r\n\r\n    word2Index = metadata['word2Index']  \r\n    index2Word = metadata['index2Word']  \r\n\r\n    unk_id = word2Index['unk']  \r\n    pad_id = word2Index['_']    \r\n\r\n    start_id = srcVocabSize  \r\n    end_id = srcVocabSize + 1 \r\n\r\n    word2Index.update({'start_id': start_id})\r\n    word2Index.update({'end_id': end_id})\r\n    index2Word = index2Word + ['start_id', 'end_id']\r\n\r\n    srcVocabSize = tgt_vocab_size = srcVocabSize + 2\r\n\r\n    target_seqs = tl.prepro.sequences_add_end_id([trainY[100]], end_id=end_id)[0]\r\n    decode_seqs = tl.prepro.sequences_add_start_id([trainY[100]], start_id=start_id, remove_last=False)[0]\r\n    target_mask = tl.prepro.sequences_get_mask([target_seqs])[0]\r\n    if not mode:\r\n        print(\"encode_seqs\", [index2Word[id] for id in trainX[100]])\r\n        print(\"target_seqs\", [index2Word[id] for id in target_seqs])\r\n        print(\"decode_seqs\", [index2Word[id] for id in decode_seqs])\r\n        print(\"target_mask\", target_mask)\r\n        print(len(target_seqs), len(decode_seqs), len(target_mask))\r\n\t\t\r\n    tf.reset_default_graph()\r\n    \r\n    session = tf.Session(config=sessionConfig)\r\n\t\r\n    encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\r\n    decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\r\n    target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\r\n    target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") \r\n        \r\n    outputNetwork, _ = createModel(encode_seqs, decode_seqs, srcVocabSize, embeddingDim, isTrain=True, reuse=False)\r\n    outputNetwork.print_params(False)\r\n        \r\n    encode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\r\n    decode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\r\n        \r\n    network, RNNnetwork = createModel(encode_seqs2, decode_seqs2, srcVocabSize, embeddingDim, isTrain=False, reuse=True)\r\n    y = tf.nn.softmax(network.outputs)\r\n        \r\n    loss = tl.cost.cross_entropy_seq_with_mask(logits=outputNetwork.outputs, target_seqs=target_seqs, input_mask=target_mask, return_details=False, name='cost')\r\n        \r\n    trainOp = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n\r\n    session.run(tf.global_variables_initializer())\r\n\r\n    tl.files.load_and_assign_npz(sess=session, name='model.npz', network=network)\t\r\n\r\n    def inference(seed):\r\n        seed_id = [word2Index.get(w, unk_id) for w in seed.split(\" \")]\r\n        \r\n        state = session.run(RNNnetwork.final_state_encode, {encode_seqs2: [seed_id]})\r\n        o, state = session.run([y, RNNnetwork.final_state_decode], {RNNnetwork.initial_state_decode: state, decode_seqs2: [[start_id]]})\r\n        w_id = tl.nlp.sample_top(o[0], top_k=3)\r\n        w = index2Word[w_id]\r\n        sentence = [w]\r\n        for _ in range(200): # 30 output length\r\n            o, state = session.run([y, RNNnetwork.final_state_decode], {RNNnetwork.initial_state_decode: state, decode_seqs2: [[w_id]]})\r\n            w_id = tl.nlp.sample_top(o[0], top_k=2)\r\n            w = index2Word[w_id]\r\n            if w_id == end_id:\r\n                break\r\n            sentence = sentence + [w]\r\n        return sentence\r\n\r\n    if mode:\r\n        while True:\r\n            input_seq = input('Input >> ')\r\n            sentence = inference(input_seq)\r\n            print(\"Output >>\", ' '.join(sentence))\r\n    else:\r\n        seeds = [\"happy birthday have a nice day\",\r\n                 \"donald trump won last nights presidential debate according to snap online polls\"]\r\n        for epoch in range(epochs):\r\n            trainX, trainY = shuffle(trainX, trainY, random_state=0)\r\n            total_loss, n_iter = 0, 0\r\n            for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), total=nStep, desc='Epoch[{}/{}]'.format(epoch + 1, epochs), leave=False):\r\n                X = tl.prepro.pad_sequences(X)\r\n                _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\r\n                _target_seqs = tl.prepro.pad_sequences(_target_seqs)\r\n                _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\r\n                _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)\r\n                _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\r\n\t\t\t\t\r\n                _, loss_iter = session.run([trainOp, loss], {encode_seqs: X, decode_seqs: _decode_seqs, target_seqs: _target_seqs, target_mask: _target_mask})\r\n                total_loss += loss_iter\r\n                n_iter += 1\r\n\r\n            print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, epochs, total_loss / n_iter))\r\n            \r\n            for seed in seeds:\r\n                print(\"Input >>\", seed)\r\n                for _ in range(5):\r\n                    sentence = inference(seed)\r\n                    print(\"Output >>\", ' '.join(sentence))\r\n            \r\n            tl.files.save_npz(network.all_params, name='model.npz', sess=session)\r\n    \r\n    session.close()\r\n    \r\n    \r\ndef createModel(encodeSeqs, decodeSeqs, vocabSize, embeddingDim, isTrain = True, reuse = False):\r\n    with tf.variable_scope(\"model\", reuse = reuse):\r\n        with tf.variable_scope(\"embedding\") as vs:\r\n            networkworkEncoder = EmbeddingInputlayer(inputs = encodeSeqs, \r\n                                                 vocabulary_size = vocabSize, \r\n                                                 embedding_size = embeddingDim, \r\n                                                 name = 'sequenceEmbedding')\r\n            vs.reuse_variables()\r\n            networkworkDecoder = EmbeddingInputlayer(inputs = decodeSeqs, \r\n                                                 vocabulary_size = vocabSize, \r\n                                                 embedding_size = embeddingDim, \r\n                                                 name = 'sequenceEmbedding')\r\n        networkworkRNN = Seq2Seq(networkworkEncoder, networkworkDecoder, \r\n                             cell_fn = tf.nn.rnn_cell.LSTMCell, \r\n                             n_hidden = embeddingDim, \r\n                             initializer = tf.random_uniform_initializer(-0.1, 0.1),\r\n                             encode_sequence_length = retrieve_seq_length_op2(encodeSeqs),\r\n                             decode_sequence_length = retrieve_seq_length_op2(decodeSeqs),\r\n                             initial_state_encode = None,\r\n                             dropout = (0.5 if isTrain else None),\r\n                             n_layer = 3,\r\n                             return_seq_2d = True,\r\n                             name = 'seq2seq')\r\n        networkworkOut = DenseLayer(networkworkRNN, n_units = vocabSize, act = tf.identity, name = 'output')\r\n    return networkworkOut, networkworkRNN\r\n\r\ndef setup(data_corpus):\r\n    metaData, indexQ, indexA = data.load_data(Path=''.format(data_corpus))\r\n    (trainX, trainY), (testX, testY), (validX, validY) = data.split_dataset(indexQ, indexA)\r\n    trainX = tl.prepro.remove_pad_sequences(trainX.tolist())\r\n    trainY = tl.prepro.remove_pad_sequences(trainY.tolist())\r\n    testX = tl.prepro.remove_pad_sequences(testX.tolist())\r\n    testY = tl.prepro.remove_pad_sequences(testY.tolist())\r\n    validX = tl.prepro.remove_pad_sequences(validX.tolist())\r\n    validY = tl.prepro.remove_pad_sequences(validY.tolist())\r\n    return metaData, trainX, trainY, testX, testY, validX, validY\r\n\r\ndef main():\r\n    try:\r\n        train()\r\n    except KeyboardInterrupt:\r\n        print('Aborted!')\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n", "comments": ["Please consider using DistributionStrategy API for transparent multi-GPU computation. Currently TF does not distribute the computation without any modification to your code.\r\n\r\nThis question is better asked on StackOverflow.", "Closing since solution is provided by @byronyi . Feel free to open a new issue if you are running into problems while using DistributionStrategy AP. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23921, "title": "added optional optimization for einsum", "body": "As I pointed out in [this issue](https://github.com/tensorflow/tensorflow/issues/19776), the `einsum` function does not have an argument `optimize` like its numpy counterpart. For this reason I implemented an optional depth-first search over pairwise contractions. The default behaviour is still the same as before, so no optimisations are performed, unless `optimize=True` or `optimize='dfspairs'` is set.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", ">     * the results using these optimizations are correct\r\n\r\nWould it be okay, if I just write tests for the functions `_einsum_optimize_dp` and `_einsum_optimize_greedy`, which I use internally to find the optimized contraction sequence?\r\n \r\n>     * these optimizations are being used\r\n\r\nI guess, to deduce this from the graph structure is extremely tedious. Would it be okay to check this via runtime comparisons?", "@mrader1248 can you please resolve conflicts.", "> @mrader1248 can you please resolve conflicts.\r\n\r\n@mrader1248  gentle ping", "I'm sorry that it took me so long. The optimizations are now performed in the function `einsum_optimize` that is called from `einsum`.\r\nI modified the method `run_test` in `EinsumTest` such that all test cases are tested with all optimization strategies. This ensures that results produced by different optimization strategies are equal and correct.\r\nFurthermore, I added a test class `EinsumOptimizeTest` that has several test cases to make sure that optimizations are actually performed in the desired way.", "You can likely use the built in NumPy contraction path optimization routines [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum_path.html#numpy.einsum_path) or see more advanced routines [here](https://optimized-einsum.readthedocs.io/en/latest/path_finding.html#performance-comparison).", "The problem with `einsum_path` from NumPy is that it's extremely slow.\r\n```python\r\n#  +---B---B---B---\r\n#  |   |   |   |\r\n#  A   |   |   |\r\n#  |   |   |   |\r\n#  +---B---B---B---\r\nA = np.random.rand(16, 16)\r\nB = np.random.rand(2, 16, 16)\r\nilabels = [\"ab\", \"cad\", \"cbe\", \"fdg\", \"feh\", \"igj\", \"ihk\"]\r\nolabels = ilabels[-2][-1] + ilabels[-1][-1]\r\nitensors = [A] + [B]*(len(ilabels)-1)\r\nishapes = [t.shape for t in itensors]\r\nequ = \",\".join(ilabels) + \"->\" + olabels\r\n\r\n%timeit np.einsum_path(equ, *itensors, optimize=(\"optimal\", np.iinfo(int).max))\r\n596 ms \u00b1 6.54 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n%timeit einsum_optimize(ishapes, ilabels, olabels, optimize=\"dp\")\r\n8.48 ms \u00b1 11.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\nWith just 7 tensors (which is not really much for my field of research), my code is approximately 70 times faster. If I have 9 tensors using\r\n```python\r\nilabels = [\"ab\", \"cad\", \"cbe\", \"fdg\", \"feh\", \"igj\", \"ihk\", \"ljm\", \"lkn\"]\r\n```\r\nmy code needs 34.8 ms \u00b1 1.18 ms and I cannot tell you, how long `einsum_path` needs, because I am not patient enough to wait for just a single run of it to finish.", "`optimal` is a full tree search of every possible path which will ultimately give you the \"best\" path and you are also turning off sieving leading to very large timings. In general `optimal` is very naive and shouldn't be used except for challenging (and small!) problems. Using `optimal` defaults or the advanced `greedy` algorithm which has had quite a bit of effort into tuning and testing are all much faster than yours:\r\n```\r\n%timeit np.einsum_path(equ, *itensors, optimize=(\"optimal\", np.iinfo(int).max))\r\n664 ms \u00b1 23.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n%timeit np.einsum_path(equ, *itensors, optimize=\"optimal\")\r\n491 \u00b5s \u00b1 20.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n%timeit np.einsum_path(equ, *itensors, optimize=\"greedy\")\r\n243 \u00b5s \u00b1 6.36 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\nThe quality of the path also matters a great deal in the overall cost of the contraction, I suggest reading into the `opt_einsum` repository for some exploratory work in performance for a variety of additional algorithms. I believe your current algorithm is identical to the \"branch-1\" algorithm in the `opt_einsum` docs.\r\n\r\nIn addition, it is likely argued that pure FLOP count is likely not very good for GPU implementations regardless and more advanced heuristics are likely needed, see [here](https://github.com/Bihaqo/tf_einsum_opt) for some comments on that.", "No, my dynamical programming optimisation is not equivalent to your branch-1 algorithm. It searches for the optimal contraction order by first finding optimal contraction sequences for all pairs of tensors, then for all triples of tensors, and so on. Therefore, the solution is really optimal and still much faster than the full depth-first search. Maybe I should have given this more complex example already in the beginning:\r\n```python\r\nA = np.random.rand(16, 2, 16)\r\nB = np.random.rand(2, 16, 16)\r\nC = np.random.rand(2, 2, 2, 2)\r\nilabels = [\"daf\", \"gebd\", \"ech\", \"ifk\", \"ljgi\", \"jhm\", \"abc\"]\r\nolabels = \"klm\"\r\nitensors = [B,C,B,B,C,B,A]\r\nishapes = [t.shape for t in itensors]\r\nequ = \",\".join(ilabels) + \"->\" + olabels\r\n\r\nnp.einsum_path(equ, *itensors, optimize=(\"optimal\", np.iinfo(int).max))[0][1:]\r\n[(0, 6), (0, 5), (0, 4), (0, 3), (0, 2), (0, 1)]\r\n\r\neinsum_optimize(ishapes, ilabels, olabels, optimize=\"dp\")\r\n[(0, 6), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]\r\n# note that einsum_path (einsum_optimize) appends (prepends) the\r\n# contracted element to the list of tensors\r\n\r\nnp.einsum_path(equ, *itensors, optimize=\"optimal\")[0][1:]\r\n[(1, 4), (0, 1, 2, 3, 4, 5)]\r\n# w/o knowledge about memory consumption this fails of course\r\n\r\nnp.einsum_path(equ, *itensors, optimize=(\"greedy\", np.iinfo(int).max))[0][1:]\r\n[(0, 3), (1, 3), (0, 1), (0, 1), (1, 2), (0, 1)]\r\n# greedy does not find the optimal solution in this case\r\n\r\n%timeit np.einsum_path(equ, *itensors, optimize=(\"optimal\", np.iinfo(int).max))[0][1:]\r\n612 ms \u00b1 17.1 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n%timeit einsum_optimize(ishapes, ilabels, olabels, optimize=\"dp\")\r\n11.1 ms \u00b1 50.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\n%timeit np.einsum_path(equ, *itensors, optimize=\"optimal\")[0][1:]\r\n232 \u00b5s \u00b1 11.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n%timeit np.einsum_path(equ, *itensors, optimize=(\"greedy\", np.iinfo(int).max))[0][1:]\r\n426 \u00b5s \u00b1 11.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\nOf course, `einsum_path` with strong memory restrictions is much faster as the search space is much smaller. But what does a fast solution help, if its result is basically useless?  Note that the previous example doesn't have tensors that are larger than the input tensors.", "Saying its \"basically useless\" is quite disingenuous as there is a very large variety of use cases and expression types for `einsum` pathing (especially in the domain of very large numbers of tensors). In addition, the greedy approach finds a path that only costs 30% more than the most optimal approach while still being valid for virtually anything a user can give it. All algorithms will find different domains that they are \"best\" in which is why `opt_einsum` took the strategy of picking different algorithms depending on the problem input. \r\n\r\nFor raw speed you might want to benchmark against the `opt_einsum` `optimal` implementation which seems a bit faster than yours:\r\n```\r\n%timeit oe.contract_path(equ, *itensors, optimize=\"optimal\", memory_limit=np.iinfo(int).max)\r\n8.93 ms \u00b1 272 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\n%timeit np.einsum_path(equ, *itensors, optimize=(\"optimal\", np.iinfo(int).max))\r\n659 ms \u00b1 22.7 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nHard to fully compare apples to apples here as I do not have your algorithms, but worth a look as the `np.einsum_path` times look comparable.", "Sorry, I did not want to insult you. With useless I meant the solution `[(1, 4), (0, 1, 2, 3, 4, 5)]`, definitely not your overall work.\r\nIf you only have to perform a single contraction of this kind, 30 % more runtime seems to be an adequate option. However, if you have simulations running for weeks and months, repeating these contractions again and again, then you care about this extra runtime. Especially in the case of tensorflow, where the computation graph is built and optimised only once, but executed many times, a few milliseconds more of optimisation time are ok for me.\r\n\r\nIf you want to have a look into my implementation, you can find it [here](https://github.com/mrader1248/tensorflow/blob/0b024689d0e080abe459c6e0fa245b5d35af2d54/tensorflow/python/ops/special_math_ops.py#L336). Just copy the functions `einsum_optimize`, `_einsum_optimize_dp`, `_find_subgraphs`, `_einsum_optimize_dp_connected`, and `_tree_to_sequence`. You do not even need tensorflow to play around with it.", "@MarkDaoust do you have additional concerns? ", "@martinwicke, @MarkDaoust: Anything else to do for me?", "@mrader1248 we'd be happy to review a PR that further improves optimizations. ", "However, this is much better than not having it, so we'll accept this now.  ", "@mrader1248 thank you for contribution , this change is failing sanity check , can you please check here once https://source.cloud.google.com/results/invocations/74054d17-9282-4740-a88c-c012f2bd67c4/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out/log", "I got it fixed. You'll see the fix in the merge commit.", "I tried to merge this, and found that there is equivalent functionality, based on https://github.com/dgasmith/opt_einsum currently being merged internally. After carefully considering both options TensorFlow owners chose to go with the `opt_einsum`."]}, {"number": 23920, "title": "tf.estimator package not installed appears in ops_to_register.h when bazel-bin/tensorflow/python/tools/print_selective_registration_header is executed", "body": "\r\n**System information**\r\nOS Platform and Distribution: maxOS High Sierra 10.13.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version:0..18.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:   Not supported when confugurate\r\n", "comments": ["I do not understand what exactly the issue is from the title above-- can you clarify what command you executed, and what the error was?", "Closing this as it is in \"awaiting response\" status for more than 7 days. Feel free to add your comments and we will reopen(if required)."]}, {"number": 23919, "title": "front camera bounding box tracking issue", "body": "\r\nThanks for clicking  in and reading my questions first :)\r\nI want to detect human face by front camera .\r\nI have changed my camera to front, and changed location like this.\r\n` location.left = 288f - result.getLocation().left;`\r\n  ` location.right = 288f - result.getLocation().right;`\r\n      \r\nI have 2 questions.\r\n       1.My code can detect human correctly, but the bounding box tracks in opposite direction(left-            >right. right->left ). Maybe I miss to  edit somewhere's location ?\r\n       2.Another question is , sometimes when target moves, a new bounding box occurs, but the old one    is still there ,too. However,if preview changes a lot (like target leaves quickly, or new target in), the old box will disappear .\r\nIf any one have any idea I will appreciate.\r\nI recorded the detection scene and put it below .\r\n[https://reurl.cc/EZ4pv](url)\r\n ", "comments": ["Problem solved!\r\nI change other file\u2019s location, and it works!", "hello, how to fix it? I have same problem.", "Wishing there were a more elegant solution, but for a hack of the source, you can flip the bounding box from left to right by adding \r\n```\r\nlocation.set((TF_OD_API_INPUT_SIZE - location.left), location.top, (TF_OD_API_INPUT_SIZE - location.right), location.bottom);\r\n```\r\njust above [this](https://github.com/tensorflow/examples/blob/f55194adcfafa5cddd57f8c05d42b8a428e9d24b/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/DetectorActivity.java#L204)"]}, {"number": 23918, "title": "tf.dynamic_partition may cause NaN loss when use it with multi gpus and it performs normally with single gpu", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **_no_**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _**Ubuntu 16.04.5 LTS**_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:**_no_**\r\n- TensorFlow installed from (source or binary):**_binary_**\r\n- TensorFlow version (use command below):**_1.12.0_**\r\n- Python version: **_Python 3.5.2_**\r\n- Bazel version (if compiling from source): _**None**_\r\n- GCC/Compiler version (if compiling from source):  **_5.4.0_**\r\n- CUDA/cuDNN version: **_cuda-9.0.176/cudnn-7.2.1_**\r\n- GPU model and memory: **_2 same GTX Titan X (Pascal), 12GB_** \r\n\r\n\r\nI just use the offical docker image of the tensorflow, the tag is `1.12.0-devel-gpu-py3`\r\n\r\n**Describe the problem**\r\n\r\n**The api `tf.dynamic_partition` may cause NaN loss when use it with multi gpus and it is normal with single gpu.**\r\n\r\nThe code to reproduce the problem:\r\n```\r\n#coding=utf-8\r\n\r\nimport os\r\nimport time\r\nimport tensorflow as tf\r\n\r\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\nnum_gpus = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(','))\r\n\r\ndef dp_and_bm_api_test(argv=None):\r\n\ta = tf.constant([[1, 2, 3],\r\n\t\t\t\t\t[4, 5, 6],\r\n\t\t\t\t\t[7, 8, 9],\r\n\t\t\t\t\t[10, 11, 12]])\r\n\t\r\n\tmask = tf.constant([1, 0, 1, 0])\r\n\tres_01 = tf.boolean_mask(a, mask)\r\n\tres_02 = tf.dynamic_partition(a, mask, num_partitions=2)[1]\r\n\r\n\twith tf.Session() as sess:\r\n\t\tr_1, r_2 = sess.run([res_01, res_02])\r\n\t\tprint(\"r_1\\n\", r_1)\r\n\t\tprint(\"r_2\\n\", r_2)\r\n\r\n\r\ndef tower_model_fn(features, labels, api_sel=0):\r\n\t\"\"\"a test tower fn\"\"\"\r\n\tnum_classes = 10\r\n\tnet = tf.layers.conv2d(features, num_classes * 2, [3,3], padding=\"same\")\r\n\tnet = tf.layers.batch_normalization(net)\r\n\tnet = tf.nn.relu(net)\r\n\tnet = tf.layers.conv2d(net, num_classes, [3,3], padding=\"same\")\r\n\tnet = tf.layers.batch_normalization(net)\r\n\tlogits = tf.nn.relu(net)\r\n\r\n\tlabels = tf.squeeze(labels, axis=3)  # reduce the channel dimension.\r\n\tlogits_by_num_classes = tf.reshape(logits, [-1, num_classes])\r\n\tlabels_flat = tf.reshape(labels, [-1, ])\r\n\tvalid_indices = tf.to_int32(labels_flat <= num_classes - 1)\r\n\r\n\tif api_sel == 0:\r\n\t\tvalid_logits = tf.boolean_mask(logits_by_num_classes, valid_indices)\r\n\t\tvalid_labels = tf.boolean_mask(labels_flat, valid_indices)\r\n\telse:\r\n\t\tvalid_logits = tf.dynamic_partition(logits_by_num_classes, valid_indices, num_partitions=2)[1]\r\n\t\tvalid_labels = tf.dynamic_partition(labels_flat, valid_indices, num_partitions=2)[1]\r\n\r\n\r\n\tcross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=valid_logits, labels=valid_labels, \r\n\t\t\t\t\t\t\tloss_collection=None)\r\n\ttf.add_to_collection(tf.GraphKeys.LOSSES, cross_entropy)\r\n\treturn None\r\n\r\ndef train(api_sel=0):\r\n\t## variable strategy\r\n\tvariable_strategy = 'CPU'\r\n\tinput_device = '/cpu:0'\r\n\t#var_device = '/gpu:0'\r\n\tvar_device = '/cpu:0'\r\n\tnum_devices = num_gpus\r\n\tdevice_type = 'gpu'\r\n\t\r\n\twith tf.Graph().as_default() as graph:\r\n\t\twith tf.device(var_device):\r\n\t\t\t## some collector\r\n\t\t\ttower_ce_loss = []\r\n\r\n\t\t\tglobal_step = tf.train.get_or_create_global_step()\t\r\n\r\n\t\t\tname_scopes = ['tower_%d' % i for i in range(num_devices)]\r\n\t\t\tfor i in range(num_devices):\r\n\t\t\t\twith tf.variable_scope(tf.get_variable_scope(), reuse=bool(i > 0)):\r\n\t\t\t\t\tworker_device = '/{0}:{1}'.format(device_type, i)\r\n\t\t\t\t\twith tf.name_scope(name_scopes[i]) as name_scope:\r\n\t\t\t\t\t\twith tf.device(worker_device):\t\r\n\r\n\t\t\t\t\t\t\timages = tf.ones([8, 321, 321, 3])\r\n\t\t\t\t\t\t\tlabels = tf.zeros([8, 321, 321, 1], dtype=tf.int32)\r\n\t\t\t\t\t\t\ttower_model_fn(images, labels, api_sel=api_sel)\r\n\t\t\t\t\t\t\tce_now = tf.get_collection(tf.GraphKeys.LOSSES, scope=name_scope)\r\n\t\t\t\t\t\t\ttower_ce_loss.append(tf.add_n(ce_now))\r\n\r\n\t\t\r\n\t\twith tf.device(var_device):\r\n\t\t\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scopes[0])\t\r\n\r\n\t\t\tce_loss = tf.add_n(tower_ce_loss)\r\n\t\t\tsession_config = tf.ConfigProto(allow_soft_placement=True,\r\n\t\t\t\t\t\t\t\t\t\t\tlog_device_placement=False\r\n\t\t\t\t\t\t\t\t\t\t\t)\r\n\t\t\t# Build an initialization operation to run below.\r\n\t\t\tinit_op = tf.global_variables_initializer()\r\n\t\t\tmax_steps = 30\r\n\t\t\tstep_gap_init_time = 0.0\r\n\t\t\twith tf.Session(config=session_config) as sess:\r\n\t\t\t\tsess.run(init_op)\r\n\t\t\t\tfor steps in range(1, max_steps + 1, 1):\r\n\t\t\t\t\tstep_gap_init_time = time.time()\r\n\t\t\t\t\tc_l = sess.run([ce_loss])\r\n\t\t\t\t\tif steps % 10 == 0:\r\n\t\t\t\t\t\tgap_time = (time.time() - step_gap_init_time) / 10\r\n\t\t\t\t\t\tprint(\"ce loss{0}, {1:1.4f}s per steps\".format(c_l, gap_time))\r\n\ttf.keras.backend.clear_session()\r\n\ttf.reset_default_graph()\r\n\r\nif __name__ == '__main__':\r\n\ttf.logging.set_verbosity(tf.logging.INFO)\r\n\tprint(\"\\nTest the two apis\")\r\n\tdp_and_bm_api_test()\r\n\tprint(\"The number of gpus is {0}\".format(num_gpus))\r\n\tprint(\"\\nUsing tf.boolean_mask\")\r\n\ttrain(api_sel=0)\r\n\tprint(\"\\nUsing tf.dynamic_partition\")\r\n\ttrain(api_sel=1)\r\n```\r\n\r\nWhen I use two gpus, I set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1\"`\r\n\r\n\r\n```\r\n2018-11-22 07:37:36.070340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2018-11-22 07:37:36.478438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-22 07:37:36.478486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1\r\n2018-11-22 07:37:36.478498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y\r\n2018-11-22 07:37:36.478511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N\r\n2018-11-22 07:37:36.478930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:17:00.0, compute capability: 5.2)\r\n2018-11-22 07:37:36.479432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11421 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:18:00.0, compute capability: 5.2)\r\nr_1\r\n [[1 2 3]\r\n [7 8 9]]\r\nr_2\r\n [[1 2 3]\r\n [7 8 9]]\r\nThe number of gpus is 2\r\n\r\nUsing tf.boolean_mask\r\n\r\nce loss[5.088441], 0.0010s per steps\r\nce loss[5.088441], 0.0010s per steps\r\nce loss[5.088441], 0.0010s per steps\r\n\r\n\r\nUsing tf.dynamic_partition\r\nce loss[nan], 0.0010s per steps\r\nce loss[nan], 0.0011s per steps\r\nce loss[nan], 0.0011s per steps\r\n```\r\n\r\n\r\nWhen I use one gpus, I set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"`\r\nThen I get\r\n```\r\n\r\nTest the two apis\r\n2018-11-22 07:41:32.112369: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2018-11-22 07:41:34.385976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.80GiB\r\n2018-11-22 07:41:34.386009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-22 07:41:34.586971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-22 07:41:34.587016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2018-11-22 07:41:34.587024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2018-11-22 07:41:34.587249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11421 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:17:00.0, compute capability: 5.2)\r\nr_1\r\n [[1 2 3]\r\n [7 8 9]]\r\nr_2\r\n [[1 2 3]\r\n [7 8 9]]\r\nThe number of gpus is 1\r\n\r\n\r\nUsing tf.boolean_mask\r\n\r\nce loss[2.3863728], 0.0009s per steps\r\nce loss[2.3863728], 0.0009s per steps\r\nce loss[2.3863728], 0.0009s per steps\r\n\r\n\r\n\r\nUsing tf.dynamic_partition\r\nce loss[2.1562214], 0.0010s per steps\r\nce loss[2.1562214], 0.0013s per steps\r\nce loss[2.1562214], 0.0010s per steps\r\n\r\n```\r\n\r\nI think the implementation of the  api `tf.dynamic_partition` is really terrible, \r\nI also reported that this api may cuase memory leak under certain situation\r\nhttps://github.com/tensorflow/tensorflow/issues/22464\r\n\r\nIt also cost me lots of time to find that it is not suitable with mulit gpus.....\r\nI think there may other potential issues about the  `tf.dynamic_partition`.\r\nAnd I don't figure out why I still use it......\r\n\r\n", "comments": ["No anyone else meet this problem ?", "Hi, does this have something to do with XLA?", "@jlebar \r\n\r\nI am not familiar with XLA...\r\nWhen I try it following https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py\r\n\r\nI find another interesting phenomenon, the appearance of NaN loss when using `tf.dynamic_partition` is related to the magnitude of the data\r\n\r\nHere is the code\r\n```\r\n#coding=utf-8\r\n\r\nimport os\r\nimport time\r\nimport tensorflow as tf\r\n#from tensorflow.contrib.compiler import xla\r\n\r\n\r\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1\"\r\nnum_gpus = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(','))\r\n\r\n\r\ndef tower_model_fn(features, labels, api_sel=1):\r\n\t\"\"\"a test tower fn\"\"\"\r\n\tnum_classes = 10\r\n\tlogits = tf.layers.dense(features, num_classes)\r\n\r\n\tlogits_by_num_classes = tf.reshape(logits, [-1, num_classes])\r\n\tlabels_flat = tf.reshape(tf.squeeze(labels), [-1, ])\r\n\tvalid_indices = tf.to_int32(labels_flat <= num_classes - 1)\r\n\r\n\tif api_sel == 0:\r\n\t\tvalid_logits = tf.boolean_mask(logits_by_num_classes, valid_indices)\r\n\t\tvalid_labels = tf.boolean_mask(labels_flat, valid_indices)\r\n\telse:\r\n\t\tvalid_logits = tf.dynamic_partition(logits_by_num_classes, valid_indices, num_partitions=2)[1]\r\n\t\tvalid_labels = tf.dynamic_partition(labels_flat, valid_indices, num_partitions=2)[1]\r\n\r\n\tcross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=valid_logits, labels=valid_labels, \r\n\t\t\t\t\t\t\tloss_collection=None)\r\n\ttf.add_to_collection(tf.GraphKeys.LOSSES, cross_entropy)\r\n\treturn None\r\n\r\ndef train(batch_size, api_sel=1):\r\n\t\"\"\"main\"\"\"\r\n\t## variable strategy\r\n\tvariable_strategy = 'CPU'\r\n\tinput_device = '/cpu:0'\r\n\tvar_device = '/cpu:0'\r\n\tnum_devices = num_gpus\r\n\tdevice_type = 'gpu'\r\n\t\r\n\twith tf.Graph().as_default() as graph:\r\n\t\twith tf.device(var_device):\r\n\t\t\t## some collector\r\n\t\t\ttower_ce_loss = []\r\n\t\t\tglobal_step = tf.train.get_or_create_global_step()\t\r\n\r\n\t\t\tname_scopes = ['tower_%d' % i for i in range(num_devices)]\r\n\t\t\tfor i in range(num_devices):\r\n\t\t\t\twith tf.variable_scope(tf.get_variable_scope(), reuse=bool(i > 0)):\r\n\t\t\t\t\tworker_device = '/{0}:{1}'.format(device_type, i)\r\n\t\t\t\t\twith tf.name_scope(name_scopes[i]) as name_scope:\r\n\t\t\t\t\t\twith tf.device(worker_device):\t\r\n\r\n\t\t\t\t\t\t\timages = tf.ones([batch_size, 20])\r\n\t\t\t\t\t\t\tlabels = tf.zeros([batch_size, 1], dtype=tf.int32)\r\n\t\t\t\t\t\t\ttower_model_fn(images, labels, api_sel=api_sel)\r\n\t\t\t\t\t\t\tce_now = tf.get_collection(tf.GraphKeys.LOSSES, scope=name_scope)\r\n\t\t\t\t\t\t\ttower_ce_loss.append(tf.add_n(ce_now))\r\n\r\n\r\n\t\twith tf.device(var_device):\r\n\t\t\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scopes[0])\t\r\n\r\n\t\t\tce_loss = tf.add_n(tower_ce_loss)\r\n\t\t\tsession_config = tf.ConfigProto(allow_soft_placement=True,\r\n\t\t\t\t\t\t\t\t\t\t\tlog_device_placement=False\r\n\t\t\t\t\t\t\t\t\t\t\t)\r\n\t\t\t# Build an initialization operation to run below.\r\n\t\t\tinit_op = tf.global_variables_initializer()\r\n\t\t\tmax_steps = 30\r\n\t\t\tstep_gap_init_time = 0.0\r\n\t\t\twith tf.Session(config=session_config) as sess:\r\n\t\t\t\tsess.run(init_op)\r\n\t\t\t\tfor steps in range(1, max_steps + 1, 1):\r\n\t\t\t\t\tstep_gap_init_time = time.time()\r\n\t\t\t\t\tc_l = sess.run([ce_loss])\r\n\t\t\t\t\tif steps % 10 == 0:\r\n\t\t\t\t\t\tgap_time = (time.time() - step_gap_init_time) / 10\r\n\t\t\t\t\t\tprint(\"ce loss{0}, {1:1.4f}s per steps\".format(c_l, gap_time))\r\n\r\nif __name__ == '__main__':\r\n\ttf.logging.set_verbosity(tf.logging.INFO)\r\n\t\r\n\tfor batch_size in [1000, 2000, 3000, 4000, 5000, 6000]:\r\n\t\tprint(\"batch_size={0}\".format(batch_size))\r\n\t\tprint(\"The number of gpus is {0}\".format(num_gpus))\r\n\t\tprint(\"\\nUsing tf.boolean_mask\")\r\n\t\ttrain(batch_size, api_sel=0)\r\n\t\tprint(\"\\nUsing tf.dynamic_partition\")\r\n\t\ttrain(batch_size, api_sel=1)\r\n```\r\n\r\nThe output\r\n\r\n```\r\nbatch_size=1000\r\n\r\nThe number of gpus is 2\r\nUsing tf.boolean_mask\r\nce loss[6.561516], 0.0003s per steps\r\nce loss[6.561516], 0.0005s per steps\r\nce loss[6.561516], 0.0004s per steps\r\nUsing tf.dynamic_partition\r\nce loss[8.170236], 0.0001s per steps\r\nce loss[8.170236], 0.0001s per steps\r\nce loss[8.103901], 0.0001s per steps\r\n\r\n\r\nbatch_size=2000\r\nThe number of gpus is 2\r\nUsing tf.boolean_mask\r\nce loss[3.8799157], 0.0003s per steps\r\nce loss[3.8799157], 0.0002s per steps\r\nce loss[3.8799157], 0.0001s per steps\r\nUsing tf.dynamic_partition\r\nce loss[5.843277], 0.0007s per steps\r\nce loss[5.843277], 0.0003s per steps\r\nce loss[5.843277], 0.0006s per steps\r\n\r\nbatch_size=3000\r\nThe number of gpus is 2\r\nUsing tf.boolean_mask\r\nce loss[5.877125], 0.0003s per steps\r\nce loss[5.877125], 0.0002s per steps\r\nce loss[5.877125], 0.0004s per steps\r\nUsing tf.dynamic_partition\r\nce loss[7.8206615], 0.0007s per steps\r\nce loss[7.82078], 0.0005s per steps\r\nce loss[6.755781], 0.0007s per steps\r\n\r\nbatch_size=4000\r\nThe number of gpus is 2\r\nUsing tf.boolean_mask\r\nce loss[7.826822], 0.0001s per steps\r\nce loss[7.826822], 0.0001s per steps\r\nce loss[7.826822], 0.0001s per steps\r\nUsing tf.dynamic_partition\r\nce loss[4.3592453], 0.0001s per steps\r\nce loss[4.3592453], 0.0001s per steps\r\nce loss[4.361978], 0.0001s per steps\r\n\r\nbatch_size=5000\r\nThe number of gpus is 2\r\nUsing tf.boolean_mask\r\nce loss[11.058659], 0.0003s per steps\r\nce loss[11.058659], 0.0005s per steps\r\nce loss[11.058659], 0.0004s per steps\r\nUsing tf.dynamic_partition\r\nce loss[nan], 0.0004s per steps\r\nce loss[nan], 0.0004s per steps\r\nce loss[nan], 0.0006s per steps\r\n\r\n\r\nbatch_size=6000\r\nThe number of gpus is 2\r\nUsing tf.boolean_mask\r\nce loss[6.5018396], 0.0001s per steps\r\nce loss[6.5018396], 0.0001s per steps\r\nce loss[6.5018396], 0.0001s per steps\r\nUsing tf.dynamic_partition\r\nce loss[nan], 0.0001s per steps\r\nce loss[nan], 0.0001s per steps\r\nce loss[nan], 0.0006s per steps\r\n\r\n```\r\n\r\n\r\n ", "@jlebar \r\nI follow the tutorial:\r\nhttps://www.tensorflow.org/xla/tutorials/xla_compile\r\n\r\nI slightly modify the code and let the function `tower_model_fn` return the cross entropy loss\r\n```\r\nfrom tensorflow.contrib.compiler import xla\r\n\r\n......\r\n\r\nce_now = xla.compile(tower_model_fn, inputs=[images, labels, api_sel])[0]\r\n```\r\n\r\nI try both `tf.boolean_mask` and `tf.dynamic_partition`, and I get info\r\n```\r\n2018-11-27 04:22:34.555284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2018-11-27 04:22:35.066707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-27 04:22:35.066779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1\r\n2018-11-27 04:22:35.066794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y\r\n2018-11-27 04:22:35.066805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N\r\n2018-11-27 04:22:35.067195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4593 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2018-11-27 04:22:35.067573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 6512 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\n2018-11-27 04:22:35.070756: I tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc:179] Subgraph fingerprint:11011464846742919204\r\n2018-11-27 04:22:35.076199: I tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc:179] Subgraph fingerprint:10675846169608442761\r\n2018-11-27 04:22:35.185947: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f47800030a0 executing computations on platform CUDA. Devices:\r\n2018-11-27 04:22:35.186050: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n2018-11-27 04:22:35.186072: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (1): GeForce GTX 1080, Compute Capability 6.1\r\n2018-11-27 04:22:35.280461: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:402] *** WARNING *** You are using ptxas 9.0.424, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n\r\n\r\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\r\n2018-11-27 04:22:02.158300: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at xla_ops.cc:295 : Invalid argument: Detected unsupported operations when trying to compile graph tower_0/cluster_12744596615876747846[] on XLA_GPU_JIT: Where (No registered 'Where' OpKernel for XLA_GPU_JIT devices compatible with node {{node tower_0/boolean_mask/Where}} = Where[T=DT_INT32, _device=\"/device:GPU:0\"](tower_0/ToInt32)\r\n        .  Registered:  device='CPU'; T in [DT_BOOL]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_BOOL]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_UINT8]\r\n  device='GPU'; T in [DT_INT8]\r\n){{node tower_0/boolean_mask/Where}}\r\n```\r\n\r\nand\r\n\r\n```\r\n\r\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\r\n2018-11-27 04:22:35.507997: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at xla_ops.cc:295 : Invalid argument: Detected unsupported operations when trying to compile graph tower_0/cluster_10675846169608442761[] on XLA_GPU_JIT: DynamicPartition (No registered 'DynamicPartition' OpKernel for XLA_GPU_JIT devices compatible with node {{node tower_0/DynamicPartition_1}} = DynamicPartition[T=DT_INT32, num_partitions=2, _device=\"/device:GPU:0\"](tower_0/Reshape_1, tower_0/ToInt32)\r\n        .  Registered:  device='CPU'; T in [DT_VARIANT]\r\n  device='CPU'; T in [DT_RESOURCE]\r\n  device='CPU'; T in [DT_STRING]\r\n  device='CPU'; T in [DT_BOOL]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n){{node tower_0/DynamicPartition_1}}\r\n\r\n```\r\n\r\nAfter removing the two operations, the code can run normally.\r\n\r\nI also try https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py.\r\n\r\nThe results are same.\r\n", "Sorry, didn't mean to ask you to try it with XLA, I was just asking if I was assigned the bug because it had something to do with XLA.  Sounds like it doesn't, so I'll unassign myself.", "> No anyone else meet this problem ?\r\n\r\nI also meet the problem. I thought it might be some wrong with my code. But I remove the tf.dynamic_partition code, The loss don't get Nan. Hence, it is likely that the above problem.", "Sorry, it's not clear from the above what you are actually trying to accomplish. Can you clarify what you are trying to do?", "@karmel \r\nI am training a semantic segmentation model. And there are some useless labels in the annotation images. The labels are in certain range, then I can get a boolean mask.  After I get the mask, I try to use `tf.boolean_mask` and `tf.dynamic_partition` to filter the useless labels. So they do not contribute to the final loss.\r\n\r\nThey both work normally when I use single GPU. \r\nWhen I use `tf.boolean_mask`, tf will warn me that it will cost  a lot of memory. \r\nSo I try `tf.dynamic_partition`, and I find that it will cause NaN loss when I use multi GPUs.\r\nBut `tf.boolean_mask` works normally with multi GPUs.\r\n\r\nThe reproducible snippet code is given in my first comments.\r\n\r\nAnd I also found the production of NaN loss when using `tf.dynamic_partition` is related to the magnitude of the data.\r\nThis is described in my second comments.\r\n\r\nThe key problem is that I think `tf.dynamic_partition` do not work properly when using multi GPUs.\r\n\r\n", "Hello,\r\nI came upon this issue after searching for the Warning message I saw `You are using ptxas 9.0.176, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.` and also encountering NaN in loss function on multiple GPU training.\r\n\r\nHowever, I have not explicitly used `tf.dynamica_partition`. I also find that a code which previously worked well on multiple GPU using tensorflow 1.10 now gives me NaN in loss with tensorflow 1.12. So I think this is a problem with the new version of tensorflow. On my cluster I have tried to use both cuda 9.2 and 10.0, and cudnn 7.3.1. NaN always occur with tensorflow-gpu 1.12\r\n\r\nI have checked the magnitudes of the data and the weights in the network. They all appear to be in reasonable range and there was no apparent change of their range before the NaN occurred (usually NaN does not appear on the first epoch of training but happens after a few hundreds of iterationd). One suspicious observation is that the norm of the gradients of the weights which are usually way below 1 suddenly changed to tens (or hundreds) of thousands in one iteration, a few iterations before NaN occurred in the loss function. But I have kept a relatively tight bound on gradient clipping in my Adam optimizier and very small learning rate (1e-5) so I would not expect one such jump in gradient to throw the network off (in fact, I observed the range of weights. They do not seem to change too much after this spike in gradient). However, as I said, the same network which worked previously with the same dataset on many hyperparameters settings now suddenly fail regardless of the hyperparameters I use. So i think it is not because of something wrong in my data.\r\n\r\nI wonder whether the warning about XLA has something to do with this.\r\n", "> I wonder whether the warning about XLA has something to do with this.\r\n\r\n@lcnature I'd be happy to help you debug this, but would you be willing to file a separate GitHub issue?  This issue is for a specific thing which we've established actually does not have to do with XLA.  Your issue sounds different, and combining issues makes things very confusing.", "Reassigning to @caisq for dynamic_partition on GPUs.", "Just bumping this up, as I run into exact same issue - tf.dynamic_partition often, but not always, transforms some valid inputs into NaNs when using more than one gpu.", "Same comment as https://github.com/tensorflow/tensorflow/issues/23918#issuecomment-446021526.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23918\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23918\">No</a>\n", "Oh I understand, this bug is still open, it just has nothing to do with XLA.  I will unassign myself, basically, same comment as https://github.com/tensorflow/tensorflow/issues/23918#issuecomment-442191501.", "\r\n@mzhaoshuai \r\nWe see that you are using old version of tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Hence moving this to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23918\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23918\">No</a>\n"]}, {"number": 23917, "title": "error raised when convert srgan model to .tflite", "body": "Hi! Recently, I've tried a lot to convert a SRGan model(.ckpt) to .tflite version.However,things dont go that smoothly.When i called the converted model(.tflite) in android studio, it ran just collapsed. Finally , i find it is two modules contained in the code which is pixelShuffler() that  account for the raised error. Because when i set input of that module as output node in the frozen graph,it works.But  i dived into the problematic module and found no operator not supported by tensorflow lite.And the code is @https://github.com/brade31919/SRGAN-tensorflow. So Has anyone converted GAN to .tflite successfully?", "comments": ["Please provide details about what platform you are using (operating system, architecture). \r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "`1.custom code`\r\n`2.Ubuntu 16.04` \r\n`3.Htc`\r\n`4.Tensorflow installed from binary`\r\n`5.Tensorflow version 1.12`\r\n`python3.5`\r\n`Bazel 0.19.0 not source`\r\n`GCC-5.4.4 not source`\r\n`CUDA-8.0`\r\n``GPU GT-750M 2G", "Closing due to stale history, feel free to re-open if this is still an issue."]}]