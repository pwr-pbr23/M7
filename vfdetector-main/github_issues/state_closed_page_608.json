[{"number": 35424, "title": "Patch 1", "body": "Math.not_equal example added", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35424) for more info**.\n\n<!-- need_author_consent -->", "This looks bad. Please make sure the PR only contains the files you changed (which shouldn't be too many) and doesn't include commits from others.\r\n\r\nI'm going ahead and closing this one but please open a clean one"]}, {"number": 35423, "title": "TF 2.0 XLA JIT reporting error: \"./bin/ptxas not found\"", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04.6 LTS\r\n- TensorFlow installed from (source or binary): pip3 install tensorflow-gpu\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: TITAN Xp\r\n\r\n**Describe the current behavior**\r\n\r\nThe test code is running with error as bellow:\r\n\r\n```\r\n2019-12-26 22:02:59.166382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-26 22:02:59.166422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-26 22:02:59.166453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-12-26 22:02:59.166482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-12-26 22:02:59.166512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-12-26 22:02:59.166541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-12-26 22:02:59.166573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-26 22:02:59.171144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-26 22:02:59.171311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-26 22:02:59.174312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-26 22:02:59.174418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-12-26 22:02:59.174508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-12-26 22:02:59.179990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11439 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:06:00.0, compute capability: 6.1)\r\nsleep\r\n2019-12-26 22:02:59.923393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-26 22:03:00.348503: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n2019-12-26 22:03:00.355159: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at xla_compile_on_demand_op.cc:218 : Not found: ./bin/ptxas not found\r\nTraceback (most recent call last):\r\n  File \"tf.py\", line 8, in <module>\r\n    c = tf.linalg.matmul(a, b)\r\n  File \"/home/thincal/.local/lib/python3.5/site-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/thincal/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py\", line 2765, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/home/thincal/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 6126, in mat_mul\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 2, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: ./bin/ptxas not found [Op:MatMul] name: MatMul/\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe test code is running successfully.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\ntry:\r\n  with tf.device('device:XLA_GPU:0'):\r\n    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\n    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\n    c = tf.linalg.matmul(a, b)\r\n    print(c)\r\nexcept RuntimeError as e:\r\n  print(e)\r\n```", "comments": ["@thincal \r\nI have tried on colab with TF version 2.0 and i am not seeing any issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/314697edacf06358aab91a9da67fb807/untitled507.ipynb). Thanks!", "same issue with cudnn v7.6.5 ;upgrate from v7.5.0", "@ravikyram thanks for your information.\r\n\r\nWe install the cuda/cudnn in another folder and configure the PATH and LDPATH as bellow:\r\n\r\n```\r\nexport PATH=/data/cuda/cuda-10.0/cuda/bin:$PATH\r\nexport LD_LIBRARY_PATH=/data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cudnn/v7.5.0/lib64:$LD_LIBRARY_PATH\r\n```\r\n\r\nIs this caused by a similar issue that [ptxas ignores the PATH](https://github.com/tensorflow/tensorflow/issues/33375) ?", "CC @Artem-B \r\n\r\n@cheshire Can we have open source documentation that we can link to as part of the error message?", "@thincal Do you want to try `tf-nightly-gpu` package? I'm not sure if the fix made it into 2.0.0.\r\n\r\n@sanjoy I'll update the error message to suggest fixing the PATH.", "@thincal XLA:GPU devices are semi-deprecated, I do not recommend to use them. As an alternative, could you use `@tf.function(experimental_compile=True)` (with a nightly TF build).\r\nOut of curiosity, where did you find the information about XLA:GPU devices? I don't think they are mentioned in the documentation anymore.\r\n@Artem-B Any clues why do we fail here? It should fall back to the driver compilation.", "@thincal Also to get a more complete error message, could you re-run with environment variable `TF_CPP_VMODULE=status=5`?", "The error reported shows a warning printed by `cuda/redzone_allocator.cc`. The file has been moved to `gpu/` some time back in early september  in #31908 , so this error is from a fairly old TF build.\r\nI believe @cheshire did fair amount of changes since then to both the redzone checker and to the way PTX compilation is handled. I'd recommend to start with checking if this is still an issue with a nightly build.", "> where did you find the information about XLA:GPU devices? I don't think they are mentioned in the documentation anymore.\r\n\r\n@Artem-B \r\n\r\n- with TITAN Xp / 2080 Ti the `device_lib.list_local_devices` list only the `XLA_GPU` device_type\r\n- with 1080 Ti the `device_lib.list_local_devices` list both the `GPU` and `XLA_GPU` device_type\r\n\r\nthis issue is raised when we using the TITAN Xp.\r\n\r\nP.S. `tf.device('device:GPU:0')` is working well in 1080 Ti FYI.\r\nP.S.2. actually `tf.device('device:GPU:0')` is working well in TITAN Xp, 2080 Ti, and 1080 Ti", "@cheshire is there a way to disable the XLA explicitly when using TensorFlow 1.x and 2.0 ?\r\n\r\nUpdate:\r\n\r\nby the way, the XLA mode depends on which device type used ? such as: `tf.device('device:GPU:0')` vs. `tf.device('device:XLA_GPU:0')` ?", "> Do you want to try `tf-nightly-gpu` package? I'm not sure if the fix made it into 2.0.0.\r\n\r\n@cheshire \r\n\r\n**Summary**:\r\n- **It is working with tf-nightly-gpu**, but needs some fix the libcuxxx version missing issue.\r\n\r\nFYI:\r\n\r\n**the first try**: it reports missing some library so that GPU can't be used:\r\n\r\n```\r\ntf-nightly-gpu: 2.1.0.dev20191227\r\ncudnn: v7.5.0\r\ncuda: v10.0\r\n```\r\n\r\n```\r\n$ python3 tf.py\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW1228 14:30:27.537421 140555689355008 tpu_cluster_resolver.py:35] Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .\r\nsleep\r\nsleep\r\nmain thread end...\r\nsleep\r\nsleep\r\nsleep\r\n2019-12-28 14:30:34.308062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-28 14:30:34.318080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:83:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2019-12-28 14:30:34.320787: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cudnn/v7.5.0/lib64:/usr/local/nvidia/lib64\r\n2019-12-28 14:30:34.321892: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cudnn/v7.5.0/lib64:/usr/local/nvidia/lib64\r\n2019-12-28 14:30:34.323004: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cudnn/v7.5.0/lib64:/usr/local/nvidia/lib64\r\n2019-12-28 14:30:34.324252: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cudnn/v7.5.0/lib64:/usr/local/nvidia/lib64\r\n2019-12-28 14:30:34.325450: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cudnn/v7.5.0/lib64:/usr/local/nvidia/lib64\r\n2019-12-28 14:30:34.326818: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /data/cuda/cuda-10.0/cuda/lib64:/data/cuda/cuda-10.0/cudnn/v7.5.0/lib64:/usr/local/nvidia/lib64\r\n2019-12-28 14:30:34.379859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-28 14:30:34.379948: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1595] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2019-12-28 14:30:34.380555: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-28 14:30:34.413199: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2100020000 Hz\r\n2019-12-28 14:30:34.413769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x65f6b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-12-28 14:30:34.413829: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-12-28 14:30:34.417437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-28 14:30:34.417491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 unknown device.\r\n```\r\n\r\n**the second try**: I just upgrade the cudnn to v7.6.1, it reports the same error as above.\r\nBut actually these shared libraries are existed but just with a version 10.0:\r\n\r\n```\r\n/data/cuda/cuda-10.0/cuda/lib64$ ls -la libcudart.so* libcublas.so* libcufft.so* libcurand.so* libcusolver.so* libcusparse.so*\r\nlrwxrwxrwx 1 thincal thincal        17 Oct 17  2018 libcublas.so -> libcublas.so.10.0\r\nlrwxrwxrwx 1 thincal thincal        21 Oct 17  2018 libcublas.so.10.0 -> libcublas.so.10.0.130\r\n-r-xr-xr-x 1 thincal thincal  70796360 Oct 17  2018 libcublas.so.10.0.130\r\nlrwxrwxrwx 1 thincal thincal        17 Oct 17  2018 libcudart.so -> libcudart.so.10.0\r\nlrwxrwxrwx 1 thincal thincal        21 Oct 17  2018 libcudart.so.10.0 -> libcudart.so.10.0.130\r\n-r-xr-xr-x 1 thincal thincal    495736 Oct 17  2018 libcudart.so.10.0.130\r\nlrwxrwxrwx 1 thincal thincal        16 Oct 17  2018 libcufft.so -> libcufft.so.10.0\r\nlrwxrwxrwx 1 thincal thincal        20 Oct 17  2018 libcufft.so.10.0 -> libcufft.so.10.0.145\r\n-r-xr-xr-x 1 thincal thincal 103177128 Oct 17  2018 libcufft.so.10.0.145\r\nlrwxrwxrwx 1 thincal thincal        17 Oct 17  2018 libcurand.so -> libcurand.so.10.0\r\nlrwxrwxrwx 1 thincal thincal        21 Oct 17  2018 libcurand.so.10.0 -> libcurand.so.10.0.130\r\n-r-xr-xr-x 1 thincal thincal  60806128 Oct 17  2018 libcurand.so.10.0.130\r\nlrwxrwxrwx 1 thincal thincal        19 Oct 17  2018 libcusolver.so -> libcusolver.so.10.0\r\nlrwxrwxrwx 1 thincal thincal        23 Oct 17  2018 libcusolver.so.10.0 -> libcusolver.so.10.0.130\r\n-r-xr-xr-x 1 thincal thincal 139257368 Oct 17  2018 libcusolver.so.10.0.130\r\nlrwxrwxrwx 1 thincal thincal        19 Oct 17  2018 libcusparse.so -> libcusparse.so.10.0\r\nlrwxrwxrwx 1 thincal thincal        23 Oct 17  2018 libcusparse.so.10.0 -> libcusparse.so.10.0.130\r\n-r-xr-xr-x 1 thincal thincal  59078736 Oct 17  2018 libcusparse.so.10.0.130\r\n```\r\n\r\n**the third try**: after making a symbolic link for above missing libraries it is running well now:\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW1228 14:56:19.771065 140537504274176 tpu_cluster_resolver.py:35] Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .\r\nsleep\r\nmain thread end...\r\nsleep\r\nsleep\r\nsleep\r\n2019-12-28 14:56:25.386761: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-28 14:56:25.395346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:83:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2019-12-28 14:56:25.400400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-28 14:56:25.426708: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-28 14:56:25.467068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-28 14:56:25.533422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-28 14:56:25.574739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-28 14:56:25.771502: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-28 14:56:25.836608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-28 14:56:25.839959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2019-12-28 14:56:25.840493: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-28 14:56:25.869541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2100220000 Hz\r\n2019-12-28 14:56:25.869935: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x69e96a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-12-28 14:56:25.869985: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-12-28 14:56:25.991250: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x69ec0e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-12-28 14:56:25.991316: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-12-28 14:56:25.992662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:83:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2019-12-28 14:56:25.992754: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-28 14:56:25.992781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-28 14:56:25.992803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-28 14:56:25.992826: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-28 14:56:25.992848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-28 14:56:25.992870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-28 14:56:25.992893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-28 14:56:25.995265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2019-12-28 14:56:25.995316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-28 14:56:25.997224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-28 14:56:25.997267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0\r\n2019-12-28 14:56:25.997321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N\r\n2019-12-28 14:56:25.999786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1)\r\n2019-12-28 14:56:26.006701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\nsleep\r\n2019-12-28 14:56:26.500355: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\r\n2019-12-28 14:56:26.500427: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:75] Searched for CUDA in the following directories:\r\n2019-12-28 14:56:26.500467: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:78]   ./cuda_sdk_lib\r\n2019-12-28 14:56:26.500496: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:78]   /usr/local/cuda\r\n2019-12-28 14:56:26.500509: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:78]   .\r\n2019-12-28 14:56:26.500520: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:80] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\r\n2019-12-28 14:56:26.502503: I tensorflow/compiler/jit/xla_compilation_cache.cc:242] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n```\r\n", "> could you re-run with environment variable `TF_CPP_VMODULE=status=5`?\r\n\r\n@cheshire \r\n\r\n```\r\n$ python3 tf.py\r\nsleep\r\nsleep\r\nmain thread end...\r\nsleep\r\n2019-12-28 15:29:42.942770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-28 15:29:42.952345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:03:00.0\r\n2019-12-28 15:29:42.955526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-28 15:29:42.994875: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-28 15:29:43.042572: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-12-28 15:29:43.120622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-12-28 15:29:43.172195: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-12-28 15:29:43.208427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-12-28 15:29:43.279940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-28 15:29:43.283625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\nsleep\r\n2019-12-28 15:29:43.284274: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-28 15:29:43.318593: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2099990000 Hz\r\n2019-12-28 15:29:43.319125: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5d6aeb0 executing computations on platform Host. Devices:\r\n2019-12-28 15:29:43.319215: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-12-28 15:29:43.472926: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5dde050 executing computations on platform CUDA. Devices:\r\n2019-12-28 15:29:43.472993: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-12-28 15:29:43.474631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:03:00.0\r\n2019-12-28 15:29:43.474727: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-28 15:29:43.474772: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-28 15:29:43.474812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-12-28 15:29:43.474852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-12-28 15:29:43.474891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-12-28 15:29:43.474928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-12-28 15:29:43.474966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-28 15:29:43.478423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-28 15:29:43.478508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-12-28 15:29:43.480579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-28 15:29:43.480616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-12-28 15:29:43.480655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-12-28 15:29:43.484205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\nsleep\r\n2019-12-28 15:29:45.430201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-12-28 15:29:45.798750: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n2019-12-28 15:29:45.861278: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at xla_compile_on_demand_op.cc:218 : Not found: ./bin/ptxas not found\r\nTraceback (most recent call last):\r\n  File \"tf.py\", line 7, in <module>\r\n    c = tf.linalg.matmul(a, b)\r\n  File \"/home/liusong02/.local/lib/python3.5/site-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/liusong02/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py\", line 2765, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/home/liusong02/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 6126, in mat_mul\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: ./bin/ptxas not found [Op:MatMul] name: MatMul/\r\n```", "> **the third try**: after making a symbolic link for above missing libraries it is running well now:\r\n\r\nJFYI, the correct fix is to install CUDA 10.1 since that's what tf-nightly is built against.", "@sanjoy @cheshire Same issue with **TensorFlow 2.1.0-rc2** and **CUDA 10.1**.", "> 2019-12-30 12:52:15.338250: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at xla_compile_on_demand_op.cc:220 : Not found: ./bin/ptxas not found\r\n\r\nInstead of looking for `./bin/ptxas`, shouldn't it check to see if `ptxas` is available first? In my case:\r\n\r\n```\r\n$ which ptxas\r\n/home/username/.local/cuda-10.1/bin/ptxas\r\n```", "@thincal What are you actually trying to do and why are you using an XLA device? If you just would like your computation to run on GPU, simply removing the `with` line should be sufficient.\r\n\r\n@netw0rkf10w What workflow are you using? \r\n\r\n> Same issue with TensorFlow 2.1.0-rc2 \r\n\r\nSorry I do not know which fix made into which release, I can only answer for behavior on nightly.\r\n\r\n> Instead of looking for ./bin/ptxas, shouldn't it check to see if ptxas is available first? \r\n\r\nIt shouldn't matter in which order do we look for ptxas, if it's in your $PATH, it will be found.", "@thincal \r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "> It shouldn't matter in which order do we look for ptxas, if it's in your $PATH, it will be found.\r\n\r\n@cheshire, actually it matters, I have `ptxas` in my `$PATH`, but the error gone only after I've created symlynk to `./bin/ptxas`.", "> > It shouldn't matter in which order do we look for ptxas, if it's in your $PATH, it will be found.\r\n> \r\n> @cheshire, actually it matters, I have `ptxas` in my `$PATH`, but the error gone only after I've created symlynk to `./bin/ptxas`.\r\n\r\ncan you show how to do that?", "> can you show how to do that?\r\n\r\nThe real `ptxas` path may differ on your system (just for info: I have Debian 10 installation with NVidia stuff from `buster-backports` repository). As far as I understand `tensorflow` library looks for `ptxas` in the `./bin` directory (note that the path is relative, i.e. the current working directory where you start your python script and `ln -s ...` is important). The commands are:\r\n`mkdir ./bin`\r\n`ln -s /usr/bin/ptxas ./bin/ptxas`", "Found a way.. You can pass an XLA flag with cuda location - \r\n```XLA_FLAGS=\"--xla_gpu_cuda_data_dir=/usr/local/cuda-11.0\" python3 ./something.py```\r\n"]}, {"number": 35422, "title": "x/Unknown while training the first epoch.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Code is ran on Google Colab (However, my machine is Windows 10 Version 1903 if that matters)\r\n- TensorFlow installed from (source or binary): No knowledge, using the already installed version on Google Colab\r\n- TensorFlow version (use command below): v2.1.0-rc1-0-g064e1535a7 2.1.0-rc1\r\n- Python version: 3.6.9\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- GPU model and memory: No knowledge, using the given Google Colab hardware.\r\n\r\n**Describe the current behavior**\r\nWhen fitting the model, on the first epoch, it counts steps like \"337/Unknown\", the Unknown stays there until the first epoch fitting is over(at step 582) but that issue gets fixed after the first epoch is over. The text turns to \"582/582\" and it shows like \"x/582\" on the next epochs.\r\n\r\n**Describe the expected behavior**\r\nIt should say like \"x/582\" at the first epoch too.\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb\r\nTry doing the \"Train the model\" part. You should encounter this problem when you start fitting the model.\r\n", "comments": ["@msteknoadam ,\r\nIssue seems to be fixed in the latest `tf-nightly 2.1.0.dev20191226`,can you try using that version?Thanks!", "Uhm, sorry but no, it's still the same. I made sure that I was on version 2.1.0-dev20191226 and it still shows like \"x/Unknown\"", "@msteknoadam Actually, an iterator generates data dynamically. So the length of a dataset iterator is unknown untill you iterate through it. [This resource](https://stackoverflow.com/questions/52264469/determine-number-of-records-in-tf-data-dataset-tensorflow) has more detailed explanation for your question.\r\n\r\nYou could pass `steps_per_epoch` argument to the `model.fit` as shown below. Then, it prints as you are expecting. \r\n```\r\n history = model.fit(train_batches,\r\n                    epochs=initial_epochs,steps_per_epoch=steps_per_epoch,\r\n                    validation_data=validation_batches)\r\n```\r\n[Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/50e34c8bdd605b4fffa88c7b0ab8f4a5/transfer_learning.ipynb) is the gist for your reference. Thanks!\r\n\r\nI am closing this issue as it was resolved. Please feel free to open new issue if you encounter any related issues. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35422\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35422\">No</a>\n", "I had the same problem. My guess is this is due to the fact the code was previous version (before 2.x).\r\nSolution: In the fit function, specify batch_size and steps_per_epoch, and it shall work beautifully. At least, worked for me."]}, {"number": 35421, "title": "Android Tensorflow Lite Crash: When using 'options.setUseNNAPI(true); ' with TFLite in SAMSUNG S10+", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Samsung Galaxy S10+\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0\r\n\r\n**Describe the current behavior**\r\nMy app has crashed...\r\n```log\r\n2019-12-26 20:32:48.140 16058-16801/? A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x64393 in tid 16801 (neuralnetworks@), pid 16058 (neuralnetworks@)\r\n2019-12-26 20:32:48.183 16807-16807/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2019-12-26 20:32:48.183 16807-16807/? A/DEBUG: Build fingerprint: 'samsung/beyond2qlteue/beyond2q:9/PPR1.180610.011/G975U1UES2BSKA:user/release-keys'\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG: Revision: '17'\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG: ABI: 'arm64'\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG: pid: 16058, tid: 16801, name: neuralnetworks@  >>> /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti <<<\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x64393\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x0  ffffffffffffffff  x1  0000000000064393  x2  0000000000000008  x3  0000000000000000\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x4  0000007cd58059b8  x5  0000000000000007  x6  0000000000000003  x7  0000000000000003\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x8  0000000000000000  x9  0000000000000010  x10 0000000000000000  x11 0000000000000000\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x12 0000000000000003  x13 0000000000000000  x14 0000000000000003  x15 aaaaaaaaaaaaaaab\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x16 0000007cd67921c0  x17 0000007cd6715a98  x18 0000000000000010  x19 0000007cd5300f28\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x20 0000000000000001  x21 0000007cd5835e18  x22 00000000000001b0  x23 0000007cd5849578\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x24 0000007cd5301588  x25 0000000000000001  x26 0000000000000000  x27 0000000000000003\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     x28 0000000000000007  x29 0000007cd5300e20\r\n2019-12-26 20:32:48.184 16807-16807/? A/DEBUG:     sp  0000007cd5300d90  lr  0000005ffb94585c  pc  0000005ffb9458a8\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG: backtrace:\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #00 pc 00000000000ac8a8  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::OpTranslation::getTensorParam(int, bool, qti::nnhal::gpu::Permute)+748)\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #01 pc 00000000000a3fbc  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::GraphTranslation::conv(qti::nnhal::gpu::OpTranslation&, DlSystem::NetworkDescriptor&, bool)+184)\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #02 pc 00000000000a8aec  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::GraphTranslation::translateOp(qti::nnhal::IOperation*, qti::nnhal::IGraph*, DlSystem::NetworkDescriptor&)+420)\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #03 pc 00000000000a8c88  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::GraphTranslation::translate(qti::nnhal::IGraph*, DlSystem::NetworkDescriptor&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>>&)+148)\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #04 pc 00000000000a0e38  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::AdrenoModelExecutor::create(qti::nnhal::IGraph*, qti::nnhal::IExecutorContext*)+464)\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #05 pc 00000000000a03e8  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (qti::nnhal::gpu::AdrenoAccelerator::createGraphExecutor(qti::nnhal::IGraph*, qti::nnhal::IExecutorContext*)+16)\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #06 pc 000000000008ea0c  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (android::hardware::neuralnetworks::V1_1::implementation::NnHalExecution::asyncCreateExecutors(android::sp<android::hardware::neuralnetworks::V1_0::IPreparedModelCallback> const&, android::sp<android::hardware::neuralnetworks::V1_0::IPreparedModel>&)+1288)\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #07 pc 000000000009196c  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (android::hardware::neuralnetworks::V1_1::implementation::PreparedModelInitWork::work()+28)\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #08 pc 000000000008a3f4  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti (android::hardware::neuralnetworks::V1_1::implementation::NnHalWorkerThread::Inner::threadFunc()+252)\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #09 pc 000000000008bdcc  /vendor/bin/hw/android.hardware.neuralnetworks@1.1-service-qti\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #10 pc 00000000000919c0  /system/lib64/libc.so (__pthread_start(void*)+36)\r\n2019-12-26 20:32:48.208 16807-16807/? A/DEBUG:     #11 pc 0000000000023fb0  /system/lib64/libc.so (__start_thread+68)\r\n```\r\n\r\n**Code to reproduce the issue**\r\n```options.setUseNNAPI(true);```\r\n\r\n", "comments": ["More info: If I'm not using ```options.setUseNNAPI(true);``` this setting for ```options``` everything will be fine, please tell me is this crash is Tensorflow's bug or SAMSUNG's OS bug?", "Looks like you hit qualcomm (`qti` in your message) NNAPI driver's problem.", "@Edward7Zhang Try using the NNAPI Delegate instead. The examples have recently been updated to use NNAPI delegate instead of `options.setUseNNAPI(true)`.\r\n\r\nCheck this [link](https://github.com/tensorflow/examples/blob/52d3a310df161ddac060ea0533fde1fe4befa93a/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L194)", "> @Edward7Zhang Try using the NNAPI Delegate instead. The examples have recently been updated to use NNAPI delegate instead of `options.setUseNNAPI(true)`.\r\n> \r\n> Check this [link](https://github.com/tensorflow/examples/blob/52d3a310df161ddac060ea0533fde1fe4befa93a/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L194)\r\n\r\nOK,I'll try this, this's seem can resolve my problem!", "```\r\n        Interpreter.Options options= new Interpreter.Options();\r\n        ...\r\n        options.setUseNNAPI(true);\r\n\r\n```\r\nFor me, i resolved this issues with set\r\n```\r\noptions.setUseNNAPI(true);\r\n```"]}, {"number": 35420, "title": "Usage example for tf.linalg.det()", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35420) for more info**.\n\n<!-- need_sender_cla -->", "Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/35420\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n You'll be able to see Jupyter notebook diff and discuss changes. Powered by <a href='https://www.reviewnb.com'>ReviewNB</a>.", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35420) for more info**.\n\n<!-- ok -->", "Hey, could anybody please review this?\r\nThis was a GCI task and I need to complete it and move on to the next task ASAP....", "Can you delete the notebook and add the example to the docstring of that function?\r\n\r\nSee https://www.tensorflow.org/community/contribute/docs_ref for writing docstrings.", "I couldn't find the docstring for that function, would it be okay if I wrote the docstring again and submitted or could you tell me its location?", "The location: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_MatrixDeterminant.pbtxt", "Hey, I've created another PR, could you review that please?\r\nhttps://github.com/tensorflow/tensorflow/pull/35457", "I deleted this repo and created a new one and made a PR from that one.", "@yashk2810 "]}, {"number": 35419, "title": "Cannot use fine-tuned ssd_mobilenet_v2 model with TF Lite on Android", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2 Simulator in Android Studio with Android 7.1.1\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.2\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0/7.5\r\n- GPU model and memory: 1070 max-q 8 GB\r\n\r\n\r\n**Describe the current behavior**\r\nI fine-tuned the ssd_mobilenet_v2 pretrained model from Tensorflow model zoo to detect two classes. Training was done with TF OD API. After converting the model to detect.tflite and put it in the \"assets\" folder of the [official Android demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) and modified labelmap.txt, upon running I got the following error:\r\n```java\r\nE/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 4609\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/detection_postprocess.cc:404 ValidateBoxes(decoded_boxes, num_boxes) was not true.\r\n    Node number 102 (TFLite_Detection_PostProcess) failed to invoke.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:152)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:296)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:193)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:183)\r\n        at android.os.Handler.handleCallback(Handler.java:751)\r\n        at android.os.Handler.dispatchMessage(Handler.java:95)\r\n        at android.os.Looper.loop(Looper.java:154)\r\n        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n```\r\nHowever, if I use model.ckpt-0 which is the initial checkpoint, the converted .tflite works fine. So I assume that the convertion is OK. Is it due to the training process?\r\nPlease help me figure this out.\r\n", "comments": ["A walk around that is found by now is:\r\nuse the saved checkpoint I need, say, model-ckpt-10000, as the initialization of a new train. Then use the model.ckpt-0 to convert to tflite.", "Can you paste the commands you used to convert? If renaming to model.ckpt-0 works for you, there may be some simple bug in the conversion process.", "Having the same problem here. This used to work, now I do not know what changed in my environment (maybe TFLITE interpreter for Android?) I tried 1.15, 2.0 and 2.1.0.\r\n\r\nError: ValidateBoxes(decoded_boxes, num_boxes) was not true when I try to use the SSD converted model.\r\n\r\nCommands I used to convert my checkpoint: python3 object_detection/export_tflite_ssd_graph.py --pipeline_config_path $PIPELINE_PATH --trained_checkpoint_prefix $CHECKPOINT --output_directory $OUTPUT_DIR --add_postprocessing_op true\r\n\r\ntoco --graph_def_file=/tmp/tflite/tflite_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=/tmp/tflite/segm_augm_200k.tflite --input_type=FLOAT --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --input_shapes=1,300,300,3 \u200a--inference_type=FLOAT --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_custom_ops\r\n\r\nI dindn't manage to make it work using the workaround MichalW17 provided, I run a training using my pre-trained checkpoint (renamed to 0) with just one step, and then I convert checkpoint 0 file. Then the model loads in Android but it seems it becomes garbage (it acts like the training process was not done).\r\n\r\n", "Can you try without specifying `--mean_values=128 --std_dev_values=128`? Since the inference is in FLOAT, these parameters are not needed. I am not sure what `change_concat_input_ranges` does.", "Hi! I removed those params, it still crashes with the same error:\r\n\r\n2020-01-10 17:43:04.877 22107-22172/org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 22107\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/detection_postprocess.cc:404 ValidateBoxes(decoded_boxes, num_boxes) was not true.\r\n    Node number 98 (TFLite_Detection_PostProcess) failed to invoke.\r\n\r\nI took this set of params from my own documentation I used a couple of months ago that was working.\r\n\r\nThanks,", "BTW, this issue seems to be duplicated here: https://github.com/tensorflow/models/issues/7918\r\n\r\nWhat's interesting in the other issue report is that the user tried it using Python with the same result.", "Update: Downgraded TFLITE to 1.12.0 in Android and now the model loads and works.\r\nUnfortunatelly to do that I had to disable the GPU delegate I was using for improved mobile performance. \r\nAs I explained this procedure was woking months ago (with GPU delegate), as I was using nightly versions of tlite and tflite-gpu. It seems I updated the nightly version recently and now I have this problem.", "This is weird. If you don't mind, could you try one of the models you obtained with a previous version of Android, with the Interpreter from the latest version? I am wondering if this is a problem with the conversion process or some TF issues in the latest release.", "I can confirm my old models (converted some time ago) work properly using the latest version.", "I had the same issue using the `ssd_mobilenet_v2_quantized_300x300_coco` model. I was able to finetune for a couple dozen steps on my data and then export the graph, convert to tf lite, and use the python tf lite interpreter. But when I finetuned on my own data again for ~200,000 steps and repeated the same process, it failed with the `ValidatedBoxed` error message. I'm on Windows 10 using TF version 1.15.0 (through conda as tensorflow-gpu). **Downgrading to tensorflow-gpu=1.13 got it to work.**\r\n\r\n**UPDATE** Although downgrading got inferences to work using the tf lite model, the output is nonsensical, predicting classes like `3443423` and containing detection boxes with values like `.23, .53, .141, 12` <-- these values should never be greater than 1 since they've been scaled to the size of the input image. ", "I have also the same issue.\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Model: SSD MobileNetV2\r\n- Tensorflow-gpu=2.2 (Installed using conda)\r\n\r\nI fine-tuned the model on my own dataset (11 classes). It has been converted successfully but the mentioned error occurs when I call invoke() method.\r\nI am using Python API of TFLite interpreter.\r\n\r\n**Code:**\r\ntflite_interpreter = tf.compat.v1.lite.Interpreter(model_path=tflite_model_file)\r\ntflite_interpreter.allocate_tensors()\r\ninput_details = tflite_interpreter.get_input_details()\r\noutput_details = tflite_interpreter.get_output_details()\r\nimage = cv2.imread('images.jpeg')\r\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\nimage = cv2.resize(image, (300, 300))\r\nimage = np.expand_dims(image, 0)\r\ntensor = tf.compat.v1.convert_to_tensor(np.array(image) / 255. , dtype='float32')\r\ntflite_interpreter.set_tensor(input_details[0]['index'], tensor)\r\ntflite_interpreter.invoke()", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35419\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35419\">No</a>\n"]}, {"number": 35418, "title": "Add usage example to vgg16.py", "body": "", "comments": ["Umm... you should also add the usage example above the arguments.", "Did it", "Can you please help me understand what is wrong with my code because the ubuntu cpu build failed", "@Ron-Rocks thank you, it is failing doctest can you please check here for [logs](https://source.cloud.google.com/results/invocations/e290df8a-4f4d-4f27-b384-2f1985b2acca/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests).\r\n\r\nPlease run the doctest locally as mentioned here in the [contributor guidelines](https://www.tensorflow.org/community/contribute/docs_ref).", "@Ron-Rocks Any update on this PR, please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@Ron-Rocks Any update on this PR and can you please resolve conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 35417, "title": "AttributeError: module 'tensorflow' has no attribute 'reset_default_graph'", "body": "\r\n- Have I written custom code\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.framework import ops\r\nops.reset_default_graph()\r\n\r\ntf.reset_default_graph()\r\nsession = tf.InteractiveSession()\r\n\r\ni also try :\r\npip uninstall tensorflow\r\nand again\r\npip install tensorflow\r\n\r\nbt still it's showing me :\r\nAttributeError: module 'tensorflow' has no attribute 'reset_default_graph'\r\n\r\n\r\n\r\n\r\n\r\nwhat should I do now\r\nplease help me out\r\n", "comments": []}, {"number": 35416, "title": "Bijectors crash tf2.1 autograph in distributed mirrored multi-gpu mode", "body": "**System information**\r\n* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n* TensorFlow installed from (source or binary): binary\r\n* TensorFlow version (use command below): v2.1.0-rc1-58-g9837ece 2.1.0-rc2 (python3 -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\")\r\n* Python version: Python 3.6.8\r\n* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2\r\n* GPU model and memory: Tesla V100-SXM2-16GB\r\n\r\n**Describe the current behavior**\r\nI'm using Bijectors as a flexible prior for a VAE. \r\n\r\nIn tf2.1 autograph distributed mirrored mode \r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L1152\r\nI am getting\r\n```\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\nwhen running with multiple GPUs (but not when running with single GPU):\r\n```\r\nbijectors = []\r\nfor i in range(16):\r\n    bijectors.append(tfb.MaskedAutoregressiveFlow(\r\n      shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(\r\n          code, hidden_layers=[1024, 1024], name=scope + \"/maf_\" + str(i))))\r\n\r\n    bijectors.append(tfb.BatchNormalization(\r\n        batchnorm_layer=tf.layers.BatchNormalization(\r\n                            name=scope + '/batch_norm_' + str(i)),\r\n        name=scope + '/batch_norm_bijector' + str(i)))\r\n\r\n    permutation=tf.get_variable('permutation_'+str(i), initializer=np.random.permutation(out_channel).astype(\"int32\"), trainable=False)\r\n    bijectors.append(tfb.Permute(permutation))\r\n    \r\nflow_bijector = tfb.Chain(list(reversed(bijectors[:-1])))\r\n```\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190\r\n\r\n**Describe the expected behavior**\r\nShould not crash\r\n\r\n**Code to reproduce the issue**\r\nTF2.x code:\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 134, in <module>\r\n    main()\r\n  File \"main.py\", line 121, in main\r\n    gan.train()\r\n  File \"/app/home/ubuntu/SPADE-Tensorflow.tf2/SPADE.py\", line 1379, in train\r\n    train_loop()\r\n  File \"/app/home/ubuntu/SPADE-Tensorflow.tf2/SPADE.py\", line 1336, in train_loop\r\n    counter, result_inputs, result_losses_det, result_outputs_det, result_outputs_resample_det, result_outputs_random_det, result_outputs_random_gen_det = train_det_grad_both(global_step, self.train_main, *inputs)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2599, in _create_graph_function\r\n    shared_func_graph=False)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1511, in __init__\r\n    func_graph, self._attrs, self._garbage_collector)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 601, in __init__\r\n    self._func_graph.inputs, self._func_graph.outputs, attrs)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 466, in __init__\r\n    function_def.ParseFromString(compat.as_bytes(proto_data))\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\n[train.CelebAMask-HQ.tf21.4xgpu.maf.log](https://github.com/tensorflow/tensorflow/files/4001259/train.CelebAMask-HQ.tf21.4xgpu.maf.log)\r\n", "comments": ["This is an internal TF bug.\r\n\r\nCan you give me a self-contained example to reproduce? Like, paste the code you need to trigger the error in a colab.research.google.com ?", "I will try to bisect my network into smallest possible example once these issues are resolved:\r\nhttps://github.com/tensorflow/tensorflow/issues/35346\r\nhttps://github.com/tensorflow/tensorflow/issues/35415\r\nATM I would have to wait for 30 minutes for each run of my network in multi-gpu mode.", "Currently, we are unable to reproduce the issue.Please update the issue with proper details, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35416\">No</a>\n"]}, {"number": 35415, "title": "Bijectors are orders of magnitude slower in tf2.1 autograph distributed mirrored single-gpu mode", "body": "**System information**\r\n* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n* TensorFlow installed from (source or binary): binary\r\n* TensorFlow version (use command below): v2.1.0-rc1-58-g9837ece 2.1.0-rc2 (python3 -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\")\r\n* Python version: Python 3.6.8\r\n* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2\r\n *GPU model and memory: Tesla V100-SXM2-16GB\r\n\r\n**Describe the current behavior**\r\nI'm using Bijectors as a flexible prior for a VAE. \r\n\r\nThis code has negligible overhead in tf1.x (for input batch size 18x256x256x3). In tf2.1 autograph distributed mirrored mode \r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L1152\r\nwith single GPU it increases training step duration 1 second (tf1.x) -> 1.9 seconds (tf2.1):\r\n```\r\nbijectors = []\r\nfor i in range(16):\r\n    bijectors.append(tfb.MaskedAutoregressiveFlow(\r\n      shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(\r\n          code, hidden_layers=[1024, 1024], name=scope + \"/maf_\" + str(i))))\r\n\r\n    bijectors.append(tfb.BatchNormalization(\r\n        batchnorm_layer=tf.layers.BatchNormalization(\r\n                            name=scope + '/batch_norm_' + str(i)),\r\n        name=scope + '/batch_norm_bijector' + str(i)))\r\n\r\n    permutation=tf.get_variable('permutation_'+str(i), initializer=np.random.permutation(out_channel).astype(\"int32\"), trainable=False)\r\n    bijectors.append(tfb.Permute(permutation))\r\n    \r\nflow_bijector = tfb.Chain(list(reversed(bijectors[:-1])))\r\n```\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190\r\n\r\nI'm using custom masked autoregressive template\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/masked_autoregressive.py\r\nbut it is as slow with the default one:\r\nhttps://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/masked_autoregressive_default_template\r\n\r\nPossible suspects:\r\n```\r\ntfb.masked_dense\r\n```\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/masked_autoregressive.py#L44\r\n\r\n```\r\ntf1.make_template()\r\n```\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/masked_autoregressive.py#L115\r\n\r\n**Describe the expected behavior**\r\nPerformance in tf2.1 and tf1.x should be comparable.\r\n\r\n**Code to reproduce the issue**\r\nTF2.x code:\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190\r\n\r\nTF1.x code:\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/develop/SPADE.py#L190\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["olegmyrk@ Thank you for posting a detailed summarization! To debug performance issues such as the one above we need [timeline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py) traces. Can you post the following traces:\r\ntrace1: TF 1.x without distribution strategy\r\ntrace2: TF 2.x without distribution strategy\r\ntrace3: TF2.x with 1 GPU Mirrored strategy\r\nDoes the performance of trace1 and trace2 match? If they do we can then look at trace2 and trace3. ", "Here is trace1 with TF 1.x\r\n[timeline.gz](https://github.com/tensorflow/tensorflow/files/4006727/timeline.gz)\r\n\r\nHere is trace3 with TF2.x with 1 GPU Mirrored strategy\r\n[trace.gz](https://github.com/tensorflow/tensorflow/files/4006729/trace.gz)\r\n\r\nPlease note that adding tracing makes training significantly slower on its own (especially in TF1.x).", "I have managed to create a minimalistic TF1 and TF2 scripts to demonstrate the issue. As you can see from the logs that building graph in TF1 is 2x faster than in TF2 single-gpu mirrored mode and 10x faster than in TF2 multi-gpu mirrored mode. \r\n\r\nNote that in TF2 code there is a commented-out line of code that also makes the training step 2x slower in multi-gpu mirrored mode.\r\n\r\nCUDA_VISIBLE_DEVICES=0 time python3 test_maf_tf1.py\r\n```\r\n...\r\nstep: 0\r\n2020-01-01 18:28:19.660927: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\nresult -6955.917\r\nduration: 12.701881170272827\r\nstep: 1\r\nresult -6996.775\r\nduration: 0.745488166809082\r\nstep: 2\r\nresult -7037.4297\r\n...\r\n```\r\n\r\nCUDA_VISIBLE_DEVICES=0 time python3 test_maf_tf2.py\r\n```\r\n...\r\nstep: 0\r\nresult tf.Tensor(-6955.917, shape=(), dtype=float32)\r\nduration: 28.593788623809814\r\nstep: 1\r\nresult tf.Tensor(-6956.736, shape=(), dtype=float32)\r\nduration: 0.6307415962219238\r\nstep: 2\r\nresult tf.Tensor(-6957.5557, shape=(), dtype=float32)\r\nduration: 0.5865404605865479\r\n...\r\n```\r\n\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 time python3 test_maf_tf2.py\r\n```\r\n...\r\nstep: 0\r\nresult tf.Tensor(-6955.917, shape=(), dtype=float32)\r\nduration: 189.2380485534668\r\nstep: 1\r\nresult tf.Tensor(-6956.736, shape=(), dtype=float32)\r\nduration: 36.806933879852295\r\nstep: 2\r\nresult tf.Tensor(-6957.5557, shape=(), dtype=float32)\r\nduration: 0.7145941257476807\r\n...\r\n```\r\n\r\nTF1 script:\r\n```\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\ntfd = tfp.distributions\r\ntfb = tfp.bijectors\r\n\r\ndef dist(loc, diag):\r\n    channel = int(loc.get_shape()[-1])\r\n    bijectors = []\r\n    for i in range(16):\r\n      bijectors.append(tfb.MaskedAutoregressiveFlow(\r\n        shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(\r\n            hidden_layers=[1024, 1024], name=\"maf_\" + str(i))))\r\n\r\n      bijectors.append(tfb.BatchNormalization(\r\n          batchnorm_layer=tf.layers.BatchNormalization(\r\n                              name='batch_norm_' + str(i)),\r\n          name='batch_norm_bijector' + str(i)))\r\n\r\n      permutation=tf.get_variable('permutation_'+str(i), dtype=tf.int32, initializer=np.random.permutation(channel).astype(\"int32\"), trainable=False)\r\n      bijectors.append(tfb.Permute(permutation))\r\n\r\n    flow_bijector = tfb.Chain(list(reversed(bijectors[:-1])))\r\n\r\n    mvn_dist = tfd.MultivariateNormalDiag(loc, diag, name='MultivariateNormalDiag')\r\n\r\n    dist = tfd.TransformedDistribution(\r\n                    distribution=mvn_dist,\r\n                    bijector=flow_bijector\r\n                )\r\n    return dist\t\r\n    \r\ndef compute():\r\n    x = tf.placeholder(tf.float32, [None, 128])\r\n    x_dist = dist(tf.zeros_like(x), tf.ones_like(x))\r\n    y = x_dist.sample()\r\n    #d = -tf.reduce_mean(x_dist.log_prob(y))\r\n    d = -tf.reduce_mean(x_dist.log_prob(x))\r\n    return x, d, y\r\n\r\nwith tf.Session() as sess:\r\n  x, d, y = compute()\r\n  optim = tf.train.AdamOptimizer(0.01).minimize(d)\r\n  tf.global_variables_initializer().run()\r\n  for i in range(0,100):\r\n      start = time.time()\r\n      print(\"step:\", i)\r\n      v = np.zeros((64,128),np.float32)\r\n      result_d, result_y, _ = sess.run([d, y, optim], feed_dict={ x : v })\r\n      print(\"result\", result_d)\r\n      print(\"duration:\", time.time()-start)  \r\n```\r\n\r\nTF2 script:\r\n```\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\ntfd = tfp.distributions\r\ntfb = tfp.bijectors\r\n\r\ndistribute_strategy = tf.distribute.MirroredStrategy()\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(([np.zeros((128,),np.float32)]))\r\ndataset = dataset.repeat(None).batch(64)\r\ndataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\ndataset = distribute_strategy.experimental_distribute_dataset(dataset)\r\ndataset = iter(dataset)\r\n\r\nwith distribute_strategy.scope():\r\n  optim = tf.keras.optimizers.Adam(0.0002)\r\n\r\n  def dist(loc, diag):\r\n    channel = loc.get_shape()[-1]\r\n    bijectors = []\r\n    for i in range(16):\r\n      bijectors.append(tfb.MaskedAutoregressiveFlow(\r\n        shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(\r\n            hidden_layers=[1024, 1024], name=\"maf_\" + str(i))))\r\n\r\n      bijectors.append(tfb.BatchNormalization(\r\n          batchnorm_layer=tf.compat.v1.layers.BatchNormalization(\r\n                              name='batch_norm_' + str(i)),\r\n          name='batch_norm_bijector' + str(i)))\r\n\r\n      permutation=tf.compat.v1.get_variable('permutation_'+str(i), dtype=tf.int32, initializer=np.random.permutation(channel).astype(\"int32\"), trainable=False)\r\n      bijectors.append(tfb.Permute(permutation))\r\n    \r\n    flow_bijector = tfb.Chain(list(reversed(bijectors[:-1])))\r\n\r\n    mvn_dist = tfd.MultivariateNormalDiag(loc, diag, name='MultivariateNormalDiag')\r\n\r\n    dist = tfd.TransformedDistribution(\r\n                    distribution=mvn_dist,\r\n                    bijector=flow_bijector\r\n                )\r\n    return dist\r\n\r\n  def compute(x):\r\n    x_dist = dist(tf.zeros_like(x), tf.ones_like(x))\r\n    y = x_dist.sample()\r\n    #I don't need this, but it makes the training step 2x slower in multi-gpu mode\r\n    #d = -tf.reduce_mean(x_dist.log_prob(y))\r\n    d = -tf.reduce_mean(x_dist.log_prob(x))\r\n    return d, y\r\n    \r\n  @tf.function \r\n  def train(x):\r\n    def train_fn(x):\r\n      with tf.GradientTape(persistent=True) as tape:\r\n        d,y  = compute(x)\r\n      vars = tf.compat.v1.trainable_variables()\r\n      g = tape.gradient(d,vars)\r\n      optim.apply_gradients(zip(g, vars))\r\n      return d, y\r\n    rd,ry = distribute_strategy.experimental_run_v2(train_fn, args=(x,))\r\n    result = tf.reduce_mean(distribute_strategy.experimental_local_results(rd))\r\n    return result\r\n\r\n  for i in range(0,100):\r\n      start = time.time()\r\n      print(\"step:\", i)\r\n      x = next(dataset)\r\n      result = train(x)\r\n      print(\"result\", result)\r\n      print(\"duration:\", time.time()-start)\r\n```", "Could you please test the performance in latest Tensorflow version, since many of the experimental modules are moved to stable and there could be improvement in performance. Refer [this](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) document for details. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35414, "title": "no such package '@com_google_protobuf error while running bazel build", "body": "**System information**\r\n- OS Platform and Distribution: Windows10 x64\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.7\r\n- Installed using: conda\r\n- Bazel version (if compiling from source): 1.1/2.0\r\n- CUDA/cuDNN version: 10.0\r\n\r\nI was trying to inspect a model following the guide [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#inspecting-graphs)\r\nWhen running the command\r\n```bash\r\nbazel build tensorflow/tools/graph_transforms:summarize_graph\r\n```\r\nit failed with these logs\r\n```\r\nINFO: Writing tracer profile to 'C:/users/yy/_bazel_yy/zxtlmlwl/command.profile.gz'\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/YY/Anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from e:\\tensorflow\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file e:\\tensorflow\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file e:\\tensorflow\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --incompatible_windows_native_test_wrapper --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file e:\\tensorflow\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Call stack for the definition of repository 'com_google_protobuf' which is a tf_http_archive (rule definition at E:/tensorflow/tensorflow/third_party/repo.bzl:121:19):\r\n - E:/tensorflow/tensorflow/tensorflow/workspace.bzl:457:5\r\n - E:/tensorflow/tensorflow/WORKSPACE:19:1\r\nINFO: Repository 'com_google_protobuf' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\r\nIf the definition of 'com_google_protobuf' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'com_google_protobuf':\r\n   Traceback (most recent call last):\r\n        File \"E:/tensorflow/tensorflow/third_party/repo.bzl\", line 101\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, <1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(<1 more arguments>)\r\nNon-zero return code(2) when executing 'C:\\Windows\\system32\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf\" \"-i\" \"E:/tensorflow/tensorflow/third_party/protobuf/protobuf.patch\"':\r\nStdout:\r\nStderr: patch: **** Can't change to directory C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf : No such file or directory\r\nERROR: Analysis of target '//tensorflow/tools/graph_transforms:summarize_graph' failed; build aborted: no such package '@com_google_protobuf//': Traceback (most recent call last):\r\n        File \"E:/tensorflow/tensorflow/third_party/repo.bzl\", line 101\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, <1 more arguments>)\r\n        File \"E:/tensorflow/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(<1 more arguments>)\r\nNon-zero return code(2) when executing 'C:\\Windows\\system32\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf\" \"-i\" \"E:/tensorflow/tensorflow/third_party/protobuf/protobuf.patch\"':\r\nStdout:\r\nStderr: patch: **** Can't change to directory C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf : No such file or directory\r\nINFO: Elapsed time: 3.975s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n    currently loading: tensorflow\r\n```\r\n\r\nI've tried diferent versions of bazel, 0.20, 1.1.0, and 2.0.0\uff0cand ```bazel clean```, the error is still there.\r\nWhat makes me confused is that the \" Can't change to directory C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf : No such file or directory\", which in fact I can find the dir at that path\r\n\r\nI've checked other similar issue, but none can fix this error\r\n", "comments": ["Same issue: tf ver 2.0, Bazel version 0.29.1\r\n\r\n`bazel build -c opt --cxxopt='--std=c++11' tensorflow/lite/tools/benchmark:benchmark_model`\r\n\r\nResult:\r\n\r\n```\r\n`INFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=135\r\nINFO: Reading rc options for 'build' from c:\\users\\user\\desktop\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/user/AppData/Local/Programs/Python/Python37-32/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\user\\desktop\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\user\\desktop\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file c:\\users\\user\\desktop\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\user\\desktop\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Call stack for the definition of repository 'com_google_protobuf' which is a tf_http_archive (rule definition at C:/users/user/desktop/tensorflow/third_party/repo.bzl:134:19):\r\n - C:/users/user/desktop/tensorflow/tensorflow/workspace.bzl:480:5\r\n - C:/users/user/desktop/tensorflow/tensorflow/workspace.bzl:480:5\r\n - C:/users/user/desktop/tensorflow/WORKSPACE:26:1\r\nINFO: Repository 'com_google_protobuf' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\r\nIf the definition of 'com_google_protobuf' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'com_google_protobuf':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/user/desktop/tensorflow/third_party/repo.bzl\", line 110\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"C:/users/user/desktop/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, <1 more arguments>)\r\n        File \"C:/users/user/desktop/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(<1 more arguments>)\r\nNon-zero return code(2) when executing 'C:\\WINDOWS\\system32\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/user/_bazel_user/e7jbxsyy/external/com_google_protobuf\" \"-i\" \"C:/users/user/desktop/tensorflow/third_party/protobuf/protobuf.patch\"':\r\nStdout:\r\nStderr: patch: **** Can't change to directory C:/users/user/_bazel_user/e7jbxsyy/external/com_google_protobuf : No such file or directory\r\nERROR: Analysis of target '//tensorflow/lite/tools/benchmark:benchmark_model' failed; build aborted: no such package '@com_google_protobuf//': Traceback (most recent call last):\r\n        File \"C:/users/user/desktop/tensorflow/third_party/repo.bzl\", line 110\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"C:/users/user/desktop/tensorflow/third_party/repo.bzl\", line 68, in _apply_patch\r\n                _execute_and_check_ret_code(ctx, <1 more arguments>)\r\n        File \"C:/users/user/desktop/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code\r\n                fail(<1 more arguments>)\r\nNon-zero return code(2) when executing 'C:\\WINDOWS\\system32\\bash.exe -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/user/_bazel_user/e7jbxsyy/external/com_google_protobuf\" \"-i\" \"C:/users/user/desktop/tensorflow/third_party/protobuf/protobuf.patch\"':\r\nStdout:\r\nStderr: patch: **** Can't change to directory C:/users/user/_bazel_user/e7jbxsyy/external/com_google_protobuf : No such file or directory\r\nINFO: Elapsed time: 17.293s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n    currently loading: tensorflow`\r\n```", "I have this exact same issue with Tensorflow 2.0 and bazel 2.0.0. The directory in question seems to exist.", "I have the same issue with TensorFlow on the master branch and bazel 2.0.0 on Windows 10 Pro 1909.", "> I have the same issue with TensorFlow on the master branch and bazel 2.0.0 on Windows 10 Pro 1909.\r\n\r\nIt seems that we should use a path scheme in bash. For example, '/mnt/c/Users/...' instead of 'C:/users/...' Then, I got the following output:\r\n```\r\npatching file BUILD\r\nHunk #2 succeeded at 208 (offset -1 lines).\r\nHunk #3 succeeded at 219 (offset -1 lines).\r\n(Stripping trailing CRs from patch; use --binary to disable.)\r\npatching file protobuf.bzl\r\n```", "> > I have the same issue with TensorFlow on the master branch and bazel 2.0.0 on Windows 10 Pro 1909.\r\n> \r\n> It seems that we should use a path scheme in bash. For example, '/mnt/c/Users/...' instead of 'C:/users/...' Then, I got the following output:\r\n> \r\n> ```\r\n> patching file BUILD\r\n> Hunk #2 succeeded at 208 (offset -1 lines).\r\n> Hunk #3 succeeded at 219 (offset -1 lines).\r\n> (Stripping trailing CRs from patch; use --binary to disable.)\r\n> patching file protobuf.bzl\r\n> ```\r\n\r\nThe following is my ad-hoc patches to this problem. \r\n```\r\ndiff --git a/third_party/py/python_configure.bzl b/third_party/py/python_configure.bzl\r\nindex 6e9a22f806..481e42c93d 100644\r\n--- a/third_party/py/python_configure.bzl\r\n+++ b/third_party/py/python_configure.bzl\r\n@@ -133,6 +133,7 @@ def _get_python_lib(repository_ctx, python_bin):\r\n def _check_python_lib(repository_ctx, python_lib):\r\n     \"\"\"Checks the python lib path.\"\"\"\r\n     cmd = 'test -d \"%s\" -a -x \"%s\"' % (python_lib, python_lib)\r\n+    cmd = cmd.replace(\"C:/Users\", \"/mnt/c/Users\")\r\n     result = raw_exec(repository_ctx, [get_bash_bin(repository_ctx), \"-c\", cmd])\r\n     if result.return_code == 1:\r\n         auto_config_fail(\"Invalid python library path: %s\" % python_lib)\r\n@@ -140,6 +141,7 @@ def _check_python_lib(repository_ctx, python_lib):\r\n def _check_python_bin(repository_ctx, python_bin):\r\n     \"\"\"Checks the python bin path.\"\"\"\r\n     cmd = '[[ -x \"%s\" ]] && [[ ! -d \"%s\" ]]' % (python_bin, python_bin)\r\n+    cmd = cmd.replace(\"C:/Users\", \"/mnt/c/Users\")\r\n     result = raw_exec(repository_ctx, [get_bash_bin(repository_ctx), \"-c\", cmd])\r\n     if result.return_code == 1:\r\n         auto_config_fail(\"--define %s='%s' is not executable. Is it the python binary?\" % (\r\ndiff --git a/third_party/repo.bzl b/third_party/repo.bzl\r\nindex a4d2b899f8..aaba57e37a 100644\r\n--- a/third_party/repo.bzl\r\n+++ b/third_party/repo.bzl\r\n@@ -26,7 +26,7 @@ def _wrap_bash_cmd(ctx, cmd):\r\n         bazel_sh = _get_env_var(ctx, \"BAZEL_SH\")\r\n         if not bazel_sh:\r\n             fail(\"BAZEL_SH environment variable is not set\")\r\n-        cmd = [bazel_sh, \"-l\", \"-c\", \" \".join([\"\\\"%s\\\"\" % s for s in cmd])]\r\n+        cmd = [bazel_sh, \"-l\", \"-c\", \" \".join([\"\\\"%s\\\"\" % s for s in cmd]).replace(\"C:/users\", \"/mnt/c/Users\")]\r\n     return cmd\r\n\r\n def _get_env_var(ctx, name):\r\n```\r\nAnd I faced another kind of problem. Not sure that it is related to the original problem or above patches:\r\n```\r\nINFO: Analyzed target //tensorflow/tools/graph_transforms:transform_graph (142 packages loaded, 8410 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base C:/users/ruddy/_bazel_ruddy/tkn72bxt/sandbox\r\nERROR: C:/users/ruddy/_bazel_ruddy/tkn72bxt/external/snappy/BUILD.bazel:89:1: Executing genrule @snappy//:snappy_stubs_public_h failed (Exit 1)\r\n/bin/bash: source external/bazel_tools/tools/genrule/genrule-setup.sh; sed -e 's/${\\(.*\\)_01}/\\1/g' -e 's/${SNAPPY_MAJOR}/1/g' -e 's/${SNAPPY_MINOR}/1/g' -e 's/${SNAPPY_PATCHLEVEL}/4/g' external/snappy/snappy-stubs-public.h.in >bazel-out/x64_windows-opt/bin/external/snappy/snappy-stubs-public.h: bad substitution\r\nTarget //tensorflow/tools/graph_transforms:transform_graph failed to build\r\nERROR: C:/users/ruddy/source/repos/ruddyscent/tensorflow/tensorflow/core/framework/BUILD:1098:1 Executing genrule @snappy//:snappy_stubs_public_h failed (Exit 1)\r\nINFO: Elapsed time: 15.628s, Critical Path: 0.74s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```", "> I have the same issue with TensorFlow on the master branch and bazel 2.0.0 on Windows 10 Pro 1909.\r\n\r\nI eventually have been succeeded in compiling TensorFlow. The problem was the incompatibility of Bazel to bash on Windows as stated at https://docs.bazel.build/versions/master/windows.html.\r\n\r\nI removed the 'Windows Subsystem for Linux' to remove bash from Windows, and then the official build procedure works as stated.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35414\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35414\">No</a>\n"]}, {"number": 35413, "title": "vgg19.preprocess_input doesn't work in TF2.1 autograph mode", "body": "**System information**\r\n\r\n* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n* TensorFlow installed from (source or binary): binary\r\n* TensorFlow version (use command below): v2.1.0-rc1-58-g9837ece 2.1.0-rc2 (python3 -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\")\r\n* Python version: Python 3.6.8\r\n* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: 10.2, cuDNN 7.6.2\r\n* GPU model and memory: Tesla V100-SXM2-16GB\r\n\r\n**Describe the current behavior**\r\nRunning \r\n```\r\ntensorflow.keras.applications.vgg19.preprocess_input\r\n```\r\ninside @tf.function results in exception:\r\n```\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: VGGLoss/Const:0\r\n```\r\n\r\n**Describe the expected behavior**\r\nShould work the same as in tf1.x\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/develop/vgg19_keras.py#L15\r\n\r\n**Code to reproduce the issue**\r\n```self.vgg_loss = VGGLoss() ```\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L631\r\n\r\n```g_nondet_vgg_loss = self.vgg_loss(fake_nondet_x_output, real_x)```\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L776\r\n\r\n```g_det_vgg_loss = self.vgg_loss(real_x, fake_det_x_stats[0][0])```\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L784\r\n\r\nThis is the line that fails:\r\n```\r\nx_vgg, y_vgg = self.vgg(preprocess_input(x)), self.vgg(preprocess_input(y))\r\n```\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/vgg19_keras.py#L15\r\n\r\nI also tried to patch the code for preprocess_input myself\r\nhttps://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/vgg19_keras.py#L62\r\nIt somewhat works but judging by the scale of the VGG loss some input normalization is off.\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 134, in <module>\r\n    main()\r\n  File \"main.py\", line 121, in main\r\n    gan.train()\r\n  File \"/app/home/ubuntu/SPADE-Tensorflow.tf2/SPADE.py\", line 1180, in train\r\n    build()\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 76, in quick_execute\r\n    raise e\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\r\n    num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: VGGLoss/Const:0\r\n```", "comments": ["@olegmyrk I am using tf-nightly and I am not running into any error. Please take a look at the [gist](https://colab.sandbox.google.com/gist/gowthamkpr/7464f8e197fb9f2917f07639ce819aa8/style_transfer.ipynb). Thanks!", "Here is the smallest code that can reproduce the bug in v2.1.0-rc1-58-g9837ece 2.1.0-rc2:\r\n```\r\nimport numpy as np \r\nimport tensorflow as tf \r\n \r\ndataset1 = tf.data.Dataset.from_tensor_slices(([np.zeros((1,224,224,3),np.float32)]))\r\n \r\n@tf.function \r\ndef run1(x): \r\n  with tf.name_scope(\"preprocess_input\"):\r\n    return tf.keras.applications.vgg19.preprocess_input(x)\r\n \r\nfor x in dataset1: \r\n  result = run1(x) \r\n  print(result)\r\n\r\ndataset2 = tf.data.Dataset.from_tensor_slices(([np.zeros((1,224,224,3),np.float32)])) \r\n \r\n@tf.function \r\ndef run2(x): \r\n  return tf.keras.applications.vgg19.preprocess_input(x)\r\n \r\nfor x in dataset2: \r\n  result = run2(x) \r\n  print(result)\r\n```\r\n\r\nIt is different from what my codes does but results in the same error:\r\n```\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: preprocess_input/Const:0\r\n```", "I am not running into any error using tf-nightly. Please find my gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/315b4908917a55858f0f0ef03f219bbf/untitled270.ipynb).", "Fails in TF 2.1.0:\r\nhttps://colab.research.google.com/gist/olegmyrk/7b3254a35f0a104fe83f1f60bf693ac0/untitled0.ipynb", "@olegmyrk I agree its failing in TF 2.1.0 but it has been fixed in the nightly version. Please find my gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/d3ec266927b357929cc2d2dc88e322a8/copy-of-untitled0.ipynb). It bug fix will be shown in the next stable version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35413\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35413\">No</a>\n"]}, {"number": 35412, "title": "some problem about pb model", "body": "when i want to load my pb modekl, i met a probelm that \"raise ValueError(\"callback %s is not found\" % token) ValueError: callback pyfunc_31 is not found\"\r\n\r\nwhat is the pyfunc_31, why i can't load it, and how can i to slove it?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nPlease, provide simple standalone code to reproduce the issue in our environment.It helps in localizing the issue faster. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@MrCrazyCrab \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35411, "title": "My GPU doesn't support CUDA", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): (I've use this) pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: GeForce GTX 745\r\n\r\n\r\nWhen I want to run test code I get error \"ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\"\r\n\r\nI know that it is caused by lack of CUDA, but my GPU doesn't support it. What can I do with it?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Ja/PycharmProjects/tf/tf.py\", line 10, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Ja\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n```\r\n\r\n\r\n", "comments": ["@kacpo55, \r\nI recommend you to uninstall older tensorflow and python version and install new tensorflow 1.12 and python. It worked for many users. Some time when you have installed different versions of python and tensorflow, there will be some modules that could affect reinstalling of TF. Follow the official [website](https://www.tensorflow.org/install/pip?lang=python3) to install Tensorflow. Let us know if that resolves. Thanks!", "@kacpo55,\r\nIf your GPU doesn't support CUDA. You can install CPU version of Tensorflow. Thanks!", "I thought that I installed CPU version. I fixed it by unistalling python and tensorflow and then installing newer version of tensorflow. Now I'm using tensorflow 2.0.2.\r\nThanks @gadagashwini"]}, {"number": 35410, "title": "Patch 2", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35410) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 35409, "title": "ImportError: DLL load failed with error code -1073741795", "body": "Traceback (most recent call last):\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"fas.py\", line 3, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Hieu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: DLL load failed with error code -1073741795", "comments": ["@hieuvominh, \r\nI suspect that your cpu does not support AVX instructtion sets.\r\nSee [Hardware Requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements).\r\nCan you please confirm?\r\nSee also [#19584](https://github.com/tensorflow/tensorflow/issues/19584)", "@hieuvominh, let us know if the issue is resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35409\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35409\">No</a>\n"]}, {"number": 35408, "title": "Cannot build gl_delegate of android", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04.3 LTS, 64bits\r\n- TensorFlow installed from: https://github.com/tensorflow/tensorflow, \r\n- TensorFlow version: 8e8fabfee3\r\n- Python version: Python 3.6.9 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 1.1.0\r\n- GCC/Compiler version (if compiling from source): Not compiled from source\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI can build the libtensorflow-lite.a. by \r\n\r\n`./tensorflow/lite/tools/make/build_aarch64_lib.sh`\r\n\r\nBut cannot build the gl_delegate by\r\n\r\n`bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:gl_delegate`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Install tensorflow and build the libtensorflow-lite.a by the instructions from [here](https://www.tensorflow.org/lite/guide/build_arm64)\r\n2. Install bazel by the instructions from [here](https://docs.bazel.build/versions/master/install-ubuntu.html) \r\n3. Install anaconda, create environment\r\n`conta create --name tensorflow python=3.6`\r\n4. Install tensorflow and others tools by conda\r\n5. Build gl_delegate by `bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:gl_delegate`\r\n\r\n**Any other info / logs**\r\n\r\n> NFO: Writing tracer profile to '/home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/command.profile.gz'\r\n> INFO: Options provided by the client:\r\n>   Inherited 'common' options: --isatty=1 --terminal_columns=111\r\n> INFO: Reading rc options for 'build' from /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc:\r\n>   'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --config=v2\r\n> INFO: Found applicable config definition build:v2 in file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n> INFO: Found applicable config definition build:android_arm64 in file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n> INFO: Found applicable config definition build:android in file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n> INFO: Found applicable config definition build:linux in file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\n> INFO: Found applicable config definition build:dynamic_kernels in file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n> DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\n> DEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n>  - /home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n>  - /home/yyyy/Qt/3rdLibs/tensorflow/WORKSPACE:37:1\r\n> ERROR: /home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'arm64-v8a'\r\n> ERROR: Analysis of target '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted\r\n> INFO: Elapsed time: 0.450s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (1 packages loaded, 1 target configured)", "comments": ["@stereomatchingkiss do you have Android NDK configured properly? if you are not sure, start from `./configure` again.", "Also, if you're building for Android, using `./tensorflow/lite/tools/make/build_aarch64_lib.sh` is probably not what you want. You should run `/.configure`' per the suggestion from @freedomtan, and then use the shared libraries for either `lite:libtensorflowlite.so` or `lite/c:libtensorflowlite_c.so`.", "Thanks to both of you, I build the gl_delegate successfully.\r\n\r\n>./tensorflow/lite/tools/make/build_aarch64_lib.sh\r\n\r\nWhat should I do if this is not the right way?Or you mean I should run ./configure before I run \"./tensorflow/lite/tools/make/build_aarch64_lib.sh\"?", "> What should I do if this is not the right way?\r\n\r\nThe main question is, do you *really* need a static library (generated by the Makefile script), or can you use a shared library (built via bazel)? `./configure` is necessary for bazel builds targeting Android, but shouldn't be required when using the Makefile-based build scripot.", ">The main question is, do you really need a static library (generated by the Makefile script)\r\n\r\nNo. I use the Makefile script because\r\nthe [doc](https://www.tensorflow.org/lite/guide/build_arm64) at here tell us to build the lib like that, not  a recommend way?\r\n\r\nps : I prefer static lib since it could generate smaller apk, if it is not too trouble to use", "Ah, right, that guide is generally for *non-Android* ARM64 builds, where it's more difficult to cross-compile with bazel. We're working on a revamped C++ guide, for Android, which should make it easier to get started if you prefer or need to use the C/C++ APIs on Android, rather than the existing prebuilt Java AAR libs.", "> We're working on a revamped C++ guide, for Android\r\n\r\nThanks, how long do you think this would come out?\r\n\r\n> if you prefer or need to use the C/C++ APIs on Android\r\n\r\nI develop cross-platform app by Qt5, so c/c++ api are much easier to use than java api for me", "> \r\n> \r\n> Ah, right, that guide is generally for _non-Android_ ARM64 builds, where it's more difficult to cross-compile with bazel. We're working on a revamped C++ guide, for Android, which should make it easier to get started if you prefer or need to use the C/C++ APIs on Android, rather than the existing prebuilt Java AAR libs.\r\n\r\nTried to figure out how to build by myself but it don't work so far. Almost every deep learning libraries are written by c++ but almost every deep learning treat c++ as 3rd class citizens, sigh.", "Expect a much more detailed and useful guide by the end of January. Thanks for your patience on this.", "@stereomatchingkiss We see that you are using older version of tensorflow .Many bug have been fixed in latest version. We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35408\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35408\">No</a>\n"]}, {"number": 35407, "title": "At Runtime: Error while reading resource variable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.2/7\r\n- GPU model and memory: RTX 2060 \r\n\r\n**Describe the current behavior**\r\nI am getting this error when trying to modify https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py:\r\n\r\n```\r\nError while reading resource variable _AnonymousVar35 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar35/class tensorflow::Var does not exist.\r\n\t [[node mul_1/ReadVariableOp (defined at C:\\Users\\Harry\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_8343]\r\n\r\nFunction call stack:\r\nkeras_scratch_graph\r\n```\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfrom keras.models import Model\r\nfrom keras.layers import Lambda, Dense, LSTM, Activation, Input, Bidirectional, Dropout, Reshape, Conv2DTranspose, TimeDistributed, Conv1D\r\nfrom keras.layers.advanced_activations import LeakyReLU\r\nfrom keras.optimizers import Adam\r\nimport keras.backend as K\r\nfrom keras.layers.merge import _Merge\r\nimport librosa\r\nimport tensorflow as tf\r\nfrom functools import partial\r\nimport sys\r\nimport os\r\nimport numpy as np\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n\r\nclass RandomWeightedAverage(_Merge):\r\n    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\r\n    def _merge_function(self, inputs):\r\n        alpha = K.random_uniform((32, 1, 1, 1))\r\n        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\r\n\r\nclass WGANGP():\r\n    def __init__(self):\r\n        self.latent_dim = 100\r\n        self.d = 64\r\n        self.c = 16\r\n        self.a = 1\r\n        self.Fs = 44100\r\n\r\n        # Following parameter and optimizer set as recommended in paper\r\n        self.n_critic = 5\r\n        optimizer = Adam(lr=1e-4, beta_1=0.5, beta_2=0.9)\r\n\r\n        # Build the generator and critic\r\n        self.generator = self.build_generator()\r\n        self.critic = self.build_critic()\r\n\r\n        #-------------------------------\r\n        # Construct Computational Graph\r\n        #       for the Critic\r\n        #-------------------------------\r\n\r\n        # Freeze generator's layers while training critic\r\n        self.generator.trainable = False\r\n\r\n        # Image input (real sample)\r\n        real_audio = Input(shape=(self.a*256*self.d, 1))\r\n\r\n        # Noise input\r\n        z_disc = Input(shape=(self.a, self.latent_dim))\r\n        # Generate image based of noise (fake sample)\r\n        fake_audio = self.generator(z_disc)\r\n\r\n        # Discriminator determines validity of the real and fake images\r\n        fake = self.critic(fake_audio)\r\n        valid = self.critic(real_audio)\r\n\r\n        # Construct weighted average between real and fake images\r\n        interpolated_audio = RandomWeightedAverage()([real_audio, fake_audio])\r\n        # Determine validity of weighted sample\r\n        validity_interpolated = self.critic(interpolated_audio)\r\n\r\n        # Use Python partial to provide loss function with additional\r\n        # 'averaged_samples' argument\r\n        partial_gp_loss = partial(self.gradient_penalty_loss,\r\n                          averaged_samples=interpolated_audio)\r\n        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\r\n\r\n        self.critic_model = Model(inputs=[real_audio, z_disc],\r\n                            outputs=[valid, fake, validity_interpolated])\r\n        self.critic_model.compile(loss=[self.wasserstein_loss,\r\n                                              self.wasserstein_loss,\r\n                                              partial_gp_loss],\r\n                                        optimizer=optimizer,\r\n                                        loss_weights=[1, 1, 10])\r\n        #-------------------------------\r\n        # Construct Computational Graph\r\n        #         for Generator\r\n        #-------------------------------\r\n\r\n        # For the generator we freeze the critic's layers\r\n        self.critic.trainable = False\r\n        self.generator.trainable = True\r\n\r\n        # Sampled noise for input to generator\r\n        z_gen = Input(shape=(self.a, self.latent_dim))\r\n        # Generate images based of noise\r\n        audio = self.generator(z_gen)\r\n        # Discriminator determines validity\r\n        valid = self.critic(audio)\r\n        # Defines generator model\r\n        self.generator_model = Model(z_gen, valid)\r\n        self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)\r\n\r\n\r\n    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\r\n        \"\"\"\r\n        Computes gradient penalty based on prediction and weighted real / fake samples\r\n        \"\"\"\r\n        gradients = K.gradients(y_pred, averaged_samples)[0]\r\n        # compute the euclidean norm by squaring ...\r\n        gradients_sqr = K.square(gradients)\r\n        #   ... summing over the rows ...\r\n        gradients_sqr_sum = K.sum(gradients_sqr,\r\n                                  axis=np.arange(1, len(gradients_sqr.shape)))\r\n        #   ... and sqrt\r\n        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\r\n        # compute lambda * (1 - ||grad||)^2 still for each single sample\r\n        gradient_penalty = K.square(1 - gradient_l2_norm)\r\n        # return the mean as loss over all the batch samples\r\n        return K.mean(gradient_penalty)\r\n\r\n\r\n    def wasserstein_loss(self, y_true, y_pred):\r\n        return K.mean(y_true * y_pred)\r\n\r\n    def apply_phaseshuffle(self, x, rad=2, pad_type='reflect'):\r\n        b, x_len, nch = x.get_shape().as_list()\r\n\r\n        phase = tf.random.uniform([], minval=-rad, maxval=rad + 1, dtype=tf.int32)\r\n        pad_l = tf.maximum(phase, 0)\r\n        pad_r = tf.maximum(-phase, 0)\r\n        phase_start = pad_r\r\n        x = tf.pad(x, [[0, 0], [pad_l, pad_r], [0, 0]], mode=pad_type)\r\n\r\n        x = x[:, phase_start:phase_start+x_len]\r\n        x.set_shape([b, x_len, nch])\r\n\r\n        return x\r\n\r\n    def build_generator(self):\r\n        d=self.d\r\n        c=self.c\r\n        a=self.a\r\n\r\n        # Prelim layers\r\n        input_layer = Input(shape=(a, 100))\r\n\r\n        dense_layer0 = TimeDistributed(Dense(256*d, input_shape=(100,)))(input_layer)#\r\n        reshape_layer0 = TimeDistributed(Reshape((c, c*d)))(dense_layer0)#\r\n        relu_layer0 = TimeDistributed(Activation('relu'))(reshape_layer0)#\r\n\r\n        # WaveCNN layers\r\n        c //= 2\r\n        expanded_layer0 = TimeDistributed(Lambda(lambda x: K.expand_dims(x, axis=1)))(relu_layer0)#relu_layer1\r\n        conv1d_t_layer0 = TimeDistributed(Conv2DTranspose(c*d, (1, 25), strides=(1, 4), padding='same'))(expanded_layer0)\r\n        slice_layer0 = Lambda(lambda x: x[:, :, 0])(conv1d_t_layer0)\r\n        relu_layer2 = TimeDistributed(Activation('relu'))(slice_layer0)\r\n\r\n        c //= 2\r\n        expanded_layer1 = TimeDistributed(Lambda(lambda x: K.expand_dims(x, axis=1)))(relu_layer2)\r\n        conv1d_t_layer1 = TimeDistributed(Conv2DTranspose(c*d, (1, 25), strides=(1, 4), padding='same'))(expanded_layer1)\r\n        slice_layer1 = Lambda(lambda x: x[:, :, 0])(conv1d_t_layer1)\r\n        relu_layer3 = TimeDistributed(Activation('relu'))(slice_layer1)\r\n\r\n        c //= 2\r\n        expanded_layer2 = TimeDistributed(Lambda(lambda x: K.expand_dims(x, axis=1)))(relu_layer3)\r\n        conv1d_t_layer2 = TimeDistributed(Conv2DTranspose(c*d, (1, 25), strides=(1, 4), padding='same'))(expanded_layer2)\r\n        slice_layer2 = Lambda(lambda x: x[:, :, 0])(conv1d_t_layer2)\r\n        relu_layer4 = TimeDistributed(Activation('relu'))(slice_layer2)\r\n\r\n        c //= 2\r\n        expanded_layer3 = TimeDistributed(Lambda(lambda x: K.expand_dims(x, axis=1)))(relu_layer4)\r\n        conv1d_t_layer3 = TimeDistributed(Conv2DTranspose(c*d, (1, 25), strides=(1, 4), padding='same'))(expanded_layer3)\r\n        slice_layer3 = Lambda(lambda x: x[:, :, 0])(conv1d_t_layer3)\r\n        relu_layer5 = TimeDistributed(Activation('relu'))(slice_layer3)\r\n\r\n        expanded_layer4 = TimeDistributed(Lambda(lambda x: K.expand_dims(x, axis=1)))(relu_layer5)\r\n        conv1d_t_layer4 = TimeDistributed(Conv2DTranspose(1, (1, 25), strides=(1, 4), padding='same'))(expanded_layer4)#strides=(1,1)\r\n        slice_layer4 = Lambda(lambda x: x[:, :, 0])(conv1d_t_layer4)\r\n        tanh_layer0 = TimeDistributed(Activation('tanh'))(slice_layer4)\r\n\r\n        reshape_layer1 = Reshape((a*256*d, 1))(tanh_layer0)\r\n\r\n        model = Model(inputs=input_layer, outputs=reshape_layer1)\r\n\r\n        print(model.summary())\r\n\r\n        return model\r\n\r\n    def build_critic(self):\r\n        d=self.d\r\n        c=self.c\r\n        a=self.a\r\n\r\n        input_layer = Input(shape=(a*256*d, 1))#d*d\r\n        reshape_layer0 = Reshape((a, 256*d, 1))(input_layer)\r\n\r\n        conv1d_layer0 = TimeDistributed(Conv1D(d, 25, strides=4, padding='same'))(reshape_layer0)#//2\r\n        LReLU_layer0 = TimeDistributed(LeakyReLU(alpha=0.2))(conv1d_layer0)\r\n        phaseshuffle_layer0 = TimeDistributed(Lambda(lambda x: self.apply_phaseshuffle(x)))(LReLU_layer0)\r\n\r\n        conv1d_layer1 = TimeDistributed(Conv1D(2*d, 25, strides=4, padding='same'))(phaseshuffle_layer0)#d\r\n        LReLU_layer1 = TimeDistributed(LeakyReLU(alpha=0.2))(conv1d_layer1)\r\n        phaseshuffle_layer1 = TimeDistributed(Lambda(lambda x: self.apply_phaseshuffle(x)))(LReLU_layer1)\r\n\r\n        conv1d_layer2 = TimeDistributed(Conv1D(4*d, 25, strides=4, padding='same'))(phaseshuffle_layer1)#2*d\r\n        LReLU_layer2 = TimeDistributed(LeakyReLU(alpha=0.2))(conv1d_layer2)\r\n        phaseshuffle_layer2 = TimeDistributed(Lambda(lambda x: self.apply_phaseshuffle(x)))(LReLU_layer2)\r\n\r\n        conv1d_layer3 = TimeDistributed(Conv1D(8*d, 25, strides=4, padding='same'))(phaseshuffle_layer2)#4*d\r\n        LReLU_layer3 = TimeDistributed(LeakyReLU(alpha=0.2))(conv1d_layer3)\r\n        phaseshuffle_layer3 = TimeDistributed(Lambda(lambda x: self.apply_phaseshuffle(x)))(LReLU_layer3)\r\n\r\n        conv1d_layer4 = TimeDistributed(Conv1D(16*d, 25, strides=4, padding='same'))(phaseshuffle_layer3)#8*d,strides=4\r\n        LReLU_layer4 = TimeDistributed(LeakyReLU(alpha=0.2))(conv1d_layer4)\r\n        phaseshuffle_layer4 = TimeDistributed(Lambda(lambda x: self.apply_phaseshuffle(x)))(LReLU_layer4)\r\n    \r\n        reshape_layer1 = Reshape((a, 256*d))(phaseshuffle_layer4)\r\n        slice_layer0 = Lambda(lambda x: x[:, 0])(reshape_layer1)#\r\n\r\n        dense_layer1 = Dense(1, input_shape=(a, 256*d))(slice_layer0)#dropout_layer1\r\n\r\n        model = Model(inputs=input_layer, outputs=dense_layer1)\r\n\r\n        print(model.summary())\r\n\r\n        return model\r\n\r\n    def train(self, epochs, batch_size, sample_interval=50):\r\n\r\n        X_train = []\r\n\r\n        for file in os.listdir(r\"C:\\Users\\Harry\\source\\repos\\tfworldhackathon\\Data\"):\r\n            with open(r\"C:\\Users\\Harry\\source\\repos\\tfworldhackathon\\Data\" + fr\"\\{file}\", \"rb\") as f:\r\n                samples, _ = librosa.load(f, sr=self.Fs)\r\n                X_train.append(np.array([np.array([sample]) for sample in samples[:self.a*256*self.d]]))\r\n                if \"17\" in file:\r\n                    break\r\n\r\n        X_train = np.array(X_train)\r\n\r\n        print(X_train.shape)\r\n\r\n        # Adversarial ground truths\r\n        valid = -np.ones((batch_size, 1))\r\n        fake =  np.ones((batch_size, 1))\r\n        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\r\n        for epoch in range(epochs):\r\n\r\n            for _ in range(self.n_critic):\r\n\r\n                # ---------------------\r\n                #  Train Discriminator\r\n                # ---------------------\r\n\r\n                # Select a random batch of images\r\n                idx = np.random.randint(0, X_train.shape[0], batch_size)\r\n                audios = X_train[idx]\r\n                # Sample generator input\r\n                noise = np.random.normal(0, 1, (batch_size, self.a, self.latent_dim))\r\n                # Train the critic\r\n                d_loss = self.critic_model.train_on_batch([audios, noise],\r\n                                                                [valid, fake, dummy])\r\n\r\n            # ---------------------\r\n            #  Train Generator\r\n            # ---------------------\r\n\r\n            g_loss = self.generator_model.train_on_batch(noise, valid)\r\n\r\n            # Plot the progress\r\n            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\r\n\r\n            # If at save interval => save generated image samples\r\n            if epoch % sample_interval == 0:\r\n                self.sample_audio(epoch)\r\n\r\n    def sample_audio(self, epoch):\r\n        print(f\"Checkpoint {epoch}\")\r\n        noise = np.random.normal(0, 1, (5, self.a, self.latent_dim))\r\n        gen_audios = self.generator.predict(noise)\r\n        for i in range(len(gen_audios)):\r\n            audio = gen_audios[i]\r\n            audio.flatten()\r\n            librosa.output.write_wav(f\"output/{epoch}-{i}.wav\", audio, sr=self.Fs)\r\n\r\n\r\nif __name__ == '__main__':\r\n    wgan = WGANGP()\r\n    wgan.train(epochs=30000, batch_size=8, sample_interval=100)\r\n```\r\n\r\n**Other info / logs**\r\nI have not seen any solutions to this issue for Tensorflow 2. \r\nThis error does not throw if I do not train the generator. This leads me to believe there is an issue when oscillating between models getting trained, not the actual training of a given model.\r\n\r\nTo restate, the error occurs on the line `d_loss = self.critic_model.train_on_batch([audios, noise], [valid, fake, dummy]) ` on the second epoch. So it seems like the following happens:\r\n\r\n- First epoch\r\n   - Train discriminator successfully \r\n   - Train generator successfully \r\n- Second epoch\r\n   - Train discriminator fails\r\n", "comments": ["Ok I fixed the issue after many hours. Replacing\r\n\r\n`from keras.optimizers import Adam`\r\n\r\nwith\r\n\r\n`from tensorflow.keras.optimizers import Adam`\r\n\r\ndid the trick. I don't know why this worked, but I would suggest that this ambiguity be cleared for others.\r\n\r\n", "@HStuart18, `from keras.optimizers import Adam` uses tensorflow as backend. Instead you can use Tensorflow.keras apis. \r\nUse these packages\r\n```\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Lambda, Dense, LSTM, Activation, Input, Bidirectional, Dropout, Reshape, Conv2DTranspose, TimeDistributed, Conv1D\r\nfrom tensorflow.keras.layers import LeakyReLU\r\nfrom tensorflow.keras.optimizers import Adam\r\nimport tensorflow.keras.backend as K\r\n```\r\nThanks!", "@HStuart18, Tenosrflow 2.0 doesn't support multi-backend keras. Instead use Tensorflow.keras. ", "> @HStuart18, `from keras.optimizers import Adam` uses tensorflow as backend. Instead you can use Tensorflow.keras apis.\r\n> Use these packages\r\n> \r\n> ```\r\n> from tensorflow.keras.models import Model\r\n> from tensorflow.keras.layers import Lambda, Dense, LSTM, Activation, Input, Bidirectional, Dropout, Reshape, Conv2DTranspose, TimeDistributed, Conv1D\r\n> from tensorflow.keras.layers import LeakyReLU\r\n> from tensorflow.keras.optimizers import Adam\r\n> import tensorflow.keras.backend as K\r\n> ```\r\n> \r\n> Thanks!\r\n\r\nWould you mind please clarifying the difference between `Keras` and `Tensorflow.Keras`? What do you mean by \u201cmulti-backend Keras\u201d? I was under the impression that Keras would exclusively use Tensorflow as  a backend.", "@HStuart18, Keras supports backends such as Tensorflow, Theano, CNTK.\r\nFor more read [here](https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/). And also take a look at this [link](https://www.tensorflow.org/guide/keras). Thanks!  "]}, {"number": 35406, "title": "import frozen graph with error \"Input 0 of node X was passed float from Y:0 incompatible with expected float_ref", "body": "It's the same problem with [this early issue](https://github.com/onnx/tensorflow-onnx/issues/77)\r\n\r\nHowever I can not solve this problem like it.\r\n\r\nBacause there is no \"Ref\" node and \"AssignSub\" node and \"AssignAdd\" node in my pb file.\r\n\r\n\r\n```bash\r\nInput 0 of node MelGAN/Generator/Generator_1thconv1d/weight_norm/Assign was passed float from MelGAN/Generator/Generator_1thconv1d/group__conv1d/g:0  incompatible with expected float_ref\r\n```\r\n\r\nMy pb file is in [my repository](https://github.com/MachineJeff/BUG)\r\n\r\nAnd the \"print node\" code is also in there.\r\n\r\nWho can help me ?", "comments": ["@MachineJeff, The my repository link is not working. Please provide the correct link. \r\nPlease provide the information asked in  issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "I have solved it by myself, thanks! @gadagashwini ", "> \r\n> \r\n> I have solved it by myself, thanks! @gadagashwini\r\n\r\nHow do you solve it? I have the similar question with you,could you please tell me some solution? Thanks!"]}, {"number": 35405, "title": "tensorflowlib for 2.0", "body": "I can see many users over the last 2 weeks struggling with getting the build to work with Windows 10. I also see a few **recent commits** to improve the build in response to the above feedback.\r\n\r\nAs [stated here](https://www.tensorflow.org/install/source_windows) that the Windows tensorflow 2.0 builds with following configurations are successful.\r\n\r\n![image](https://user-images.githubusercontent.com/59223977/71443261-34f74d00-270a-11ea-8cc5-ee2902f336b8.png)\r\n![image](https://user-images.githubusercontent.com/59223977/71443269-44769600-270a-11ea-8857-750f7f3598ee.png)\r\n\r\nPlease **update** [the links](https://www.tensorflow.org/install/lang_c) here with the successful windows tensorflow 2.0 builds provided above \r\n\r\nFor example: we need\r\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.0.0.zip\r\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.0.0.zip\r\n\r\n_Although these tentative builds are old_, but it will provide a starting point **to start TESTING the Tensorflow 2.0 features through bindings.**", "comments": ["[Community effort](https://github.com/guikarist/tensorflow-windows-build-script/issues/38#issue-513074418)", "@Dave-Stein \r\n\r\nPlease, have a look at the note mentioned in the [link ](https://www.tensorflow.org/install/lang_c).As stated it can be included in the future releases.Thanks!", "@ravikyram I read with excitement of the latest features of tensorflow 2.0 a few months back. With all the problems users here feedback regarding building tensorflow 2.1, we would like to see a commitment to Windows platform. We need soon the compiled version of tensorflowlib 2.0.0. We understand that perhaps we may need to wait for tensorflow 2.1, we are curious to compare tensorflowlib 1.15 and 2.0.0. \r\n\r\n==> please try your best to make tensorflowlib 2.0.0 available! ", "These lists are meant for me to track the issues so when there are enough success stories, I will get back to try again to compile in Windows.\r\n\r\nTracking challenges in compiling for windows\r\n- Tensorflow2.0\r\n[Case1](https://github.com/tensorflow/tensorflow/issues/32997#issuecomment-564682504)\r\n- Tensorflow2.1\r\n[Case1](https://github.com/tensorflow/tensorflow/issues/35977#issue-551290058)\r\n\r\nTracking community feedback that make windows compile works\r\n\r\n- Tensorflow2.0\r\n[Case1] (https://github.com/tensorflow/tensorflow/issues/35253#issuecomment-570895670)", "@ravikyram @gunan  any update to [this](https://www.tensorflow.org/install/lang_c) for 2.0?\r\n\r\n[FYI](https://github.com/tensorflow/tensorflow/issues/36576) for anyone looking for libtensorflow >= 2.0 ", "Update after 3 months\r\nTensorflow2.2 for windows\r\n[Case 1](https://github.com/tensorflow/tensorflow/issues/39905)\r\n[Case 2](https://github.com/tensorflow/tensorflow/issues/39407)", "https://github.com/tensorflow/tensorflow/issues/17778#issuecomment-384816014\r\n\r\nhttps://spleeterpp.readthedocs.io/en/latest/build.html#tensorflow-cc-for-windows\r\n\r\nOut of date\r\nhttps://joe-antognini.github.io/machine-learning/build-windows-tf", "Are we going to have an official fix for the windows linking symbols issue ? Adding TF_EXPORT does not seem to be a solid solution, you have a big chance to miss some functions.\r\nI'm struggling on how to fix the tensorflow::op::Cast class.\r\n\r\n//////////////////////////////////////////\r\nauto uint8Caster = Cast(root.WithOpName(\"uint8_Cast\"), outTensor, tensorflow::DT_UINT8);\r\n\r\nBut Cast class is defined in math_ops.h, which is \"// This file is MACHINE GENERATED! Do not edit.\"\r\n\r\nHow to add TF_EXPORT to class Cast ?\r\n///////////////////////////////////////////", "@av8ramit @bmzhao is working to fix libtensorflow releases.\r\nAs mentioned in other issues, owners of our C++ APIs have left, and now we are ramping up on the releases, moving them to 2.0, and cleaning up stale build scripts all at the same time. This is taking some time, thus the delay. Moreover, we have lost multiple people with windows expertise, so that has been quite difficult for us to ramp up as well.", "We need to invite more discussion on the future of WINDOW support for tensorflow.  6 months with no progress but challenges. **Google needs to hire those WHO CLIAM HERE are able to address these challenges IMMEDIATELY.** ", "@gunan \r\nHi Gunan\r\n\r\nIf you need more people to work on windows, maybe I can help, I've been working on windows more than 10 years. \r\n\r\n\r\n\r\n", "@heyufei2008 Thank you for your offer to help.\r\nWe are working through the backlog, however we welcome any contributions, if you like to help!", "@gunan  Great. how can I start work on some issues? I need some basic introductions or document  to get my hands in. Thanks.", "To get started, you can go over our guide to build TF from sources here:\r\nhttps://www.tensorflow.org/install/source_windows\r\nWhile I know that the guide is for python, the C++ library builds the same code with a different build target (tensorflow:tensorflow, I think)\r\n\r\nAlso, this is a great community effort, I recommend reading this as well:\r\nhttps://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows \r\n\r\nAnd finally, @av8ramit can help with the mechanisms we use for exporting c++ symbols from our shared objects\r\n", "@gunan \r\nThanks.\r\n", "We are also building nightly libtensorflow packages. Please pardon the difficult browser navigation, but they can be found in the `libtensorflow-nightly` GCS bucket [here](https://storage.googleapis.com/libtensorflow-nightly). They are listed by operating system. The directories correspond to the date they were built. We will also be publishing official binaries along with additional instructions on tensorflow.org as a part of the 2.3 release.", "As for how we export shared object symbols on Windows, that depends. For exporting a symbol from a dependency in [pywrap_tensorflow](https://github.com/tensorflow/tensorflow/blob/492bb4d4a3fd5aafae620d2b4db1535b2d4f4a7b/tensorflow/python/BUILD#L5985), we would request that you add them to [this file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt) and add the target [here](https://github.com/tensorflow/tensorflow/blob/492bb4d4a3fd5aafae620d2b4db1535b2d4f4a7b/tensorflow/python/BUILD#L6049) as we have a script that exposes the mangled symbol from the larger shared object. For smaller standalone shared objects, using pybind11 and creating a standalone `pybind_extension` invocation should be enough. ", "@av8ramit \r\nThanks.\r\n\r\nCan you give me an example showing how to change  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/def_file_filter/symbols_pybind.txt\r\n\r\nand how to change https://github.com/tensorflow/tensorflow/blob/492bb4d4a3fd5aafae620d2b4db1535b2d4f4a7b/tensorflow/python/BUILD#L6049\r\n\r\nI need to export symbol for this:\r\n\r\n//////////////////////////////////////////\r\nauto uint8Caster = Cast(root.WithOpName(\"uint8_Cast\"), outTensor, tensorflow::DT_UINT8);\r\n\r\nBut Cast class is defined in math_ops.h, which is \"// This file is MACHINE GENERATED! Do not edit.\"\r\n\r\nHow to add TF_EXPORT to class Cast ?\r\n///////////////////////////////////////////", "Ah my apologies, I missed your previous message. Unfortunately since that is autogenerated, the suggestion I provided will not apply. That suggestion is for explicitly defined functions. [Here](https://github.com/tensorflow/tensorflow/commit/9d41ea557a79e33c5c766b075d307501acd2265b#diff-145ec99c6d4762e904542472583a3fb1) is an example.\r\n\r\n\r\nLet me dig into this a bit more and get back to you tomorrow.", "> the `libtensorflow-nightly` GCS bucket [here](https://storage.googleapis.com/libtensorflow-nightly).\r\n\r\n@av8ramit  Finally, something that we all have been looking for.", "I would like to remind everyone that TensorFlow is an open source project.\r\nWhile we do our best to provide many features, the community is more than welcome to contribute and fill those gaps.\r\nWe have been lacking windows expertise for a long time. This is no secret. So we depend on everyone here to work with us, and fill the gaps where we fall short.", "@gunan @av8ramit   I have followed discussions between SciSharp and tensorflow team.\r\nThe SciSharp team is currently offering tensorflow.dll (TF2.2 CPU), Now we have the GPU version.\r\nFrom the ongoing work. it is increasing clear that we need MORE than this binary eventually. We need to contribute to the source code that create the tensorflow.dll so that the SciSharp team has more control on what are being exposed to optimize tensorflow performance for the .NET community.\r\n\r\nTODAY is a NEW beginning!", "@heyufei2008 so the symbols from that file are exported in [libtensorflow_cc](https://github.com/tensorflow/tensorflow/blob/de0a617f4ef321ebf579a4539cec85690f55fb4c/tensorflow/BUILD#L735). So to expose on Windows you need to adjust the [def_file_filter](https://github.com/tensorflow/tensorflow/blob/de0a617f4ef321ebf579a4539cec85690f55fb4c/tensorflow/BUILD#L752) target. \r\n\r\nIf you trace the rabbit hole for that target you'll end up [here](https://github.com/tensorflow/tensorflow/blob/de0a617f4ef321ebf579a4539cec85690f55fb4c/tensorflow/tensorflow.bzl#L1804). Now a lot of this is unmaintained, which is why the def_file_filter for `libtensorflow_cc` does not make sense to come from the last link. We may need to create a new def_file_filter for `libtensorflow_cc` and then export from the ops you mentioned previously. We do not currently ship libtensorflow_cc as a package.", "@av8ramit Thanks for your information.\r\ndef_file_filter is very complicated, I don't know how it works.\r\nCan you tell me how to change it so that I can do this ? \r\n//////////////////////////////////////////\r\nauto uint8Caster = Cast(root.WithOpName(\"uint8_Cast\"), outTensor, tensorflow::DT_UINT8);\r\n//////////////////////////////////////////\r\n\r\nAnother way: Can I build a static library instead of a dll ? Static tensorflow library is ok for me, then we can bypass the issue of limited export symbol in windows.\r\n\r\nThanks.\r\nJohn\r\n", "Unfortunately I'm not too familiar with Windows def files either. I think you need to copy the def file target we have currently given to libtensorflow_cc and then create a new one with the cc_library target that contains the `uint8Caster`. \r\n\r\nI'm not sure about the static library, but if that bypasses the limited export issue, it could work.", "@av8ramit I did some test following your message, but failed to make it work.\r\nI'm doing object detection, \"auto uint8Caster = Cast(root.WithOpName(\"uint8_Cast\"), outTensor, tensorflow::DT_UINT8);\" is a must to me.\r\n\r\nPlease give me more help on this.\r\n\r\nThanks.\r\n\r\n", "So you created a new def file target applied it to the libtensorflow_cc and tried building a new shared object? What was the issue you saw?", "@av8ramit Hi Amit\r\nFirst, according to this in build file:\r\ntf_cc_shared_object(\r\n    name = \"tensorflow_cc\",\r\n    linkopts = select({\r\n        \"//tensorflow:macos\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow:tf_exported_symbols.lds)\", \r\nI added one line in tf_exported_symbols_lds.\r\n{\r\n*tensorflow*\r\n*toco*\r\n*perftools*gputools*\r\n*tf_*\r\n*TF_*\r\n*TFE_*\r\n*nsync_*\r\n*stream_executor*\r\n*Cast*     // I added.\r\n}\r\n\r\nNothing changed.\r\n\r\nthen I add these lines to def_file_filter.py.tpl:\r\n\r\n@@ -214,6 +215,9 @@ def main():\r\n      def_fp.write(\"LIBRARY \" + args.target + \"\\n\")\r\n    def_fp.write(\"EXPORTS\\n\")\r\n    def_fp.write(\"\\t ??1OpDef@tensorflow@@UEAA@XZ\\n\")\r\n    def_fp.write(\"\\t ??0Cast@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@W4DataType@2@@z\\n\")\r\n    def_fp.write(\"\\t ??0Div@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\\n\")\r\n    def_fp.write(\"\\t ??0Subtract@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z\\n\")\t\r\n\r\nThen it says: can not find Cast()...\r\n\r\n\r\n", "libtensorflow_cc.dll.exp : error LNK2001: unresolved external symbol \"??0Cast@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@W4DataType@2@@z\" (??0Cast@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@W4DataType@2@@z)\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\tensorflow_cc.dll : fatal error LNK1120: 1 unresolved externals\r\nTarget //tensorflow:tensorflow_cc.lib failed to build\r\nINFO: Elapsed time: 97.091s, Critical Path: 47.80s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully", "Hey @heyufei2008 \r\n\r\nSo `tf_exported_symbols.lds` is only applied for MacOS builds as seen in [this line](https://github.com/tensorflow/tensorflow/blob/c41f4652b45bf70f20686e612b41574b4b8139d7/tensorflow/BUILD#L758).  `def_file_filter.py.tpl` has filters to expose symbols, but those symbols need to come from somewhere. And that filter is applied to what comes from the `        \"//tensorflow:windows\": \":tensorflow_filtered_def_file\",` target. So you need to create similar logic.\r\n\r\nAs you go down the path reverse engineering that target you'll create a dependency similar to [this one](https://github.com/tensorflow/tensorflow/blob/de0a617f4ef321ebf579a4539cec85690f55fb4c/tensorflow/tensorflow.bzl#L1804-L1813). In that target you'll need to add the cc_library where the symbol you want to expose will be defined. ", " Until tf1.10 version with cmake tool, there is no such issue. Please let me know how to export all cc_api in tensorflow.dll by bazel too?", "@av8ramit @gunan \r\n\r\n> I would like to remind everyone that TensorFlow is an open source project.\r\n> While we do our best to provide many features, the community is more than welcome to contribute and fill those gaps.\r\n\r\nAsk your opinions on setting up this repo for C# API to tensorflow.dll\r\n[https://github.com/tensorflow/csharp](https://github.com/tensorflow/tensorflow/pull/38027#issuecomment-606736249)\r\n\r\nWith this, we can mobilize .NET C# tensorflow api developers to contribute on regular basis\r\n\r\n[Unit Tests for c# NativeAPI ](https://github.com/SciSharp/TensorFlow.NET/tree/master/test/TensorFlowNET.UnitTest/NativeAPI)to call the tensorflow.dll \r\n", "@GeorgeS2019 Please avoid adding unnecessary and unrelated comments on different issues. This will not bring extra attention to any issues. Moreover, it further causes extra delays for us, trying to check if these are connected, and wastes our time.\r\n\r\nAbout csharp, You may want to start a thread in developers@tensorflow.org.\r\nIf there is enough interest, it may happen.", "@gunan  My issue is still not solved.", "@heyufei2008 I am aware. Have you tried the suggestion in https://github.com/tensorflow/tensorflow/issues/35405#issuecomment-646306403 ?", "> About csharp, You may want to start a thread in developers@tensorflow.org.\r\nIf there is enough interest, it may happen.\r\n\r\n@gunan  thx for suggestion.  Not trying to create confusion, BUT to bring developers interested on .NET into ONE common thread.  As u suggested, the new focus => For anyone interested to contribute to **Tensorflow.NET** (https://github.com/tensorflow/csharp) AND **tensorflow.dll for .NET**<= will be **developers@tensorflow.org.**", "> \r\n> \r\n> I would like to remind everyone that TensorFlow is an open source project.\r\n> While we do our best to provide many features, the community is more than welcome to contribute and fill those gaps.\r\n> We have been lacking windows expertise for a long time. This is no secret. So we depend on everyone here to work with us, and fill the gaps where we fall short.\r\n\r\nI think it would help to start with the documentation, and make a quick note of the status of (1) building from source on Windows and (2) the intended way to use the C++ API (and other APIs?) (i.e. does it require building from source? Static libraries? Dynamic libraries? Direct integration of source?). I have been struggling for days to understand the status of building on Windows because the documentation is sparse to say the least. I'm extremely reluctant to edit the documentation myself (extremely unfamiliar with git; extremely lost with regard to building on windows...) so have tried to raise the issue [here](https://github.com/tensorflow/tensorflow/issues/43519)\r\n\r\nMeanwhile, my current aim as an exercise in understanding the process is to create a \"hello ML world\" program in C++ for Windows desktop (console application). The intent is to invoke \"helloML image.jpg\" where \"image.jpg\" contains an image of a hand-written digit. The output should say something like \"hello ML world! The image contained the digit 7\" or whatever.\r\n\r\nI will document that process as I go, and aim to make the result available as a tutorial.", "@gunan @av8ramit   @omatai \r\n\r\nI would argue that the support we get from tensorflowlib 2.x has improved a great deal.\r\n\r\nThe [sharing](https://github.com/tensorflow/tensorflow/issues/35405#issuecomment-641619156) of the [nightly built link](https://storage.googleapis.com/libtensorflow-nightly) has made a GREAT impact to the Tensorflow.NET and tensorflowSharp communities. \r\n\r\nTensorflow.NET and Csharp wrapper support are now more recognized than before. \r\n\r\nI do not denied there is still room for improvement, especially on the documentation and consistent build scripts for Windows.\r\n\r\nSuggestions\r\n\r\n1. We need to setup a consensus format on the TensorFlow C Api Unit tests so that e.g. the Tensorflow.NET community can parse the C Api unit tests to automatically generate the C# version of the binding codes to the C API. unit tests\r\n\r\nRight now, the TensorFlow team is dong such an incredible GOOD JOB that it is getting more and more challenging to keep up by e.g. C# Tensorflow.NET community.  We are severely behind with code coverage of the C Api unit tests with C# wrappers. \r\n\r\nI need suggestion from the TensorFlow team how we could collaborate to sort out a more Parser friendly format of the TensorFlow C API so that the Tensorflow.NET community has a better chance of automatically generate the code skeleton framework to be UP-TO-DATE with each release of the TensorFlow C API. \r\n\r\n2. We also hope the TensorFlow team can keep up with the documentation of the C API within the code to make it easy to port those documentation to the C# equivalent. \r\n\r\nPorting the C unit test code will still need to be done manually. However, to get **MORE WINDOW SUPPORT** (see below) to keep up with the speed that TensorFlow team is progressing, we need a C# overview of TensorFlow C API code coverage to coordinate among the C# tensorflow.NET community to coordinate the porting of the C API. \r\n\r\n> I would like to remind everyone that TensorFlow is an open source project.\r\nWhile we do our best to provide many features, the community is more than welcome to contribute and fill those gaps.\r\nWe have been lacking windows expertise for a long time. This is no secret. So we depend on everyone here to work with us, and fill the gaps where we fall short.\r\n\r\n3. The more the Tensorflow.NET community is able to keep up to date with those TensorFlow C API, the more engagement we can expect from the Windows Community. \r\n\r\nPlease advice how best to proceed with an Agreeable C API unit test format that will be a WIN WIN for other community like Tensorflow.NET and the fast moving TensorFlow team.\r\n\r\n\r\n==> I do not speak for the Tensorflow.NET management team. I hope [this suggestion](https://github.com/SciSharp/TensorFlow.NET/issues/618#issuecomment-700037288) will be evaluated. I also hope that the TensorFlow team is open to a more coordinated community effort to address the various needs of the TensorFlow Windows developer communities.  \r\n\r\n\r\n\r\n\r\n", "@gunan @av8ramit I would like to feedback that the level of effort the Tensorflow.NET team invested into the C# port of the  tensorflow C API unit tests is probably more comprehensive and perhaps better structured and more aligned than that of the tensorflow Java wrapper project. \r\n\r\nIn other words, the Tensorflow.NET initiative is getting serious and on par, more and more in my view, catching up to e.g. Tensorflow Java wrapper project. This I hope is again more aligned with the vision quoted below. \r\n\r\n> I would like to remind everyone that TensorFlow is an open source project.\r\nWhile we do our best to provide many features, the community is more than welcome to contribute and fill those gaps.\r\nWe have been lacking windows expertise for a long time. This is no secret. So we depend on everyone here to work with us, and fill the gaps where we fall short.\r\n\r\nWith that, the Windows developer community welcome more compatible coordination and collaboration (_e.g. finding the best structure meta information or format for the Tensorflow C API Unit tests to support e.g. TensorFlow.NET initiative to be as much Up-To-Date with the latest changes reflected by the C API Unit Tests_ )  from the TensorFlow team.", "I think the [SIG Build](https://groups.google.com/a/tensorflow.org/forum/#!forum/build) would be a better forum to discuss these gaps, reach out for community feedback, and see how we can proceed going forward. ", "Here's my real experience: I started with Tensorflow 2+ years ago on version 1.5. I could not understand it, but it worked. But since I could not understand it, I could not modify it. So I was forced to mothball it. I returned to Tensorflow about 6 months ago. This time with Keras, I could understand it, but I could not make it work. Most of the inability to make it work rested with the documentation.\r\n\r\nFinally, after relegating TF to something that only justified 2-3 hours per week attention because the investment in it promised so little payback, we got it to work, and wanted to integrate the network we had trained in C++ on Windows in a desktop application. That was mid last week.\r\n\r\nI have subsequently spent 3 solid days trying to work out how to progress past that point. My assessment: the documentation needs priority, because the signal-to-noise in the alternatives to the documentation (github issues, stackoverflow) is so low as to reinforce our original assessment that TF should be set aside completely.\r\n\r\nAnd yet it would have only taken 1 paragraph of documentation to change that 3 solid days of angst and confusion to an acceptable rate of progress. The paragraph:\r\n\r\nTo use the Tensorflow API in a <x> application, typically you will need to build a Tensorflow library for <x> from the source. In certain cases, there may be pre-compiled libraries available. Please see the Tensorflow install instructions for your platform. Those instructions are directed towards building a pip installation package. You should instead direct the bazel build process to produce the library you require. For example, to build a library for a Windows C++ application, you should invoke: bazel build --config=opt //tensorflow:tensorflow.dll\r\n\r\nThat paragraph would have saved me 2.5 days. That is typical of our experience with tensorflow - it is a slow death by a thousand paper cuts.\r\n\r\nThe build process is actually fine - once I knew what to do (2.5 days late) it ran smoothly without issue. But understanding what to build is a nightmare that could be dramatically simplified with some simple documentation changes.\r\n\r\nI'm about to try to integrate the library/DLL I have produced into a hello world program. Normally I would expect to complete that within an hour. Based on some months of experience with Tensorflow, I would expect it to take several days. I'll let you know how long it actually takes.", "> I think the SIG Build would be a better forum to discuss these gaps, reach out for community feedback, and see how we can proceed going forward.\r\n\r\n@av8ramit   => thanks for the right next step to proceed. \ud83d\udc4d \r\n", "@av8ramit \r\n\r\nHere to update [the formation of the Tensorflow.NET SIG](https://github.com/tensorflow/community/pull/337) to serve the TensorFlow .NET community that there will be commitment to work with the tensorflow SIG Build to ensure up-to-date to what have been and what have not been implemented (both API and Unit Tests) with respect to the latest changes/commits of tesnsorflow  c API and c API tests", "In case anyone was waiting to know how long it took me to integrate TensorFlow into a Windows C++ application... the answer is an infinite amount of time - we gave up :-(\r\n\r\nFor anyone else wanting to proceed down the same path, I suggest that you are better off at present converting your TF model into an ONNX model, and then using Windows ML (potentially slow, despite using Onnx Runtime ) or Onnx Runtime itself, or DirectML if every millisecond matters. ", "Hi @Dave-Stein! Could you please check on latest stable version of TF 2.6 and the instructions at this[ link ](https://www.tensorflow.org/install/source_windows)for installing Tensorflow in windows from Source. let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35405\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35405\">No</a>\n"]}, {"number": 35404, "title": "ValueError: Flattening a PerReplica to components is not supported in replica context.", "body": "**System information**\r\n- windows\r\n- TensorFlow installed from conda\r\n- TensorFlow version:2.0\r\n- Python version:3.7.4\r\n- CUDA/cuDNN version: 10.2/7.6\r\n- GPU model and memory: 2 nvidia rtx 2070s 8GB\r\n\r\n**Describe the current behavior**\r\ni follow the distributed training tutorials to change my code(custom model) for cumtom training loop. but when i run the script, it shows mistake \"ValueError: Flattening a PerReplica to components is not supported in replica context.\". i don't understande why it is happens.  it can run in the train step,  but it can't run in the test step\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\n# i use this strategy before,but it make the same mistake\r\n# strategy = tf.distribute.MirroredStrategy(\r\n#     cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n\r\nstrategy = tf.distribute.MirroredStrategy(\r\n        cross_device_ops=tf.distribute.ReductionToOneDevice(\"/device:CPU:0\"))\r\n\r\ndef train_step_fn(rgb, spec):\r\n            with tf.GradientTape() as tape:\r\n                fake_spec = model(rgb, training=True)\r\n                loss = compute_loss(spec, fake_spec)\r\n\r\n            gradients = tape.gradient(loss, model.trainable_variables)\r\n            opt.apply_gradients(zip(gradients, model.trainable_variables))\r\n            # update metrics\r\n            rmse1.update_state(spec, fake_spec)\r\n            rmse2.update_state(spec, fake_spec)\r\n            rrmse1.update_state(spec, fake_spec)\r\n            rrmse2.update_state(spec, fake_spec)\r\n            sam.update_state(spec, fake_spec)\r\n            return loss\r\n\r\ndef test_step_fn(rgb, sepc):\r\n            fake_spec = model(rgb, training=False)\r\n            loss = loss_object(spec, fake_spec)\r\n            # update metrics\r\n            rmse1.update_state(spec, fake_spec)\r\n            rmse2.update_state(spec, fake_spec)\r\n            rrmse1.update_state(spec, fake_spec)\r\n            rrmse2.update_state(spec, fake_spec)\r\n            sam.update_state(spec, fake_spec)\r\n            return loss\r\n\r\n@tf.function\r\ndef distributed_train_step(rgb, spec):\r\n            per_replica_losses = strategy.experimental_run_v2(train_step_fn,\r\n                                                              args=(rgb, spec))\r\n            return strategy.reduce(tf.distribute.ReduceOp.SUM,\r\n                                   per_replica_losses,\r\n                                   axis=None)\r\n\r\n@tf.function\r\ndef distributed_test_step(rgb, spec):\r\n            return strategy.experimental_run_v2(test_step_fn, args=(rgb, spec))\r\n\r\nfor epoch in range(parser.epochs):\r\n            # train\r\n            for step, (rgb, spec) in enumerate(datas[0]):\r\n                train_mean_loss = distributed_train_step(rgb, spec)\r\n                steps += 1\r\n                ckpt.steps.assign(steps)\r\n                if step == 50:\r\n                    break\r\n            # val\r\n             rmse1.reset_state()\r\n             rmse2.reset_state()\r\n             rrmse1.reset_state()\r\n             rrmse2.reset_state()\r\n             sam.reset_state()\r\n            for step, (rgb, spec) in enumerate(datas[1]):\r\n\r\n                test_mean_loss = distributed_test_step(rgb, spec)\r\n```\r\n**Other info / logs**\r\n```\r\nValueError: in converted code:\r\n    mytrainT.py:163 distributed_test_step  *\r\n        return strategy.experimental_run_v2(test_step_fn, args=(rgb, spec))\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py:760 experimental_run_v2\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    mytrainT.py:144 test_step_fn  *\r\n        loss = loss_object(spec, fake_spec)\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py:125 __call__\r\n        with K.name_scope(scope_name or self.__class__.__name__), graph_ctx:\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\contextlib.py:112 __enter__\r\n        return next(self.gen)\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:435 graph_context_for_symbolic_tensors\r\n        if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:435 <genexpr>\r\n        if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:345 is_symbolic_tensor\r\n        return tensor._is_graph_tensor  # pylint: disable=protected-access\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\composite_tensor.py:119 _is_graph_tensor\r\n        components = self._type_spec._to_components(self)  # pylint: disable=protected-access\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\distribute\\values.py:500 _to_components\r\n        \"Flattening a PerReplica to components is not supported in replica \"\r\n\r\n    ValueError: Flattening a PerReplica to components is not supported in replica context.\r\n```", "comments": ["x7night@ The reason the train step works is because it uses the `compute_loss` function and the test step uses a `loss_object`. Can you provide details about how you create your `loss object`? I was not able to reproduce this issue with a simple loss_object that i created. \r\nAlso, can you augment your above snippet with where you open the strategy scope, build your model, create losses etc?  ", "@anj-s \r\nhere is my code. i follow the tutorials(https://tensorflow.google.cn/tutorials/distribute/custom_training) to change my code\r\n```\r\n    strategy = tf.distribute.MirroredStrategy(\r\n        cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n    print('replicas nums:', strategy.num_replicas_in_sync)\r\n    # Global batch size\r\n    global_batch_size = parser.batchSize\r\n    batch_size_per_replica = int(global_batch_size /\r\n                                 strategy.num_replicas_in_sync)\r\n    print('per repilca size:', batch_size_per_replica)\r\n    # move to GPU\r\n    with strategy.scope():\r\n        # prepare\r\n        def getData(data, data_path):\r\n            data[0], data[1] = utils.getDataset_(data_path,\r\n                                                 split=0.9,\r\n                                                 shuffle=128,\r\n                                                 batch=global_batch_size)\r\n            data[0] = strategy.experimental_distribute_dataset(data[0])\r\n            data[1] = strategy.experimental_distribute_dataset(data[1])\r\n            return data\r\n\r\n        datas = [0, 1]\r\n        datas = getData(datas, data_path)\r\n\r\n        # build model\r\n        print(\"\\nbuilding models ...\\n\")\r\n        model = Model()\r\n\r\n        # optimizers\r\n        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\r\n            initial_learning_rate=parser.lr,\r\n            decay_steps=20000,\r\n            decay_rate=0.9,\r\n            staircase=True)\r\n        opt = tf.optimizers.Adam(learning_rate=lr_schedule, decay=1e-6)\r\n\r\n        # writer = SummaryWriter(opt.outf)\r\n        file_writer = tf.summary.create_file_writer(parser.out)\r\n        # tf.summary.flush()\r\n        file_writer.set_as_default()\r\n\r\n        ckpt = tf.train.Checkpoint(steps=tf.Variable(1),\r\n                                   net=model,\r\n                                   optimizer=opt)\r\n        manager = tf.train.CheckpointManager(ckpt,\r\n                                             './tf_ckpts',\r\n                                             max_to_keep=15)\r\n\r\n        rmse1 = modules.RMSE1()\r\n        rmse2 = modules.RMSE2()\r\n        rrmse1 = modules.rRMSE1()\r\n        rrmse2 = modules.rRMSE2()\r\n        sam = modules.SAM()\r\n\r\n        print('start train\\n')\r\n        steps = 0\r\n        start = datetime.datetime.now()\r\n        old_loss = 100.\r\n        cur_loss = 100.\r\n\r\n        loss_object = tf.keras.losses.MeanSquaredError(\r\n            reduction=tf.keras.losses.Reduction.NONE)\r\n\r\n        def compute_loss(labels, predictions):\r\n            per_example_loss = loss_object(labels, predictions)\r\n            return tf.nn.compute_average_loss(\r\n                per_example_loss, global_batch_size=global_batch_size)\r\n\r\n        def train_step_fn(rgb, spec):\r\n            with tf.GradientTape() as tape:\r\n                fake_spec = model(rgb, training=True)\r\n                loss = compute_loss(spec, fake_spec)\r\n\r\n            gradients = tape.gradient(loss, model.trainable_variables)\r\n            opt.apply_gradients(zip(gradients, model.trainable_variables))\r\n            # update metrics\r\n            rmse1.update_state(spec, fake_spec)\r\n            rmse2.update_state(spec, fake_spec)\r\n            rrmse1.update_state(spec, fake_spec)\r\n            rrmse2.update_state(spec, fake_spec)\r\n            sam.update_state(spec, fake_spec)\r\n            return loss\r\n\r\n        def test_step_fn(rgb, sepc):\r\n            fake_spec = model(rgb)\r\n            loss = loss_object(spec, fake_spec)\r\n            # update metrics\r\n            rmse1.update_state(spec, fake_spec)\r\n            rmse2.update_state(spec, fake_spec)\r\n            rrmse1.update_state(spec, fake_spec)\r\n            rrmse2.update_state(spec, fake_spec)\r\n            sam.update_state(spec, fake_spec)\r\n            return loss\r\n\r\n        @tf.function\r\n        def distributed_train_step(rgb, spec):\r\n            per_replica_losses = strategy.experimental_run_v2(train_step_fn,\r\n                                                              args=(rgb, spec))\r\n            return strategy.reduce(tf.distribute.ReduceOp.SUM,\r\n                                   per_replica_losses,\r\n                                   axis=None)\r\n\r\n        @tf.function\r\n        def distributed_test_step(rgb, spec):\r\n            return strategy.experimental_run_v2(test_step_fn, args=(rgb, spec))\r\n\r\n        for epoch in range(parser.epochs):\r\n            # pre\r\n            status = 'train'\r\n            # progress bar\r\n            widgets = [\r\n                progressbar.Variable('status'),\r\n                ' ',\r\n                progressbar.Variable('epoch', width=2),\r\n                ' ',\r\n                progressbar.Variable('step', width=4),\r\n                ' ',\r\n                progressbar.Timer(),\r\n                ' ',\r\n                progressbar.Variable('rmse1', precision=4),\r\n                ' ',\r\n                progressbar.Variable('rmse2', precision=4),\r\n                ' ',\r\n                progressbar.Variable('rrmse1', precision=4),\r\n                ' ',\r\n                progressbar.Variable('rrmse2', precision=4),\r\n                ' ',\r\n                progressbar.Variable('sam', precision=4),\r\n            ]\r\n            bar = progressbar.ProgressBar(widgets=widgets).start()\r\n            # train\r\n            for step, (rgb, spec) in enumerate(datas[0]):\r\n\r\n                train_mean_loss = distributed_train_step(rgb, spec)\r\n\r\n                bar.update(status=status,\r\n                           epoch=epoch,\r\n                           step=step,\r\n                           rmse1=float(rmse1.result()),\r\n                           rmse2=float(rmse2.result()),\r\n                           rrmse1=float(rrmse1.result()),\r\n                           rrmse2=float(rrmse2.result()),\r\n                           sam=float(sam.result()))\r\n                if steps % 10 == 0:\r\n                    with file_writer.as_default():\r\n                        tf.summary.scalar('loss',\r\n                                          float(train_mean_loss),\r\n                                          step=steps)\r\n                        tf.summary.scalar('rmse1',\r\n                                          float(rmse1.result()),\r\n                                          step=steps)\r\n                        tf.summary.scalar('rmse2',\r\n                                          float(rmse2.result()),\r\n                                          step=steps)\r\n                        tf.summary.scalar('rrmse1',\r\n                                          float(rrmse1.result()),\r\n                                          step=steps)\r\n                        tf.summary.scalar('rrmse2',\r\n                                          float(rrmse2.result()),\r\n                                          step=steps)\r\n                        tf.summary.scalar('sam',\r\n                                          float(sam.result()),\r\n                                          step=steps)\r\n                steps += 1\r\n                ckpt.steps.assign(steps)\r\n                if step == 50:\r\n                    break\r\n            # val\r\n            status = 'val'\r\n            rmse1.reset_state()\r\n            rmse2.reset_state()\r\n            rrmse1.reset_state()\r\n            rrmse2.reset_state()\r\n            sam.reset_state()\r\n            for step, (rgb, spec) in enumerate(datas[1]):\r\n\r\n                test_mean_loss = distributed_test_step(rgb, spec)\r\n\r\n                bar.update(status=status,\r\n                           epoch=epoch,\r\n                           step=step,\r\n                           rmse1=float(rmse1.result()),\r\n                           rmse2=float(rmse2.result()),\r\n                           rrmse1=float(rrmse1.result()),\r\n                           rrmse2=float(rrmse2.result()),\r\n                           sam=float(sam.result()))\r\n\r\n            with file_writer.as_default():\r\n                tf.summary.scalar('val_rmse1',\r\n                                  float(rmse1.result()),\r\n                                  step=epoch)\r\n                tf.summary.scalar('val_rmse2',\r\n                                  float(rmse2.result()),\r\n                                  step=epoch)\r\n                tf.summary.scalar('val_rrmse1',\r\n                                  float(rrmse1.result()),\r\n                                  step=epoch)\r\n                tf.summary.scalar('val_rrmse2',\r\n                                  float(rrmse2.result()),\r\n                                  step=epoch)\r\n                tf.summary.scalar('val_sam', float(sam.result()), step=epoch)\r\n\r\n            # post\r\n            bar.finish()\r\n            rmse1.reset_state()\r\n            rmse2.reset_state()\r\n            rrmse1.reset_state()\r\n            rrmse2.reset_state()\r\n            sam.reset_state()\r\n            datas = getData(datas, data_path)\r\n            # save model\r\n            save_path = manager.save()\r\n```\r\n\r\nthe error logs:\r\n```\r\nraceback (most recent call last):\r\n  File \"mytrainT.py\", line 310, in <module>\r\n    main(des)\r\n  File \"mytrainT.py\", line 235, in main\r\n    test_mean_loss = distributed_test_step(rgb, spec)\r\n  File \"C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1848, in _get_concrete_function_internal_garba\r\nge_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    mytrainT.py:163 distributed_test_step  *\r\n        return strategy.experimental_run_v2(test_step_fn, args=(rgb, spec))\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py:760 experimental_run_v2\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    mytrainT.py:144 test_step_fn  *\r\n        loss = loss_object(spec, fake_spec)\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py:125 __call__\r\n        with K.name_scope(scope_name or self.__class__.__name__), graph_ctx:\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\contextlib.py:112 __enter__\r\n        return next(self.gen)\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:435 graph_context_for_symbolic_tensors\r\n        if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:435 <genexpr>\r\n        if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:345 is_symbolic_tensor\r\n        return tensor._is_graph_tensor  # pylint: disable=protected-access\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\composite_tensor.py:119 _is_graph_tensor\r\n        components = self._type_spec._to_components(self)  # pylint: disable=protected-access\r\n    C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\distribute\\values.py:500 _to_components\r\n        \"Flattening a PerReplica to components is not supported in replica \"\r\n\r\n    ValueError: Flattening a PerReplica to components is not supported in replica context.\r\n```", "Hey, did you figure something about this error? I've got the same with a very similar code.", "@x7night there is a small type in the `def test_step_fn(rgb, sepc):`. So i think it is using `spec` from the `for` loop later which has `PerReplica` values (because the loop variables are in the global scope unfortunately). I think this should get fixed if you fix the typo in `test_step_fn`.\r\n\r\nPlease re-open if this is not the case.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35404\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35404\">No</a>\n"]}, {"number": 35403, "title": "using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.", "body": "something is wrong when I use model.get_layer(),\r\nI was confused what's wrong with my code, and I have never used a `tf.Tensor` as a Python `bool` in my code\r\n\r\nHere are my code:\r\n------------------------------------\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\ndef seq(k):\r\n    result = tf.keras.Sequential(name='seq'+str(k))\r\n    result.add(tf.keras.layers.Conv2D(64, (3, 3), strides=2, padding='same', use_bias=False))\r\n    result.add(tf.keras.layers.BatchNormalization(name='bn_'+str(k)))\r\n    result.add(tf.keras.layers.GaussianNoise(stddev=1, name='noise_'+str(k)))\r\n    result.add(tf.keras.layers.ReLU())\r\n\r\n    return result\r\n\r\n\r\ndef testModel():\r\n    input_layer = layers.Input(shape=(256, 256, 3))\r\n    l0 = seq(0)(input_layer)\r\n    l1 = seq(1)(l0)\r\n    l2 = seq(2)(l1)\r\n    model = tf.keras.Model(input_layer, l2)\r\n    return model\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = testModel()\r\n    model.summary()\r\n    seq0 = model.get_layer('seq_0')\r\n    bn0 = seq0.get_layer('bn_0').output\r\n    noise0 = seq0.get_layer('noise_0').output\r\n\r\n    sub0 = tf.keras.layers.Subtract()([noise0, bn0])\r\n\r\n    new_model = tf.keras.Model(model.input, [sub0, model.output])\r\n\r\n    test_tensor = tf.ones(shape=(1, 256, 256, 3))\r\n\r\n    (out, z) = new_model(test_tensor)\r\n```\r\n----------------------------------------------------------------\r\n2019-12-25 15:15:54.403715: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\nTraceback (most recent call last):\r\n  File \"/home/chenhao-gpu/PycharmProjects/GAN/test.py\", line 36, in <module>\r\n    (out, z) = new_model(test_tensor)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 708, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 860, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py\", line 659, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py\", line 517, in _fused_batch_norm\r\n    training, _fused_batch_norm_training, _fused_batch_norm_inference)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\", line 59, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/smart_cond.py\", line 59, in smart_cond\r\n    name=name)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 1201, in cond\r\n    if pred:\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 765, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 534, in _disallow_bool_casting\r\n    self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 523, in _disallow_in_graph_mode\r\n    \" this function with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.", "comments": ["@happpyosu \r\n\r\nWhich version of TensorFlow you are using?.Can you please provide colab link or reproducible code with proper indentation. It helps us in localizing the issue faster.Thanks!", "@happpyosu \r\n\r\nAny update on this issue please. Thanks!", "@ravikyram \r\nSorry for the late reply. I am using Tensorflow-gpu==2.0.0. \r\nI was building a model with tf.keras API and met the problem. First of all, I define a  Sequential model as:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\ndef seq_model(k):\r\n    model = tf.keras.Sequential(name='seq_'+str(k))\r\n    model.add(layers.Conv2D(64, (3, 3), strides=2, padding='same', use_bias=False))\r\n    model.add(layers.BatchNormalization(name='bn_'+str(k)))\r\n    model.add(layers.GaussianNoise(stddev=1, name='noise_'+str(k)))\r\n    model.add(layers.ReLU())\r\n\r\n    return model\r\n```\r\nThen I made another model by using the seq_model:\r\n```python\r\ndef mymodel():\r\n    input = tf.keras.Input(shape=(224, 224, 3))\r\n\r\n    s0 = seq_model(k=0)(input)\r\n    s1 = seq_model(k=1)(s0)\r\n    s2 = seq_model(k=2)(s1)\r\n\r\n    return tf.keras.Model(input, s2)\r\n```\r\nThen I try to use the API model.get_layer() to get the output of the layer named \"noise_0\" and \"bn_0\"\r\n```python\r\nmodel = mymodel()\r\nnoise_0 = model.get_layer('seq_0').get_layer('noise_0').output\r\nbn_0 = model.get_layer('seq_0').get_layer('bn_0').output\r\n```\r\nSince I want to get the noise value added by \"noise_0\" layer, I build a new Subtract layer, and put the output of the Substrat layer into a new model:\r\n\r\n```python\r\nsub_0 = layers.Subtract()([noise_0, bn_0])\r\nnew_model = tf.keras.Model(model.input, [model.output, sub_0])\r\n```\r\nIt is still no problem until here, however, if I call the new_model, an exception raised:\r\n```python\r\ntest_tensor = tf.zeros(shape=(1, 224, 224, 3))\r\nout_of_new_model = new_model(test_tensor, training=True)\r\n```\r\nThe terminal output:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/home/chenhao-gpu/PycharmProjects/GAN/test.py\", line 35, in <module>\r\n    out_of_new_model = new_model(test_tensor, training=True)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 708, in call\r\n    convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 860, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py\", line 659, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py\", line 517, in _fused_batch_norm\r\n    training, _fused_batch_norm_training, _fused_batch_norm_inference)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\", line 59, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/smart_cond.py\", line 59, in smart_cond\r\n    name=name)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 1201, in cond\r\n    if pred:\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 765, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 534, in _disallow_bool_casting\r\n    self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n  File \"/home/chenhao-gpu/anaconda3/envs/tf2s/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 523, in _disallow_in_graph_mode\r\n    \" this function with @tf.function.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\r\nProcess finished with exit code 1\r\n```\r\nI am confused whether I correctly used the API model.get_layers().\r\n\r\n\r\n\r\n\r\n\r\n", "@happpyosu \r\nCan you please try with `!pip install tensorflow-gpu==2.1-rc2`and see if the error still persists. I am not seeing any issue with TF 2.1.Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!\r\n", "Thanks, it works after updating to tf2.1", "I am facing the same error with TF 2.5. Please let me know if there is any solution. "]}, {"number": 35402, "title": "TF2.0 hub Universal Sentence Encoder Multilingual Sentenepieceop not registered problem", "body": "\r\n**System information**\r\n- Have I written custom code : No\r\n- OS Platform and Distribution : Windows 10 / Google Colab\r\n- TensorFlow version (use command below):tensorflow==2.0.0\r\n- Python version:Python 3.6.9\r\n\r\nHere is my code.\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport tf_sentencepiece\r\nembedding_layer = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\r\n\r\n```\r\nProblematic output\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _get_op_def(self, type)\r\n   3819     try:\r\n-> 3820       return self._op_def_cache[type]\r\n   3821     except KeyError:\r\n\r\nKeyError: 'SentencepieceOp'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py in _get_op_def(self, type)\r\n   3822       with c_api_util.tf_buffer() as buf:\r\n   3823         # pylint: disable=protected-access\r\n-> 3824         c_api.TF_GraphGetOpDef(self._c_graph, compat.as_bytes(type), buf)\r\n   3825         # pylint: enable=protected-access\r\n   3826         data = c_api.TF_GetBuffer(buf)\r\n\r\nNotFoundError: Op type not registered 'SentencepieceOp' in binary running on e2f7765a82a9. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n\r\n```\r\n\r\n\r\nI want to get multilingual sentence embeddings using pretrained Google Universal sentence encoder.  But I could not able to get it from Windows 10 and Google Colab even if the internet shows diffrent. I was able to get sentence vectors using diffrent hub link like https://tfhub.dev/google/universal-sentence-encoder/4. That is not dependent to sentencepiece.", "comments": ["@cbahcevan \r\n\r\nThis issue is more suitable for TensorFlow Hub repo. Please post it on TF Hub repo from [here.](https://github.com/tensorflow/hub/issues) Thanks!", "Thank you for your response. I will post it \r\n> @cbahcevan\r\n> \r\n> This issue is more suitable for TensorFlow Hub repo. Please post it on TF Hub repo from [here.](https://github.com/tensorflow/hub/issues) Thanks!\r\n\r\n"]}, {"number": 35401, "title": "[ROCm] Improved launch config calculation", "body": "This patch allows Tensorflow to make use of the ROCm API method hipOccupancyMaxPotentialBlockSize to calculate the launch config.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35401) for more info**.\n\n<!-- need_author_consent -->", "@ekuznetsov139 \r\nCould you please sign cla ? Thank you.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35401) for more info**.\n\n<!-- ok -->"]}, {"number": 35400, "title": "[ROCm] disabling 3D pooling ops subtests of //tensorflow/cc:gradients_nn_grad_test", "body": "These need to be disabled for ROCm since the underlying framework does not support 3D pooling at this time.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35400) for more info**.\n\n<!-- need_author_consent -->", "@ekuznetsov139 \r\nCould you please sign cla ? Thank you.", "@googlebot I consent", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35400) for more info**.\n\n<!-- ok -->"]}, {"number": 35399, "title": "[ROCm] Adding ROCm support for the CSR Sparse Matrix Ops", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35399) for more info**.\n\n<!-- need_author_consent -->", "Closing as a duplicate of #34800 "]}, {"number": 35398, "title": "Add usage example to math_ops.py", "body": "", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` "]}, {"number": 35397, "title": "no such target '//tensorflow:windows': target 'windows' not declared in package", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from source:\r\n- TensorFlow version 1.13.1:\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?:anaconda3\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source):4.8\r\n- CUDA/cuDNN version: CUDA10, cuDNN7.4\r\n- GPU model and memory: GPU memory 22G, model bert-base-chinese\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build :loader --config=monolithic\r\n\r\n**Any other info / logs**\r\n![image](https://user-images.githubusercontent.com/11387828/71431384-6bf93e80-26c9-11ea-9b98-f915b0e8d17b.png)\r\ni don\u2018t know what happen, i build a tensor add operator demo and it success, but when i load pb model it fails and i rowback to the demo it never success again and report such error, how should i do?", "comments": ["os is linux", "@jude2014, Please provide the complete code snippet to replicate the reported issue. Thanks!", "@jude2014, Any update on the issue. \r\nProvide the exact sequence of commands / steps that you executed before running into the problem. ", "@jude2014, Is this still an issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35397\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35397\">No</a>\n"]}, {"number": 35396, "title": "Added Usage Example for MobileNetV2", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35396) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35396) for more info**.\n\n<!-- ok -->", "Anything else I could do to get this PR approved?", "@dynamicwebpaige  @fchollet  Sir, could you please review my PR?", "Please follow guideliness at https://www.tensorflow.org/community/contribute/docs_ref and make it a doctest", "@akalakheti Any update on this PR, please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@akalakheti Any update on this PR and can you please resolve conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 35395, "title": "[ROCm] stateful random ops", "body": "This test enables and fixes the //tensorflow/python:stateful_random_ops test on the ROCm platform.", "comments": ["@ekuznetsov139 Can you please check build failures? Thanks!"]}]