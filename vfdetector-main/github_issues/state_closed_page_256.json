[{"number": 46777, "title": "Deprecation warnings Model.state_updates when calling keras model `predict` after `get_weights`, in static execution", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): No\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.12\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nSimilar to #44178 I am seeing the following deprecation warning:\r\n\r\n```\r\n/path/to/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\r\n  warnings.warn('`Model.state_updates` will be removed in a future version. '\r\n```\r\n\r\nThat is happening only in static execution (not eager mode). And it seems only when calling `get_weights` before calling `predict`.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo warning.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\n\r\n\r\ndef build_model():\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Dense(1, input_shape=(1,)),\r\n        tf.keras.layers.Dense(10),\r\n        tf.keras.layers.Dense(1)\r\n    ])\r\n    model.compile(optimizer='sgd', loss='mse')\r\n    return model\r\n\r\n\r\ndef do_test():\r\n    x = np.asarray([[0], [1]])\r\n    y = x\r\n    model = build_model()\r\n    model.fit(x=x, y=y, epochs=1000, verbose=0)\r\n    model.get_weights()\r\n    result = model.predict(x=x)\r\n    print('result:', result)\r\n    print('result close?:', np.allclose(result, y, atol=0.01))\r\n\r\n\r\ndef test_with_graph_static_execution():\r\n    with tf.Graph().as_default():  # pylint: disable=not-context-manager\r\n        do_test()\r\n\r\n\r\ndef test_without_explicit_graph_using_static_execution():\r\n    disable_eager_execution()\r\n    do_test()\r\n\r\n\r\ndef test_without_graph_eager_execution():\r\n    do_test()\r\n\r\n\r\ntest_with_graph_static_execution()\r\n# test_without_explicit_graph_using_static_execution()\r\n# test_without_graph_eager_execution()\r\n```\r\n\r\nIn the above example code, you may comment / uncomment to call the desired test function. Out of the three, `test_without_graph_eager_execution` is not showing the warning, `test_with_graph_static_execution` and `test_without_explicit_graph_using_static_execution` are.\r\n\r\nThe specific model doesn't seem to be relevant, neither whether it was first fit.\r\n\r\nRemoving the call to `get_weights` will also avoid the warning (but the warning only appears when then calling `predict`).\r\n(Of course, in the real use case the weights will be used)\r\n\r\nThe main motivation for using the static execution is to improve performance.\r\n\r\n**Other info / logs**\r\n\r\n<details>\r\n<summary>traceback</summary>\r\n\r\n```\r\n  File \"/path/to/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 991, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/path/to/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\", line 712, in predict\r\n    callbacks=callbacks)\r\n  File \"/path/to/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\", line 185, in model_iteration\r\n    f = _make_execution_function(model, mode)\r\n  File \"/path/to/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\", line 555, in _make_execution_function\r\n    return model._make_execution_function(mode)\r\n  File \"/path/to/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 2084, in _make_execution_function\r\n    self._make_predict_function()\r\n  File \"/path/to/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 2072, in _make_predict_function\r\n    updates=self.state_updates,\r\n  File \"/path/to/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2325, in state_updates\r\n    warnings.warn('`Model.state_updates` will be removed in a future version. '\r\n```\r\n</details>", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/7064020ac8077570aad3b03b7b6c8805/46777.ipynb). Thanks!", "@de-code,\r\nIn Tensorflow 2.x, if you want to run your Code in Graph Mode, it is recommended to use **`tf.function`** instead of **`Disabling the Eager Execution`**.\r\n\r\nBy adding **`tf.function`**, the function, `test_without_explicit_graph_using_static_execution` runs without any warning. \r\n\r\nThe function, `test_with_graph_static_execution` results in warning because **`tf.Graph().as_default():`** is deprecated and is not thread safe as per the [documentation](https://www.tensorflow.org/api_docs/python/tf/Graph#using_graphs_directly_deprecated).\r\n\r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/b26381c61f757979da269af6559f9548/46777.ipynb#scrollTo=UMdiyrzT5T9c) of the working code.", "> @de-code,\r\n> In Tensorflow 2.x, if you want to run your Code in Graph Mode, it is recommended to use **`tf.function`** instead of **`Disabling the Eager Execution`**.\r\n\r\nThank you, I didn't realise that disabling eager execution was deprecated as well. Can you please confirm that using `disable_eager_execution` is indeed deprecated?\r\n\r\n> By adding **`tf.function`**, the function, `test_without_explicit_graph_using_static_execution` runs without any warning.\r\n> \r\n> Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/b26381c61f757979da269af6559f9548/46777.ipynb#scrollTo=UMdiyrzT5T9c) of the working code.\r\n\r\nThank you for updating gist.\r\nIs the `tf.function(do_test())` correct? Isn't that running `do_test()` first and then passing the return value (`None`) to `tf.function`?\r\n\r\nI've updated the gist to add print output relating to the eager mode and keep original version as `test_without_explicit_graph_using_disable_eager_execution`:\r\n\r\n```python\r\ndef test_without_explicit_graph_using_disable_eager_execution():\r\n    disable_eager_execution()\r\n    do_test()\r\n```\r\n\r\nand your updated version as `test_without_explicit_graph_using_tf_function`:\r\n\r\n```python\r\ndef test_without_explicit_graph_using_tf_function():\r\n    tf.function(do_test())\r\n```\r\n\r\nI've also tried to update it to something like `tf.function(do_test)()` but that didn't like calling `model.fit` as part of that (neither did it like wrapping `model.predict`.\r\n\r\nIs there actually a way to make the Keras model to use the eager mode less?\r\n\r\nThe output shows that `model.run_eagerly` is actually `False` in all cases (unlike `tf.executing_eagerly()`). Yet I am experiencing significant performance difference (~10x), depending on whether I call `disable_eager_execution` (or use the deprecated `tf.Graph().as_default()`).\r\n\r\nIf understand you correctly, then disabling eager mode globally in any way is deprecated.\r\nIn that case I guess we could close this issue and I could create another performance related issue (if that isn't already covered).\r\nIt would probably be good to mark `disable_eager_execution` as deprecated.", "Was able to reproduce the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/affe462d0c1be1ab07d12a601202682c/untitled51.ipynb#scrollTo=MqcdqCUM9qEk)..Thanks !", "Since the disable_eager_execution is deprecated in Tf 2.x. This function is not necessary if you are using TF2. Eager execution is enabled by default. If you want to use Graph mode please consider tf.function. If you are facing performance issue, with all the required information raise new issue. Thanks!", "Thank you for your response.\r\n\r\n> Since the disable_eager_execution is deprecated in Tf 2.x. This function is not necessary if you are using TF2. Eager execution is enabled by default.\r\n\r\nIsn't that why `disable_eager_execution` is necessary with TF2. Because the default is enabled by default, that is an approach to disable it. (`enable_eager_execution` wouldn't be necessary in TF2)\r\n\r\nCan you confirm that `disable_eager_execution` is indeed deprecated? (I asked that in my previous [comment](https://github.com/tensorflow/tensorflow/issues/46777#issuecomment-774341175) as well).\r\n\r\n> If you want to use Graph mode please consider tf.function.\r\n\r\n`tf.function` wasn't mentioned before. It is unclear how to use it in combination with Keras.\r\nThe examples above don't work.\r\n\r\n> If you are facing performance issue, with all the required information raise new issue.\r\n\r\nFair enough. I am not sure what the would be the ideal tickets is, there seem to be a few:\r\n\r\n- What is the supported method of training and running Keras models in graph mode in TF2?\r\n- Performance issue with training Keras model in eager mode, compared to graph mode, in TF2\r\n\r\nThe latter will be more difficult to measure and validate.\r\n\r\n", "@de-code,\r\n\r\ntf.compat.v1.disable_eager_execution, This API was designed for TensorFlow v1. For more information refer [here](https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_eager_execution).\r\n\r\nSince you are using builtin Keras functions, classes and layers so there is no need to use a @tf.function decorator, they are already treated as a graph by the TF compiler. @tf.function is basically used to convert a normal function into a TensorFlow Graph as mentioned [here](https://www.tensorflow.org/api_docs/python/tf/function). \r\n\r\nWhen you are using custom model \r\n```\r\n@tf.function\r\ndef custom_model(model, ...):\r\n  ...\r\n  outputs = model(...)\r\n  ...\r\n```\r\n\r\n```\r\n# Create the model\r\nmodel = define model layers(...)\r\n# It's a good idea to initialize it too\r\nmodel(<dummy input>)  # or model.build(...)\r\n\r\ncustom_model(model, ...)\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Okay, thank you.\r\n\r\nBased on your response I understand that disabling eager mode globally (except for using `tf.function`) is indeed deprecated,\r\nand that Keras models should already be using the graph mode.\r\n\r\nIn that case the Keras model shouldn't have become faster when disabling eager mode.\r\n\r\nI will test it again and raise a performance issue instead, if that is still an issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46777\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46777\">No</a>\n"]}, {"number": 46776, "title": "flattening operation using tf.reshape inside @tf.function graph raises ValueError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `No`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Windows 10 19042.746`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): `binary`\r\n- TensorFlow version (use command below): `2.4.1`\r\n- Python version: `3.8.5`\r\n- CUDA/cuDNN version: `11.0/8.0`\r\n- GPU model and memory: `2x RTX2080 12GB`\r\n\r\n**Describe the current behavior**\r\nCalling function wrapped with @tf.function and reshaping operation properly reshapes tensor with value `-1` for flattening raises `ValueError`\r\n\r\n**Describe the expected behavior**\r\nCalling function wrapped with @tf.function and reshaping operation properly reshapes tensor with value `-1` for flattening converts tensor into `1D` shape\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\n@tf.function\r\ndef tf_function_wrapped_function():\r\n  reshaped = tf.reshape(tf.ones((64, 64)), -1)\r\n```\r\n\r\n**Full TraceBack**\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \".\\tests\\test_tf_dark_com_preds.py\", line 36, in <module>\r\n    test_tf_reshape()\r\n  File \".\\tests\\test_tf_dark_com_preds.py\", line 32, in test_get_com_preds\r\n    assert test_tf_reshape()\r\n  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 871, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 725, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3196, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 977, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in user code:\r\n    .\\tests\\test_tf_reshape.py:27 tf_function_wrapped_function  *\r\n        reshaped = tf.reshape(tf.ones((64, 64)), -1)\r\n    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\r\n        return target(*args, **kwargs)\r\n    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:195 reshape\r\n        result = gen_array_ops.reshape(tensor, shape, name)\r\n    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8376 reshape\r\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\r\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\r\n    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:590 _create_op_internal\r\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\r\n    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3528 _create_op_internal\r\n        ret = Operation(\r\n    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2015 __init__\r\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\r\n    C:\\Users\\iam\\.conda\\envs\\alyce\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1856 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: Shape must be rank 1 but is rank 0 for '{{node Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](ones, Reshape/shape)' with input shapes: [64,64], [].\r\n```\r\n**Findings**\r\n\r\n1. This example raises `ValueError`\r\n```python\r\n@tf.function\r\ndef tf_function_wrapped_function():\r\n  reshaped = tf.reshape(tf.ones((64, 64)), 64*64)\r\n```\r\n\r\n2. This example do not raise any error\r\n```python\r\n@tf.function\r\ndef tf_function_wrapped_function():\r\n  reshaped = tf.reshape(tf.ones((64, 64)), [1, 64*64])\r\n```\r\n\r\n3. This example raises `ValueError`\r\n```python\r\n@tf.function\r\ndef tf_function_wrapped_function():\r\n  reshaped = tf.reshape(tf.ones((64, 64, 64)), 64*64)\r\n```\r\n\r\n4. This example do not raise any error\r\n ```python\r\ndef tf_function_wrapped_function():\r\n  reshaped = tf.reshape(tf.ones((64, 64)), -1)\r\n```\r\n\r\n5. This is an alternative way that I passed this ValueError\r\n ```python\r\n@tf.function\r\ndef tf_function_wrapped_function():\r\n  reshaped = tf.reshape(tf.ones((64, 64)), (1, 64*64))[0]\r\n```\r\n", "comments": ["@nyanye \r\nI ran the code shared but do not see any error as reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f36092446a679ac58056dea78f2e7d15/untitled511.ipynb)", "@Saduf2019 I think the error is not an universal one but very specific one for my TensorFlow distribution which is compiled for Python 3.8.5 and Windows operating system.", "@nyanye\r\nCould you please try on virtual env and let us know if you still face the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46776\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46776\">No</a>\n", "The error is till there"]}, {"number": 46775, "title": "hexagon", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": []}, {"number": 46774, "title": "Switch to single-pip-package and install nvprune", "body": "- nvprune is now required for building TF\n- since 'tensorflow' is now the GPU package, I switched the -gpu images to install 'tensorflow' and the non-gpu images to install 'tensorflow-cpu'.", "comments": []}, {"number": 46773, "title": "Time series tutorial throws error when selecting multiple columns", "body": "**Tensorflow version is 2.4.0**.\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/structured_data/time_series\r\n\r\nIssue is also listed as a [StackOverflow question](https://stackoverflow.com/questions/65944671/how-to-select-multiple-label-columns-for-the-tensorflow-time-series-tutorial).\r\n\r\n\r\n## Description of issue (what needs changing):\r\nThe current time series tutorial only works when specifying one label, or leaving `label_columns=None` which defaults to all columns as labels. When selecting multiple columns in the WindowGenerator, such as:\r\n```python\r\nOUT_STEPS = 24\r\nmulti_window = WindowGenerator(input_width=24,\r\n                               label_width=OUT_STEPS,\r\n                               shift=OUT_STEPS,label_columns=['T (degC)','p (mbar)'])\r\n```\r\n\r\nthe tutorial throws the error:\r\n```\r\nValueError: Dimensions must be equal, but are 19 and 2 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](feed_back/transpose, IteratorGetNext:1)' with input shapes: [?,24,19], [?,24,2].\r\n```\r\n### Clear description\r\n\r\nAs someone who is new to the framework, it is not clear how to adapt the tutorial to address an arbitrary subset of columns as labels, given the error provided.", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0a5efc348d77dfc37f64daba60efea44/46773.ipynb#scrollTo=_5iaHSaJ9Rxv&line=3&uniqifier=1). Thanks!", "Closing this issue since [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) is the right platform for it. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46773\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46773\">No</a>\n", "@ymodak or @amahendrakar, can either of you provide an explanation or answer for the issue, either here or on StackOverflow as to what needs to be modified (or fixed) for label selection? As someone who does not yet understand the framework, providing an MRE on SO is rather difficult, and so posting the issue here as a question about the tutorial seemed a bit more appropriate.\r\n\r\nAs the tutorial exists, it seems to allow you to select any arbitrary subset of columns as labels, but there is no clear explanation in the tutorial as to why this is not possible or what would need to be changed to enable that functionality, which I think is a valuable learning opportunity currently lost in the tutorial.", "I apologize for the inconvenience. I think the case `(Multiple Steps for Multiple Features)` you are trying is out of the scope of the tutorial.\r\n\r\nThe tutorial is split into 2 sections:\r\n1) Forecast for a single timestep:\r\n         - A single feature.\r\n         - All features\r\nThese tutorial sections for single timestep execute successfully for multiple features across various architectures in the tutorial such as Linear, DNN, CNN, RNN.\r\nSee [gist](https://colab.research.google.com/gist/ymodak/a69f2dcecda7d1704ed21496a95fb0ad/time_series.ipynb#scrollTo=D1bbPiR3VAm_) for your reference. I have used `label_columns=['T (degC)','p (mbar)']`\r\n2) Forecast multiple steps:\r\n        - Single-shot: Make the predictions all at once.\r\n        - Autoregressive: Make one prediction at a time and feed the output back to the model.\r\nAdding multiple features `(label_columns=['T (degC)','p (mbar)'])` here fails with the error you reported.\r\nThanks!\r\n\r\n"]}, {"number": 46772, "title": "Using MaxPooling1D with 4D and higher Tensors", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): Not sure?\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, MaxPooling1D layer only takes 3D tensor as input, and data_format argument specifies if features are first or last.\r\nIf tensor is 4D, an error gets thrown. However, the same API can be used to handle higher-dimensional data, e. 4D tensors, and there are use-cases for that. For example, when trying to implement multiple instance learning, data is organized in bags of instances, and for time series would become 4D.\r\n\r\n```\r\nn = 6\r\nsample_size = 300\r\ncode_size = 50\r\nlearning_rate = 0.001\r\nn_bags= None\r\n\r\n# autoencoder: n_bags X n_instances_in_bag X n_samples (timesteps) X n_measurements\r\ninput_window = Input(shape=(n_bags,sample_size, n)) \r\nx = Conv1D(filters=40, kernel_size=21, activation='relu', padding='valid')(input_window)\r\nx = MaxPooling1D(pool_size=2)(x)\r\n```\r\n\r\nCurrently, this throws an error:\r\n```\r\nValueError: Input 0 of layer max_pooling1d_4 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, None, 280, 40]\r\n```\r\n\r\nbut there is no reason why this can't work. Note that Conv1D layer works just fine with this.\r\n\r\n**Will this change the current api? How?**\r\nI believe the API can remain the same\r\n\r\n\r\n", "comments": ["@dmitra79 @jvishnuvardhan PoolingND layers only take inputs of (N+2) dimensions. I would like to add the feature of accepting inputs of >=N dimensions.", "It was [suggested](https://stackoverflow.com/questions/65945784/using-maxpool1d-on-inputs-with-4-dimensions-4d-tensor) to me that one can use a 2D filter with length 1 in some dimensions, ex: MaxPooling2D with (1,2) would work here. So there is a workaround. But it'd be nice to have the API support this\r\n", "It sounds like you want to do 1D pooling over one dimension of a multi-dimensional tensor. I'd recommend implementing your own layer to do that. It would simply implement a `call` method that would use `tf.nn.pool`. Ref: https://www.tensorflow.org/api_docs/python/tf/nn/pool"]}, {"number": 46771, "title": "Fix the 9KB binary size increase.", "body": "https://github.com/tensorflow/tensorflow/commit/9ee7896d229278f582a3a381c6d22ec0559d9765 added a function that was unused for the TFLM build yet still contributed to binary size increase.\r\n\r\nThis change moves that function inside #ifndef TF_LITE_STATIC_MEMORY and so avoids the size increase.\r\n\r\nManually tested with the following commands:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nBefore this change:\r\n```\r\n   text\t   data\t    bss\t    dec\r\n  67192\t  44512\t  24968\t 136672\r\n```\r\n\r\nAfter this change:\r\n```\r\n   text\t   data\t    bss\t    dec\r\n  62184\t  40180\t  24872\t 127236\r\n```\r\n\r\nFixes http://b/178731766\r\n", "comments": []}, {"number": 46770, "title": "Inconsistency of computations with different batch size", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla K80 11441MiB\r\n\r\n**Describe the current behavior**\r\n\r\nComputations with batch size 2 or two consecutive computations with batch size 1 lead to different results if the number of units is bigger than 64 (for tf.keras.layers.Dense) or the last dimension is equal to 1 (tf.matmul).\r\n\r\n\r\n**Describe the expected behavior**\r\nResults supposed to be same. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nlinear = tf.keras.layers.Dense(units=65)\r\ndata = 50*tf.ones(shape=[2, 93, 3286])\r\nr = linear(data)\r\nr_0 = linear(data[:1])\r\nr_1 = linear(data[1:])\r\nprint(tf.reduce_max(tf.abs(r[:1]-r_0)/tf.abs(r[:1])), tf.reduce_max(tf.abs(r[:1]-r_0)))\r\nprint(tf.reduce_max(tf.abs(r[1:]-r_1)/tf.abs(r[1:])), tf.reduce_max(tf.abs(r[1:]-r_1)))\r\n```\r\n\r\n```python\r\ndata_1 = 1.22222*tf.random.normal(shape=(2, 20, 10, 1))\r\ndata_2 = 0.37*tf.random.normal(shape=(2, 20, 10, 52))\r\nr = tf.matmul(data_1, data_2, transpose_a=True)\r\nr_0 = tf.matmul(data_1[:1], data_2[:1], transpose_a=True)\r\nr_1 = tf.matmul(data_1[1:], data_2[1:], transpose_a=True)\r\nprint(tf.reduce_max(tf.abs(r[:1]-r_0)/tf.abs(r[:1])), tf.reduce_max(tf.abs(r[:1]-r_0)))\r\nprint(tf.reduce_max(tf.abs(r[1:]-r_1)/tf.abs(r[1:])), tf.reduce_max(tf.abs(r[1:]-r_1)))\r\n```\r\n", "comments": ["@artem-s-shevchenko \r\n\r\nI have tried in colab with TF version 2.2, nightly version. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/65f8ac5564cc2c33470eaf77772cb47d/untitled644.ipynb).You are also seeing the same behavior?\r\nThanks!", "@ravikyram \r\nNot exactly same.\r\n\r\nFor tf.keras.layers.Dense:\r\ntf.Tensor(1.754481e-05, shape=(), dtype=float32) tf.Tensor(0.00024414062, shape=(), dtype=float32)\r\ntf.Tensor(1.754481e-05, shape=(), dtype=float32) tf.Tensor(0.00024414062, shape=(), dtype=float32)\r\n\r\nFor tf.matmul:\r\ntf.Tensor(0.00043521042, shape=(), dtype=float32) tf.Tensor(4.7683716e-07, shape=(), dtype=float32)\r\ntf.Tensor(0.00037936267, shape=(), dtype=float32) tf.Tensor(4.7683716e-07, shape=(), dtype=float32)\r\n", "Hi @artem-s-shevchenko, it's not related to Dense layer or matmul, but the reduction in TensorFlow backend is non-deterministic. If you change to test with `np.testing.assert_allclose(r_0.numpy(), r_1.numpy())`, you can find they are the same. There is an RFC in community https://github.com/tensorflow/community/pull/346.\r\n\r\nhttps://colab.research.google.com/drive/1GlMBO3Ldek1QybLw01B-izVslBBZAVOy?usp=sharing", "Hi, @WindQAQ ,\r\nIf I try ```np.testing.assert_allclose(r[:1].numpy(), r_0.numpy()) ``` or ```np.testing.assert_allclose(r[1:].numpy(), r_1.numpy())```, problem is still present.", "@artem-s-shevchenko Can you please share a gist showing AssertionError? I tried WindQAQ's attached gist and find that AssertionError is not raised.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ymodak \r\nhttps://gist.github.com/artem-s-shevchenko/17be8e0b03c3f76e2eefc7e1f19a3d80\r\nhttps://gist.github.com/artem-s-shevchenko/daa582568968e0cc6423500e7f7b6b34\r\nColab [gist](https://colab.research.google.com/gist/ymodak/e3af1bad65491dbd83b34fb030d411ec/untitled16.ipynb)", "Was able to reproduce the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/b3cf81415e542275bf098fcb4689df30/untitled52.ipynb)..Thanks !", "We make no guarantees of consistency if order of operations change, or if data layout changes.  Different code paths may be exercised, and they may use different intrinsics (e.g. in one case may use fused multiply-adds, but not in another).  The results *should* be consistent within some appropriate tolerance.", "@artem-s-shevchenko, \r\n\r\nCan you take a look at [this](https://github.com/tensorflow/tensorflow/issues/46770#issuecomment-921153932) comment and let us know if it helps?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46770\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46770\">No</a>\n"]}, {"number": 46769, "title": "PoseNet model readme points to the wrong paper reference", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models/pose_estimation/overview.md\r\nor\r\nhttps://www.tensorflow.org/lite/models/pose_estimation/overview\r\n\r\n## Description of issue (what needs changing):\r\nIn Further Reading the reference is pointing to the wrong PoseNet paper. It points to\r\nhttps://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.pdf\r\nwhich is also called PoseNet but it is about camera relocalization, not body pose detection.\r\n\r\n### Correct links\r\n\r\nAccording to this blog post\r\nhttps://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\r\nI think the correct papers are here:\r\nhttps://arxiv.org/abs/1701.01779\r\nand\r\nhttps://arxiv.org/abs/1803.08225\r\n\r\n", "comments": ["Thanks for the report! Will be fixed shortly with https://arxiv.org/abs/1803.08225"]}, {"number": 46768, "title": "Fix the hifimini build.", "body": "https://github.com/tensorflow/tensorflow/pull/46712 broke the hifimini build and is the reason why the Xtensa build badge is currently red.\r\n\r\nThis change fixes the hifimini build.\r\n\r\nManually tested that all the tests pass with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test -j8\r\n```\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46767, "title": "RFC: Port 16x8 TensorFlow Lite operators to TensorFlow Lite for Microcontrollers", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Thanks @dansitu for the RFC. In general the TFLM team is supportive of pursuing this addition.\r\n\r\nA next step would be to share this RFC with the SIG-micro community and also add it as a discussion item for the next SIG-micro meeting.\r\n\r\nWe'd be up for working with you to make the implementation plan more specific so that the work involved can be appropriately scoped.", "Tagging @freddan80 and @yair-ehrenwald as two contributors that I know are interested in this direction.\r\n", "Last comment is that can you add an RFC section to the bottom of the [TFLM README](https://github.com/tensorflow/tensorflow/blob/f57dcc666650319e8e9a7904b3aa66703a323638/tensorflow/lite/micro/README.md) and link to the current RFC from there?\r\n\r\nSomething like:\r\n\r\n# RFCs\r\n\r\n 1. Port 16x8 TensorFlow Lite operators to TensorFlow Lite for Microcontrollers\r\n", "I haven't looked into the details of the RFC, but will do so next week. In general, we're supportive of the idea to bring in the 16b activation support into TFLM.", "Thank you @advaitjain, I'll address these ASAP.", "I added some thoughts.", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Yes, this is still being worked on.\n\nOn Sat, Mar 6, 2021 at 6:01 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> It has been 18 days with no activity and the awaiting response label was\n> assigned. Is this PR still valid? Assigning the stalled label. Please\n> comment to reassure me that this is still being worked on.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/46767#issuecomment-792137703>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAC4SY5MXQ256CZONUTA5PLTCLF45ANCNFSM4WXUCLCQ>\n> .\n>\n\n\n-- \nDaniel Situnayake\nFounding TinyML Engineer, Edge Impulse <http://edgeimpulse.com/>\ndan@edgeimpulse.com\n+1 415-940-2607\nTwitter <http://twitter.com/dansitu> | LinkedIn\n<http://linkedin.com/in/situnayake>\n", "@dansitu  Any update on this PR? Please. Thanks!", "> @dansitu Any update on this PR? Please. Thanks!\r\n\r\nOur team will be working further on this in the next week or so.", "I've updated the RFC based on the comments, and we plan to begin work on this soon. @aureleq from the Edge Impulse team will be helping coordinate the process on our side.", "Thanks @advaitjain, that proposal sounds great and I've updated the RFC accordingly. I've also moved it into the correct directory."]}, {"number": 46765, "title": "numpy() is not working on Tensor object in TF2.x", "body": "**Please consider the following code snippet**\r\n\r\nfrom tensorflow.keras.layers import Layer\r\n\r\nclass SimpleDense(Layer):\r\n    \r\n    def __init__(self, units=32):\r\n        '''Initializes the instance attributes'''\r\n        super(SimpleDense, self).__init__()\r\n        self.units = units\r\n\r\n    def build(self, input_shape):\r\n        '''Create the state of the layer (weights)'''\r\n        # initialize the weights\r\n        w_init = tf.random_normal_initializer()\r\n        self.w = tf.Variable(name=\"kernel\",\r\n            initial_value=w_init(shape=(input_shape[-1], self.units),\r\n                                 dtype='float32'),\r\n            trainable=True)\r\n\r\n        # initialize the biases\r\n        b_init = tf.zeros_initializer()\r\n        self.b = tf.Variable(name=\"bias\",\r\n            initial_value=b_init(shape=(self.units,), dtype='float32'),\r\n            trainable=True)\r\n\r\n    def call(self, inputs):\r\n        '''Defines the computation from inputs to outputs'''\r\n        print(inputs.numpy().shape)\r\n        print(self.w.numpy().shape)\r\n        print(tf.matmul(inputs,self.w).numpy().shape)\r\n        print(self.b.numpy().shape)\r\n        print((tf.matmul(inputs,self.w)+self.b).numpy().shape)\r\n        return tf.matmul(inputs, self.w) + self.b\r\n\r\nmy_dense = SimpleDense(units=1)\r\nx = tf.ones((3, 2))\r\ny = my_dense(x)   # I am able to execute all the print of the call method.\r\n\r\nxs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\r\nys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\r\n\r\nmy_layer = SimpleDense(units=1)\r\nmodel = tf.keras.Sequential([my_layer])\r\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\r\nmodel.fit(xs, ys, epochs=500,verbose=0)  # I am not able to execute any of the print of the call method.\r\n\r\n- There error is following.\r\nAttributeError: in converted code:\r\n        print(inputs.numpy().shape)\r\n\r\n    AttributeError: 'Tensor' object has no attribute 'numpy'\r\n\r\n_**I am not able to understand the above error as per my knowledge Tensor object has numpy() attribute available in tf2.x version and I am using TF2.0**_\r\n\r\nPlease Explain why i am getting the above error. It will be very helpful.\r\n\r\n", "comments": ["Hi @Alok-Ranjan23. Because the model is compiled with `run_eagerly=False` by default, `.numpy()` attribute can not be used. You can manually set `run_eagerly=True` to make it  run in eager mode and access numpy array. BTW, if you only want to get the shape of tensor, you can directly access it with `inputs.shape`.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#compile", "Hi @WindQAQ , As per my knowledge eager_execution is **by-default** in TF2.x. Then why you are telling me to set  **run_eagerly=True** in model.compile(...) whereas i think that it is already set True in TF2.x. Please explain. ", "> As per my knowledge eager_execution is **by-default** in TF2.x.\r\n\r\nCorrect.\r\n\r\n> Then why you are telling me to set **run_eagerly=True** in model.compile(...) whereas i think that it is already set True in TF2.x. \r\n\r\nFor keras model compilation, it is set to False for performance issue. The model will execute in the scope of `tf.function`. Eager execution is enable by default, but when inside `tf.function`, tensor behaves like in graph mode.\r\n\r\n![image](https://user-images.githubusercontent.com/11615393/106235408-77157c80-61af-11eb-9dbe-28b5b52a3d30.png)\r\n\r\n\r\n", "Thanks @WindQAQ ,\r\n\r\nAs per my understanding, there are two eager environment, One (i.e. TF2.x eager mode) is set as True and Other (i.e. tf.keras eager_mode) is set as False so that it will run inside tf.function for performance issue. Am i correct?\r\n\r\nDoes setting **eager_mode as True in TF2.x** not affect TF2.x performance with respect to TF1.x? \r\n\r\nCan you please provide a link for tf.function documentation? ", "> As per my understanding, there are two eager environment, One (i.e. TF2.x eager mode) is set as True and Other (i.e. tf.keras eager_mode) is set as False so that it will run inside tf.function for performance issue. Am i correct?\r\n> Does setting **eager_mode as True in TF2.x** not affect TF2.x performance with respect to TF1.x?\r\n\r\nHere is a more detailed explanation of `tf.function`. For performance, in most cases, eager context will introduce some overhead, but `tf.function` makes the certain piece of your program (typically the training/inference pipeline) run in graph mode, so that the overhead is removed.\r\n\r\nhttps://www.tensorflow.org/guide/function\r\n\r\n> Can you please provide a link for tf.function documentation?\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/function?version=nightly", "Thanks @WindQAQ ,\r\n\r\n**If** I will create my model without using tf.keras, then **tf.function** will be in picture or not?", "> **If** I will create my model without using tf.keras, then **tf.function** will be in picture or not?\r\n\r\nNope, you should decorate it in your own.", "Hi @WindQAQ ,\r\n\r\nHow did TF2.x handle **eager_mode=True(default)** performance overhead if it will not use tf.function", "Unfortunately, most optimizations occurs in graph mode so it's recommended to use `tf.function`. Is there any reason that you don't or you can't use it?", "Hi @WindQAQ ,\r\nAs you have said that in reply of that(If I will create my model without using tf.keras, then tf.function will be in picture or not?),\r\nIt will not use tf.function, So the performance will degrade in TF2.x if i will not use my own written tf.function. Am i correct?\r\n\r\nCan i run TF2.x code(without tf.keras) without using tf.function?\r\n", "> As you have said that in reply of that(If I will create my model without using tf.keras, then tf.function will be in picture or not?),\r\n> It will not use tf.function, So the performance will degrade in TF2.x if i will not use my own written tf.function. Am i correct?\r\n\r\nCorrect.\r\n\r\n> Can i run TF2.x code(without tf.keras) without using tf.function?\r\n\r\nYes you can, just not to decorate your function with `tf.function`. When you want, just do\r\n\r\n```python\r\n@tf.function\r\ndef func():\r\n    # Your function\r\n```", "Thank you @WindQAQ , closing this issue now, Thank you for the clarification."]}, {"number": 46762, "title": "MHA not attending the specified dimension", "body": "Hi, it looks like if I add a channel dimension, the number of parameters in the Dense layer is no longer correct:\r\nThis is ok:\r\n\r\ninputs = Input(shape=[1800])\r\ny = MultiHeadAttention(num_heads=10,\r\nkey_dim=10,\r\nuse_bias=False,\r\nattention_axes = 0,\r\n)(inputs,inputs)\r\nmodel = Model(inputs=inputs, outputs=y)\r\nprint(model.summary())  # This shows that we have 720,000  parameters\r\nThis is not:\r\n\r\ninputs = Input(shape=[1800,1]). # added channel dim here\r\ny = MultiHeadAttention(num_heads=10,\r\nkey_dim=10,\r\nuse_bias=False,\r\nattention_axes = 0,  #if this is set to None the same error persists\r\n)(inputs,inputs)\r\nmodel = Model(inputs=inputs, outputs=y)\r\nprint(model.summary()) # this is not correct, it shows 400 parameters, although I specify to attend the first axis", "comments": ["I have tried in colab with TF version 2.4, nightly version (`2.5.0-dev20210129`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/cecbb03c0bfd90335a36bd0ebac44708/untitled645.ipynb). Thanks!", "inputs = Input(shape=[1800]) is not a valid input to MHA.\r\ninputs = Input(shape=[1800,1]). is valid. [batch size, 1800, 1]\r\nThe expected number of params is indeed 400 = 10 * 10 * 4, where 10 * 10 is for q, k, v and output projection matrices. The attention axis should be the sequence dimension, which is 1800.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46762\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46762\">No</a>\n"]}, {"number": 46761, "title": "[tf.data] eager mode support for experimental benchmarks part1", "body": "This PR is a follow up of https://github.com/tensorflow/tensorflow/pull/46320 and extends the eager mode support to a subset of experimental benchmarks.\r\n\r\nSample result:\r\n\r\n```console\r\nentry {\r\n  name: \"CsvDatasetBenchmark.csv_strings_map_decode_csv_with_cols_256.eager\"\r\n  iters: 10\r\n  wall_time: 0.0020652782201766966\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 5000.0\r\n    }\r\n  }\r\n}\r\n\r\n#####\r\n\r\nentry {\r\n  name: \"OptimizationBenchmark.map_fusion_noopt_chain_length_50.eager\"\r\n  iters: 10\r\n  wall_time: 2.3424625396728516e-05\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 100.0\r\n    }\r\n  }\r\n}\r\n\r\nentry {\r\n  name: \"OptimizationBenchmark.map_fusion_opt_chain_length_50.eager\"\r\n  iters: 10\r\n  wall_time: 5.298852920532227e-06\r\n  extras {\r\n    key: \"num_elements\"\r\n    value {\r\n      double_value: 100.0\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\ncc: @jsimsa ", "comments": ["@kvignesh1420 thanks again for the PR ... let me know if you are looking for further opportunities of how to contribute to tf.data", "@jsimsa I was thinking of refactoring the remaining experimental benchmarks after this PR is merged. Also, I am definitely interested in contributing to `tf.data`. It would be great if you can point me to modules that need enhancements or any new features that we can work on. Let me know. Thanks!", "@kvignesh1420 refactoring the remaining experimental benchmarks makes sense.\r\n\r\nHere are some further test code related suggestions:\r\n1. Switching `tensorflow/python/data/util/` tests to use TF combinations (like the rest of tests under `tensorflow/python/data/`) instead of the `test_util` decorator to express which combination of (graph + eager) x (TF 1 + TF 2) tests should be executed.\r\n2. Adding eager mode coverage for tests in `tensorflow/python/data/kernel_tests/map_test.py` that have a TODO for this.\r\n3. Adding tests for `OptionalSpec` and `DatasetSpec` to `tensorflow/python/data/util/structure_test.py`.\r\n4. Moving checkpoint tests from `tensorflow/python/data/experimental/kernel_tests/serialization` to respective tests in `tensorflow/python/data/kernel_tests` and `tensorflow/python/data/experimental/kernel_tests` (e.g. `map_dataset_serialization_test.py` to `map_test.py`) and creating checkpoint tests for transformations that do not have one at the moment.\r\n5. Address all TODO(b/121264236) as support for emulating multiple devices in TF 2 now exists.\r\n6. Refactor the test in `tensorflow/python/data/kernel_tests/from_sparse_tensor_slices_test.py` to be a parameterized test.\r\n\r\nFor non-test code, we could discuss steps needed to graduate certain experimental APIs (e.g. tf.data.experimental.{save, load, make_batched_features_dataset, ...}) to core APIs (e.g. under tf.data.*). ", "@jsimsa thanks for the suggestions! I will take a look.\r\nAlso, I have rebased with the master branch and resolved a minor conflict in this PR. Can you please add the required labels?\r\n\r\ncc: @gbaned "]}, {"number": 46760, "title": "Multiheadattention layer dimensions", "body": "## This works ok\r\ninputs = Input(shape=[1800])\r\ny = MultiHeadAttention(num_heads=10,\r\nkey_dim=10,\r\nuse_bias=False,\r\nattention_axes = 0,\r\n)(inputs,inputs)\r\nmodel = Model(inputs=inputs, outputs=y)\r\nprint(model.summary())\r\n\r\n## This is not working properly:\r\n\r\ninputs = Input(shape=[1800,1]). # added channel dim here\r\ny = MultiHeadAttention(num_heads=10,\r\nkey_dim=10,\r\nuse_bias=False,\r\nattention_axes = 0,\r\n)(inputs,inputs)\r\nmodel = Model(inputs=inputs, outputs=y)\r\nprint(model.summary())", "comments": ["@je-santos,\r\nI was able to run the code without any issues with TensorFlow v2.4. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/771a3515014438784017faacdb81816f/46760.ipynb). Thanks!", "Hi, the code runs all right. But the dimensions are not correct. In the second case the dense layers do not have the right number of parameters.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46760\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46760\">No</a>\n"]}, {"number": 46759, "title": "How to install Tensorflow on AIX7.2 server", "body": "I am trying to install Tensorflow on AIX7.2 server.  Usually, the \"any\" wheel file is good on AIX.  Otherwise, I may try to build it with tar.zip source file.  However,  Tensorflow has neither \"any\" wheel file nor tar.zip source file at PYPI website.  I tried to download tensorflow-master.zip file here, but the installation try failed as there is no setup.py file.  Is there any chance I can install Tensorflow on AIX?\r\n\r\nThanks.  ", "comments": ["@bergen288,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nAlso, please take a look at [this guide](https://www.tensorflow.org/install/source#tested_build_configurations) to build TensorFlow from source and check if it helps. Thanks!", "Below is the information.  I checked your guide.  It looks like Bazel is required in order to build Tensorflow from source.  Unfortunately, I don't see AIX is listed as supported in Bazel website.  Sounds like I can't install Tensorflow on AIX anyway.\r\n```\r\nOS Platform: AIX7.2\r\nMobile device:  N/A\r\nTensorFlow installed from (source or binary): source (to be installed)\r\nTensorFlow version: 2.4?\r\nPython version: 3.7.9\r\nInstalled using virtualenv? pip? conda?: pip\r\nBazel version (if compiling from source): to be installed (unsupported?)\r\nGCC/Compiler version (if compiling from source): 8.3.0\r\nCUDA/cuDNN version: ?\r\nGPU model and memory: ?\r\n```", "@bergen288 \r\n\r\nCan you try updating to the latest stable version 2.6.0 and let us know if the issue still persists? ", "Sorry, I forgot to close the issue.  Since AIX has so many installation issues with Python/R packages, I have requested Windows Server 2016 to dedicate for Python/R workloads.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46759\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46759\">No</a>\n"]}, {"number": 46758, "title": "Pass kwargs in apply_gradients to wrapped optimizer", "body": "This allows usage of tfa.optimizers.DecoupledWeightDecayExtension optimizers like AdamW with the apply_gradients parameter \"decay_var_list\" to select decay vars. \r\nThis is a common application for AdamW since weight decay on BatchNorm params or biases is not always intended.", "comments": ["I'm a little worried about passing `kwargs` through `merge_call` and `call_for_each_replica`. Distribute Strategies often will modify inputs in strange ways when passed to one of these functions, which is why we use a `_UnwrapPreventer` for the variables when calling `call_for_each_replica`. But there is no trick equivalent to `_UnwrapPreventer` for `merge_call`.\r\n\r\nThe distribute strategy team is working on deprecating `merge_call` and removing it from Optimizer and LossScaleOptimizer, so I think it's best to just wait until they do so before merging this PR. They plan on doing this in a few weeks. Alternatively, if you want this in sooner, please manually test this change with a DecoupledWeightDecayExtension optimizer where you pass `decay_var_list` with a MirroredStrategy to confirm that it works correctly. Unfortunately you cannot add a TF test itself which does this since it uses TF-addons.", "@andreABbauer Can you please check @reedwm's comments and resolve conflicts?. Thanks!", "@andreABbauer Any update on this PR? Please. Thanks!"]}, {"number": 46756, "title": "\"minlength\" attribute for tf.math.bincount not honored during run_eagerly=False", "body": "-Microsoft Windows 10 version 10.0.18363 Build 18363\r\n-pip install tensorflow\r\n-TensorFlow v2.4.0-49-g85c8b2a817f 2.4.1\r\n-Python 3.7.6\r\n\r\nI am doing a Monte Carlo simulation inside my custom metric to determine the probabilities over 10k samples. I have used the minlength and maxlength attributes of the `tf.math.bincount` such that the size of the tensor `result` should be same as tensor `mus`\r\nUpon using `tf.math.bincount` in a custom metric like below,\r\n\r\n```\r\n    distr = tfp.distributions.Normal(loc=mus, scale=sigmas)\r\n    samples_min = tf.math.argmin(distr.sample((10000,)), axis=1)\r\n    result = tf.math.bincount(tf.cast(tf.reshape(samples_min, (10000,)), dtype=tf.int32), minlength=mus.shape[0], maxlength=mus.shape[0], dtype=tf.float32) / tf.constant(10000, dtype=tf.float32)\r\n\r\n    filter1 = tf.where(tf.squeeze(y_public_estimate) < result, tf.ones_like(result), tf.zeros_like(result))\r\n\r\n```\r\nNow if I run the above code with `model.compile(run_eagerly=False...` it throws below error,  and so I tried to debug the issue with `model.compile(run_eagerly=True...` and it doesn't produce any error ever. \r\n\r\nI suspect the `InvalidArgumentError` is generated in the less-than operation between ` tf.squeeze(y_public_estimate)` and `result `  where their shapes are mismatched, but it only happens when `run_eagerly=False` not when `run_eagerly=True` and btw the length of the tensor `y_public_estimate` is same as tensor `mus`, so I am thinking, maybe the `minlength` or `maxlength` attribute in `tf.math.bincount` does not seem to work with `run_eagerly=False`\r\n\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-99-3eba1530fe17> in <module>\r\n     10 \r\n     11 model = build_model(continuous, categoricals_map)\r\n---> 12 history = model.fit(train_generator, validation_data=valid_generator, epochs=900, callbacks=[terminate_onnan, check_pointer, early_stop], verbose=1, workers=NUM_CORES)\r\n\r\ne:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1098                 _r=1):\r\n   1099               callbacks.on_train_batch_begin(step)\r\n-> 1100               tmp_logs = self.train_function(iterator)\r\n   1101               if data_handler.should_sync:\r\n   1102                 context.async_wait()\r\n\r\ne:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    826     tracing_count = self.experimental_get_tracing_count()\r\n    827     with trace.Trace(self._name) as tm:\r\n--> 828       result = self._call(*args, **kwds)\r\n    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n    830       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\ne:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    853       # In this case we have created variables on the first call, so we run the\r\n    854       # defunned version which is guaranteed to never create variables.\r\n--> 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    856     elif self._stateful_fn is not None:\r\n    857       # Release the lock early so that multiple threads can perform the call\r\n\r\ne:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n   2942     return graph_function._call_flat(\r\n-> 2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n   2944 \r\n   2945   @property\r\n\r\ne:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1917       # No tape is watching; skip to running the function.\r\n   1918       return self._build_call_outputs(self._inference_function.call(\r\n-> 1919           ctx, args, cancellation_manager=cancellation_manager))\r\n   1920     forward_backward = self._select_forward_and_backward_functions(\r\n   1921         args,\r\n\r\ne:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    558               inputs=args,\r\n    559               attrs=attrs,\r\n--> 560               ctx=ctx)\r\n    561         else:\r\n    562           outputs = execute.execute_with_cancellation(\r\n\r\ne:\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError:  Incompatible shapes: [18] vs. [17]\r\n\t [[node Less (defined at <ipython-input-95-93a59f364a63>:39) ]] [Op:__inference_train_function_130872]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node Less:\r\n truediv (defined at <ipython-input-95-93a59f364a63>:37)\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\n", "comments": ["@mdalvi,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here and also the dataset you are using. Thanks!", "@amahendrakar \r\nThis may not be the most optimized code, but yeah! it reproduces the error\r\n\r\n```\r\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\n# In[21]:\r\n\r\n\r\nimport numpy as np\r\nimport joblib\r\nfrom urllib.request import urlopen\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TerminateOnNaN\r\nfrom tensorflow.keras.layers import Dense, Dropout, Input, Concatenate, Softmax, Reshape, Embedding, concatenate\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\n\r\n# In[19]:\r\n\r\n\r\nX = joblib.load(urlopen(\"https://turingequations.com/issue_data/X.joblib\"))\r\ny = joblib.load(urlopen(\"https://turingequations.com/issue_data/y.joblib\"))\r\nbatch_indexes = joblib.load(urlopen(\"https://turingequations.com/issue_data/batch_indexes.joblib\"))\r\nembeddng_size = joblib.load(urlopen(\"https://turingequations.com/issue_data/embedding_size.joblib\"))\r\n\r\n\r\n# In[22]:\r\n\r\n\r\ndef nnelu(ip):\r\n    \"\"\"\r\n    Computes the Non-Negative Exponential Linear Unit\r\n    https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca\r\n    \"\"\"\r\n    return tf.add(tf.constant(1, dtype=tf.float32), tf.nn.elu(ip))\r\ntf.keras.utils.get_custom_objects().update({'nnelu': nnelu})\r\n\r\nclass BatchGenerator(tf.keras.utils.Sequence):\r\n    \"\"\"\r\n    https://stackoverflow.com/questions/17315737/split-a-large-pandas-dataframe\r\n    https://github.com/mohantyaditya/Datagenerator/blob/master/datagenerator.py\r\n    https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/#comment-505744\r\n    \"\"\"\r\n    def __init__(self, X, y, batch_indexes, shuffle=True):\r\n        \r\n        # List of numpy arrays of data\r\n        self.X = X\r\n        \r\n        # Numpy array of labels\r\n        self.y = y\r\n        \r\n        # Pre-populated list of batch indices\r\n        self.batch_indexes = batch_indexes\r\n        \r\n        # To shuffle the events after end of each epoch\r\n        self.shuffle = shuffle\r\n        \r\n        \r\n        self._data = [self._one_time_process(index) for index, _ in enumerate(self.batch_indexes)]\r\n        self._data_index = list(range(self._data.__len__()))\r\n        \r\n        # Default\r\n        self.on_epoch_end()\r\n    \r\n    def _one_time_process(self, index):\r\n        \r\n        X_data = list()\r\n        indices = self.batch_indexes[index]\r\n        \r\n        for arr in self.X:\r\n            if arr.shape.__len__() == 1:\r\n                X_data.append(np.array(arr[indices]))\r\n            else:\r\n                X_data.append(arr[indices, :])\r\n        return X_data, self.y[indices, :]\r\n    \r\n    \r\n    def _generate_data(self, index):\r\n        return self._data[self._data_index[index]]\r\n        \r\n    def __len__(self):\r\n        return self._data_index.__len__()\r\n\r\n    def __getitem__(self, index):\r\n        return self._generate_data(index)\r\n\r\n    def on_epoch_end(self):\r\n        if self.shuffle:\r\n            np.random.shuffle(self._data_index)\r\n\r\ndef get_parameter_vectors(parameter_vector, nb_dimensions, nb_parameters):\r\n    \"\"\"\r\n    Returns an unpacked list of parameters vectors.\r\n    https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca\r\n    \"\"\"\r\n    return [parameter_vector[:, i * nb_dimensions:(i + 1) * nb_dimensions] for i in range(nb_parameters)]\r\n\r\ndef n_strategy(y_true, y_pred):\r\n    \"\"\"\r\n    :param y_true:\r\n    :param y_pred:\r\n    :return:\r\n    \"\"\"\r\n    _, y_public_estimate, y_payoffs, y_filter, y_place = get_parameter_vectors(y_true, 1, 5)\r\n    alpha_pred, mu_pred, sigma_pred = get_parameter_vectors(y_pred, 3, 3)\r\n    \r\n    mus = tf.expand_dims(tf.reduce_sum(tf.math.multiply(alpha_pred, mu_pred), axis=-1), axis=1)\r\n    sigmas = tf.expand_dims(tf.reduce_sum(tf.math.multiply(alpha_pred, sigma_pred), axis=-1), axis=1)\r\n\r\n    distr = tfp.distributions.Normal(loc=mus, scale=sigmas)\r\n    samples_min = tf.math.argmin(distr.sample((10000,)), axis=1)\r\n    result = tf.math.bincount(tf.cast(tf.reshape(samples_min, (10000,)), dtype=tf.int32), minlength=mus.shape[0], maxlength=mus.shape[0], dtype=tf.float32) / tf.constant(10000, dtype=tf.float32)\r\n\r\n    filter1 = tf.expand_dims(tf.where(tf.squeeze(y_public_estimate) < result, tf.ones_like(result), tf.zeros_like(result)), axis=1)\r\n    after_filters = tf.expand_dims(tf.reduce_prod(tf.concat([filter1, y_filter], axis=1), axis=1), axis=1)\r\n    \r\n    term1 = y_place * after_filters * y_payoffs                   \r\n    term2 = after_filters * tf.constant(-1., dtype=tf.float32)\r\n\r\n    return tf.reduce_sum(term1 + term2, axis=0)\r\n\r\n\r\ndef mixture_loss(nb_dimensions=2):\r\n    \"\"\"\r\n    Computes the mean negative log-likelihood loss of y given the mixture parameters.\r\n    https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca\r\n\r\n    How to use:\r\n    model.compile(loss=mixture_loss(nb_dimensions=2), ...)\r\n    \"\"\"\r\n\r\n    def func(y_true, y_pred):\r\n        y_t, _, _, _, _ = get_parameter_vectors(y_true, 1, 5)\r\n        alpha, mu, sigma = get_parameter_vectors(y_pred, nb_dimensions, 3)  # Unpack parameter vectors\r\n\r\n        distr = tfp.distributions.MixtureSameFamily(\r\n            mixture_distribution=tfp.distributions.Categorical(probs=alpha),\r\n            components_distribution=tfp.distributions.Normal(loc=mu, scale=sigma))\r\n\r\n        log_likelihood = distr.log_prob(tf.transpose(y_t))  # Evaluate log-probability of y\r\n        return -tf.reduce_mean(log_likelihood, axis=-1)\r\n\r\n    return func\r\n\r\ndef get_embeddings():\r\n    \"\"\"\r\n    Generates embedding layers using categorical mappings\r\n    :return: list, list\r\n    \"\"\"\r\n    embedding_inputs = []\r\n    embedding_outputs = []\r\n    for nb_unique_classes, embedding_size in embeddng_size:\r\n        \r\n        # One Embedding Layer for each categorical variable\r\n        model_input = Input(shape=(1,))\r\n        model_output = Embedding(nb_unique_classes, embedding_size)(model_input)\r\n        model_output = Reshape(target_shape=(embedding_size,))(model_output)\r\n\r\n        embedding_inputs.append(model_input)\r\n        embedding_outputs.append(model_output)\r\n\r\n    return embedding_inputs, embedding_outputs\r\n\r\ndef build_model():\r\n    K.clear_session()\r\n    continuous_inputs = [Input(shape=(153,))]\r\n    \r\n    embedding_inputs, embedding_outputs = get_embeddings()\r\n\r\n    # ========================\r\n    # UNIT MODULE\r\n    # ========================   \r\n    model_unit = concatenate(embedding_outputs + continuous_inputs) \r\n    model_unit = Dense(1024, activation='sigmoid', kernel_initializer=\"lecun_normal\")(model_unit)\r\n    model_unit = Dropout(0.5)(model_unit)\r\n    model_unit = Dense(512, activation='sigmoid', kernel_initializer=\"lecun_normal\")(model_unit)\r\n    \r\n    alphas = Dense(3, activation='softmax', kernel_initializer=\"lecun_normal\")(model_unit)   # Create vector for alpha (softmax constrained)\r\n    mus = Dense(3, activation='linear', kernel_initializer=\"lecun_normal\")(model_unit)       # Create vector for mus\r\n    sigmas = Dense(3, activation='nnelu', kernel_initializer=\"lecun_normal\")(model_unit)     # Create vector sigmas (nnelu constrained)\r\n    model_outputs = Concatenate()([alphas, mus, sigmas])           # Concatenate (required for model compilation)\r\n\r\n    optim = Adam(learning_rate=0.0001)\r\n    model = Model(inputs=embedding_inputs + continuous_inputs, outputs=model_outputs)\r\n    model.compile(loss=mixture_loss(3), optimizer=optim, metrics=[n_strategy], run_eagerly=False)\r\n    return model\r\n\r\n\r\nmodel = build_model()\r\ntrain_generator = BatchGenerator(X, y, batch_indexes, shuffle=True)\r\nmodel.fit(train_generator, epochs=300)\r\n\r\n```\r\n\r\n\r\n\r\n", "Try to use\r\n\r\n```python\r\nlength = mus.shape[0] or tf.shape(mus)[0]\r\nresult = tf.math.bincount(tf.cast(tf.reshape(samples_min, (10000,)), dtype=tf.int32), minlength=length, maxlength=length, dtype=tf.float32) / tf.constant(10000, dtype=tf.float32)\r\n```\r\n\r\nhttps://colab.research.google.com/drive/1ttvOoUpoaF75PWPIMpbhDzEce38yzx1D?usp=sharing", "> Try to use\r\n> \r\n> ```python\r\n> length = mus.shape[0] or tf.shape(mus)[0]\r\n> result = tf.math.bincount(tf.cast(tf.reshape(samples_min, (10000,)), dtype=tf.int32), minlength=length, maxlength=length, dtype=tf.float32) / tf.constant(10000, dtype=tf.float32)\r\n> ```\r\n> \r\n> https://colab.research.google.com/drive/1ttvOoUpoaF75PWPIMpbhDzEce38yzx1D?usp=sharing\r\n\r\n@WindQAQ \r\nThis works! Thanks.\r\nSo what's this `shape` and `or` thing, some kind of bug? ", "This is not a bug, but the semantics of shape is different in eager and graph mode. The first dimension of `mus` is variable, so when you access `mus.shape[0]`, it returns `None` in graph mode. Therefore, you should use `tf.shape(mus)[0]` to get the real underlying value in runtime.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46756\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46756\">No</a>\n"]}, {"number": 46755, "title": "An error when loading quantized model ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): tf-nightly 2.5.0.dev20210112\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-774313fb0438> in <module>()\r\n    156     else:\r\n    157         with quantize_scope:\r\n--> 158             loaded_model = tf.keras.models.load_model('q_aware_model_fine_tuned.h5')\r\n    159         inputs = [tf.keras.layers.Input(shape=(height, width, 1), name=\"input_{}\".format(i)) for i in range(4)]\r\n    160         q_aware_model = tf.keras.models.Model(inputs, loaded_model(inputs))\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_weights_from_hdf5_group(f, layers)\r\n    706     symbolic_weights = _legacy_weights(layer)\r\n    707     weight_values = preprocess_weights_for_loading(\r\n--> 708         layer, weight_values, original_keras_version, original_backend)\r\n    709     if len(weight_values) != len(symbolic_weights):\r\n    710       raise ValueError('Layer #' + str(k) + ' (named \"' + layer.name +\r\n\r\nValueError: Layer #9 (named \"quant_activation\" in the current model) was found to correspond to layer quant_activation in the save file. However the new layer quant_activation expects 3 weights, but the saved weights have 1 elements.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/181562QquolAr3Yz5eKflZMH0zLKrzpSF?usp=sharing\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["@suakim \r\nI ran the code shared an face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/430e7c03d0a9b53a5f378e8aea44f3f7/untitled513.ipynb), please share all dependencies to replicate and analyse issue faced.", "@Saduf2019 Please check this file in google drive and let me know if you can't download: https://drive.google.com/file/d/1AViTqRry0K-P1ELZJ5_Y2yU_9ZIiohm6/view?usp=sharing", "I am able to replicate the issue [on tf 2.4 and tf nightly] reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/32f98f3970d3d5fc89852434fb523845/untitled516.ipynb)", "Can you try using `tf` format to save and load model instead of `h5` and check if that works? Thanks!", "Hello, I tried to save the model by using q_aware_model.save('q_aware_model_fine_tuned'), but failed to load this model. Is it the same way with what you suggested?\r\n\r\nThe error is as below: \r\nAssertionError: Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [<tf.Variable 'output_min:0' shape=() dtype=float32, numpy=-6.0>, <tf.Variable 'output_min:0' shape=() dtype=float32, numpy=-6.0>, <tf.Variable 'output_max:0' shape=() dtype=float32, numpy=6.0>, <tf.Variable 'output_max:0' shape=() dtype=float32, numpy=6.0>, <tf.Variable 'output_max:0' shape=() dtype=float32, numpy=6.0>, <tf.Variable 'output_min:0' shape=() dtype=float32, numpy=-6.0>]\r\n\r\nI attached the code [here](https://colab.research.google.com/drive/1q9fuKSz-XZSM2mCm9P6DknJ9wx_NAYOJ?usp=sharing)", "I tried to run the code on colab using TF v2.5 & faced different error ,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/7d5fcd70702581ff1f60ed020722a741/untitled53.ipynb)..Thanks !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46755\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46755\">No</a>\n", "@suakim whats the reason for closing the issue? Did you solve the problem? I am still facing the exact same thing."]}, {"number": 46754, "title": "error: 'tfl.max_pool_2d' op quantization parameters violate the same scale constraint", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly 2.5.0.dev20210112\r\n\r\n### 2. Code\r\n\r\nhttps://colab.research.google.com/drive/18SeujsYiwNr3bwpamGBnQ09EG8ord7I2#scrollTo=BOs2yEVkHRmx\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- Model produces wrong results and/or has lesser accuracy.\r\n- Model produces correct results, but it is slower than expected.\r\n\r\n### 4. (optional) RNN conversion support\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n### 5. (optional) Any other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    216                                                  debug_info_str,\r\n--> 217                                                  enable_mlir_converter)\r\n    218       return model_str\r\n\r\n5 frames\r\nException: /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:206:0: error: 'tfl.max_pool_2d' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.04703366522695504:-1> vs. !quant.uniform<i8:f32, 0.047032601225609871:-1>\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/pooling.py:300:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:167:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py:625:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1032:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:563:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:428:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1032:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/tflite_keras_util.py:184:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:612:0: note: called from\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    218       return model_str\r\n    219     except Exception as e:\r\n--> 220       raise ConverterError(str(e))\r\n    221 \r\n    222   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:206:0: error: 'tfl.max_pool_2d' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 0.04703366522695504:-1> vs. !quant.uniform<i8:f32, 0.047032601225609871:-1>\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/pooling.py:300:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:167:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py:625:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1032:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:563:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:428:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1032:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/tflite_keras_util.py:184:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:612:0: note: called from", "comments": ["Is there any updates? ", "Hello, sorry for late response.\r\n\r\nwill take a look - I think recently added `DownCastScale` method might have caused discrepancy between scales, but need to check. Can you make sure this happens only in nightly, and not in TF 2.4?\r\n\r\nhttps://github.com/tensorflow/tensorflow/search?q=downcastscale", "Hello, thanks for the reponse. I just tried it with tf 2.4.1 (not tf-nightly), but had the same error. Please find the attached [code](https://colab.research.google.com/drive/1iVxnf1hfDt3R5XOJ3-4yiOzRjQ9d3UMb?usp=sharing)", "Thanks for testing :) in that case the problem lies in somewhere else, will take a look.", "The main problem is current QAT toolkit doesn't support concat op yet. \r\n(concat op inputs should have the same scale, so this scale guessing algorithm may have some problem.)\r\n\r\nif you are okay to use add op instead of concat op, then it seems works fine.", "@Xhark, thank you for the answer. Yes it works now after changing 'concat' to 'add'.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46754\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46754\">No</a>\n"]}, {"number": 46752, "title": "Correctly set out-of-boundary values.", "body": "This is a fix for incorrectly set out-of-boundary pixels: #46637 ", "comments": ["@eli-osherovich  Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "I will add some tests shortly\r\n", "@mihaimaruseac \r\nDone.\r\n", "@hyeygit \r\nThis is not how it works in all other packages (at least those that I am aware of; including `keras-preprocessing`, which uses `scipy`). The flow that you describe is: Pad->Transform->Crop.  However, it is more common to not transform pixels that lie outside the original image boundaries but to use `fill_value` for those.\r\n\r\nAs I mentioned earlier, this is the behavior that is common in `scipy`/`skimage` and, if my memory serves me well, in `octave`/`matlab`", "The behavior described on https://github.com/tensorflow/tensorflow/pull/46752#pullrequestreview-589180100 is what happens when you rotate the image in Gimp too (for example).\r\n\r\nI think we have so many tests that now rely on this behavior, so it is not that easy to change. This might be a reason why the internal tests failed.", "@mihaimaruseac \r\nThank you for looking into it.\r\n\r\n@eli-osherovich \r\nChanging the existing behavior is not backwards compatible and may cause large breakage. We could add an option for it as an alternative with current behavior as the default.\r\n\r\nFor example, if we look at [`rotate`](https://scikit-image.org/docs/dev/api/skimage.transform.html#rotate) from `skimage`, there is a `clip` option that users can set.\r\n\r\n_**clip** bool, optional\r\nWhether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range._\r\n\r\nIf we input \r\n```\r\na = [[1, 1, 1],\r\n     [1, 1, 1], \r\n     [1, 1, 1]]\r\n```\r\nand set `clip=False`, we get the same behavior as `ImageProjectiveTransformV3` (#46637):\r\n```\r\n# `order=1` for 'BILINEAR'\r\nskimage.transform.rotate(image=a, angle=45, order=1, clip=False)\r\n\r\narray([[0.58578644, 1.        , 0.58578644],\r\n       [1.        , 1.        , 1.        ],\r\n       [0.58578644, 1.        , 0.58578644]])\r\n```\r\n`skimage` sets `clip` to `True` by default. Doing so clips the values outside the given input range: \r\n```\r\n# `order=0` for 'NEAREST'\r\nskimage.transform.rotate(image=a, angle=45, order=0, clip=True)\r\n\r\narray([[1., 1., 1.],\r\n       [1., 1., 1.],\r\n       [1., 1., 1.]])\r\n```\r\n```\r\n# `order=1` for 'BILINEAR'\r\nskimage.transform.rotate(image=a, angle=45, order=1, clip=True)\r\narray([[1., 1., 1.],\r\n       [1., 1., 1.],\r\n       [1., 1., 1.]])\r\n```\r\n```\r\n# `order=2` for 'BIQUADRATIC`\r\nskimage.transform.rotate(image=a, angle=45, order=2, clip=True)\r\narray([[0., 1., 0.],\r\n       [1., 1., 1.],\r\n       [0., 1., 0.]])\r\n```\r\n\r\nNotice that `NEAREST` and `BILINEAR` results are identical. We see the effect of `clip`ping with higher order interpolations which [`ImageProjectiveTransformV3`](https://www.tensorflow.org/api_docs/python/tf/raw_ops/ImageProjectiveTransformV3) currently doesn't support.\r\n", "@hyeygit \r\nThanks for your prompt response. I think the best counterpart in `skimage` is `skimage.transform.warp` because `rotate` does additional things (padding before transforming). \r\n\r\nAt any rate, I personally, do not see a big problem here. Even though I would really prefer consistent behavior across different packages in the same ecosystem/software stack.\r\n", "I have to correct myself:  `rotate` does not pad image by default. It calls `warp` pretty much straight forward.", "In fact, `skimage` looks buggy to me after I dig into the internals. \r\nIn the code below, `scipy` produces the result that I expect, whereas `skimage` does something wrong:\r\n```python\r\na = np.ones((3, 3))\r\ntransform =  skimage.transform.SimilarityTransform(translation=(-1,-1)) + skimage.transform.SimilarityTransform(rotation=np.deg2rad(45)) + skimage.transform.SimilarityTransform(translation=(1,1))\r\n\r\nskimage.transform.warp(a, transform, order=1, cval=42)\r\narray([[1., 1., 1.],\r\n       [1., 1., 1.],\r\n       [1., 1., 1.]])\r\n\r\nscipy.ndimage.affine_transform(a, transform.params, order=1, cval=42)\r\narray([[42.,  1., 42.],\r\n       [ 1.,  1.,  1.],\r\n       [42.,  1., 42.]])\r\n```", "@hyeygit  Can you please take a look on above comments from @eli-osherovich. Thanks!", "What is the verdict here? Will it stay as it is today?\r\n", "Sorry for the late reply. \r\n\r\n@eli-osherovich \r\nThank you for providing additional explanation and examples.\r\nAlso, I agree that having consistent behavior across different packages is easier for users.\r\n\r\nRegarding merging this PR, it should not be a problem as long as there aren't things that rely on current behavior and all checks pass. I'll bring in the PR and run internal tests.", "Unfortunately, there are many internal targets that rely on the current behavior and this V3 op would need to stay as it is. (Introducing a V4 op with a flag for switching between the two behavior is an option but would need API owner's approval.)", "@hyeygit  Any update on this PR? Please. Thanks!", "@eli-osherovich We unfortunately cannot merge in the PR due to failing internal targets that rely on the default behavior which is WAI. May we close the PR? ", "Sure.\n\nOn Tue, Mar 23, 2021, 10:59 PM hyeygit ***@***.***> wrote:\n\n> @eli-osherovich <https://github.com/eli-osherovich> We unfortunately\n> cannot merge in the PR due to failing internal targets that rely on the\n> default behavior which is WAI. May we close the PR?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/46752#issuecomment-805255727>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AASS73VROIJZRFLDNS4UZU3TFD6LHANCNFSM4WWQPL3Q>\n> .\n>\n"]}, {"number": 46751, "title": "update", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46751) for more info**.\n\n<!-- need_sender_cla -->", "@grapefruitL  Can you please sign CLA. Thanks!"]}, {"number": 46749, "title": "[TFL] Fix BatchMatMul constant RHS.", "body": "Fixes #46724. For constant RHS and !adj_y, transpose of RHS only occurs once, so the allocation should be persistent across runs. Old commit does this, but the allocation is accidentally overridden to kTfLiteArenaRw for constant RHS.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/7c0a3f12dd00b0aeb9d2a91d157b0e9d1a4c1250/tensorflow/lite/kernels/batch_matmul.cc#L193-L199\r\n\r\n/cc @abattery for visibility.", "comments": ["Verified that this fixes the issue."]}, {"number": 46747, "title": "Improve rendering for tf.distribute.cluster_resolver.TPUClusterResolver in tpu_strategy API docs", "body": "This should fix the auto-linking and rendering\r\n\r\n<img width=\"500\" alt=\"image\" src=\"https://user-images.githubusercontent.com/19637339/106074345-9a82ed80-6103-11eb-9074-0c5433eba08d.png\">\r\n", "comments": ["@8bitmp3  Can you please address Ubuntu Sanity errors? Thanks!", "> @8bitmp3 Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\nthanks @gbaned `exited with error code 1 ` - do you have an idea what it means? @rxsang ", "> > Ubuntu Sanity errors? Thanks!\r\n> \r\n> thanks\r\n\r\nI don't think that's relevant, maybe a transient issue?", "> > > Ubuntu Sanity errors? Thanks!\r\n> > \r\n> > \r\n> > thanks\r\n> \r\n> I don't think that's relevant, maybe a transient issue?\r\n\r\n@gbaned @rxsang I think I found it in the logs: `2. do_pylint: Python 3 pylint  FAIL`", "@gbaned @rxsang OK, PTAL, thank you"]}, {"number": 46746, "title": "Add in optimizations for softmax for Fusion F1.", "body": "Confirmed that the test passes with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_softmax_test -j8\r\n```\r\n\r\nHowever, the latency improvement is only ~1000 ticks, as tested with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_keyword_benchmark -j8\r\n```\r\n\r\nSince Softmax is currently a small fraction of the overall keyword_benchmark latency we will focus on the latency of only this particular OP.\r\n\r\nWith the optimized implementation:\r\n```\r\nSOFTMAX took 749 ticks (0 ms).\r\n```\r\n\r\nReference implementation:\r\n```\r\nSOFTMAX took 2052 ticks (2 ms).\r\n```\r\n\r\nAnd with the LUT hifimini implementation (for completeness):\r\n```\r\nSOFTMAX took 1142 ticks (1 ms).\r\n```\r\n\r\nThe gain of ~1500 ticks ticks is worth merging because after all the optimizations (e.g.  #47098), this will mean a ~5% improvement for the keyword benchmark.\r\n\r\nAnd the benefits might be more significant for other models too.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@pnikam-cad @nyadla-sys: The kernel tests pass ok with the optimized implementation. However, I am not seeing any meaningful latency improvement.\r\n\r\nThis could happen for any combination of the following reasons:\r\n * the optimizations could be improved for the case of int8 inputs and int16 output.\r\n * for the Fusion F1, the reference implementation of softmax doesn't really have much room to be optimized.\r\n * for the fusion F1 (in contrast to the hifimini), the contribution of softmax to the overall latency is minimal.\r\n\r\nI'll look more closely at these tomorrow but sharing the current status with you guys.\r\n", "I looked a little bit into the softmax performance. Couple of observations, inline with the points mentioned above :\r\n- The reference implementation is already well optimized because of the lookup table approach, and the contribution of the softmax operator is much less now. So we may not see big difference when switching to optimized implementation.\r\n- For the softmax kernel in the HiFi 4 NN library, this case is going into a slightly unoptimized path compared to HiFi mini. We should be able to fix this in the next version, that should give us some improvement.\r\n ", "I have updated the commit description with the latest per-op profiling. Especially with all the other optimizations, the ~1500 ticks improvement that we get from softmax is worth merging."]}, {"number": 46744, "title": "Distributed training for transformer model.", "body": "Hello,\r\n\r\nI am attempting to adapt TensorFlow's transformer tutorial to work on multiple GPUs using there distributed training tutorial,\r\n\r\nTransformer: https://www.tensorflow.org/tutorials/text/transformer\r\nDistributed training: https://www.tensorflow.org/tutorials/distribute/custom_training\r\n\r\nHowever, when I execute the program I get the error: \"TypeError: train_step() argument after ** must be a mapping, not Tensor\" in `per_replica_losses = strategy.run(train_step, args=(inp, tar),)`.\r\n\r\n\r\nThe code that is causing the issue is as follows:\r\n`train_step_signature = [\r\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\r\n]\r\n\r\n@tf.function(input_signature=train_step_signature)\r\ndef train_step(inp, tar):\r\n  #inp, tar = dataset_inputs\r\n  tar_inp = tar[:, :-1]\r\n  tar_real = tar[:, 1:]\r\n\r\n  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\r\n\r\n  with tf.GradientTape() as tape:\r\n    predictions, _ = transformer(inp, tar_inp,\r\n                                 True,\r\n                                 enc_padding_mask,\r\n                                 combined_mask,\r\n                                 dec_padding_mask)\r\n    loss = loss_function(tar_real, predictions)\r\n\r\n  gradients = tape.gradient(loss, transformer.trainable_variables)\r\n  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n\r\n  train_loss(loss)\r\n  train_accuracy(accuracy_function(tar_real, predictions))`\r\n\r\n@tf.function(input_signature=train_step_signature)\r\ndef distributed_train_step(inp, tar):\r\n  per_replica_losses = strategy.run(train_step, args=(inp, tar),)\r\n\r\n  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                         axis=None)\r\n\r\nfor epoch in range(EPOCHS):\r\n  start = time.time()\r\n  BatchTime = time.time()\r\n\r\n  train_loss.reset_states()\r\n  train_accuracy.reset_states()\r\n\r\n  for (batch, (dataset_inputs)) in enumerate(train_dataset):\r\n\r\n      distributed_train_step(dataset_inputs)\r\n\r\n      if batch % 50 == 0:\r\n\r\n           print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, batch, train_loss.result(), train_accuracy.result()))\r\n\r\n      if batch % 100 == 0:\r\n          ckpt_save_path = ckpt_manager.save()\r\n          print ('Saving checkpoint for batch {} at {}'.format(batch,\r\n                                                                 ckpt_save_path))\r\n          print(\"Time taken: {}\".format(time.time() - BatchTime))\r\n          BatchTime = time.time()\r\n\r\n  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\r\n                                               train_loss.result(),\r\n                                              train_accuracy.result()))\r\nprint ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))`\r\n\r\n\r\nThanks\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1c8330f8697cc271fe92d1865ce9a574/46744.ipynb#scrollTo=bJlf26srz1ur). Thanks!", "Hi @Zotec-exe, are you distributing the dataset? Please see[ this section of the docs](https://www.tensorflow.org/tutorials/distribute/custom_training#setup_input_pipeline) that explains the use of ` strategy.experimental_distribute_dataset` to wrap your dataset for `MirroredStrategy` with custom training loops.", "Yes i am, i should of mentioned.\r\n\r\nThis is my version to prepare the dataset:\r\n\r\n```train_dataset = train_examples.map(tf_encode)\r\ntrain_dataset = train_dataset.filter(filter_max_length)\r\n\r\n\r\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\r\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\ntrain_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\n\r\nval_dataset = val_examples.map(tf_encode)\r\nval_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)\r\nval_dataset = strategy.experimental_distribute_dataset(val_dataset)", "Ah okay. Please provide a fully reproducible colab with your training code so that nothing gets lost.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Here's a link:\r\n\r\nhttps://colab.research.google.com/drive/1CWmHaonRfKno2cjUMyl__SGUG1mA1c-A?usp=sharing", "just a thought, but I wonder if the problem is that `train_step` function does not return the loss. See example of `train_step` in the tutorial [here](https://www.tensorflow.org/tutorials/distribute/custom_training#training_loop).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46744\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46744\">No</a>\n"]}, {"number": 46743, "title": "Instructions to add git hook for code-style checks.", "body": "Installing such a hook should enable fixing formatting and other style issues faster.\r\n\r\nAlso, updated the pigweed patch to have the format_code error message give a command that can be copy-pasted into the terminal to fix the formatting errors.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46742, "title": "Fix crash when tf.sequence_mask takes a non-integer lengths", "body": "This PR tries to address the issue raised in #46698 where\r\ntf.sequence_mask will crash abruptly if lengths is not passed\r\nwith an integer tensor.\r\n\r\nThis PR applies a dtype check and throw out ValueError to avoid\r\nprogram crash.\r\n\r\nThis PR fixes #46698.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Hmm, this was rolledback."]}, {"number": 46741, "title": "Fix crash when invalid keepdims value is being passed to tf.math.reduce_prod", "body": "This PR tries to address the issue raised in #46700 where\r\ntf.math.reduce_prod will crash if keepdims is being passed\r\nwith a non-boolean value (e.g. numpy value)\r\n\r\nThe issue was that keepdims is passed through pywrap\r\nwhich can not interprete numpy values, thus crashes.\r\n\r\nA way to detect the type mismatch before being passed\r\nto pywrap is to use `bool(keepdims)` to give python a chance\r\nto convert to bool (and throw out error when appropriate).\r\n\r\nThis PR also fixes all reduce_ ops.\r\n\r\nThis PR fixes #46700.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 46739, "title": "Difference between regular and fast nms in TFLite ", "body": "What is the difference between regular and fast Non-Max-Suppression (NMS) in TFLite ? is there any documentation on that?\r\nI mean it terms of accuracy/performance. I understand there is a difference in the time complexity, but why do they yield different results? \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc\r\n\r\n", "comments": ["(I am not a detection expert, so feel free to verify what I say based on code link mentioned in the answer)\r\n\r\n'fast' NMS is equivalent to `class_agnostic_non_max_suppression` from the object detection codebase (see [here](https://github.com/tensorflow/models/blob/master/research/object_detection/core/post_processing.py#L626)).\r\n\r\nThe high-level overview is that 'regular' NMS performs [Non-Max-Suppression](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for *each class separately*, keeping the best scored boxes & pruning away overlapping candidates per-class. These per-class results are then combined into the final output.\r\n\r\n\r\nFast NMS, on the other hand, doesn't consider classes at all, and just performs NMS *once* over the whole set of bounding boxes - for each selected candidate at the end, its 'most likely' class is chosen. The time complexity win comes from the fact that NMS is done only once instead of number-of-classes times.\r\n\r\nThe latter method is less accurate, since it is likely to prune away outputs where \r\n\r\n1) same bounding boxes can be scored highly for very similar classes (TV vs Monitor): In fast NMS, you will likely only see 1 class in the output\r\n2) boxes that have very high overlap but two different classes: fast NMS might just look at the higher-probability class", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46739\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46739\">No</a>\n"]}]