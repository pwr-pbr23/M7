[{"number": 34010, "title": "TFLite 5D Tensors with the OpenCL Back-End", "body": "I am planning to gradually add support for 5D tensors in TFLite in the OpenCL GPU back-end, which was not available in r2.0.\r\nMy main concern with the OpenGL API was the limit on the number of dimensions that work groups have, but that's no longer the issue with OpenCL as it seems to allow N-dimensional work groups.\r\n\r\nWhat is the status of the OpenCL back-end? What about 5D tensors? (this is a massive re-write so it would be cool to know there's no duplicate work)\r\nWhy was the initial GPU back-end implemented with OpenGL and not OpenCL?\r\n\r\nMy ultimate goal is to support Conv3D. #19658\r\n\r\nEDIT:\r\nWhile its true that OpenCL's API allows N-dimensional work groups, I came across the CL_DEVICE_MAX_WORK_ITEM_DIMENSIONS. This setting is bound to 3 on all of the GPUs I queried, any tips to how go around it? or should I do multiple enqueue's?", "comments": ["This would be greatly appreciated and is in great need of the hour. Thank You.", "Do you have an example model to demonstrate 5D usage? Also, does your desired use-case currently work with the TFLite CPU path? While we do support >4D cases for certain ops in our CPU backend, that's not the case (yet) for all ops.", "I do. I can work around it by altering it post-training but that adds maintenance.\r\nI specifically prioritize OpenCL support, however, I expect that a CPU implementation will follow once I am done.", "Any chance you can link to the model? or a trimmed down version of the model?\r\n\r\nI'd like to see what other operators would be needed for this kind of model, as I suspect it might be more than just ZeroPadding3D.", "I am afraid I not at liberty to share the model but you are correct- ZeroPadding3D is not the only missing operation as it requires MaxPooling3D and Conv3D as well.\r\nI made several pull requests over the week to advance this agenda and the next item would be to extend schema.fbs with these new operations. The various pooling and convolutions handlers are too coupled together so I will keep my changes separate to avoid breaking stuff.\r\n"]}, {"number": 33899, "title": "Porting gradients to C++ from python", "body": "**System information**\r\n- TensorFlow version (you are using): v2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm writing a Java gradient optimizer to allow training models on the JVM. I've got pretty much everything working, but some of the gradients can't be calculated as they aren't available in C++, and so I get errors of the form `Exception in thread \"main\" org.tensorflow.TensorFlowException: No gradient defined for op: SparseSoftmaxCrossEntropyWithLogits.`. What's the roadmap for porting gradients from Python to C++? If there isn't a roadmap, would a pull request to add this gradient be accepted (and is there any relevant guidance for working in the TF C++ codebase)? The relevant issues according to GitHub are all from 2016/7 and the internals might be quite different after the v2 release.\r\n\r\n**Will this change the current api? How?**\r\nIt will expand the set of models that can be trained using languages other than python. It should only add API endpoints and won't change any of the existing ones.\r\n\r\n**Who will benefit with this feature?**\r\nUsers of the the TF Java API, users of the C API, and other languages which bind the C API. Without the ability to train models using a specific language binding it's very hard to say Tensorflow actually supports that language.\r\n", "comments": ["There isn't a roadmap, and we'll happily take pull requests.", "Ok. Is there any programmatic way to figure out what ops have a C++ gradient and what ones don't? Or some grep based magic I could run on the source tree? I'd prefer to fix a bunch of them at once rather than run into them piecemeal as we expand our usage of the TF Java API.", "I believe you want the REGISTER_OP_GRADIENT macro\n\nOn Wed, Nov 6, 2019 at 11:54 AM Adam Pocock <notifications@github.com>\nwrote:\n\n> Ok. Is there any programmatic way to figure out what ops have a C++\n> gradient and what ones don't? Or some grep based magic I could run on the\n> source tree? I'd prefer to fix a bunch of them at once rather than run into\n> them piecemeal as we expand our usage of the TF Java API.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33899?email_source=notifications&email_token=AAABHROOBDPJLJK5XSTV7A3QSMOIFA5CNFSM4JHUO5TKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDHYQIY#issuecomment-550471715>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMAWJP32IZRSSW3OS3QSMOIFANCNFSM4JHUO5TA>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 33880, "title": " Support sparse inputs for Embedding layer", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0\r\n- Are you willing to contribute it (Yes/No): Yes. And the code change is ready.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe tf.keras.layers.Embedding only can be used with dense inputs. The user must customize a layer for sparse tensor inputs by using tf.nn.embedding_lookup_sparse.  Or, the user needs to use tf.feature_columns.embedding_columns to transform sparse inputs to tf.layer.DenseFeature.\r\n\r\n**Will this change the current api? How?**\r\nThe Embedding layer should add the `combiner` argument for sparse inputs. The value of `combiner` can be \"sum\", \"mean\" and so on.\r\n\r\n**Who will benefit with this feature?**\r\nEmbedding with sparse inputs is commonly used in advertising and recommendation system in which the input ids of a sample are very sparse. \r\n\r\n**Any Other info.**\r\n", "comments": ["Any workaround on this ? This is my approach so far, but I feel it's messy and leads to a much bigger model than should be.\r\n```\r\ninput_layer = Input(input_shape)\r\nx = Dense(input_shape, kernel_initializer='ones', bias_initializer='zeros', name='unpack')(input_layer)\r\nx = Embedding(input_shape, 128)(x)\r\n...\r\nmodel.get_layer(name='unpack').trainable = False\r\n```", "Adding @tanzhenyu who is working on sparse inputs", "Any updates?", "@tanzhenyu  any updates on this, or potentially a workaround for it for those of us who have sparse inputs and would like to use an embedding?", "Follow up on my previous message. I've been able to overcome this with the following code from `tensorflow==2.5.0`\r\n\r\nIf you're seeing this message in the future and its not resolved, here was the implementation I came up with to overcome this problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import backend\r\nfrom tensorflow.python.ops import embedding_ops\r\nfrom tensorflow.python.ops import math_ops\r\n\r\n\r\nclass SparseEmbedding(tf.keras.layers.Embedding):\r\n    def __init__(self, *args, combiner=None, mask_zero=None, **kwargs):\r\n        assert mask_zero is None, \"Cannot use sparse embedding with mask zero!\"\r\n        self._combiner = combiner\r\n        super().__init__(*args, **kwargs)\r\n\r\n    def compute_mask(self, inputs, mask=None):\r\n        return None\r\n\r\n    def call(self, inputs):\r\n        dtype = backend.dtype(inputs)\r\n        if dtype != 'int32' and dtype != 'int64':\r\n            raise TypeError(f\"\"\"\r\n            Failed to process inputs! Expected dtype to be integers!\r\n            \r\n            expected: int32 or int64\r\n            received: {dtype}\r\n            name: {inputs.name}\r\n            \"\"\")\r\n        out = embedding_ops.safe_embedding_lookup_sparse_v2(\r\n            embedding_weights=self.embeddings,\r\n            sparse_ids=inputs,\r\n            combiner=self._combiner\r\n        )\r\n        if self._dtype_policy.compute_dtype != self._dtype_policy.variable_dtype:\r\n            # Instead of casting the variable as in most layers, cast the output, as\r\n            # this is mathematically equivalent but is faster.\r\n            out = math_ops.cast(out, self._dtype_policy.compute_dtype)\r\n        return out\r\n\r\n    def get_config(self):\r\n        base_config = super().get_config()\r\n        config = {\r\n            \"combiner\": self._combiner\r\n        }\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n```\r\n\r\nThis code works. However there are two issues here. First we should probably only have an `Embedding` class that handles sparse/not-sparse. But I am unsure of how we want to handle the case of the mask. So without that implemented I wouldn't create a PR because this should be split this way."]}, {"number": 33863, "title": "Build breaks: The value 'REPOSITORY_NAME' has been removed in favor of 'repository_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#repository_name).", "body": "TF 2.0.0 fails to be built by bazel-1.1.0:\r\n\r\n```\r\nINFO: Found applicable config definition build:opt in file /usr/ports/science/py-tensorflow/work-py36/tensorflow-2.0.0/.tf_configure.bazelrc: --copt=-march=native --copt=-I/usr/local/include --host_copt=-march=native --define with_default_optimizations=true\r\nERROR: /usr/ports/science/py-tensorflow/work-py36/bazel_out/05d0f16da4b074c80492981824445ee0/external/com_google_protobuf/protobuf.bzl:274:40: The value 'REPOSITORY_NAME' has been removed in favor of 'repository_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#repository_name).\r\nERROR: /usr/ports/science/py-tensorflow/work-py36/bazel_out/05d0f16da4b074c80492981824445ee0/external/com_google_protobuf/protobuf.bzl:275:33: The value 'PACKAGE_NAME' has been removed in favor of 'package_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#package_name). \r\nERROR: /usr/ports/science/py-tensorflow/work-py36/bazel_out/05d0f16da4b074c80492981824445ee0/external/com_google_protobuf/protobuf.bzl:275:11: The value 'PACKAGE_NAME' has been removed in favor of 'package_name()', please use the latter (https://docs.bazel.build/versions/master/skylark/lib/native.html#package_name). \r\nINFO: Call stack for the definition of repository 'local_config_syslibs' which is a syslibs_configure (rule definition at /usr/ports/science/py-tensorflow/work-py36/tensorflow-2.0.0/third_party/systemlibs/syslibs_configure.bzl:154:21):\r\n - /usr/ports/science/py-tensorflow/work-py36/tensorflow-2.0.0/tensorflow/workspace.bzl:73:5\r\n - /usr/ports/science/py-tensorflow/work-py36/tensorflow-2.0.0/WORKSPACE:19:1\r\nERROR: Skipping '//tensorflow:libtensorflow_cc.so': error loading package 'tensorflow': in /usr/ports/science/py-tensorflow/work-py36/tensorflow-2.0.0/tensorflow/core/platform/default/build_config.bzl: Extension 'protobuf.bzl' has errors\r\nERROR: error loading package 'tensorflow': in /usr/ports/science/py-tensorflow/work-py36/tensorflow-2.0.0/tensorflow/core/platform/default/build_config.bzl: Extension 'protobuf.bzl' has errors\r\n```\r\n\r\nBoth bazel and TF are of the latest versions. What's going on?", "comments": ["As per [the table](https://www.tensorflow.org/install/source_windows#tested_build_configurations) at the end of this documentation page, Only `Bazel 0.26.1` has been tested with `TensorFlow-2.0.0`\r\nThe instructions recommend installing Bazel 0.24.1. (With a misleading instruction for which I have made a PR.)\r\nThe docs will be updated soon. :)", "@oanush I can help resolve this issue. Can you assign me here?", "@yurivict are you compiling from `master` or from `r2.0`?\r\n\r\n@nikochiko I'll assign you, thanks for offering to help", "@mihaimaruseac \ud83d\udc4d Thank you!", "I'm compiling from 1.14.0", "Oh, the `r1.14` branch (and all other `r.*` branches) has not been ported to work with the newest Bazel. Usually we cut each release branch with a range of Bazel versions that we know we can compile with and use only one of these to do the actual release. The range is visible in `configure.py` (running `./configure` with Bazel outside the range will tell you to upgrade/downgrade) and the version we've used is listed in the table at the documentation page about installing from source (linked above)", "I actually would rather much prefer cmake to bazel, because all these issues simply don't exist with any cmake-based projects.\r\n\r\nMaybe you could rather put your efforts to create the cmake build files instead? I think everybody outside of google struggles with bazel, and would rather see cmake do the build, because it's so much simpler.", "@yurivict There was actually a very similar discussion here: https://github.com/tensorflow/tensorflow/issues/6923.\r\nAlso see, another issue which referenced this mentioned one: https://github.com/tensorflow/tensorflow/issues/13061", "Also, SIG Build can help with build issues  https://groups.google.com/a/tensorflow.org/forum/#!forum/build", "The discussion was in 2017, and yet there's still no cmake build and it is extremely hard to build TensorFlow."]}, {"number": 33802, "title": "Pull Requests: Map needed for request workflow", "body": "At the moment external contributors don't have any way of knowing what the lifecycle of a pull request is. We have [documentation on contributing code](https://www.tensorflow.org/community/contribute/code), but it's silent on the details of what happens to a request once it's been submitted. Chandni presented a fantastic flowchart at the contributor's summit today that would be a great foundation for documentation explaining the stages that a PR goes through.\r\n\r\nThis would help external contributors understand what they need to do to successfully submit code to the project, and combined with https://github.com/tensorflow/tensorflow/issues/33801 will give them the visibility they need to be effective TensorFlow developers.\r\n\r\n/cc @freddan80 @jenselofsson ", "comments": ["Thanks for raising this issue! It would be great to see such a flowchart."]}, {"number": 33801, "title": "Pull Requests: Status information should be available", "body": "There is currently no way to tell who needs to take the next action on a pull request, or what state a PR is in. This delays external contributions and frustrates developers. Here is the sort of information that is needed to enable efficient contributions:\r\n\r\n - Who needs to take action? Is it the contributor, a member of the gtech team, or a Google TensorFlow engineer? This should be clearly and publicly visible on the request, so that stakeholders can communicate with the responsible individual.\r\n\r\n - Where is the request in the approval workflow? We'll need a map of the stages involved, and a way to map the current state to each node in the graph.\r\n\r\nThere are other pieces of information that would be nice to have, but these are essential to shepherding contributions through our process.\r\n\r\n/cc @jenselofsson @freddan80", "comments": []}, {"number": 33759, "title": "What is the right way to use coverage.py with Tensorflow?", "body": "I apologize if this is the wrong way to ask this question. I'm the maintainer of coverage.py, for measuring code coverage in Python projects.  A user wrote an issue for me: https://github.com/nedbat/coveragepy/issues/856\r\n\r\nAfter digging into it, I see that his tf.keras.Model.call() function is not executed directly, but is transformed into a temporary file, and executed there.  So coverage.py reports that his code is unexecuted, even though he can see the effects of its execution.\r\n\r\nI also see that the transformed code has an `ag_source_map__` parameter which can be used to map back from the transformed code to the original code.  A coverage.py plugin could use that information to report coverage usefully.\r\n\r\nMy questions are:\r\n1. Is there a reason people haven't reported this to coverage.py before? Is there a existing known way to get coverage reports on this kind of code?\r\n2. What is a stable public API for getting the transformation mapping?\r\n3. Would TensorFlow be interested in maintaining a coverage.py plugin to make this work properly?", "comments": ["Has there been any progress on this on the TF side? This is an important feature for TF in production.", "I second that request. This issue needs discussion and maybe special addressing on the ` coverage.py` side. In particular for TF2, all `tf.function` decorated functions are not observed by `coverage.py` (as a work-around, in some cases they can be dynamically un-decorated for unittesting)", "Does anyone want to take on writing a coverage.py plugin? I can help with the coverage.py side of things.", "The mentioned issue ( https://github.com/Sujit-O/pykg2vec/issues/123 ) hinted me at the function [`tf.config.experimental_run_functions_eagerly(run_eagerly)`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/config/experimental_run_functions_eagerly), which looks like the ideal solution. However it does not seem to work completely as advertised, cf. https://github.com/tensorflow/tensorflow/issues/38170 , as certain functions cease working if switched between eager/non-eager modes.", "With tensorflow 2.5, even if I run tf.config.experimental_run_functions_eagerly(True) directly before my tests, the coverage is not correctly reported. I want to write tests for a custom loss function around CRFs for a production model, where having adequate test coverage is key. Currently, I am stuck on having to exclude this file from coverage.py.\r\n", "I'm the coverage.py maintainer.  I'd be glad to work closely with someone from the Tensorflow side to find a solution to this problem.", "Hi @nedbat, Found some articles of unit testing and code coverage of Tensorflow codes , It indicates use of **tf.test api** .Attaching below for reference .\r\nhttps://theaisummer.com/unit-test-deep-learning/\r\nhttps://www.tensorflow.org/api_docs/python/tf/test/Benchmark\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/community/contribute/tests.md . Are they helpful to proceed?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Bump; this is still an issue.", "@gsakkis:\r\n> I'm the coverage.py maintainer. I'd be glad to work closely with someone from the Tensorflow side to find a solution to this problem.", "Ok! Reopening as requested. "]}, {"number": 33755, "title": "Dense variable constraints are not allowed with sparse gradients", "body": "I'm training a simple document embedding model with the Keras API in TF 2.0. When I try to add `embeddings_constraint=tf.keras.constraints.UnitNorm(axis=1)` to an `Embedding` layer, I get the following error (truncated):\r\n\r\n```\r\nFile \"C:\\Users\\Steven\\Miniconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 459, in apply_grad_to_update_var\r\n    \"Cannot use a constraint function on a sparse variable.\")\r\n```\r\n\r\nI have verified that the variable is not sparse; the `Embedding` layer creates a dense matrix to hold the embeddings. However, reading [the code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py#L126) shows that this error is raised when the *gradients* are sparse, which seems normal for embedding layers and should not (AFAIK) affect the application of constraints to the dense variable after the gradient update.\r\n\r\n**In summary, variable constraints should be allowed in this context, and it is a bug that they are not.**", "comments": ["@skearnes Thanks for reporting this issue. Can you please share a simple standalone code to reproduce the issue? Thanks!", "I have the same problem.  Code based on https://keras.io/layers/embeddings/\r\n\r\n    import numpy as np\r\n    from tensorflow.keras.layers import Embedding                         \r\n    from tensorflow.keras.models import Sequential\r\n    from tensorflow.keras.constraints import UnitNorm\r\n\r\n    model = Sequential()\r\n    #model.add(Embedding(1000, 64, input_length=10))    # This version works\r\n    model.add(Embedding(1000, 64, input_length=10, embeddings_constraint=UnitNorm))    # This version fails\r\n    input_array = np.random.randint(1000, size=(32, 10))\r\n    model.compile('rmsprop', 'mse')\r\n    output_array = model.predict(input_array)\r\n    assert output_array.shape == (32, 10, 64)\r\n\r\n    target = np.zeros_like(output_array)\r\n    model.fit(input_array, target, epochs=5, verbose=1) ", "+1 thanks @RoMa03 ", "The error message should come from [optimizer_v2.py line 459](https://github.com/tensorflow/tensorflow/blob/bf9c196f37b9cbb3109b2891aaf9da85bf5f712a/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L459) ? +1 on having the same issue as op btw.", "Same here.", "I also meet the bug, is there anyone who solved this? Thx~", "Same here...", "Same here.", "Same here", "Same here", "IIUC this is a problem in both tf v1 optimizer and tf v2 keras optimizer?", "same here", "same problem here", "Encountered the same bug when using tf.gather on a custom layer weights. ", "same here", "same here, however with some other implementation i was able to do it\r\ncheck this please\r\n`\r\nclass BigClamModel(keras.Model):\r\n  def __init__(self , nodes , communities):\r\n      super(BigClamModel , self).__init__()\r\n      initializer = HalfNormal(0.3 , 0.1)\r\n      self.embedding = keras.layers.Embedding(input_dim=nodes , output_dim = communities  , embeddings_initializer= initializer , name = \"embedding\" , embeddings_constraint=tf.keras.constraints.UnitNorm(axis=1))\r\n      self.DotLayer = keras.layers.Dot(axes = 1 , name = \"dot_product\")\r\n      self.out = keras.layers.Lambda(lambda x : (1.0 - keras.activations.exponential(-1.0 * x)) , name = \"output\")\r\n\r\n  def call(self , inputs):\r\n    inp1 , inp2 = inputs[0] , inputs[1]\r\n    U = self.embedding(inp1)\r\n    V = self.embedding(inp2)\r\n    ret = self.out(self.DotLayer([U , V]))\r\n    return ret \r\n  \r\n  def model(self):\r\n      inp1 = keras.layers.Input(shape = () ,name = \"first_endpoints\" , dtype = tf.int32)\r\n      inp2 = keras.layers.Input(shape = () ,name = \"second_endpoints\" , dtype = tf.int32)\r\n      return keras.Model(inputs=[inp1,inp2], outputs=self.call([inp1,inp2]))\r\n`", "I have the same problem. Any suggested solutions?", "Hey all -- I am still working on this, should be able to submit a change without requiring any user code change.", "[this](https://stackoverflow.com/questions/63122880/tensorflow-2-runtimeerror-cannot-use-a-constraint-function-on-a-sparse-variabl) is a simple workaround for the issue.\r\n```python\r\nfrom tensorflow.keras.layers import Embedding\r\nfrom tensorflow.keras.constraints import UnitNorm\r\n. . .\r\nemb = Embedding(input_dim, output_dim, name='embedding_name')\r\nnorm_layer = UnitNorm(axis=1)\r\nnorm_embedding = norm_layer(emb(embedding_id_input))\r\n. . .\r\n```\r\n", "This is a serious problem and makes tf.keras unsuitable for prototyping some modern language models. We're used the same workaround as rzilleruelo, but it damages the optimization trajectory and stability.\r\n\r\nConsider an embedding vector with large norm: We want to make larger gradient updates to it, to balance the representation learning, but when normalizing as a layer instead of a constraint we actually scale it down to target norm, and so we end up making *smaller* updates to it instead of larger ones. The problem is even worse with very short embedding vectors, where the value (and therefore the gradients) get scaled up and then the optimizer clobbers the tiny vector with a large update. Unlike some companies, we don't have extra millions to burn on compute as a workaround.\r\n\r\nSo our workaround for the problems of the workaround, is using a penalty on the logarithm of the computed scaling factor like so:\r\n```python\r\ndef PostUnitNorm(v, regularizer=0):\r\n\tvlen = tf.norm(v, ord=2, axis=-1, keepdims=True)\r\n\tvlen = tf.math.maximum(vlen, 1e-8)\r\n\tif regularizer > 0:\r\n\t\tvlen += l.Activation(tf.math.log, activity_regularizer=tf.keras.regularizers.l1(regularizer))(vlen) * 1e-16\r\n\treturn v / vlen\r\n```\r\nBut this is still broken, because the penalty must be obscenely strong to achieve the desired effect, which makes the term able to completely dominate the optimization and halt progress until the embedding vectors come back into the desired scale. ", "I have the same problem with tensorflow 2.4.1. Has anyone found another solution that does not involve switching to a functional model?", "Ricardo Zilleruelo's solution of following the embedding layer with a\nseparate normalisation layer looks pragmatic to me?\n\nfrom tensorflow.keras.layers import Embeddingfrom\ntensorflow.keras.constraints import UnitNorm\n. . .emb = Embedding(input_dim, output_dim,\nname='embedding_name')norm_layer = UnitNorm(axis=1)norm_embedding =\nnorm_layer(emb(embedding_id_input))\n. . .\n\nHe chose to wrap the two as a function but you could easily have them as\nseparate layers if you prefer.\n\nRobert\n", "I am getting the same error for non_neg() constraint and the workaround suggested by @rzilleruelo works for me. Now, I am interested in extracting the weights after applying the non_neg() constraint, how can I do that ?\r\n\r\nFor the Embedded layer I could do this :\r\nmodel.get_layer(name='embedding_name').get_weights()\r\n", "I tried to run the code on colab with TF v2.5  after following  [this article](https://github.com/keras-team/keras/issues/14459) and didn't get error after removing this constraint `embeddings_constraint=UnitNorm `in the code .Please find the [gist here](https://colab.research.google.com/gist/mohantym/eb370120505877a14669c6d4eff3cb28/33755.ipynb). Thanks !", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "As of TF 2.4, I too am getting this error; I have had to remove embedding constraints.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33755\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33755\">No</a>\n", "Since the defect still exists, I believe it should remain open. ", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33755\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33755\">No</a>\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33755\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33755\">No</a>\n", "Nothing has changed. The warnings persist. ", "Why is this still an issue. It's been two years!", "I just ran into this - vote for a fix - the workaround\r\n\r\n```\r\nnorm_embedding = norm_layer(emb(embedding_id_input))\r\n```\r\n\r\nprobably solves must use cases, but it is essentially a constraint on the activity - the constraint is applied after the embeddings are looked up. The constraint one passes to the keras Embedding, per the documentation, is on the full matrix (like the embeddings_regularizer). \r\n", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "I added an alternative workaround to https://stackoverflow.com/a/70427151/2280020, @umbra-scientia "]}, {"number": 33749, "title": "Pretrained Inception V4 to Keras Application Folder", "body": "Would it be possible to add Inception V4 to the Keras Applications folder?\r\n\r\nSystem/User Specifics: Tensorflow 2.0 with `tf.keras` module\r\n\r\nDue to recent discoveries made in neural networks and also according to many benchmarking standards, Inception V4 is one of the best networks available with very high accuracies in both Top 5 and Top 1. \r\n\r\nInception V4 should be included because it has been consistently proven to be one of the most accurate networks for Image Recognition. It will benefit many users who want a network with very high accuracy and also have the resources to run it. ", "comments": ["It has been 3 months, do we have any info about this request? \r\n", "keras applications is actually in the merging process with tf model garden. Maybe we should have a formal discussion of where this should go.", "I think we should make this contributions welcome. @Alpheron Would you be interested?", "So I think tensorflow-slim has an Inception V4 model which has been written which can be found [here](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py). However, according to the documentation saying that Slim is not supported with Tensorflow 2.0, I think it would be fair to say that this model must be converted sometime in the future to be able to work with Keras. In fairness, the models found [here](https://github.com/tensorflow/models/blob/master/research/slim/README.md#pre-trained-models) are all very capable models not all of which are supported. I am not 100% sure about if the Tensorflow Model Garden was also merged recently. In that case, it may not make sense to only implement Inception V4 to Keras Applications, but it may make sense to port the entire Tensorflow Slim model library to Keras Applications. That might sound a bit ambitious but I think the models all are very capable in different use cases and may be of use to people. As far as Inception V4 goes, I think that I will be able to port that to Keras and will be able to issue a PR sometime in the very near future (around a week or so)."]}, {"number": 33688, "title": "GradientTape: Allow to execute backward functions on same device as forward functions", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently GradientTape.gradient() is executed on the device of the scope it is called in. Have a look at the following code:\r\n\r\n```\r\nwith tf.GradientTape() as tape:\r\n    with tf.device('/gpu:1'):\r\n        x = f1(input)\r\n    with tf.device('/gpu:2'):\r\n        x = f2(x)\r\n    with tf.device('/gpu:0'):\r\n        g = tape.gradient(x, f_vars)\r\n```\r\n\r\nHere all gradient calculations will be carried out by GPU:0 and all variables needed for the gradient calculation will also be allocated on GPU:0. This is a problem if these temporary variables are too large to fit into the VRAM of GPU:0.\r\n\r\nPlease provide a way to execute the backward functions on the device of the corresponding forward function and allocate temporary variables for gradient calculation there. This allows to split a large model and distribute it among as many GPUs as necessary.\r\n\r\n**Will this change the current api? How?**\r\nIt will add a parameter to tf.GradientTape that controls if the user wants the current or the suggested behavior.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to train large models that do not fit into the VRAM of a single GPU.", "comments": ["I'm happy to help with this but I would need some hints where to implement it.", "This is pretty bad, I agree. @allenlavoie @saxenasaurabh I think we can probably add device annotations in https://github.com/tensorflow/tensorflow/blob/c103b2b42dd710eda1f1e795ab7b2af3960233ef/tensorflow/python/eager/backprop.py#L141 right?", "So basically the colocate_gradients_with_ops option to `tf.gradients` in 1.x. Yeah, seems fine. Device labels leaking into SavedModels might be a consequence, but mostly those will save inference functions.\r\n\r\nIf you just don't have a scope around the `tape.gradient` call, isn't this the behavior you'd get? I'd hope the placer would catch on and start placing the operations on the devices their inputs come from (although if backprop started on the wrong device I could see it continuing with bad greedy placements executing eagerly).", "Yes, having something like colocate_gradients_with_ops would be good.\r\n\r\nI tried a toy example without the scope around `tape.gradient`. But the operations are just placed on GPU:0 and not on the device of the forward function.", "> This is pretty bad, I agree. @allenlavoie @saxenasaurabh I think we can probably add device annotations in\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/c103b2b42dd710eda1f1e795ab7b2af3960233ef/tensorflow/python/eager/backprop.py#L141\r\n> right?\r\n\r\nAdding the annotations there might work, but we have to know which annotation to add. I just had a short look at `python/eager/tape.py` and `python/eager/pywrap_tfe_src.cc` and as far as I can tell the device is not recorded on the tape? So this might have to be solved first.\r\nBut I have to admit that I do not fully understand how recording to the tape works...", "The device is not recorded on the tape but we can just use the device of\nthe gradient tensor or of any other tensor we kept around from the forward\npass.\n\nOn Wed, Oct 30, 2019 at 5:31 AM Niels Ole Salscheider <\nnotifications@github.com> wrote:\n\n> This is pretty bad, I agree. @allenlavoie <https://github.com/allenlavoie>\n> @saxenasaurabh <https://github.com/saxenasaurabh> I think we can probably\n> add device annotations in\n>\n>\n> https://github.com/tensorflow/tensorflow/blob/c103b2b42dd710eda1f1e795ab7b2af3960233ef/tensorflow/python/eager/backprop.py#L141\n> right?\n>\n> Adding the annotations there might work, but we have to know which\n> annotation to add. I just had a short look at python/eager/tape.py and\n> python/eager/pywrap_tfe_src.cc and as far as I can tell the device is not\n> recorded on the tape? So this might have to be solved first.\n> But I have to admit that I do not fully understand how recording to the\n> tape works...\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33688?email_source=notifications&email_token=AAABHRMYVQMMXFVSAMI4SYDQRF5BFA5CNFSM4JEVEGYKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECT6YPQ#issuecomment-547875902>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHROXHIQX63DI3RGON2DQRF5BFANCNFSM4JEVEGYA>\n> .\n>\n\n\n-- \n - Alex\n", "Ok, something like this allows me to train my distributed model:\r\n\r\n```\r\ndiff --git a/tensorflow/python/eager/backprop.py b/tensorflow/python/eager/backprop.py\r\nindex e2a4992996..36e60fd257 100644\r\n--- a/tensorflow/python/eager/backprop.py\r\n+++ b/tensorflow/python/eager/backprop.py\r\n@@ -138,7 +138,9 @@ def _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs,\r\n   if grad_fn is None:\r\n     return [None] * num_inputs\r\n \r\n-  return grad_fn(mock_op, *out_grads)\r\n+  device = None if not inputs or len(inputs) == 0 else inputs[0].device\r\n+  with ops.device(device):\r\n+    return grad_fn(mock_op, *out_grads)\r\n \r\n \r\n pywrap_tensorflow.TFE_Py_RegisterGradientFunction(_gradient_function)\r\n```", "inputs[0] might be none while another inputs is not, but otherwise yeah,\nthis is a patch I'd accept.\n\nOn Wed, Oct 30, 2019 at 10:11 AM Niels Ole Salscheider <\nnotifications@github.com> wrote:\n\n> Ok, something like this allows me to train my distributed model:\n>\n> diff --git a/tensorflow/python/eager/backprop.py b/tensorflow/python/eager/backprop.py\n> index e2a4992996..36e60fd257 100644\n> --- a/tensorflow/python/eager/backprop.py\n> +++ b/tensorflow/python/eager/backprop.py\n> @@ -138,7 +138,9 @@ def _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs,\n>    if grad_fn is None:\n>      return [None] * num_inputs\n>\n> -  return grad_fn(mock_op, *out_grads)\n> +  device = None if not inputs or len(inputs) == 0 else inputs[0].device\n> +  with ops.device(device):\n> +    return grad_fn(mock_op, *out_grads)\n>\n>\n>  pywrap_tensorflow.TFE_Py_RegisterGradientFunction(_gradient_function)\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33688?email_source=notifications&email_token=AAABHRLUEOM4V3FYOR5T2JDQRG53LA5CNFSM4JEVEGYKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECU5WTQ#issuecomment-548002638>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNLQ7LWCDMGPODOPYLQRG53LANCNFSM4JEVEGYA>\n> .\n>\n\n\n-- \n - Alex\n", "@jaingaurav Why was a64ff0f2cda9d4e35ea450d4e945009a90ddee9a rolled back?\r\nSince the rollback this issue is valid again.", "@olesalscheider: We unfortunately hit a number of model performance regressions due to this change. We're still trying to investigate but this is isn't surprising given how careful one needs to be about device placement.\r\n\r\nI'm not sure what all models we have available publicly for you to look into. But the issue seemed pretty wide-spread (at least for pure eager workloads).", "Does this just occur on models that are split across multiple devices or also in the single GPU case?\r\n\r\nI could imagine that my change introduces more points where different devices have to be synchronized and that this comes with some overhead. But I don't know how to avoid that...\r\n\r\nIf we can't find a solution maybe we can introduce a flag for the GradientTape that decides the behavior of the device placement? Slower training is still better than no training...", "We found a performance regression in the single-gpu case, which is what\nworried us to the point of rolling this back.\n\nOn Thu, Nov 21, 2019 at 4:14 AM Niels Ole Salscheider <\nnotifications@github.com> wrote:\n\n> Does this just occur on models that are split across multiple devices or\n> also in the single GPU case?\n>\n> I could imagine that my change introduces more points where different\n> devices have to be synchronized and that this comes with some overhead. But\n> I don't know how to avoid that...\n>\n> If we can't find a solution maybe we can introduce a flag for the\n> GradientTape that decides the behavior of the device placement? Slower\n> training is still better than no training...\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33688?email_source=notifications&email_token=AAABHROXGT6UQ3GTHFWXQYTQUZ3UHA5CNFSM4JEVEGYKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE2AVPA#issuecomment-557058748>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKIDET62CASPQHQEGTQUZ3UHANCNFSM4JEVEGYA>\n> .\n>\n\n\n-- \n - Alex\n", "Hm, that's weird. The only explanation I have is that some of the operations have outputs or inputs on the CPU and thus the operations for the backward pass end up there...\r\n\r\nIt might be helpful to log the device placement for a model with performance regression with and without the patch and have a look at the diff. Maybe that can bring some clarity.", "I tried a few models that I had here but for these the diff of the device placement was empty and the performance did not change (neither in eager nor in graph mode). I think it will be difficult for me to reproduce this without more information...", "I am trying to go model parallel and I seem to be hitting this issue. I am in 2.3.0.dev20200619. The forward pass is executed based on my device placement. However, the tape.gradients execution happens only in GPU 0. Is there a particular setting that enables the fix listed in here?\r\n\r\nAs a reference, a profile picture (4 GPUs). Note forward pass operators happening in all GPUs, and then a ton of P2P transfers happening to GPU 0 in the gradient computation phase.\r\n\r\n![image](https://user-images.githubusercontent.com/24900898/85205596-4c467a00-b2ea-11ea-8b61-16bb281451ac.png)\r\n", "I got confused with the transitions in this issue as well as in the attached bug. However, I have confirmed that the fix attempt did not make it to master. IMO this is quite an important feature for anyone trying to go model parallel. While this is sorted out I have implemented a workaround that may help others. The idea is to use a tape per device and stitch the gradient computation manually. This is a small test that demonstrates it. Note that I am doing the same computation twice, first the traditional single tape and with the tape per device approach after. The tape management here is generic so I believe this method can be directly re-used by others. Attaching a profile for 10 iterations right after which demonstrates how the gradient computation happens in the expected device the second round (and the execution is way way faster btw):\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python import eager\r\n\r\ngpus = tf.config.experimental.list_logical_devices('GPU')\r\nassert len(gpus) > 1\r\n\r\nNUM_ITERATIONS = 10\r\nNUM_LAYERS = 10\r\n\r\n# Device mapping fnc\r\ndef get_device(layer_id):\r\n    if layer_id < NUM_LAYERS // 2:\r\n        g = gpus[0]\r\n    else:\r\n        g = gpus[1]\r\n    return g\r\n\r\nw = []\r\nfor i in range(NUM_LAYERS):\r\n    device = get_device(i)\r\n    with tf.device(device):\r\n        w.append(tf.Variable(tf.random.uniform([2000,2000])))\r\n\r\nwith tf.device(gpus[0]):\r\n    input_tensor = tf.random.uniform([100,2000])\r\n\r\ndef get_w(layer_id):\r\n    if layer_id == -1:\r\n        return w\r\n    return w[layer_id]\r\n\r\ndef fetch_inputs(it):\r\n    return input_tensor\r\n\r\ndef execute_layer(layer_idx, i):\r\n    wi = get_w(layer_idx)\r\n    return tf.matmul(i, wi)\r\n\r\neager.profiler.start()\r\n\r\n# Compute gradients with single tape\r\nfor it in range(NUM_ITERATIONS):\r\n\r\n    with tf.GradientTape() as tape:\r\n\r\n        t = fetch_inputs(it)\r\n\r\n        tape.watch(w)\r\n        tape.watch(t)\r\n\r\n        for layer_idx in range(10):\r\n\r\n            device = get_device(layer_idx)\r\n            wi = get_w(layer_idx)\r\n\r\n            with tf.device(device):\r\n                t = execute_layer(layer_idx, t)\r\n\r\n    grad_orig = tape.gradient(t, w, unconnected_gradients=tf.UnconnectedGradients.NONE)\r\n\r\n\r\n# Use tape per device group\r\nfor it in range(NUM_ITERATIONS):\r\n\r\n    tape = []\r\n    inputs = []\r\n    loss = []\r\n    prev_gradient = []\r\n    tape_device = []\r\n    current_device = None\r\n\r\n    t = fetch_inputs(it)\r\n\r\n    for layer_idx in range(NUM_LAYERS):\r\n\r\n        device = get_device(layer_idx)\r\n\r\n        # Create new tape if switching devices\r\n        if current_device != device:\r\n            tape.append(tf.GradientTape())\r\n            current_device = device\r\n            with tape[-1]:\r\n                tape[-1].watch(t)\r\n            inputs.append([t])\r\n            loss.append(None)\r\n            tape_device.append(device)\r\n\r\n        with tf.device(device):                \r\n            with tape[-1]:\r\n                t = execute_layer(layer_idx, t)\r\n                loss[-1] = t\r\n\r\n    # Final gradient re-construction\r\n    grad = []\r\n    out_grad = None\r\n    for tape_idx in reversed(range(0, len(tape))):\r\n\r\n        # Compute gradients current tape\r\n        with tf.device(tape_device[tape_idx]):\r\n            current_g = tape[tape_idx].gradient(loss[tape_idx],\r\n                                inputs[tape_idx] + get_w(-1),\r\n                                output_gradients=out_grad,\r\n                                unconnected_gradients=tf.UnconnectedGradients.NONE)\r\n\r\n        out_grad = current_g[0:len(inputs[tape_idx])]\r\n\r\n        if len(grad) == 0:\r\n            grad = current_g[len(inputs[tape_idx]):]\r\n        else:\r\n            for idx, g in enumerate(current_g[len(inputs[tape_idx]):]):\r\n                if g is not None:\r\n                    assert grad[idx] is None\r\n                    grad[idx] = g\r\n\r\nprofiler_result = eager.profiler.stop()\r\neager.profiler.save(\"/workspace/hdda/rammat01/log_test_placement\", profiler_result)\r\n\r\n\r\n# Check if gradients are equal\r\nfor g0, g1 in zip(grad, grad_orig):\r\n    tf.debugging.assert_equal(g0, g1)\r\n\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/24900898/85295058-65c3ff00-b46d-11ea-9b09-bc8bae69260b.png)\r\n", "It made it into master for a short amount of time, but then got reverted. If you build tensorflow yourself you can just revert the revert / apply a64ff0f2cda9d4e35ea450d4e945009a90ddee9a again... I don't really have a test case with degraded performance here, so I don't know how to push this further.", "Maybe we should resubmit it as an optional flag, turned off by default?\n\nOn Mon, Jun 22, 2020 at 7:06 AM Niels Ole Salscheider <\nnotifications@github.com> wrote:\n\n> It made it into master for a short amount of time, but then got reverted.\n> If you build tensorflow yourself you can just revert the revert / apply\n> a64ff0f\n> <https://github.com/tensorflow/tensorflow/commit/a64ff0f2cda9d4e35ea450d4e945009a90ddee9a>\n> again... I don't really have a test case with degraded performance here, so\n> I don't know how to push this further.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33688#issuecomment-647541685>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRN2FEXUH722XGNCJ2DRX5QPNANCNFSM4JEVEGYA>\n> .\n>\n\n\n-- \n - Alex\n", "> It made it into master for a short amount of time, but then got reverted. If you build tensorflow yourself you can just revert the revert / apply [a64ff0f](https://github.com/tensorflow/tensorflow/commit/a64ff0f2cda9d4e35ea450d4e945009a90ddee9a) again... I don't really have a test case with degraded performance here, so I don't know how to push this further.\r\n\r\nAny model parallel setting will benefit from this since it avoids p2p transfers of the operator's inputs during gradient computation. Look at the timeline I posted and compare blue vs red boxes. Both are doing exactly the same amount of work.", "+1 on this feature. I'm also running into this issue.", "+1 on this feature. I'm also running into this issue.", "> I got confused with the transitions in this issue as well as in the attached bug. However, I have confirmed that the fix attempt did not make it to master. IMO this is quite an important feature for anyone trying to go model parallel. While this is sorted out I have implemented a workaround that may help others. The idea is to use a tape per device and stitch the gradient computation manually. This is a small test that demonstrates it. Note that I am doing the same computation twice, first the traditional single tape and with the tape per device approach after. The tape management here is generic so I believe this method can be directly re-used by others. Attaching a profile for 10 iterations right after which demonstrates how the gradient computation happens in the expected device the second round (and the execution is way way faster btw):\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from tensorflow.python import eager\r\n> \r\n> gpus = tf.config.experimental.list_logical_devices('GPU')\r\n> assert len(gpus) > 1\r\n> \r\n> NUM_ITERATIONS = 10\r\n> NUM_LAYERS = 10\r\n> \r\n> # Device mapping fnc\r\n> def get_device(layer_id):\r\n>     if layer_id < NUM_LAYERS // 2:\r\n>         g = gpus[0]\r\n>     else:\r\n>         g = gpus[1]\r\n>     return g\r\n> \r\n> w = []\r\n> for i in range(NUM_LAYERS):\r\n>     device = get_device(i)\r\n>     with tf.device(device):\r\n>         w.append(tf.Variable(tf.random.uniform([2000,2000])))\r\n> \r\n> with tf.device(gpus[0]):\r\n>     input_tensor = tf.random.uniform([100,2000])\r\n> \r\n> def get_w(layer_id):\r\n>     if layer_id == -1:\r\n>         return w\r\n>     return w[layer_id]\r\n> \r\n> def fetch_inputs(it):\r\n>     return input_tensor\r\n> \r\n> def execute_layer(layer_idx, i):\r\n>     wi = get_w(layer_idx)\r\n>     return tf.matmul(i, wi)\r\n> \r\n> eager.profiler.start()\r\n> \r\n> # Compute gradients with single tape\r\n> for it in range(NUM_ITERATIONS):\r\n> \r\n>     with tf.GradientTape() as tape:\r\n> \r\n>         t = fetch_inputs(it)\r\n> \r\n>         tape.watch(w)\r\n>         tape.watch(t)\r\n> \r\n>         for layer_idx in range(10):\r\n> \r\n>             device = get_device(layer_idx)\r\n>             wi = get_w(layer_idx)\r\n> \r\n>             with tf.device(device):\r\n>                 t = execute_layer(layer_idx, t)\r\n> \r\n>     grad_orig = tape.gradient(t, w, unconnected_gradients=tf.UnconnectedGradients.NONE)\r\n> \r\n> \r\n> # Use tape per device group\r\n> for it in range(NUM_ITERATIONS):\r\n> \r\n>     tape = []\r\n>     inputs = []\r\n>     loss = []\r\n>     prev_gradient = []\r\n>     tape_device = []\r\n>     current_device = None\r\n> \r\n>     t = fetch_inputs(it)\r\n> \r\n>     for layer_idx in range(NUM_LAYERS):\r\n> \r\n>         device = get_device(layer_idx)\r\n> \r\n>         # Create new tape if switching devices\r\n>         if current_device != device:\r\n>             tape.append(tf.GradientTape())\r\n>             current_device = device\r\n>             with tape[-1]:\r\n>                 tape[-1].watch(t)\r\n>             inputs.append([t])\r\n>             loss.append(None)\r\n>             tape_device.append(device)\r\n> \r\n>         with tf.device(device):                \r\n>             with tape[-1]:\r\n>                 t = execute_layer(layer_idx, t)\r\n>                 loss[-1] = t\r\n> \r\n>     # Final gradient re-construction\r\n>     grad = []\r\n>     out_grad = None\r\n>     for tape_idx in reversed(range(0, len(tape))):\r\n> \r\n>         # Compute gradients current tape\r\n>         with tf.device(tape_device[tape_idx]):\r\n>             current_g = tape[tape_idx].gradient(loss[tape_idx],\r\n>                                 inputs[tape_idx] + get_w(-1),\r\n>                                 output_gradients=out_grad,\r\n>                                 unconnected_gradients=tf.UnconnectedGradients.NONE)\r\n> \r\n>         out_grad = current_g[0:len(inputs[tape_idx])]\r\n> \r\n>         if len(grad) == 0:\r\n>             grad = current_g[len(inputs[tape_idx]):]\r\n>         else:\r\n>             for idx, g in enumerate(current_g[len(inputs[tape_idx]):]):\r\n>                 if g is not None:\r\n>                     assert grad[idx] is None\r\n>                     grad[idx] = g\r\n> \r\n> profiler_result = eager.profiler.stop()\r\n> eager.profiler.save(\"/workspace/hdda/rammat01/log_test_placement\", profiler_result)\r\n> \r\n> \r\n> # Check if gradients are equal\r\n> for g0, g1 in zip(grad, grad_orig):\r\n>     tf.debugging.assert_equal(g0, g1)\r\n> ```\r\n> \r\n> ![image](https://user-images.githubusercontent.com/24900898/85295058-65c3ff00-b46d-11ea-9b09-bc8bae69260b.png)\r\n\r\nso In my understanding, by using such method, I could split the training of my model into two or multi-gpus, so that it won't drop into OOM trouble like I only training in one GPU?\r\n\r\nwould ur example works for the following toy model? let's say:\r\n\r\n```\r\n    with tf.device('GPU:0'):\r\n        l1 = layers.Conv3D()(input)\r\n        l2 = layers.Conv3D()(l1)\r\n        \r\n    with tf.device('GPU:1'):\r\n        l3 = layers.Conv3D()(l2)\r\n        output = layers.Conv3D()(l3)\r\n```\r\nso, how would you compute the gradient of variables in `l1` with respect to `output`? If u create a `tape` only on `GPU:0`, then would this `tape` fail to see the `output` in `GPU:1`?", "> It made it into master for a short amount of time, but then got reverted. If you build tensorflow yourself you can just revert the revert / apply [a64ff0f](https://github.com/tensorflow/tensorflow/commit/a64ff0f2cda9d4e35ea450d4e945009a90ddee9a) again... I don't really have a test case with degraded performance here, so I don't know how to push this further.\r\n\r\nI still a bit confused even if we could have a placer for `tape`. For example, if you place the 1st half of your model on `gpu0` and second half on `gpu1` (hence the output tensor is on `gpu1`). Then, when you compute the gradient of variables on `gpu0` with resecpet to output tensor, should u need all gradients of the second half model, which are on `gpu1`, so that you could use chain rule? but if we do that, will we force to copy all gradients from `gpu1` to `gpu0` and hence we save nothing? \r\n\r\nalso, please refer the toy example that I put [here](https://github.com/tensorflow/tensorflow/issues/33688#issuecomment-803298276)", "I'm having the same issue as I was trying to distribute one model over multiple GPUs.\r\n\r\n@ramonmatas can this (https://github.com/tensorflow/tensorflow/issues/33688#issuecomment-647530665) be replicated over multiple workers? What I mean is: Can we run this on n-machines using e.g. the `tf.distribute.MultiWorkerMirroredStrategy`?", "> I'm having the same issue as I was trying to distribute one model over multiple GPUs.\r\n> \r\n> @ramonmatas can this ([#33688 (comment)](https://github.com/tensorflow/tensorflow/issues/33688#issuecomment-647530665)) be replicated over multiple workers? What I mean is: Can we run this on n-machines using e.g. the `tf.distribute.MultiWorkerMirroredStrategy`?\r\n\r\nMy understanding is that strategy is orthogonal to what we are discussing here. In a multi-worker setting you could still apply model parallelism through device placement in each local node, then parallelize externally through strategy. I do not know if strategy messes up with device placement even further though, I have never tried.", "> My understanding is that strategy is orthogonal \r\n\r\nBy \"_strategy_\", do you by mean `tf.distributed.Strategy`? The problem I have here is that any mirrored-strategy will automatically copy the model on each device but also all gradient-resources.\r\n\r\nI wouldn't know how to avoid this.\r\n\r\nI can only do this on _one_ machine using `tf.distributed.Strategy` but now I can't use multiple workers. Could you elaborate please?", "> I'm having the same issue as I was trying to distribute one model over multiple GPUs.\r\n> \r\n> @ramonmatas can this ([#33688 (comment)](https://github.com/tensorflow/tensorflow/issues/33688#issuecomment-647530665)) be replicated over multiple workers? What I mean is: Can we run this on n-machines using e.g. the `tf.distribute.MultiWorkerMirroredStrategy`?\r\n\r\nhi @stefan-falk , have you solve this issue? I am trying to split my model into multi-devices but for forward pass it works, the backward grad update all fall into gpu0....", "> Maybe we should resubmit it as an optional flag, turned off by default?\r\n> [\u2026](#)\r\n\r\nHi is this flag being added? @alextp This is really an import feature that people who train large segmentation model could be benefit from ^^. Or any plan on that? ", "> I got confused with the transitions in this issue as well as in the attached bug. However, I have confirmed that the fix attempt did not make it to master. IMO this is quite an important feature for anyone trying to go model parallel. While this is sorted out I have implemented a workaround that may help others. The idea is to use a tape per device and stitch the gradient computation manually. This is a small test that demonstrates it. Note that I am doing the same computation twice, first the traditional single tape and with the tape per device approach after. The tape management here is generic so I believe this method can be directly re-used by others. Attaching a profile for 10 iterations right after which demonstrates how the gradient computation happens in the expected device the second round (and the execution is way way faster btw):\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from tensorflow.python import eager\r\n> \r\n> gpus = tf.config.experimental.list_logical_devices('GPU')\r\n> assert len(gpus) > 1\r\n> \r\n> NUM_ITERATIONS = 10\r\n> NUM_LAYERS = 10\r\n> \r\n> # Device mapping fnc\r\n> def get_device(layer_id):\r\n>     if layer_id < NUM_LAYERS // 2:\r\n>         g = gpus[0]\r\n>     else:\r\n>         g = gpus[1]\r\n>     return g\r\n> \r\n> w = []\r\n> for i in range(NUM_LAYERS):\r\n>     device = get_device(i)\r\n>     with tf.device(device):\r\n>         w.append(tf.Variable(tf.random.uniform([2000,2000])))\r\n> \r\n> with tf.device(gpus[0]):\r\n>     input_tensor = tf.random.uniform([100,2000])\r\n> \r\n> def get_w(layer_id):\r\n>     if layer_id == -1:\r\n>         return w\r\n>     return w[layer_id]\r\n> \r\n> def fetch_inputs(it):\r\n>     return input_tensor\r\n> \r\n> def execute_layer(layer_idx, i):\r\n>     wi = get_w(layer_idx)\r\n>     return tf.matmul(i, wi)\r\n> \r\n> eager.profiler.start()\r\n> \r\n> # Compute gradients with single tape\r\n> for it in range(NUM_ITERATIONS):\r\n> \r\n>     with tf.GradientTape() as tape:\r\n> \r\n>         t = fetch_inputs(it)\r\n> \r\n>         tape.watch(w)\r\n>         tape.watch(t)\r\n> \r\n>         for layer_idx in range(10):\r\n> \r\n>             device = get_device(layer_idx)\r\n>             wi = get_w(layer_idx)\r\n> \r\n>             with tf.device(device):\r\n>                 t = execute_layer(layer_idx, t)\r\n> \r\n>     grad_orig = tape.gradient(t, w, unconnected_gradients=tf.UnconnectedGradients.NONE)\r\n> \r\n> \r\n> # Use tape per device group\r\n> for it in range(NUM_ITERATIONS):\r\n> \r\n>     tape = []\r\n>     inputs = []\r\n>     loss = []\r\n>     prev_gradient = []\r\n>     tape_device = []\r\n>     current_device = None\r\n> \r\n>     t = fetch_inputs(it)\r\n> \r\n>     for layer_idx in range(NUM_LAYERS):\r\n> \r\n>         device = get_device(layer_idx)\r\n> \r\n>         # Create new tape if switching devices\r\n>         if current_device != device:\r\n>             tape.append(tf.GradientTape())\r\n>             current_device = device\r\n>             with tape[-1]:\r\n>                 tape[-1].watch(t)\r\n>             inputs.append([t])\r\n>             loss.append(None)\r\n>             tape_device.append(device)\r\n> \r\n>         with tf.device(device):                \r\n>             with tape[-1]:\r\n>                 t = execute_layer(layer_idx, t)\r\n>                 loss[-1] = t\r\n> \r\n>     # Final gradient re-construction\r\n>     grad = []\r\n>     out_grad = None\r\n>     for tape_idx in reversed(range(0, len(tape))):\r\n> \r\n>         # Compute gradients current tape\r\n>         with tf.device(tape_device[tape_idx]):\r\n>             current_g = tape[tape_idx].gradient(loss[tape_idx],\r\n>                                 inputs[tape_idx] + get_w(-1),\r\n>                                 output_gradients=out_grad,\r\n>                                 unconnected_gradients=tf.UnconnectedGradients.NONE)\r\n> \r\n>         out_grad = current_g[0:len(inputs[tape_idx])]\r\n> \r\n>         if len(grad) == 0:\r\n>             grad = current_g[len(inputs[tape_idx]):]\r\n>         else:\r\n>             for idx, g in enumerate(current_g[len(inputs[tape_idx]):]):\r\n>                 if g is not None:\r\n>                     assert grad[idx] is None\r\n>                     grad[idx] = g\r\n> \r\n> profiler_result = eager.profiler.stop()\r\n> eager.profiler.save(\"/workspace/hdda/rammat01/log_test_placement\", profiler_result)\r\n> \r\n> \r\n> # Check if gradients are equal\r\n> for g0, g1 in zip(grad, grad_orig):\r\n>     tf.debugging.assert_equal(g0, g1)\r\n> ```\r\n> \r\n> ![image](https://user-images.githubusercontent.com/24900898/85295058-65c3ff00-b46d-11ea-9b09-bc8bae69260b.png)\r\n\r\nhi I tried this toy example and I have an issue here. To be abbreviate, I increase the tensor size to 8000, 8000 and add `tf.config.experimental.set_memory_growth(gpu, True)` so that I know how much VARM is consumed by each GPU. \r\n\r\nI used two gpus. However, the peak memory consumption (on gpu0) is exactly the same no matter we split into two taps or only use one tap. \r\n\r\nhere is the code I tried (slightly modified from yours):\r\n```\r\n\r\nimport tensorflow as tf\r\n# from tensorflow.python import eager\r\n\r\n# if additional flags are needed, define it here.\r\n# gpus = tf.config.experimental.list_physical_devices('GPU')\r\n\r\n# Currently, memory growth needs to be the same across GPUs\r\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\ngpus = tf.config.experimental.list_logical_devices('GPU')\r\nassert len(gpus) > 1\r\n\r\nNUM_ITERATIONS = 128\r\nNUM_LAYERS = 10\r\n\r\n\r\n# Device mapping fnc\r\ndef get_device(layer_id):\r\n    if layer_id < NUM_LAYERS // 2:\r\n        g = gpus[0]\r\n    else:\r\n        g = gpus[1]\r\n    return g\r\n\r\n\r\nw = []\r\nfor i in range(NUM_LAYERS):\r\n    device = get_device(i)\r\n    with tf.device(device):\r\n        w.append(tf.Variable(tf.random.uniform([8000, 8000])))\r\n\r\nwith tf.device(gpus[0]):\r\n    input_tensor = tf.random.uniform([8000, 8000])\r\n\r\n\r\ndef get_w(layer_id):\r\n    if layer_id == -1:\r\n        return w\r\n    return w[layer_id]\r\n\r\n\r\ndef fetch_inputs(it):\r\n    return input_tensor\r\n\r\n\r\ndef execute_layer(layer_idx, i):\r\n    wi = get_w(layer_idx)\r\n    return tf.matmul(i, wi)\r\n\r\n\r\n# eager.profiler.start()\r\n\r\nprint(f'input tensor size: {input_tensor.shape}', flush=True)\r\n\r\n####### -------------------- run this first ------------------------- ##########\r\n# Compute gradients with single tape\r\nfor it in range(NUM_ITERATIONS):\r\n\r\n    with tf.GradientTape() as tape:\r\n\r\n        t = fetch_inputs(it)\r\n\r\n        tape.watch(w)\r\n        tape.watch(t)\r\n\r\n        for layer_idx in range(10):\r\n            device = get_device(layer_idx)\r\n            wi = get_w(layer_idx)\r\n\r\n            with tf.device(device):\r\n                t = execute_layer(layer_idx, t)\r\n\r\n    grad_orig = tape.gradient(t, w, unconnected_gradients=tf.UnconnectedGradients.NONE)\r\n\r\nprint(f'iter1 done', flush=True)\r\n\r\n\r\n####### -------------------- run this second ------------------------- ##########\r\n# Use tape per device group\r\n#\r\nfor it in range(NUM_ITERATIONS):\r\n\r\n    tape = []\r\n    inputs = []\r\n    loss = []\r\n    prev_gradient = []\r\n    tape_device = []\r\n    current_device = None\r\n\r\n    t = fetch_inputs(it)\r\n\r\n    for layer_idx in range(NUM_LAYERS):\r\n\r\n        device = get_device(layer_idx)\r\n\r\n        # Create new tape if switching devices\r\n        if current_device != device:\r\n            tape.append(tf.GradientTape())\r\n            current_device = device\r\n            with tape[-1]:\r\n                tape[-1].watch(t)\r\n            inputs.append([t])\r\n            loss.append(None)\r\n            tape_device.append(device)\r\n\r\n        with tf.device(device):\r\n            with tape[-1]:\r\n                t = execute_layer(layer_idx, t)\r\n                loss[-1] = t\r\n\r\n    # Final gradient re-construction\r\n    grad = []\r\n    out_grad = None\r\n    for tape_idx in reversed(range(0, len(tape))):\r\n\r\n        # Compute gradients current tape\r\n        with tf.device(tape_device[tape_idx]):\r\n            current_g = tape[tape_idx].gradient(loss[tape_idx],\r\n                                                inputs[tape_idx] + get_w(-1),\r\n                                                output_gradients=out_grad,\r\n                                                unconnected_gradients=tf.UnconnectedGradients.NONE)\r\n\r\n        out_grad = current_g[0:len(inputs[tape_idx])]\r\n\r\n        if len(grad) == 0:\r\n            grad = current_g[len(inputs[tape_idx]):]\r\n        else:\r\n            for idx, g in enumerate(current_g[len(inputs[tape_idx]):]):\r\n                if g is not None:\r\n                    assert grad[idx] is None\r\n                    grad[idx] = g\r\n\r\n# profiler_result = eager.profiler.stop()\r\n# eager.profiler.save(\"/workspace/hdda/rammat01/log_test_placement\", profiler_result)\r\n\r\n# Check if gradients are equal\r\n# for g0, g1 in zip(grad, grad_orig):\r\n#     tf.debugging.assert_equal(g0, g1)\r\n\r\n```", "@WingsOfPanda No, I wasn't able to make this work properly. There is [tensorflow/mesh](https://github.com/tensorflow/mesh) (unfortunately it's for TF 1) which seems to be supporting model parallelism. Idk if there's already something we can use for TF2 but my focus has shifted by now."]}, {"number": 33590, "title": "Fix behavior difference between tf.io.GFile and python file for utf8", "body": "This PR tries to address the issue raised in #33563 where\r\ntf.io.GFile behavior is different from python file for utf8.\r\n\r\nThe issue was that tf.io.GFile's read does not take utf8 into\r\nconsideration for non binary mode.\r\n\r\nThis PR fixes the discrepancy.\r\n\r\nThis PR fixes #33563\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang  Can you please resolve conflicts? Thanks!", "Thanks @mihaimaruseac @gbaned for the review and sorry for the late response as the notice slipped through. The PR has been rebased and updated and the review comment has also been addressed. Please take a look.", "@yongtang Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!"]}, {"number": 33544, "title": "TF2.0 for compute capability 3.0", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): no\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nwould like to use TF2.0 gpu with old cards Geforce GTX 870m\r\n\r\n**Will this change the current api? How?**\r\nnon\r\n\r\n**Who will benefit with this feature?**\r\nall users with older gpu cards\r\n\r\n**Any Other info.**\r\n", "comments": ["@VitaMusic,\r\nCan you please provide a Use Case supporting your Feature Request. Thanks! ", "developing, testing, executing computer vision models with TF2.0-gpu/TRT-TF on a notebook with the gpu card of compute capability 3.0.\r\nactually TF v1.10 is used but would like to pass to TF2.0", "@VitaMusic Currently TensorFlow only supports compute capabilities >= 3.5. One way is to build TF on GPUs with compute capability =3.0 is to build from source. Thanks!\r\n\r\n", "thank you for your reply, I've built on linux with cc=3.0 without XLA,\nbut on windows I stagnate on the path for VS2017. I have vs2017 and\nintel2019, ideally I'd like to compile it with intel c++ but if it is not\npossible with vs.\nhow to indicate BAZEL_VS var ? the path C:\\Program Files (x86)\\Microsoft\nVisual Studio\\2017\\Enterprise does not work\nif intel c++ is possible what modifications to do in bazel config ? thanks\n\nOn Tue, Oct 22, 2019 at 10:19 PM Vishnuvardhan Janapati <\nnotifications@github.com> wrote:\n\n> @VitaMusic <https://github.com/VitaMusic> Currently TensorFlow only\n> supports compute capabilities >= 3.5. One way is to build TF on GPUs with\n> compute capability =3.0 is to build from source. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33544?email_source=notifications&email_token=AC7AFFJORWQUIFMCPAU5DRTQP5N6PA5CNFSM4JCUCUT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEB7BH4Y#issuecomment-545133555>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFN7NKK46UEGRIC2JCDQP5N6PANCNFSM4JCUCUTQ>\n> .\n>\n", "Hi,\r\nThe questions you posed are bazel on windows questions.\r\n@meteorcloudy @laszlocsomor ", "@VitaMusic : How does it not work, what error does Bazel print? See this for envvar configuration: https://docs.bazel.build/versions/master/windows.html#build-c-with-msvc", "Also, bazel autodetects visual studio path for me, and for tensorflow CI. I do not know why you need to set BAZEL_VS.", "here is the stdout of bazel\r\n(cuda 10.1, cudnn 7.6, compute capability = 3.0, /arch:AVX2)\r\n\r\n-------------------------------\r\n\r\nbazel --output_user_root user_root --output_base base build --config=opt --config=mkl --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\n\r\n----------------------------\r\n\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Writing tracer profile to 'F:/tf2/tensorflow/base/command.profile.gz'\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=c:/anaconda/envs/tf2/python.exe\r\nINFO: Reading rc options for 'build' from f:\\tf2\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --incompatible_remove_legacy_whole_archive --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2\r\nINFO: Reading rc options for 'build' from f:\\tf2\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=c:/anaconda/envs/tf2/python.exe --action_env PYTHON_LIB_PATH=c:/anaconda/envs/tf2/lib/site-packages --python_path=c:/anaconda/envs/tf2/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --config=cuda --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file f:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file f:\\tf2\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file f:\\tf2\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:monolithic in file f:\\tf2\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:opt in file f:\\tf2\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:mkl in file f:\\tf2\\tensorflow\\.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c opt\r\nINFO: Found applicable config definition build:v2 in file f:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nLoading: \r\nLoading: 0 packages loaded\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at F:/tf2/tensorflow/base/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - F:/tf2/tensorflow/base/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - F:/tf2/tensorflow/WORKSPACE:37:1\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\nF:/tf2/tensorflow/base/external/org_tensorflow\r\nF:/tf2/tensorflow\r\n[end of symlink chain]\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': no such package '@org_tensorflow//third_party/gpus': Could not access F:/tf2/tensorflow/base/external/org_tensorflow: Infinite symlink expansion\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': no such package '@org_tensorflow//third_party/gpus': Could not access F:/tf2/tensorflow/base/external/org_tensorflow: Infinite symlink expansion\r\nINFO: Elapsed time: 0.255s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\nFAILED: Build did NOT complete successfully (1 packages loaded)", "I would be glad to have a working script for windows compilation from source\r\n(vs2019, cuda10.1, cudnn7.6.5, cc=3.0, avx2, mkl)", "this is r2.0\r\nvs2019 installed but not detected\r\nset BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.23.2810\r\nset BAZEL_SH=C:\\msys64\\usr\\bin\\bash.exe\r\n---------------------------------\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Writing tracer profile to 'F:/tf2/base/command.profile.gz'\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=c:/anaconda/envs/tf2/python.exe\r\nINFO: Reading rc options for 'build' from f:\\tf2\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2\r\nINFO: Reading rc options for 'build' from f:\\tf2\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=c:/anaconda/envs/tf2/python.exe --action_env PYTHON_LIB_PATH=c:/anaconda/envs/tf2/lib/site-packages --python_path=c:/anaconda/envs/tf2/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --config=cuda --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file f:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2\r\nINFO: Found applicable config definition build:cuda in file f:\\tf2\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file f:\\tf2\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:monolithic in file f:\\tf2\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:opt in file f:\\tf2\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:mkl in file f:\\tf2\\tensorflow\\.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c opt\r\nINFO: Found applicable config definition build:v2 in file f:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2\r\nLoading: \r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at F:/tf2/base/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - F:/tf2/base/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - F:/tf2/tensorflow/WORKSPACE:35:1\r\nLoading: 0 packages loaded\r\nINFO: Call stack for the definition of repository 'local_config_cuda' which is a cuda_configure (rule definition at F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl:1268:18):\r\n - F:/tf2/tensorflow/tensorflow/workspace.bzl:68:5\r\n - F:/tf2/tensorflow/WORKSPACE:19:1\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in _create_local_cuda_repository\r\n\t\tfind_cc(repository_ctx)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in find_cc\r\n\t\t_get_msvc_compiler(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in _get_msvc_compiler\r\n\t\tfind_msvc_tool(repository_ctx, <2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n\t\t_get_vc_full_version(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n\t\t_get_latest_subversion(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n\t\trepository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in _create_local_cuda_repository\r\n\t\tfind_cc(repository_ctx)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in find_cc\r\n\t\t_get_msvc_compiler(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in _get_msvc_compiler\r\n\t\tfind_msvc_tool(repository_ctx, <2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n\t\t_get_vc_full_version(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n\t\t_get_latest_subversion(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n\t\trepository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in _create_local_cuda_repository\r\n\t\tfind_cc(repository_ctx)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in find_cc\r\n\t\t_get_msvc_compiler(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in _get_msvc_compiler\r\n\t\tfind_msvc_tool(repository_ctx, <2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n\t\t_get_vc_full_version(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n\t\t_get_latest_subversion(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n\t\trepository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\r\nINFO: Elapsed time: 19.865s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n---------------------------------", "when using vs2019 command prompt, no influence on path detection for VC\r\n----------------------------------------------------------------------------------\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)\r\nWARNING: Waiting for server process to terminate (waited 10 seconds, waiting at most 60)\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Writing tracer profile to 'F:/tf2/base/command.profile.gz'\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=c:/anaconda/envs/tf2/python.exe\r\nINFO: Reading rc options for 'build' from f:\\tf2\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2\r\nINFO: Reading rc options for 'build' from f:\\tf2\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=c:/anaconda/envs/tf2/python.exe --action_env PYTHON_LIB_PATH=c:/anaconda/envs/tf2/lib/site-packages --python_path=c:/anaconda/envs/tf2/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --config=cuda --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file f:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2\r\nINFO: Found applicable config definition build:cuda in file f:\\tf2\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file f:\\tf2\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:monolithic in file f:\\tf2\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:opt in file f:\\tf2\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:mkl in file f:\\tf2\\tensorflow\\.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c opt\r\nINFO: Found applicable config definition build:v2 in file f:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2\r\nLoading: \r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/tools/pip_package\r\nINFO: Call stack for the definition of repository 'local_config_cuda' which is a cuda_configure (rule definition at F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl:1268:18):\r\n - F:/tf2/tensorflow/tensorflow/workspace.bzl:68:5\r\n - F:/tf2/tensorflow/WORKSPACE:19:1\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in _create_local_cuda_repository\r\n\t\tfind_cc(repository_ctx)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in find_cc\r\n\t\t_get_msvc_compiler(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in _get_msvc_compiler\r\n\t\tfind_msvc_tool(repository_ctx, <2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n\t\t_get_vc_full_version(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n\t\t_get_latest_subversion(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n\t\trepository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in _create_local_cuda_repository\r\n\t\tfind_cc(repository_ctx)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in find_cc\r\n\t\t_get_msvc_compiler(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in _get_msvc_compiler\r\n\t\tfind_msvc_tool(repository_ctx, <2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n\t\t_get_vc_full_version(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n\t\t_get_latest_subversion(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n\t\trepository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in _create_local_cuda_repository\r\n\t\tfind_cc(repository_ctx)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in find_cc\r\n\t\t_get_msvc_compiler(<1 more arguments>)\r\n\tFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in _get_msvc_compiler\r\n\t\tfind_msvc_tool(repository_ctx, <2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n\t\t_get_vc_full_version(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n\t\t_get_latest_subversion(<2 more arguments>)\r\n\tFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n\t\trepository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\r\nINFO: Elapsed time: 14.927s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)", "What's your `--output_base` and `--output_user_root`?", "> What's your `--output_base` and `--output_user_root`?\r\n\r\nNever mind, I was looking at your older error log with the symlink cycle.\r\n\r\nBut this error though:\r\n\r\n> C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\r\n\r\nDoes this really not exist?", "It doesn't exist. I have 2019 version of VS\n\n\u0432\u0442, 19 \u041d\u043e\u044f 2019 \u0433., 9:12 L\u00e1szl\u00f3 Csomor <notifications@github.com>:\n\n> What's your --output_base and --output_user_root?\n>\n> Never mind, I was looking at your older error log with the symlink cycle.\n>\n> But this error though:\n>\n> C:/Program Files (x86)/Microsoft Visual\n> Studio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\n>\n> Does this really not exist?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33544?email_source=notifications&email_token=AC7AFFPZSLLCJV3NPRQ7IOLQUONW7A5CNFSM4JCUCUT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEENHDQQ#issuecomment-555381186>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFJD67VUWSD7BJPHYTTQUONW7ANCNFSM4JCUCUTQ>\n> .\n>\n", "Try this in a normal cmd.exe (not in the VS 2019 command prompt):\r\n\r\n```\r\nset BAZEL_VS=\r\nset BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\r\nset\u00a0BAZEL_VC_FULL_VERSION=14.23.2810\r\n```\r\n\r\nand run Bazel.", "here is the output after this variables\n\n-----------------------------------------------\nStarting local Bazel server and connecting to it...\n... still trying to connect to local Bazel server after 10 seconds ...\nWARNING: The following configs were expanded more than once: [v2]. For\nrepeatable flags, repeats are counted twice and may lead to unexpected\nbehavior.\nINFO: Writing tracer profile to 'F:/tf2/base/command.profile.gz'\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\nINFO: Options provided by the client:\n  'build' options: --python_path=c:/anaconda/envs/tf2/python.exe\nINFO: Reading rc options for 'build' from f:\\tf2\\tensorflow\\.bazelrc:\n  'build' options: --apple_platform_type=macos --define\nframework_shared_object=true --define open_source_build=true\n--define=use_fast_cpp_protos=true --define=allow_oversize_protos=true\n--spawn_strategy=standalone --strategy=Genrule=standalone -c opt\n--announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr\n--define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\n--config=v2\nINFO: Reading rc options for 'build' from\nf:\\tf2\\tensorflow\\.tf_configure.bazelrc:\n  'build' options: --action_env\nPYTHON_BIN_PATH=c:/anaconda/envs/tf2/python.exe --action_env\nPYTHON_LIB_PATH=c:/anaconda/envs/tf2/lib/site-packages\n--python_path=c:/anaconda/envs/tf2/python.exe --action_env\nCUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\n--action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --config=cuda --config\nmonolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN\n--host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI\n--verbose_failures --distinct_host_configuration=false\n--define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\nINFO: Found applicable config definition build:v2 in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2\nINFO: Found applicable config definition build:cuda in file\nf:\\tf2\\tensorflow\\.bazelrc: --config=using_cuda\n--define=using_cuda_nvcc=true\nINFO: Found applicable config definition build:using_cuda in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env\nTF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\nINFO: Found applicable config definition build:monolithic in file\nf:\\tf2\\tensorflow\\.bazelrc: --define framework_shared_object=false\nINFO: Found applicable config definition build:opt in file\nf:\\tf2\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define\nwith_default_optimizations=true\nINFO: Found applicable config definition build:mkl in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=build_with_mkl=true\n--define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c\nopt\nINFO: Found applicable config definition build:v2 in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2\nLoading:\nLoading: 0 packages loaded\nLoading: 0 packages loaded\nLoading: 0 packages loaded\n    currently loading: tensorflow/tools/pip_package\nLoading: 0 packages loaded\n    currently loading: tensorflow/tools/pip_package\nINFO: Call stack for the definition of repository 'local_config_cuda' which\nis a cuda_configure (rule definition at\nF:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl:1268:18):\n - F:/tf2/tensorflow/tensorflow/workspace.bzl:68:5\n - F:/tf2/tensorflow/WORKSPACE:19:1\nLoading: 0 packages loaded\n    currently loading: tensorflow/tools/pip_package\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\n   Traceback (most recent call last):\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\n_create_local_cuda_repository(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in\n_create_local_cuda_repository\nfind_cc(repository_ctx)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in\nfind_cc\n_get_msvc_compiler(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in\n_get_msvc_compiler\nfind_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(<2 more\narguments>)\ntype 'NoneType' has no method replace()\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such\npackage '@local_config_cuda//cuda': Traceback (most recent call last):\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\n_create_local_cuda_repository(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in\n_create_local_cuda_repository\nfind_cc(repository_ctx)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in\nfind_cc\n_get_msvc_compiler(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in\n_get_msvc_compiler\nfind_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(<2 more\narguments>)\ntype 'NoneType' has no method replace()\nWARNING: Target pattern parsing failed.\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent\ncall last):\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\n_create_local_cuda_repository(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in\n_create_local_cuda_repository\nfind_cc(repository_ctx)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in\nfind_cc\n_get_msvc_compiler(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in\n_get_msvc_compiler\nfind_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(<2 more\narguments>)\ntype 'NoneType' has no method replace()\nINFO: Elapsed time: 29.214s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (0 packages loaded)\nFAILED: Build did NOT complete successfully (0 packages loaded)\n\n-----------------------------------------------\n\nOn Tue, Nov 19, 2019 at 12:56 PM L\u00e1szl\u00f3 Csomor <notifications@github.com>\nwrote:\n\n> Try this in a normal cmd.exe (not in the VS 2019 command prompt):\n>\n> set BAZEL_VS=\n>\n> set BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\n>\n> set BAZEL_VC_FULL_VERSION=14.23.2810\n>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33544?email_source=notifications&email_token=AC7AFFPVJVQM4XL5RFSUSC3QUPH5FA5CNFSM4JCUCUT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEEN5SKI#issuecomment-555473193>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFJO2PJPMGXTWHT4GRDQUPH5FANCNFSM4JCUCUTQ>\n> .\n>\n", "Try this:\r\n```\r\nbazel shutdown\r\nset CC_CONFIGURE_DEBUG=1\r\n```\r\nthen run Bazel again. This will print debug info, please paste that here.", "`ERROR: no such package '@local_config_cuda//cuda'`\r\nThis suggests you may not have run the configure command before build.\r\nCould you confirm you have performed this step?\r\nhttps://www.tensorflow.org/install/source_windows#configure_the_build", "Yes I did. I confirm.\n\n\u0441\u0440, 20 \u041d\u043e\u044f 2019 \u0433., 1:03 Gunhan Gulsoy <notifications@github.com>:\n\n> ERROR: no such package '@local_config_cuda//cuda'\n> This suggests you may not have run the configure command before build.\n> Could you confirm you have performed this step?\n> https://www.tensorflow.org/install/source_windows#configure_the_build\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33544?email_source=notifications&email_token=AC7AFFJVWAGOZITWRCD4QYTQUR5GPA5CNFSM4JCUCUT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEEQGKMY#issuecomment-555771187>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFLABUC6MOFVFKVKWALQUR5GPANCNFSM4JCUCUTQ>\n> .\n>\n", "My only remaining explanation is, maybe you need to run `bazel clean --expunge`\r\nOther than that, I cannot reproduce the problem as everything seems to be working fine for me with either visual studio 2017 or 2019 (on different VMs)", "After running `bazel clean --expunge`, you need to run `configure` again, and then run bazel build command you are using.\r\n\r\nFinally, with CUDA compute capability 3.0, we are not testing. so after going through all this trouble TF may crash.", " I did the following\n\nbazel --output_user_root user_root --output_base base shutdown\nbazel --output_user_root user_root --output_base base clean --expunge\nset CC_CONFIGURE_DEBUG=1\nset BAZEL_VS=\nset BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual\nStudio\\2019\\Enterprise\\VC\nset BAZEL_VC_FULL_VERSION=14.23.2810\nset BAZEL_SH=C:\\msys64\\usr\\bin\\bash.exe\nconda activate tf2\npython configure.py\n\nbazel --output_user_root ..\\user_root --output_base ..\\base build\n--config=opt --config=mkl --config=v2\n//tensorflow/tools/pip_package:build_pip_package > ..\\output.txt 2>&1\n\n\nand always ERROR: An error occurred during the fetch of repository\n'local_config_cuda':\n\nwhat I do wrong ?\n\n----------------------------------------------------------------\nStarting local Bazel server and connecting to it...\nWARNING: The following configs were expanded more than once: [v2]. For\nrepeatable flags, repeats are counted twice and may lead to unexpected\nbehavior.\nINFO: Writing tracer profile to 'F:/tf2/base/command.profile.gz'\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\nINFO: Options provided by the client:\n  'build' options: --python_path=c:/anaconda/envs/tf2/python.exe\nINFO: Reading rc options for 'build' from f:\\tf2\\tensorflow\\.bazelrc:\n  'build' options: --apple_platform_type=macos --define\nframework_shared_object=true --define open_source_build=true\n--define=use_fast_cpp_protos=true --define=allow_oversize_protos=true\n--spawn_strategy=standalone --strategy=Genrule=standalone -c opt\n--announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr\n--define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\n--config=v2\nINFO: Reading rc options for 'build' from\nf:\\tf2\\tensorflow\\.tf_configure.bazelrc:\n  'build' options: --action_env\nPYTHON_BIN_PATH=c:/anaconda/envs/tf2/python.exe --action_env\nPYTHON_LIB_PATH=c:/anaconda/envs/tf2/lib/site-packages\n--python_path=c:/anaconda/envs/tf2/python.exe --action_env\nCUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\n--action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --config=cuda --config\nmonolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN\n--host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI\n--verbose_failures --distinct_host_configuration=false\n--define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\nINFO: Found applicable config definition build:v2 in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2\nINFO: Found applicable config definition build:cuda in file\nf:\\tf2\\tensorflow\\.bazelrc: --config=using_cuda\n--define=using_cuda_nvcc=true\nINFO: Found applicable config definition build:using_cuda in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env\nTF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\nINFO: Found applicable config definition build:monolithic in file\nf:\\tf2\\tensorflow\\.bazelrc: --define framework_shared_object=false\nINFO: Found applicable config definition build:opt in file\nf:\\tf2\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define\nwith_default_optimizations=true\nINFO: Found applicable config definition build:mkl in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=build_with_mkl=true\n--define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c\nopt\nINFO: Found applicable config definition build:v2 in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2\nLoading:\nLoading: 0 packages loaded\nLoading: 0 packages loaded\nLoading: 0 packages loaded\n    currently loading: tensorflow/tools/pip_package\nLoading: 0 packages loaded\n    currently loading: tensorflow/tools/pip_package\nINFO: Call stack for the definition of repository 'local_config_cuda' which\nis a cuda_configure (rule definition at\nF:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl:1268:18):\n - F:/tf2/tensorflow/tensorflow/workspace.bzl:68:5\n - F:/tf2/tensorflow/WORKSPACE:19:1\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\n   Traceback (most recent call last):\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\n_create_local_cuda_repository(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in\n_create_local_cuda_repository\nfind_cc(repository_ctx)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in\nfind_cc\n_get_msvc_compiler(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in\n_get_msvc_compiler\nfind_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(<2 more\narguments>)\ntype 'NoneType' has no method replace()\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such\npackage '@local_config_cuda//cuda': Traceback (most recent call last):\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\n_create_local_cuda_repository(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in\n_create_local_cuda_repository\nfind_cc(repository_ctx)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in\nfind_cc\n_get_msvc_compiler(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in\n_get_msvc_compiler\nfind_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(<2 more\narguments>)\ntype 'NoneType' has no method replace()\nWARNING: Target pattern parsing failed.\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent\ncall last):\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\n_create_local_cuda_repository(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in\n_create_local_cuda_repository\nfind_cc(repository_ctx)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in\nfind_cc\n_get_msvc_compiler(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in\n_get_msvc_compiler\nfind_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace(<2 more\narguments>)\ntype 'NoneType' has no method replace()\nINFO: Elapsed time: 12.219s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (0 packages loaded)\nFAILED: Build did NOT complete successfully (0 packages loaded)\n\n----------------------------------------------------------------------------\n\n\n\n\nOn Wed, Nov 20, 2019 at 1:52 AM Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> After running bazel clean --expunge, you need to run configure again, and\n> then run bazel build command you are using.\n>\n> Finally, with CUDA compute capability 3.0, we are not testing. so after\n> going through all this trouble TF may crash.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33544?email_source=notifications&email_token=AC7AFFOUUN5G6KZ7WBMMQDDQUSC47A5CNFSM4JCUCUT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEEQJI7Y#issuecomment-555783295>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFJM4YKLVXXVXE7TEMLQUSC47ANCNFSM4JCUCUTQ>\n> .\n>\n", "In my case, I never need to set the following:\r\n```\r\nset BAZEL_VS=\r\nset BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual\r\nStudio\\2019\\Enterprise\\VC\r\nset BAZEL_VC_FULL_VERSION=14.23.2810\r\n```\r\n\r\nMoreover, Visual Studio 2019 version numbers will be of the form `16.x.x` Your VC full version that starts with 14 is inconsistent with the VC path.\r\n\r\nCould you try not setting any of that, and also share your full configure script output?", "I've the both , v14.23.28105 too.\nin my previous message I showed that without this settings it can not find\na good version of VS c++ tools.\nI'll do output of configure script too.\n\nOn Wed, Nov 20, 2019 at 6:34 PM Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> In my case, I never need to set the following:\n>\n> set BAZEL_VS=\n> set BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual\n> Studio\\2019\\Enterprise\\VC\n> set BAZEL_VC_FULL_VERSION=14.23.2810\n>\n> Moreover, Visual Studio 2019 version numbers will be of the form 16.x.x\n> Your VC full version that starts with 14 is inconsistent with the VC path.\n>\n> Could you try not setting any of that, and also share your full configure\n> script output?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33544?email_source=notifications&email_token=AC7AFFMO4PDSTIP4EG6P2OTQUVYI5A5CNFSM4JCUCUT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEETCKVA#issuecomment-556148052>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFPJWPWSDMXEJTG22CTQUVYI5ANCNFSM4JCUCUTQ>\n> .\n>\n", "That may be the reason bazel gets confused.\r\nYou have two toolchains installed, and you are using two different environment variables to tell bazel to use two different visual studio compilers. You use BAZEL_VC to tell it to look for 2019, but the BAZEL_VC_FULL_VERSION tells it to look for 2015.\r\n\r\nPlease try not setting either of those, in a new terminal, bazel clean, configure, and try bazel build.", "I did\nbut now the both errors cuda and vs detection problems\n\n[image: image.png]\n\n\n---------------------------------------------------------\nExtracting Bazel installation...\nStarting local Bazel server and connecting to it...\nWARNING: The following configs were expanded more than once: [v2]. For\nrepeatable flags, repeats are counted twice and may lead to unexpected\nbehavior.\nINFO: Writing tracer profile to 'F:/tf2/base/command.profile.gz'\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\nINFO: Options provided by the client:\n  'build' options: --python_path=c:/anaconda/envs/tf2/python.exe\nINFO: Reading rc options for 'build' from f:\\tf2\\tensorflow\\.bazelrc:\n  'build' options: --apple_platform_type=macos --define\nframework_shared_object=true --define open_source_build=true\n--define=use_fast_cpp_protos=true --define=allow_oversize_protos=true\n--spawn_strategy=standalone --strategy=Genrule=standalone -c opt\n--announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr\n--define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\n--config=v2\nINFO: Reading rc options for 'build' from\nf:\\tf2\\tensorflow\\.tf_configure.bazelrc:\n  'build' options: --action_env\nPYTHON_BIN_PATH=c:/anaconda/envs/tf2/python.exe --action_env\nPYTHON_LIB_PATH=c:/anaconda/envs/tf2/lib/site-packages\n--python_path=c:/anaconda/envs/tf2/python.exe --action_env\nCUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\n--action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --config=cuda --config\nmonolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN\n--host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI\n--verbose_failures --distinct_host_configuration=false\n--define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\nINFO: Found applicable config definition build:v2 in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2\nINFO: Found applicable config definition build:cuda in file\nf:\\tf2\\tensorflow\\.bazelrc: --config=using_cuda\n--define=using_cuda_nvcc=true\nINFO: Found applicable config definition build:using_cuda in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env\nTF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\nINFO: Found applicable config definition build:monolithic in file\nf:\\tf2\\tensorflow\\.bazelrc: --define framework_shared_object=false\nINFO: Found applicable config definition build:opt in file\nf:\\tf2\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define\nwith_default_optimizations=true\nINFO: Found applicable config definition build:mkl in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=build_with_mkl=true\n--define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 -c\nopt\nINFO: Found applicable config definition build:v2 in file\nf:\\tf2\\tensorflow\\.bazelrc: --define=tf_api_version=2\nLoading:\nLoading: 0 packages loaded\nLoading: 0 packages loaded\nLoading: 0 packages loaded\nLoading: 0 packages loaded\nLoading: 0 packages loaded\nLoading: 0 packages loaded\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible\nform can be obtained by modifying arguments shallow_since = \"1556410077\n-0400\"\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker'\nwhich is a git_repository (rule definition at\nF:/tf2/base/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\n - F:/tf2/base/external/bazel_toolchains/repositories/repositories.bzl:37:9\n - F:/tf2/tensorflow/WORKSPACE:35:1\nLoading: 0 packages loaded\nINFO: Call stack for the definition of repository 'local_config_cuda' which\nis a cuda_configure (rule definition at\nF:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl:1268:18):\n - F:/tf2/tensorflow/tensorflow/workspace.bzl:68:5\n - F:/tf2/tensorflow/WORKSPACE:19:1\nLoading: 0 packages loaded\n    currently loading: tensorflow/tools/pip_package\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\n   Traceback (most recent call last):\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\n_create_local_cuda_repository(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in\n_create_local_cuda_repository\nfind_cc(repository_ctx)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in\nfind_cc\n_get_msvc_compiler(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in\n_get_msvc_compiler\nfind_msvc_tool(repository_ctx, <2 more arguments>)\nFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\",\nline 367, in find_msvc_tool\n_get_vc_full_version(<2 more arguments>)\nFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\",\nline 357, in _get_vc_full_version\n_get_latest_subversion(<2 more arguments>)\nFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\",\nline 336, in _get_latest_subversion\nrepository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\nC:/Program Files (x86)/Microsoft Visual\nStudio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such\npackage '@local_config_cuda//cuda': Traceback (most recent call last):\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\n_create_local_cuda_repository(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in\n_create_local_cuda_repository\nfind_cc(repository_ctx)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in\nfind_cc\n_get_msvc_compiler(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in\n_get_msvc_compiler\nfind_msvc_tool(repository_ctx, <2 more arguments>)\nFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\",\nline 367, in find_msvc_tool\n_get_vc_full_version(<2 more arguments>)\nFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\",\nline 357, in _get_vc_full_version\n_get_latest_subversion(<2 more arguments>)\nFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\",\nline 336, in _get_latest_subversion\nrepository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\nC:/Program Files (x86)/Microsoft Visual\nStudio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\nWARNING: Target pattern parsing failed.\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent\ncall last):\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\n_create_local_cuda_repository(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1102, in\n_create_local_cuda_repository\nfind_cc(repository_ctx)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 226, in\nfind_cc\n_get_msvc_compiler(<1 more arguments>)\nFile \"F:/tf2/tensorflow/third_party/gpus/cuda_configure.bzl\", line 144, in\n_get_msvc_compiler\nfind_msvc_tool(repository_ctx, <2 more arguments>)\nFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\",\nline 367, in find_msvc_tool\n_get_vc_full_version(<2 more arguments>)\nFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\",\nline 357, in _get_vc_full_version\n_get_latest_subversion(<2 more arguments>)\nFile \"F:/tf2/base/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\",\nline 336, in _get_latest_subversion\nrepository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\nC:/Program Files (x86)/Microsoft Visual\nStudio/2017/Enterprise/VC/Tools/MSVC (No such file or directory)\nINFO: Elapsed time: 20.139s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (0 packages loaded)\nFAILED: Build did NOT complete successfully (0 packages loaded)\n----------------------------------------------------------\n\nOn Wed, Nov 20, 2019 at 6:49 PM Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> That may be the reason bazel gets confused.\n> You have two toolchains installed, and you are using two different\n> environment variables to tell bazel to use two different visual studio\n> compilers. You use BAZEL_VC to tell it to look for 2019, but the\n> BAZEL_VC_FULL_VERSION tells it to look for 2015.\n>\n> Please try not setting either of those, in a new terminal, bazel clean,\n> configure, and try bazel build.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33544?email_source=notifications&email_token=AC7AFFM7XNHI4OK3U62QWXDQUV2DDA5CNFSM4JCUCUT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEETGZKA#issuecomment-556166312>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFI6PVIB25DJ4JELDKTQUV2DDANCNFSM4JCUCUTQ>\n> .\n>\n", "@VitaMusic Knowing how magical path resolutions can be, I'd recommend to give it a go with a few predefined env vars that I have used to make sure bazel build picks the correct envs for tensorflow build. This way I can have multiple VS and CUDA versions installed in parallel (Win10).\r\n\r\nAdjust the following vars to match your env and try again with TF 2.1. Also ensure your `C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\{BAZEL_VC_FULL_VERSION}\\bin\\Hostx64\\x64` path contains `cl.exe`. If not, you'll get cryptic repository loading failures.\r\n\r\n```\r\nSET BAZEL_VC_FULL_VERSION=14.24.28314\r\nSET TF_VC_VERSION=16.4\r\nSET TF_CUDA_COMPUTE_CAPABILITIES=3.0\r\nSET TF_NEED_CUDA=1\r\nSET BAZEL_VC=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\r\nSET BAZEL_VS=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\r\nSET CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\nSET CUDA_TOOLKIT_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\r\n```\r\nOptimization flags\r\n/arch:AVX2\r\n\r\n```\r\nbazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nbazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_cuda_10_1\r\n```", "All this because the binaries are not compiled to support 3.0?  Since the code supports 3.0 is there any reason NOT to compile the binaries with 3.0 support?   Seems insane to make anyone go through all this before they can be productive....\r\n\r\nIn my case, I have an old iMac I'd like to repurpose and it's only after all the pain, time & energy spent installing Linux, TF, GPU drivers  and all the associated cruft that I found out the binaries don't support 3.0 so I have to spend days recompiling things just to experiment with TF+GPU...  Like I said, seems insane.", "Is this still being followed?\r\nI dont have compute power support for a month now and was hoping i could do the testing and development on my private laptop, just to find out that its GPU is not supported. It is quite a elaborate project right now and i am not keen on going back to older tf versions and i am currently struggling with the compiling on my own part. I would like to second @ckmaresca reply just above and ask why this hard limit was set at the first place?", "@schmidtijoe One of the likely reasons for limiting the set of built-in compute capabilities is the final bundle size.\r\n\r\nI don't remember the exact figures, but it is pretty significant.\r\n\r\nThe compiling process in itself is somewhat time-consuming, but depending on the TF and CUDA version combination, might not be that complicated.", "Hi @VitaMusic!\r\nWe are checking to see whether you still need help in this issue . Have you tried Latest Stable version TF 2.6 yet? Attaching relevant threads for reference, [link1](https://medium.com/analytics-vidhya/step-by-step-guide-to-setup-gpu-with-tensorflow-on-windows-laptop-c84634f59857),[link2](https://medium.com/analytics-vidhya/tensorflow-gpu-how-to-install-tensorflow-with-nvidia-cuda-cudnn-and-gpu-support-on-windows-6158cffc1c29),[link3 ](https://www.tensorflow.org/install/source_windows) . Thanks!", "hi, no I did not tried this version yet, is it by default compatible with\nold compute capacities ? if yes , I'll be happy.\n\nOn Tue, Oct 26, 2021 at 11:03 AM mohantym ***@***.***> wrote:\n\n> Hi @VitaMusic <https://github.com/VitaMusic>!\n> We are checking to see whether you still need help in this issue . Have\n> you tried Latest Stable version TF 2.6 yet? Attaching relevant threads for\n> reference, link1\n> <https://medium.com/analytics-vidhya/step-by-step-guide-to-setup-gpu-with-tensorflow-on-windows-laptop-c84634f59857>\n> ,link2\n> <https://medium.com/analytics-vidhya/tensorflow-gpu-how-to-install-tensorflow-with-nvidia-cuda-cudnn-and-gpu-support-on-windows-6158cffc1c29>\n> ,link3 <https://www.tensorflow.org/install/source_windows> . Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33544#issuecomment-951732498>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFJYEVERJQM7I6RZWMTUIZVEZANCNFSM4JCUCUTQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "@VitaMusic ! Could you check these threads for confirmations on default compute capability.[ Link1](https://stackoverflow.com/a/50995856/11530462),[Link2 ](https://www.tensorflow.org/install/gpu#hardware_requirements). Thank you!", "I checked, my problem was to run the latest TF on windows 7 with geforce\ngtx 870m\nwhich has CC==3.0, it was not possible as the minimum req was 3.5\nas I see on the links the situation is the same for the latest TF versions.\n\nOn Thu, Oct 28, 2021 at 2:02 PM mohantym ***@***.***> wrote:\n\n> @VitaMusic <https://github.com/VitaMusic> ! Could you check these threads\n> for confirmations on default compute capability. Link1\n> <https://stackoverflow.com/a/50995856/11530462>,Link2\n> <https://www.tensorflow.org/install/gpu#hardware_requirements>. Thank you!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33544#issuecomment-953778164>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AC7AFFPNARCWITJNJD4HBULUJE3UTANCNFSM4JCUCUTQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "@VitaMusic,\r\nYou need to add the CUDA compatibility version in `gpu_device.cc` file.\r\n```\r\nstd::vector<se::CudaComputeCapability> GetSupportedCudaComputeCapabilities() {\r\n  std::vector<se::CudaComputeCapability> cuda_caps = {\r\nComputeCapabilityFromString(\"3.0\"), ComputeCapabilityFromString(\"3.5\"), `ComputeCapabilityFromString(\"5.2\")};\r\n```\r\nDo changes in this [file](https://github.com/tensorflow/tensorflow/blob/r2.6/tensorflow/core/common_runtime/gpu/gpu_device.cc#L1717). Thanks!"]}, {"number": 33464, "title": "TFLite-micro: AllocateTensors produces HardFault even for small models", "body": "I am deploying a model consisting of a GRU with 128 input units and 64 Hidden units (total 37,056 parameters) on the following platform:\r\nNRF52832 Arm cortex M4, with 64 KB ram and 512 KB FLASH.\r\n\r\nTo build the tflite model I follow the steps below:\r\n```python \r\nconverter=tf.lite.TFLiteConverter.from_keras_model(new_gru)\r\nconverter.optimizations= [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ngruQ=converter.convert()\r\nf=open('gruQ.tflite',\"wb\").write(gruQ)\r\n```\r\nand I convert it to a c++ array with\r\n`xxd -i gruQ.tflite > gruQ.h`\r\n\r\nOn the embedded platform:\r\n```cpp\r\nstatic tflite::MicroErrorReporter micro_error_reporter;\r\n      error_reporter = &micro_error_reporter;\r\n\r\n   //extern unsigned char* quantQ5_tflite;\r\n   const tflite::Model* model = ::tflite::GetModel(gruQ_tflite);\r\n\t\r\n   if (model->version() != TFLITE_SCHEMA_VERSION) {\r\n     error_reporter->Report(\r\n         \"Model provided is schema version %d not equal \"\r\n         \"to supported version %d.\\n\",\r\n         model->version(), TFLITE_SCHEMA_VERSION);\r\n   }\r\n\r\n   // This pulls in all the operation implementations we need\r\n   tflite::ops::micro::AllOpsResolver resolver;\r\n\r\n   static tflite::MicroInterpreter static_interpreter(\r\n         model, resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n     interpreter = &static_interpreter;\r\n\r\n     TfLiteStatus allocate_status = interpreter->AllocateTensors();\r\n```\r\n\r\nThe last line of code generates a HardFault regardless the size of the TensorArena, which I also tried to set to the maximum allowed by my system (45*1024). More into the details, the function call that produces it is:\r\n`if (auto* array = buffer->data()) {`\r\nin\r\n`tensorflow/lite/experimental/micro/micro_allocator.cc    Line 259`\r\nThe size of the converted model (flatbuffer) is 43304\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7.3\r\n", "comments": ["Sorry you're hitting this issue! As a debugging step, can you try running your same code on Linux/x86? It would be helpful to know if it works there, and if it does I can suggest some further debugging steps.", "Thanks for your reply, do you mean to compile the same code using Linux/x86 as target instead of ARM cortex M4, or just to do model conversion and compile the firmware in a linux/x86 environment?\r\n\r\nI have also some updates.\r\nI tried to convert the model using tflite_convert command line tool instead of doing it with the python API. Now it allocate successfully the tensors, but it generates HardFault when executing `interpreter->Invoke();`.\r\nBy doing some debug, I found out that the critical part is in `op_resolver.cc   Line:36`, \r\n` *registration = op_resolver.FindOp(builtin_code, version);`.\r\nWhen the operation is a BuiltinOperator_MUL.\r\nElementwise multiplication is used inside a GRU during the last steps to generate the new hidden state, but I don't see any support to MUL in ` lite/experimental/micro/kernels/all_ops_resolver.cc` . \r\nThe consequence is that when FindOp is called, in `micro_mutable_ops_resolver.cc    Line:22` the for loop keeps going until an invalid index for `registrations_[i]` is reached, thus causing HardFault.\r\nIs my guess correct? Is there any possible workaround?", "I did notice in your code snippet that the OpResolver isn't declared static, like the interpreter is. I'm not sure what the rest of your code looks like, but if the OpResolver object has a shorter lifetime than the interpreter then you could end up with mysterious crashes like this. If you look in the examples, you can see we declare resolvers as static in the setup() function:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/hello_world/main_functions.cc#L63", "@petewarden, I fixed OpResolver declaring it as static, but the problem remains.\r\n\r\nI guess the only solution is to implement in TFLite micro the elementwise multiplication (BuiltinOperator_MUL) which is used inside (among others) GRUs  \r\nShall I open a new issue/feature request and close this one?", "operation MUL is now integrated with TFLite Micro. However, GRUs cannot still be deployed due to missing operator SUB.\r\n\r\nI take the chance for a related questions. Is there a way to enforce the converter to generate tflite ops of a specific version? For example, I noticed that after conversion my model contains AVERAGE_POOL_2D operations in version 2. Despite the declared compatibility in all_ops_resolver is only for version 1, by enforcing it to accept also version 2 (by just manually modifying the resolver), the code executes without problems. I am now wondering if it is possible to tell the converter to generate only versions 1 of that operation.\r\nAlso, is there a document that clearly shows differences between versions of the same operation?", "I notice you're using OPTIMIZE_FOR_SIZE in your converter optimizations.  I am wondering if you are inadvertently creating a hybrid quantized model (which we do not support on Micro).\r\n\r\nCan you try uploading your TFLite model to https://lutzroeder.github.io/netron/ to check if both the weight tensors and activation tensors are quantized?  If only the weights are quantized, you likely have a hybrid model, and will need to either disable the OPTIMIZE_FOR_SIZE flag to get a float model, or add input and output types along with a representative dataset.\r\n\r\n"]}, {"number": 33463, "title": "Undefined symbols for architecture arm64:   \"NewGpuDelegate(GpuDelegateOptions const*)", "body": "System information\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone X.s, 13.1.3\r\n    TensorFlow installed from (source or binary): source\r\n    TensorFlow version (use command below): master branch cloned on 2019-10-8\r\n    Python version: 3.7.4\r\n    Bazel version (if compiling from source):  0.26.1\r\n    GCC/Compiler version (if compiling from source): NA\r\n    CUDA/cuDNN version: NA\r\n    GPU model and memory: NA\r\n\r\nDescribe the current behavior:\r\nIt fails to build the library in Xcode.\r\nUndefined symbols for architecture arm64:   \"NewGpuDelegate(GpuDelegateOptions const*)\r\n\r\nDescribe the expected behavior:\r\nI want to build my C++ library with the ability to run it with GPU in an iOS app\r\n\r\nCode to reproduce the issue\r\n\r\nOther info / logs\r\nI changed the extension of the file having this line of code: \r\n_delegate = NewGpuDelegate(&options);_\r\nfrom cpp to mm, as otherwise I couldn't include header files for metal(which are written in obj-c).\r\nI tested the tensorflow ios/camera example and it builds and runs fine. But using the same .framework lib gives me the same undefined symbol error.\r\n\r\nBazel Command for building the tensorflow lib:\r\n**bazel build -c opt --cpu ios_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=hidden --linkopt -s --strip always --cxxopt=-std=c++14 :tensorflow_lite_gpu_framework --apple_platform_type=ios**\r\n\r\nAny help would be appreciated!", "comments": ["same problem"]}, {"number": 33336, "title": "tf.sparse.sparse_dense_matmul can't broadcast as tf.linalg.matmul does.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\ntf.sparse.sparse_dense_matmul can't broadcast on the leading dimensions. the following code is an example.\r\n\r\n```python\r\n#!/usr/bin/python3\r\n\r\nimport numpy as np;\r\nimport tensorflow as tf;\r\n\r\ndef main():\r\n\r\n    a = tf.sparse.SparseTensor(\r\n        indices = [[0, 0, 1], [0, 0, 2], [0, 1, 2], [0, 1, 3], [0, 2, 1], [0, 2, 3]],\r\n        values = [1., 1., 1., 1., 1., 1.],\r\n        dense_shape = [1, 3, 4]\r\n    ); # a.shape = (1,3,4)\r\n    b = tf.constant(np.random.normal(size = (4, 4, 5)), dtype = tf.float32);\r\n    c = tf.linalg.matmul(tf.sparse.to_dense(a),b); # will be succeed\r\n    c = tf.sparse.sparse_dense_matmul(a,b); # will be fail\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    main();\r\n```\r\n\r\n**Will this change the current api? How?**\r\n\r\nno api need to be changed.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nuser handling sparse matrices will be benefit from the change.\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi, I would like to handle this and add the feature. Please let me know if no one is working on it.", "@familyguy12 Thanks for coming forward to support the development. Please feel free to contribute through PR. Thanks!", "@jvishnuvardhan It seems that this method is working as mentioned in the [documentation](https://www.tensorflow.org/api_docs/python/tf/sparse/sparse_dense_matmul). If I understand correctly, I am supposed to modify the way sparse matrix multiplication works so that it handles the case when the first arg is of rank != 2. Is that right?", "the tensorflow's traditional matmul does matrix production on the last two dimensions of the two inputs. if the leading dimensions of the two inputs are not equal, matmul may broadcast the small one to the large one if plausible. ", "In the code mentioned, `tf.rank(a).eval()` is 3. The documentation says that the sparse multiplication works only for rank-2 tensors. Even in the traditional matmul function, it says that\r\n\r\n> If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding a_is_sparse or b_is_sparse flag to True. These are False by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes bfloat16 or float32.\r\n\r\nSo, basically it is not supposed to work for matrices that are sparse, with rank > 2. IMO, it could work as the multiplication is performed on the last two dimensions only. So, I should handle the case where rank>2 in `sparse_matmul` and perform multiplication only on the last two dimensions, instead of giving a `ValueError`, right?\r\nThis is my first contribution to the repository and I apologise for too many questions.", "the tf.sparse.sparse_dense_matmul can only handle dot product of two 2-rank tensors. the function is too constrained. I would appreciate if dot product between higher rank tensors are supported. besides, I would like to have auto broadcast of sparse_dense_matmul supported. Thx.", "if sparse_dense_matmul can't support dot product between tensors over rank 2. please add a tf.sparse.map_fn which support tensor slice over sparse tensor. Or some operations are impossible.", "please support matmul for sparse tensor. pytorch has deepspeed which can handle sparse tensor matmul having rank over 2. implementing sparse attention with tensorflow 2 is impossible without such feature.", "@familyguy12 any progress?", "@familyguy12 any progress?"]}, {"number": 33312, "title": "Fix discrepancy between tf.io.gfile.mkdir and os.mkdir's created mode", "body": "This fix tries to address the issue raised in #32963 where the modes of the directory created between tf.io.gfile.mkdir and os.mkdir are different (one is 0777 and another is 0755).\r\n\r\nThis fix fixes the issue.\r\n\r\nThis fix fixes #32963.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["This test fails internally\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \".../absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \".../absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \".../tensorflow/python/lib/io/file_io_test.py\", line 634, in testCreateDirMode\r\n    self.assertEqual(tf_mode, os_mode)\r\nAssertionError: 17407 != 16895\r\n```", "I will have a fix on the internal infrastructure and then revisit this and get it merged.", "Thanks @mihaimaruseac \ud83d\udc4d for the update , let me know if there is anything I could help.", "@mihaimaruseac Any update on this PR, please. ", "(Posted wrong update here, confused with another PR)\r\n\r\nCurrently working on modular filesystem and once the support for POSIX is complete I can move to the internal changes.", "@mihaimaruseac Any update on this PR, please. Thanks!", "Unfortunately the differences between Google and external POSIX filesystem are harded than anticipated, so this will have to wait until after modular filesystems land :(", "@yongtang Can you please resolve conflicts? Thanks!", "Well, it's too early to fix conflicts as we cannot yet merge this due to large differences between internal and external file system :-/", "Thanks @mihaimaruseac @gbaned, let me know when it is ready to move forward.", "@mihaimaruseac Any update on this PR, please. Thanks!", "> Unfortunately the differences between Google and external POSIX filesystem are harded than anticipated, so this will have to wait until after modular filesystems land :(\r\n\r\nThis is still the case unfortunately.", "@mihaimaruseac Any update on this PR, please. Thanks!", "Same as before unfortunately.", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!"]}, {"number": 33131, "title": "how to assign value to a EagerTensor slice? ----'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment", "body": "as in numpy or pytorch ,we can do someting like this, but how to do it with tf2.0.\r\nthe following code will raise exception as :\r\n\r\n`'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment`\r\nprediction[:,:,0]=tf.math.sigmoid(prediction[:,:,0])\r\n", "comments": ["@aohan237 ,\r\nThanks for reporting the issue \r\nCan you share a simple and standalone code to reproduce the issue?", "@oanush \r\nsure.\r\n```\r\nimport tensorflow as tf\r\nbb=tf.ones([3,3,3])\r\nbb[:,:,1]=bb[:,:,1]*2\r\n```\r\n\r\nmy solution is to change tensor to variable,then finish the ops, convert to tensor.\r\nbut it stucks in the process of changing tensor to variable, when in real model ", "Could reproduce the error with TF Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/e3131ae96e71d1b1ba7f9e325ce23117/33131.ipynb).", "@aohan237  TensorFlow tensor object is not **assignable**, so you cannot use it on the left-hand side of an assignment. You can refer to the following [comment](https://github.com/tensorflow/tensorflow/issues/14132#issuecomment-483002522) and it should help you in solving your problem.", "@gowthamkpr  thanks. \r\nthis is not that pythonic.   \r\ni just convert the tf.Tensor to tf.Varible and then back to tf.Tensor  to make this happen.  is it correct?", "Yes @aohan237 Can you please close this issue as it has been resolved. thanks!", "sure\uff5ethanks for your help", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33131\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33131\">No</a>\n", "This is a dramatic flaw of the framework that you can't do item assignment", "@aliutkus \r\nsure.  go to pytorch, everythin goes fine", "yeah, I know... but can't for some reason", "PyTorch supports this, and TF needs it.", "still no support for this?", "still no support for this ?", "no it's not\r\nyou need to use `tensorflow.tensor_scatter_nd_update`, it's so much cooler", "For lack of a good solution on this (I had posted an [SO question about this topic](https://stackoverflow.com/questions/62092147/how-to-efficiently-assign-to-a-slice-of-a-tensor-in-tensorflow)), I decided to create my own module for tensor slice assignment using `tensor_scatter_nd_update`. I found it useful to have a separate module for this given how using `tensor_scatter_nd_update` can be such a pain at times with all the meshgrid and reshaping going on.\r\n\r\nYou can find it [here](https://github.com/zaccharieramzi/tf-slice-assign) and install it with `pip install tf-slice-assign`.\r\n\r\nIf you try it out don't hesitate to give me feedback on it (PRs or Issues welcome). Ideally once it's well tested and benchmarked it could go directly in the TensorFlow core (if not too slow) so noone would have to use a separate module for this which would be ridiculous in the long run.", "I really think this should be addressed.", "You can do this: \r\n\r\n```\r\nimport tensorflow as tf \r\nimport numpy as np\r\n\r\ninput_shape = (1,100,4)\r\n\r\nx = np.zeros(input_shape)\r\nx[:,0] = np.array([1,2,3,4])\r\n\r\nx = tf.Variable(x)\r\nprint(x)\r\n```\r\n```\r\n<tf.Variable 'Variable:0' shape=(1, 100, 4) dtype=float64, numpy=\r\narray([[[1., 2., 3., 4.],\r\n        [0., 0., 0., 0.],\r\n....................\r\n....................\r\n....................\r\n```", "@MinaGabriel this is a fairly good solution in some situations but sometimes you just need to assign to a tensor not a variable, hence why people are requesting this feature.", "> no it's not\r\n> you need to use `tensorflow.tensor_scatter_nd_update`, it's so much cooler\r\n\r\n@aliutkus  I haven't used `tf.tensor_scatter_nd_update` and currently I'm trying to figure it out but I'm running into some errors which I'm unable to resolve. How would this `pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio`  in numpy look like using `tf.tensor_scatter_nd_update`?", "Here's a small snippet that I used to get around with the problem. It's bad I know\r\n\r\n```python\r\nqueue = tf.Variable(queue)\r\nqueue[i, bs:].assign(queue[i, :-bs])\r\nqueue[i, :bs].assign(projection[crop_id * bs: (crop_id + 1) * bs])\r\nqueue = tf.convert_to_tensor(queue)\r\n```\r\n\r\nIt's exactly the snippet I had used so there are some rough edges. ", "Convert it to a numpy array and then you can do whatever you want, like this:\r\n```\r\nx = x.numpy()\r\nx[0][0]  = 1.\r\nmodel.fit(x, y)\r\n```\r\nBUT, do you really need this?", "I've found that using [TensorArray](https://www.tensorflow.org/api_docs/python/tf/TensorArray) is a good way around this.\r\nThere is a good example in the TF docs here about [accumulating_values_in_a_loop](https://www.tensorflow.org/guide/function#accumulating_values_in_a_loop)\r\n\r\nbasically something like this\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function()\r\ndef loop():\r\n    max_len = 10\r\n    dtype = tf.float32\r\n    samples = tf.TensorArray(dtype=dtype, size=max_len, clear_after_read=False)\r\n    for i in tf.range(max_len):\r\n        sample = tf.cast(i**2, dtype=dtype)\r\n        samples = samples.write(i, sample)\r\n    return samples.stack()\r\n\r\nprint(loop())\r\n# tf.Tensor([ 0.  1.  4.  9. 16. 25. 36. 49. 64. 81.], shape=(10,), dtype=float32)\r\n```\r\n", "Not solved, TensorFlow's development sucks!", "> Not solved, TensorFlow's development sucks!\n\nSo choose pytorch", "> I've found that using [TensorArray](https://www.tensorflow.org/api_docs/python/tf/TensorArray) is a good way around this.\r\n> There is a good example in the TF docs here about [accumulating_values_in_a_loop](https://www.tensorflow.org/guide/function#accumulating_values_in_a_loop)\r\n> \r\n> basically something like this\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> \r\n> @tf.function()\r\n> def loop():\r\n>     max_len = 10\r\n>     dtype = tf.float32\r\n>     samples = tf.TensorArray(dtype=dtype, size=max_len, clear_after_read=False)\r\n>     for i in tf.range(max_len):\r\n>         sample = tf.cast(i**2, dtype=dtype)\r\n>         samples = samples.write(i, sample)\r\n>     return samples.stack()\r\n> \r\n> print(loop())\r\n> # tf.Tensor([ 0.  1.  4.  9. 16. 25. 36. 49. 64. 81.], shape=(10,), dtype=float32)\r\n> ```\r\n\r\nThe documentation states that `TensorArray`s:\r\n\r\n> Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.\r\n\r\nThey are a write-once structure, a very specific use for index assignment... ", "Can't believe this is still an issue at 2021.", "Why doesn't tensorflow support assignment of eagertensor just like pytorch? ", "Tensor in TensorFlow is not mutable, assignment to EagerTensor is illegal.", "it means EagerTensor is not that Eager as announced.\r\nsupposed use tf as a static solution for now, if you want dynamic, then go to pytorch", "It's Nov. 2021 and I still have to use concatenation for tenser slice assignment in Tensorflow ", "after 2 years, i still receive some updates, so i think this issue should not be marked as closed, it should be some other status.\r\n\r\nsuch as , feature not supported/ not intended, but can not be marked as closed\r\n\r\nwhat do you thinks guys?", "We are going into 2022 and this is not addressed until now.\r\n ", "Please address. Because of this I cannot use tf.data.Dataset, since processing there must still use old TF ", "> Please address. Because of this I cannot use tf.data.Dataset, since processing there must still use old TF\r\n\r\nThe only reason why this issue still remains is from the core design of TF for being available in various programming languages or hardware platforms. Therefore, we shouldn't expect anything further. ", "This looks like an irrelevant excuse: unless we are only talking of google TPU, pytorch, which runs on all devices, accepts item assignment", "2022, still got this error ", "> Convert it to a numpy array and then you can do whatever you want, like this:\r\n\r\nPlease rethink before converting your Eager tensor into NumPy array cause I stuck in a problem where first during forward propagation i tried to convert the tensor to NumPy for some stuff, while calculating gradients i found out that **numpy functions cannot be used on tensors. If used, the computational graph does not remain connected.**\r\n\r\nhttps://stackoverflow.com/questions/62345351/gradients-returned-by-tape-gradient-is-none-in-custom-training-loop"]}, {"number": 33005, "title": "Will DepthwiseConv2D support non-square Strides in the future? Ex: `strides=(1,3)`", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): \r\n2.0.0, 1.14.0\r\n- Are you willing to contribute it (Yes/No): \r\nNo, sorry, that is well outside my expertise. \r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWill DepthwiseConv2D support non-square Strides in the future? \r\nThis would make it consistent with the majority of other layers that accept the strides argument and can take non-square strides. Ex: `strides=(1,3)`\r\n\r\nOr at minimum, please update the documentation to specify this limitation.\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D\r\n\"strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width.\"\r\n\r\n\r\ndepthwise_conv_op\r\n\"Current implementation only supports equal length strides in the row and column dimensions.\"\r\nhttps://github.com/tensorflow/tensorflow/blob/242a42139513aa5903743c37c0ef15ed00f93fed/tensorflow/core/kernels/depthwise_conv_op.cc#L288\r\n\r\n\r\n**Will this change the current api? How?**\r\nNo, there is already a strides option that should be able to take non-square shaped strides. This would actually make it more consistent with the rest of the API.\r\n\r\n\r\n**Who will benefit with this feature?**\r\nBob will.\r\n\r\n\r\n**Any Other info.**\r\nI'm using the `tf.keras.layers.DepthwiseConv2D`  implementation.\r\n\r\n2.0.0:\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation only supports equal length strides in the row and column dimensions. [Op:DepthwiseConv2dNative] name: model/depthwise_conv2d/depthwise/`\r\n\r\n1.14.0:\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation only supports equal length strides in the row and column dimensions.\r\n         [[{{node depthwise_conv2d/depthwise}}]]`\r\n\r\n\r\nSimple example:\r\n`tf.keras.layers.DepthwiseConv2D((3, 3), strides=(1,2))`", "comments": ["@Raukk Can you please mention any use case for this kind of feature? Thank!", "@jvishnuvardhan In my understanding, I think the same use cases for `Conv2D` would apply to a `DepthwiseConv2D` and `SeparableConv2D`, since they are essentially a drop in replacement for a `Conv2D` (that do different math under the hood). If you know of a reason why the `Conv2D` Use-cases would not apply to `DepthwiseConv2D` or `SeparableConv2D` then I can look for Specific Use-cases.\r\n\r\nI'll link to a set of stupid Colab Notebooks that show it working with `Conv2D` but failing if I replace any of the Conv2D layers with either `SeparableConv2D` or `DepthwiseConv2D` beacuse of the stride.\r\n\r\nIn general, I expect the layer arguments in the Keras API to be consistent across all layers that use them (except where noted by the documentation). A few examples of these arguments are `kernel_size`, `strides`, `padding`, `data_format`, and `dilation_rate` which I expect to have consistent usage, formatting, and effect. In this case, the usage and behavior of `tf.keras.layers.Conv2D(filters, kernel_size, strides=(1, 2))` to match `tf.keras.layers.SeparableConv2D(filters, kernel_size, strides=(1, 2))` and `tf.keras.layers.DepthwiseConv2D(kernel_size, strides=(1, 2))`\r\n\r\n\r\nI would totally accept that the structure of the calculations or the optimizations make it impossible to implement this functionality. At which point I would ask that it be added to the documentation that this case is not implemented. I'm sure you'd agree that it's really frustrating when something isn't implemented, but the documentation implies that it is implemented. All three layers (Conv2D, SeparableConv2D, DepthwiseConv2D) say the same wording in the online documentation:\r\n\"strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width.\"\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D\r\n\r\nColab Links:\r\nhttps://colab.research.google.com/gist/Raukk/288b0889fe4f7200937af6914b7a0fe1/stride_standardconv_example_tf2.ipynb\r\nhttps://colab.research.google.com/gist/Raukk/7f494ebbf99fafe24a187c37fc4483b4/stride_separableconv_example_tf2.ipynb\r\nhttps://colab.research.google.com/gist/Raukk/774b026b28365328d0bbb1acebd92707/stride_depthwiseconv_example_tf2.ipynb\r\nPlease let me know if you have an issue with these links.", "Yes! I agree with you. The document indicates it has been implemented already, but when I prepared everything and started training, it shows the 'not support' errors, which is frustrating.", "I would also like to see some sort of response to this.", "Still waiting for this feature to be added ;)\r\n", "Likewise, waiting for this feature to be added (hopefully for TF >= 1.14)", "Just stumbled upon this issue. I wanted to implement an EfficientNet that uses (1, 2)-strides. However, this does not seem to be possible with the current implementation of Depthwise Conv?", "The issue still persists with TF 2.5.0, there is no sign of any progress at all. Please, it would be awesome having non-squared strides, or at least clarifying the documentation @tensorflowbutler."]}, {"number": 32809, "title": "TF 2.0 Feature: Flops calculation", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.0 RC2\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI am missing the opportunity to compute the number of floating point operations of a tf.keras Model in TF 2.0. \r\nIn TF 1.x tf.profiler was available [see here](https://stackoverflow.com/questions/45085938) but I can find anything equivalent for TF 2.0 yet.\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEverbody interested in the computational complexity of a TensorFlow model.\r\n\r\n**Any Other info.**\r\n", "comments": ["Any updates?", "We're working on adding cost model for tf 2.0. Since this is a pretty large feature, it will take more time to enable it. ", "Have you made any progress yet? What do you expect when the feature will be available (in the nightly builds)?", "I concur, this would be highly useful", "This feature would be very helpful for me too.", "I am also looking for this feature", "Please, I need it too.", "Just make sure this way can still work in tf2.0:\r\n\r\nimport tensorflow as tf\r\nimport keras.backend as K\r\n \r\n \r\ndef get_flops(model):\r\n    run_meta = tf.RunMetadata()\r\n    opts = tf.profiler.ProfileOptionBuilder.float_operation()\r\n \r\n    # We use the Keras session graph in the call to the profiler.\r\n    flops = tf.profiler.profile(graph=K.get_session().graph,\r\n                                run_meta=run_meta, cmd='op', options=opts)\r\n \r\n    return flops.total_float_ops  # Prints the \"flops\" of the model.\r\n \r\n\r\nthen it's already perfect \ud83d\ude04", "@Li-markus \r\n\r\nI had to tweak it to avoid errors with TF 2.0, but I am still not able to get it working.\r\n\r\n```\r\nmodel = tf.keras.models.Sequential([\r\n      InputLayer((32, 32, 1)),\r\n      Conv2D(8, 5, padding='same', activation='relu'),\r\n      MaxPool2D(2),\r\n      Conv2D(16, 5, padding='same', activation='relu'),\r\n      MaxPool2D(2),\r\n      Flatten(),\r\n      Dense(128, activation='relu'),\r\n      Dense(64, activation='relu'),\r\n      Dense(10, activation='softmax')\r\n  ])\r\n\r\ndef get_flops(model):\r\n  tf.compat.v1.disable_eager_execution()\r\n  sess = tf.compat.v1.Session()\r\n\r\n  run_meta = tf.compat.v1.RunMetadata()\r\n  profiler = tf.compat.v1.profiler\r\n  opts = profiler.ProfileOptionBuilder.float_operation()\r\n  # We use the Keras session graph in the call to the profiler.\r\n  flops = profiler.profile(graph=sess.graph, \r\n                           run_meta=run_meta, cmd='op', options=opts)\r\n\r\n  return flops.total_float_ops  # Prints the \"flops\" of the model\r\n```\r\n\r\nThe output was:\r\n```\r\n0\r\n```", "I have a [related question](https://stackoverflow.com/questions/59460310/flops-in-tensor-flow-convolution-layer) but I don't know if this kind of questions should be posted here on Git. Could someone give an opinion on it? Wondering if that could even be a bug.", "@qiuminxu any updates on this?", "This works using TF 2.1\r\n\r\n```\r\ndef get_flops(model_h5_path):\r\n    session = tf.compat.v1.Session()\r\n    graph = tf.compat.v1.get_default_graph()\r\n        \r\n\r\n    with graph.as_default():\r\n        with session.as_default():\r\n            model = tf.keras.models.load_model(model_h5_path)\r\n\r\n            run_meta = tf.compat.v1.RunMetadata()\r\n            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n        \r\n            # We use the Keras session graph in the call to the profiler.\r\n            flops = tf.compat.v1.profiler.profile(graph=graph,\r\n                                                  run_meta=run_meta, cmd='op', options=opts)\r\n        \r\n            return flops.total_float_ops\r\n```", "Note that the above doesn't seem to work when loading from a SavedModel; only from an h5.", "I implemented small lib to calculate FLOPs/MACs: [https://github.com/evgps/flopco-keras](https://github.com/evgps/flopco-keras)", "> This works using TF 2.1\r\n> \r\n> ```\r\n> def get_flops(model_h5_path):\r\n>     session = tf.compat.v1.Session()\r\n>     graph = tf.compat.v1.get_default_graph()\r\n>         \r\n> \r\n>     with graph.as_default():\r\n>         with session.as_default():\r\n>             model = tf.keras.models.load_model(model_h5_path)\r\n> \r\n>             run_meta = tf.compat.v1.RunMetadata()\r\n>             opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n>         \r\n>             # We use the Keras session graph in the call to the profiler.\r\n>             flops = tf.compat.v1.profiler.profile(graph=graph,\r\n>                                                   run_meta=run_meta, cmd='op', options=opts)\r\n>         \r\n>             return flops.total_float_ops\r\n> ```\r\n\r\nI was able to use this code to calculate flop. However, when I call the function get_flops more than once, it seems the returned values are accumulated. For example, when I call get_flops(model_1), it returns 1000, then I call it the second time get_flops(model_1) (the same model), it would return 2000, then 3000 the third time... There must be some static variable here that is accumulated. Anyone has any idea ?", "> > This works using TF 2.1\r\n> > ```\r\n> > def get_flops(model_h5_path):\r\n> >     session = tf.compat.v1.Session()\r\n> >     graph = tf.compat.v1.get_default_graph()\r\n> >         \r\n> > \r\n> >     with graph.as_default():\r\n> >         with session.as_default():\r\n> >             model = tf.keras.models.load_model(model_h5_path)\r\n> > \r\n> >             run_meta = tf.compat.v1.RunMetadata()\r\n> >             opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n> >         \r\n> >             # We use the Keras session graph in the call to the profiler.\r\n> >             flops = tf.compat.v1.profiler.profile(graph=graph,\r\n> >                                                   run_meta=run_meta, cmd='op', options=opts)\r\n> >         \r\n> >             return flops.total_float_ops\r\n> > ```\r\n> \r\n> I was able to use this code to calculate flop. However, when I call the function get_flops more than once, it seems the returned values are accumulated. For example, when I call get_flops(model_1), it returns 1000, then I call it the second time get_flops(model_1) (the same model), it would return 2000, then 3000 the third time... There must be some static variable here that is accumulated. Anyone has any idea ?\r\n\r\n@henglicad try to reset the graph like this:\r\n\r\ntf.compat.v1.reset_default_graph()", "> > > This works using TF 2.1\r\n> > > ```\r\n> > > def get_flops(model_h5_path):\r\n> > >     session = tf.compat.v1.Session()\r\n> > >     graph = tf.compat.v1.get_default_graph()\r\n> > >         \r\n> > > \r\n> > >     with graph.as_default():\r\n> > >         with session.as_default():\r\n> > >             model = tf.keras.models.load_model(model_h5_path)\r\n> > > \r\n> > >             run_meta = tf.compat.v1.RunMetadata()\r\n> > >             opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n> > >         \r\n> > >             # We use the Keras session graph in the call to the profiler.\r\n> > >             flops = tf.compat.v1.profiler.profile(graph=graph,\r\n> > >                                                   run_meta=run_meta, cmd='op', options=opts)\r\n> > >         \r\n> > >             return flops.total_float_ops\r\n> > > ```\r\n> > \r\n> > \r\n> > I was able to use this code to calculate flop. However, when I call the function get_flops more than once, it seems the returned values are accumulated. For example, when I call get_flops(model_1), it returns 1000, then I call it the second time get_flops(model_1) (the same model), it would return 2000, then 3000 the third time... There must be some static variable here that is accumulated. Anyone has any idea ?\r\n> \r\n> @henglicad try to reset the graph like this:\r\n> \r\n> tf.compat.v1.reset_default_graph()\r\n\r\nIt works! Thanks @Habib-allah a lot!", "> This works using TF 2.1\r\n> \r\n> ```\r\n> def get_flops(model_h5_path):\r\n>     session = tf.compat.v1.Session()\r\n>     graph = tf.compat.v1.get_default_graph()\r\n>         \r\n> \r\n>     with graph.as_default():\r\n>         with session.as_default():\r\n>             model = tf.keras.models.load_model(model_h5_path)\r\n> \r\n>             run_meta = tf.compat.v1.RunMetadata()\r\n>             opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n>         \r\n>             # We use the Keras session graph in the call to the profiler.\r\n>             flops = tf.compat.v1.profiler.profile(graph=graph,\r\n>                                                   run_meta=run_meta, cmd='op', options=opts)\r\n>         \r\n>             return flops.total_float_ops\r\n> ```\r\n\r\nIt works Perfectly ", "> @Li-markus\r\n> \r\n> I had to tweak it to avoid errors with TF 2.0, but I am still not able to get it working.\r\n> \r\n> ```\r\n> model = tf.keras.models.Sequential([\r\n>       InputLayer((32, 32, 1)),\r\n>       Conv2D(8, 5, padding='same', activation='relu'),\r\n>       MaxPool2D(2),\r\n>       Conv2D(16, 5, padding='same', activation='relu'),\r\n>       MaxPool2D(2),\r\n>       Flatten(),\r\n>       Dense(128, activation='relu'),\r\n>       Dense(64, activation='relu'),\r\n>       Dense(10, activation='softmax')\r\n>   ])\r\n> \r\n> def get_flops(model):\r\n>   tf.compat.v1.disable_eager_execution()\r\n>   sess = tf.compat.v1.Session()\r\n> \r\n>   run_meta = tf.compat.v1.RunMetadata()\r\n>   profiler = tf.compat.v1.profiler\r\n>   opts = profiler.ProfileOptionBuilder.float_operation()\r\n>   # We use the Keras session graph in the call to the profiler.\r\n>   flops = profiler.profile(graph=sess.graph, \r\n>                            run_meta=run_meta, cmd='op', options=opts)\r\n> \r\n>   return flops.total_float_ops  # Prints the \"flops\" of the model\r\n> ```\r\n> \r\n> The output was:\r\n> \r\n> ```\r\n> 0\r\n> ```\r\n\r\nI'm also always get 0 flops and parameters, have you resolved the issue? ", "> This works using TF 2.1\r\n> \r\n> ```\r\n> def get_flops(model_h5_path):\r\n>     session = tf.compat.v1.Session()\r\n>     graph = tf.compat.v1.get_default_graph()\r\n>         \r\n> \r\n>     with graph.as_default():\r\n>         with session.as_default():\r\n>             model = tf.keras.models.load_model(model_h5_path)\r\n> \r\n>             run_meta = tf.compat.v1.RunMetadata()\r\n>             opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n>         \r\n>             # We use the Keras session graph in the call to the profiler.\r\n>             flops = tf.compat.v1.profiler.profile(graph=graph,\r\n>                                                   run_meta=run_meta, cmd='op', options=opts)\r\n>         \r\n>             return flops.total_float_ops\r\n> ```\r\n\r\nThis always returns flops as x2 the number of model parameters, which is incorrect. Has anyone else experienced this issue?", "None of the described methods have worked for me in anything TF 2.x; I spent an hour writing my own calculator and then moved on. I'd recommend doing the same.", "For those still struggling with this, check out [keras-flops](https://pypi.org/project/keras-flops/). I verified the number of flops against the EfficientNets and ResNets. `get_flops(model, batch_size=1)` consistently returns double the number of flops reported in the literature, so it's likely that the literature assumes [fused multiply-adds](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation) whereas this api does not.  ", "I found an easy way to deploy TF2 model as graph definition with `convert_variables_to_constants_v2_as_graph`, and then estimate the FLOPs with `tf.compat.v1.profiler.profile`. My Tensorflow version is 2.2.0.\r\n\r\n```\r\ndef get_flops(concrete_func):\r\n    import tensorflow as tf\r\n    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\r\n    \r\n    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.graph_util.import_graph_def(graph_def, name='')\r\n\r\n        run_meta = tf.compat.v1.RunMetadata()\r\n        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\r\n\r\n        return flops.total_float_ops\r\n```\r\n\r\nAnd any Keras model can be used to compute FLOPs. For example:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\ninitial_model = keras.Sequential(\r\n    [\r\n        keras.Input(shape=(10)),\r\n        layers.Dense(10)\r\n    ]\r\n)\r\n\r\nmodel = keras.Model(\r\n    inputs=initial_model.inputs,\r\n    outputs=[layer.output for layer in initial_model.layers],\r\n)\r\n\r\nconcrete = tf.function(lambda inputs: model(inputs))\r\nconcrete_func = concrete.get_concrete_function(\r\n    [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in initial_model.inputs])\r\n\r\nget_flops(concrete_func)\r\n```\r\n\r\nThe resulting FLOPs of this example is 210 (2 x 10 x 10 + 10).", "Any way to make it work with SavedModel format?", "It is 2021 now, and is there an easy way to get the FLOPs of my model in TF 2.4? Thanks.", "This function works for me in TF 2.3:\r\n\r\n```\r\ndef get_flops(model, write_path=tempfile.NamedTemporaryFile().name):\r\n    concrete = tf.function(lambda inputs: model(inputs))\r\n    concrete_func = concrete.get_concrete_function(\r\n        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\r\n    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\r\n    with tf.Graph().as_default() as graph:\r\n        tf.graph_util.import_graph_def(graph_def, name='')\r\n        run_meta = tf.compat.v1.RunMetadata()\r\n        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n        if write_path:\r\n            opts['output'] = 'file:outfile={}'.format(write_path)  # suppress output\r\n        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\r\n        return flops.total_float_ops\r\n```\r\n\r\n", "@wmcnally Thanks and I tried it, and it works perfectly in TF 2.4.\r\nI improved it a little bit, so guys can put it in a separate file and run it. The code is put below,\r\n\r\n```Python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\r\n\r\ndef get_flops(model):\r\n    concrete = tf.function(lambda inputs: model(inputs))\r\n    concrete_func = concrete.get_concrete_function(\r\n        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\r\n    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\r\n    with tf.Graph().as_default() as graph:\r\n        tf.graph_util.import_graph_def(graph_def, name='')\r\n        run_meta = tf.compat.v1.RunMetadata()\r\n        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\r\n        return flops.total_float_ops\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.Input(shape=(10 ,10 ,3)))\r\nmodel.add(tf.keras.layers.Conv2D(2, 3, activation='relu'))\r\nmodel.summary()\r\nprint(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\r\n```\r\n\r\nAnd the output in the console should be,\r\n ==================Model Analysis Report======================\r\n \r\n Doc:\r\n op: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\r\n flops: Number of float operations. Note: Please read the implementation for the math behind it.\r\n \r\n Profile:\r\n node name | # float_ops\r\n Conv2D                   6.91k float_ops (100.00%, 98.18%)\r\n BiasAdd                    128 float_ops (1.82%, 1.82%)\r\n \r\n ======================End of Report==========================\r\n The FLOPs is:7040\r\n", "Hello, I wrote code like this (from yunlong12) :\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\r\n\r\ndef get_flops(model):\r\n    concrete = tf.function(lambda inputs: model(inputs))\r\n    concrete_func = concrete.get_concrete_function(\r\n        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\r\n    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\r\n    with tf.Graph().as_default() as graph:\r\n        tf.graph_util.import_graph_def(graph_def, name='')\r\n        run_meta = tf.compat.v1.RunMetadata()\r\n        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\r\n        return flops.total_float_ops\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.Input(shape=(10, 1)))\r\nmodel.add(tf.keras.layers.Conv1D(2, 3, activation='relu'))\r\nmodel.summary()\r\nprint(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\r\n```\r\n\r\nHere is the result:\r\n\r\n```\r\n==================Model Analysis Report======================\r\n\r\nDoc:\r\nop: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are \r\naggregated together.\r\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\r\n\r\nProfile:\r\nnode name | # float_ops\r\nConv2D                   96 float_ops (100.00%, 85.71%)\r\nBiasAdd                   16 float_ops (14.29%, 14.29%)\r\n\r\n======================End of Report==========================\r\nThe FLOPs is:112\r\n```\r\n\r\nI used the Conv1D layer, however, It has shown the Conv2D profile report.\r\nIs Conv2D right? I don't understand well...\r\n", "> total_float_ops\r\nthere\u2018re many \"Incomplete shape.\"  in the screen. i don't know whether it is true. But thanks for your share!", "> Hello, I wrote code like this (from yunlong12) :\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\r\n> \r\n> def get_flops(model):\r\n>     concrete = tf.function(lambda inputs: model(inputs))\r\n>     concrete_func = concrete.get_concrete_function(\r\n>         [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\r\n>     frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\r\n>     with tf.Graph().as_default() as graph:\r\n>         tf.graph_util.import_graph_def(graph_def, name='')\r\n>         run_meta = tf.compat.v1.RunMetadata()\r\n>         opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n>         flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\r\n>         return flops.total_float_ops\r\n> \r\n> model = tf.keras.models.Sequential()\r\n> model.add(tf.keras.Input(shape=(10, 1)))\r\n> model.add(tf.keras.layers.Conv1D(2, 3, activation='relu'))\r\n> model.summary()\r\n> print(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\r\n> ```\r\n> \r\n> Here is the result:\r\n> \r\n> ```\r\n> ==================Model Analysis Report======================\r\n> \r\n> Doc:\r\n> op: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are \r\n> aggregated together.\r\n> flops: Number of float operations. Note: Please read the implementation for the math behind it.\r\n> \r\n> Profile:\r\n> node name | # float_ops\r\n> Conv2D                   96 float_ops (100.00%, 85.71%)\r\n> BiasAdd                   16 float_ops (14.29%, 14.29%)\r\n> \r\n> ======================End of Report==========================\r\n> The FLOPs is:112\r\n> ```\r\n> \r\n> I used the Conv1D layer, however, It has shown the Conv2D profile report.\r\n> Is Conv2D right? I don't understand well...\r\n\r\nHello Kaintels\r\n\r\nMay I know how to obtain the Model Analysis Report?\r\nI direct copy your code on colab can only the the The FLOPs is:112.\r\nThanks.\r\n", "Hello @hcleung3325.\r\n\r\nI ran this code in pycharm.\r\nmy tensorflow version is 2.2.0 and pycharm version is community 2020.3.2\r\n\r\nI hope it helps.\r\nThanks.", "Here is a much simpler snippet to get the flops for a tf-2.x model. \r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.profiler.model_analyzer import profile\r\nfrom tensorflow.python.profiler.option_builder import ProfileOptionBuilder\r\nprint('TensorFlow:', tf.__version__)\r\n\r\nmodel = tf.keras.applications.ResNet50()\r\n\r\nforward_pass = tf.function(\r\n    model.call,\r\n    input_signature=[tf.TensorSpec(shape=(1,) + model.input_shape[1:])])\r\n\r\ngraph_info = profile(forward_pass.get_concrete_function().graph,\r\n                        options=ProfileOptionBuilder.float_operation())\r\n\r\n# The //2 is necessary since `profile` counts multiply and accumulate\r\n# as two flops, here we report the total number of multiply accumulate ops\r\nflops = graph_info.total_float_ops // 2\r\nprint('Flops: {:,}'.format(flops))\r\n```\r\n\r\nOutput:\r\n```\r\nTensorFlow: 2.5.0\r\nFlops: 3,864,223,160\r\n```\r\nPS: We should not be importing anything from `tensorflow.python.*`, But until there is an official public API for this functionality, this should be fine.", "> Here is a much simpler snippet to get the flops for a tf-2.x model.\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> from tensorflow.python.profiler.model_analyzer import profile\r\n> from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\r\n> print('TensorFlow:', tf.__version__)\r\n> \r\n> model = tf.keras.applications.ResNet50()\r\n> \r\n> forward_pass = tf.function(\r\n>     model.call,\r\n>     input_signature=[tf.TensorSpec(shape=(1,) + model.input_shape[1:])])\r\n> \r\n> graph_info = profile(forward_pass.get_concrete_function().graph,\r\n>                         options=ProfileOptionBuilder.float_operation())\r\n> \r\n> # The //2 is necessary since `profile` counts multiply and accumulate\r\n> # as two flops, here we report the total number of multiply accumulate ops\r\n> flops = graph_info.total_float_ops // 2\r\n> print('Flops: {:,}'.format(flops))\r\n> ```\r\n> \r\n> Output:\r\n> \r\n> ```\r\n> TensorFlow: 2.5.0\r\n> Flops: 3,864,223,160\r\n> ```\r\n> \r\n> PS: We should not be importing anything from `tensorflow.python.*`, But until there is an official public API for this functionality, this should be fine.\r\n\r\nI think it  counts multiply as two flops while accumulate as one.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.engine.base_layer import Layer\r\nfrom tensorflow.keras.layers import Conv2D\r\nfrom tensorflow.python.profiler.model_analyzer import profile\r\nfrom tensorflow.python.profiler.option_builder import ProfileOptionBuilder\r\nprint('TensorFlow:', tf.__version__)\r\n\r\n\r\nmodel = tf.keras.applications.ResNet50()\r\nforward_pass = tf.function(\r\n    model.call,\r\n    input_signature=[tf.TensorSpec(shape=(1,) + (1, 1, 3))])\r\n\r\ngraph_info = profile(forward_pass.get_concrete_function().graph,\r\n                        options=ProfileOptionBuilder.float_operation())\r\n\r\nprint('Flops: {:,}'.format(flops))     # 7\r\n```               \r\n", "###  Hellow, i use code  like this (from yunlong12) \r\n`import tensorflow as tf\r\nfrom tensorflow.python.keras.engine.base_layer import Layer\r\nfrom tensorflow.keras.layers import Conv2D\r\nfrom tensorflow.python.profiler.model_analyzer import profile\r\nfrom tensorflow.python.profiler.option_builder import ProfileOptionBuilder\r\nprint('TensorFlow:', tf.__version__)\r\n\r\n\r\nmodel = tf.keras.applications.ResNet50()\r\nforward_pass = tf.function(\r\n    model.call,\r\n    input_signature=[tf.TensorSpec(shape=(1,) + (1, 1, 3))])\r\n\r\ngraph_info = profile(forward_pass.get_concrete_function().graph,\r\n                        options=ProfileOptionBuilder.float_operation())\r\n\r\nprint('Flops: {:,}'.format(flops))     # 7\r\n`\r\nThe ### outpus:\r\n> 22:21:14.790480:** I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: \r\n  graph_to_optimize  \r\n\r\n> function_optimizer: Graph size after: 216 nodes (198), 329 edges (310), time = 3.612ms.  \r\n\r\n> function_optimizer: function_optimizer did nothing. time = 0.07ms.  \r\n\r\n\r\nBefore getting the FLOPS results, there aforementioned outputs are confusing, I don't know how to interpret the both time = 3.612ms and time = 0.07ms. ", "I'm trying to measure floating point operations in a model, and how much they increase as you change the input shape. I'm using the following code snippet:\r\n\r\n```\r\nimport tensorflow\r\nimport keras\r\nimport tensorflow    as tf\r\nimport keras.backend as K\r\nimport numpy as np\r\nimport keras.backend         as K\r\nimport tensorflow            as tf\r\nfrom keras.layers            import Input\r\nfrom tensorflow.keras.applications.resnet50      import ResNet50\r\n\r\nfrom tensorflow.keras.preprocessing         import image\r\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\r\nimport numpy as np\r\n\r\ndef pre_process(img_size = (224, 224) ):\r\n    x = np.random.rand(img_size[0],img_size[1],img_size[2])\r\n    x = np.expand_dims(x, axis=0)\r\n    x = preprocess_input(x)\r\n\r\n    return x\r\n\r\ndef get_total_flops(graph,session,model):\r\n        \r\n    run_meta = tf.compat.v1.RunMetadata()\r\n    opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n\r\n    flops = tf.compat.v1.profiler.profile(graph=graph,\r\n        run_meta=run_meta, cmd=\"scope\", options=opts\r\n    )\r\n\r\n    return flops.total_float_ops\r\n\r\n\r\nsession = tf.compat.v1.Session()\r\ngraph = tf.compat.v1.get_default_graph()\r\n            \r\nINPUT_SHAPE = (128,128,3)\r\n    \r\nmodel = ResNet50(weights= None,input_tensor=Input(shape=INPUT_SHAPE),classes=1)\r\nwith graph.as_default():\r\n    with session.as_default():\r\n        x     = pre_process(img_size=INPUT_SHAPE)\r\n        \r\n        for i in range(20):\r\n            preds = model.predict(x)\r\n\r\n        total_flops = get_total_flops(graph,session,model)\r\n        \r\n        print('total_flops: ', total_flops)\r\n```\r\nthe output is:\r\n\r\n`total_flops: 23563360`\r\n\r\nI'm getting the exactly same number for different INPUT_SHAPES. e.g. INPUT_SHAPE=(256,256,3), INPUT_SHAPE=(512,512,3) ...\r\n\r\nAs i understood, a bigger image will lead to more operations in convolution layers, but it is returning the exactly same FLOPs.\r\n\r\ni'm using:\r\n\r\ntensorflow-gpu==2.6.0\r\nkeras==2.6.0\r\n\r\n", "> Note that the above doesn't seem to work when loading from a SavedModel; only from an h5.\r\n\r\nActually when we got the checkpoint, we can recover the graph any way, I proposed [this solution](https://stackoverflow.com/questions/67702185/retrain-frozen-graph-in-tensorflow-2-x) then ", "Hello \r\n\r\nHow can we do this for .pb files. \r\n\r\nThe solution available doesn't work for TF2.5 and above\r\n", "@srihari-humbarwadi\r\n\r\nI tried your method, but I get:\r\n> TypeError: The decorated function call has 2 required argument(s), but tf.function was only passed an input_signature of length 1. This covers 1 required argument(s): ['inputs'], but TensorSpecs are still required for the remaining 1 argument(s): ['training'].\r\n\r\nAnd without `input_signature=[tf.TensorSpec(shape=(1, 300, 6))]`:\r\n\r\n> TypeError: tf__call() missing 2 required positional arguments: 'inputs' and 'training'\r\n\r\nI'm using the custom model that contains only `tf.keras` predefined layers: \r\n* Add,\r\n* Dense,\r\n* Dropout,\r\n* MultiHeadAttention,\r\n* LayerNormalization,\r\n* Normalization,...\r\n\r\n**EDIT**\r\n\r\nThis code worked:\r\n```python\r\n@tf.function\r\ndef xy():\r\n    out, attn = model(X_test[0])\r\n\r\n# You can also do that in Python API.\r\ntf.compat.v1.profiler.profile(\r\n    xy.get_concrete_function().graph,\r\n    options=tf.compat.v1.profiler.ProfileOptionBuilder.float_operation())\r\n```\r\n\r\nNow is `//2` necessary??? Why TF is calculated 2 times greater?", "> Here is a much simpler snippet to get the flops for a tf-2.x model.\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> from tensorflow.python.profiler.model_analyzer import profile\r\n> from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\r\n> print('TensorFlow:', tf.__version__)\r\n> \r\n> model = tf.keras.applications.ResNet50()\r\n> \r\n> forward_pass = tf.function(\r\n>     model.call,\r\n>     input_signature=[tf.TensorSpec(shape=(1,) + model.input_shape[1:])])\r\n> \r\n> graph_info = profile(forward_pass.get_concrete_function().graph,\r\n>                         options=ProfileOptionBuilder.float_operation())\r\n> \r\n> # The //2 is necessary since `profile` counts multiply and accumulate\r\n> # as two flops, here we report the total number of multiply accumulate ops\r\n> flops = graph_info.total_float_ops // 2\r\n> print('Flops: {:,}'.format(flops))\r\n> ```\r\n> \r\n> Output:\r\n> \r\n> ```\r\n> TensorFlow: 2.5.0\r\n> Flops: 3,864,223,160\r\n> ```\r\n> \r\n> PS: We should not be importing anything from `tensorflow.python.*`, But until there is an official public API for this functionality, this should be fine.\r\n\r\nHi,\r\nHow can I use this code with a multi inputs model? \r\nThanks", "\r\n> > Here is a much simpler snippet to get the flops for a tf-2.x model.\r\n> > ```python\r\n> > import tensorflow as tf\r\n> > from tensorflow.python.profiler.model_analyzer import profile\r\n> > from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\r\n> > print('TensorFlow:', tf.__version__)\r\n> > \r\n> > model = tf.keras.applications.ResNet50()\r\n> > \r\n> > forward_pass = tf.function(\r\n> >     model.call,\r\n> >     input_signature=[tf.TensorSpec(shape=(1,) + model.input_shape[1:])])\r\n> > \r\n> > graph_info = profile(forward_pass.get_concrete_function().graph,\r\n> >                         options=ProfileOptionBuilder.float_operation())\r\n> > \r\n> > # The //2 is necessary since `profile` counts multiply and accumulate\r\n> > # as two flops, here we report the total number of multiply accumulate ops\r\n> > flops = graph_info.total_float_ops // 2\r\n> > print('Flops: {:,}'.format(flops))\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > Output:\r\n> > ```\r\n> > TensorFlow: 2.5.0\r\n> > Flops: 3,864,223,160\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > PS: We should not be importing anything from `tensorflow.python.*`, But until there is an official public API for this functionality, this should be fine.\r\n> \r\n> Hi, How can I use this code with a multi input model? Thanks\r\n\r\n@sendjasni You can change the `input_signature` accordingly for arbitrary number of inputs \r\n```python\r\ninput_signature = [(\r\n    tf.TensorSpec(shape=<shape of input 1>), \r\n    tf.TensorSpec(shape=<shape of input 2>),\r\n    tf.TensorSpec(shape=<shape of input 3>),\r\n    tf.TensorSpec(shape=<shape of input 4>))]\r\nforward_pass = tf.function(\r\n    model.call,\r\n    input_signature=input_signature)\r\n```", "@srihari-humbarwadi \r\nThanks for the prompt response.\r\n\r\nSo I've added this:\r\n```python\r\ninput_signature = [\r\n    tf.TensorSpec(shape=(1,) + (256, 256, 3)), \r\n    tf.TensorSpec(shape=(1,) + (256, 256, 3)),\r\n    tf.TensorSpec(shape=(1,) + (256, 256, 3)),\r\n    tf.TensorSpec(shape=(1,) + (256, 256, 3)),\r\n    tf.TensorSpec(shape=(1,) + (256, 256, 3)),\r\n    tf.TensorSpec(shape=(1,) + (256, 256, 3))\r\n    ]\r\nforward_pass = tf.function(\r\n        model.call,\r\n        input_signature=input_signature)\r\n```\r\nHowever, I got the following error:\r\n\r\n> TypeError: in user code:\r\n>     TypeError: tf__call() takes from 2 to 4 positional arguments but 7 were given\r\n", "@sendjasni I updated my comment; the inputs should be supplied in a tuple. The general idea is being,  `input_signature` should match the structuring of the model's inputs ", "> @sendjasni I updated my comment; the inputs should be supplied in a tuple. The general idea is being, `input_signature` should match the structuring of the model's inputs\r\n\r\nGreat, it worked. Thanks.", "@MarkDaoust Can we guide the community on how to prepare a PR around this? Do you know a sponsor/owner candidate from the team?", "> @sendjasni I updated my comment; the inputs should be supplied in a tuple. The general idea is being, `input_signature` should match the structuring of the model's inputs\r\n\r\nBtw, do you have an idea on how to compute the inference time?\r\n", "> I implemented small lib to calculate FLOPs/MACs: https://github.com/evgps/flopco-keras\r\n\r\nThis repo doesn't work anymore.", "We need to \"migrate\" this ticket to https://github.com/keras-team/keras"]}, {"number": 32796, "title": "When does tensorflow lite support 3d cnn?", "body": "\r\nI trained 3d cnn network on tensorflow, now I want to port to Android phone, but browsed the document and found that tensorflow-lite does not support 3d cnn operation, when will it support it?", "comments": ["Can you tell us more about your use case to help us prioritize? For example, what kind of task this model is trying to do, its application scenarios, etc. Thanks!", "For me, it's video classifiction and temporal detection.", "@Jessespace,\r\nCan you please refer [this Tutorial](https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub) which demonstrates the use of **`3D CNN`** and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This tutorial isn't about tensorflow-lite, is it ?\r\n\r\nI see here that still no 3d ops are supported https://www.tensorflow.org/mlir/tfl_ops\r\n\r\nI see in quantization schemes that 3D conv still isn't supported https://www.tensorflow.org/lite/performance/quantization_spec"]}, {"number": 32556, "title": "Bazel workspace.bzl requires a dependency on rules_closure", "body": "Per https://stackoverflow.com/questions/52482972/running-load-within-skylark-macro\r\nusers of Tensorflow are adding rules_closure to their `WORKSPACE` file even though they don't use it.\r\n\r\nrules_closure is minimally maintained (my team has just acquired it). We don't want new dependencies on it and have discovered that most dependencies are due to this issue.\r\n\r\nI suspect the Tensorflow users don't actually want to build a Closure Compiler-minified UI, so any `load()` statement from rules_closure doesn't belong in the tensorflow bazel rules distribution.\r\n\r\nNote, #15997 suggests customizing the dependencies, but in this issue I propose that users should never observe this dependency at all.", "comments": ["Thanks for mentioning this. I don't think I quite follow you though, what exactly can we do to avoid this without making our own files more confusing?", "The typical way we handle this in rules_nodejs is to \"fence\" regions of BUILD files that are only for our internal use, then strip these from the published distribution artifact.\r\nHow exactly to do this depends on how you build the Tensorflow distribution. I'm happy to help fix this if you can give some pointers to where the distribution is built.", "@alexeagle Could you please let us know if you still need help on this ? if it is resolved then please feel free to move this issue to close status ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "yes I think this is still a problem, though I don't know where the workspace.bzl file is now, nor what instructions you give Bazel users. @gkdn has been maintaining rules_closure lately and might be closer to understanding the impact.", "@alexeagle, \r\nYou can find the workspace.bzl file [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace0.bzl).\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Activity for the bot!"]}, {"number": 32532, "title": "Improving compatibility with Scikit-learn", "body": "The original issue in keras-team/keras is [this](https://github.com/keras-team/keras/issues/12832\r\n)\r\n\r\nScikit-learn estimators are instanced only with hyper-parameter information. No knowledge on the data (X or y) is needed until the estimator is fitted. On the other hand, KerasRegressor and KerasClassifier receive as argument a build_fn that instantiates and compiles the estimator. At this point, the input and output shapes are needed to define the network's architecture. This is a subtle but important difference between Scikit-learn estimators and KerasClassifier/KerasRegressor.\r\n\r\nAlso, Scikit-learn's GridSearchCV and RandomizedSearchCV (and probably Scikit-optimize's BayesSearchCV) automatically infer the most appropriate CV folding (stratified or not) from the base_estimator. KerasClassifier and KerasRegressor, however, do not have the attribute needed for this to work.\r\n\r\nMy proposed solution (with some additional tests) is in [this](https://github.com/daviddiazvico/tensorflow/commit/e33e1c5f88e9832b2578f495ee85bcae7e9a5a4f) commit (I'll open a PR now).\r\n\r\nThank you!", "comments": ["Hello,\r\n\r\nMaking the sklearn wrapper classes agnostic to input shape seems like a good idea. However the commit you link make many assumptions that wouldn't apply. Since these classes are meant for compatibility with sklearn, we should only support single Numpy arrays as input, since this is what sklearn supports.", "Hi,\r\n\r\nThanks for the comment.\r\n\r\nI'm not sure I understand it, though. Just to confirm:\r\n\r\nI guess you mean that no dict should be allowed as input (like in [this test](https://github.com/tensorflow/tensorflow/pull/32533/commits/e33e1c5f88e9832b2578f495ee85bcae7e9a5a4f#diff-7145b17c14e03227607f6a4617309181R343\r\n)). That is totally true. I included this to provide a better integration with the functional Model class. But I will remove it if you think it is the best option.\r\n\r\nHowever, if you mean that inputs should not be tensors with order>2, I think it may be interesting to keep it as it is.\r\n\r\nAs far as I know CNNs and maybe even RNNs could work with the hyperparameter search (check [test](https://github.com/tensorflow/tensorflow/pull/32533/commits/e33e1c5f88e9832b2578f495ee85bcae7e9a5a4f#diff-7145b17c14e03227607f6a4617309181R312)), calibrated classifiers ([test](https://github.com/tensorflow/tensorflow/pull/32533/commits/e33e1c5f88e9832b2578f495ee85bcae7e9a5a4f#diff-7145b17c14e03227607f6a4617309181R334)), ensembles ([test](https://github.com/tensorflow/tensorflow/pull/32533/commits/e33e1c5f88e9832b2578f495ee85bcae7e9a5a4f#diff-7145b17c14e03227607f6a4617309181R324)), etc. provided by sklearn, as long as no tranformer is applied before the keras model in a pipeline. If only numpy matrices are allowed as input, we will lose that functionality.\r\n\r\nIf you confirm me that we are in the first case, I'll do the changes right now.", "@daviddiazvico Are you raising any PR to update the code and docs? Thanks!", "> @daviddiazvico Are you raising any PR to update the code and docs? Thanks!\r\n\r\n[This one](https://github.com/daviddiazvico/tensorflow/commit/e33e1c5f88e9832b2578f495ee85bcae7e9a5a4f). It's been open for a while, but I'm afraid I couldn't find where is the documentation to update. The only related doc I could find is just a pointer to [keras docs](https://github.com/daviddiazvico/tensorflow/blob/master/tensorflow/python/keras/README.md).\r\n\r\nI opened this same PR in keras ([here](https://github.com/daviddiazvico/keras/commit/053d3b3593650ccbbbc828e524f658fd13c3f6c9)) long time ago, and the changes to the docs are really minimal. If you could please point me out where can I find them in TF I'd fix them."]}, {"number": 32492, "title": "SIGSEGV", "body": "```\r\nimport tensorflow as tf\r\nfrom tensorflow_probability import distributions as tfd\r\n\r\nmu = [1., 2., 3.]\r\nmu = tf.stack([mu, mu], 0)\r\ncov = [[ 0.36,  0.12,  0.06],\r\n       [ 0.12,  0.29, -0.13],\r\n       [ 0.06, -0.13,  0.26]]\r\ncov = tf.stack([cov, cov], 0)\r\nscale = tf.linalg.cholesky(cov)\r\n\r\nmvn = tfd.MultivariateNormalTriL(\r\n    loc=mu,\r\n    scale_tril=scale)\r\n\r\nl = mvn.bijector.scale.to_dense()\r\nl = tf.linalg.triangular_solve(l, tf.eye(3))\r\n```\r\n\r\n**System information**\r\n- Ubuntu 18.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 2.0.0-dev20190913\r\n- Python version: 3.7\r\n- No GPU\r\n\r\ngit version: v1.12.1-11024-ge8506e0\r\ntensorflow-probability version: 0.9.0-dev20190913\r\n\r\n**Describe the current behavior**\r\n`Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)`\r\n\r\nSometimes it hangs for a bit, sometimes it doesn't crash. But 2/3 times I get a SIGSEGV.\r\nMost modifications to the posted code somehow solve the crash.\r\n\r\n**Describe the expected behavior**\r\nIt should complain about dimensions that mismatch at the last operation.\r\n\r\n\r\n", "comments": ["To determine if this is a bug of tf or tf_probability, can you change the script to\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow_probability import distributions as tfd\r\n\r\nmu = [1., 2., 3.]\r\nmu = tf.stack([mu, mu], 0)\r\ncov = [[ 0.36,  0.12,  0.06],\r\n       [ 0.12,  0.29, -0.13],\r\n       [ 0.06, -0.13,  0.26]]\r\ncov = tf.stack([cov, cov], 0)\r\nscale = tf.linalg.cholesky(cov)\r\n\r\nmvn = tfd.MultivariateNormalTriL(\r\n    loc=mu,\r\n    scale_tril=scale)\r\n\r\nl = mvn.bijector.scale.to_dense()\r\ntf.print(l)\r\n\r\n\r\nl = tf.linalg.triangular_solve(l, tf.eye(3))\r\n```\r\n\r\nThen run it in a loop until the first SIGSEGV and see the value of `l` tensor printed in the last run. Then use the following script\r\n\r\n```python\r\nimport tensorflow as tf\r\nl = ... # insert here the tensor that got printed\r\nl = tf.linalg.triangular_solve(l, tf.eye(3))\r\n```\r\n\r\nRun it, if it still crashes, please post the code here. Otherwise, might need to open issue on tf-probability repo, but still please update here.", "No need to loop, code is deterministic so L is always the same.\r\nI tried that. It doesn't crash.", "It's kinda hard to know which step makes the system unstable, but most if not all of MultivariateNormalTriL is just plain tensorflow code under the hood. So the issue will probably still come back.", "```python\r\nimport tensorflow as tf\r\n\r\nl = [\r\n     [[0.6,  0  , 0],\r\n      [0.2,  0.5, 0],\r\n      [0.1, -0.3, 0.4],\r\n     ],\r\n     [[0.6,  0  , 0],\r\n      [0.2,  0.5, 0],\r\n      [0.1, -0.3, 0.4],\r\n     ],\r\n    ]\r\n\r\nl = tf.linalg.triangular_solve(l, tf.eye(3))\r\n```\r\n\r\nOut of 10 runs, the above segfaults 8 times and the other 2 times ends with\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 14, in <module>\r\n    l = tf.linalg.triangular_solve(l, tf.eye(3))\r\n  File \"/tmp/gh/19/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_linalg_ops.py\", line 1786, in matrix_triangular_solve\r\n    matrix, rhs, lower=lower, adjoint=adjoint, name=name, ctx=_ctx)\r\n  File \"/tmp/gh/19/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_linalg_ops.py\", line 1841, in matrix_triangular_solve_eager_fallback\r\n    attrs=_attrs, ctx=ctx, name=name)\r\n  File \"/tmp/gh/19/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: All input tensors must have the same rank. [Op:MatrixTriangularSolve]\r\n```\r\n\r\nThe error makes sense, `tf.eye(3)` is a 2D tensor whereas `l` is a 3D one. The segfault though, that's something we'll have to look at.", "Heh, indeed, it crashes. I was apparently just extremely unlucky that I had a few non-crashing runs in a row :)", "One-linering the problem seems to make the crash much more consistent.\r\n`tf.linalg.triangular_solve([1], tf.eye(3))`\r\nSIGSEGVs 20/20 runs"]}, {"number": 32434, "title": "Int32 overflow in sparse_reshape on Windows", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 64-bit\r\n- TensorFlow installed from: Source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\nWhen a large sparse tensor is reshaped using `sparse_reshape` on Windows it fails, due to the fact that it uses `np.prod` to determine whether the number of elements in the old and new SparseTensor are the same. This can fail on Windows, due to the fact that numpy converts python ints to dtype np.int32 on Windows, which will overflow for large dimensions.\r\n\r\n**Describe the expected behavior**\r\n`sparse_reshape` should work similarly on both Windows and Unix-based systems.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nidxs = tf.constant([[1, 0, 1],\r\n                             [1, 4, 2]], dtype=tf.int64)\r\nvalues = tf.constant([12, 42])\r\n\r\nA = tf.sparse.SparseTensor(idxs, values, (8, 42000, 40000))\r\ntf.sparse.reshape(A, (8, 40000, 42000))\r\n```\r\n\r\nOutput:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\thomasoerkild\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\", line 769, in sparse_reshape\r\n    reshaped_size))\r\nValueError: Cannot reshape a tensor with 555098112 elements to shape [8, 40000, 42000] (13440000000 elements).\r\n```\r\n\r\n**Other info / logs**\r\nThe issue might be resolved, by forcing the shape inputted to `np.prod` to be of type np.int64.\r\n", "comments": []}, {"number": 32410, "title": "Possible issue in tf.scatter_nd documentation.", "body": "Thank you for explaining about tf.scatter_nd using some wonderful visualizations. I have a doubt whether the cubes used for the higher dimensional explanation of tf.scatter_nd is right or not.\r\nPlease check the visualization of the Cube tagged as 'output' in [tf.scatter_nd](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/scatter_nd#) documentation, the second and fourth indices are shaded whereas the given indices according to the example is tf.constant([[0], [2]]) so the first and third indices (0 , 2) should be shaded instead of second and fourth. Please correct me if I'm wrong\r\n", "comments": ["Instead of changing the cube, it would be better if we change indices \r\nFrom\r\nindices = tf.constant([[0], [2]]) \r\nTo \r\nindices = tf.constant([[1], [3]])", "@praveenjune17,\r\nSorry for the delayed response. I think it's a [good idea](https://github.com/tensorflow/tensorflow/issues/32410#issuecomment-530669422).  \r\n\r\nCreated [a CL](https://critique-ng.corp.google.com/cl/370902959) to fix it."]}, {"number": 32322, "title": "compute_mask not called when custom layer does not alter inputs and mask is not None", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta\r\n- Python version: 3.7.3-2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen the layer does not alter the inputs (but alters the mask), if `mask` is not `None`, the layer does not execute the `compute_mask` method. \r\n**Describe the expected behavior**\r\nCall `compute_mask` and attach the mask to output regardless.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass RandomMaskingNoAlter(tf.keras.layers.Layer):\r\n\r\n  def call(self, inputs):\r\n    return inputs\r\n\r\n  def compute_mask(self, inputs, mask=None):\r\n    print('no alter executed')\r\n    if mask is None:\r\n      return None\r\n    random_mask = tf.cast(tf.random.uniform(tf.shape(mask), 0, 1, dtype=tf.int32), tf.bool)\r\n    return tf.math.logical_and(random_mask, mask)\r\n\r\nclass RandomMaskingAlter(tf.keras.layers.Layer):\r\n    \r\n  def call(self, inputs):\r\n    return inputs + 0\r\n\r\n  def compute_mask(self, inputs, mask=None):\r\n    print('alter executed')\r\n    if mask is None:\r\n      return None\r\n    random_mask = tf.cast(tf.random.uniform(tf.shape(mask), 0, 1, dtype=tf.int32), tf.bool)\r\n    return tf.math.logical_and(random_mask, mask)\r\n\r\nx = np.array([[1, 4, 2, 2, 0, 0], [1, 1, 1, 0, 0, 0], [3, 2, 2, 3, 4, 1]], dtype='i4')\r\ny = tf.keras.layers.Embedding(5, 5, mask_zero=True)(x)\r\n\r\nz1 = RandomMaskingNoAlter()(x)\r\nz2 = RandomMaskingNoAlter()(y)\r\nz3 = RandomMaskingAlter()(x)\r\nz4 = RandomMaskingAlter()(y)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@deasmhumhna ,\r\nWhen tried executing the given code the following output was received \r\n`no alter executed\r\n alter executed\r\n alter executed` is the same output received from your side also?Thanks!", "> @deasmhumhna ,\r\n> When tried executing the given code the following output was received\r\n> `no alter executed alter executed alter executed` is the same output received from your side also?Thanks!\r\n\r\nYes, that is what I get. The result should be `no alter executed no alter executed alter executed alter executed` (`compute_mask` should always be called if implemented by a subclass). I am however curious why it runs when `mask=None` but not if there is a pre-existing mask. Some kind of optimization?", "Issue replicating with TF 2.0beta, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/bbbb2f8fd4f731bb259b71f4370a2a4a/32322.ipynb) of colab.Thanks!", "Issue is replicating with TF nightly version (`2.4.0-dev20200902`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/2ace9e7eb906a0159c27748a0b159434/untitled305.ipynb). Thanks!", "Issue is replicating with TF nightly version (2.6.0-dev20210416).Please, find the [gist here](https://colab.research.google.com/gist/Saduf2019/74c25be65ca20bc5653aac311d0038b5/untitled590.ipynb). Thanks!", "It seems that the problem still exists in TF 2.5.\r\n\r\nAny plans to fix it?", "Able to reproduce the issue in `TF 2.7`, and the reported issue still exists. Please find the [gist here](https://colab.research.google.com/gist/kumariko/3f7841f8e8e6a03dc94e71e1b66ffdbc/32322.ipynb#scrollTo=DmrGe-N49JR6). Thanks!", "Hi @deasmhumhna , Edited your code at this part to resolve the issue.\r\n```\r\nclass RandomMaskingNoAlter(tf.keras.layers.Layer):\r\n\r\n  def call(self, inputs):\r\n    return inputs\r\n```\r\n\r\nto\r\n\r\n```\r\nclass RandomMaskingNoAlter(tf.keras.layers.Layer):\r\n\r\n  def call(self, inputs):\r\n    return inputs+0\r\n```\r\n\r\nproviding [gist](https://colab.research.google.com/gist/mohantym/e746a27163286c6424fc58b93df76ae5/32322.ipynb#scrollTo=WKwaGW2QJoZu) for reference", "@mohantym \r\nThis is a not-so-nice hack to somehow circumvent the bug. \r\nDefinitely, not a solution. \r\n\r\nCan somebody from TF take care of it.\r\n\r\n", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "I would suggest that Keras team will add a unit test to cover this issue (and endless number of other issues). Passing down to the users the task of testing issues in every new release with a hope that the issue is somehow fixed does not look like a good strategy.....\r\n", "Thanks @mohantym that saved me. I was wondering why mine wasn't working either, and that fixed it for me. I agree, this is a crappy fix though. I'm running TF 2.6.0 installed via Conda and it's an issue there, but I can't check a newer version"]}, {"number": 32122, "title": "Cannot seek on write only tf.gfile.GFile", "body": "**System information**\r\n\r\n-  Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nCalling `seek()` on a `tf.gfile.GFile` opened in write only mode raises `tensorflow.python.framework.errors_impl.PermissionDeniedError`.\r\n\r\n**Describe the expected behavior**\r\nGFile should support the Python IO semantics that supports seeking on a write only file.\r\n\r\nMore generally it would be preferable if GFile followed the API of Python's [`io.IOBase`](https://docs.python.org/3/library/io.html#io.IOBase).\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.io.gfile.GFile('test.txt', 'w') as f:\r\n    f.seek(0)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/VENV/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/VENV/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 146, in seek\r\n    self._preread_check()\r\n  File \"/VENV/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py\", line 82, in _preread_check\r\n    \"File isn't open for reading\")\r\ntensorflow.python.framework.errors_impl.PermissionDeniedError: File isn't open for reading\r\n```", "comments": ["Related to #32090, right?", "It is at least related in the sense that GFile does not behave as normal Python file like objects.\r\n\r\nThis specific bug seems to be caused by an assumption that only readable files can be seeked.\r\nIt seems that `tell()` had the same behaviour until [this commit](https://github.com/tensorflow/tensorflow/commit/3cee532afd32c2db28e9359765d88ad5f1567f5f).", "Well, yes, the API is incomplete.\r\n\r\nMy work on modular filesystems will first separate all filesystems into DSOs and then write a test for functionality requirements and then we can start completing the API and checking in every system to make sure it works. Maybe even fuzz the entire thing"]}, {"number": 32090, "title": "Initial read on nonexistent tf.gfile.GFile in w+ mode crashes", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nPython raises `tensorflow.python.framework.errors_impl.NotFoundError` when doing a first read (no writes before it) on a nonexistent `tf.gfile.GFile` in `w+` mode.\r\n\r\n**Describe the expected behavior**\r\nRead on an empty `w+` file should return an empty string.\r\nOne problem with the current behaviour is that numpy.savez() crashes when writing to a GFile.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.io.gfile.GFile('test.txt', 'w+') as f:\r\n    f.read()\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_gfile.py\", line 5, in <module>\r\n    f.read()\r\n  File \"/VENV/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 122, in read\r\n    self._preread_check()\r\n  File \"/VENV/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 84, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512)\r\ntensorflow.python.framework.errors_impl.NotFoundError: test.txt; No such file or directory\r\n```", "comments": ["`test.txt` is not in your current working directory. Please, make sure `test.txt` and `your python-file/jupyter-notebook` are in same directory.Thanks!", "I expect that the `w` mode (and `w+`, `w+b`) should have similar semantics to Python `open`, where `w` means [truncate the file first](https://docs.python.org/3/library/functions.html#open).\r\n\r\nSince this isn't the case it breaks things like `numpy.savez()`.\r\n\r\nWorkarounds are to either write something, e.g. `\"\"`, to the file before reading so that the file is created, or manually create the file first as you suggest. But those are workarounds to a problem that I think should be solved in GFile.", "This happens on both nightly and 2.0.\r\n\r\nAs I'm working on [modularizing filesystem support](https://github.com/tensorflow/community/blob/master/rfcs/20190506-filesystem-plugin-modular-tensorflow.md), I'm assigning this to me, although it will take a while until I can get to the python side of things.\r\n\r\nIn the end, the expected behavior should be similar to Python's:\r\n\r\n```python\r\n>>> with open('this_file_does_not_exist_at_all', 'w+') as f: f.read()\r\n... \r\n''\r\n```", "Related to the fact that GFile does not truncate files the same way as Python, reading an existing file opened with `w+` will return the text in the file instead of `\"\"`.\r\n\r\nExample:\r\n```\r\nimport tensorflow as tf\r\n\r\nwith open('existing.txt', 'w') as f:\r\n    f.write('txt')\r\n    f.flush()\r\n\r\nwith tf.io.gfile.GFile('existing.txt', 'w+') as f:\r\n    print(f.read())  # Prints txt, should print \"\"\r\n```", "I'll have to handle that too, thanks for pointing it out", "`f.seek()` also crashes in the same way as `f.read()`.", "Any update on this? I'm running into this error trying to write to s3 with gfile and pysoundfile", "@mihaimaruseac Any updates on this? This still seems to be an issue with TF 2.5 and prevent's the use of `GFile` together with `np.savez`.", "issue still exists in `2.6.0` and `nightly`. Here's the [gist](https://colab.research.google.com/gist/sanatmpa1/985a559b4ff8e165576f2803457ad8d1/32090.ipynb#scrollTo=hRPeuchG--s3). Thanks!", "Problem is the modularization effort stalled since members left the team last year.\r\n\r\nWe onboarded new members recently, so we should pick up these items again. Apologies for the delays.", "from @eirism in https://github.com/tensorflow/tensorflow/issues/32090#issuecomment-526563509:\r\n> Workarounds are to either write something, e.g. `\"\"`, to the file before reading so that the file is created, or manually create the file first as you suggest. But those are workarounds to a problem that I think should be solved in GFile.\r\n\r\ncould you give an example of such a workaround? I tried the following unsuccessful:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nwith tf.io.gfile.GFile(\"output.npz\", \"w\") as file:\r\n    file.write(\"\")\r\n    np.savez(file, content=np.array([1, 2, 3]))\r\n```\r\nStill getting\r\n```\r\ntensorflow.python.framework.errors_impl.PermissionDeniedError: File isn't open for reading\r\n```", "@patzm Since `numpy.savez` both reads and writes to the file you need to open it in `w+` mode. You get the error because `w` is only for writing to the file, you can not read from a `w` file.", "sadly also doesn't work for me. Neither `w+` nor `wb+`. Could you post a minimal example maybe?\r\nBtw, I think this is also related to #32975.", "a similar approach that works for me.\r\n```\r\nio_buffer = io.BytesIO()\r\nnp.savez(io_buffer, ...)\r\nwith gfile.Open(path, \"wb\") as f:\r\n  f.write(io_buffer.getvalue())\r\n```"]}, {"number": 32085, "title": "Documentation for ./configure environment variables", "body": "I'm looking for documentation with all of the `TF_*` environment variables that can be set to make `./configure` unattended. All I can find are various github issues or random articles using them, but is there a definitive official list somewhere?", "comments": ["@sjdrc ,\r\nCan you please check if this [link](https://tensorflow.google.cn/install/source_windows?hl=en#configure_the_build) helps.Thanks!", "No. This describes using the script interactively. I want to set environment variables such as `TF_NEED_CUDA` and `TF_NEED_AWS` to make the process unattended.", "Just want to bump you on this. There are various pieces of information for this all over the place. All we need is a single official source listing and explaining the variables.", "Hello? I have been waiting for quite some time for a response on this. Surely this is documented somewhere.", "Still no answer to this? I am trying to build TF inside a Docker image so an interactive script isn't going to work. ", "I was able to build TF inside Docker via `yes \"\" | python configure.py`", "@mihaimaruseac I need to override some of the options specifically setting a custom Cuda version. For now I am digging through the code and think I found what I needed. Documentation would still be worthwhile.", "It just seems like if they just had a list of all the variables you could configure it would make this easier for a lot of people. Why is this documentation missing, and can a tensorflower please comment on this?", "The `configure.py` script is extremely convoluted and grew organically.\r\n\r\nIf someone were to provide a PR documenting the features I think it can be accepted."]}, {"number": 32082, "title": "[TF 2.0] tf.assert_equal([], [1.0]) doesn't raise error", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDarwin Kernel Version 18.6.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n2.0.0-dev20190827\r\n- Python version:\r\nPython 3.6.8 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\n`tf.assert_equal([], [1.0])` doesn't raise any error. \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.assert_equal([], [1.0])\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@David-Mao I could reproduce the issue. Are you interested in raising PR to update code/doc accordingly? Thanks!", "I think the reason for the issue is that, `tf.assert_equal` follows the same as `==` which automatically broadcast. With braodcast `tf.assert_equal([], [1.0])` is correct. A similar case below shows the same that will broadcast:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.assert_equal([[1.0, 2.0], [1.0, 2.0]], [1.0, 2.0])\r\n```\r\n\r\nFrom that standpoint the behavior of `tf.assert_equal` works as expected, though indeed it might be misleading.\r\n\r\nWondering if it makes sense to have an additional arg of `broadcast=[True|False]` to explicitly call out broadcast or not?", "@yongtang \r\n> I think the reason for the issue is that, `tf.assert_equal` follows the same as `==` which automatically broadcast. With braodcast `tf.assert_equal([], [1.0])` is correct. \r\n\r\nBefore I submitted this ticket I  carefully read the API doc and I noticed the broadcast thing, but I still think it's too anti-intuitive an argument to be made here. At least in this case it seems to me more of a bug than a feature. \r\n\r\nI like the idea of adding the broadcast arg (and maybe have broadcast=False to be the default value).\r\n\r\n@jvishnuvardhan I read the source code and it seems to be a non-trivial fix. I'm not sure I can make the PR easily...", "Empty list should not broadcast to list of one element; broadcasting should just add dimensions or expand dimensions sized 1 to larger sizes", "I think it was resolved. I am closing the issue. But, please let me know if I'm mistaken. Thanks!\r\n\r\nPlease feel free to open a PR to update docs to reflect @alextp comments so that community will get benefited. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32082\">No</a>\n", "@jvishnuvardhan \r\nI just upgrade to tf-nightly-2.0-preview-2.0.0.dev20190923 and `tf.assert_equal([], [1.0])\r\n` still passes? I think @alextp 's comment was saying that this is indeed a bug, not work as expected?\r\n\r\n> I think it was resolved. I am closing the issue. But, please let me know if I'm mistaken. Thanks!\r\n> \r\n> Please feel free to open a PR to update docs to reflect @alextp comments so that community will get benefited. Thanks!\r\n\r\n", "When I said \"should\" I didn't mean it actually does right now...\n\nOn Mon, Sep 23, 2019 at 10:30 PM David-Mao <notifications@github.com> wrote:\n\n> I just upgrade to tf-nightly-2.0-preview-2.0.0.dev20190923 and tf.assert_equal([],\n> [1.0]) still passes?\n>\n> I think it was resolved. I am closing the issue. But, please let me know\n> if I'm mistaken. Thanks!\n>\n> Please feel free to open a PR to update docs to reflect @alextp\n> <https://github.com/alextp> comments so that community will get\n> benefited. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32082?email_source=notifications&email_token=AAABHRIT4DLP3ZJ2EX37UEDQLGQVXA5CNFSM4ISAY2UKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7NDGHQ#issuecomment-534393630>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRM4NA3Z3RXIPL5VJJLQLGQVXANCNFSM4ISAY2UA>\n> .\n>\n\n\n-- \n - Alex\n", "@jvishnuvardhan I think @alextp was saying empty list `[]` should do broadcast first, when shape does not match for cwise ops. Let's re-open the issue. I think I might be able to take a look at the code path and see if I can have a fix.", "Update:\r\n\r\n~~was saying empty list [] should do broadcast first, when shape does not match for cwise ops~~\r\n\r\nThere was a typo in the last message:\r\nwas saying empty list [] should **not** do broadcast first, when shape does not match for cwise ops\r\n\r\n", "...and even if you do broadcast the empty list should broadcast to 0, not\nto 1, I think\n\nOn Tue, Sep 24, 2019 at 9:03 AM Yong Tang <notifications@github.com> wrote:\n\n> Update:\n>\n> was saying empty list [] should do broadcast first, when shape does not\n> match for cwise ops\n>\n> There was a typo in the last message:\n> was saying empty list [] should *not* do broadcast first, when shape does\n> not match for cwise ops\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32082?email_source=notifications&email_token=AAABHRJO6RVCLI5FT7Y6BHDQLI24BA5CNFSM4ISAY2UKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7O3UZY#issuecomment-534624871>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMY7GT6J32C3UD5SILQLI24BANCNFSM4ISAY2UA>\n> .\n>\n\n\n-- \n - Alex\n", "Sorry for misunderstanding @alextp comments. Thanks @yongtang for reopening the issue. ", "Added a PR #33066 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32082\">No</a>\n", "This issue still appears in TensorFlow 2.2.0. Here is the [Colab](https://colab.research.google.com/drive/1f8zX66XM71aYxfxvU9UieakaQKAyz_Mg?usp=sharing) that reproduce this issue.", "> This issue still appears in TensorFlow 2.2.0. Here is the [Colab](https://colab.research.google.com/drive/1f8zX66XM71aYxfxvU9UieakaQKAyz_Mg?usp=sharing) that reproduce this issue.\r\n\r\nYes indeed. ping @yongtang ", "Looks like the PR was rolled back at some point in 1d5fb46b89611b95c8fda8f2f3d597d88455f5f2 , will take a look and see if I can resubmit.", "This doesn't seem to need API Review, removing the label. Please re-add if needed.", "Error exists with `TF Version 2.4.1` as well. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/a7fbe746814ddcfe80c7e5abd9f2595b/tensorflow-issue-32082.ipynb).", "Was able to replicate the issue with TF v2.5,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/33af09960a5643137c5fa270292ec06d/tensorflow-issue-32082.ipynb#scrollTo=bIPYVTSiKGic)..Thanks!", "Checked in `TF 2.6.0` and the issue still persists. Please find the [gist here](https://colab.research.google.com/gist/sanatmpa1/c239a0283aaa27bdf633529e04134458/32082.ipynb)", "@David-Mao Was able to replicate  the issue in TF v2.7.0, please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/1797118607564a9864534880b1b9f52d/32082.ipynb#scrollTo=h0KJMbLXB3MB). Thanks!"]}]