[{"number": 18769, "title": "InvalidArgumentError for save/restore of variables (same version, same OS, same directory)", "body": "I get an InvalidArgumentError with no further information when I try to save and then restore parts of my model later to continue training it (due to needing my laptop for class).\r\n\r\nInitialization:\r\nsaver = tf.train.Saver({\"embeddings\": embeddings, \"weights\": nce_weights, \"biases\": nce_biases})\r\n\r\nSave:\r\nsaver.save(sess, model_checkpoint_path)\r\n\r\nLoad:\r\nsaver.restore(sess, model_checkpoint_path)\r\n\r\n```\r\n2018-04-21 22:45:00.143245: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument\r\nTraceback (most recent call last):\r\n  File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1312, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1420, in _call_tf_sessionrun\r\n    status, run_metadata)\r\n  File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument\r\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\n... <contains sensitive info> ...\r\n\r\nInvalidArgumentError (see above for traceback): /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument\r\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n```\r\n\r\n**Clarification requested by tensorflowbutler**\r\n_Have I written custom code:_\r\nYes, I modified this code (https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py) to work with TensorFlow 1.7 and to use the same embeddings variable for documents as for words with average instead of concatenation. I also updated the saved variables to include nce_weights and nce_biases so that training may be resumed.\r\n_OS Platform and Distribution_\r\nMacOS 10.13.4 (17E199)\r\n_TensorFlow installed from_\r\npip on VirtualEnv, according to instructions (https://www.tensorflow.org/install/install_mac)\r\n_TensorFlow version_\r\n1.7\r\n_Bazel version_\r\nNA\r\n_CUDA/cuDNN version_\r\nNA\r\n_GPU model and memory_\r\nNA\r\n_Exact command to reproduce_\r\nsaver = tf.train.Saver({\"embeddings\": embeddings, \"weights\": nce_weights, \"biases\": nce_biases})\r\nsaver.restore(sess, \"../trained_model/saved_stuff\")", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Ticket updated with requested information. Will use template in the future!", "Possibly related to: https://github.com/tensorflow/tensorflow/issues/18640", "Update: I tried this on Windows as well, and I got rid of the stuff for loading the model graph and used the Saver to explicitly restore my variables. On Windows, it fails for the same reason (because I am loading an embedding larger than 2GB) but with a better error message:\r\n\r\n`OutOfRangeError (see above for traceback): Read fewer bytes than requested\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]`", "@rohan100jain bump :)", "I am using tensorflow-gpu 1.7.0, and meet exactly the same problem when restoring model in distributed tensorflow environment.\r\n\r\nwhen the model file is larger than 3GB, the error below occurs:\r\nInvalidArgumentError (see above for traceback): hdfs://xxxx/model.ckpt-5154361.data-00001-of-00008; Invalid argument\r\n         [[Node: save_1/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](_recv_save_1/Const_0_S7, save_1/RestoreV2_1/tensor_names, save_1/RestoreV2_1/shape_and_slices)]]\r\n         [[Node: save_1/restore_all/NoOp_S10 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:ps/replica:0/task:0/device:GPU:0\", send_device_incarnation=1493071510865629599, tensor_name=\"edge_147_save_1/restore_all/NoOp\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\n\r\n\r\nwhen I decrease the model size, the error changed:\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read less bytes than requested\r\n         [[Node: save_1/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT], _device=\"/job:ps/replica:0/task:1/device:CPU:0\"](_recv_save_1/Const_0_S1, save_1/RestoreV2_3/tensor_names, save_1/RestoreV2_3/shape_and_slices)]]\r\n\r\nList the smaller models:\r\n-rw-r--r--   3 root supergroup   11893084 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00000-of-00008\r\n-rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00001-of-00008\r\n-rw-r--r--   3 root supergroup  106483864 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00002-of-00008\r\n-rw-r--r--   3 root supergroup 5130000440 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.data-00003-of-00008\r\n-rw-r--r--   3 root supergroup     375688 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00004-of-00008\r\n-rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00005-of-00008\r\n-rw-r--r--   3 root supergroup  232909600 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00006-of-00008\r\n-rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00007-of-00008\r\n-rw-r--r--   3 root supergroup       1564 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.index\r\n-rw-r--r--   3 root supergroup     571996 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.meta", "Any updates, @rohan100jain ?", "I think the root cause may be:\r\nin https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/platform/hadoop/hadoop_file_system.cc\r\nline 213:\r\n\r\n`      tSize r = hdfs_->hdfsPread(fs_, file_, static_cast<tOffset>(offset), dst,\r\n                                 static_cast<tSize>(n));\r\n`\r\n\r\nthe last param static_cast<tSize>(n) means the number of bytes required. ie., the tensor's size.\r\nin https://github.com/tensorflow/tensorflow/blob/r1.7/third_party/hadoop/hdfs.h\r\nline 75,\r\n\r\ntypedef int32_t tSize;    /// size of data for read/write io ops\r\n\r\ntSize is int32, but tensor's size is int64, when the tensor is big enough, overflow occurs. \r\n\r\nMy solution is to use partitioned variables for large tensor, then problem solved.\r\n", "But I'm not using HDFS -- this is all on my local SSD.", "I am having a very similar issue as well. Following [this tutorial](https://www.tensorflow.org/tutorials/estimators/cnn), but working with larger images.", "@allenlavoie seems like we're having issues with large restore? I thought the V2 version of the save restore ops was supposed to fix this. Any ideas?", "We can put >2GB in checkpoints now. https://github.com/tensorflow/tensorflow/commit/b8c86c3bbd8271ed968087f24e7fb704103bc733#diff-f4340b63dcfb03e060905682f7471faa fixes an int32 size issue for string dtypes. I don't see a similar issue for Tensors, and this error doesn't look like it's a length/checksum issue. From \"OP_REQUIRES failed at save_restore_v2_ops.cc:184\" and reading the exceptions in the function it's calling, it looks vaguely like this could be complaining about a dtype mismatch? But that could just be the first thing that doesn't match if the whole file is corrupted for some reason.\r\n\r\nCan someone distill their issue into a snippet I can run, e.g. with fill()? I ran the following and it seems to work:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import io_ops\r\n\r\ndef main(_):\r\n  tf.enable_eager_execution()\r\n  large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))\r\n  path = '/some/network/path'\r\n  io_ops.save_v2([path], [\"a\"], [\"\"], [large])\r\n  restored = io_ops.restore_v2(path, [\"a\"], [\"\"], [tf.float32])\r\n  print(tf.reduce_sum(restored))\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n\r\n```\r\n\r\nPrints:\r\n\r\n`tf.Tensor(5e+09, shape=(), dtype=float32)`", "> We can put >2GB in checkpoints now. [b8c86c3#diff-f4340b63dcfb03e060905682f7471faa](https://github.com/tensorflow/tensorflow/commit/b8c86c3bbd8271ed968087f24e7fb704103bc733#diff-f4340b63dcfb03e060905682f7471faa) fixes an int32 size issue for string dtypes. I don't see a similar issue for Tensors, and this error doesn't look like it's a length/checksum issue. From \"OP_REQUIRES failed at save_restore_v2_ops.cc:184\" and reading the exceptions in the function it's calling, it looks vaguely like this could be complaining about a dtype mismatch? But that could just be the first thing that doesn't match if the whole file is corrupted for some reason.\r\n> \r\n> Can someone distill their issue into a snippet I can run, e.g. with fill()? I ran the following and it seems to work:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from tensorflow.python.ops import io_ops\r\n> \r\n> def main(_):\r\n>   tf.enable_eager_execution()\r\n>   large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))\r\n>   path = '/some/network/path'\r\n>   io_ops.save_v2([path], [\"a\"], [\"\"], [large])\r\n>   restored = io_ops.restore_v2(path, [\"a\"], [\"\"], [tf.float32])\r\n>   print(tf.reduce_sum(restored))\r\n> \r\n> if __name__ == '__main__':\r\n>   tf.app.run()\r\n> ```\r\n> Prints:\r\n> \r\n> `tf.Tensor(5e+09, shape=(), dtype=float32)`\r\n\r\nI've tried this snippet on Mac OS X 10.14.1 + TF 1.12.0\r\nGot such error:\r\n\r\n> restored = io_ops.restore_v2(path, [\"a\"], [\"\"], [tf.float32])\r\n> 2018-11-08 17:27:05.767089: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/alex/HDD/Develop/tmp.data-00000-of-00001; Invalid argument\r\n> Traceback (most recent call last):\r\n>   File \"<stdin>\", line 1, in <module>\r\n>   File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1486, in restore_v2\r\n>     ctx=_ctx)\r\n>   File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1511, in restore_v2_eager_fallback\r\n>     attrs=_attrs, ctx=_ctx, name=name)\r\n>   File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n>     six.raise_from(core._status_to_exception(e.code, message), None)\r\n>   File \"<string>\", line 3, in raise_from\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/alex/HDD/Develop/tmp.data-00000-of-00001; Invalid argument [Op:RestoreV2]", "Interesting, thank you for trying that out. So maybe it is a Mac-specific issue.\r\n\r\nJust to check, do you have >20GB of disk and >20GB of RAM free before running that snippet? This could be a badly reported OOM or out-of-disk error I suppose.", "I have about 400Gb free space on disk and total 16Gb of RAM (swapfile ON).\r\nSuch issue occurs even if try to load 2.7Gb variable file.", "Interesting. I'll find a Mac next week and see if I can reproduce on it.", "Finally i found the way how to use save/load embeddings. Variable partitioning is the answer.\r\nWith tf.variable_axis_size_partitioner and max_shard_bytes=2 ** 30 large embeddings loading works well.", "The issue is still there", "Problem still exist when I restore a 18GB model.\r\n\r\nMy OS is Windows Server 2016\r\nTensorflow 1.13.0 installed from pip\r\n512GB memory and hard disk\r\nOnly run on CPU.\r\n\r\nPlease advise.", "I also put the below in my code\r\n\r\ntf.variable_axis_size_partitioner(\r\n    max_shard_bytes=2**35,\r\n    axis=0,\r\n    bytes_per_string_element=16,\r\n    max_shards=None\r\n)\r\n\r\nHowever it still shows below error when restoring the model:\r\n\r\n2019-03-16 08:12:32.430456: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read fewer bytes than requested", "> It has been 17 days with no activity and the `awaiting response` label was assigned. Is this still an issue?\r\n\r\nYes, it is still an issue", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes it\u2019s still an issue", "Finally got a chance to debug this on a mac. Apparently the `pread` system call, despite taking an eight-byte `size_t` for its `nbytes` argument, returns `EINVAL` if \"The sum of the iov_len values in the iov array overflowed a 32-bit integer.\" And presumably `pread` is implemented in terms of `readv` so they have the same limitation.\r\n\r\nI have a change out for review which just limits reads to INT32_MAX on every platform. Seems to work if we do that. I checked that the checkpoints themselves were identical to what gets written on Linux, so existing checkpoints will start working once that change is in.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=18769\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=18769\">No</a>\n", "Uh, this is still an issue? I built the latest version of tensorflow from source and it's still happening! I've tried practically everything. One of the most arcane and infuriating errors I have dealt with...", "@Sam-DevZ operating system and version, TF git version, repro instructions? The [repro I posted](https://github.com/tensorflow/tensorflow/issues/18769#issuecomment-430030868) works for me on `1.14.1-dev20190522` (nightly) and macOS 10.14.2.", "Hi! Your snippet doesn't work either. Code: \r\n\r\n```Python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import io_ops\r\n\r\nprint('Tensorflow version: ' + tf.__version__)\r\nprint('MacOS Version: ' + '10.14.5')\r\n\r\ndef main(_):\r\n  tf.enable_eager_execution()\r\n  large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))\r\n  path = '/Users/clearlycoder/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN'\r\n  io_ops.save_v2([path], [\"a\"], [\"\"], [large])\r\n  restored = io_ops.restore_v2(path, [\"a\"], [\"\"], [tf.float32])\r\n  print(tf.reduce_sum(restored))\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n\r\nOutput: \r\n\r\n```\r\nTensorflow version: 1.12.2\r\nMacOS Version: 10.14.5\r\n2019-05-23 07:16:10.724624: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN.data-00000-of-00001; Invalid argument\r\nTraceback (most recent call last):\r\n  File \"/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py\", line 16, in <module>\r\n    tf.app.run()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py\", line 12, in main\r\n    restored = io_ops.restore_v2(path, [\"a\"], [\"\"], [tf.float32])\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1486, in restore_v2\r\n    ctx=_ctx)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1511, in restore_v2_eager_fallback\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN.data-00000-of-00001; Invalid argument [Op:RestoreV2]\r\n[Finished in 80.3s with exit code 1]\r\n[cmd: ['/Library/Frameworks/Python.framework/Versions/3.6/bin/python3', '-u', '/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py']]\r\n[dir: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN]\r\n[path: /usr/bin:/bin:/usr/sbin:/sbin]\r\n```", "@allenlavoie ", "Oh, er, I'm guessing version 0.12.2 is not recent enough. I need `1.14.1-dev20190522`, right? Haha.", "Sorry! It's definitely fixed. I ran: \r\n\r\n```\r\npip uninstall tensorflow\r\npip install tf-nightly\r\n```\r\n\r\nAnd now it works perfectly. Sorry about that!"]}, {"number": 18768, "title": "Bad error message: InvalidArgumentError with no further info (besides Invalid argument)", "body": "When loading a model saved like the following to resume training (same machine, same version), I get an InvalidArgumentError:\r\n\r\nCode:\r\nsaver = tf.train.Saver({\"embeddings\": embeddings, \"weights\": nce_weights, \"biases\": nce_biases})\r\n\r\nSave:\r\nsaver.save(sess, model_checkpoint_path)\r\n\r\nLoad:\r\nsaver.restore(sess, model_checkpoint_path)\r\n\r\nExpected Output:\r\nSomething that actually tells me what went wrong (even if it is internal stuff, I want to know!)\r\n\r\nActual Output:\r\n```\r\n2018-04-21 22:45:00.143245: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument\r\nTraceback (most recent call last):\r\n  File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1312, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1420, in _call_tf_sessionrun\r\n    status, run_metadata)\r\n  File \"/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument\r\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\n... <contains sensitive info> ...\r\n\r\nInvalidArgumentError (see above for traceback): /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument\r\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n```\r\n\r\n**Clarification requested by tensorflowbutler**\r\n_Have I written custom code:_\r\nYes, I modified this code (https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py) to work with TensorFlow 1.7 and to use the same embeddings variable for documents as for words with average instead of concatenation. I also updated the saved variables to include nce_weights and nce_biases so that training may be resumed.\r\n_OS Platform and Distribution_\r\nMacOS 10.13.4 (17E199)\r\n_TensorFlow installed from_\r\npip on VirtualEnv, according to instructions (https://www.tensorflow.org/install/install_mac)\r\n_TensorFlow version_\r\n1.7\r\n_Bazel version_\r\nNA\r\n_CUDA/cuDNN version_\r\nNA\r\n_GPU model and memory_\r\nNA\r\n_Exact command to reproduce_\r\nsaver = tf.train.Saver({\"embeddings\": embeddings, \"weights\": nce_weights, \"biases\": nce_biases})\r\nsaver.restore(sess, \"../trained_model/saved_stuff\")", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks! I will use the template in the future.", "@tatianashp bump :-)", "Any updates, @tatianashp ?", "@rothn,\r\nSorry for the delayed response. In **`Tensorflow Major Version`** of **`2.x`**, **`saving`** and **`loading`** the **`Models`** can be done using [tf.saved_model.save](https://www.tensorflow.org/api_docs/python/tf/saved_model/save) and [tf.saved_model.load](https://www.tensorflow.org/api_docs/python/tf/saved_model/load) and the error reporting has been improved over the subsequent versions of Tensorflow. So, this issue should no longer persist. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 18767, "title": "Documentation issues regarding installing on ubuntu", "body": "While I was perusing the documentation for install_linux, I got to the section *Determine which TensorFlow to install*, and then I continued on.\r\n\r\nI was at *Determine how to install TensorFlow* and I settled on *Virtualenv*. So I went to that section, and I followed the steps. But when I got to the sub-section (2) stating\r\n\r\n> where `targetDirectory` specifies the top of the Virtualenv tree. Our instructions assume that `targetDirectory` is `~/tensorflow`, but you may choose any directory.\r\n\r\nI was caught off guard a bit. I didn't know if I had missed something, so I spent a bit of time back-tracking the documentation to see if that was the case. I finally decided to `mkdir ~/tensorflow` and give it a go.\r\n\r\nIt would be nice if there was a (sub-)section stating this. I don't want to write it, so I'm making it as an issue instead. I did do this amazing thing though: #18766", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "> Have I written custom code\r\n\r\nNo\r\n\r\n> OS Platform and Distribution\r\n\r\nLinux Mint\r\n\r\n> TensorFlow installed from\r\n\r\nMy issue is regarding the documentation before installation, so I'll say this is N/A.\r\n\r\n> TensorFlow version\r\n\r\nN/A\r\n\r\n> Bazel version\r\n\r\nN/A\r\n\r\n> CUDA/cuDNN version\r\n\r\nN/A\r\n\r\n> GPU model and memory\r\n\r\nN/A\r\n\r\n> Exact command to reproduce\r\n\r\nN/A\r\n\r\nMy issue is simply regarding some confusion I faced during reading the installation documentation.", "thanks for the PR.", "@lamberta maybe see if this can be clarified. But the branching install is going to be some cognitive burden no matter how much we describe the steps.", "I don't disagree that it will be a cognitive burden. (I like that phrase). I mean more in the sense of \"did I miss something?\" and then having to re-read to identify that I did not in fact miss anything.\r\n\r\nOr even something like \"read all the way through before doing anything.\" If I were better with words I would probably be able to come up with something better than that. No words, only code.\r\n\r\nThanks!", "Thanks for reporting.\r\nThe entire page could use a scrubbing, but I went through the pip install instructions using virtualenv and system install and clarified some steps (and cut out some cruft)."]}, {"number": 18766, "title": "update $ source spacing", "body": "When viewing install_linux, the spacing was off for 'Next Steps' section.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "What a weird thing."]}, {"number": 18765, "title": "[INTEL MKL] Fix compilation failure.", "body": "Replaced calls to depreacted method StringPiece::contains with str_util::StrContains.", "comments": ["@martinwicke Thanks"]}, {"number": 18764, "title": "Feature Request: Gradients for angles in tf.contrib.image.rotate()", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Mac OS 10.13\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: tf-nightly | 1.8.0.dev20180328\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n#9423 requested gradients for  tf.contrib.image.rotate(), but the fix #9533 only provided gradients with respect to the image.  I would like to get gradients with respect to the angle. \r\n\r\nHappy to try to try to write a fix, if folks can give an idea for how to start a solution.  ", "comments": ["The short answer is that the output `None` in from the function `_image_projective_transform_grad` in that linked PR should be replaced by the appropriate backwards gradient with respect to the transformation. I'd have to look a little more info exactly what projective transformation is doing to figure out what that gradient is.", "Because `angle` is applied to index, other than value,  I think it's correct to set `None` for angle gradient. The situation is quite like `tf.gather`, whose index grad is always `None` as well.", "I agree that there is no gradient with respect to the angle if doing nearest-neighbor interpolation. But bilinear interpolation is continuous, so the gradient should be well defined.", "@Noahyt is that enough information to get you started?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "It would be an overstatement to say that the gradient is a zero for nearest neighbor. Rather, it is zero almost everywhere :)\r\nI would be interested in this one as well.", "@Noahyt It seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. since contrib has been depreciated in Tensorflow 2.x check the name of the module without the tf.contrib part to know it's new location, also please refer this [link](https://stackoverflow.com/questions/53806057/tf-contrib-image-rotate-causes-error-the-truth-value-of-an-array-with-more-t) ."]}, {"number": 18763, "title": "Multiple Classes fails in Eager Mode (\"tf.keras.Model\")", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo \r\n- **Bazel version**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nTried on MacOS using tensorflow as well as Linux Ubuntu 16.04 using tensorflow-gpu\r\n- **TensorFlow installed from (source or binary)**:\r\nInstalled utilizing pip\r\n- **TensorFlow version (use command below)**:\r\n1.7\r\n- **Python version**: \r\n3.6\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf  \r\nimport tensorflow.contrib.eager as tfe  \r\n\r\ntfe.enable_eager_execution()\r\n\r\nclass CustomLayer(tf.keras.Model):\r\n    def __init__(self):\r\n        super(CustomLayer, self).__init__()\r\n        print(\"blah\")\r\n\r\nclass CustomNetwork(tf.keras.Model):\r\n    def __init__(self):\r\n        super(CustomNetwork, self).__init__()\r\n        self.custom_layers = CustomLayer()\r\n\r\n    def forward(self, x, y=None):\r\n        x = self.custom_layers(x)\r\n\r\nCustomNetwork().forward(tf.convert_to_tensor([1]))\r\n```\r\n\r\n### Describe the problem\r\nTrying to utilize multiple classes fails in tensorflow eager mode utilizing \"tf.keras.Model\". If I change \"tf.keras.Model\" to \"tfe.Network\" it works - keep in mind I am utilizing tensorflow 1.7.  The error I get running the above code results in the error below:\r\n\r\n### Source code / logs\r\n```\r\nblah\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-12-9afa9b91ddef> in <module>()\r\n----> 1 CustomNetwork().forward(tf.convert_to_tensor([1]))\r\n\r\n<ipython-input-11-484119102aec> in forward(self, x, y)\r\n      5 \r\n      6     def forward(self, x, y=None):\r\n----> 7         x = self.custom_layers(x)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)\r\n    237     \"\"\"\r\n    238     # Actually call the layer (optionally building it).\r\n--> 239     output = super(Layer, self).__call__(inputs, **kwargs)\r\n    240     if context.executing_eagerly():\r\n    241       return output\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    712 \r\n    713         if not in_deferred_mode:\r\n--> 714           outputs = self.call(inputs, *args, **kwargs)\r\n    715           if outputs is None:\r\n    716             raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in call(self, inputs, training, mask)\r\n    635     outputs, _ = self._run_internal_graph(inputs,\r\n    636                                           training=training,\r\n--> 637                                           mask=masks)\r\n    638     return outputs\r\n    639 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\r\n    770     # does not return a list the same size as `call`\r\n    771     tensor_map = {}\r\n--> 772     for x, y, mask in zip(self.inputs, inputs, masks):\r\n    773       tensor_map[str(id(x))] = (y, mask)\r\n    774 \r\n\r\nTypeError: zip argument #1 must support iteration\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Believe I need to use .call instead of .forward"]}, {"number": 18762, "title": "static libtensorflow.a crashed on TF_NewGraph()", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nMaster\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n1.10\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nBUILD_ALL_ANDROID_PATH=\"${TF_ROOT_DIR}/tensorflow/contrib/makefile/build_all_android.sh\"\r\n\r\nCC_PREFIX=${CC_PREFIX} NDK_ROOT=${NDK_ROOT} \"${BUILD_ALL_ANDROID_PATH}\" \\\r\n-x \"${GEN_LIBS_DIR}\" \\\r\n-s \"${TF_ROOT_DIR}/tensorflow/contrib/makefile/sub_makefiles/hexagon_graph_execution/Makefile.in\" \\\r\n-t hexagon_graph_execution\r\n```\r\nI am working to create tensorflow using DSP hexagon .\r\nI integrated into my  android application after success compilation libtensorflow-core.a library.\r\nBut on TF_NewGraph() I have crash:\r\n\r\noutput logcat:\r\n`**graph.cc:283 Non-OK-status: status status: Not found: Op type not registered 'NoOp' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process**.`\r\n\r\n\r\nPlease advise how to resolve it", "comments": ["@satok16 -- taking a guess that this is something you would be able to help with?", "small update ,\r\n1. target architecture tensorflow = arm64-v8a.\r\n2. libhexagon_controller.so compiled = android_Release_aarch64.\r\n3. to tensorflow Makefile I added  -ladsprpc otherwise it not compiled.", "It sounds like this may be a general build file question, rather than something directly related to Hexagon. This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear statement of the issue, rather than a problem that will need some debugging first."]}, {"number": 18761, "title": "Tensor Hub generated model- Coreml conversions issue", "body": "I am trying to convert a tensor flow model created using tensor flow hub using the following code. Base model used for the model was inceptionv3\r\n\r\n`\r\n\r\nimport tensorflow as tf\r\ntf_model_path = 'Inception_v3_US_output_graph.pb'\r\nwith open(tf_model_path, 'rb') as f:\r\n    serialized = f.read()\r\ntf.reset_default_graph()\r\noriginal_gdef = tf.GraphDef()\r\noriginal_gdef.ParseFromString(serialized)\r\n\r\n\r\nwith tf.Graph().as_default() as g:\r\n    tf.import_graph_def(original_gdef, name='')\r\n    ops = g.get_operations()\r\n    N = len(ops)\r\n    for i in [0,1,2,N-3,N-2,N-1]:\r\n        print('\\n\\nop id {} : op type: \"{}\"'.format(str(i), ops[i].type));\r\n        print('input(s):'),\r\n        for x in ops[i].inputs:\r\n            print(\"name = {}, shape: {}, \".format(x.name, x.get_shape())),\r\n        print('\\noutput(s):'),\r\n        for x in ops[i].outputs:\r\n            print(\"name = {}, shape: {},\".format(x.name, x.get_shape())),`\r\n\r\n**This generated following output** \r\n\r\n> op id 0 : op type: \"Placeholder\"\r\ninput(s):\r\n\r\n>output(s):\r\nname = Placeholder:0, shape: (?, 224, 224, 3),\r\n\r\n\r\n>op id 1 : op type: \"Const\"\r\ninput(s):\r\n\r\n>output(s):\r\nname = module/conv0/weights:0, shape: (3, 3, 3, 32),\r\n\r\n\r\n>op id 2 : op type: \"Const\"\r\ninput(s):\r\n\r\n>output(s):\r\nname = module/conv0_bn/gamma:0, shape: (32,),\r\n\r\n\r\n>op id 3236 : op type: \"MatMul\"\r\ninput(s):\r\n>name = input/BottleneckInputPlaceholder:0, shape: (?, 1056), \r\nname = final_retrain_ops/weights/final_weights/read:0, shape: (1056, 2), \r\n\r\n>output(s):\r\nname = final_retrain_ops/Wx_plus_b/MatMul:0, shape: (?, 2),\r\n\r\n\r\n>op id 3237 : op type: \"Add\"\r\ninput(s):\r\n>name = final_retrain_ops/Wx_plus_b/MatMul:0, shape: (?, 2), \r\n>name = final_retrain_ops/biases/final_biases/read:0, shape: (2,), \r\n\r\n>output(s):\r\n>name = final_retrain_ops/Wx_plus_b/add:0, shape: (?, 2),\r\n\r\n\r\n>op id 3238 : op type: \"Softmax\"\r\ninput(s):\r\n>name = final_retrain_ops/Wx_plus_b/add:0, shape: (?, 2), \r\n\r\n>output(s):\r\nname = final_result:0, shape: (?, 2),\r\n\r\n**Then I tried to convert this to a coreml model using the following code** \r\n\r\n`import tfcoreml\r\n\r\ninput_tensor_shapes = {\"Placeholder:0\":[1,224,224,3]} # batch size is 1\r\n\r\nimage_input_name = ['Placeholder:0']\r\n\r\ncoreml_model_file = 'Inception_v3_US_output_graph.pb'\r\n\r\noutput_tensor_names = ['final_result:0']\r\n\r\nclass_labels = 'Inception_v3_US_output_labels.txt'\r\n\r\n\r\ncoreml_model = tfcoreml.convert(\r\n        tf_model_path=tf_model_path,\r\n        mlmodel_path=coreml_model_file,\r\n        input_name_shape_dict=input_tensor_shapes,\r\n        output_feature_names=output_tensor_names,\r\n        image_input_names = image_input_name,\r\n        class_labels = class_labels)`\r\n\r\n**But ended up getting the following error. Following are the last few lines of the error message** \r\n\r\n> 2308/3239: Converting op name: Placeholder ( type:  Placeholder )\r\nSkipping name of placeholder\r\n2309/3239: Converting op name: module_apply_default/hub_input/Mul ( type:  Mul )\r\n2310/3239: Converting op name: module_apply_default/hub_input/Sub ( type:  Sub )\r\n2311/3239: Converting op name: module_apply_default/conv0/Conv2D ( type:  Conv2D )\r\n2312/3239: Converting op name: module_apply_default/conv0_bn/FusedBatchNorm ( type:  FusedBatchNorm )\r\n2313/3239: Converting op name: module_apply_default/cell_stem_1/Relu ( type:  Relu )\r\n2314/3239: Converting op name: module_apply_default/cell_stem_1/Pad ( type:  Pad )\r\n2315/3239: Converting op name: module_apply_default/cell_stem_1/strided_slice ( type:  StridedSlice )\r\n---------------------------------------------------------------------------\r\n>AssertionError                            Traceback (most recent call last)\r\n<ipython-input-8-5c93b11d7780> in <module>()\r\n     18         output_feature_names=output_tensor_names,\r\n     19         image_input_names = image_input_name,\r\n---> 20         class_labels = class_labels)\r\n\r\n>~/tfcoreml/tf-coreml/tfcoreml/_tf_coreml_converter.py in convert(tf_model_path, mlmodel_path, output_feature_names, input_name_shape_dict, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, predicted_probabilities_output)\r\n    491       class_labels=class_labels,\r\n    492       predicted_feature_name=predicted_feature_name,\r\n--> 493       predicted_probabilities_output=predicted_probabilities_output)\r\n\r\n>~/tfcoreml/tf-coreml/tfcoreml/_tf_coreml_converter.py in _convert_pb_to_mlmodel(tf_model_path, mlmodel_path, output_feature_names, input_name_shape_dict, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, predicted_probabilities_output)\r\n    289   context.input_feed_dict = input_feed_dict\r\n    290   context.skip_ops = skip_ops\r\n--> 291   convert_ops_to_layers(context)\r\n    292   sess.close()\r\n    293 \r\n\r\n>~/tfcoreml/tf-coreml/tfcoreml/_ops_to_layers.py in convert_ops_to_layers(context)\r\n    146         print('%d/%d: Converting op name: %s ( type:  %s )' % (\r\n    147             i+1, len(context.all_ops), op.name, op.type))\r\n--> 148         translator(op, context)\r\n    149       connect_skipped_ops(context)\r\n\r\n>~/tfcoreml/tf-coreml/tfcoreml/_layers.py in strided_slice(op, context)\r\n   1184     skip(op,context)\r\n   1185   else:\r\n-> 1186     assert False, 'Strided Slice case not handled'\r\n   1187   context.translated[output_name] = True\r\n   1188 \r\n\r\n# AssertionError: Strided Slice case not handled\r\n\r\nHow can I fix this?\r\n\r\n@aseemw", "comments": ["@johnyquest7 Kindly open an issue on the [tfcoreml](https://github.com/tf-coreml/tf-coreml) repo and close this one. \r\n\r\nMeanwhile, \r\n- can you update the tfcoreml to latest master and see if you still get the same error message\r\n- The error is raised while converting the op named \"module_apply_default/cell_stem_1/strided_slice\". Can you execute the .pb graph and get the inputs and outputs of this op? That way we can find the configuration in which this op is being used. This would tell whether there is a bug in the converter or this particular configuration of strided slice is not supported by CoreML.  ", "Thanks @aseemw for pointing me in the right direction. Will close the issue. ", "![stide_slice](https://user-images.githubusercontent.com/22123236/39091914-eb838e2c-45c4-11e8-888b-8c261bf3e277.jpeg)\r\n"]}, {"number": 18760, "title": "Update .gitignore for cmake generated files", "body": "After running cmake on Linux with:\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh CMAKE tensorflow/tools/ci_build/builds/cmake.sh\r\n```\r\n\r\nthe following file is left:\r\n```\r\nubuntu@ubuntu:~/tensorflow$ git status\r\nOn branch master\r\nYour branch is up-to-date with 'origin/master'.\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n\r\n        api_init_files_list.txt\r\n\r\nnothing added to commit but untracked files present (use \"git add\" to track)\r\nubuntu@ubuntu:~/tensorflow$\r\n```\r\n\r\nThis fix updates the .gitignore file so that cmake generated files\r\nis not added with git inadvertently.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18759, "title": "Add Pull Request Template ", "body": "Create PR Template to make easier to contribute to this repo and easier to check if the pull request is consistent and followed all guidelines.\r\n\r\nIf this repo had my pull request, this commit would be like this:\r\n\r\n<!-- tensorflow pull request template -->\r\n\r\n##### Pull Request Checklist\r\n<!-- Before sending your pull requests, make sure you followed this list and tick all items -->\r\n- [x] Read [contributing guideline](CONTRIBUTING.md).\r\n- [x] Read [code of conduct](CODE_OF_CONDUCT.md).\r\n- [x] Fill [Contributor License Agreement (CLA)](https://cla.developers.google.com/).\r\n- [x] Check if my changes are consistent with the [guidelines](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#general-guidelines-and-philosophy-for-contribution).\r\n- [x] Changes are consistent with the [Coding Style](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#c-coding-style)\r\n- [x] Run [Unit Tests](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#running-unit-tests).\r\n\r\n##### Issue Fix\r\n<!-- Does this pull request fix any issue ? -->\r\n- [ ] Yes\r\n- [x] No\r\n\r\nFixed issue: none\r\n\r\n##### Description\r\n<!-- Detailed description of what you've done -->\r\n\r\nAdd pull request template ...", "comments": ["I like this. @ewilderj, @yifeif, WDYT?", "Thank you for your reply @martinwicke.\r\n\r\nI think having a PR template it's good to standardize PR and consequently save time of contributors and reviewers.\r\n\r\nThis one is just a first suggestion and can be improved if necessary.", "This is great. Thanks @filipefilardi!", "I think this is a good start, thank you!\r\n\r\nHowever, some of the things apply only to the first time, so you may want to separate it out, because after your first time the CLA will be on file, etc, etc.. If you think about the usability of a PR comment, it's a bad idea to put the description and details right at the bottom, because it will take more time for others to be able to read it.\r\n\r\nI suggest instead of a new file, you suggest this checklist in a section at the top of CONTRIBUTING.md, where it will act as a kind of table of contents/TL;DR for that file. Once people have been through it a few times, it won't be necessary, and I am loathe to add the extra noise to PR descriptions.\r\n", "Many thanks @ewilderj \r\n\r\nI made the changes as suggested and i do believe this might be better.", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18758, "title": "nightly builds no available for macos", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS HIgh Sierra\r\n- **TensorFlow installed from (source or binary)**: Pypi\r\n- **TensorFlow version (use command below)**: nightly build\r\n- **Python version**:  3.6\r\n\r\n### Describe the problem\r\nTensorflow nightly builds are not available for macOS since March 30.  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Working on this asap. Thanks @nthakor for filing.", "This is [fixed](https://pypi.org/project/tf-nightly/1.9.0.dev20180428/#files) now. Python binaries for 2.7, 3.3-3.6 are up."]}, {"number": 18757, "title": "Fix typo", "body": "There was a typo in comment.", "comments": []}, {"number": 18756, "title": "model_to_estimator custom Keras layers", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: nightly\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: nightly/docker\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n`tf.estimator.model_to_estimator` doesn't support custom Keras layers in the model:\r\n`ValueError: Unknown layer: MyCustomLayer` \r\n\r\n/cc @fchollet\r\n", "comments": ["Is it related to https://github.com/keras-team/keras/issues/8880?"]}, {"number": 18755, "title": "tf.profile.ProfileOptionBuilder.trainable_variables_parameter errors with pooling layer", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: GPU 1.4.1\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA8, cuDNN 6.0.20\r\n- **GPU model and memory**: Tesla P4\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\ntf.profile.ProfileOptionBuilder.trainable_variables_parameter() errors with max_pooling layer.\r\n\r\n### Source code / logs\r\n```python\r\n    opts = (tf.profiler.ProfileOptionBuilder(\r\n         tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())\r\n         .with_node_names(hide_name_regexes=['.*pool.*'])\r\n         .build())\r\n     params_stats = tf.profiler.profile(\r\n         graph,\r\n         options=opts\r\n     )   \r\n     logger.info('Total params: {}'.format(params_stats.total_parameters))\r\n \r\n```\r\nThe code above works ok for [Resnet](https://github.com/tensorflow/models/blob/r1.4.0/official/resnet/resnet_model.py), but when I want to profile my vgg_net or shuffle_net, it crashes with:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"model_analysis.py\", line 50, in <module>\r\n    profile_model_params(graph)\r\n  File \"model_analysis.py\", line 25, in profile_model_params\r\n    options=opts\r\n  File \"/home/ymwan/python3/lib/python3.5/site-packages/tensorflow/python/profiler/model_analyzer.py\", line 312, in profile\r\n    graph, op_log, run_meta, add_trace=cmd == 'code')\r\n  File \"/home/ymwan/python3/lib/python3.5/site-packages/tensorflow/python/profiler/tfprof_logger.py\", line 146, in _merge_default_with_oplog\r\n    graph, run_meta, add_trace=add_trace, add_trainable_var=add_trainable_var)\r\n  File \"/home/ymwan/python3/lib/python3.5/site-packages/tensorflow/python/profiler/tfprof_logger.py\", line 88, in _get_logged_ops\r\n    graph, op.node_def, REGISTERED_FLOP_STATS)\r\n  File \"/home/ymwan/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2362, in get_stats_for_node_def\r\n    result = stats_func(graph, node)\r\n  File \"/home/ymwan/python3/lib/python3.5/site-packages/tensorflow/python/profiler/internal/flops_registry.py\", line 376, in _max_pool_grad_flops\r\n    max_pool_ops = kernel_area * orig_out_shape.num_elements()\r\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\r\n```\r\nThis problem seems related to 'max_pool2d' (and error says) as ResNet doesn't have this problem.\r\nFrom my code you can see `hide_name_regexes`, try to skip this problem but failed.\r\nHere, someone else also report this issue: https://github.com/vahidk/TensorflowFramework/issues/2", "comments": ["Nagging Assignee @tatianashp: It has been 154 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@irmowan So sorry this fell through the cracks. I am closing the issue since it has been a long time. \r\nIf this is still a problem for you please reopen and post the results from running with the latest version of TensorFlow. "]}, {"number": 18754, "title": "Fix numerical warning in the Iris example", "body": "When running the script [iris_monitors.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/monitors/iris_monitors.py), the following message is prompted: `WARNING:tensorflow:float64 is not supported by many models, consider casting to float32`\r\n\r\nThe [numpy](https://docs.scipy.org/doc/numpy-1.14.0/user/basics.types.html) data type `float` is shorthand  for `float64`. So, `float32` is explicitly specified as solution.\r\n\r\n\r\n", "comments": []}, {"number": 18753, "title": "Fix tutorials directories are not Python packages", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 18752, "title": "install_java command line error", "body": "", "comments": []}, {"number": 18751, "title": "Tensorflow_lite Android Demo compiled failed on Windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n\r\n### Describe the problem\r\n\r\nI download the tensorflow , use the git clone today.\r\nAnd when i open the android studio, and open the tensorflow_lite demo(contrib/lite/java/demo).\r\nsome errors occur.\r\n\r\n\r\n### Source code / logs\r\n\r\nsome errors occured on my android studio console:  Error:Could not find tensorflow-lite.jar (org.tensorflow:tensorflow-lite:0.1.8-rc1).\r\nSearched in the following locations:\r\n    https://jcenter.bintray.com/org/tensorflow/tensorflow-lite/0.1.8-rc1/tensorflow-lite-0.1.8-rc1.jar\r\n\r\nConsult IDE log for more details (Help | Show Log)\r\n\r\n", "comments": ["I also have same error.... it worked well few days ago...weird!", "+1", "Same. Please fix this. I cant even fetch the dependency", "the same to me", "I replaced `org.tensorflow:tensorflow-lite:+` with `org.tensorflow:tensorflow-lite:+@aar` in the gradle.\r\nIt worked.", "@sokunmin, do you have insight in what that does? Should we do this for all platforms and not just on Windows?", "Hi @1icas, could you try again? Sorry that we had a bad push last Friday (mismatched artifact name) and this error was expected. But we immediately fixed the repo during the weekend. ", "@aselle I used it on Mac OS X, and encountered the same problem. I was happened to find .aar in https://jcenter.bintray.com/org/tensorflow/tensorflow-lite/0.1.7/, and it worked for me since I changed to aar. Yes, it should work for all platforms. \r\n\r\n@isaisachen I tried and it's still not working. \r\n\r\nThanks.\r\n\r\n", "@sokunmin Did you try on Mac for the first time today or you tried last Friday as well?\r\n\r\nThe demo app configured below\r\n    compile 'org.tensorflow:tensorflow-lite:+'\r\n1st step Android Studio does is downloading \r\nhttps://google.bintray.com/tensorflow/org/tensorflow/tensorflow-lite/maven-metadata.xml\r\nwhich tells you what's the latest version.\r\n\r\nAnd we once had a broken 0.1.8-rc1 published. \r\n\r\nIf you don't clear the project / gradle local cache, it will stick with 0.1.8-rc1 without re-consulting maven-metadata.xml (which is fixed now w/ all 0.1.8-rc* removed).\r\n\r\nYou can try deleting ~/.gradle/caches on Mac OS and clean / rebuild your Android project.\r\n", "@isaisachen Hey, it works like a charm now since I deleted the caches folder using `org.tensorflow:tensorflow-lite:+`. I did try today and last week. Thanks for your great help!", "I'm very sorry reply so late. I solve this problem use another method. I download the tensorflow-lite.aar and add this aar to this demo(also need modified the build.gradle). ", "Sovled", "> \r\n> \r\n> I'm very sorry reply so late. I solve this problem use another method. I download the tensorflow-lite.aar and add this aar to this demo(also need modified the build.gradle).\r\n\r\n@1icas I have the same problem, the other solutions proposed above don't work for me.\r\n\r\nWhat line did you edit in the build.gradle? and after downloading the tensorflow-lite.aar, where did you add it in the project?", "> @sokunmin Did you try on Mac for the first time today or you tried last Friday as well?\r\n> \r\n> The demo app configured below\r\n> compile 'org.tensorflow:tensorflow-lite:+'\r\n> 1st step Android Studio does is downloading\r\n> https://google.bintray.com/tensorflow/org/tensorflow/tensorflow-lite/maven-metadata.xml\r\n> which tells you what's the latest version.\r\n> \r\n> And we once had a broken 0.1.8-rc1 published.\r\n> \r\n> If you don't clear the project / gradle local cache, it will stick with 0.1.8-rc1 without re-consulting maven-metadata.xml (which is fixed now w/ all 0.1.8-rc* removed).\r\n> \r\n> You can try deleting ~/.gradle/caches on Mac OS and clean / rebuild your Android project.\r\n\r\nI am wondering where should I palce the new xml file ?"]}, {"number": 18750, "title": "tensorflow debugger doesn't work while exceuting session run", "body": "### Describe the problem\r\nI ran tensorflow debugger using the command \"python ***.py --debug\" but got the following error, and python stops working.\r\n\r\ntfdbg> run\r\n2018-04-21 16:33:11.682709: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\debug\\debug_graph_utils.cc:240] For debugging, tfdbg has set the parallel_iterations attribute of all scheduled Enter/RefEnter nodes to 1. (This does not affect subsequent non-debug runs.)\r\n2018-04-21 16:33:15.836838: F C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\debug\\debug_io_utils.cc:623] Non-OK-status: env->NewWritableFile(file_path, &f) status: Not found: Failed to create a NewWriteableFile: C:\\Users\\CHENGC~1\\AppData\\Local\\Temp\\tfdbg_z4heiebg/_tfdbg_device_,job_localhost,replica_0,task_0,device_CPU_0/bboxes_matching_batch_dict/bboxes_matching_batch_6/map/while/bboxes_matching_single/while/bboxes_jaccard/strided_slice_8/stack_0_DebugIdentity_1524299595833304 : \u7cfb\u7edf\u627e\u4e0d\u5230\u6307\u5b9a\u7684\u8def\u5f84\u3002\r\n; No such process", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 19 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I met same question.Have you solved it?Could you please give any guidance? @ccyyy Thanks a lot.", "I had a similar problem and I was running this on Windows. Looks like when the code in debug_io_utils.cc is creating a file, if the file path goes beyond 256 characters, it throws this issue as Windows did not allow it to create a file in the path mentioned. \r\n\r\nI changed the setting so that Windows allow file path beyond 256 characters: https://www.howtogeek.com/266621/how-to-make-windows-10-accept-file-paths-over-260-characters/  and it worked.\r\n\r\nSince the file path mentioned above (C:\\Users\\CHENGC~1\\AppData\\Local\\Temp\\tfdbg_z4heiebg/tfdbg_device,job_localhost,replica_0,task_0,device_CPU_0/bboxes_matching_batch_dict/bboxes_matching_batch_6/map/while/bboxes_matching_single/while/bboxes_jaccard/strided_slice_8/stack_0_DebugIdentity_1524299595833304 ) is beyound 256 characters, it might be throwing this issue.\r\n\r\nHope this helps!!!"]}, {"number": 18749, "title": "tf.tile not working with multiples of type tf.int64", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4 LTS\r\n- **TensorFlow installed from (source or binary)**: Compiled from Source\r\n- **TensorFlow version (use command below)**: tf.VERSION = 1.2.1\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: cuda-9.0\r\n- **GPU model and memory**: GeForce 940M 2002MiB\r\n- **Exact command to reproduce**: Detailed Below.\r\n\r\nThe following program throws an error saying `No OpKernel was registered to support Op 'Tile' with these attrs.`\r\n```\r\nmin_rating = tf.constant(0, tf.int64)\r\nmax_rating = tf.constant(12, tf.int64)\r\nm = max_rating - min_rating + 1\r\nk = tf.range(m, dtype=tf.int64)\r\nd = tf.tile(k, tf.to_int64(tf.reshape(m, [1])))\r\n\r\nwith tf.Session() as sess:\r\n    a = sess.run([d])\r\n    print a\r\n```\r\nHere is the detailed log:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 16, in <module>\r\n    a = sess.run([d])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Tile' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n  device='CPU'; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_COMPLEX128]; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_COMPLEX64]; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_INT16]; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_HALF]; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_DOUBLE]; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_FLOAT]; Tmultiples in [DT_INT32]\r\n\r\n\t [[Node: Tile = Tile[T=DT_INT64, Tmultiples=DT_INT64](range, Reshape)]]\r\n\r\nCaused by op u'Tile', defined at:\r\n  File \"test.py\", line 7, in <module>\r\n    d = tf.tile(k, tf.to_int64(tf.reshape(m, [1])))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3740, in tile\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2583, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Tile' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\r\n  device='CPU'; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_COMPLEX128]; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_COMPLEX64]; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_INT16]; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_HALF]; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_DOUBLE]; Tmultiples in [DT_INT32]\r\n  device='GPU'; T in [DT_FLOAT]; Tmultiples in [DT_INT32]\r\n\r\n\t [[Node: Tile = Tile[T=DT_INT64, Tmultiples=DT_INT64](range, Reshape)]]\r\n```\r\nHowever this is working correctly:\r\n```\r\nmin_rating = tf.constant(0, tf.int64)\r\nmax_rating = tf.constant(12, tf.int64)\r\nm = max_rating - min_rating + 1\r\nk = tf.range(m, dtype=tf.int64)\r\nd = tf.tile(k, tf.to_int32(tf.reshape(m, [1])))\r\n\r\nwith tf.Session() as sess:\r\n    a = sess.run([d])\r\n    print a\r\n```\r\nThe [tile docs](https://www.tensorflow.org/api_docs/python/tf/tile) says that the `multiples` argument to `tf.tile` can be of type `int32` or `int64`.", "comments": ["Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This should be fixed in more recent versions of TensorFlow. I tested the code and it appears to be working."]}, {"number": 18748, "title": "build_all_android.sh x86 abi build issue", "body": "It took a bit of effort to get the build_all_android.sh script to work.  The effort was primarily due to the same issues encountered here: https://github.com/tensorflow/tensorflow/issues/14186\r\nArmed with the work around outlined there, I modified compile_nsync.sh to have \r\nAR := ${NDK_ROOT}/toolchains/'\"$toolchain\"'/prebuilt/'\"$android_os_arch\"'/bin/'\"$bin_prefix\"'-ar\r\nThis does result in being able to build the arm variants (armeabi, armeabi-v7a and arm64-v8a) as well as the x86_64 variant.  However, x86 variant fails with:\r\n/Users/swinston/Downloads/android-ndk-r12b/platforms/android-21/arch-x86/usr/lib/crtend_android.o\r\n/tmp/77a95b0085967f7191ad958665724b6f/sysroot/usr/include/unistd.h:173: error: undefined reference to '__page_size'\r\n/tmp/77a95b0085967f7191ad958665724b6f/sysroot/usr/include/unistd.h:173: error: undefined reference to '__page_size'\r\n/tmp/77a95b0085967f7191ad958665724b6f/sysroot/usr/include/unistd.h:173: error: undefined reference to '__page_size'\r\n/tmp/77a95b0085967f7191ad958665724b6f/sysroot/usr/include/unistd.h:173: error: undefined reference to '__page_size'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nThis is a OSX build host and using ndk r12b.\r\n\r\nNB: count me amongst those eager for r16/r17 ndk support!\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: Yes, as outlined in the original post.\r\nOS Platform and Distribution: OSX 10.13.4\r\nTensorFlow installed from: N/A\r\nTensorFlow version: master\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: See original post.", "Could you provide your modifications as a PR, so we can take a look at what's going on.", "PR: #18866 is all I changed.  Without that change I had the same issues building as the above referenced issue.", "I have the same problem and error message, you solved it now?", "Nope.  Still not working.\n\nOn Tue, Apr 16, 2019, 8:04 PM kunkun007 <notifications@github.com> wrote:\n\n> I have the same problem and error message, you solved it now?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/18748#issuecomment-483917977>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADqAY3xKagkgcXAWz4-ZHTNMVo3v2vQkks5vho8vgaJpZM4TePqe>\n> .\n>\n", "@gpx1000 ,\r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/18748\">No</a>\n"]}, {"number": 18747, "title": "[Cherrypick] Fix critical loss computation bug in Model training/eval with eager execution", "body": "Fixes #18642.\r\n\r\nPiperOrigin-RevId: 193423288", "comments": []}, {"number": 18746, "title": "Branch 193740595", "body": "", "comments": ["I saw the same failure during pull, and both internal and external continuous builds are green. Looks like some changes are not happy merged together.", "Ah #18434 removed a call that was used in cl/193669636. Trying again with a fix."]}, {"number": 18745, "title": "[Intel MKL] Adding support for MKL to docker CI infrastructure. ", "body": "Creating a new PR because rebasing #18020 was a mess. See comments there. ", "comments": ["Nagging Reviewer @case540: It has been 17 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "@case540 Don't bother. As you requested, I'm designing a new solution using Dockerfile ARG and --build-arg rather than sed. ", "Nagging Reviewer @case540: It has been 16 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @case540: It has been 41 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Closing because https://github.com/tensorflow/tensorflow/pull/20164 was merged"]}, {"number": 18744, "title": "TensoRT SSD layers", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes I have create a custom class to load tensorflow zoo object detection models\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux xenial\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:\r\n1.7\r\n- **Python version**: 2.7 \r\n- **Bazel version (if compiling from source)**: 0.10\r\n- **GCC/Compiler version (if compiling from source)**: 4\r\n- **CUDA/cuDNN version**: 7 tegra\r\n- **GPU model and memory**: jetson tx2\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nTensorRT with any kind of SSD models is breaking tensorflow because of the ones layer not implemented. Any guideline on how to implement. \r\n### Source code / logs\r\n\r\nusing frozen_graph.pb of mobilnet and fastrcnn\r\nTraceback (most recent call last):\r\n  File \"tests/trt_od.py\", line 107, in <module>\r\n    minimum_segment_size = 2,\r\n  File \"/home/nvidia/Documents/tfinterface/tfinterface/estimator/getters.py\", line 38, in get\r\n    return cls(model_path, *args, **kwargs)\r\n  File \"tests/trt_od.py\", line 53, in __init__\r\n    **trt_ops\r\n  File \"/home/nvidia/.local/lib/python2.7/site-packages/tensorflow/contrib/tensorrt/python/trt_convert.py\", line 115, in create_inference_graph\r\n    int(msg[0]))\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:\r\n         [[Node: ones = Fill[T=DT_INT32](strided_slice_14, ones/Const)]]\r\n         [[Node: ones = Fill[T=DT_INT32](strided_slice_14, ones/Const)]]\r\n.\r\n", "comments": ["I guess you need to implement the tensorrt layer for ssd with NVPlugin, then try to integrate into tf", "@ouceduxzk But is not supposed that if the layer is not supported in tensorrt it is normally executed in tensorflow ? Am I mistaken?", "@Davidnet  I did not understand what you said, basically tensorrt support layers that tf have, but if tensorflow does not have, if you still want to use tensorrt, you can use the plugin feature to impl yourself.", "@ouceduxzk From what I understood from this [article](https://devblogs.nvidia.com/tensorrt-integration-speeds-tensorflow-inference/) if there are layers or operations in the graph that are not implemented in tensorrt, they are normally executed in tensorflow, so that layer should be executed normally in tensorflow right ?", "@samikama Could you please take a look.", "Hi, @Davidnet \r\nThere are some problems with shape inference in object detection models and some unsupported ops. We are trying to solve these issues. Support of SSD will increase with TRT4.0 and upcoming TF releases. Sorry for the inconvenience.", "@Davidnet,\r\n\r\nI am closing this since we can't handle SSD networks yet. When we switch to TRT4.0, support for SSD will increase.\r\n\r\nSami\r\n", "Is this supported now? I am facing the following error when I tried to create tensorrt optimized graph using\r\n`import tensorflow.contrib.tensorrt as trt`\r\n\r\n`trt_graph = trt.create_inference_graph(graph_def, ['num_detections', 'detection_boxes', 'detection_scores', 'detection_classes'], max_batch_size=batch_size, precision_mode=precision_mode)` for ssd_mobilenet_v1_coco from object detection model zoo.\r\n\r\n\r\n`tensorflow.python.framework.errors_impl.NotFoundError: No attr named 'identical_element_shapes' in NodeDef:\r\n\t [[Node: Preprocessor/map/TensorArray_1 = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=<unknown>, tensor_array_name=\"\"](Preprocessor/map/TensorArrayUnstack/strided_slice)]] for 'Preprocessor/map/TensorArray_1' (op: 'TensorArrayV3') with input shapes: [].\r\n`", "Just to clarify, this seems to be a bug in the tf-trt integration, where it include some nodes in the trt subgraph which caused the problem. In general, the integration should not put the node into the segment before the it is confident enough to handle it. This issue will hopefully be fixed by #19871. Meanwhile we're still finding the root cause, so I'm reopening this one.", "Nagging Assignee @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aaroey: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @aaroey  @samikama \r\nI am still getting the same error although I built tensorflow Master a few days ago from source.\r\nI am using TensorRT 4.0, Cuda 9.0, Cudnn 7.1.4\r\n\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:\r\n\t [[{{node MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones}} = Fill[T=DT_INT32](MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, MultipleGridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]\r\n\r\nAny guideline for it?\r\nRegards", "This is fixed by #21100, now if TRT op fails it will fall back to execute the native tf function.", "Hi @aaroey @samikama\r\nI am still getting the same error although i built tensorflow v1.10.1 from source.\r\ntensorflow.python.framework.errors_impl.NotFoundError: No attr named 'index_type' in NodeDef:\r\n         [[Node: model_1/forward_1_batchnorm/ones_like = Fill[T=DT_FLOAT](model_1/forward_1_batchnorm/ones_like/Shape, model_1/forward_1_batchnorm/ones_like/Const)]]", "@anhtu812 sometimes between different TF versions schema change and frozen graph needs to be updated. Did you freeze the graph with same tensorflow or are you loading it from a file saved by an earlier revision? If it is the second case, could you please try loading and saving frozen graph def to update it to current schema? If not, some fixes might have missed 1.10 cutoff and may require you to try master.", "thanks @samikama, i re-freeze  with new version tensorflow, it have not this error."]}, {"number": 18743, "title": "r1.8 cherry-pick request: Fix CheckpointSaverHook to properly save every save_checkpoints_steps", "body": "cc @chrisying", "comments": ["@yifeif I think you'll have to merge this one as I don't have permission. "]}, {"number": 18742, "title": "Roll forward the custom optimizers change", "body": "@samikama ", "comments": []}, {"number": 18741, "title": "TensorflowJS does not work on mobile Chrome browser. [not sure it is right place]", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\nSorry it is a tensorflowJS related problem but tensorflowJS does not support submit an issue?\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nhttps://storage.googleapis.com/tfjs-examples/mobilenet/dist/index.html\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nAndroid 6.0.1\r\nChrome 65.0.3325.109\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\ntfjs-example-mobilenet.js\r\n\r\n- **TensorFlow version (use command below)**:\r\nn/a\r\n\r\n- **Python version**: \r\nn/a\r\n\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nn/a\r\n\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n\r\n- **GPU model and memory**:\r\nn/a\r\n\r\n- **Exact command to reproduce**:\r\nn/a\r\n\r\n\r\n### Describe the problem\r\nTensorflowJS does not work on mobile Chrome browser.\r\nvisit https://storage.googleapis.com/tfjs-examples/mobilenet/dist/index.html on Android Chrome and wait for finishing model loading then it will use default cat.jpg to do a demo predict.\r\n```\r\nuse default cat.jpg,\r\nAndroid Chrome output:\r\ncoyote, prairie wolf, brush wolf, Canis Iatrans 0.075\r\nEgyptian cat 0.074\r\nhare 0.049\r\nwood rabbit, cottontail, conttontail rabbit 0.033\r\n...\r\n```\r\n\r\nrun on for example MacOSX 10.12 Chrome 65.0.3328.181 output:\r\n```\r\nEgyptian cat\t0.757\r\ntabby, tabby cat\t0.076\r\nSiamese cat, Siamese\t0.058\r\ntiger cat\t0.021\r\nlynx, catamount\t0.015\r\n...\r\n```\r\n\r\n![cat.jpg](https://storage.googleapis.com/tfjs-examples/mobilenet/dist/ed21e89820228ec168bfdf72fb128449.jpg \"cat.jpg\")\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Ah, the correct location for this issue is https://github.com/tensorflow/tfjs/issues  @dna2github can you close this issue and open a new one there?   Thanks!", "@bileschi thx and will close this issue."]}, {"number": 18740, "title": "Frequent \"Premature EOF\" error in LLVM archive when running bazel build", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nPatches against tf dev tree\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource \r\n- **TensorFlow version (use command below)**:\r\n1.6.0rc1 (last upstream commit 0abc4c9ecae912676f6070ca4b76b35c80351c26)\r\n- **Python version**: \r\nn/a\r\n- **Bazel version (if compiling from source)**:\r\n0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\nFrom the dockerfile:\r\n\r\n\r\n`\r\nENV GCC_HOST_COMPILER_PATH \"/usr/bin/gcc\"\r\nENV PYTHON_BIN_PATH \"/usr/bin/python3\"\r\nENV PYTHON_LIB_PATH \"/usr/lib/python3/dist-packages\"\r\nENV CC_OPT_FLAGS \"-march=native\"\r\n\r\nENV TF_NEED_GCP 0\r\nENV TF_NEED_GDR 0\r\nENV TF_NEED_HDFS 0\r\nENV TF_NEED_MKL 0\r\nENV TF_NEED_MPI 0\r\nENV TF_NEED_OPENCL 0\r\nENV TF_NEED_CUDA 0\r\nENV TF_ENABLE_XLA 1\r\nENV TF_NEED_JEMALLOC 1\r\nENV TF_NEED_VERBS 0\r\nENV TF_NEED_S3 0\r\n\r\nWORKDIR /tmp/tensorflow-knureon\r\n\r\nRUN ./configure \\\r\n\t&& bazel build --config=opt --config=kpi --copt=-mfma --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package \\\r\n\t&& bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n`\r\n\r\n\r\n### Describe the problem\r\nAbout 60% of the time I try from console and 100% of the time our CI system attempts to build TensorFlow, this error is observed, always related to the LLVM archive:\r\n\r\n`\r\n20-Apr-2018 11:43:32 | ERROR: /tmp/tensorflow-knureon/tensorflow/tools/pip_package/BUILD:85:1: no such package '@llvm//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz, https://github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz] to /root/.cache/bazel/_bazel_root/0726af6ab00d84b2e399e1660b859bdb/external/llvm/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz: Premature EOF and referenced by '//tensorflow/tools/pip_package:licenses'\r\n-- | --\r\n20-Apr-2018 11:43:32 | ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@llvm//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz, https://github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz] to /root/.cache/bazel/_bazel_root/0726af6ab00d84b2e399e1660b859bdb/external/llvm/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz: Premature EOF\r\n`\r\n\r\nI can use wget to fetch the tarball from the github source from within the Docker container without issues and the tarball opens without any errors reported.  The sha256 hash of the tarball is the same as what is specified in workspace.bzl:\r\n\r\n`\r\nsha256sum 8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz \r\n63d4da54dc7bc9a79e2ad266d230f4f759520cccb344a2dd49c2c6383ab75285  8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz\r\n`\r\n\r\n### Source code / logs\r\n[TF-TFLOW-JOB1-170.log](https://github.com/tensorflow/tensorflow/files/1933283/TF-TFLOW-JOB1-170.log)\r\n\r\n\r\nI have searched StackOverflow and github for the issue and found bazel LLVM errors, but nothing that would return \"Premature EOF\"", "comments": ["Just reproduced the error again in the container.  Only 4 megs of the archive are getting downloaded.  Is there a workaround I can do to patch this?  Is it possible to direct bazel to use wget or use different HTTP file transfer settings?\r\n\r\n`\r\nERROR: /tmp/tensorflow-knureon/tensorflow/tools/pip_package/BUILD:85:1: no such package '@llvm//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz, https://github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz] to /root/.cache/bazel/_bazel_root/0726af6ab00d84b2e399e1660b859bdb/external/llvm/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz: Premature EOF and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@llvm//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz, https://github.com/llvm-mirror/llvm/archive/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz] to /root/.cache/bazel/_bazel_root/0726af6ab00d84b2e399e1660b859bdb/external/llvm/8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz: Premature EOF\r\nINFO: Elapsed time: 187.525s\r\nFAILED: Build did NOT complete successfully (172 packages loaded)\r\nroot@5224ce0ce952:/tmp/tensorflow-knureon# cd ~/.cache/bazel/_bazel_root/ \r\n0726af6ab00d84b2e399e1660b859bdb/ cache/                            install/                          \r\nroot@5224ce0ce952:/tmp/tensorflow-knureon# cd ~/.cache/bazel/_bazel_root/0726af6ab00d84b2e399e1660b859bdb/external/llvm/\r\nroot@5224ce0ce952:~/.cache/bazel/_bazel_root/0726af6ab00d84b2e399e1660b859bdb/external/llvm# ll\r\ntotal 4104\r\ndrwxr-xr-x  2 root root    4096 Apr 20 19:27 ./\r\ndrwxr-xr-x 52 root root    4096 Apr 20 19:27 ../\r\n-rw-r--r--  1 root root 4194304 Apr 20 19:28 8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz\r\nroot@5224ce0ce952:~/.cache/bazel/_bazel_root/0726af6ab00d84b2e399e1660b859bdb/external/llvm# sha256sum 8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz \r\n0c0ceed4f1c039dd9ba8e19987bfa64b6a5d1db48804b3ed8480e4a5efe7dc55  8f7bcdf3c65b9a47e35653d525135beb18f3ac25.tar.gz\r\n`", "The problem appears to be related to some sort of settling of the internet connection between our build servers and github.  When I run bazel a *second* time from the same container (after running bazel clean and deleting the .cache folder), the download works and the build continues as normal.\r\n\r\nA weak workaround is to add a 'sleep 30' command to the docker RUN statement between the ./configure and 'bazel' calls.  When I do this, the build succeeds 50% of the time.  I also want to try to just wget the file directly to ~/.cache/bazel/_bazel_root/0726af6ab00d84b2e399e1660b859bdb/external/llvm before running bazel, as wget always seems to work.\r\n\r\nI'm not sure there is anything here for anyone outside our company to support, so feel free to close this if you don't have any input.  I mostly wanted to problem and possible workaround documented on the internet for the next person who ran into the issue."]}]