[{"number": 45180, "title": "Errors when loading a model having keras.Lambda", "body": "**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow 2.3.0\r\n- Python version: 3.6\r\n\r\n## Problem description\r\nWhile creating function for model creation using Functional API I noticed an interesting behaviour on how the graph is saved and loaded when you use keras.layers.Lambda as one of the layers. \r\n\r\nCurrently, tf2 makes it impossible to change batch_size within the model definition, so let's say you have multiple images and you want to make a decision based on these images from different points. \r\n\r\nE.g. You have 10 different shots from the camera (always the same point of view for corresponding shots) and you want to decide whether a phone is damaged. \r\n\r\n### Current behaviour\r\nWhen creating model within the scope of notebook within it is saved it is possible to save model (`keras.save_model()`) and load it (`keras.load_model()`) in a different notebook. \r\n\r\nWhen extracting a function to another file (let's say model.py) and performing the same scope: create->train->`keras.save_model()` and then loading the model in a different notebook, model throws errors. \r\n\r\n### Expected behaviour \r\nWorking saving and loading for a model defined in a different location. I also tried that with model subclassing and I am almost certain that it also does not work. \r\n\r\n### Reproduced problem with a proposed solution\r\nhttps://colab.research.google.com/drive/16LfnXVFHs9jNTOm0FUGjjnwAUczmOYdC?usp=sharing\r\n\r\n## Proposed solution/ My workaround\r\nFrom what I found the issue occurs partly because of keras.layers.Lambda is saving bytecode of the inner function. As the function is simple Python lambda not using any 3rd party functions it is only needed to focus on serialization and override module value of the function: \r\n```python\r\ndef _serialize_function_to_config(self, inputs, allow_raw=False):\r\n    if isinstance(inputs, python_types.LambdaType):\r\n      output = generic_utils.func_dump(inputs)\r\n      output_type = 'lambda'\r\n      module = inputs.__module__\r\n    elif callable(inputs):\r\n      output = inputs.__name__\r\n      output_type = 'function'\r\n      module = inputs.__module__\r\n    elif allow_raw:\r\n      output = inputs\r\n      output_type = 'raw'\r\n      module = None\r\n    else:\r\n      raise ValueError(\r\n          'Invalid input for serialization, type: %s ' % type(inputs))\r\n\r\n    return output, output_type, module\r\n```\r\n\r\nby subclassing:\r\n```python\r\nclass SafeSaveLambda(keras.layers.Lambda):\r\n    def _serialize_function_to_config(self, inputs, allow_raw=False):\r\n        output, output_type, module = super()._serialize_function_to_config(inputs, allow_raw)\r\n        \r\n        return output, output_type, \"__main__\"\r\n```\r\nand using SafeSaveLambda in place of ``keras.layers.Lambda``. Solution and investigation are available in GoogleColab. ", "comments": ["@rpytel1 \r\nUnable to access the colab shared, please share colab with access and issue reported.", "@Saduf2019 \r\nI have changed permissions and checked with incognito if I may access the notebook and it worked. ", "I am able to replicate this on [tf 2.4](https://colab.research.google.com/gist/Saduf2019/f1c124e221c876ea2c6a19147514aaca/untitled483.ipynb) and as reported on [tf 2.3](https://colab.research.google.com/gist/Saduf2019/b6ae6c4127e0013c0a4358010d14da91/untitled483.ipynb) and [nightly](https://colab.research.google.com/gist/Saduf2019/d1508e252364618225b895700960e74b/untitled483.ipynb)", "@ymodak any update on a suggested fix or better workaround?", "@rpytel1 \r\nThis is fixed on latest tf, can you refer to [this gist](https://colab.research.google.com/gist/Saduf2019/2c47e0eddc42065670e63ceb3536fa78/untitled594.ipynb) and update.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45180\">No</a>\n", "Hi all, \r\nSorry for the late response. The problem remains in the newest tf-nightly. @Saduf2019, your collab has mistakes as it is only restarting Keras session which is only removing TensorFlow objects and clears the GPU memory, while all python imports or custom defined functions are still in the memory. Here it is crucial to restart the runtime machine (in Jupyter it is called restarting kernel) to simulate few notebooks in one. The problem occurs when we do the following: \r\n1. We create a model using Functional API in one file (let's say ``create_model.py``) in one notebook (let's say ``train_model.ipynb``  and train it and save it.\r\n2. We load a model in a different notebook (let's say in ``test_model..pynb``). Then it throws exceptions and fails to load a graph. \r\n\r\nPlease find an example with code here: \r\nhttps://colab.research.google.com/drive/1B2hETw6NJN-F51F3taUIKjdSnRPJbQRU?usp=sharing\r\n\r\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45180\">No</a>\n", "Hi @rpytel1 ! \r\nWe are checking to see whether you still need help in this issue . Have you tried this issue in latest version TF 2.7 yet? Could not replicate this issue in Colab though.Attaching [Gist](https://colab.sandbox.google.com/gist/mohantym/8c71471f07115e0a8377b81e350b89ab/lambda_problem_remains.ipynb#scrollTo=eLYkf69l3o1A) for reference .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45180\">No</a>\n"]}, {"number": 45178, "title": "tensorflow.keras MirroredStrategy hangs without error message", "body": "Unexpected hangs occurs, running adapted https://keras.io/guides/distributed_training/\r\n**System information**\r\n- Have I written custom code : https://keras.io/guides/distributed_training/ \r\n- OS Platform and Distribution : Windows 10 pro (build 19041) \r\n- CPU Intel: 2x Intel Xeon 6140 18 Cores (36 logical) \r\n- System Memory : 384 GB\r\n- TensorFlow installed from (source or binary): pip installed\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7\r\n- GPU model and memory: 4 x Tesla V100-SXM2-32GB \r\n\r\n**Describe the current behavior**\r\nTraining hangs without any message when starting the learning, when large dataset are used as inputs.\r\n\r\n**Describe the expected behavior**\r\n\r\nTraining is expected to work in the same circumstances even with large dataset\r\n\r\n**Standalone code to reproduce the issue**\r\nChanging the get_dataset() to get a larger dataset from sample dataset following:\r\n\r\nnum_val_samples = 1000\r\nand then adding the 2 lines:\r\n    x_train = x_train[:-num_val_samples]\r\n    y_train = y_train[:-num_val_samples]\r\n    x_train = np.concatenate([x_train for i in range(50)])    \r\n    y_train = np.concatenate([y_train for i in range(50)])    \r\n    return (\r\n        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\r\n        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),\r\n        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\r\n    )\r\n\r\nWhen the oversize is 46 it runs flawlessly, when setting to 47 or higher it just hangs with no error messages\r\n\r\nThe stragegy is defined with, to work on our Windows HPC : \r\ncross_tower_ops = tf.distribute. ReductionToOneDevice()\r\nstrategy = tf.distribute.MirroredStrategy(devices=['/gpu:0','/gpu:1','/gpu:2','gpu:3'],\r\n                                            cross_device_ops=cross_tower_ops)\r\n", "comments": ["We also tried changing the data set with the strategy.experimental_distribute_dataset(val_dataset)", "Hi @WiestDaessle, to clarify, the training works okay when you run the tutorial? But it is hanging when you make these modifications? Is that correct?\r\nCan you please provide the exact code you are running into issues with so I can reproduce it on my end. You can provide it in a colab notebook, or just paste it a comment, but the full code (properly indented and with import statements) greatly helps us to troubleshoot. Also please clarify what you mean by 'the oversize'.\r\n\r\nNote that if you are using `MirroredStrategy` and `model.fit` then you should not need to use `strategy.experimental_distribute_dataset`. Distributing the dataset this way is only required when using a custom training loop.", "Hi @nikitamaia, \r\nThe sample is working after changing the strategy to point on our 4 GPUs using:\r\ncross_tower_ops = tf.distribute.HierarchicalCopyAllReduce(num_packs=4)\r\nstrategy = tf.distribute.MirroredStrategy(devices=['/gpu:0','/gpu:1','/gpu:2','gpu:3'],\r\n                                            cross_device_ops=cross_tower_ops)\r\n\r\nThe code just stops (no warning, no error, no feedback) when applying the code in the next comment\r\n\r\n", "```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # or any {'0', '1', '2'}\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\ntf.get_logger().setLevel('DEBUG')\r\n\r\ndef get_compiled_model():\r\n    # Make a simple 2-layer densely-connected neural network.\r\n    inputs = keras.Input(shape=(784,))\r\n    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\r\n    x = keras.layers.Dense(256, activation=\"relu\")(x)\r\n    outputs = keras.layers.Dense(10)(x)\r\n    model = keras.Model(inputs, outputs)\r\n    model.compile(\r\n        optimizer=keras.optimizers.Adam(),\r\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        metrics=[keras.metrics.SparseCategoricalAccuracy()],\r\n    )\r\n    return model\r\n\r\n\r\ndef get_dataset():\r\n    batch_size = 10240\r\n    num_val_samples = 10000\r\n\r\n    # Return the MNIST dataset in the form of a `tf.data.Dataset`.\r\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n    x_train = np.concatenate([x_train for i in range(50)])    \r\n    y_train = np.concatenate([y_train for i in range(50)])\r\n\r\n    # Preprocess the data (these are Numpy arrays)\r\n    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\r\n    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\r\n    y_train = y_train.astype(\"float32\")\r\n    y_test = y_test.astype(\"float32\")\r\n\r\n    # Reserve num_val_samples samples for validation\r\n    x_val = x_train[-num_val_samples:]\r\n    y_val = y_train[-num_val_samples:]\r\n    x_train = x_train[:-num_val_samples]\r\n    y_train = y_train[:-num_val_samples]\r\n    \r\n    print(len(x_train), len(y_train),len(x_val),len(y_val))    \r\n    return (\r\n        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\r\n        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),\r\n        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\r\n    )\r\n\r\n\r\n# Create a MirroredStrategy.\r\n#strategy = tf.distribute.MirroredStrategy()\r\n# cross_tower_ops = tf.distribute.ReductionToOneDevice()\r\ncross_tower_ops = tf.distribute.HierarchicalCopyAllReduce(num_packs=4)\r\n# Reduce to CPU\r\n#cross_tower_ops = tf.distribute. ReductionToOneDevice(reduce_to_device=\"/device:CPU:0\")\r\nstrategy = tf.distribute.MirroredStrategy(devices=['/gpu:0','/gpu:1','/gpu:2','gpu:3'],\r\n                                            cross_device_ops=cross_tower_ops)\r\nprint(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\r\n\r\n# Open a strategy scope.\r\nwith strategy.scope():\r\n    # Everything that creates variables should be under the strategy scope.\r\n    # In general this is only model construction & `compile()`.\r\n    model = get_compiled_model()\r\n    # model.summary()\r\n\r\n# Train the model on all available devices.\r\ntrain_dataset, val_dataset, test_dataset = get_dataset()\r\n    # train_dataset, val_dataset, test_dataset = (strategy.experimental_distribute_dataset(train_dataset),\r\n    #         strategy.experimental_distribute_dataset(val_dataset), \r\n    #         strategy.experimental_distribute_dataset(test_dataset))\r\n    # model.summary()\r\nmodel.fit(train_dataset, epochs=200, validation_data=val_dataset)\r\n\r\n# Test the model on all available devices.\r\nmodel.evaluate(test_dataset)\r\n```", "@WiestDaessle please provide the code with the proper indentation and formatting to help with the debugging process. Thanks!", "fucking markdown not taking the indent into account,\r\nshould be ok now", "I tried to run the code (with 2 V100s) and I saw that it was hanging on `train_dataset, val_dataset, test_dataset = get_dataset()` making me suspect that this is not an issue with the distribution strategy. Can you just try and execute your `get_datatset()` function without a strategy and see if it still hangs? I seem to be hitting a memory issue with `tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)` ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45178\">No</a>\n"]}, {"number": 45177, "title": "Output of tflite is not correct", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary):   package manager\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThe output of tflite is not correct.\r\n**Describe the expected behavior**\r\nThe output should match the result of equation.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n# Original model definition\r\n\"\"\"\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.InputLayer(input_shape=(2, 2, 1)),\r\n  tf.keras.layers.Conv2D(filters=1, kernel_size=(2, 2), use_bias = True, name = 'EX_Conv_1')\r\n])\r\n\"\"\"\r\n\r\n# Run tflite model\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimg = np.array([[-2,2],\r\n                [3,1]],dtype = np.int8)\r\n\r\nimg = np.expand_dims(img, axis = -1)\r\n\r\ninterpreter = tf.lite.Interpreter(model_path = 'xx.tflite') # you may download the file in attachment\r\n\r\ninterpreter.allocate_tensors()\r\n\r\nimg = img / 255.\r\ninput_scale, input_zero_point = interpreter.get_tensor_details()[0][\"quantization\"]\r\nresized_image = img / input_scale + input_zero_point\r\ninterpreter.set_tensor(0, np.expand_dims(resized_image, axis = 0).astype(interpreter.get_tensor_details()[0][\"dtype\"]))\r\n\r\ninterpreter.invoke()\r\n```\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/5595639/model.zip)\r\n\r\n\r\n**Other info / logs**\r\nBy using interpreter.get_tensor_details() and  interpreter.get_tensor(k), we should get following info:\r\nInput:\r\narray([[[[-86],\r\n         [ 84]],\r\n        [[126],\r\n         [ 41]]]], dtype=int8)\r\n\r\nIntput scale:   x_s = 9.22722028917633e-05\r\nInput zero point: x_zp = -1\r\n\r\nCNN weight:\r\narray([[[[1],\r\n         [3]],\r\n        [[5],\r\n         [7]]]], dtype=int8)\r\n\r\nCNN weight scale: w_s = 0.0005354330642148852\r\nCNN weight zero point: w_zp = 0\r\n\r\nCNN bias:\r\narray([3441], dtype=int32)\r\nCNN bias scale: b_s = 2.906211049591434e-09\r\nCNN bias zero point: b_zp = 0\r\n\r\nOutput scale: z_s = 2.7681662118084205e-07\r\nOutput zero point: z_zp =  -122\r\n\r\nIf I understand correctly, the quantized output should be calculated as:\r\noutput_quant =  {(x_s * w_s) \\*[(-86 - x_zp)\\*(1-w_zp) + (84-x_zp)\\*(3-w_zp) +(126-x_zp)\\*(5-w_zp) + (41-x_zp)\\*(7-w_zp)] + b_s\\*(3441-b_zp)} / z_s - z_zp \r\n= [(x_s*w_s)\\*(-85\\*1+85\\*3+127\\*5+42\\*7)+b_s\\*(3441)] / z_s-122 = 110.2729521400995 ->(int)-> 110\r\n\r\nWhile the actual output of tflite shows 127.", "comments": ["@neesetifa \r\nCan you please share a colab gist with the error reported as we are unable to access the .zip shared.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45177\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45177\">No</a>\n"]}, {"number": 45176, "title": "How can I build tensorflow C++ API on x86 platform", "body": "Thanks, I have compiled tensorflow C++ API on window10. But, the tensorflow_cc.dll is x64, it could not be used on my project. Is any methods to build a x86 version? I'm appreciating for your reply.\r\n", "comments": ["> Thanks, I have compiled tensorflow C++ API on window10. But, the tensorflow_cc.dll is x64, it could not be used on my project. Is any methods to build a x86 version? I'm appreciating for your reply.\r\n\r\nI would also be interested in the x86 API, but how did you manage to build the x64 version in the first place ?\r\nI haven't found any official tutorials on the subject and the methods in unofficial sources don't seem to work for me.\r\n\r\nIt would be greatly appreciated if you could be so kind to provide your method of compiling the sources.\r\nThx in advance !", "> > Thanks, I have compiled tensorflow C++ API on window10. But, the tensorflow_cc.dll is x64, it could not be used on my project. Is any methods to build a x86 version? I'm appreciating for your reply.\r\n> \r\n> I would also be interested in the x86 API, but how did you manage to build the x64 version in the first place ?\r\n> I haven't found any official tutorials on the subject and the methods in unofficial sources don't seem to work for me.\r\n> \r\n> It would be greatly appreciated if you could be so kind to provide your method of compiling the sources.\r\n> Thx in advance !\r\n\r\nhttps://github.com/gulingfengze/tensorflow-windows-build-script  I compiled sucessfully by this project. ", "Hi @woshildh ! I found instructions for  building Tensorflow in x86_64 architecture [with C](https://www.tensorflow.org/install/lang_c). Can you try once building f[rom source](https://www.tensorflow.org/install/source_windows) with Suitable compiler flags in latest version (2.8)? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45176\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45176\">No</a>\n"]}, {"number": 45175, "title": "Google Colab with TPU Compilation failure: Dynamic Spatial Convolution is not supported", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-0-gb36436b087 2.3.0\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source):-\r\n- CUDA/cuDNN version:-\r\n- GPU model and memory:-\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nHere is Colab notebook to reproduce the issue. I am trying to train MobileFaceNet model, which I modified to use MobileNet v1. It trains fine on my local computer with 1070 Ti, but the speed is too slow. I am using MSRA dataset, truncated to 1763749 entries. So, I wanted to try using TPU in Colab. I can run the code now, but on the beginning of the first epoch I get the following error:\r\n\r\n  (6) Invalid argument: {{function_node __inference_train_function_230773}} Compilation failure: Dynamic Spatial Convolution is not supported: lhs shape is f32[32,<=114,<=114,<=3] \r\n\t [[{{node mobile_face_net/conv1/Conv2D}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_18136726471139321858/_5]]\r\n\t [[tpu_compile_succeeded_assert/_18136726471139321858/_5/_185]]\r\n  (7) Invalid argument: {{function_node __inference_train_function_230773}} Compilation failure: Dynamic Spatial Convolution is not supported: lhs shape is f32[32,<=114,<=114,<=3] \r\n\t [[{{node mobile_face_net/conv1/Conv2D}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_18136726471139321858/_5]]\r\n\t [[tpu_compile_succeeded_assert/_18136726471139321858/_5/_199]]\r\n  (8) Invalid argument: {{function_node __inference_train_function_2307 ... [truncated]\r\n\r\nYou can see the full output in the Colab notebook \r\nhttps://colab.research.google.com/drive/1mwWbgA__rHpnEJK82Ih5acAv4aM_nRKE?usp=sharing\r\n\r\n**Describe the expected behavior**\r\nModel training and evaluating properly with TPU in Colab,\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1mwWbgA__rHpnEJK82Ih5acAv4aM_nRKE?usp=sharing\r\n", "comments": ["@AIWintermuteAI,\r\nCould you please share the contents of the directory `gs://data_bukcet/msra_tf`, so that we can reproduce the issue on our end? Thanks!", "Sure! I don't know how to share from Google Cloud bucket, but I shared this dataset on Google drive, here is the link\r\nhttps://drive.google.com/drive/folders/1Ufm1uRFBT_xLLasj9kdXEaAGcn0aNyWp?usp=sharing", "Was able to reproduce the issue with TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/bdb41549778786b899cc1d8473ba64f8/45175.ipynb). Thanks!", "From your Colab notebook, I think the issue you have encountered was different to mine\r\nFile system scheme '[local]' not implemented (file: '/content/msra_tf/6.tfrecord')\r\nThis is because Cloud TPU cannot use files stored on Colab file system(or Drive) for that matter, it needs files to be placed on google bucket.\r\nhttps://cloud.google.com/tpu/docs/troubleshooting#cannot_use_local_filesystem\r\nActually this documentation also refers to the issue I mentioned:\r\nhttps://cloud.google.com/tpu/docs/troubleshooting#dynamic_shapes_not_supported\r\nUnfortunately I do not see dynamic shapes used anywhere in my model, I even set drop_remainder to True when batching.\r\nIdeas?", "Hi @AIWintermuteAI, I printed `dataset_train`\r\n`<PrefetchDataset shapes: ((256, None, None, None), (256, 35275)), types: (tf.float32, tf.float32)>`\r\n\r\nand `dataset_val`\r\n`<PrefetchDataset shapes: ((256, None, None, None), (256, 35275)), types: (tf.float32, tf.float32)>`\r\n\r\nYou can see that the dimensions are all None and so I suspect that's what is causing the problem here, similar to #40864. I see you are calling tf.reshape but it looks like the shape of `example['height']` etc are also None, so that doesn't solve the problem. Try passing explicit size.\r\n", "Thank you, setting image dimensions explicitly worked. Would be nice to add it to documentation - since the training works fine on CPU/GPU, it might confuse people.\r\nClosing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45175\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45175\">No</a>\n"]}, {"number": 45174, "title": "Fix a mistake in code comment", "body": "", "comments": ["@firejq  As discussed on this PR internally, iff means \"if and only if\". Hence no change required in the existing line and closing the PR.  Thank you!. \r\nCC @jsimsa, @mihaimaruseac "]}, {"number": 45173, "title": "TPUEstimator does not update meta-weights", "body": "**System information**\r\n\r\n- OS Platform and Distribution: Debian GNU/Linux 9.11 (stretch)\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.5.3\r\n\r\nI train a neural network using `TPUEstimator`. In `model_fn`, losses are stored in `losses` list. After the end of an epoch, meta-weights `theta` should be updated using `losses`. But for the entire epoch, only one tensor is added to `losses` (`len(losses) = 1`) and `theta` remains the same (`theta==theta_prev = True`).\r\n\r\nHere is [gist](https://gist.github.com/KirillAI/ed88ac43b3da1643629b08e76bd0db5c) with the complete code. Part of `model_fn`:\r\n\r\n```\r\n...\r\n\r\n    def update_theta():\r\n        global losses\r\n        if len(losses) > 1:\r\n            losses_by_updates = [[] for _ in range(num_updates)]\r\n            for i in range(num_tasks):\r\n                for j in range(num_updates):\r\n                    losses_by_updates[j].append(losses[i*num_updates+j])\r\n            losses_by_updates = [tf.reduce_sum(loss) / tf.to_float(num_tasks) for loss in losses_by_updates]\r\n            gradients = theta_optimizer.compute_gradients(losses_by_updates[-1])\r\n            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\r\n                theta_op = theta_optimizer.apply_gradients(gradients)\r\n            weights = copy_theta_to_weights()\r\n            return theta_op\r\n        else:\r\n            return tf.no_op()\r\n...\r\n\r\n    loss = 0.0\r\n    for key in logits_train_dic:\r\n        logit_train = logits_train_dic[key]\r\n        loss += tf.square(labels[key]-logit_train)\r\n    loss_op = tf.reduce_mean(loss)\r\n    losses.append(loss_op)\r\n\r\n...\r\n```\r\n\r\nIs there any way to write loss to a list inside `model_fn`? Or maybe there is some other way to update the meta-weights using `TPUEstimator`?\r\n", "comments": ["@KirillAI \r\nCould you please let us know if this is still an issue in latest stable TF v2.6.0 ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45173\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45173\">No</a>\n"]}, {"number": 45172, "title": "label_image linker fix", "body": "While building label_image using makefile we get linker error. So to solve this, tensorflow/lite/tools/command_line_flags.cc and tensorflow/lite/tools/tool_params.cc are added to LABEL_IMAGE_SRCS", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45172) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@gbaned @srjoglekar246 the label_image build error is fixed in the Commit Id: 684cec69dbbd0ede4eaa59397750be265fac17b3 "]}, {"number": 45170, "title": "SubProcess ended with return code: 4294967295 with tensorflow-gpu 2.4.0rc cuda11.0 cudnn8.0.2 on windows 10", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0rc3\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): --\r\n- GCC/Compiler version (if compiling from source): -- \r\n- CUDA/cuDNN version: CUDA11.0 cudnn-11.0-windows-x64-v8.0.2.39 graphics driver version: 457.30\r\n- GPU model and memory: RTX 3070 8G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nA lot of **I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295** are printed when run tf code on windows 10.\r\n**Describe the expected behavior**\r\nI have no idea of what the meaning of these logs. And when I ran the same code on other machines with RTX2070s or RTX 2080Ti or GTX1080Ti and tensorflow-gpu 2.3,   these logs hadn't been shown.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Sequential\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\nx = np.random.normal(size=(100, 28, 28, 1)).astype(np.float32)\r\ny = np.zeros([100, 10], dtype=np.float32)\r\ny[:, 1] = 1.\r\n\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(buffer_size=100).batch(32)\r\nnum_classes = 10\r\n\r\nmodel = Sequential([\r\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\r\n  layers.MaxPooling2D(),\r\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\r\n  layers.MaxPooling2D(),\r\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n  layers.MaxPooling2D(),\r\n  layers.Flatten(),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(num_classes)\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\nepochs=3\r\nhistory = model.fit(\r\n  train_ds,\r\n  epochs=epochs\r\n)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nHere is the logs:\r\n```\r\nC:\\Users\\dell\\Anaconda3\\envs\\tf24rcpy38\\python.exe F:/python_ws/helloworld/main.py\r\n2020-11-25 13:14:29.035547: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-25 13:14:31.301152: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-25 13:14:31.301719: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-11-25 13:14:31.330039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-25 13:14:31.330233: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-25 13:14:31.338262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-25 13:14:31.338376: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-25 13:14:31.341810: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-25 13:14:31.342848: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-25 13:14:31.350072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-25 13:14:31.352437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-25 13:14:31.353100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-25 13:14:31.353270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-25 13:14:31.357024: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-25 13:14:31.357735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:65:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.815GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-25 13:14:31.357961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-25 13:14:31.358056: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-25 13:14:31.358145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-25 13:14:31.358240: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-25 13:14:31.358335: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-25 13:14:31.358452: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-25 13:14:31.358577: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-25 13:14:31.358702: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-25 13:14:31.358878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-25 13:14:32.004910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-25 13:14:32.005024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-11-25 13:14:32.005085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-11-25 13:14:32.005286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6177 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:65:00.0, compute capability: 8.6)\r\n2020-11-25 13:14:32.006266: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nEpoch 1/3\r\n2020-11-25 13:14:32.533116: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-11-25 13:14:32.625497: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-25 13:14:33.613410: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-25 13:14:33.619608: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-25 13:14:35.296892: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2020-11-25 13:14:35.310216: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n\r\n2020-11-25 13:14:35.310607: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code -1, output: \r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2020-11-25 13:14:35.320291: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n\r\n2020-11-25 13:14:35.331970: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n\r\n# ...(lots of SubProcess ended with return code: 4294967295)\r\n\r\n2020-11-25 13:14:36.014622: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n\r\n2020-11-25 13:14:36.230275: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n\r\n2020-11-25 13:14:36.231899: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n2020-11-25 13:14:36.289355: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n\r\n2020-11-25 13:14:36.302906: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n\r\n2020-11-25 13:14:36.315136: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n\r\n# ...(lots of SubProcess ended with return code: 4294967295)\r\n\r\n1/4 [======>.......................] - ETA: 16s - loss: 2.3410 - accuracy: 0.0000e+002020-11-25 13:14:37.514428: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n\r\n# ...(lots of SubProcess ended with return code: 4294967295)\r\n\r\n4/4 [==============================] - 7s 591ms/step - loss: 1.7706 - accuracy: 0.5053\r\nEpoch 2/3\r\n4/4 [==============================] - 0s 6ms/step - loss: 0.0604 - accuracy: 1.0000\r\nEpoch 3/3\r\n4/4 [==============================] - 0s 5ms/step - loss: 8.0053e-05 - accuracy: 1.0000\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\nI use **...(lots of SubProcess ended with return code: 4294967295)** to represents many lines of \"SubProcess ended with return code: 4294967295\" for saving space.\r\n", "comments": ["I have the same problem\r\nSystem information:\r\nPython version: 3.6.8\r\ntf-nightly-gpu 2.5.0\r\nCUDA11.1\r\nGPU RTX 3070 8G\r\nwindows 10\r\n\r\n\r\n", "same for me on RTX 3070, testet with cuda 11.0 and 11.1", "I had the same problem on my system with an RTX 3070.\r\n\r\nI solved the issue for me as follows:\r\n- delete the cuda v.10.1 variable value and change to v.11.1 in the variable name: $Path\r\n- after this my pycharm asks for cusolver64_10.dll\r\n- I simply copied this from v10.1 to v11.1\r\nnow my system is running", "The `SubProcess ended with return code: 4294967295` error is most likely relates to PTX compiler (ptxas.exe) not supporting your GPU (sm_86) in CUDA <= 11.0.\r\n\r\nThis issue is tracked at https://github.com/tensorflow/tensorflow/issues/44750 including some workarounds.\r\n", "@cheshire can you PTAL (also see the internal bug).", "@xldrx Thanks, it works.\r\nI follow the suggestions at [#44750](https://github.com/tensorflow/tensorflow/issues/44750) and the [article](https://dobromyslova.medium.com/making-work-tensorflow-with-nvidia-rtx-3090-on-windows-10-7a38e8e582bf):\r\n>copy C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin\\ptxas.exe into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\\ptxas.exe.\r\n\r\nThe _SubProcess ended with return code: 4294967295_ error has gone.", "@SilenceEagle Thanks,I solved the problem according to your method!!!", "> @xldrx Thanks, it works.\r\n> I follow the suggestions at [#44750](https://github.com/tensorflow/tensorflow/issues/44750) and the [article](https://dobromyslova.medium.com/making-work-tensorflow-with-nvidia-rtx-3090-on-windows-10-7a38e8e582bf):\r\n> \r\n> > copy C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin\\ptxas.exe into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\\ptxas.exe.\r\n> \r\n> The _SubProcess ended with return code: 4294967295_ error has gone.\r\n\r\nThanks,I solved the problem according to your method!!!", "> > @xldrx Thanks, it works.\r\n> > I follow the suggestions at [#44750](https://github.com/tensorflow/tensorflow/issues/44750) and the [article](https://dobromyslova.medium.com/making-work-tensorflow-with-nvidia-rtx-3090-on-windows-10-7a38e8e582bf):\r\n> > > copy C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin\\ptxas.exe into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\\ptxas.exe.\r\n> > \r\n> > \r\n> > The _SubProcess ended with return code: 4294967295_ error has gone.\r\n> \r\n> Thanks,I solved the problem according to your method!!!\r\n\r\nworked! Better, would be using the 11.1 cuda version and remove the 11.0.   ", "> @xldrx @SilenceEagle Thanks, it works.\r\n> I follow the suggestions at [#44750](https://github.com/tensorflow/tensorflow/issues/44750) and the [article](https://dobromyslova.medium.com/making-work-tensorflow-with-nvidia-rtx-3090-on-windows-10-7a38e8e582bf):\r\n> \r\n> > copy C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin\\ptxas.exe into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin\\ptxas.exe.\r\n> \r\n> The _SubProcess ended with return code: 4294967295_ error has gone.\r\n\r\nwow!! thanks!! I solved the problem!!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45170\">No</a>\n", "> The `SubProcess ended with return code: 4294967295` error is most likely relates to PTX compiler (ptxas.exe) not supporting your GPU (sm_86) in CUDA <= 11.0.\r\n> \r\n> This issue is tracked at #44750 including some workarounds.\r\n\r\nThanks! It solved my problem!\r\nBut my logs still show 'SubProcess ended with return code: 0'\r\nIs there any solution to this problem?\r\nThank you!", "> > The `SubProcess ended with return code: 4294967295` error is most likely relates to PTX compiler (ptxas.exe) not supporting your GPU (sm_86) in CUDA <= 11.0.\r\n> > This issue is tracked at #44750 including some workarounds.\r\n> \r\n> Thanks! It solved my problem!\r\n> But my logs still show 'SubProcess ended with return code: 0'\r\n> Is there any solution to this problem?\r\n> Thank you!\r\n\r\nI think 'return code: 0' means successfully ended the process."]}, {"number": 45169, "title": "Compile label_wav.cc file for speech recognition", "body": "This is the file location location: tensorflow/examples/speech_recognition/label_wav.cc\r\n\r\nCan you please provide steps to compile this file and run for speech recognition.", "comments": ["@sairmreddy \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there."]}, {"number": 45168, "title": "TFlite minimal failed ", "body": "I have already build TFlite minimal from [here ](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal) with cmake in vscode.\r\nAnd I download the model **mobilenet** from [here](https://www.tensorflow.org/lite/models/image_classification/overview) .\r\n\r\nWhen I try to use ./minimal like this  `./minimal mobilenet_v1_1.0_224_quant.tflite`,it raised an error: Could not open './mobilenet_v1_1.0_224_quant.tflite'.\r\n\r\nIt happened in Mmap_Allocation.cc:\r\n```\r\nMMAPAllocation::MMAPAllocation(const char* filename,\r\n                               ErrorReporter* error_reporter)\r\n    : Allocation(error_reporter, Allocation::Type::kMMap),\r\n      mmapped_buffer_(MAP_FAILED) {\r\n  mmap_fd_ = open(filename, O_RDONLY);\r\n  if (mmap_fd_ == -1) {\r\n    error_reporter_->Report(\"Could not open '%s'.\", filename);\r\n    return;\r\n  }\r\n```\r\n\r\nany advice is helpful,thx.", "comments": ["ok I solved problem,I have to `chmod +x` every files used,otherwise it can't be read if you unzip it from windows and copy to your wsl.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45168\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45168\">No</a>\n"]}, {"number": 45167, "title": "tensorflow linear regression error blows up", "body": "I am trying to fit a very simple linear regression model using tensorflow. However, the loss (mean squared error) blows up instead of reducing to zero.\r\n\r\nFirst, I generate my data:\r\n`x_data = np.random.uniform(high=10,low=0,size=100)\r\ny_data = 3.5 * x_data -4 + np.random.normal(loc=0, scale=2,size=100)`\r\n\r\nThen, I define the computational graph:\r\n`X = tf.placeholder(dtype=tf.float32, shape=100)\r\nY = tf.placeholder(dtype=tf.float32, shape=100)\r\nm = tf.Variable(1.0)\r\nc = tf.Variable(1.0)\r\nYpred = m*X + c\r\nloss = tf.reduce_mean(tf.square(Ypred - Y))\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)\r\ntrain = optimizer.minimize(loss)`\r\n\r\nFinally, run it for 100 epochs:\r\n`steps = {}\r\nsteps['m'] = []\r\nsteps['c'] = []\r\n\r\nlosses=[]\r\n\r\nfor k in range(100):\r\n    _m = session.run(m)\r\n    _c = session.run(c)\r\n    _l = session.run(loss, feed_dict={X: x_data, Y:y_data})\r\n    session.run(train, feed_dict={X: x_data, Y:y_data})\r\n    steps['m'].append(_m)\r\n    steps['c'].append(_c)\r\n    losses.append(_l)`\r\n\r\nHowever, when I plot the losses, I get:\r\n![image](https://user-images.githubusercontent.com/42157989/100177859-6de50680-2f05-11eb-9232-11fef76ae197.png)\r\n", "comments": ["@raffibaihaqy02,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45167\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45167\">No</a>\n"]}, {"number": 45166, "title": "[ROCm] Adding / removing no_rocm tag from some unit tests", "body": "This PR has two commits\r\n* one to remove the `no_rocm` tag from unit-tests that are now passing on the ROCm platform\r\n* another to add the `no_rocm` tag to unit-tests that are now failing on the ROCm platform\r\n\r\n---------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work ", "comments": ["@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "Looks good, thanks. I'm merging this change manually."]}, {"number": 45165, "title": "how to handle non-tail recursive in tf when using control flow", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nHi,  I am learning tensorflow and I am curious whether there is non-tail recursion in tf when using control flow\r\nThe forward prop is generally loop, tail recursion, etc., but after the derivation, will there be non-tail recursion in the backprop?  How to deal with it\uff1f\r\nVery much looking forward to your reply.", "comments": ["@liangzelang \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "* tf verrsion:\r\nlastest\r\n* How to reproduce the issue:\r\nI did not meet some issues\uff0c i just want to know if there will be  non_tail recursion after Automatic differentiation and  how to handle this situation in some contro_flow cases.", "@liangzelang\r\nplease create a ticket on stackoverflow as this is not a bug or a feature request, and move this to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45164, "title": "mbed build broken with upgrade to Python 3.9 as part of CI", "body": "@tensorflow/micro\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/bdde0e95363f7799dc9d59eb9e01d66b3ad836e2 updated the docker image used for TFLM for the CI servers. This update to Python 3.9 resulted in the mbed failing with the following error:\r\n```\r\n[mbed] Auto-installing missing Python modules (jsonschema, mbed_cloud_sdk, jinja2, mbed_ls, mbed_host_tests, mbed_greentea, pyelftools, manifest_tool, icetea, pycryptodome, pyusb, cmsis_pack_manager, cryptography, click, cbor)...\r\nTraceback (most recent call last):\r\n  File \"/workspace/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/make.py\", line 30, in <module>\r\n    from tools.utils import args_error\r\n  File \"/workspace/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/utils.py\", line 33, in <module>\r\n    from intelhex import IntelHex\r\n  File \"/usr/local/lib/python3.9/site-packages/intelhex/__init__.py\", line 44, in <module>\r\n    from intelhex.compat import (\r\n  File \"/usr/local/lib/python3.9/site-packages/intelhex/compat.py\", line 60, in <module>\r\n    array_tobytes = getattr(array.array, \"tobytes\", array.array.tostring)\r\nAttributeError: type object 'array.array' has no attribute 'tostring'\r\n[mbed] ERROR: \"/usr/local/bin/python\" returned error.\r\n       Code: 1\r\n       Path: \"/workspace/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed\"\r\n       Command: \"/usr/local/bin/python -u /workspace/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM\"\r\n       Tip: You could retry the last command with \"-v\" flag for verbose output\r\n---\r\n```\r\n", "comments": ["Is this fixed?", "mbed + python 3.9 + CI is not fixed. However, with TFLM moving to a [standalone github repo](https://www.github.com/tensorflow/tflite-micro) the CI will be quite different and the current issue can be closed as a `wontfix`.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45164\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45164\">No</a>\n"]}, {"number": 45163, "title": "MirrorStrategy incompatible with tf.Estimator when pulling model from tf hub", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary Docker image - tensorflow/tensorflow:2.3.1-gpu\r\n\r\n- TensorFlow version (use command below):\r\nv2.3.0-54-gfcc4b966f1 2.3.1\r\n\r\n- Python version:\r\nPython 3.6.9\r\n\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\ncuda: 10.1, V10.1.243\r\nCUDNN_MAJOR 7\r\nCUDNN_MINOR 6\r\nCUDNN_PATCHLEVEL 5\r\n\r\n- GPU model and memory:\r\nGeForce GTX 1060 6GB x 2\r\n\r\n\r\n**Describe the current behavior**\r\nWhen running estimator which has a hub.KerasLayer defined in the model_fn, using a mirrored strategy to train fails with a \"TypeError: Expected tf.group() expected Tensor arguments not 'None' with type '<class 'NoneType'>\". \r\n\r\nSome other notes:\r\n* Using a single device strategy trains without issue.\r\n* Running without a hub.KerasLayer defined in the model_fn using mirrored strategy also trains without issue.\r\n* Tried using a a couple different hub.KerasLayers and both failed in mirrored strategy - \r\n   * https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\r\n   * https://tfhub.dev/google/nnlm-en-dim50/2\r\n\r\nIt seems to an issue with how the weights are being initialized when using a hub.KerasLayer in the model_fn when using a mirrored strategy. Maybe global_variables() isn't pulling in the relevant variables?\r\n\r\n\r\n**Describe the expected behavior**\r\nThe estimator should train in both the one device strategy and the mirrored strategy.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nDockerfile:\r\n```\r\nFROM tensorflow/tensorflow:2.3.1-gpu AS tf_build\r\nRUN python3 -m pip install tensorflow-hub==0.10.0\r\n```\r\n\r\ntemp.py:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport numpy as np\r\n\r\nnum_examples = 100\r\nstr_len = 5\r\nnum_labels = 3\r\nbatch_size = 4\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n  bert_layer = hub.KerasLayer(\r\n    'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\r\n    trainable=True\r\n  )\r\n  encoder_inputs = dict(\r\n    input_word_ids=features['input_word_ids'],\r\n    input_mask=features['input_mask'],\r\n    input_type_ids=features['input_type_ids']\r\n  )\r\n  encoder_outputs = bert_layer(encoder_inputs)\r\n  logits = encoder_outputs['pooled_output']\r\n\r\n  dense_layer = tf.keras.layers.Dense(\r\n    num_labels, activation=None, dtype=tf.float32)\r\n  logits = dense_layer(logits)\r\n\r\n  loss = tf.nn.softmax_cross_entropy_with_logits(\r\n    labels=features['labels'], logits=logits)\r\n  loss = tf.reduce_mean(loss)\r\n\r\n  global_step = tf.compat.v1.train.get_or_create_global_step()\r\n  optimizer = tf.compat.v1.train.AdamOptimizer()\r\n  train_op = optimizer.minimize(loss=loss, global_step=global_step)\r\n\r\n  probs = tf.nn.softmax(logits)\r\n\r\n  return tf.estimator.EstimatorSpec(\r\n    mode=mode,\r\n    loss=loss,\r\n    train_op=train_op,\r\n    predictions={'probs': probs}\r\n  )\r\n\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n# strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\r\nrun_config = tf.estimator.RunConfig(\r\n  model_dir='./tmp',\r\n  save_summary_steps=1000,\r\n  log_step_count_steps=1,\r\n  save_checkpoints_secs=1000,\r\n  keep_checkpoint_max=1,\r\n  train_distribute=strategy,\r\n)\r\n\r\nest = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\r\n\r\n\r\ndef input_fn():\r\n  labels = np.zeros((num_examples, num_labels))\r\n  labels[np.arange(num_examples), np.random.randint(low=0, high=num_labels)] = 1\r\n\r\n  features = dict(\r\n    input_word_ids=np.random.randint(\r\n      low=1, high=100, size=(num_examples, str_len), dtype='int32'),\r\n    input_mask=np.zeros((num_examples, str_len), dtype='int32'),\r\n    input_type_ids=np.zeros((num_examples, str_len), dtype='int32'),\r\n    labels=labels.astype('float32'),\r\n  )\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices(features)\r\n  return dataset.repeat().batch(batch_size)\r\n\r\n\r\nest.train(input_fn, steps=500)\r\n```\r\n\r\nto run:\r\nbuild docker image:\r\n```\r\ndocker build . -t temp_image/temp_image:latest\r\n```\r\nrun script:\r\n```\r\ndocker run --gpus all -it -v ${PWD}:/tmp -w /tmp --rm temp_image/temp_image:latest python temp.py\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nstack trace:\r\n```\r\n2020-11-25 01:06:04.045230: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-25 01:06:05.718785: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-11-25 01:06:05.762730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:04:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1\r\ncoreClock: 1.7085GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2020-11-25 01:06:05.763811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:81:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1\r\ncoreClock: 1.7085GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2020-11-25 01:06:05.763864: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-25 01:06:05.766507: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-11-25 01:06:05.768582: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-11-25 01:06:05.768936: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-11-25 01:06:05.771694: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-25 01:06:05.773286: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-11-25 01:06:05.778866: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-11-25 01:06:05.782227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-11-25 01:06:05.795975: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100095000 Hz\r\n2020-11-25 01:06:05.806344: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5b3c870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-11-25 01:06:05.806381: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-11-25 01:06:06.064338: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5ba86c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-11-25 01:06:06.064388: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1060 6GB, Compute Capability 6.1\r\n2020-11-25 01:06:06.064401: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1060 6GB, Compute Capability 6.1\r\n2020-11-25 01:06:06.065488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:04:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1\r\ncoreClock: 1.7085GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2020-11-25 01:06:06.066152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:81:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1\r\ncoreClock: 1.7085GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2020-11-25 01:06:06.066202: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-25 01:06:06.066235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-11-25 01:06:06.066257: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-11-25 01:06:06.066278: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-11-25 01:06:06.066298: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-25 01:06:06.066324: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-11-25 01:06:06.066346: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-11-25 01:06:06.068886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-11-25 01:06:06.068937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-11-25 01:06:07.467433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-25 01:06:07.467507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \r\n2020-11-25 01:06:07.467523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N \r\n2020-11-25 01:06:07.467533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N \r\n2020-11-25 01:06:07.469936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5547 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:04:00.0, compute capability: 6.1)\r\n2020-11-25 01:06:07.472078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 5548 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1060 6GB, pci bus id: 0000:81:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:339: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\nWARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\r\nWARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\r\nWARNING:tensorflow:AutoGraph could not transform <function _combine_distributed_scaffold.<locals>.<lambda> at 0x7fa56ae5a158> and will run it as-is.\r\nCause: could not parse the source code:\r\n\r\n      lambda scaffold: scaffold.ready_op, args=(grouped_scaffold,))\r\n\r\nThis error may be avoided by creating the lambda in a standalone statement.\r\n\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <function _combine_distributed_scaffold.<locals>.<lambda> at 0x7fa56ae5a158> and will run it as-is.\r\nCause: could not parse the source code:\r\n\r\n      lambda scaffold: scaffold.ready_op, args=(grouped_scaffold,))\r\n\r\nThis error may be avoided by creating the lambda in a standalone statement.\r\n\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:Issue encountered when serializing variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'NoneType' object has no attribute 'name'\r\nWARNING:tensorflow:Issue encountered when serializing variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'NoneType' object has no attribute 'name'\r\nWARNING:tensorflow:Issue encountered when serializing trainable_variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'NoneType' object has no attribute 'name'\r\nWARNING:tensorflow:Issue encountered when serializing trainable_variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'NoneType' object has no attribute 'name'\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/util.py:96: DistributedIteratorV1.initialize (from tensorflow.python.distribute.input_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the iterator's `initializer` property instead.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/util.py:96: DistributedIteratorV1.initialize (from tensorflow.python.distribute.input_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the iterator's `initializer` property instead.\r\nTraceback (most recent call last):\r\n  File \"temp.py\", line 76, in <module>\r\n    est.train(input_fn, steps=500)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 349, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1173, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1235, in _train_model_distributed\r\n    self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1349, in _actual_train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1507, in _train_with_estimator_spec\r\n    log_step_count_steps=log_step_count_steps) as mon_sess:\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 749, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1231, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1236, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 902, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 660, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 200, in finalize\r\n    default_init_op)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 297, in get_or_default\r\n    op = default_constructor()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 196, in default_init_op\r\n    variables.global_variables_initializer(),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 3290, in global_variables_initializer\r\n    return variables_initializer(global_variables())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 3267, in variables_initializer\r\n    return control_flow_ops.group(*[v.initializer for v in var_list], name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2923, in group\r\n    \"'%s' with type '%s'\" % (inp, type(inp)))\r\nTypeError: Expected tf.group() expected Tensor arguments not 'None' with type '<class 'NoneType'>'\r\n```\r\n", "comments": ["@CRSilkworth,\r\nTF Hub issues are tracked in tensorflow/hub repo. Could you please submit a new issue from [this link](https://github.com/tensorflow/hub/issues/new) and fill in the template, so that we can track the issue there. Thanks!", "Ok, will do.", "@CRSilkworth,\r\nMarking this issue as closed, since it is being tracked in TF-hub repo. Please feel free to re-open the issue if necessary. Thanks!"]}, {"number": 45162, "title": "Training a tf.keras.Model, with a tf.data.Dataset mapped with a randomly selected action from a large list of actions (callables provided by the user) takes a lot of time to start, if it ever does.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nYes, I have, there's a colab notebook below with the custom code.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\ncolab notebook\r\n- TensorFlow installed from (source or binary):\r\n\r\ncolab default version 2.3.0\r\n\r\n- TensorFlow version (use command below):\r\n\r\n2.3.0\r\n\r\n- Python version:\r\n\r\n3.6.9\r\n\r\n- GPU model and memory:\r\n\r\nIt happens using any of the GPUs available in Colab : Nvidia K80s, T4s, P4s and P100s, or even when I'm not using GPU\r\n\r\n**Describe the current behavior**\r\n\r\nTraining a tf.keras.Model, with a tf.data.Dataset mapped with a custom augmentator, which randomly selects an action from a large list of actions (callables provided during the instantiation of the augmentator) takes a lot of time to start, if it ever does.\r\n\r\nIf it ever manages to start training (more than 20 minutes), then it takes even more time to start the epoch 3, epoch 5, epoch 7, etc.\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nTraining should start fast as usual, and shouldn't have overhead to start in some epochs.\r\n\r\n**Standalone code to reproduce the issue**\r\nI have created a colab with about 40 lines of code in which the code can be reproduced: \r\n\r\nhttps://colab.research.google.com/drive/1FyNuTlX6puKIosWPNojcEN9zZi9-60Ve?usp=sharing\r\n\r\n**Other info**\r\n\r\n\r\nA little bit more information about the colab contents and the issue here: \r\n\r\n\r\n- Context a RandomAugmentationPolicy object can be instantiated with a list of callables, and is used to map a dataset with tf.data.Dataset.map. For each img in the dataset, it randomly samples one of its actions, and augments the img.\r\n\r\n- Problem : When the length of the list of actions provided to instantiate a RandomAugmentationPolicy object is about 24 or more, training a tf.keras.Model using model.fit never starts the epoch 3 when running on colab.  When running on an ubuntu machine (aws p3.2xlarge) with a CPU that has 8 cores it never starts epoch 7.\r\n\r\n- Sample code to reproduce the issue : The code in the colab first defines the RandomAugmentationPolicy class previously mentioned, then it created a mocked dataset, and it then tries to train a model tf.keras.Model.fit.\r\n\r\n- Note : The only reasonable way I found to apply the 'selected_action' (L22 class definition), is to iterate over all the actions and augment the img when the action idx is equal to the desired action.\r\n\r\n\r\n\r\nThank you in advance for your help with this issue.\r\n\r\n\r\n\r\n", "comments": ["When i use tf2.3-cpu to run program, have the problem, Can you tell me the reason?\r\n\r\n2020-11-25 16:55:33.614954: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n2020-11-25 16:55:33.615418: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-11-25 16:55:33.622449: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-B511OAC\r\n2020-11-25 16:55:33.623218: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-B511OAC\r\n2020-11-25 16:55:33.624131: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-25 16:55:33.637363: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16247dbe220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-11-25 16:55:33.637884: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version", "> When i use tf2.3-cpu to run program, have the problem, Can you tell me the reason?\r\n> \r\n> 2020-11-25 16:55:33.614954: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n> 2020-11-25 16:55:33.615418: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n> 2020-11-25 16:55:33.622449: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-B511OAC\r\n> 2020-11-25 16:55:33.623218: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-B511OAC\r\n> 2020-11-25 16:55:33.624131: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-11-25 16:55:33.637363: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16247dbe220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n> 2020-11-25 16:55:33.637884: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version\r\n\r\nSorry, but I couldn't completely understand you.  That shouldn't be a problem, but just to verify, I ran the same code on a macbookPro with OS catalina and a new conda environment that I created with the following 3 lines 1.) conda create -n test_dependencies python==3.6.9 , 2.) conda activate test_dependencies 3.) pip install tensorflow-cpu==2.3.0 . I didn't get that message you showed an I still have the same problem on the mac, training never starts the epoch 7 in this case (maybe related to the number of cores in the CPU, 8 in the macbook vs 2 in colab).", "@juanma9613 \r\nI ran the code on tf nightly and do not face the error, please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/cf53e5235f36d8b4d916a46d02bc1e92/untitled472.ipynb)", "> @juanma9613\r\n> I ran the code on tf nightly and do not face the error, please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/cf53e5235f36d8b4d916a46d02bc1e92/untitled472.ipynb)\r\n\r\n@Saduf2019 Thank you, I was able to run it in 2.5.0-dev20201125.\r\n\r\nI still need to be able to run it in a more stable version, do you have an idea of how to avoid the for loop in the method augment_imgs_with_actions ? My original idea was to do just self._actions[selected_action]_  , but I can't index a list with a tensor. Do you have any suggestion in that case?\r\n\r\nThank you for your quick answer, I really appreciate your help!\r\n\r\n\r\n", "@juanma9613 \r\nit is a bug in 2.3 and will be fixed in the next release.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45162\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45162\">No</a>\n"]}, {"number": 45161, "title": "[ROCm] Adding ROCm support for the `run_pip_tests.sh` script", "body": "The script `tensorflow/tools/ci_build/builds/run_pip_tests.sh` currently does not work correctly when run on the ROCm platform (specifically on multi-GPU nodes). This PR adds the necessary updates to make it run correctly on ROCm.\r\n\r\n\r\n------------------------------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work ", "comments": ["@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping"]}, {"number": 45160, "title": "copy of PR43566", "body": "Making a new PR from #43566 to try and debug issues with the CI that we are\nseeing.\n\n- TFLu: Port TFL detection postprocess operator\n- TFLu: detection_pp: review comments and fixes\n- TFLu: Add support for Renode test for Stm32f4 target\n- TFLu: Remove support for unknown output dims for detecion_pp\n- TFlu: update detection_pp with from review comments\n- TFLu: Fix micro_intepreter_test with ppd\n- TFLu: remove ununsed function in ppd\n- Fix a few remaining issues.\n- fix the char vs unsigned char discrepancy.\n- TFLu: Correct name of readme file\n- TFLu: fix ci_sanity test\n- Fix internal errors.\n- Fix ci_sanity.sh check\n- Fix hello world example\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45160) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 45159, "title": "[ROCm] Adding new CI scripts for ROCm platform", "body": "This PR adds new scripts for running CI jobs on the ROCm platform\r\n * `tensorflow/tools/ci_build/linux/rocm/run_cpu.sh` - this mirrors the functionality of upstream CI job - `Ubuntu CPU`\r\n * `tensorflow/tools/ci_build/linux/rocm/run_gpu_single.sh` - this mirrors the functionality of upstream CI job - `Linux GPU`  (but for ROCm instead of CUDA)\r\n * `tensorflow/tools/ci_build/linux/rocm/run_gpu_multi.sh` - Variant of `Linux GPU` for tests with the `multi_gpu` tag\r\n\r\nThese new CI scripts will replace the existing ones\r\n * `tensorflow/tools/ci_build/linux/rocm/run_cc_core.sh`\r\n * `tensorflow/tools/ci_build/linux/rocm/run_csb_core.sh`\r\n * `tensorflow/tools/ci_build/linux/rocm/run_py3_core.sh`\r\n * `tensorflow/tools/ci_build/xla/linux/rocm/run_py3.sh`\r\n\r\nThe old CI scripts will removed in a separate PR, once the transition to the new CI scripts is completed.\r\n\r\n-------------------------------------------\r\n\r\n/cc @rsanthanam-amd @cheshire @chsigg @nvining-work ", "comments": ["@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping", "@cheshire @chsigg gentle ping"]}, {"number": 45158, "title": "int32 instead of int32_t in  tensorflow/tensorflow/lite/kernels/internal/common.h ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Devuan Beowulf (Debian 10)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): 3.6.0\r\n- GCC/Compiler version (if compiling from source): (Debian 8.3.0-6) 8.3.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\nLine 192 of  tensorflow/tensorflow/lite/kernels/internal/common.h  references int_32\r\n\r\n     int32x4x4_t input_val, int32 quantized_multiplier, int shift) { \r\n                                              ^^^^^\r\ncompilation shows int32 is undefined \r\n\r\n**Describe the expected behavior**\r\n\r\nIt should use int32_t\r\n    int32x4x4_t input_val, int32_t quantized_multiplier, int shift) {\r\n                                             ^^^^^^\r\n**Standalone code to reproduce the issue**\r\n\r\nAny compilation attempt should fail\r\n\r\n**Other info / logs** \r\n\r\nNA\r\n", "comments": ["This should be fixed already. You might want to get the latest code version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45158\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45158\">No</a>\n"]}, {"number": 45157, "title": "\"Error: Found an unshardable source dataset\" when using Mirrored Strategy with train on batch", "body": "**System information**\r\n- Have I written custom code: Custom\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Tested from compiled source and binary\r\n- TensorFlow version (use command below): 2.4.0rc2\r\n- Python version: 3.9/3.8/3.7\r\n- Bazel version (if compiling from source): 3.7.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0/7\r\n- CUDA/cuDNN version: 11.1/10.1\r\n- GPU model and memory: RTX 3090/ RTX 2080\r\n\r\n**Describe the current behavior**\r\nWhen using MirroredStrategy with Keras, it produces a lot of outputs (see below) as described in this issue #42146. For each batch, it keeps printing the same logs with the error `Error: Found an unshardable source dataset`\r\nI tried to disable the `tf.data.experimental.AutoShardPolicy.OFF` on `train_dataset` or to set it to DATA, but it seems to not have any effect (probable because the `for loop` unpack the train dataset)\r\nIt works without any problem on TF 2.3.1\r\n\r\n**Describe the expected behavior**\r\nI expect to have the same behavior as on TF 2.3.1: no extensive printed log, no error. Have it works with the default shard policy.\r\nOr at least, to find a way to apply the shard policy on the two subset train_images, train_labels\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow.keras import datasets, layers, models\r\nimport tensorflow as tf\r\n\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\ntrain_dataset = train_dataset.with_options(options)\r\ntrain_dataset = train_dataset.batch(32)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    model = models.Sequential()\r\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\r\n    model.add(layers.MaxPooling2D((2, 2)))\r\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n    model.add(layers.MaxPooling2D((2, 2)))\r\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n    model.add(layers.Flatten())\r\n    model.add(layers.Dense(64, activation='relu'))\r\n    model.add(layers.Dense(10))\r\n\r\n    model.compile(optimizer='adam',\r\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                  metrics=['accuracy'])\r\n\r\n\r\nfor train_images, train_labels in train_dataset:\r\n    history = model.train_on_batch(train_images, train_labels)\r\n```\r\n\r\n**Other info / logs** \r\n```\r\n2020-11-24 17:17:49.116426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-24 17:17:49.885169: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-24 17:17:49.885621: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-11-24 17:17:49.917150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2020-11-24 17:17:49.917174: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-24 17:17:49.918707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-24 17:17:49.918727: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-24 17:17:49.919263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-24 17:17:49.919382: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-24 17:17:49.920901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-11-24 17:17:49.921285: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-24 17:17:49.921350: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-24 17:17:49.922684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\r\n2020-11-24 17:17:49.923076: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-24 17:17:49.924740: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-24 17:17:49.925430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1727] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.69GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2020-11-24 17:17:49.925441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-24 17:17:49.925448: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-24 17:17:49.925455: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-24 17:17:49.925462: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-24 17:17:49.925468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-24 17:17:49.925474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2020-11-24 17:17:49.925480: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-24 17:17:49.925486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-24 17:17:49.926781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1869] Adding visible gpu devices: 0\r\n2020-11-24 17:17:49.926800: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-24 17:17:50.236484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-24 17:17:50.236514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1273]      0 \r\n2020-11-24 17:17:50.236519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1286] 0:   N \r\n2020-11-24 17:17:50.238508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1413] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22185 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2020-11-24 17:17:51.610137: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorDataset/_2\"\r\nop: \"TensorDataset\"\r\ninput: \"Placeholder/_0\"\r\ninput: \"Placeholder/_1\"\r\nattr {\r\n  key: \"Toutput_types\"\r\n  value {\r\n    list {\r\n      type: DT_DOUBLE\r\n      type: DT_UINT8\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: 32\r\n        }\r\n        dim {\r\n          size: 32\r\n        }\r\n        dim {\r\n          size: 32\r\n        }\r\n        dim {\r\n          size: 3\r\n        }\r\n      }\r\n      shape {\r\n        dim {\r\n          size: 32\r\n        }\r\n        dim {\r\n          size: 1\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n\r\n```\r\nThank you!", "comments": ["Hi @houseofai, I was able to repro this and I see that the logs look different in 2.3 vs in nightly. I also was able to repro using `model.fit` instead of `model.train_on_batch`.\r\n\r\nNote that #42146 was in reference to `MultiWorkerMirroredStrategy`, and the use of an autoshard policy is not necessary when using `MirroredStrategy` since you only have one machine. This can be a bit confusing (especially given the logs you are seeing), so in case you haven't already taken a look at the [Distributed Input guide](https://www.tensorflow.org/tutorials/distribute/keras) I am linking it here.", "This is a warning, not an error. The warning is expected, as we cannot shard tensor based dataset by FILE.\r\n\r\nYou can pass the the dataset with options to model.fit() and the option should take effect. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45157\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45157\">No</a>\n", "Thanks @nikitamaia \r\nShould I understand that now the default behavior with MirroredStrategy and train_on_batch is to print those verbose warnings all the way long the training?", "Yes, the reason the options aren't working is that `model.train_on_batch` wraps the tensors in a dataset again, so the specified options are not taking effect. If you pass the dataset to `model.fit` with the options then you won't see the warning. That being said, since there is no autosharding when you're doing single worker training the autosharding options register as no-op, but the code is shared with MWMS so you see the warning regardless. I think ideally the verbose warnings shouldn't be there and in the future we can improve this, but for now the training is working as expected.", "Thanks a lot @nikitamaia for the detailed answer", "@crccw Why was this closed? As @nikitamaia explained using options to steer the sharding of a dataset passed to `model.train_on_batch` is not working due to some internals of that function. The proposed \"solution\" is to not use this function which is hardly a solution.\r\n\r\nSo may I rephrase the issue/bug report as \"Dataset options ignored for dataset passed to `model.train_on_batch`\" with OPs code  as a reproducer. And although sharding is not required for MirroredStrategy this is still a valid bug in TF 2.4: a) The option is clearly ignored and b) the same happens for MultiWorkerMirroredStrategy", "Can't believe they close this problem, this is atrocious", "Hi all, it seems that my explanation resulted in more confusion. \r\n\r\nThe solution was not to avoid `train_on_batch`, but instead to explain that `train_on_batch` will result in some warnings that you can ignore (although ideally, we would silence these warnings).\r\n\r\nThe sharding options set on the original dataset don't apply to `train_on_batch` because `train_on_batch` takes tensors as inputs and creates a new dataset internally. This is intended Keras behavior.\r\n\r\nBecause the options are not taking effect, you are unable to explicitly set the autoshardpolicy. Thus, the autoshardpolicy is set to AUTO [as indicated in the docs](https://www.tensorflow.org/tutorials/distribute/input#distributed_datasets):\r\n\r\n```\r\nAUTO: This is the default option which means an attempt will be made to shard by FILE. \r\nThe attempt to shard by FILE fails if a file-based dataset is not detected. \r\ntf.distribute will then fall back to sharding by DATA. \r\n```\r\n\r\nSo first the policy tries to shard by FILE, but it cannot since `train_on_batch` takes tensors as input. So then the DATA policy is used.`Error: Found an unshardable source dataset` is indicating that the input could not be sharded by FILE, and will instead be sharded by DATA.\r\n\r\nTo summarize, it seems the main point of confusion here is the warning `Error: Found an unshardable source dataset` which in this case is telling us that with `model.train_on_batch` you cannot shard by FILE. However, the phrasing of the warning is confusing/misleading. \r\n\r\nA fix in this case either means writing a better warning message, or from the Keras side explicitly setting the autoshardpolicy to DATA when `train_on_batch` is used. In the end the same effect occurs, but just without the warning message. \r\n\r\nHopefully this clears things up!\r\n", "Hi all, please be aware that you also have this warring message not only appears at training time but also when you are just predicting and you want to leverage with `MirroredStrategy` to increase the batch size.", "I changed the wording on the warning message (and removed the word \"Error\"). See 92bd8e1"]}, {"number": 45156, "title": "\"NotFoundError: No CPU devices are available in this process\" when trying to use GPU / Cuda", "body": "**System information**\r\n- OS: Windows 10, build 1904:\r\n- TensorFlow installed from: conda\r\n- TensorFlow version: tensorflow-gpu 2.1.0\r\n- Python version: 3.7.9\r\n- Installed using: conda\r\n- CUDA version: cudatoolkit 10.1.243\r\n- cdNN version: cudnn 7.6.5\r\n- GPU model and memory: GeForce RTX 2070\r\n\r\nWhen I run a script that imports tensorflow, I got the initial message that says that we are good to go:\r\n\r\n```\r\n2020-11-24 10:34:33.383215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-11-24 10:34:35.172255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-11-24 10:34:35.201507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-11-24 10:34:35.201929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-11-24 10:34:35.205687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-11-24 10:34:35.208452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-24 10:34:35.209263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-11-24 10:34:35.212090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-24 10:34:35.213642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-11-24 10:34:35.219337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-24 10:34:35.219630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n```\r\n\r\nAnd the code runs well until the point a NN structure is to be fit(). There, I get the following error:\r\n\r\n`tensorflow.python.framework.errors_impl.NotFoundError: No CPU devices are available in this process`\r\n\r\nFrom what I understand, tensorflow is trying to use my CPU when it should be using my GPUs, correct?\r\n\r\nIf so, **is there something I should do in my installation to deactivate the usage of CPUs and only force it to use GPUs?**\r\nOr **is it missing some line of code here/there to make this forcing?**\r\n\r\nThanks.\r\n\r\nFull error stack:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"postProc01_QRNN_win10.py\", line 280, in <module>\r\n    _cur_nn_model, _cur_model_custom_objs = create_empty_model(_li_df[X_COLS], [20, 20], [0.25, 0.5, 0.75])\r\n  File \"postProc01_QRNN_win10.py\", line 189, in create_empty_model\r\n    output_activation=\"relu\", input_normalization=True, metrics=[])\r\n  File \"D:\\andre\\git_repos\\tibagi_codes\\coding_playground\\TK_proj_downscalingComparisons\\lib\\my_quantile_mapping_lib_v2.py\", line 54, in build_mqnn\r\n    normalizer.adapt(training_x_values)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_preprocessing_layer.py\", line 180, in adapt\r\n    self.build(shape)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\preprocessing\\normalization.py\", line 94, in build\r\n    initializer=init_ops.zeros_initializer)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_preprocessing_layer.py\", line 107, in _add_state_variable\r\n    **kwargs)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 446, in add_weight\r\n    caching_device=caching_device)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer_utils.py\", line 142, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 258, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 219, in _variable_v1_call\r\n    shape=shape)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 197, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 2596, in default_variable_creator\r\n    shape=shape)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1411, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1542, in _init_from_args\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer_utils.py\", line 122, in <lambda>\r\n    init_val = lambda: initializer(shape, dtype=dtype)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py\", line 114, in __call__\r\n    return array_ops.zeros(shape, dtype)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 2434, in zeros\r\n    output = _constant_if_small(zero, shape, dtype, name)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 2392, in _constant_if_small\r\n    return constant(value, shape=shape, dtype=dtype, name=name)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 258, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 266, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 95, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File \"C:\\Users\\dellalia\\Anaconda_3_7\\envs\\ai_geo_gpu_3_7\\lib\\site-packages\\tensorflow_core\\python\\eager\\context.py\", line 509, in ensure_initialized\r\n    context_handle = pywrap_tensorflow.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No CPU devices are available in this process\r\n```\r\n\r\n", "comments": ["@adlzanchetta \r\nThis issue is more suitable on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues) repo since its related to TF installation with Anaconda.\r\nPlease post it on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues).\r\nThanks!\r\nWith respect to the error please refer to: [link](https://github.com/tensorflow/tensorflow/issues/2175), ", "Found issue. It was in the code. There was a line that unregistered the CPUs to be used as an attempt to force the use of GPUs, but for some reason the GPUs were just not being used. Thanks for the support.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45156\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45156\">No</a>\n", "I'm running into the same problem and am confused by adlzanchetta's resolution of this issue when he says \"for some reason\".  Wasn't the original problem that the GPUs were not being used?  Why must the CPUs be registered for the GPUs to be used?"]}, {"number": 45154, "title": "[ROCm] Updating find_rocm_config.py to \u2026", "body": "Updating find_rocm_config.py to pickup the commit that was missed when merging PR #44471 ( https://github.com/tensorflow/tensorflow/pull/44471 )\r\n\r\nSee the following comments (in PR # 44471 for further details)\r\nhttps://github.com/tensorflow/tensorflow/pull/44471#issuecomment-724678159\r\nhttps://github.com/tensorflow/tensorflow/pull/44471#issuecomment-724729560\r\n\r\n--------------------------------------------------------------------------------\r\n\r\n/cc @akuegel @cheshire @chsigg @nvining-work ", "comments": ["I think the compressed version of the find_rocm_config.py script had an older version where the last few lines were still commented out. I was going to fix this, but then someone else already approved the merge. So this fix will go in separately.", "sorry about that, my fault. Fortunately those lines being commented do not affect any functionality (for now).\r\n\r\nwill fix this in the next PR (related to rocm config) I file. thanks", "Actually I noticed that we get failures if those lines are not commented out. So in my change I kept them commented out, just improved some other things slightly.", "@akuegel \r\n\r\nThink @jerryyin figured out (during out weekly sync of this repo --> ROCm fork) why you possibly maybe getting failures with those lines not commented out.\r\n\r\nWhen uncommented, the `_find_rocblas_config` function is invoked, and this commit introduces a bug in that routine\r\nhttps://github.com/tensorflow/tensorflow/commit/3ec134727f7cb07b62c99407298c782ce6effed3#diff-5601f1b9a25269295e766e91038a92cfc2372d411fdfe75fcfd86f98124dcf6dR267\r\n\r\nThe value of `path` gets over-written in the loop, and will not work as expected in the second iteration, leading to failure.", "Good catch :)\r\nThose changes were suggested to me in an internal review, and I didn't spot this bug.\r\nThe intent of the change was to avoid doing another call to os.path.exists after the for loop.\r\nI guess we should just use another variable in the for loop to assign os.path.join(path, f) to.", "I have a change for this under review and will try to get it landed tomorrow.", "The fix has landed now: https://github.com/tensorflow/tensorflow/commit/3a52a5462c609c37a3b6048ee17465b1a3d6ba22"]}, {"number": 45153, "title": "Android tensorflow-lite-select-tf-ops.aar generated from tflite model build and deploy error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: \r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am following [this](https://www.tensorflow.org/lite/guide/build_android) guide to reduce my apk size. I followed the `Set up build environment using Docker` process to generate, `tensorflow-lite.aar`, `tensorflow-lite-select-tf-ops.aar` with my tflite model. \r\n\r\nAfter running, `bash tensorflow/lite/tools/build_aar.sh --input_models=custom_model.tflite --target_archs=arm64-v8a,armeabi-v7a` I found the `aar` files in tmp folder of docker container in `/tensorflow_src/bazel-bin/tmp/tensorflow-lite.aar` and `/tensorflow_src/bazel-bin/tmp/tensorflow-lite-select-tf-ops.aar`.\r\n\r\nI followed the part below by making `libs` folder and copying the aar files.\r\n\r\n```\r\nallprojects {\r\n    repositories {\r\n        jcenter()\r\n        flatDir {\r\n            dirs 'libs'\r\n        }\r\n    }\r\n}\r\n\r\ndependencies {\r\n    compile(name:'tensorflow-lite', ext:'aar')\r\n}\r\n```\r\n\r\n\r\nMy current code and model requires implementations below to work correctly.\r\n\r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\r\nimplementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n```\r\n\r\nThe problem comes when the above three implementation or commenting two except `implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'` gives errors,\r\n\r\n```\r\ncannot find symbol class Interpreter`\r\ncannot find symbol class NnApiDelegate\r\nerror: package org.tensorflow.lite.nnapi does not exist\r\n``` \r\n\r\nThis error is not show when `fat_apk` is built using command, `bazel build -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/lite/java:tensorflow-lite`. But using this method there is no `tensorflow-lite-select-tf-ops.aar` in `/tensorflow_src/bazel-bin/tensorflow/lite/java/` folder.\r\n\r\nUsing the previous with above fat_apk tensorflow-lite aar it crashes with error,\r\n\r\n```\r\njava.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZNK6google8protobuf7Message11GetTypeNameEv\" referenced by \"/data/app/com.example.myapp-PkLKD5MHuq9TB2R3mDgNcA==/base.apk!/lib/arm64-v8a/libtensorflowlite_flex_jni.so\"...\r\n```\r\n\r\nMy main goal is reduce size of `select-ft-ops` as I noticed it reduces app size by 60+mb. If there is an alternate method please suggest. Also looking for building instructions of tensorflow-lite-gpu aar.\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- Setup docker in Ubuntu 18.04 and download dockerfile from [here](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile). \r\n- Follow commands from `Set up build environment using Docker` section from [here](https://www.tensorflow.org/lite/guide/build_android#set_up_build_environment_using_docker).\r\n- Run `./configure`.\r\n- Use command, `bash tensorflow/lite/tools/build_aar.sh --input_models=custom_model.tflite --target_archs=arm64-v8a,armeabi-v7a`.\r\n- Get the `aar` files from `tmp` folder as mentioned [here](https://www.tensorflow.org/lite/guide/ops_select) in section `Building the Android AAR` and paste in `libs` folder of android project.\r\n- Paste following, \r\n```\r\nallprojects {\r\n    repositories {\r\n        jcenter()\r\n        flatDir {\r\n            dirs 'libs'\r\n        }\r\n    }\r\n}\r\n\r\ndependencies {\r\n    compile(name:'tensorflow-lite', ext:'aar')\r\n    compile(name:'tensorflow-lite-select-tf-ops', ext:'aar')\r\n}\r\n```\r\n- Commenting out following,\r\n```\r\n//implementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\n//implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Following, https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/lite/g3doc/guide/ops_select.md, I was able to get `tensorflow-lite-select-tf-ops.aar` that seems to work with `fat_apk_cpu` build of `tensorflow-lite.aar` . \r\n\r\nCommand used to generate,\r\n```\r\nbazel build --cxxopt='--std=c++14' -c opt   \\\r\n  --config=android_arm64 --config=monolithic  \\\r\n  //tensorflow/lite/java:tensorflow-lite-select-tf-ops\r\n```\r\n\r\nTook config from here, https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/BUILD.\r\n\r\nBut, [this](https://www.tensorflow.org/lite/guide/build_android#build_and_install) part is still not working for reducing size of select-tf-ops with given tflite model.\r\n\r\nI am confused about below as I cannot see any branch named Head in tensorflow. \r\n\r\n> Caution: Following feature is experimental and only available at HEAD. You can build smaller AAR files targeting only a set of models as follows:", "I was able to build `tensorflow-lite.aar` and `tensorflow-lite-gpu.aar` with `TF 2.3.0` for `arm64-v8a`. App works fine with newly built `aar`, but when it is included in app the size is slightly more than usual implementation for both release and debug builds. It is larger by around ~2mb. \r\n\r\nThe `aar` were pasted into `app/libs` directory. I followed this for building aar,\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/35170\r\n\r\nBelow are the working `aar` files. I used `--config=monolithic` for both of these,\r\n\r\n[SEPARATE_TFLITE_BUILD_WITH_GPU.zip](https://github.com/tensorflow/tensorflow/files/5601544/SEPARATE_TFLITE_BUILD_WITH_GPU.zip)\r\n\r\nApp `build.gradle`,\r\n\r\n```\r\nndk {\r\n    //abiFilters 'armeabi-v7a', 'arm64-v8a'\r\n    //abiFilters 'armeabi-v7a'\r\n    abiFilters 'arm64-v8a'\r\n}\r\n```\r\n\r\n```\r\ndependencies {\r\n    ...\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.3.0'\r\n//    implementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\n//    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\r\n\r\n    implementation files('libs/tensorflow-lite.aar')\r\n    implementation files('libs/tensorflow-lite-gpu.aar')\r\n}\r\n```\r\n\r\n\r\nThe problem with `tensorflow-lite-select-tf-ops.aar`  still persists. I used below command to generate it,\r\n\r\n```\r\nbash tensorflow/lite/tools/build_aar.sh --input_models=custom_model.tflite --target_archs=arm64-v8a\r\n```\r\n\r\n**The TFLite model I am using for aar generation is `FP16` quantized.**\r\n\r\nAfter build finished I get two files, `tensorflow-lite-select-tf-ops.aar` of size 2.739mb and `tensorflow-lite.aar` of size 144kb. Here are the generated `aar` files,\r\n\r\n[SELECT_TF_OPS_WITH_INPUT_MODEL.zip](https://github.com/tensorflow/tensorflow/files/5601525/SELECT_TF_OPS_WITH_INPUT_MODEL.zip)\r\n\r\n\r\n# Error:\r\n\r\nIncluding above attached aar in app it crashes with error like,\r\n\r\n```\r\njava.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZNK6google8protobuf7Message11GetTypeNameEv\" referenced by \"/data/app/com.example.myapp-PkLKD5MHuq9TB2R3mDgNcA==/base.apk!/lib/arm64-v8a/libtensorflowlite_flex_jni.so\"...\r\n```\r\n\r\n**This error is somewhat similar to,**\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/37161 and https://github.com/tensorflow/tensorflow/pull/37033\r\n\r\n\r\nIncluding implementation for `custom_tensorflowlite_flex.aar` and `custom_tensorflowlite_flex.aar` with `tensorflow-lite-select-tf-ops.aar` file found in `tmp` folder for `custom_model.tflite` still produces above **UnsatisfiedLinkError** error.\r\n\r\nIf I put both on the project I get error with **symbol not found Interpreter** and codes like `Interpreter.Options options = (new Interpreter.Options());` does not compile. If I put my [separately built](https://github.com/tensorflow/tensorflow/issues/35170) `tensorflow-lite.aar` size of 1.01mb with `implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.3.0'` it works.\r\n\r\n\r\nI  also tried edited the dockerfile below by replacing `FROM tensorflow/tensorflow:devel` with `FROM tensorflow/tensorflow:2.3.0`. In ubuntu 20.04 `FROM tensorflow/tensorflow:devel` works.\r\n\r\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile\r\n\r\nI was able build `tensorflow-lite-select-tf-ops.aar` successfully on ubuntu 18.04 with `FROM tensorflow/tensorflow:2.3.0`. I am not sure if this is OS version problem. **Now on ubuntu 20.04 I get,**\r\n\r\n```\r\nubuntu@ip-172-31-14-29:~$ docker build . -t tflite-builder -f tflite-android2.Dockerfile\r\nSending build context to Docker daemon  2.974MB\r\nStep 1/17 : FROM tensorflow/tensorflow:2.3.0\r\n ---> 539d0e818045\r\nStep 2/17 : ENV ANDROID_DEV_HOME /android\r\n ---> Using cache\r\n ---> 4ade7614b16b\r\nStep 3/17 : RUN mkdir -p ${ANDROID_DEV_HOME}\r\n ---> Using cache\r\n ---> b2531200046f\r\nStep 4/17 : ENV ANDROID_SDK_FILENAME tools_r25.2.5-linux.zip\r\n ---> Using cache\r\n ---> 0ab178e71e00\r\nStep 5/17 : ENV ANDROID_SDK_URL https://dl.google.com/android/repository/${ANDROID_SDK_FILENAME}\r\n ---> Using cache\r\n ---> 1408762ceea5\r\nStep 6/17 : ENV ANDROID_API_LEVEL 23\r\n ---> Using cache\r\n ---> 1e20aa8d980a\r\nStep 7/17 : ENV ANDROID_NDK_API_LEVEL 18\r\n ---> Using cache\r\n ---> 619a6e784914\r\nStep 8/17 : ENV ANDROID_BUILD_TOOLS_VERSION 28.0.0\r\n ---> Using cache\r\n ---> 32d81af7433a\r\nStep 9/17 : ENV ANDROID_SDK_HOME ${ANDROID_DEV_HOME}/sdk\r\n ---> Using cache\r\n ---> 8b90b07c7c63\r\nStep 10/17 : ENV PATH ${PATH}:${ANDROID_SDK_HOME}/tools:${ANDROID_SDK_HOME}/platform-tools\r\n ---> Using cache\r\n ---> f412a21692d1\r\nStep 11/17 : RUN cd ${ANDROID_DEV_HOME} &&     wget -q ${ANDROID_SDK_URL} &&     unzip ${ANDROID_SDK_FILENAME} -d android-sdk-linux &&     rm ${ANDROID_SDK_FILENAME} &&     bash -c \"ln -s ${ANDROID_DEV_HOME}/android-sdk-* ${ANDROID_SDK_HOME}\"\r\n ---> Running in b9cf0267b396\r\n/bin/sh: 1: wget: not found\r\nThe command '/bin/sh -c cd ${ANDROID_DEV_HOME} &&     wget -q ${ANDROID_SDK_URL} &&     unzip ${ANDROID_SDK_FILENAME} -d android-sdk-linux &&     rm ${ANDROID_SDK_FILENAME} &&     bash -c \"ln -s ${ANDROID_DEV_HOME}/android-sdk-* ${ANDROID_SDK_HOME}\"' returned a non-zero code: 127\r\nubuntu@ip-172-31-14-29:~$ ls\r\ntensorflow-lite-select-tf-ops.aar  tflite-android.Dockerfile\r\ntensorflow-lite.aar                tflite-android2.Dockerfile\r\nubuntu@ip-172-31-14-29:~$ nano tflite-android2.Dockerfile\r\nubuntu@ip-172-31-14-29:~$ docker build . -t tflite-builder -f tflite-android2.Dockerfile\r\nSending build context to Docker daemon  2.974MB\r\nStep 1/17 : FROM tensorflow/tensorflow:2.3.0\r\n ---> 539d0e818045\r\nStep 2/17 : ENV ANDROID_DEV_HOME /android\r\n ---> Using cache\r\n ---> 4ade7614b16b\r\nStep 3/17 : RUN mkdir -p ${ANDROID_DEV_HOME}\r\n ---> Using cache\r\n ---> b2531200046f\r\nStep 4/17 : ENV ANDROID_SDK_FILENAME tools_r25.2.5-linux.zip\r\n ---> Using cache\r\n ---> 0ab178e71e00\r\nStep 5/17 : ENV ANDROID_SDK_URL https://dl.google.com/android/repository/${ANDROID_SDK_FILENAME}\r\n ---> Using cache\r\n ---> 1408762ceea5\r\nStep 6/17 : ENV ANDROID_API_LEVEL 23\r\n ---> Using cache\r\n ---> 1e20aa8d980a\r\nStep 7/17 : ENV ANDROID_NDK_API_LEVEL 18\r\n ---> Using cache\r\n ---> 619a6e784914\r\nStep 8/17 : ENV ANDROID_BUILD_TOOLS_VERSION 28.0.0\r\n ---> Using cache\r\n ---> 32d81af7433a\r\nStep 9/17 : ENV ANDROID_SDK_HOME ${ANDROID_DEV_HOME}/sdk\r\n ---> Using cache\r\n ---> 8b90b07c7c63\r\nStep 10/17 : ENV PATH ${PATH}:${ANDROID_SDK_HOME}/tools:${ANDROID_SDK_HOME}/platform-tools\r\n ---> Using cache\r\n ---> f412a21692d1\r\nStep 11/17 : RUN cd ${ANDROID_DEV_HOME} &&     wget -q ${ANDROID_SDK_URL} &&     unzip ${ANDROID_SDK_FILENAME} -d android-sdk-linux &&     rm ${ANDROID_SDK_FILENAME} &&     bash -c \"ln -s ${ANDROID_DEV_HOME}/android-sdk-* ${ANDROID_SDK_HOME}\"\r\n ---> Running in f73c18575539\r\n/bin/sh: 1: wget: not found\r\nThe command '/bin/sh -c cd ${ANDROID_DEV_HOME} &&     wget -q ${ANDROID_SDK_URL} &&     unzip ${ANDROID_SDK_FILENAME} -d android-sdk-linux &&     rm ${ANDROID_SDK_FILENAME} &&     bash -c \"ln -s ${ANDROID_DEV_HOME}/android-sdk-* ${ANDROID_SDK_HOME}\"' returned a non-zero code: 127\r\nubuntu@ip-172-31-14-29:~$\r\n```\r\n\r\n\r\n", "Tried with command below to generate trimmed `tensorflow-lite-select-tf-ops.aar`  and `tensorflow-lite.aar` following https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/build_aar_with_docker.sh. \r\n\r\nWith the provided `tensorflow-lite.aar` still get compilation error,\r\n\r\n```\r\nerror: cannot find symbol\r\nimport org.tensorflow.lite.Interpreter;\r\n                          ^\r\n  symbol:   class Interpreter\r\n  location: package org.tensorflow.lite\r\n```\r\n\r\nGet the results from `bazel-bin/tmp`. Build commands,\r\n```\r\ndocker build . -t tflite-builder -f tflite-android.Dockerfile\r\ndocker run -it -v $PWD:/host_dir tflite-builder bash\r\ncd tensorflow_src\r\nwget \"...\" -O \"custom_model.tflite\"\r\nbash tensorflow/lite/tools/build_aar_with_docker.sh --input_models=custom_model.tflite --target_archs=arm64-v8a --checkpoint=master\r\n```\r\n\r\n", "@thaink could you check this issue?", "Are build building it at Tensorflow v2.3.0? Or at what commit are you using?\r\n\r\nThere seems to be a divergence between compiler versions. The error related to \"_ZNK6google8protobuf7Message11GetTypeNameEv\" did not happen in my debian system but reported by another.\r\nAnyway, this error should be fixed, so could you try it at v2.4.0-rc4?\r\n", "Thank you for looking into it. I will give `v2.4.0-rc4` a try.  \r\n\r\nI used ubuntu 20.04 on AWS EC2 and did not track any specific commits. For my previous builds I modified the first line provided [Dockerfile](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile) and tried with `FROM tensorflow/tensorflow:devel` once, `FROM tensorflow/tensorflow:2.3.0` another time. \r\n\r\n", "As mentioned earlier, this error was fixed before v2.4.0.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45153\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45153\">No</a>\n", "@quickgrid Is the solution to this issue to replace the first line of the Dockerfile?\r\nDo I have to replace FROM tensorflow/tensorflow:devel to..... FROM tensorflow/tensorflow:2.4.0 to have a succesdfull build of the two final desired .aar files?\r\nThanks", "I ended up using default `org.tensorflow` implementations. Did not get a chance to try to rebuild and test with the AAR files. It is likely fixed as mentioned above. I think the build will work as is with the given Dockerfile without needing to modifying it.", "@quickgrid @khanhlvg  Unfortunately default implementation gives me the same errors as you have mentioned back in November and December. If you have time please check your .aar files and come back with the results. During that I will give a try with FROM tensorflow/tensorflow:2.4.0 . Procedure with a Xeon processor takes 3 hours to complete and I cannot do a lot of experiments.", "Did `tensorflow:2.4.0` version work? Also are you getting these errors for `implementation 'org.tensorflow:tensorflow-lite:2.4.0'` and `implementation 'org.tensorflow:tensorflow-lite-gpu:2.4.0'` or only custom built AAR's? \r\n\r\nI did the previous builds on aws c5 metal instance. It probably took maybe ~20 minutes per build. Other attempts failed as they took too long per build. If I remember correctly a combination of org.tensorflow and custom built aar increased apk size more than using only org.tensorflow implementations.\r\n", "@farmaker47 caching is enabled when you build so It would be reasonable if the build time is about < 30minutes after the first build. I just tried but actually cannot reproduce the issue. Is there any quick way I can reproduce your issue?", "@thaink Hi, thanks for looking into this. I tried another time in the weekend after you have commited changes to build_aar_with_docker.sh and unfortunately I got the same error. The only thing  we can do is give you my .tflite file and try. WDYT?", "@farmaker47 I would prefer a dummy project that explaining how you import those aar files. ", "@thaink Ok I will clean the current and get back to you. Inside you can see the build.gradle file and the .tflite file that I used to generate the .aar ones. Thank you", "@thaink Hi again,\r\nI send you the [link ](https://github.com/farmaker47/OCR_with_Keras/tree/cleaned_interpreter)of my project. Please verify that you are at cleaned_interpreter branch. I followed the instructions [here](https://www.tensorflow.org/lite/guide/build_android#add_aar_directly_to_project) and inserted inside libs folder 3 .aar files. The tensorflow-lite.aar is generated with no arguments when I executed the build_aar_with_docker.sh file. The other 2 (tensorflow-lite-main.aar, tensorflow-lite-ops.aar) where generated adding arguments specified [here](https://www.tensorflow.org/lite/guide/reduce_binary_size#build_aar_files_for_android_project_2). The .tflite file I used when executed the script (with arguments) is inside assets folder of the project (ocr_dr.tflite) and you can use it in the future to build your own custom .aar files if you want. \r\n\r\nInside app's build.gradle file you can see the code for using the .aar files.\r\n\r\n1. When using the (default) tensorflow-lite.aar file (that was generated with no arguments) and the main dependency for the special operators `implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'` (line 78 of build.gradle file) project runs flawlessly.\r\n2. When using only the custom .aar files (tensorflow-lite-main.aar, tensorflow-lite-ops.aar) that where generated adding arguments specified [here](https://www.tensorflow.org/lite/guide/reduce_binary_size#build_aar_files_for_android_project_2), 2 errors occur. First it cannot find Interpreter class to build the project. Second if you use the default .aar file (tensorflow-lite.aar) and the  tensorflow-lite-ops.aar the project builds but app crashes at start with `_ZNK6google8protobuf7Message11GetTypeNameEv` error.\r\n\r\nI am really interested to find out what is wrong with the set up! Thank you for looking into this!\r\n\r\nFYI @khanhlvg ", "I found that I had to add `build --config=monolithic` to `.bazelrc` to avoid the `_ZNK6google8protobuf7Message11GetTypeNameEv` error.\r\n\r\nIt looks like by default the compilation assumes that some of the implementation is already in a system library on device.\r\n\r\nI was able to extract `tensorflow-lite_dummy_app_for_so_deploy.jar` from the build directory which contains the Java implementation of `Interpreter` needed, I'm not sure why this wasn't included in the aar automatically.", "Thanks! I'm going to try that now, will update. The issue with the missing classes in the tensorflow-lite.aar is fixed in master - see https://github.com/tensorflow/tensorflow/issues/45488", "I've added a comment here - https://github.com/tensorflow/tensorflow/issues/46989. Your suggestion helped, thanks!", "I confirmed today that with the latest change at [build_aar.sh file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/build_aar.sh) where at line 125 was added `--config=monolithic` we have a working tensorflow_select_tf_ops.aar file. We have to use the tensorflow_lite.aar file that is downloaded from Maven repository though for everything to work correctly.\r\nFYI @thaink ", "@farmaker47 So what error is still remaining with tensorflow_lite.aar? ", "@thaink My apologies for the mistake! Everything is working correctly. Tensorflow_lite.aar and tensorflow_lite_select_tf_ops.aar are generated and working correctly inside my AS project. Check [this](https://github.com/farmaker47/OCR_with_Keras/tree/with_custom_support_library) repository (the specific branch). Project from 120MB is now 38.8MB!!\r\n\r\nInside this project there is also a custom tensorflow_lite_support.aar file which I have generated inside Docker, based on your method. This library contains some extra functions for grayscale and buffer from one channel images. You can check out the procedure here:\r\nhttps://github.com/farmaker47/Build_TensorFlow_Lite_Support_Library_With_Docker\r\n\r\nThank you again! I hope we collaborate again.\r\n\r\n@khanhlvg FYI\r\n"]}, {"number": 45152, "title": "Loading a Custom Model TFlite into Flutter ", "body": "I follow along this [Colab](https://colab.research.google.com/drive/1qXn9q6m5ug7EWJsJov6mHaotHhCUY-wG?usp=sharing)  to  train a custom model.\r\n\r\n[Conversion process Colab](https://colab.research.google.com/drive/1VRH4K_LPXlTv-YvobWjvxOAowxTYIli8?usp=sharing) \r\n\r\nAfter completing the training process I converted the .pb to .tflite and I got [these files](https://drive.google.com/drive/folders/1I8pU34M9gagLFdImmrpS8qUY6Wge0jds?usp=sharing). When I loaded the model into the [official Android demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android)  I got the following error:\r\n\r\n\r\n```\r\n java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example.sd_detect/com.example.sd_detect.MainActivity}: java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.\r\n```\r\nThe solution to that error is discussed in [this issue](https://github.com/tensorflow/tensorflow/issues/44431)\r\n\r\nI followed the solution:\r\n\r\nInstalling the metadata library with:\r\n```\r\npip install tflite-support\r\n\r\n```\r\nand then executing the following command into python\r\n```\r\n>>> from tflite_support import metadata as _metadata\r\n>>> populator = _metadata.MetadataPopulator.with_model_file('final_model.tflite')\r\n>>> populator.load_associated_files([\"final_model.txt\"])\r\n>>> populator.populate() \r\n```\r\n\r\n\r\nAnd I got the following warning:\r\n\r\n    /home/username/.local/lib/python3.8/site-packages/tensorflow_lite_support/metadata/python/metadata.py:342: UserWarning: File, 'final_model.txt', does not exsit in the metadata. But packing it to tflite model is still allowed.\r\n      warnings.warn(\r\n\r\nBack to Android Studio example I was able to run the model successfully, by just modifying this information:\r\n\r\n    private static final String TF_OD_API_MODEL_FILE = \"final_model.tflite\";  \r\n    private static final String TF_OD_API_LABELS_FILE = \"final_model.txt\";  \r\n\r\nBut my main goal is to use this model into a Flutter application, but when I try to run into Flutter I got the following exeption:\r\n\r\n    /// Throws a [StateError] if the given [expression] is `false`.\r\n    \r\n    void  checkState(bool expression, {message}) {\r\n    \r\n    if (!expression) {\r\n    \r\n    throw  StateError(_resolveMessage(message, 'failed precondition'));\r\n    \r\n    }\r\n    \r\n    }\r\nThe flutter [project](https://github.com/am15h/object_detection_flutter) is fully working with coco_ssd_mobilenet_v1_1.0_quant_2018_06_29\r\n\r\n\r\nI also tried to re-run the metadata commands on my flutter application assets but still the same issue.\r\n\r\nWhat am I missing?", "comments": ["@mlopez0 \r\nPlease share a simple stand alone code to replicate the issue reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "The Issue is flutting-plugin related! \r\n\r\nThanks for the support."]}, {"number": 45150, "title": "How to generate a Reshape TFLite model with the new shape as attributes?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): v2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```py\r\nimport tensorflow as tf  # tf 2.3.0\r\nimport numpy as np\r\n\r\nISHAPE = (1, 2, 3, 4)\r\nOSHAPE = (int(np.product(ISHAPE)),)\r\n\r\n\r\ndef genWithKeras():\r\n  data = tf.keras.Input(dtype='float32', name='input', batch_size=ISHAPE[0], shape=ISHAPE[1:])\r\n  reshape = tf.keras.layers.Reshape(OSHAPE, name='reshaped')(data)\r\n  model = tf.keras.Model(inputs=[data], outputs=[reshape])\r\n  return tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n\r\ndef genWithTFModel():\r\n  class ReshapeModule(tf.Module):\r\n    def __init__(self):\r\n      super(ReshapeModule, self).__init__()\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(ISHAPE, tf.float32)])\r\n    def __call__(self, data):\r\n      return tf.reshape(data, OSHAPE)\r\n\r\n  module = ReshapeModule()\r\n  tf.saved_model.save(module, 'reshape.saved_model')\r\n  return tf.lite.TFLiteConverter.from_saved_model('reshape.saved_model')\r\n\r\nconverter = genWithKeras()\r\n# converter = genWithTFModel()\r\ntflite_model = converter.convert()\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n![](https://user-images.githubusercontent.com/4936589/99535868-78bf0900-29e4-11eb-86e9-8482d57e5a4b.png)\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nNone\r\n```\r\n\r\n**Failure details**\r\nMay I know the details of how the TFLite models are generated, in particular, the code path to produce the TFLite models? **Should be irrelevant to the model detail**.\r\n\r\nWe are working on [`tflite2onnx`](https://github.com/jackwish/tflite2onnx) which can convert TFLite models to ONNX. When converting the [palm_detection](https://github.com/google/mediapipe/blob/master/mediapipe/modules/palm_detection/palm_detection.tflite) model from [MediaPipe](https://github.com/google/mediapipe), we noticed that the `Reshape` of it accepts **only one input** - the output shape is as the attribute of `Reshape` as the figure below.\r\n\r\n<img width=\"528\" alt=\"palm-detect\" src=\"https://user-images.githubusercontent.com/4936589/99535493-ee76a500-29e3-11eb-829f-30239911b9fd.png\">\r\n\r\nWe tried to build a standalone case for `Reshape` (a model which contains `Reshape` only), but there are **always two inputs** in our model with either Keras API or TensorFlow API (as the repro code above). We are curious about how to generate a Reshape TFLite model with the new shape as attributes?\r\n\r\n\r\n**RNN conversion support**\r\nNone\r\n\r\n**Any other info / logs**\r\n\r\nThe source issue we have is: https://github.com/jackwish/tflite2onnx/issues/23#issuecomment-725070754 and we are moved from [this MediaPipe issue](https://github.com/google/mediapipe/issues/1310)", "comments": ["Hi, @renjie-liu Do you have any idea for that? And I'm just for some watermelons on this problem, please forget me.....", "The TFLite Reshape op always have two inputs, data and shape. In my understanding, there are no ways currently to generate the shape as an attribute.", "@abattery Thanks for your reply! Yes, we have not found a way to generate such a model. But as described in the issue description, the [palm_detection](https://github.com/google/mediapipe/blob/master/mediapipe/modules/palm_detection/palm_detection.tflite) model from [MediaPipe](https://github.com/google/mediapipe) contains such a `Reshape` with shape as its attribute (as the figure below). \r\n\r\n![](https://user-images.githubusercontent.com/4936589/99535493-ee76a500-29e3-11eb-829f-30239911b9fd.png)\r\n\r\nMaybe they are using some _special_ TensorFlow build?...\r\n\r\n", "Maybe the old TensorFlow Lite TOCO converter was able to create this kind of Reshape node.", "Sorry for the late reply, just saw the thread.\r\n\r\n- In tf, [reshape](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/array_ops.cc#L1368-L1376) op always have two inputs.\r\n- In the mlir converter, the tflite [reshape](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/ir/tfl_ops.td#L2575-L2593) op indeed will always have two inputs.\r\n- In TOCO, we do not have such restriction, so during transformations, we may introduce `reshape` op without the shape tensor as inputs.\r\n- And in the kernel, we can handle `reshape` without shape tensor as well (if the shape attribute is presented)\r\n\r\nBut it's good to ensure the `reshape` op has two inputs:\r\n\r\n- It's good for us to do feature parity with tf and reduce fragmentations\r\n- It's easier to handle dynamic shape case.", "@abattery @renjie-liu Yes, maybe they were using TOCO. I can try some legacy versions. Good to know the restriction in MLIR converter, Thanks!\r\n\r\n"]}, {"number": 45148, "title": "tf-trained model not working after converting to tf-lite for raspi", "body": "**System information**\r\nLaptop: \r\nLinux Ubuntu\r\nTensorflow 1.15.0\r\n\r\nRaspi:\r\nRaspberry Pi 4\r\ntflite-runtime 2.5.0\r\ntensorflow-estimator 1.14.0\r\nCoral Edge TPU\r\n\r\nHello everybody, I am stuck at getting my trained model running on raspi. I trained ssd_mobilenet_v2_coco model from tensorflow 1 modelzoo with my own custom dataset on google cloud with this config file where I did few changes:\r\n[ssd_mobilenet_v2_coco_config.zip](https://github.com/tensorflow/tensorflow/files/5589151/ssd_mobilenet_v2_coco_config.zip)\r\n\r\nAfter training I exported the frozen_inference_graph which worked pretty well on my laptop, so I exported the tflite graph (tflite_graph.pb and tflite_graph.pbtxt) with the following command:\r\n`python export_tflite_ssd_graph.py \\ \r\n--pipeline_config_path=gs://objects1119/ssd_mobilenet_v2_coco.config \\ \r\n--trained_checkpoint_prefix=gs://objects1119/model.ckpt-189879 \\ \r\n--output_directory=tflite \\\r\n--add_postprocessing_op=true`\r\n\r\nafter that I converted it to detect.tflite file with:\r\n`tflite_convert --graph_def_file=tflite/tflite_graph.pb  --output_file=tflite/detect.tflite  --output_format=TFLITE --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=127 --change_concat_input_ranges=false --allow_custom_ops`\r\n\r\nYou can see the file here:\r\n[detect_tflite.zip](https://github.com/tensorflow/tensorflow/files/5589264/detect_tflite.zip)\r\n\r\nAfter that I converted it to edgetpu-file with this google colab: https://colab.research.google.com/drive/1o6cNNNgGhoT7_DR4jhpMKpq3mZZ6Of4N?usp=sharing \r\nwhile using the EdjeElectronics Tutorial: \r\nhttps://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md \r\n everything worked without errors and I gut this file:\r\n[detect_edgetpu.zip](https://github.com/tensorflow/tensorflow/files/5589397/detect_edgetpu.zip)\r\n\r\nBut when I finally try to run it on the raspi with this file\r\n[TFLite_detection_webcam.zip](https://github.com/tensorflow/tensorflow/files/5589323/TFLite_detection_webcam.zip)\r\nI am always facing the same output, no matter what input. There is always Class Zero predicted on the same areas of the image with prediction of 50%. What am I doing wrong? Is it because of the changed input shapes in the config file for training? In my case it would be really complicated to train again so I try to avoid this if its possible.\r\nThank you very much in advance.\r\n", "comments": ["Are there any suggestions or do you need more information about the issue?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45148\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45148\">No</a>\n"]}, {"number": 45147, "title": "python3.6 Mac OS X 11.0.1  Could not find a version that satisfies the requirement tensorflow", "body": "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow", "comments": ["@DAJINZI01,\r\nCould you please update pip using the below commands and check if you are still facing the same issue?\r\n\r\n```\r\npip install --upgrade pip\r\npip install tensorflow\r\n```\r\n\r\nThanks!", "there is nothing to do with pip. i think it is the problem of Mac OS X 11.0.1 system", "@DAJINZI01,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/44615#issuecomment-727439589) from a similar issue and let us know if it helps. \r\n\r\nAlso, could you please let us know which version of TensorFlow you are trying to install. Thanks!", "thinks. I reboot my machine, and it works.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45147\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45147\">No</a>\n"]}, {"number": 45146, "title": "Missing call to model.optimizer._clip_gradients(grads)?", "body": "Hi,\r\n\r\nI've been having problems training a model and, after reading the keras training loop code, it seems that the call to \r\nmodel.optimizer._clip_gradients(grads) introduced in 69da929ad4d5ba605507efa1f52b382a55b6a969 is missing again.\r\n\r\nHas it been moved somewhere else?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ab953ab8c5612c67b07e4159fb9eed9edef089ff/tensorflow/python/keras/engine/training_eager_v1.py#L278", "comments": ["@jponf \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\n", "Hi, the behavior was already explained on a previous issue #33929, were it is mentioned that TF 2.2 fixed it. But it seems that it is happening again in TF 2.3.0\r\n\r\n* TF version: 2.3.0\r\n* TF Keras version: 2.4.0\r\n\r\nI was training a CRNN and tried to apply clipnorm to avoid exploding gradients, but the issue persisted after adding the parameter to the optimizer. Then I just sub-classed tf.keras.Model and modified the `train_step` method to apply `tf.clip_by_global_norm` before calling the optimizer and so far it seems the exploding gradients problem is gone.\r\n", "@jponf \r\nI ran the code on tf-nightly and [tf 2.3](https://colab.research.google.com/gist/Saduf2019/ccbeccee471ec883c4deb875538cc173/untitled476.ipynb) do not see the nan values, please refer to this [gist here](https://colab.research.google.com/gist/Saduf2019/77fb086287b1846cf61302ad3a2f3f93/untitled472.ipynb).", "In my particular case it wasn't NaNs what I found, but the running mean of the batchnorm layers that skyrocketed to values between 1000 and 300000, even though the input was normalized between 0 and 1. \r\n\r\nA complete rewrite of the model in PyTorch (with gradient clipping) did not has this issue, and as I said, a custom `train_step` with `tf.clip_by_global_norm` before applying the gradients didn't show that behavior either. That is what lead me to the issue mentioned in my previous comment.\r\n\r\nIf my schedule allows me to, I'll try to explore this issue in more detail. \r\n\r\nIn the meanwhile, what I found is that line 277, were gradient clipping is applied, in\r\nhttps://github.com/tensorflow/tensorflow/blob/69da929ad4d5ba605507efa1f52b382a55b6a969/tensorflow/python/keras/engine/training_eager.py\r\nis gone in:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training_eager_v1.py\r\n\r\nWhich may be completely unrelated to my problem but so far is the only thing that I know may explain the behavior that I observed.", "@jponf Can you please share a simple standalone code to reproduce the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45146\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45146\">No</a>\n"]}]