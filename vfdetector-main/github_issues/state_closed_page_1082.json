[{"number": 20813, "title": "cannot find package \"github.com/tensorflow/tensorflow/tensorflow/go\"", "body": "Hi,\r\n## System Information\r\n\r\n#### os version: OS X 10.13.6\r\n\r\n#### python version: Python 2.7.15\r\n\r\ngo get -d github.com/tensorflow/tensorflow/tree/master/tensorflow/go\r\n```\r\npackage github.com/tensorflow/tensorflow/tree/master/tensorflow/go: cannot find package \"github.com/tensorflow/tensorflow/tree/master/tensorflow/go\" in any of:\r\n\t/usr/local/opt/go/libexec/src/github.com/tensorflow/tensorflow/tree/master/tensorflow/go (from $GOROOT)\r\n\t/Users/ebubekirtabak/go-workspace/src/github.com/tensorflow/tensorflow/tree/master/tensorflow/go (from $GOPATH)\r\n```", "comments": []}, {"number": 20812, "title": "Upgrade canned estimators for TensorFlow 2.0 (support for tf.optimizers).", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**:  3.5.4\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: \r\n\r\n`\r\n# Code example of train method with tf.contrib.learn ( Which works )\r\n    def get_stepw_decay_optimizer():\r\n      def step_decay(global_step):\r\n        return tf.train.piecewise_constant(global_step, boundaries=[10000,20000,50000,100000], values=[0.001,0.0005,0.0001,0.00005,0.00002])\r\n                                         # use customized decay function in learning_rate\r\n      return tf.train.AdamOptimizer(learning_rate=step_decay(tf.train.get_global_step()))#,epsilon=0.001)\r\n\r\n    classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_cols,\r\n                                          n_classes=2,\r\n                                          hidden_units=[32,64,64,64,64,64,64,64,64,64,64,64,64,\r\n                                                        64,64,64,64,64,32],\r\n                                          dropout = 0.1,\r\n                                          optimizer=get_stepw_decay_optimizer)\r\n\r\n    classifier.fit(input_fn=get_input_fn(training_set), steps=125000)\r\n`\r\n`\r\n# Same code example of train_and_evaluate with tf.estimator ( Which fails )\r\n    def get_stepw_decay_optimizer():\r\n      def step_decay(global_step):\r\n        return tf.train.piecewise_constant(global_step, boundaries=[10000,20000,50000,100000], values=[0.001,0.0005,0.0001,0.00005,0.00002])\r\n                                         # use customized decay function in learning_rate\r\n      return tf.train.AdamOptimizer(learning_rate=step_decay(tf.train.get_global_step()))#,epsilon=0.001)\r\n\r\n    estimator = tf.estimator.DNNClassifier(feature_columns=create_feature_cols(),\r\n                                          n_classes=2,\r\n                                          hidden_units=[32,64,64,64,64,64,\r\n                                                        64,64,64,64,64,64,\r\n                                                        64,64,64,64,64,64,\r\n                                                        32],\r\n                                          dropout = 0.1,\r\n                                          optimizer=get_stepw_decay_optimizer)\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn = make_input_fn(traindf, None), \r\n                                      max_steps = num_train_steps)\r\n\r\n    exp = tf.estimator.LatestExporter(\"decision\", serving_fn)\r\n\r\n    eval_spec = tf.estimator.EvalSpec(input_fn = make_input_fn(evaldf, 1), \r\n                                    steps = None, \r\n                                    exporters = exp,\r\n                                    start_delay_secs = 1, # start evaluating after N seconds, \r\n                                    throttle_secs = 40)  # evaluate every N seconds\r\n\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n`\r\n\r\n### Describe the problem\r\n**While trying to implement learning rate decay in canned estimator, I encountered the below issue.**\r\n\r\nWhen using the canned DNNClassifier from **tf.contrib.learn** it is possible to pass a function as input to parameter \"optimizer\" when using \"fit\" method but errors out when using \"train_and_evaluate\" of estimator from class **tf.estimator.DNNClassifier**, **The given object is not an Optimizer instance. Given: <function get_stepw_decay_optimizer at 0x000002076BACBF28>**. I believe, both the methods should work the same way and not raise any errors.\r\n\r\n### Source code / logs\r\nError Logs\r\n`INFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\hrafiq\\AppData\\Local\\Temp\\tmpa1rpeahr\r\nINFO:tensorflow:Using config: {'_session_config': None, '_log_step_count_steps': 100, '_is_chief': True, '_service': None, '_tf_random_seed': None, '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_num_worker_replicas': 1, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_model_dir': 'C:\\\\Users\\\\hrafiq\\\\AppData\\\\Local\\\\Temp\\\\tmpa1rpeahr', '_num_ps_replicas': 0, '_task_type': 'worker', '_master': '', '_task_id': 0, '_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002076BFCBC88>}\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 40 secs (eval_spec.throttle_secs) or training is finished.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-28-53806042763d> in <module>()\r\n----> 1 train_and_evaluate(None, num_train_steps=200000)\r\n\r\n<ipython-input-27-182baf3372f9> in train_and_evaluate(output_dir, num_train_steps)\r\n     20                                     start_delay_secs = 1, # start evaluating after N seconds,\r\n     21                                     throttle_secs = 40)  # evaluate every N seconds\r\n---> 22     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    428       config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    429     logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 430     executor.run_local()\r\n    431     return\r\n    432 \r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py in run_local(self)\r\n    607           input_fn=self._train_spec.input_fn,\r\n    608           max_steps=self._train_spec.max_steps,\r\n--> 609           hooks=train_hooks)\r\n    610 \r\n    611       # Final export signal: For any eval result with global_step >= train\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    300 \r\n    301     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 302     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    303     logging.info('Loss for final step: %s.', loss)\r\n    304     return self\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n    709       with ops.control_dependencies([global_step_read_tensor]):\r\n    710         estimator_spec = self._call_model_fn(\r\n--> 711             features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n    712       # Check if the user created a loss summary, and add one if they didn't.\r\n    713       # We assume here that the summary is called 'loss'. If it is not, we will\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n    692     if 'config' in model_fn_args:\r\n    693       kwargs['config'] = config\r\n--> 694     model_fn_results = self._model_fn(features=features, **kwargs)\r\n    695 \r\n    696     if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py in _model_fn(features, labels, mode, config)\r\n    332           dropout=dropout,\r\n    333           input_layer_partitioner=input_layer_partitioner,\r\n--> 334           config=config)\r\n    335     super(DNNClassifier, self).__init__(\r\n    336         model_fn=_model_fn, model_dir=model_dir, config=config)\r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn.py in _dnn_model_fn(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config)\r\n    167                      'Given type: {}'.format(type(features)))\r\n    168   optimizer = optimizers.get_optimizer_instance(\r\n--> 169       optimizer, learning_rate=_LEARNING_RATE)\r\n    170   num_ps_replicas = config.num_ps_replicas if config else 0\r\n    171 \r\n\r\nc:\\users\\hrafiq\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\optimizers.py in get_optimizer_instance(opt, learning_rate)\r\n     75   if not isinstance(opt, optimizer_lib.Optimizer):\r\n     76     raise ValueError(\r\n---> 77         'The given object is not an Optimizer instance. Given: {}'.format(opt))\r\n     78   return opt\r\n\r\nValueError: The given object is not an Optimizer instance. Given: <function get_stepw_decay_optimizer at 0x000002076BACBF28>\r\n`\r\n", "comments": ["Any updates on this issue team ? We are not able to use train evaluate pipeline due to the above stated issue. \r\n\r\nPlease let us know if you need us to provide any other information.", "We updated the issue as we found that issue is in tf.estimator package itself and works in tf.contrib. \r\n\r\nHowever we think it should work same in both the places and tf.estimator.DNNClassifier is the new version for tf.contrib.learn.DNNClassifier .", "No updates yet ? Its over a week now and no response from anyone.\r\n\r\nWe are stuck over this in a client build and really appreciate if someone can look into the incident.", "I've encountered this problem as well. The example given in [DNNRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor) and [DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) do not work anymore.\r\n\r\n - until there is a fix, has anybody found a workaround?", "Fixed the issue myself, the way is to pass function as lambda: \r\n\r\n```\r\nestimator = tf.estimator.DNNLinearCombinedRegressor(\r\n                                          linear_feature_columns=wide,\r\n                                          dnn_feature_columns=deep,\r\n                                          dnn_hidden_units=[32,64,128,128,64,64,32,16,8],\r\n#                                           dnn_dropout = 0.2,\r\n                                          linear_optimizer=tf.train.FtrlOptimizer(learning_rate=0.1),\r\n                                          dnn_optimizer= lambda: tf.train.AdamOptimizer(tf.train.exponential_decay(\r\n                                                            learning_rate=0.01,\r\n                                                            global_step=tf.train.get_global_step(),\r\n                                                            decay_steps=10000,\r\n                                                            decay_rate=0.96)),\r\n```", "@rafiqhasan Thanks for providing solution to the issue. I am closing it as it was resolved by @rafiqhasan ", "This is NOT a solution, it's a work-around. Please fix the actual issue. (and the workaround doesn't work with DNNClassifier anyway.) \r\n\r\nValueError: The given object is not an Optimizer instance. Given: (<function <lambda> at 0x000001341C603A60>,)", "> This is NOT a solution, it's a work-around. Please fix the actual issue. (and the workaround doesn't work with DNNClassifier anyway.)\r\n> \r\n> ValueError: The given object is not an Optimizer instance. Given: (<function at 0x000001341C603A60>,)\r\n\r\nDoesn't give any error on TF 1.10+ , see below working code for your reference:\r\n\r\n`\r\nestimator = tf.estimator.DNNClassifier(feature_columns=create_feature_cols(),\r\n                                          n_classes=NCLASSES,\r\n                                          hidden_units=[256,128,32],\r\n                                          model_dir=output_dir,\r\n                                           dropout = 0.1,\r\n                                          optimizer=lambda: tf.train.AdamOptimizer(\r\n                                                tf.train.exponential_decay(\r\n                                                learning_rate=0.01, global_step=tf.train.get_global_step(),\r\n                                                decay_steps=4000, decay_rate=0.9)),\r\n                                          batch_norm = True,\r\n                                          config = run_config)\r\n`", "We are in the process of upgrading canned estimators for TF 2.0 (this is a 2.0-blocker). When that process is complete, `tf.estimator.*` will support Keras optimizers; however, they do not today.\r\n\r\nPlease track this issue to keep updated on progress.", "@dynamicwebpaige Thanks for the info, I'm having this problem today with TF 2.0. There's any known workaround to make optimizers work with canned estimators?\r\nBoth tf.keras.optimizers and tf.optimizers are currently not working.", "@ClaudioDavi tf.keras.optimizers should work with canned estimator in TF 2.0. What problems do you have? Could you show a code example?", "Current Canned estimators in TF 2.0 support tf.keras.optimizers already. Check DNN here as an example: https://github.com/tensorflow/estimator/blob/c956dd32561bac645a1cd870d3c8cfe8e9fe969b/tensorflow_estimator/python/estimator/canned/dnn.py#L580\r\n\r\nThe migration guide is available here: https://www.tensorflow.org/beta/guide/migration_guide#premade_estimators\r\n\r\nI will close this issue for now. Please re-open it or file a new one if you find more issues. Thanks."]}, {"number": 20811, "title": "MKL_DNN License File missing ", "body": "Hello, \r\n\r\nIt seems like since commit #20576 (specific PR : [here](https://github.com/tensorflow/tensorflow/commit/03ab64c3a68f3b990bf690ede06e3066ad4e35a0#diff-2f07a0589130c83666437024dd03d252)), building TF with MKL option forces the configure step to look for a LICENSE file inside the `third_party/mkl_dnn` folder, which actually doesn't exist. \r\n\r\nCurrently, I'm using this hack in my Docker for the build to proceed without needing the explicit DNN LICENSE file (by copying the MKL license file into the MKL_DNN folder since they are identical licenses), but it'd be great if we could have this fixed sometime? I opened a small PR with this small fix (copied the LICENSE file from the original MKL-DNN repo in #20810 (identical to MKL license, but just for pedantic reasons)) so maybe this will help other users out as well.\r\n\r\n## Environment Info:\r\nHave I written custom code: No\r\nOS Platform and Distribution: Tested on Debian:Stretch\r\nTensorFlow installed from: Source\r\nTensorFlow version:  1.9.0 (Master branch)\r\nBazel version: 0.15.0\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: \r\n```\r\nd /opt && \\\r\n    git clone https://github.com/tensorflow/tensorflow.git && \\\r\n    cd /opt/tensorflow && \\\r\n    cp /opt/tensorflow/third_party/mkl/LICENSE /opt/tensorflow/third_party/mkl_dnn/LICENSE && \\\r\n    /bin/bash ./configure \\\r\n    && \\\r\n    bazel build --config=opt --config=mkl \\\r\n    tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nThanks!\r\n\r\nNafis", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Added the info to my main post, thanks!", "Has this issue been fixed by #20810? Should I close it?", "Yep it's fixed!"]}, {"number": 20810, "title": "Adding MKL DNN license from Intel's repo", "body": "Adding the LICENSE file so that `third_party/mkl_dnn` LICENSE file is picked up upon MKL configured build.\r\n\r\nThis build works now in a Docker (refer to #20811), where initially the missingness of the MKL-DNN LICENSE file would error out since commit #20576 forces the check for the DNN license.\r\n\r\n```\r\nbazel build --config=opt --config=mkl \\\r\n    tensorflow/tools/pip_package:build_pip_package \r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "`I signed it!`", "CLAs look good, thanks!\n\n<!-- ok -->", "@qlzh727 hmm it looks like the MKL LICENSE file in `third_party/mkl/LICENSE` also has the two fileds as blank. We should be consistent with that eh?", "I think they they should be consistent. Also waiting for input from @martinwicke who probably have more detailed knowledge.", "Hmm is there a reason why `import/copybara` check failed?", "@yifeif, not sure why copybara is not porting the PR to internal.", "Discussed offline with @qlzh727. Fixed now."]}, {"number": 20809, "title": "Fix support for renorm with float16", "body": "This PR fixes a bug when using BatchNormalization with `renorm` set and the input tensor `x` data type is float16 or bfloat16.\r\n\r\nA simple code to exploit the bug:\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import BatchNormalization\r\n\r\nX = tf.placeholder(tf.float16, [8, 256, 256, 32], 'X')\r\n\r\nrenorm_clipping = {\r\n    'rmin': tf.constant(1, tf.float16, [], 'rmin'),\r\n    'rmax': tf.constant(1, tf.float16, [], 'rmax'),\r\n    'dmax': tf.constant(0, tf.float16, [], 'dmax'),\r\n}\r\n\r\nBatchNormalization(renorm=True, renorm_clipping=renorm_clipping)(X, True)\r\n```", "comments": ["Could we have a look at this please?", "Nudge @fchollet for review.", "Thanks for the change and sorry for the long delay. The code looks good, can you add a unit test for that? Thanks.", "No problem, I'll be adding the unittest asap.", "@fchollet Thank you for the review.\r\n\r\nFor the first part I think it might not be the best to add the type check before a cast for a couple of reasons\r\n  1) It would add a lot of redundant code before each cast.\r\n  2) Type checking using ```x.dtype != target_dtype``` would work if ```x``` is a tensor but will fail for python lists, tuples, scalars.\r\n\r\nA better design would be to add the type check in the cast function itself which already takes place as per [here](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/math_ops.py#L654) if ```x``` is a tensor, so the unnecessary cast ops wouldn't be a problem, the case where an unnecessary cast op might emerge if ```x``` is not a tensor and a ```convert_to_tensor``` call might be sufficient is already a ```TODO```.\r\n\r\nKindly correct me if I missed something, as for the second part I'll add it right away.", "@fchollet @qlzh727 I've added the unit test kindly check and review.", "Ping @fchollet for review.", "@qlzh727 I've fixed the indentation problem, please rerun the checks.", "@Mostafa-Alaa , can u please resolve the merge conflict? Thanks.", "Nagging Reviewer @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 89 days with no activity and the `awaiting review` label has been applied.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 20808, "title": "Feature requested by issue 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter ", "body": "Feature requested by issue 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter ", "comments": ["Thanks for adding the gradients!\r\n\r\nIt looks like the lint is failing due to 4 space indentation and some other style issues. Checkout [this section](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#python-coding-style) on how to run `pylint` locally so this PR can be reviewed and merged.", "Ping @chsigg for review again.", "This PR is failed for internal test since it was using placeholder in tests, which is not eager mode compatible. @chsigg, can u please take a look again? Thanks.", "Ping @chsigg again for internal merge.", "Nagging Reviewer @chsigg, @azaks2: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 20807, "title": "Fix bug inside boston.py in boosted_trees with python 3", "body": "This fix tries to address the issue raised in #20776 where run the sample with boston.py in boosted_trees throws out error with python 3:\r\n```\r\n$ python3 boston.py \\\r\n>   --batch_size=404 --output_dir=\"/tmp/boston\" --depth=4 --learning_rate=0.1 \\\r\n>   --num_eval_steps=1 --num_trees=500 --l2=0.001 \\\r\n>   --vmodule=training_ops=1\r\n......\r\n......\r\nTraceback (most recent call last):\r\n  File \"boston.py\", line 169, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n......\r\n......\r\n  File \"/usr/lib/python3.5/genericpath.py\", line 145, in _check_arg_types\r\n    raise TypeError(\"Can't mix strings and bytes in path components\") from None\r\nTypeError: Can't mix strings and bytes in path components\r\n```\r\nThe reason for the error was because in python 3, the export_dir was returned as bytes.\r\n\r\nThis fix as the `compat.as_bytes` which is consistent with other places in tensorflow.\r\n\r\nThis fix fixes #20776\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@sshrdp, do u want to take a look?"]}, {"number": 20806, "title": "tf.estimator.export_savedmodel is not exporting properly ", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\n== cat /etc/issue ===============================================\r\nLinux 309288d6d453 4.14.33+ #1 SMP Wed Jun 20 01:15:52 PDT 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"17.10 (Artful Aardvark)\"\r\nVERSION_ID=\"17.10\"\r\nVERSION_CODENAME=artful\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.2.0-8ubuntu3.2) 7.2.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 309288d6d453 4.14.33+ #1 SMP Wed Jun 20 01:15:52 PDT 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                    1.14.5   \r\nprotobuf                 3.6.0    \r\ntensorflow               1.9.0rc2 \r\ntensorflow-hub           0.1.0    \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.9.0-rc2\r\ntf.GIT_VERSION = unknown\r\ntf.COMPILER_VERSION = unknown\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh.1: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\nunknown 1.9.0-rc2\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n```\r\nfeature_columns = [tf.feature_column.numeric_column(\"x\", shape=[28, 28])]\r\n\r\nclassifier = tf.estimator.DNNClassifier(\r\n feature_columns=feature_columns,\r\n hidden_units=[256, 32],\r\n optimizer=tf.train.AdamOptimizer(1e-4),\r\n n_classes=10,\r\n dropout=0.1,\r\n model_dir=\"mnist_model\",\r\n)\r\n\r\nclassifier.train(input_fn=train_input_fn, steps=10)\r\n\r\nimage = tf.placeholder(tf.float32, [None, 28*28])\r\nmodel_path = classifier.export_savedmodel(\"exported_fashion_minist\",\r\n                          tf.estimator.export.build_raw_serving_input_receiver_fn({\"x\":image}))\r\n```\r\nwould not contain `serving_default` signature, and if i do `image = tf.placeholder(tf.string, [None, 28*28])`, there would be no way to convert the string to numberical_column, which means the feature column is not working\r\n\r\n```\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\r\nINFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\r\nINFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'x': <tf.Tensor 'Placeholder_3:0' shape=(?, 784) dtype=float32>}\r\nINFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'x': <tf.Tensor 'Placeholder_3:0' shape=(?, 784) dtype=float32>}\r\nWARNING:tensorflow:Export includes no default signature!\r\nINFO:tensorflow:Restoring parameters from mnist_model/model.ckpt-10\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:SavedModel written to: exported_fashion_minist/temp-b'1531602129'/saved_model.pb\r\nb'exported_fashion_minist/1531602129'\r\n```\r\n", "comments": ["nvm..\r\n\r\nthere is `tf.estimator.export.build_parsing_serving_input_receiver_fn`"]}, {"number": 20805, "title": "Eager execution and custom layer", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colaboratory\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.9.0-rc2\r\n- **Python version**:  version 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: Colab GPU\r\n- **Exact command to reproduce**: https://gist.github.com/nairouz/035a830d1e58a3759a6a1e193f5defed\r\n\r\n### Describe the problem\r\nI have a simple Keras model with one custom layer which works fine on the graph based execution. When I switched to eager execution via tf.enable_eager_execution(), I got stuck on a weird error.\r\n\r\n### Source code \r\n    import numpy as np\r\n    import tensorflow as tf\r\n    import tensorflow.keras.backend as K\r\n    from tensorflow.keras.models import Model\r\n    from tensorflow.keras.layers import Layer, Input\r\n    from tensorflow.keras.losses import kullback_leibler_divergence\r\n\r\n    tf.enable_eager_execution()\r\n\r\n    class ClusteringLayer(Layer):\r\n        def __init__(self, output_dim, input_dim=None, alpha=1.0, **kwargs):\r\n            self.output_dim = output_dim\r\n            self.input_dim = input_dim\r\n            self.alpha = alpha\r\n            super(ClusteringLayer, self).__init__(**kwargs)\r\n\r\n        def build(self, input_shape):\r\n            self.W = self.add_weight(name='kernel', shape=(self.output_dim, input_shape[1].value), initializer='Identity', trainable=True)\r\n            super(ClusteringLayer, self).build(input_shape)\r\n\r\n        def call(self, x, mask=None):\r\n            q = 1.0/(1.0 + K.sqrt(K.sum(K.square(K.expand_dims(x, 1) - self.W), axis=2))**2 /self.alpha)\r\n            q = q**((self.alpha+1.0)/2.0)\r\n            q = K.transpose(K.transpose(q)/K.sum(q, axis=1))\r\n            return q\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return (input_shape[0], self.output_dim)\r\n\r\n    def clustering_loss(y_true, y_pred): \r\n        a = K.square(y_pred) / K.sum(y_pred, axis=0) \r\n        p = K.transpose(K.transpose(a) / K.sum(a, axis=1))\r\n        loss = kullback_leibler_divergence(p, y_pred)\r\n        return loss\r\n\r\n    input1 = Input(shape=(10,), name=\"input\")\r\n    out = ClusteringLayer(output_dim = 5, name='clustering')(input1)\r\n    model = Model(inputs=input1, outputs=out) \r\n    model.compile(optimizer=tf.train.AdamOptimizer(1e-3), loss={'clustering' : clustering_loss})\r\n    np.random.seed(0)\r\n    X = np.random.random((20, 10)).astype(np.float32)\r\n    Y = np.random.random((20, 5)).astype(np.float32)\r\n    model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)\r\n\r\n###Logs\r\nclustering\r\nEpoch 1/10\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-2-1f474aaabb09> in <module>()\r\n     37 Y = np.random.random((20, 5))\r\n     38 \r\n---> 39 model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)\r\n     40 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1331           initial_epoch=initial_epoch,\r\n   1332           steps_per_epoch=steps_per_epoch,\r\n-> 1333           validation_steps=validation_steps)\r\n   1334     else:\r\n   1335       return training_arrays.fit_loop(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n   1039             shuffle=shuffle,\r\n   1040             num_train_samples=num_train_samples,\r\n-> 1041             do_validation=do_validation)\r\n   1042       callbacks.on_epoch_end(epoch, epoch_logs)\r\n   1043       if callback_model.stop_training:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in batch_fit_loop(model, inputs, targets, epoch_logs, index_array, out_labels, callback_model, batch_size, sample_weights, val_inputs, val_targets, val_sample_weights, callbacks, shuffle, num_train_samples, do_validation)\r\n    395         targets_batch,\r\n    396         sample_weights=sample_weights_batch,\r\n--> 397         training=True)\r\n    398 \r\n    399     if not isinstance(outs, list):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, sample_weights, training)\r\n    787       outs, loss, loss_metrics = _model_loss(model, inputs, targets,\r\n    788                                              sample_weights=sample_weights,\r\n--> 789                                              training=training)\r\n    790       if loss is None:\r\n    791         raise ValueError('The model cannot be run '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, sample_weights, training)\r\n    126       outs = model.call(inputs[0], training=training)\r\n    127     else:\r\n--> 128       outs = model.call(inputs[0])\r\n    129   else:\r\n    130     if model._expects_training_arg:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    718     outputs, _ = self._run_internal_graph(inputs,\r\n    719                                           training=training,\r\n--> 720                                           mask=masks)\r\n    721     return outputs\r\n    722 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)\r\n    939     output_shapes = []\r\n    940     for x in self.outputs:\r\n--> 941       assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\r\n    942       tensor, mask = tensor_map[str(id(x))]\r\n    943       output_shapes.append(backend.int_shape(x))\r\n\r\nAssertionError: Could not compute output DeferredTensor('None', shape=(5,), dtype=float32)\r\n\r\n\r\n", "comments": ["Even the simplistic custom layer from the Keras documentation  produces the same error:\r\n\r\n[https://keras.io/layers/writing-your-own-keras-layers/](url)\r\n\r\n    class MyLayer(Layer):\r\n\r\n        def __init__(self, output_dim, **kwargs):\r\n            self.output_dim = output_dim\r\n            super(MyLayer, self).__init__(**kwargs)\r\n\r\n        def build(self, input_shape):\r\n            # Create a trainable weight variable for this layer.\r\n            self.kernel = self.add_weight(name='kernel', \r\n                                      shape=(input_shape[1].value, self.output_dim),\r\n                                      initializer='uniform',\r\n                                      trainable=True)\r\n            super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\r\n\r\n        def call(self, x):\r\n            return K.dot(x, self.kernel)\r\n\r\n        def compute_output_shape(self, input_shape):\r\n            return (input_shape[0], self.output_dim)`\r\n\r\nI  also made sure to verify that it has nothing to do with my custom loss function by trying the vanilla mean_square_error, but still the same error. I have asked the same question on Stackoverflow and I didn't receive any response till now:\r\n[https://stackoverflow.com/questions/51342304/keras-custom-layer-and-eager-execution](url)\r\nI think it is more like a bug.", "When I tried to check the output of my custom layer, I was surprised to find that this layer is generating two outputs. The first one is ambiguous and undesired.\r\n\r\n### Code\r\n\r\n    input1 = Input(shape=(10,), name=\"input\")\r\n    layer = ClusteringLayer(output_dim = 5, name='clustering')\r\n    layer1 = MyLayer(output_dim=5, name=\"nairouz\")\r\n    out = layer(input1)\r\n    out1 = layer1(input1)\r\n    print(out)\r\n    print(out1)\r\n\r\n### Output\r\n`[<DeferredTensor 'None' shape=(?,) dtype=float32>, <DeferredTensor 'None' shape=(5,) dtype=float32>]\r\n [<DeferredTensor 'None' shape=(?,) dtype=float32>, <DeferredTensor 'None' shape=(5,) dtype=float32>]`\r\n", "There is another undesirable behavior in the case of the static graph for the newest version 1.9. \r\nI was compelled to use the attribute \"value\" for the \"input_shape\" argument otherwise it would produce an error. \r\n\r\n### Code: without using the \"value\" attribute\r\n\r\n    class ClusteringLayer(Layer):\r\n        def __init__(self, output_dim, input_dim=None, alpha=1.0, **kwargs):\r\n            self.output_dim = output_dim\r\n            self.input_dim = input_dim\r\n            self.alpha = alpha\r\n            super(ClusteringLayer, self).__init__(**kwargs)\r\n\r\n        def build(self, input_shape):\r\n            self.W = self.add_weight(name='kernel', shape=(self.output_dim, input_shape[1]), initializer='Identity', trainable=True)\r\n            super(ClusteringLayer, self).build(input_shape)\r\n        \r\n        def call(self, x, mask=None):\r\n            q0 = 1.0/(1.0 + K.sqrt(K.sum(K.square(K.expand_dims(x, 1) - self.W), axis=2))**2 / self.alpha)\r\n            q1 = q0**((self.alpha+1.0)/2.0)\r\n            q2 = K.transpose(K.transpose(q1)/K.sum(q1, axis=1))\r\n            return q2\r\n    \r\n        def compute_output_shape(self, input_shape):\r\n            return (input_shape[0], self.output_dim)\r\n\r\n### Error:\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    520     try:\r\n--> 521       str_values = [compat.as_bytes(x) for x in proto_values]\r\n    522     except TypeError:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in <listcomp>(.0)\r\n    520     try:\r\n--> 521       str_values = [compat.as_bytes(x) for x in proto_values]\r\n    522     except TypeError:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/compat.py in as_bytes(bytes_or_text, encoding)\r\n     60     raise TypeError('Expected binary or unicode string, got %r' %\r\n---> 61                     (bytes_or_text,))\r\n     62 \r\n\r\nTypeError: Expected binary or unicode string, got 5\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-5188e04a18fe> in <module>()\r\n     48 input1 = Input(shape=(10,), name=\"input\")\r\n     49 print(input1)\r\n---> 50 out = ClusteringLayer(output_dim=5, name='clustering')(input1)\r\n     51 \r\n     52 np.random.seed(0)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    694         if all(hasattr(x, 'get_shape') for x in input_list):\r\n    695           input_shapes = nest.map_structure(lambda x: x.get_shape(), inputs)\r\n--> 696         self.build(input_shapes)\r\n    697 \r\n    698       # Check input assumptions set after layer building, e.g. input shape.\r\n\r\n<ipython-input-4-5188e04a18fe> in build(self, input_shape)\r\n      7 \r\n      8     def build(self, input_shape):\r\n----> 9         self.W = self.add_weight(name='kernel', shape=(self.output_dim, input_shape[1]), initializer='uniform', trainable=True)\r\n     10         super(ClusteringLayer, self).build(input_shape)\r\n     11 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, getter)\r\n    532         trainable=trainable and self.trainable,\r\n    533         partitioner=partitioner,\r\n--> 534         use_resource=use_resource)\r\n    535 \r\n    536     if regularizer is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\r\n    495     new_variable = getter(\r\n    496         name=name, shape=shape, dtype=dtype, initializer=initializer,\r\n--> 497         **kwargs_for_getter)\r\n    498 \r\n    499     # If we set an initializer and the variable processed it, tracking will not\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in make_variable(name, shape, dtype, initializer, partition_info, trainable, caching_device, validate_shape, constraint, use_resource, partitioner)\r\n   1871       validate_shape=validate_shape,\r\n   1872       constraint=constraint,\r\n-> 1873       use_resource=use_resource)\r\n   1874   return v\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource)\r\n   2232                          name=name, dtype=dtype,\r\n   2233                          constraint=constraint,\r\n-> 2234                          use_resource=use_resource)\r\n   2235 \r\n   2236 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>(**kwargs)\r\n   2222              constraint=None,\r\n   2223              use_resource=None):\r\n-> 2224   previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n   2225   for getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n   2226     previous_getter = _make_getter(getter, previous_getter)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)\r\n   2194         collections=collections, validate_shape=validate_shape,\r\n   2195         caching_device=caching_device, name=name, dtype=dtype,\r\n-> 2196         constraint=constraint)\r\n   2197   elif not use_resource and context.executing_eagerly():\r\n   2198     raise RuntimeError(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint)\r\n    310           name=name,\r\n    311           dtype=dtype,\r\n--> 312           constraint=constraint)\r\n    313 \r\n    314   # pylint: disable=unused-argument\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint)\r\n    415               with ops.name_scope(\"Initializer\"), ops.device(None):\r\n    416                 initial_value = ops.convert_to_tensor(\r\n--> 417                     initial_value(), name=\"initial_value\", dtype=dtype)\r\n    418               self._handle = _eager_safe_variable_handle(\r\n    419                   shape=initial_value.get_shape(),\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in <lambda>()\r\n   1858         initializer = initializer(dtype=dtype)\r\n   1859       init_val = lambda: initializer(  # pylint: disable=g-long-lambda\r\n-> 1860           shape, dtype=dtype, partition_info=partition_info)\r\n   1861       variable_dtype = dtype.base_dtype\r\n   1862   if use_resource is None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py in __call__(self, shape, dtype, partition_info)\r\n    253       dtype = self.dtype\r\n    254     return random_ops.random_uniform(\r\n--> 255         shape, self.minval, self.maxval, dtype, seed=self.seed)\r\n    256 \r\n    257   def get_config(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py in random_uniform(shape, minval, maxval, dtype, seed, name)\r\n    232     maxval = 1\r\n    233   with ops.name_scope(name, \"random_uniform\", [shape, minval, maxval]) as name:\r\n--> 234     shape = _ShapeTensor(shape)\r\n    235     minval = ops.convert_to_tensor(minval, dtype=dtype, name=\"min\")\r\n    236     maxval = ops.convert_to_tensor(maxval, dtype=dtype, name=\"max\")\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py in _ShapeTensor(shape)\r\n     41   else:\r\n     42     dtype = None\r\n---> 43   return ops.convert_to_tensor(shape, dtype=dtype, name=\"shape\")\r\n     44 \r\n     45 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n   1009       name=name,\r\n   1010       preferred_dtype=preferred_dtype,\r\n-> 1011       as_ref=False)\r\n   1012 \r\n   1013 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1105 \r\n   1106     if ret is None:\r\n-> 1107       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1108 \r\n   1109     if ret is NotImplemented:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    215                                          as_ref=False):\r\n    216   _ = as_ref\r\n--> 217   return constant(v, dtype=dtype, name=name)\r\n    218 \r\n    219 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    194   tensor_value.tensor.CopyFrom(\r\n    195       tensor_util.make_tensor_proto(\r\n--> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    198   const_tensor = g.create_op(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    523       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\r\n    524                       \"Contents: %s. Consider casting elements to a \"\r\n--> 525                       \"supported type.\" % (type(values), values))\r\n    526     tensor_proto.string_val.extend(str_values)\r\n    527     return tensor_proto\r\n\r\nTypeError: Failed to convert object of type <class 'tuple'> to Tensor. Contents: (5, Dimension(10)). Consider casting elements to a supported type.", "@anj-s, I think you wrote the deferred tensor implementation for this layer, right?\r\n\r\n@nairouz , while we investigate, can you try writing your own training loop? (doing a for loop over the data, then passing a loss function to optimizer.minimize for some tensorflow optimizer). I believe that's supposed to work.", "Regarding this error: \"AssertionError: Could not compute output DeferredTensor('None', shape=(5,), dtype=float32)\"\r\nThe value returned by the compute_output_shape function is flattened which causes it to be interpreted as multiple outputs. I am going to test a fix on the custom layer example above.\r\n\r\nAs a workaround you could wrap the output shape returned by compute_output_shape in a TensorShape.  For example: TensorShape((input_shape[0], self.output_dim)). Let me know if this works.", "@anj-s  Thank you for the response.\r\n It is working!", "Duplicate of #20338 ", "@raymond-yuan What about the second undesirable behavior?", "@nairouz I'm sorry can you clarify what the second undesirable behavior is and what code you used to reproduce the error? Is it that when defining your own custom layer's `build` or `compute_output_shape` function you found that you had to provide the argument `value` instead of `input_shape`?\r\n\r\n```\r\nclass MyLayer(keras.layers.Layer):\r\n\r\n  def __init__(self, output_dim, **kwargs):\r\n    self.output_dim = output_dim\r\n    super(MyLayer, self).__init__(**kwargs)\r\n\r\n  def build(self, input_shape):\r\n    shape = tf.TensorShape((input_shape[1], self.output_dim))\r\n    # Create a trainable weight variable for this layer.\r\n    self.kernel = self.add_weight(name='kernel',\r\n                                  shape=shape,\r\n                                  initializer='uniform',\r\n                                  trainable=True)\r\n    # Be sure to call this at the end\r\n    super(MyLayer, self).build(input_shape)\r\n\r\n  def call(self, inputs):\r\n    return tf.matmul(inputs, self.kernel)\r\n\r\n  def compute_output_shape(self, input_shape):\r\n    shape = tf.TensorShape(input_shape).as_list()\r\n    shape[-1] = self.output_dim\r\n    return tf.TensorShape(shape)\r\n\r\n  def get_config(self):\r\n    base_config = super(MyLayer, self).get_config()\r\n    base_config['output_dim'] = self.output_dim\r\n\r\n  @classmethod\r\n  def from_config(cls, config):\r\n    return cls(**config)\r\n\r\ninput1 = layers.Input(shape=(10,), name=\"input\")\r\nlayer = MyLayer(output_dim=5, name='clustering')\r\nout = layer(input1)\r\nprint(out)\r\n```\r\nFor reference, this example does not need `value` and works. \r\n", "@raymond-yuan \r\nWhen defining my own custom layer's build function.\r\nFor the graph based execution, there should be no problem running the code without the TensorShape trick.\r\nTry running the code that you gave me without the TensorShape trick in the case of the graph based execution, you will get an error.\r\n\r\n###  Code:\r\n\r\n    import tensorflow.keras as keras \r\n\r\n    class MyLayer(keras.layers.Layer):\r\n\r\n      def __init__(self, output_dim, **kwargs):\r\n        self.output_dim = output_dim\r\n        super(MyLayer, self).__init__(**kwargs)\r\n\r\n      def build(self, input_shape):\r\n        # Create a trainable weight variable for this layer.\r\n        self.kernel = self.add_weight(name='kernel',\r\n                                      shape=(input_shape[1], self.output_dim),\r\n                                      initializer='uniform',\r\n                                      trainable=True)\r\n        # Be sure to call this at the end\r\n        super(MyLayer, self).build(input_shape)\r\n\r\n      def call(self, inputs):\r\n        return tf.matmul(inputs, self.kernel)\r\n\r\n      def compute_output_shape(self, input_shape):\r\n        shape = tf.TensorShape(input_shape).as_list()\r\n        shape[-1] = self.output_dim\r\n        return tf.TensorShape(shape)\r\n\r\n      def get_config(self):\r\n        base_config = super(MyLayer, self).get_config()\r\n        base_config['output_dim'] = self.output_dim\r\n\r\n      @classmethod\r\n      def from_config(cls, config):\r\n        return cls(**config)\r\n\r\n    input1 = keras.layers.Input(shape=(10,), name=\"input\")\r\n    layer = MyLayer(output_dim=5, name='clustering')\r\n    out = layer(input1)\r\n    print(out)\r\n\r\n### Error\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/random_ops.pyc in random_uniform(shape, minval, maxval, dtype, seed, name)\r\n    232     maxval = 1\r\n    233   with ops.name_scope(name, \"random_uniform\", [shape, minval, maxval]) as name:\r\n--> 234     shape = _ShapeTensor(shape)\r\n    235     minval = ops.convert_to_tensor(minval, dtype=dtype, name=\"min\")\r\n    236     maxval = ops.convert_to_tensor(maxval, dtype=dtype, name=\"max\")\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/random_ops.pyc in _ShapeTensor(shape)\r\n     41   else:\r\n     42     dtype = None\r\n---> 43   return ops.convert_to_tensor(shape, dtype=dtype, name=\"shape\")\r\n     44 \r\n     45 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n   1009       name=name,\r\n   1010       preferred_dtype=preferred_dtype,\r\n-> 1011       as_ref=False)\r\n   1012 \r\n   1013 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1105 \r\n   1106     if ret is None:\r\n-> 1107       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1108 \r\n   1109     if ret is NotImplemented:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    215                                          as_ref=False):\r\n    216   _ = as_ref\r\n--> 217   return constant(v, dtype=dtype, name=name)\r\n    218 \r\n    219 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name, verify_shape)\r\n    194   tensor_value.tensor.CopyFrom(\r\n    195       tensor_util.make_tensor_proto(\r\n--> 196           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    197   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    198   const_tensor = g.create_op(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    523       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\r\n    524                       \"Contents: %s. Consider casting elements to a \"\r\n--> 525                       \"supported type.\" % (type(values), values))\r\n    526     tensor_proto.string_val.extend(str_values)\r\n    527     return tensor_proto\r\n\r\nTypeError: Failed to convert object of type <type 'tuple'> to Tensor. Contents: (Dimension(10), 5). Consider casting elements to a supported type.\r\n\r\n", "I believe this is no longer an error in tf 2.0 because Dimension is gone. Please comment or reopen if you can still reproduce this with nightly and after calling tf.enable_v2_behavior()"]}, {"number": 20804, "title": "Error converting saved_model.pb to .tflite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.5.1-32 bit\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: nothing since i have AMD radeon\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- --savedmodel_directory=C:\\Users\\LENOVO-PC\\DIZIZIT\\saved_model -- output_file=C:\\Users\\LENOVO-PC\\DIZIZIT\\saved_model\\maonani.tflite\r\n\r\n\r\n### Describe the problem\r\nIm trying to convert my saved_model.pb from object detection api file to tensorflow lite but it gives me error such as \r\n\r\n\r\n```\r\nERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/kernels/BUILD:63:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:gemm_support' failed (Exit 2)\r\n.\\tensorflow/contrib/lite/context.h(183): error C2144: syntax error: 'float' should be preceded by ';'\r\n.\\tensorflow/contrib/lite/context.h(183): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\nTarget //tensorflow/contrib/lite/toco:toco failed to build\r\n\r\n```\r\n\r\n\r\nI tried reinstalling all the pre req of tensorflow and use  the code again and it gave different error saying\r\n\r\n```\r\nERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/kernels/BUILD:41:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:eigen_support' failed (Exit 2)\r\ncl : Command line error D8021 : invalid numeric argument '/Wno-error=reorder'\r\nTarget //tensorflow/contrib/lite/toco:toco failed to build\r\n\r\n```\r\nfor the 3rd attempt it gives me error saying:\r\n\r\n```\r\nERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/profiling/BUILD:37:1: C++ compilation of rule '//tensorflow/contrib/lite/profiling:time' failed (Exit 2)\r\ntensorflow/contrib/lite/profiling/time.cc(17): fatal error C1083: Cannot open include file: 'sys/time.h': No such file or directory\r\nTarget //tensorflow/contrib/lite/toco:toco failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n\r\n### Source code / logs\r\nC:\\Users\\LENOVO-PC\\tensorflow>bazel run -c opt tensorflow/contrib/lite/toco:toco -- --savedmodel_directory=C:\\Users\\LENOVO-PC\\DIZIZIT\\saved_model -- output_file=C:\\Users\\LENOVO-PC\\DIZIZIT\\saved_model\\maonani.tflite\r\nINFO: Build options have changed, discarding analysis cache.\r\nINFO: Analysed target //tensorflow/contrib/lite/toco:toco (1 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/kernels/BUILD:63:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:gemm_support' failed (Exit 2)\r\n.\\tensorflow/contrib/lite/context.h(183): error C2144: syntax error: 'float' should be preceded by ';'\r\n.\\tensorflow/contrib/lite/context.h(183): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int\r\nTarget //tensorflow/contrib/lite/toco:toco failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 6.715s, Critical Path: 1.81s\r\nINFO: 1 process: 1 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n2nd attempt was too long to post here\r\n", "comments": ["Unfortunately, tflite is currently not supported under windows. We hope to improve this in the futrue.", "is there a reference where will i know if it's already supported? @aselle ", "It will be added to the Tensorflow release notes.\r\n", "Duplicate of #21085 ", "TFLite and toco should now build and run on Windows. We haven't exhaustively tested all build permutations, but bazel + MSVC 2015 should be functional."]}, {"number": 20803, "title": "Initialize variable from TF Hub/MobilNet checkpoint fails for \"expanded_conv_15/expand/weights\" with Container localhost does not exist. ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Collab\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9_rc2\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:   https://colab.research.google.com/drive/1X2TqjiK3ctRWCGEmJ3Id2VbbL64MWuzT\r\n\r\n### Describe the problem\r\nInitialize variable from TF Hub/MobilNet checkpoint fails for \"expanded_conv_15/expand/weights\" with Container localhost does not exist. \r\n\r\n### Source code / logs\r\nCan be executed at https://colab.research.google.com/drive/1X2TqjiK3ctRWCGEmJ3Id2VbbL64MWuzT\r\nLogs: [tflog.txt](https://github.com/tensorflow/tensorflow/files/2195060/tflog.txt)\r\n\r\n```\r\n!wget https://www.tensorflow.org/images/daisies.jpg\r\nimport tensorflow as tf, tensorflow_hub as hub\r\n\r\ndef read_tensor_from_image_file(file_name, height, width, mean=0, std=128):  \r\n  file_reader = tf.read_file(file_name, name=\"file_reader\")\r\n  image_reader = tf.image.decode_jpeg(file_reader, channels=3, name=\"jpeg_reader\")\r\n  float_caster = tf.cast(image_reader, tf.float32)\r\n  dims_expander = tf.expand_dims(float_caster, 0)\r\n  resized = tf.image.resize_bilinear(dims_expander, [height, width])\r\n  normalized = tf.divide(tf.subtract(resized, [mean]), [std])\r\n  return normalized\r\n\r\nmodule = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/2\")\r\nheight, width = hub.get_expected_image_size(module)\r\nimage = read_tensor_from_image_file(\"daisies.jpg\", height, width)\r\nfeatures = module(image)\r\n\r\nsess = tf.Session()\r\nresult = sess.run(features)\r\nprint(result)\r\n\r\n```\r\n\r\n\r\nFailedPreconditionError: Error while reading resource variable module_4/MobilenetV2/expanded_conv_15/expand/weights from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/module_4/MobilenetV2/expanded_conv_15/expand/weights)\r\n\t [[Node: module_4_apply_default/MobilenetV2/expanded_conv_15/expand/Conv2D/ReadVariableOp = ReadVariableOp[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](module_4/MobilenetV2/expanded_conv_15/expand/weights)]]\r\n", "comments": ["It needs sess.run(tf.global_variables_initializer())? The following code seem to work:\r\n\r\n```\r\n!wget https://www.tensorflow.org/images/daisies.jpg\r\nimport tensorflow as tf, tensorflow_hub as hub, numpy as np\r\n\r\ndef read_tensor_from_image_file(file_name, height, width, mean=0, std=128):  \r\n  file_reader = tf.read_file(file_name, name=\"file_reader\")\r\n  image_reader = tf.image.decode_jpeg(file_reader, channels=3, name=\"jpeg_reader\")\r\n  float_caster = tf.cast(image_reader, tf.float32)\r\n  dims_expander = tf.expand_dims(float_caster, 0)\r\n  resized = tf.image.resize_bilinear(dims_expander, [height, width])\r\n  normalized = tf.divide(tf.subtract(resized, [mean]), [std])\r\n  return normalized\r\n\r\n\r\nmodule = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/2\")\r\nheight, width = hub.get_expected_image_size(module)\r\nimage = read_tensor_from_image_file(\"daisies.jpg\", 224, 224)\r\nfeatures = module(image)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  sess.run(tf.tables_initializer())\r\n\r\n  result = sess.run(features)\r\n  print(np.squeeze(result))\r\n```\r\n\r\nStill, seem like a bug to me, as the:\r\n * requirement to run tf.global_variables_initializer is not documented\r\n * error message is very poor\r\n * no minimalistic sample code is available", "@dchichkov Glad to see that you found that issue in your code yourself. The example on the TFHub Overview [page](https://www.tensorflow.org/hub/) shows minimalistic sample code including the call to tf.global_variables_intializer(). \r\n\r\nWe welcome contributions that help us to improve TensorFlow. If you feel that additional documentation  would be helpful please feel free to submit a PR. \r\n\r\n", "Having the same issue, any ideas?", "Have you tried adding  sess.run(tf.global_variables_initializer())?", "This is completely broken, you can't even load a TF hub model it will load with sess.run(tf.global_variables_initializer())? and then produce completely random results"]}, {"number": 20802, "title": "tensorflow.exp : error lnk2001 : unresolved symbol \"public: class Eigen::TensorMap<>....\"", "body": "windows build tensorflow-1.8  almost success,the finnal tensorflow.dll failed\r\nanybody has ideas", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "have you install Eigen3 and put it into your PATH?", "i see \uff0cmaybe it is the problem of internet, when i build it agian ,no longer show that problem.", "Closing since it seems like the issue is resolved."]}, {"number": 20801, "title": "Is eager execution scoped in each tf.Graph context?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 18.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nIt is documented that enabling eager execution is for a lifetime (https://www.tensorflow.org/guide/eager). I normally understand that `tf.executing_eagerly()` will be a global attribute and not affected by any context whether graphs or sessions. However, my following code tells otherwise. You shall see that the output from `tf.executing_eagerly()` differs between graphs in the same lifetime. Is this really an expected behavior? (I think this isn't the case in Tensorflow 1.8.0)\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\nprint(tf.executing_eagerly()) # True\r\n\r\nwith tf.Graph().as_default():\r\n    print(tf.executing_eagerly()) # False\r\n```", "comments": ["It is actually more problematic than I originally thought. The inability for graphs to inherit the `tf.executing_eagerly()` property from the default graph making a widespread breaking of code inside non-default graphs.\r\n\r\nI will now go back to the 1.8.0. ", "Eager execution is a way way of interacting with tensorflow which is not fully incompatible with graphs. By default, once eager execution is enabled, calling an op will execute it. However, if you create a graph and enter its context we assume that you mean to be adding ops to that graph instead, and we'll do that. This is only useful to an end-user if you're writing a tutorial or if you want to switch from prototyping an eager model to training it using Estimator (which creates graphs under the hood). Being able to add ops to a graph after enabling eager execution, though, is used internally in our libraries a lot (it's how we build graph functions, for example).", "@alextp Is there a way then to re-enable the eager mode inside a graph. Or, is there a way to \"separate\" many eager environments? Say, if I want to have many `global_step` ? ", "The eager environments do not depend on global state as much as the graph\nenvironments, so having multiple global steps is easy: just create two\nvariables, and decide which one to pass to optimizers and summaries when\nthose ask for global steps.\n\nOn Tue, Jul 17, 2018 at 4:09 AM Konpat <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> Is there a way then to re-enable the\n> eager mode inside a graph. Or, is there a way to \"separate\" many eager\n> environments? Say, if I want to have many global_step ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20801#issuecomment-405544741>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQh5SMT7AA4WsqMhfXYg0HA_AXgjks5uHcX3gaJpZM4VP1oT>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp That's true if I want to sacrifice the compatibility between graph and eager modes. Since many of the Tensorflow code base make use of `tf.train.get_global_step()`, I need then to supply explicit global step to every one of those. To me, that breaks the compatibility between graph and eager.", "@alextp I now come to question the philosophy of Tensorflow core team toward the eager mode. Should it be as compatible as possible with graph mode? Which naturally requires making eager as analogous to its graph mode counterpart as possible. For example, graph mode has \"graph\", eager should have a kind of that too. Or, eager is more like a \"glue\" rather than an \"exchangeable\" part of the graph mode. Either way, I think \"the positioning of the eager mode\" should be made very clear.", "Eager should be as compatible with graph as possible, with some caveats\nthat relying on global state as much as the graph API does is not nice, so\nwe're trying to get rid of those types of global state. This part of the\nAPI doesn't have a good alternative yet, but it will have one soon.\n\nOn Tue, Jul 17, 2018 at 11:05 AM Konpat <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> I now come to question the philosophy\n> of Tensorflow core team toward the eager mode. Should it be as compatible\n> as possible with graph mode? Which naturally requires making eager as\n> analogous to its graph mode counterpart as possible. For example, graph\n> mode has \"graph\", eager should have a kind of that too. Or, eager is more\n> like a \"glue\" rather than an \"exchangeable\" part of the graph mode. Either\n> way, I think \"the positioning of the eager mode\" should be made very clear.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20801#issuecomment-405672738>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxX1_KBvjO2NwNiL4hg9bfbCov0SKks5uHieEgaJpZM4VP1oT>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Thanks for the clarification. I understand that there will be a graph thing of eager too?", "I'd rather not have a graph per se in eager. Right now one exists and there\nare subtle ways in which the code depends on it (scoping of graph\ncollections, for example) but those will be phased out.\n\nOn Tue, Jul 17, 2018 at 12:54 PM Konpat <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> Thanks for the clarification. I\n> understand that there will be a graph thing of eager too?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20801#issuecomment-405705286>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxa0mLZ1dK97Pq2-SpS8PYJ9kFES5ks5uHkD9gaJpZM4VP1oT>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 20800, "title": "Fix invalid check of ShapeForPlaceholder", "body": "", "comments": ["Ping peter for review again.", "Ping @petewarden for review again.", "Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied."]}, {"number": 20799, "title": "fix invalid check of ShapeForPlaceholder", "body": "Fix invalid check of ShapeForPlaceholder", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 20798, "title": "Can not convet .pb to .tflite format using tflite_convert", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: GTX 1060/6G\r\n- **Exact command to reproduce**: \r\ntflite_convert   --output_file=/tmp/net.tflite --saved_model_dir=models/ssd_mobilenet_v1_plate_0.004_set2_150\\*150/saved_model\r\n\r\n### Describe the problem\r\nHi, I want to covert a .pb model to .tflite one. The model is train with tensorflow object detection API. The input tensor shape is (None, None, None, 3) but it seems that tflite_convert doesn't support this kind of input.\r\n\r\n### Source code / logs\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor:0' has invalid shape '[None, None, None, 3]'.", "comments": ["I have the same problem. please update if you have other option or if this is solve", "I have the same issue.\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 320, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 316, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 121, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 273, in convert\r\n    \"invalid shape '{1}'.\".format(tensor.name, shape))\r\n**ValueError: None is only supported in the 1st dimension. Tensor 'pnet/input:0' has invalid shape '[None, None, None, 3]'.**\r\n\r\nIf I cannot use the [None,None,None,3] shape, my code will be very ugly...\r\n\r\nand I think following is related issue.\r\n\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 216, in toco_convert\r\n    input_array.shape.dims.extend(map(int, input_tensor.get_shape()))\r\nTypeError: **__int__ returned non-int (type NoneType)**\r\n", "Please follow steps on this blog post and let us know if you still fail to convert SSD Mobilenet V1\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193", "Thanks for your reply. The problem was that I used old version of tensorflow object detectjon API. The new one seems to have complete solution for tensorflow lite. Maybe I should close the issue.", "@MohammadMoradi How did you figure this out?", "@MohammadMoradi I have the same problem, I don't know how to solve it. Any help is welcomed !", "@Elites2017 David.Where you able to solve it?", "Hi I am facing the same issue.\r\n\r\n\r\n\tmodel.inputs----------> [<tf.Tensor 'conv2d_1_input:0' shape=(?, 32, 32, 3) dtype=float32>]\r\n\tTraceback (most recent call last):\r\n\t  File \"training.py\", line 199, in <module>\r\n\t    tflite_model = tf.contrib.lite.toco_convert( frozen_graph, model.inputs, [out.op.name for out in model.outputs])\r\n\t  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 243, in toco_convert\r\n\t    *args, **kwargs)\r\n\t  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 212, in build_toco_convert_protos\r\n\t    input_array.shape.dims.extend(map(int, input_tensor.get_shape()))\r\n\tTypeError: __int__ returned non-int (type NoneType)\r\n\r\nWhat should I do to solve this? I am using keras with tf version 1.10.1", "> @MohammadMoradi I have the same problem, I don't know how to solve it. Any help is welcomed !\r\n\r\nWere you able to solve this?", "> \r\n> ### System information\r\n> \r\n>     * **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n> \r\n>     * **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n> \r\n>     * **TensorFlow installed from (source or binary)**: binary\r\n> \r\n>     * **TensorFlow version (use command below)**: 1.9\r\n> \r\n>     * **Python version**: 3.5\r\n> \r\n>     * **Bazel version (if compiling from source)**:\r\n> \r\n>     * **GCC/Compiler version (if compiling from source)**:\r\n> \r\n>     * **CUDA/cuDNN version**: 9.0/7.0\r\n> \r\n>     * **GPU model and memory**: GTX 1060/6G\r\n> \r\n>     * **Exact command to reproduce**:\r\n>       tflite_convert   --output_file=/tmp/net.tflite --saved_model_dir=models/ssd_mobilenet_v1_plate_0.004_set2_150*150/saved_model\r\n> \r\n> \r\n> ### Describe the problem\r\n> \r\n> Hi, I want to covert a .pb model to .tflite one. The model is train with tensorflow object detection API. The input tensor shape is (None, None, None, 3) but it seems that tflite_convert doesn't support this kind of input.\r\n> ### Source code / logs\r\n> \r\n> ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor:0' has invalid shape '[None, None, None, 3]'.\r\n\r\n\r\n@pkulzc Do you think this error is caused because we don't specify the input shape in the file **export_inference_graph.py** if so, should we edit this file by putting the dimension of our images? I'm facing the same problem too\r\n ", "> \r\n> \r\n> Thanks for your reply. The problem was that I used old version of tensorflow object detectjon API. The new one seems to have complete solution for tensorflow lite. Maybe I should close the issue.\r\n\r\n@inakaaay @MohammadMoradi How did you Guys figure this out ?", "I use the command to convert .pb to .tflite\r\n`tflite_convert --output_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/optimized_graph.tflite --graph_def_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=ImageTensor --input_shapes=1,513,513,3 --output_arrays=SemanticPredictions \u2013allow_custom_ops `\r\nand it works\r\nI use the pretrained model downloaded from [https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md](url)", "It's okay.\r\n\r\nNow I got this problem when trying to build the demo app on Android Studio. (Windows 10)\r\n\r\n**\\contrib\\lite\\examples\\android\\BUILD\\android-profile**", "> I use the command to convert .pb to .tflite\r\n> `tflite_convert --output_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/optimized_graph.tflite --graph_def_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=ImageTensor --input_shapes=1,513,513,3 --output_arrays=SemanticPredictions \u2013allow_custom_ops`\r\n> and it works\r\n> I use the pretrained model downloaded from [https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md](url)\r\n\r\n@wwwecho do you try to inference using coverted tflite? I met runtime error ```Failed to run on the given Interpreter: tensorflow/contrib/lite/kernels/depthwise_conv.cc:99 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 32)Node number 30 (DEPTHWISE_CONV_2D) failed to prepare.```", "> > I use the command to convert .pb to .tflite\r\n> > `tflite_convert --output_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/optimized_graph.tflite --graph_def_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=ImageTensor --input_shapes=1,513,513,3 --output_arrays=SemanticPredictions \u2013allow_custom_ops`\r\n> > and it works\r\n> > I use the pretrained model downloaded from [https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md](url)\r\n> \r\n> @wwwecho do you try to inference using coverted tflite? I met runtime error `Failed to run on the given Interpreter: tensorflow/contrib/lite/kernels/depthwise_conv.cc:99 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 32)Node number 30 (DEPTHWISE_CONV_2D) failed to prepare.`\r\n\r\nsame problem.", "Share my problem and the solution.\r\nproblem:  I don't know what is the accutally input_arrays and output_arrays.\r\nI use keras to build the model, and try use \r\n`start_layer.input` which is \r\n`<tf.Tensor 'convolution2d_input_1:0' shape=(?, 3, 448, 448) dtype=float32>`\r\nand `end_layer.output` which is `<tf.Tensor 'add_11:0' shape=(?, 1470) dtype=float32>`\r\n. But I got errors like`Invalid tensors 'convolution2d_input_1:0' were found`\r\nmy origin command is \r\n```\r\ntflite_convert \\\r\n  --output_file=tf.tflite \\\r\n  --graph_def_file=tf.pb \\\r\n  --input_arrays=convolution2d_input_1 \\\r\n  --output_arrays=dense_3/add_11 \\\r\n  --input_shape=1,3,448,448\r\n```\r\nfix: add `print(list(tensor_name_to_tensor))` to contrib/lite/python/convert_saved_model.py line 176 in function get_tensors_from_tensor_names()\r\n\r\nafter execute the tflite_convert command again, it print all the tensor name. \r\nmy finally command which execute success\r\n```\r\ntflite_convert \\\r\n  --output_file=tf.tflite \\\r\n  --graph_def_file=tf.pb \\\r\n  --input_arrays=convolution2d_1_input \\\r\n  --output_arrays=dense_3/BiasAdd \\\r\n  --input_shape=1,3,448,448\r\n```\r\n", "\r\n> > I use the command to convert .pb to .tflite\r\n> > `tflite_convert --output_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/optimized_graph.tflite --graph_def_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=ImageTensor --input_shapes=1,513,513,3 --output_arrays=SemanticPredictions \u2013allow_custom_ops`\r\n> > and it works\r\n> > I use the pretrained model downloaded from [https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md](url)\r\n> \r\n> @wwwecho do you try to inference using coverted tflite? I met runtime error `Failed to run on the given Interpreter: tensorflow/contrib/lite/kernels/depthwise_conv.cc:99 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 32)Node number 30 (DEPTHWISE_CONV_2D) failed to prepare.`\r\n\r\nFacing the same issue in Tensorflow 1.12.0", "This issue is closed. Please open this issue up\r\n", "Support really needs to be added to tflite_convert for fully convolutional models (those having multiple **None**s in the input shape).", "I have the same problem with tflite_convert while converting. so i checked with tflite_convert --help which is giving me the below problem i have tensorflow version 1.12. please see the log below.\r\n\r\n\r\n(tensorflow) C:\\Users\\H156759>python\r\nPython 3.6.7 |Anaconda, Inc.| (default, Dec 10 2018, 20:35:02) [MSC v.1915 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.12.0'\r\n>>> exit()\r\n\r\n(tensorflow) C:\\Users\\H156759>tflite_convert --help\r\nTraceback (most recent call last):\r\n  File \"C:\\Pavans\\Anaconda3\\envs\\tensorflow\\Scripts\\tflite_convert-script.py\", line 6, in <module>\r\n    from tensorflow.contrib.lite.python.tflite_convert import main\r\nModuleNotFoundError: No module named 'tensorflow.contrib.lite.python.tflite_convert'", "Hi all!\r\nI use tensorflow 1.11.0 and Debian 4.9.88-1+deb9u1 (2018-05-07) x86_64 GNU/Linux.\r\nI have trained a neural network and I want to use it on a mobile device (Android OS).\r\nMy main issue is that of I can't create tflite-file.\r\nI have tried to change input parameters of tflite_convert. \r\nSometimes I receive error messages at the moment of creating tflite-file:\r\n\r\n```\r\n#1 ValueError: Invalid tensors 'input' were found.\r\n\r\n                tflite_convert \\\r\n                  --graph_def_file=/storage/src/basic_detector/veh_models_frozen_inference_graph.pb \\\r\n                  --output_file=veh_models.lite \\\r\n                  --input_format=TENSORFLOW_GRAPHDEF \\\r\n                  --output_format=TFLITE \\\r\n                  --input_shape=1,289,204,3 \\\r\n                  --input_array=input \\\r\n                  --output_array=final_result \\\r\n                  --inference_type=FLOAT \\\r\n                  --input_data_type=FLOAT\r\n```\r\n```\r\n#2 ValueError: Invalid tensors 'ImageTensor' were found.\r\n\r\n                tflite_convert \\\r\n                    --graph_def_file=/storage/src/basic_detector/veh_models_frozen_inference_graph.pb \\\r\n                    --output_file=veh_models.lite \\\r\n                    --inference_type=FLOAT \\\r\n                    --inference_input_type=QUANTIZED_UINT8 \\\r\n                    --input_array=normalized_input_image_tensor \\\r\n                    --input_shapes=1,289,204,3 \\\r\n                    --output_array=SemanticPredictions \\\r\n                    --allow_custom_ops\r\n```\r\n\r\n\r\n```\r\n#3 ValueError: Invalid tensors 'ImageTensor' were found.\r\n\r\n                tflite_convert \\\r\n                   --graph_def_file=/storage/src/tflite_files/poets_graph.pb \\\r\n                   --output_file=veh_models.lite \\\r\n                   --input_format=TENSORFLOW_GRAPHDEF \\\r\n                   --output_format=TFLITE \\\r\n                   --input_shape=1,224,224,3 \\\r\n                   --input_array=input \\\r\n                   --output_array=final_result \\\r\n                   --inference_type=FLOAT \\\r\n                   --input_data_type=FLOAT\r\n\r\n```\r\n\r\n\r\nSometimes tflite-file is created but then a runtime error occurs on a mobile phone:\r\n\r\n```\r\n                tflite_convert \\\r\n                  --graph_def_file=/storage/src/basic_detector/veh_models_frozen_inference_graph.pb \\\r\n                  --output_file=veh_models.lite \\\r\n                  --input_format=TENSORFLOW_GRAPHDEF \\\r\n                  --output_format=TFLITE \\\r\n                  --input_shape=1,289,204,3 \\\r\n                  --input_arrays=Preprocessor\\/sub \\\r\n                  --output_arrays=concat,concat_1 \\\r\n                  --inference_type=FLOAT \\\r\n                  --input_data_type=FLOAT\r\n\r\n```\r\n\r\nA tflite-file is created successfully but I receive runtime error:\r\n\r\n```\r\nW/System.err: java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 1335, 11] and a Java object with shape [1, 10].\r\n                                      at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:240)\r\n                                      at org.tensorflow.lite.Tensor.copyTo(Tensor.java:116)\r\n                                      at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:152)\r\n                                      at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)\r\n                                      at org.tensorflow.lite.Interpreter.run(Interpreter.java:195)\r\n                                      at com.example.root.neuralnetworks01.MainActivity.classifyFrame(MainActivity.java:239)\r\n                                      at com.example.root.neuralnetworks01.MainActivity$2.onClick(MainActivity.java:169)\r\n                                      at android.view.View.performClick(View.java:4757)\r\n                                      at android.view.View$PerformClick.run(View.java:19757)\r\n                                      at android.os.Handler.handleCallback(Handler.java:739)\r\n                                      at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                      at android.os.Looper.loop(Looper.java:135)\r\n                                      at android.app.ActivityThread.main(ActivityThread.java:5219)\r\n                                      at java.lang.reflect.Method.invoke(Native Method)\r\n                    W/System.err:     at java.lang.reflect.Method.invoke(Method.java:372)\r\n                                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:898)\r\n                                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:693)\r\n```\r\n\r\nI don't understand meaning of input parameters for tflite_convert:\r\n  --input_array\r\n  --output_array\r\n  --inference_type\r\n  --input_data_type\r\n\r\nI tried to repeat the example that is described in the article (https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#1) but it didn't work out.\r\n\r\nI attached my config file used for training the network.\r\nMaybe I need to consider and get some parameters from there to use them when I'm going to run tflite_convert.\r\nCan you help me? Thanks.\r\n\r\n\r\n[ssd_vehicle_model_config.txt](https://github.com/tensorflow/tensorflow/files/2855716/ssd_vehicle_model_config.txt)\r\n", "SSD MobileNet model is for detection, where the conversion instructions are different as detailed in [blog](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193).\r\nFor classification, you would use MobileNet model as given in the codelab you referenced.\r\nThe meaning of the following can be found [here](https://www.tensorflow.org/lite/convert/cmdline_reference): \r\n--input_array\r\n--output_array\r\n--inference_type\r\n--input_data_type", "@Elites2017 hello, when i convert the mobilenet-ssd .pb file to tflite file, i get the same error about [None,None,None,3], what can i do to solve the problem? can i ues the export_tflite_ssd_graph to convert? thanks", "> SSD MobileNet model is for detection, where the conversion instructions are different as detailed in [blog](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193).\r\n> For classification, you would use MobileNet model as given in the codelab you referenced.\r\n> The meaning of the following can be found [here](https://www.tensorflow.org/lite/convert/cmdline_reference):\r\n> --input_array\r\n> --output_array\r\n> --inference_type\r\n> --input_data_type\r\n\r\nI have read the manual, but it seems there is not enough information about a list of possible values that could be set for input_arrays and output_arrays.\r\nHow can I understand what values for input_arrays and output_arrays I need to put there so those can correspond to my config file that was used for NN training?\r\nIn the manual, it says there should be names of activation tensors. To be honest  I don't understand what is it either.\r\nWhat can I use for understanding what values need put there? Can you please point me somewhere for further reading? Thank you.\r\n", "I have read the following article, and it was helpful to me: https://heartbeat.fritz.ai/neural-networks-on-mobile-devices-with-tensorflow-lite-a-tutorial-85b41f53230c.\r\nI have reproduced exactly what was written there. This was my first step.\r\n\r\nAfter that, I took my own dataset of pictures (14 classes of cars by different manufacturers), followed the instructions, and got good results.\r\n\r\nThe results were impressive, with about 97% accuracy. I didn't expect such effectiveness.\r\nHowever, this seems more like an incidental success than a logical one because, prior to this, I had spent a lot of time using a different approach (described here: https://github.com/tensorflow/models).\r\nNow I'm a little bit confused.\r\nI designed my own model for servers and desktop computers, and it worked and continues to work.\r\nBut it doesn't work on mobile devices. I have described my issues with tflite_convert and runtime errors previously.\r\nMaybe my model is incompatible with mobile devices. I don't know. Why does an alternative method of creating pb-file have success and give positive totals?\r\nCan anybody help me understand this?", "So it isn't possible to use fully convolutional models with input like `[None, None, None, 3]`?", "@mrgloom No, you must specify the spatial dimensions. The batch dimension gets reduced to 1 in tflite export.\r\n\r\nHas anyone fixed this error:\r\n\r\n> tensorflow/contrib/lite/kernels/depthwise_conv.cc:99 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 6)Node number 23 (DEPTHWISE_CONV_2D) failed to prepare.\r\n\r\nAny updates would be great. Tensorflow r1.12, training/testing works fine but tflite model fails.\r\n\r\nWhy is params->depth_multiplier * SizeOfDimension(input, 3) = 0 ??\r\n(I am getting pretty much same error as @kismeter \r\n\r\n", "Solved it with extra parameters inside code:\r\n\r\n    import tensorflow as tf\r\n\r\n    graph_def_file = \"mask_rcnn_resnet50_atrous_coco_2018_01_28/frozen_inference_graph.pb\"\r\n    input_arrays = [\"image_tensor\"]\r\n    output_arrays = [\"detection_scores\",\"detection_boxes\",\"detection_classes\",\"detection_masks\"]\r\n\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n            graph_def_file, input_arrays, output_arrays,input_shapes={\"image_tensor\":[1,600,600,3]})\r\n    tflite_model = converter.convert()\r\n    open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\nAlthough it cannot produce .tflite because it raises kmerge error \"OperatorType::kMerge Found Sub as non-selected output from Switch, but only Merge supported\"", "hi.\r\ni dont now what should i put in my output and input array .\r\nmy code is:\r\n\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = \"./ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb\"\r\ninput_arrays = [\"image_tensor\"]\r\noutput_arrays = [\"MobilenetV2/Predictions/Reshape_1\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n\r\nand my error is:\r\n\r\nValueError: Invalid tensors 'MobilenetV2/Predictions/Reshape_1' were found.\r\n\r\n\r\ni have:\r\npython = 3.7\r\ntensorflow =1.15\r\nprotobuf=3.7\r\n\r\nplease help\r\n", "@Davari393  check the .pb file with Tensorboard to see input and output array names", "> @Davari393 check the .pb file with Tensorboard to see input and output array names\r\n\r\ncan you tell me how?", "@Davari393  check this link \r\nhttps://www.tensorflow.org/tensorboard/r1/overview", "thanks so much", "It's better to use summarize_graph, it will print input and output nodes:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#inspecting-graphs\r\nhttps://stackoverflow.com/a/52029732/1179925", "hi.\r\nmy code is:\r\n\r\nfrom tensorflow.contrib import lite\r\nconverter = lite.TFLiteConverter.from_keras_model_file( r'/content/drive/My Drive/inceptionv3-transfer-learning__fine_tune.model' ) # Your model's name\r\nmodel = converter.convert()\r\nfile = open( 'model.tflite' , 'wb' )\r\nfile.write( model )\r\n\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_1' has invalid shape '[None, None, None, 3]'.\r\n\r\nwhat will my input_array,input_shape and output_array be?", "> > @wwwecho do you try to inference using coverted tflite? I met runtime error `Failed to run on the given Interpreter: tensorflow/contrib/lite/kernels/depthwise_conv.cc:99 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 32)Node number 30 (DEPTHWISE_CONV_2D) failed to prepare.`\r\n> \r\n> Facing the same issue in Tensorflow 1.12.0\r\n@SanthoshRajendiran  @wwwecho   sorry for my bad English. \r\nWhen I convert the mobilenetv2+deeplabv3+ model to  a tflite file and then deploy it to Andriod, I face the same issue:\r\nInternal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/depthwise_conv.cc:108 params->depth_multiplier * SizeOfDimension(input, 3) != SizeOfDimension(filter, 3) (0 != 32)Node number 16 (DEPTHWISE_CONV_2D) failed to prepare.\r\n\r\nCan you give me some suggestions If you have solved this problem? Thanks.", "> tflite_convert --output_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/optimized_graph.tflite --graph_def_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=ImageTensor --input_shapes=1,513,513,3 --output_arrays=SemanticPredictions \u2013allow_custom_ops\r\n\r\ni got this error.\r\n\r\nValueError: Invalid tensors 'ImageTensor' were found.\r\n"]}, {"number": 20797, "title": "import tensorflow failed with python3.7 on Mac", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n1. Mac 10.13.6 Sierra\r\n2. Python3.7 (brew install python3)\r\n3. install tensorflow1.9.0\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.9.0-py3-none-any.whl\r\n\r\n- **Python version**: \r\nPython 3.7.0 (default, Jun 29 2018, 20:13:13) \r\n[Clang 9.1.0 (clang-902.0.39.2)] on darwin\r\n\r\n- **Exact command to reproduce**:\r\nimport tensorflow;\r\n\r\nYou can obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nDudeMacBook-Pro:~ simbaba$ python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 114\r\n    def TFE_ContextOptionsSetAsync(arg1, async):\r\n                                             ^\r\nSyntaxError: invalid syntax\r\nDudeMacBook-Pro:~ simbaba$ \r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nPython 3.7.0 (default, Jun 29 2018, 20:13:13) \r\n[Clang 9.1.0 (clang-902.0.39.2)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensor as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensor'\r\n>>> import tensorf as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorf'\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 114\r\n    def TFE_ContextOptionsSetAsync(arg1, async):\r\n                                             ^\r\nSyntaxError: invalid syntax\r\n\r\n", "comments": ["duplicated", "i have the same error"]}, {"number": 20796, "title": "TypeError: 'EagerTensor' object cannot be interpreted as an integer", "body": "Running automatic_differentiation.ipynb on Colab, and there's a error.\r\n\r\n```\r\ndef f(x, y):\r\n  output = 1\r\n  for i in range(y):\r\n    output = tf.multiply(output, x)\r\n  return output\r\n\r\ndef g(x, y):\r\n  # Return the gradient of `f` with respect to it's first parameter\r\n  return tfe.gradients_function(f)(x, y)[0]\r\n\r\nassert f(3.0, 2).numpy() == 9.0   # f(x, 2) is essentially x * x\r\nassert g(3.0, 2).numpy() == 6.0   # And its gradient will be 2 * x\r\nassert f(4.0, 3).numpy() == 64.0  # f(x, 3) is essentially x * x * x\r\nassert g(4.0, 3).numpy() == 48.0  # And its gradient will be 3 * x * x\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-21-2c4ab5c7a8b8> in <module>()\r\n     10 \r\n     11 assert f(3.0, 2).numpy() == 9.0   # f(x, 2) is essentially x * x\r\n---> 12 assert g(3.0, 2).numpy() == 6.0   # And its gradient will be 2 * x\r\n     13 assert f(4.0, 3).numpy() == 64.0  # f(x, 3) is essentially x * x * x\r\n     14 assert g(4.0, 3).numpy() == 48.0  # And its gradient will be 3 * x * x\r\n\r\n<ipython-input-21-2c4ab5c7a8b8> in g(x, y)\r\n      7 def g(x, y):\r\n      8   # Return the gradient of `f` with respect to it's first parameter\r\n----> 9   return tfe.gradients_function(f)(x, y)[0]\r\n     10 \r\n     11 assert f(3.0, 2).numpy() == 9.0   # f(x, 2) is essentially x * x\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)\r\n    367     \"\"\"Computes the gradient of the decorated function.\"\"\"\r\n    368 \r\n--> 369     _, grad = val_and_grad_function(f, params=params)(*args, **kwds)\r\n    370     return grad\r\n    371 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)\r\n    466       raise ValueError(\"Functions to be differentiated cannot \"\r\n    467                        \"receive keyword arguments.\")\r\n--> 468     val, vjp = make_vjp(f, params)(*args, **kwds)\r\n    469     return val, vjp(dy=dy)\r\n    470 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in decorated(*args, **kwds)\r\n    522         sources.append(args[i])\r\n    523         tape.watch(args[i])\r\n--> 524       result = f(*args)\r\n    525       if result is None:\r\n    526         raise ValueError(\"Cannot differentiate a function that returns None; \"\r\n\r\n<ipython-input-21-2c4ab5c7a8b8> in f(x, y)\r\n      1 def f(x, y):\r\n      2   output = 1\r\n----> 3   for i in range(y):\r\n      4     output = tf.multiply(output, x)\r\n      5   return output\r\n\r\nTypeError: 'EagerTensor' object cannot be interpreted as an integer", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: NO\r\nOS Platform and Distribution: Colab\r\nTensorFlow installed from: Colab\r\nTensorFlow version: Colab\r\nBazel version: Colab\r\nCUDA/cuDNN version: Colab\r\nGPU model and memory: Colab\r\nExact command to reproduce: just run the sample code", "I have the same problem. I noticed the documentation for c = tf.multiply(a, b) will give the output tensor as the same type of the first input argument (i.e a). Then I changed the order of tf.multiply(output, x) --> tf.multiply(x, output). So the f(x, y) function at least works for a non-integer x. But the derivative function g(x, y) still gives error of \"TypeError: 'EagerTensor' object cannot be interpreted as an integer\". Hope this can be solved soon.", "In the function **f(x, y)** at the line \"for i in range(y)\", I just cast y to be int as the following:\r\n\r\nfor i in range(int(y)) and it solved the problem.", "Thanks Bigchaipat. It works.", "Thanks Bigchaipat, it works for me, too.  \r\nGPU Type: Nvidia 1060\r\nOS: ubuntu 18.06\r\nTensorflow Version: 1.90", "Nagging Assignee @jart: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20795, "title": "Fix typo in bounding box example", "body": "The example in the [tf.image.draw_bounding_boxes documentation](https://www.tensorflow.org/api_docs/python/tf/image/draw_bounding_boxes) has a typo that makes it really confusing. The relative values for `[y_min, x_min, y_max, x_max]` are computed by:\r\n\r\n```python\r\ny_min = y_min_px / height\r\ny_max = y_max_px / height\r\nx_min = x_min_px / width\r\nx_max = x_max_px / width\r\n```\r\n\r\nSince the example doesn't explicitly describe this simple formula, the fact that the absolute pixel value for `x_max` is wrong can make the reader wonder if there is in fact another formula for computing `x_max`.", "comments": ["Thanks for fixing the document."]}, {"number": 20794, "title": "Add all keep nodes to output lists", "body": "Add all nodes to keep to output list of tftrt conversion", "comments": []}, {"number": 20793, "title": "[XLA:GPU] avoid hard coded CUDA PlatformId", "body": "Follow OpKernelConstruction -> DeviceBase -> GpuDeviceInfo -> Stream -> StreamExecutor -> Platform to get platform ID", "comments": ["@jlebar This is another minor change to avoid hard-coded dependency to CUDA. Another PR would be filed for `xla_gpu_device` which shares a similar issue but requires a different solution.", "I think this is right, but I'm not familiar with this code, so I'd like @hawkinsp to sign off."]}, {"number": 20792, "title": "[Intel MKL] Upgrading the the CI install_bazel.sh script to install bazel 0.15.0", "body": "See https://github.com/tensorflow/tensorflow/commit/1e2438318dd250132572a23458598f8c4c4d9ce5\r\n\r\nWithout this, CI builds can't compile because of recent changes to third_party/gpus/cuda_configure.bzl: \r\n\r\n@gunan Can you take a look at this one?", "comments": ["@yifeif is working on another upgrade to 0.15, that may just supersede this change.", "OK. I tested on that version as well. Just wanted to go with the flow....", "closing because this has been fixed in another branch."]}, {"number": 20791, "title": "Patch boringssl for ppc64le", "body": "This commit adds a patch file for the bazel BUILD file contained in the\r\nboringssl.tar.gz file. It set the necessary compile options to build boringssl\r\non pp64le. (Basically adds linux_ppc64le everywhere linux_x86_64 is)\r\n\r\nFixes #20677", "comments": ["Related question , if I wanted to upstream this fix so the patch wouldn't be required for Tensorflow where would I do it? The tar.gz file that is downloaded for boringssl includes more files then what I get when cloning https://boringssl.googlesource.com/boringssl/", "I found that the boringssl repo has a master-with-bazel branch that I need to work from. ", "@gunan Can we get a quick re-review? Looks like @wdirons had to make one last change to resolve https://github.com/tensorflow/tensorflow/issues/20677\r\n\r\nAlso is it possible to pick this over to the 1.10 branch?\r\n\r\nThx!"]}, {"number": 20790, "title": "pip3 install tensorflow fails on Python 3.7", "body": "Have I written custom code\r\n`No`\r\n\r\nOS Platform and Distribution\r\n`macOS 10.13.6 (17G65)`\r\n\r\nTensorFlow installed from\r\n`install failed`\r\n\r\nTensorFlow version\r\n`latest`\r\n\r\nBazel version\r\nCUDA/cuDNN version\r\n`N/A`\r\n\r\nGPU model and memory\r\n`N/A`\r\n\r\nExact command to reproduce\r\n```\r\n~: pip3 --version\r\npip 10.0.1 from /usr/local/lib/python3.7/site-packages/pip (python 3.7)\r\n~: pip3 install tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Python 3.7.0 (default, Jun 29 2018, 20:13:13) \r\n[Clang 9.1.0 (clang-902.0.39.2)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensor as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensor'\r\n>>> import tensorf as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorf'\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 114\r\n    def TFE_ContextOptionsSetAsync(arg1, async):\r\n                                             ^\r\nSyntaxError: invalid syntax\r\n", "if you use Anaconda, you can use  `conda install -c hesi_m tensorflow ` to install tensorflow 1.9.0 for CPU \r\n\r\nif you specifically use Keras also, you can use  `conda install -c hesi_m keras` which installs Keras 2.2.0 in accompany with everything including tensorflow 1.9.0\r\n\r\nIt is better to make a fresh anaconda environment for your experiments", "See #17022 and #20517", "@lutzroeder refer to the issue #20690.\r\n\r\nand find anywhere the variable async is used and replace it with async1 so its not a python 3.7 keyword anymore. you will find those \"async\" variables on line 114, 115, 154 (for the tf version 1.10.0)\r\nSo this (for instance):\r\ndef TFE_ContextOptionsSetAsync(arg1, async):\r\nbecomes this:\r\ndef TFE_ContextOptionsSetAsync(arg1, async1):\r\n\r\nThis solved my issue!", "any update on the issue? I get this error on Windows 8 \r\n\r\n```\r\n>pip --version\r\n18.1\r\n\r\n>py --version\r\n3.7.2\r\n\r\n>pip install tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the\r\nons: )\r\nNo matching distribution found for tensorflow\r\n```"]}, {"number": 20789, "title": "Tensorflow 1.8.0 and 1.9.0 behavioural difference for tf.layers.Layer API ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0 and 1.9.0\r\n- **Python version**: 3.5.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cpu\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n      import tensorflow as tf\r\n\r\n\r\n      class LayersBaseTest(tf.test.TestCase):\r\n\r\n        def test_create_variable(self):\r\n  \r\n          class MyLayer(tf.layers.Layer):\r\n\r\n             def build(self, _):\r\n               # Do not mark the layer as built.\r\n                pass\r\n\r\n              def call(self, inputs):\r\n                self.my_var = self.add_variable('my_var', [2, 2])\r\n                if self.built:\r\n                  self.add_variable('this_will_break_on_second_call', [2, 2])\r\n                return inputs + tf.square(self.my_var)\r\n\r\n          layer = MyLayer(name='my_layer')\r\n          inputs = tf.random_uniform((2,), seed=1)\r\n          outputs = layer.apply(inputs)\r\n          self.assertEqual(layer.built, True)\r\n          self.assertEqual(outputs.op.name, 'my_layer/add')\r\n          self.assertListEqual([v.name for v in layer.variables], ['my_layer/my_var:0'])\r\n          with self.assertRaisesRegex(ValueError, 'my_layer/this_will_break_on_second_call'):\r\n            layer.apply(inputs)\r\n          # The list of variables hasn't changed.\r\n          self.assertListEqual([v.name for v in layer.variables], ['my_layer/my_var:0'])\r\n\r\n\r\n      if __name__ == '__main__':\r\n        tf.test.main()\r\n\r\n### Describe the problem\r\nthe above tests passed with tensorflow version `1.8.0`\r\n\r\nThrows the following errors with tensorflow version `1.9.0`. The `tf.layers.Layer` shouldn't create another variable with the same name even after throwing the `ValueError` on the second call to `layer.apply`. This behaviours s unexpected. \r\n###  logs\r\n>FAIL: test_create_variable (__main__.LayersBaseTest)\r\n >----------------------------------------------------------------------\r\n >Traceback (most recent call last):\r\n  >File \"test_layer_base_tf1.9.py\", line 29, in test_create_variable\r\n   >self.assertListEqual([v.name for v in layer.variables], ['my_layer/my_var:0'])\r\n  >AssertionError: Lists differ: ['my_layer/my_var:0', 'my_layer/my_var:0'] != ['my_layer/my_var:0']\r\n   >First list contains 1 additional elements.\r\n   >First extra element 1:  \r\n    >'my_layer/my_var:0'\r\n    > - ['my_layer/my_var:0', 'my_layer/my_var:0']\r\n    > + ['my_layer/my_var:0']", "comments": ["I think you should call `add_variable` in `build` method, rather than `call` method.", "@cy89 any updates on this? ", "@n3011 could you try @facaiy 's suggestion? Or explain why it is that you need to build the variables at call time, not at build time?", "@cy89 I think my issue relates to behavioural difference between the two versions . can you reply on that? ", "I'm afraid this is intended behavior going forward. By the way, perhaps the issue is related to #20842. "]}, {"number": 20788, "title": "Issue with `natural_exp_decay` documentation", "body": "The example in the `tf.train.natural_exp_decay` documentation uses an nonexistent function called `exponential_time_decay`. Also, it is missing the `decay_steps` parameter in both the example and the showed equation\r\n\r\nEquation:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/learning_rate_decay.py#L316\r\nExample\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/learning_rate_decay.py#L326", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@martinwicke who owns these? The docstring seems to have been there for years, and it doesn't look as if exponential_time_decay has ever existed.", "I bet the function was renamed a long time ago, but the docstring wasn't updated. Everybody owns this. A PR to fix the docstring would be very welcome. I think this virtual function name should just be corrected to natural_exp_decay. I am also not sure that the earlier \"decay exponentially with a base of 0.96\" is useful (or correct). ", "I made an internal change. ", "Updated, should be in sync now."]}, {"number": 20787, "title": "tensorflow-1.9.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.", "body": "From my Cmd:  I have downloaded it from here: https://pypi.org/project/tensorflow/#files\r\n\r\nF:\\Python\\Scripts>pip3 install tensorflow-1.9.0-cp36-cp36m-win_amd64.whl\r\ntensorflow-1.9.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\r\n\r\nF:\\Python\\Scripts>python --version\r\nPython 3.6.5\r\n\r\nI tried upgrade Pip, Wheel but they're all up-to-date.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "if you use Anaconda, you can use  `conda install -c hesi_m tensorflow ` to install tensorflow 1.9.0 for CPU \r\n\r\nif you specifically use Keras also, you can use  `conda install -c hesi_m keras` which installs Keras 2.2.0 in accompany with everything including tensorflow 1.9.0\r\n\r\nIt is better to make a fresh anaconda environment for your experiments"]}, {"number": 20786, "title": "Consolidate versioning for GPU hardware", "body": "Different methods are used in GPU common runtime, StreamExecutor, and XLA to\r\ntrack GPU hardware versions. To support upcoming ROCm SteramExecutor and AMDGPU\r\nXLA, it's now consolidated to DeviceVersion in StreamExecutor in this commit.", "comments": ["@jlebar This is the PR created to consolidate GPU hardware versioning after our discussion in #20757. Now that particular PR could be greatly simplified. However this particular PR affects quite a few places in TensorFlow and XLA.", "@jlebar Updated the PR per review comments. Please be cautious reviewing the static member function `DeviceVersion::Parse` as it would be used by bazel-generated header file `cuda_config.h` and I'm not very sure making its return type be `StatusOr<DeviceVersion>` a good idea.", "@jlebar revised the PR again per comments. I'm still somewhat uncomfortable with changes made in `cuda_configure.bzl` because for obvious reason I can't really test it.", "@jlebar Thanks. Updated the PR once again per review comments.", "LGTM, but I'd like @yifeif or @gunan to sign off on the build system changes, so not approving.\r\n\r\nAlso given that we had build errors it would be nice if the tests would complete.  I don't know how to kick off another set of tests, though.  Or maybe that happens automatically?", "Please resolve the merge conflict before we kick off the tests and merge process.", "@jlebar / @gunan after some unexpected hiatus I'm back working on this PR. I've rebased and fixed all known failures on CUDA path. I'm wondering could we give this PR another test on your end?", ">  jlebar added the kokoro:force-run  label just now\r\n\r\nNo idea if this does what I was hoping, but it's worth a try.  :)", "@jlebar I just fixed errors in some test targets. could you help re-run the tests?\r\n\r\nNotice there is one failure which i don't think has anything to do with this PR:\r\n\r\n```\r\n//tensorflow/python/estimator:keras_test\r\n```\r\n\r\nalso i noticed the syntax asked by `clang-format` is a bit different on CI now. could you let me know the latest syntax definition is?", "> also i noticed the syntax asked by clang-format is a bit different on CI now. could you let me know the latest syntax definition is?\r\n\r\n/me summons @yifeif \r\n\r\n> could you help re-run the tests?\r\n\r\nOK.  @yifeif is there any way to make this process of rerunning the tests less work for us?  Like, is it possible to give whchung the ability to kick tests himself?", "@jlebar / @yifeif I fixed the errors found in the last round of tests. Wondering is it possible for me to apply `kokoro:force-run` label?\r\n\r\nAlso I'm looking forward to hearing from @yifeif wrt #20277.", "> Wondering is it possible for me to apply kokoro:force-run label?\r\n\r\nYeah, I know this is not a good experience for you as our partner.  :(  We're currently discussing internally whether and how it might be technically possible to allow this, within our security constraints.  I'll let you know.", "@whchung Yifei is on vacation, have patience.\r\n\r\nOnly people with write access to TensorFlow can apply labels (and consequently, trigger tests). That means that you will have to rely on @jlebar for the time being. ", "Sorry for the delay @whchung, I was on vacation last week. I replied in #20277. We are using clang-format  version 3.9 with -style=google in CI. The failure in `//tensorflow/python/estimator:keras_test` should be irrelevant. ", "Nagging Assignee @qlzh727: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 20785, "title": "Add support for Cyclic Learning Rate", "body": "Added support for cyclic learning rate as described in: https://arxiv.org/pdf/1506.01186.pdf\r\nAlso added functional tests for each policy.", "comments": ["@ Reviewers and Assignees, is there any other file i should modify?", "Hi Mahmoud Aslan(@mhmoodlan),\r\nWe are researchers working on identifying redundant development and duplicated pull requests. We have found there is a pull request: https://github.com/tensorflow/tensorflow/pull/20756 which might be a potentially duplicate to this one. We would like to build the link between developers to reduce redundant development. We would really appreciate if you could help us to validate and give us feedback.\r\nThank you very much for your time!\r\n", "Hi Luyao Ren(@FancyCoder0),\r\nThe pull request you are referring to ([#20756](https://github.com/tensorflow/tensorflow/pull/20756)) is closed, however, they opened another one ([20758](https://github.com/tensorflow/tensorflow/pull/20758)) considering the same idea and they explain in this [comment](https://github.com/tensorflow/tensorflow/pull/20758#issuecomment-405236311) the main difference between their pull request ([20758](https://github.com/tensorflow/tensorflow/pull/20758)) and this one.", "@sguada, not sure whether you are the best person here, can u take a look if you don't mind?\r\n\r\n\r\n\r\n", "It would be good to see some experiment results to show the advantages of the implementation.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/20758#issuecomment-413770310", "Closing given @alextp 's comment above."]}, {"number": 20784, "title": "BUG: Estimator training with tf.contrib.distribute fails under 1.9.0 but works under 1.8.0", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.9.0 and and v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**:\r\n3.6.5 \r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n9.0\r\n- **GPU model and memory**:\r\nTitan 1080 Ti 12GB\r\n- **Exact command to reproduce**:\r\nSee description\r\n\r\n### Describe the problem\r\nTraining Estimator single or multi-gpu with OneDeviceStrategy and MirroredStrategy respectively works under 1.8.0 but produces a graph assertion error under 1.9.0. I know of no breaking changes regarding this mentioned in the [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.9.0).\r\n\r\n### Source code / logs\r\n\r\nI broke it down to the following MWE. Under 1.8.0, both versions work fine, whereas in 1.9.0, the commented out lines work fine and the marked line produces the error as outlined in the Traceback.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef model_fn(features, labels=None, mode=tf.estimator.ModeKeys.TRAIN, params=None):\r\n\r\n    train_images_batch = features\r\n    res = tf.layers.conv2d(inputs=train_images_batch, filters=3, kernel_size=3, strides=1, padding='same', data_format='channels_first')\r\n    loss = tf.reduce_mean((train_images_batch - res) ** 2)\r\n    optimizer = tf.train.AdamOptimizer().minimize(loss = loss)\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=tf.estimator.ModeKeys.TRAIN,\r\n        loss=loss, train_op=optimizer)\r\n\r\nclass Dataset():\r\n    def __init__(self):\r\n        self.dataset = tf.data.Dataset.from_tensors(tf.ones([3,124,124]))\r\n\r\n    def map_func(self, entry):\r\n\r\n        return (tf.zeros([3,124,124]))\r\n      \r\n    def input_fn(self):\r\n\r\n        #dataset = tf.data.Dataset.from_tensors(np.ones(shape=(3,124,124)))\r\n        #dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(1, count=1))\r\n        dataset = self.dataset.apply(tf.contrib.data.shuffle_and_repeat(1, count=1)) # this line actually produces the error\r\n        dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func= self.map_func, num_parallel_batches=1, batch_size=1))\r\n        dataset = dataset.prefetch(buffer_size = 1)\r\n\r\n        return dataset\r\n\r\ndset = Dataset()\r\n\r\nsingle_gpu = tf.contrib.distribute.OneDeviceStrategy(device='/gpu:0')\r\n\r\nrunconfig = tf.estimator.RunConfig(train_distribute=single_gpu)\r\n\r\nestimator = tf.estimator.Estimator(model_fn=model_fn,\r\n                                   config=runconfig)\r\n\r\nestimator.train(input_fn = lambda: dset.input_fn())\r\n```\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/staff/rippel/TF_MSD/MWE.py\", line 40, in <module>\r\n    estimator.train(input_fn = lambda: dset.input_fn())\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 366, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1117, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1145, in _train_model_distributed\r\n    input_fn, model_fn_lib.ModeKeys.TRAIN))\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 987, in _get_features_and_labels_from_input_fn\r\n    return estimator_util.parse_input_fn_result(result)\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/estimator/util.py\", line 104, in parse_input_fn_result\r\n    iterator = result.make_initializable_iterator()\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 606, in make_initializable_iterator\r\n    dataset_iterator = self._dataset.make_initializable_iterator()\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/prefetching_ops_v2.py\", line 181, in make_initializable_iterator\r\n    shared_name=shared_name)\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/prefetching_ops_v2.py\", line 105, in __init__\r\n    self._input_dataset)\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 311, in make_initializer\r\n    dataset._as_variant_tensor(), self._iterator_resource, name=name)  # pylint: disable=protected-access\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2082, in _as_variant_tensor\r\n    self._input_dataset._as_variant_tensor(),  # pylint: disable=protected-access\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/batching.py\", line 485, in _as_variant_tensor\r\n    input_resource = self._input_dataset._as_variant_tensor()\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/shuffle_ops.py\", line 62, in _as_variant_tensor\r\n    sparse.as_dense_shapes(self.output_shapes, self.output_classes)))\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 3554, in shuffle_and_repeat_dataset\r\n    output_types=output_types, output_shapes=output_shapes, name=name)\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 350, in _apply_op_helper\r\n    g = ops._get_graph_from_inputs(_Flatten(keywords.values()))\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5663, in _get_graph_from_inputs\r\n    _assert_same_graph(original_graph_element, graph_element)\r\n  File \"/home/staff/rippel/.conda/envs/rippel_tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5599, in _assert_same_graph\r\n    original_item))\r\nValueError: Tensor(\"buffer_size:0\", shape=(), dtype=int64, device=/device:CPU:0) must be from the same graph as Tensor(\"TensorDataset:0\", shape=(), dtype=variant).\r\n```", "comments": ["I would say this is expected behavior. One of the key features of estimator is that it performs graph and session management for you. So what you've done is you've created a tensor in the default graph (implicitly when you create self.dataset). However model_fn is called within estimator, using the estimator's graph. So you're trying to create a tensor in one graph that depends on another graph which is not allowed. The fact that is worked before is a bug in 1.8 I believe, rather than vice versa. \r\n\r\nThanks for submitting, and for providing a lovely minimal example."]}]