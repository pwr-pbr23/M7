[{"number": 52041, "title": "onnx->tensorflow->tflite", "body": "_Originally posted by @abattery in https://github.com/tensorflow/tensorflow/issues/46006#issuecomment-918006725_\r\n\r\nHi @abattery !\r\nI have try your method\r\n\r\nPlease try out TensorFlow 2.6 or tf-nightly version with converter.experimental_enable_resource_variables = True.\r\nAnd it can convert tensorflow to tflite. But when it put some warning and log error when start network forward.\r\n\r\nHere is the warning.\r\n\r\n```\r\n2021-09-17 10:46:25.251638: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1844] Graph contains the following resource op(s), that use(s) resource type. Currently, the resource type is not natively supported in TFLite. Please consider not using the resource type if there are issues with either TFLite converter or TFLite runtime:\r\nResource ops: AssignVariableOp, ReadVariableOp, VarHandleOp\r\nDetails:\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> ()\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> () : {device = \"\"}\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<768x1024xf32>) -> () : {device = \"\"}\r\n        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>) -> (tensor<1024xf32>) : {device = \"\"}\r\n        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\r\n        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<1024xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_18\"}\r\n        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_18\"}\r\n2021-09-17 10:46:25.252215: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1855] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):\r\nFlex ops: FlexAssignVariableOp, FlexReadVariableOp, FlexVarHandleOp\r\nDetails:\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> ()\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>, tensor<1024xf32>) -> () : {device = \"\"}\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<1x1xf32>) -> ()\r\n        tf.AssignVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>, tensor<768x1024xf32>) -> () : {device = \"\"}\r\n        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<1024xf32>>>) -> (tensor<1024xf32>) : {device = \"\"}\r\n        tf.ReadVariableOp(tensor<!tf_type.resource<tensor<?x?xf32>>>) -> (tensor<?x?xf32>) : {device = \"\"}\r\n        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<1024xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_bias_lstm_18\"}\r\n        tf.VarHandleOp() -> (tensor<!tf_type.resource<tensor<?x?xf32>>>) : {allowed_devices = [], container = \"\", device = \"\", shared_name = \"lstm_kernel_lstm_18\"}\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2021-09-17 16:00:43.773435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-17 16:00:43.774454: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1829] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 37 nodes with 1 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 4 nodes with 1 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 29 nodes with 1 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 27 nodes with 1 partitions.\r\nTraceback (most recent call last):\r\n  File \"onnx2tflite.py\", line 59, in <module>\r\n    interpreter.invoke()\r\n  File \"/root/anaconda3/envs/tfnight/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 858, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (0 != 256)Node number 25 (RESHAPE) failed to prepare.\r\nNode number 30 (WHILE) failed to invoke.\r\n```\r\n\r\nIt there any suggestion to solve the problem?\r\n\r\nThanks.", "comments": ["I find the similar problem in this issue.\r\nhttps://github.com/tensorflow/tensorflow/issues/51668", "Hi @lzc16 \r\n\r\nLooks like you're trying to run with Flex and you get this failure. Can you share the original saved model and the conversion command/script you used.\r\nOr if you have a small reproduce sample please share it\r\n\r\nThanks", "[cldnn.txt](https://github.com/tensorflow/tensorflow/files/7203982/cldnn.txt)\r\n\r\n```\r\nimport os\r\nos.environ[\"TF_KERAS\"] = '1'\r\nimport onnx\r\nfrom onnx_tf.backend import prepare\r\n\r\nonnx_model = onnx.load(\"cldnn.onnx\")  # load onnx model\r\ntf_exp = prepare(onnx_model)  # prepare tf representation\r\ntf_exp.export_graph(\"cldnn_tflite_saved_model.pb\")  # export the model\r\nprint('saved_model export success, saved as cldnn_tflite_saved_model' )\r\n\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('cldnn_tflite_saved_model.pb')\r\n#converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_enable_resource_variables = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nwith open('cldnn.tflite','wb') as g:\r\n    g.write(tflite_model)\r\nprint('tflite export success, saved as cldnn.tflite' )\r\n    \r\nh0 = tf.zeros([1,1,256], tf.float32)\r\nc0 = tf.zeros([1,1,256], tf.float32)\r\nh1 = tf.zeros([1,1,256], tf.float32)\r\nc1 = tf.zeros([1,1,256], tf.float32)\r\ninputs = tf.random.normal(shape=[150,1,5,257],dtype=tf.float32)\r\n\r\nh0_tflite = h0\r\nh1_tflite = h1\r\nc0_tflite = c0\r\nc1_tflite = c1\r\ninterpreter = tf.lite.Interpreter(model_path='cldnn.tflite')\r\ninterpreter.allocate_tensors()\r\n\r\nfor idx in range(inputs.shape[0]):\r\n    interpreter.set_tensor(input_details[0]['index'], h1_tflite)\r\n    interpreter.set_tensor(input_details[1]['index'], c1_tflite)\r\n    interpreter.set_tensor(input_details[2]['index'], h0_tflite)\r\n    interpreter.set_tensor(input_details[3]['index'], c0_tflite)\r\n    interpreter.set_tensor(input_details[4]['index'], inputs[idx:idx + 1])\r\n    interpreter.invoke()\r\n    h1_tflite = interpreter.get_tensor(output_detials[0]['index'])\r\n    c1_tflite = interpreter.get_tensor(output_detials[2]['index'])\r\n    h0_tflite = interpreter.get_tensor(output_detials[3]['index'])\r\n    c0_tflite = interpreter.get_tensor(output_detials[4]['index'])\r\n    result = interpreter.get_tensor(output_detials[5]['index'])\r\n```\r\n\r\nHi @karimnosseir !\r\n\r\nThanks for your reply!\r\n\r\nPlease rename the cldnn.txt to cldnn.onnx because github can't support upload onnx document.\r\n\r\nAnd I post the script above. Please let me know if you have any problem with the document.\r\n\r\nAgain, Thanks for your help! \r\n\r\nBest regards\r\n\r\n", "I looked at the issue, the failure is because FullyConnected op has invalid inputs. I tried running the saved model and i am getting failure also.\r\nCan you please verify this model works in TF (didn't work for me using the example code you sent)\r\n\r\nThanks", "Hi, @karimnosseir ,\r\n\r\nI can run the model with onnx with the code below.\r\n```\r\nimport numpy as np\r\nimport onnxruntime as ort\r\nmodel = onnx.load(\"cldnn.onnx\")\r\nort_session = ort.InferenceSession('cldnn.onnx')\r\ntest_arr = np.array(np.random.randn(150,1,5,257), dtype=np.float32)\r\nc11 = np.zeros([1, 1, 256], dtype=np.float32)\r\nh11 = np.zeros([1, 1, 256], dtype=np.float32)\r\nc22 = np.zeros([1, 1, 256], dtype=np.float32)\r\nh22 = np.zeros([1, 1, 256], dtype=np.float32)\r\n\r\nfor idx in range(150):\r\n    outputs, snr, h11, c11, h22, c22= ort_session.run(None, {'input': test_arr[idx:idx + 1], \"h1_n\":h11, \"c1_n\":c11, \"h2_n\":h22, \"c2_n\":c22})\r\n    if idx==0:\r\n        stream_onnx_outputs = outputs\r\n        stream_onnx_snroutput = snr\r\n    else:\r\n        stream_onnx_outputs = np.vstack((stream_onnx_outputs, outputs))\r\n        stream_onnx_snroutput = np.vstack((stream_onnx_snroutput, snr))\r\n```\r\n\r\nIt seems that when I converted to TF it brought some error in LSTM, which caused model don't work in TF.\r\nTherefore, is there any suggestion to convert to TF correctly?\r\n\r\nThanks.\r\nBest regards!", "It looks like it is a problem with onnx-tf not with TF/TFLite. Might be good to reach to ONNX and file issue for them.\r\nI am closing this issue.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52041\">No</a>\n"]}, {"number": 52044, "title": "Issue with tflite interpreter", "body": "Hi,\r\nTraining a model with MNIST database and converting to tflite, while inferencing I'm getting error.\r\nMy code details are as given below \r\n`\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n(ds_train, ds_test), ds_info = tfds.load( 'mnist',\r\n                                         split=['train', 'test'],\r\n                                         shuffle_files=True,\r\n                                         as_supervised=True,\r\n                                         with_info=True,\r\n                                         )\r\ndef normalize_img(image, label):\r\n  return tf.cast(image, tf.float32) / 255., label\r\n\r\nds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\nds_train = ds_train.cache()\r\nds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\r\nds_train = ds_train.batch(128)\r\nds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\r\nds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n\r\nds_test = ds_test.batch(128)\r\nds_test = ds_test.cache()\r\nds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n#model \r\nmodel = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Reshape((1,784)),\r\n        tf.keras.layers.Conv1D( 16, 1,use_bias=False,activation = 'relu' ),\r\n\t\ttf.keras.layers.Conv1D(16, 1,use_bias=False ,activation = 'relu' ),\r\n\t\ttf.keras.layers.Conv1D( 16, kernel_size=3,  strides=1, padding= \"causal\", dilation_rate=2**0, groups=16,use_bias=False,activation ='relu'),\r\n\t\ttf.keras.layers.Conv1D( 16, 1 ,use_bias=False) ,\r\n\t\ttf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(10)\r\n        ])\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.Adam(0.001),\r\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\r\n)\r\n\r\nmodel.fit(\r\n    ds_train,\r\n    epochs=1,\r\n    validation_data=ds_test,\r\n)\r\nmodel.summary()\r\nmodel.save(\"./model.h5\")\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\n# Save the tflite model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n`\r\n\r\ntflite interpreter code \r\n`import numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"./assignment.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n`\r\n\r\n\r\nI'm Getting\r\nException has occurred: RuntimeError\r\n\r\ntensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (16 != 1)Node number 13 (CONV_2D) failed to prepare.\r\n\r\nWhen I'm trying tflite convertion and tflite interpreter I'm getting error which is as mentioned .\r\nI tried to debug the issue and understood that if I kept the output of dilated convolution  as the input for the next layers then I'm getting this error. (due to dilated convolution dependency )\r\n\r\n1.How to resolve this error?\r\n2. If we cannot do it directly with tflite , then is there any alternative method for post quantization of model such as given above code?\r\n\r\nAny help will be greatly appreciable . Thanks in Advance!", "comments": ["@vanajareedy0426 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose), please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52044\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52044\">No</a>\n"]}, {"number": 52040, "title": "[TF-TRT] Improve logging message for the unconverted/unsupported OPs", "body": "@bixia1 for review\r\n\r\nThis PR improves the log message that reports unsupported operations. Previously, the log messages show a list of not-converted operations. For unsupported operations used multiple times in the graph, the operation appears multiple times in the list. The new log message will show a list of unique unsupported operations with the number of instance they are used in the graph in a better format.", "comments": ["Let's revise the PR description by describing the improvement of the log message, instead of just showing the old and new log. Something like this:\r\n\r\nThis PR improves the log message that reports unsupported operations. Previously, the log messages show a list of not-converted operations. For unsupported operations used multiple times in the graph, the operation appears multiple times in the list. The  new log message will show a list of unique unsupported operations with the number of instance they are used in the graph in a better format.\r\n", "Copying here the changes produced by this PR\r\n\r\n### Before\r\n\r\n```bash\r\n2021-09-17 02:27:20.631534: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:790] There are 836 ops of 15 different types in the graph that are not converted to TensorRT: NoOp, Mul, Pack, Prod, ConcatV2, Identity, Shape, StridedSlice, Tile, Placeholder, Fill, Reshape, GatherV2, Cast, AddV2, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).\r\n```\r\n\r\n### After\r\n\r\n```bash\r\n################################################################################\r\nTensorRT unsupported/unconverted OP Report:\r\n\t- Reshape -> 197x\r\n\t- GatherV2 -> 146x\r\n\t- Prod -> 146x\r\n\t- Pack -> 125x\r\n\t- Shape -> 87x\r\n\t- ConcatV2 -> 73x\r\n\t- AddV2 -> 36x\r\n\t- StridedSlice -> 14x\r\n\t- NoOp -> 5x\r\n\t- Fill -> 2x\r\n\t- Cast -> 1x\r\n\t- Identity -> 1x\r\n\t- Mul -> 1x\r\n\t- Placeholder -> 1x\r\n\t- Tile -> 1x\r\n--------------------------------------------------------------------------------\r\n\t - Total unconverted OPs: 836\r\n\t - Total unconverted OP Types: 15\r\nFor more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.\r\n################################################################################\r\n```\r\n\r\nIt clearly improves on readability and ability to debug performance issues. Here an example with some transformer-like model with poor conversion rate", "@bixia1 all your comments have been fixed"]}, {"number": 52039, "title": "Fix segfault with static unpack nodes", "body": "Models that have dequantize operations can occasionally cause a segfault because the tensors are skipped in the delegate creation process and the code tries to get an element from an empty array which is undefined.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52039) for more info**.\n\n<!-- need_author_cla -->", "@dimitarz Can you please sign CLA. Thanks!", "@googlebot I fixed it.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52039) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52039) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "@mihaimaruseac The AMD ROCm build failed. I can't seem to be able to access the build job to see what went wrong. Any guidance would be appreciated.", "Any update on this one?", "@dimitarz Any update on this PR? Please. Thanks!"]}, {"number": 52038, "title": "Tensor board wont load log files ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.1\r\n- Python version:3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuDNN v 8100\r\n- GPU model and memory: 3080 10gb \r\nI am trying to explore the log files generated by \r\ntf.debugging.experimental.enable_dump_debug_info(\r\n    \"/tmp/tfdbg2_logdir\",\r\n    tensor_debug_mode=\"FULL_HEALTH\",\r\n    circular_buffer_size=-1)\r\nbut after loading the tensorboard 2.6 with the following command \r\ntensorboard --logdir /tmp/tfdbg2_logdir/ --load_fast=false\r\n\r\nI get a message that there is no data to be loaded \r\nI have checked and there are log files in this dir \r\n", "comments": ["@vulkomilev ,\r\nThis issue is more suitable for TensorBoard repo. Please post it on [tensorflow/tensorboard](https://github.com/tensorflow/tensorboard/issues) repo from here. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52038\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52038\">No</a>\n"]}, {"number": 52037, "title": "tf.audio.decode_wav mmap like scipy.io.wavfile.read", "body": "would be much faster in my use case to read in just a portion of a WAV file.  scipy has a nice memory map keyword argument which permits this:  `scipy.io.wavfile.read(path_to_wavfile, mmap=True)`.  sadly `tf.audio.decode_wav` does not.  thanks!", "comments": ["Hi @bjarthur \r\nCould you please fill the feature template from [here ](https://github.com/tensorflow/tensorflow/issues/new/choose), It will help us expedite the issue. Thanks!", "Hi @Saduf2019 ,Could you look at this feature request?", "**System information**\r\n- TensorFlow version (you are using):  2.5 currently, will upgrade to 2.6 soon\r\n- Are you willing to contribute it (Yes/No): no\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**. add a means to (lazily?) load a small portion of a WAV file, perhaps with a memory map keyword argument, or alternatively by passing in an offset and length.\r\n\r\n**Will this change the current api? How?**. will add to the API in a backwards compatible way\r\n\r\n**Who will benefit with this feature?**. those with large WAV files that only train on a portion of them\r\n\r\n**Any Other info.**\r\n", "@bjarthur \r\nPlease feel free to submit a pr for the change requested or share the link where the change is to be made.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52036, "title": "Trying to follow simple Tutorial For LOADING TEXT but tensorflow_text won't import ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary? Not sure, just following https://www.tensorflow.org/tutorials/load_data/text\r\n- TensorFlow version: \r\ntensorflow                    2.6.0\r\ntensorflow-estimator          2.6.0\r\ntensorflow-hub                0.12.0\r\ntensorflow-io-gcs-filesystem  0.21.0\r\ntensorflow-text               2.6.0\r\ntensorflow-text-nightly       2.7.0.dev20210916\r\n- Python version: Python 3.7.7\r\n- Installed using virtualenv? pip? conda?: Pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nFirst steps of loading text. \"Be sure you're using stable versions of tf and tf-text\"   Followed instructions to uninstall and re install tf-nightly / tensorflow-text-nightly.\r\n\r\nFirst issue was package conflict/dependency issues  about wanting \"flat buffers 2.0\" but I currently had flat buffers 1.12; after updating to flat buffers2.0 I get another package conflict that a separate tevnsofrlow package wants flat buffers 1.12 but I now have flatbuffers 1.12 which is, you guessed it, incompatible. \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFrom the tutorial:\r\n'''\r\nimport collections\r\nimport pathlib\r\nimport re\r\nimport string\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import losses\r\nfrom tensorflow.keras import preprocessing\r\nfrom tensorflow.keras import utils\r\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow_text as tf_text\r\n'''\r\n\r\nwarning:2021-09-16 13:16:16.308765: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/dropout/temp_rate_is_zero\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAlreadyExistsError                        Traceback (most recent call last)\r\n/var/folders/wg/tzq4qwy97x3_999pctn35jlh0000gn/T/ipykernel_1983/2446760814.py in <module>\r\n      6 import tensorflow as tf\r\n      7 \r\n----> 8 from tensorflow.keras import layers\r\n      9 from tensorflow.keras import losses\r\n     10 from tensorflow.keras import preprocessing\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/api/_v2/keras/__init__.py in <module>\r\n      8 import sys as _sys\r\n      9 \r\n---> 10 from keras import __version__\r\n     11 from keras.api._v2.keras import __internal__\r\n     12 from keras.api._v2.keras import activations\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/__init__.py in <module>\r\n     23 \r\n     24 # See b/110718070#comment18 for more details about this import.\r\n---> 25 from keras import models\r\n     26 \r\n     27 from keras.engine.input_layer import Input\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/models.py in <module>\r\n     18 import tensorflow.compat.v2 as tf\r\n     19 from keras import backend\r\n---> 20 from keras import metrics as metrics_module\r\n     21 from keras import optimizer_v1\r\n     22 from keras.engine import functional\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/metrics.py in <module>\r\n     24 \r\n     25 import numpy as np\r\n---> 26 from keras import activations\r\n     27 from keras import backend\r\n     28 from keras.engine import base_layer\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py in <module>\r\n     18 \r\n     19 from keras import backend\r\n---> 20 from keras.layers import advanced_activations\r\n     21 from keras.utils.generic_utils import deserialize_keras_object\r\n     22 from keras.utils.generic_utils import serialize_keras_object\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/layers/__init__.py in <module>\r\n     29 \r\n     30 # Image preprocessing layers.\r\n---> 31 from keras.layers.preprocessing.image_preprocessing import CenterCrop\r\n     32 from keras.layers.preprocessing.image_preprocessing import RandomCrop\r\n     33 from keras.layers.preprocessing.image_preprocessing import RandomFlip\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/layers/preprocessing/image_preprocessing.py in <module>\r\n     22 from keras.engine import base_layer\r\n     23 from keras.engine import base_preprocessing_layer\r\n---> 24 from keras.preprocessing import image as image_preprocessing\r\n     25 from keras.utils import control_flow_util\r\n     26 from tensorflow.python.ops import stateless_random_ops\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/preprocessing/__init__.py in <module>\r\n     24 from keras.preprocessing import text\r\n     25 from keras.preprocessing import timeseries\r\n---> 26 from keras.utils import all_utils as utils\r\n     27 \r\n     28 # This exists for compatibility with prior version of keras_preprocessing.\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/all_utils.py in <module>\r\n     32 from keras.utils.generic_utils import serialize_keras_object\r\n     33 from keras.utils.layer_utils import get_source_inputs\r\n---> 34 from keras.utils.multi_gpu_utils import multi_gpu_model\r\n     35 from keras.utils.np_utils import normalize\r\n     36 from keras.utils.np_utils import to_categorical\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/multi_gpu_utils.py in <module>\r\n     18 from keras import backend\r\n     19 from keras.engine.training import Model\r\n---> 20 from keras.layers.core import Lambda\r\n     21 from keras.layers.merge import concatenate\r\n     22 \r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/layers/core/__init__.py in <module>\r\n     18 from keras.layers.core.activity_regularization import ActivityRegularization\r\n     19 from keras.layers.core.dense import Dense\r\n---> 20 from keras.layers.core.dropout import Dropout\r\n     21 from keras.layers.core.flatten import Flatten\r\n     22 from keras.layers.core.lambda_layer import Lambda\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/keras/layers/core/dropout.py in <module>\r\n     26 keras_temporary_dropout_rate = tf.__internal__.monitoring.BoolGauge(\r\n     27     '/tensorflow/api/keras/dropout/temp_rate_is_zero',\r\n---> 28     'Temporarily record if Keras dropout layer was created w/'\r\n     29     'constant rate = 0')\r\n     30 \r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/monitoring.py in __init__(self, name, description, *labels)\r\n    359     \"\"\"\r\n    360     super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,\r\n--> 361                                     len(labels), name, description, *labels)\r\n    362 \r\n    363   def get_cell(self, *labels):\r\n\r\n~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/monitoring.py in __init__(self, metric_name, metric_methods, label_length, *args)\r\n    133           self._metric_name, len(self._metric_methods)))\r\n    134 \r\n--> 135     self._metric = self._metric_methods[self._label_length].create(*args)\r\n    136 \r\n    137   def __del__(self):\r\n\r\nAlreadyExistsError: Another metric with the same name already exists.\r\n", "comments": ["@bestmatthew \r\nYou may refer to similar solved issues: [link](https://stackoverflow.com/questions/58012741/error-importing-tensorflow-alreadyexistserror-another-metric-with-the-same-nam),[link1](https://discuss.tensorflow.org/t/alreadyexistserror-another-metric-with-the-same-name-already-exists/3787),[link2](https://github.com/tensorflow/tensorflow/issues/32574)", "@Saduf2019  thank you for the links.\r\n\r\nlink0 = don't import twice. not an issue. \r\n\r\nlink1 = same error, no solution. OK....\r\n\r\nlink2 = error cause by importing tensorflow_core, which I'm not doing. \r\n\r\nHowever I do not think these links are helpful. \r\n\r\nI appreciate the help and would love any additional input. Thanks! \r\n", "@bestmatthew \r\nThe code provided seems incomplete please share a colab gist of possible with the error reported.", "Hi @Saduf2019, unfortunately  the code is complete and  provided above. \r\n\r\nIt is VERBATIM copied from the tensorflow tutorial for loading text. (https://www.tensorflow.org/tutorials/load_data/text)\r\n\r\nThe block of code, again:\r\n'''\r\n# Be sure you're using the stable versions of both tf and tf-text, for binary compatibility.\r\npip uninstall -y tensorflow tf-nightly keras\r\n\r\npip install -q -U tf-nightly\r\npip install -q -U tensorflow-text-nightly\r\n'''\r\n\r\n'''\r\nimport collections\r\nimport pathlib\r\nimport re\r\nimport string\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import losses\r\nfrom tensorflow.keras import preprocessing\r\nfrom tensorflow.keras import utils\r\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow_text as tf_text\r\n'''\r\n\r\n", "@bestmatthew \r\nI ran the code shared and do not face any issues as reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/171b9d50384674d134162546e2d1c71d/untitled641.ipynb)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52036\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52036\">No</a>\n", "@bestmatthew Did you make any progress on this issue? I'm having an issue related to flatbuffer dependency when installing tf-text as well. [edit] I resolved my issue.\r\n\r\nTensorflow is very sensitive to version conflicts. Check here: https://www.tensorflow.org/install/\r\nYou can check if your current pip version is supported. Both your pip version and your python version can cause conflicts. As they say on that page, easiest solution is ``` pip install --upgrade pip``` and then install tensorflow_text (it is likely that installing it will automatically reinstall tensorflow)\r\n\r\nIf you need to use python 3.7.7 then you should install a combination of pip and tensorflow that will not have conflicts in that environment.", "@nershman thank you for understanding tensorflow better than the tf staff. \r\n\r\nI was not able to resolve the issue, TF refuses to import so I ran my work in google colabs out of necessity. I have not been able to get TF running in my native ide but will troubleshoot this a little further with my package versions. I am moving to a new machine shortly and I imagine (hope) the new environment will also not have this issue. Thanks for the suggestion, will report back later this week if I can get it running a more current python iteration, I was using python 3.7.7 for school but have no reason to stick with it anymore. Thanks. ", "@bestmatthew I'm also having issues with imports but I'm on on a earlier version of macOS. If it mentions a \"symbol not found\" or a missing dylib it may be an issue with how TF was built. There seems to be a few issues related to this which are in the process of being resolved. If you have more issues another option is to try building TF yourself (there's an option for that in pip, but you need to have XCode installed). Good luck and I hope you can get it working!"]}, {"number": 52035, "title": "bug while summing ragged  tensors", "body": "**System information**\r\nNot needed, check colab example below \r\n\r\n**Current behavior**\r\nWhen summing 2 ragged tensor of (apparently) the same shape the following error is rised:\r\n`InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'Unable to broadcast: dimension size mismatch in dimension' \r\n`\r\n\r\n**Describe the expected behavior**\r\nWell, the sum to proceed correctly\r\n\r\n**Standalone code to reproduce the issue**\r\ncolab: https://colab.research.google.com/drive/1U7IVgd7T2Y4LrUXCsgBmlHU0VWBX6Ka4?usp=sharing", "comments": ["@aiace9 ,\r\nPlease take a look at this SO [link](https://stackoverflow.com/questions/61947237/broadcasting-with-ragged-tensor) with similar error.It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52035\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52035\">No</a>\n"]}, {"number": 52034, "title": "Fix arguments passed to get_environment when failing to locate executables", "body": "PR fixes an issue where `get_python_bin` and `get_bash_bin` pass a string in place of `repository_ctx` to `get_environ` if these functions fail to locate an executable.", "comments": []}, {"number": 52033, "title": "TF-TRT Add n_build_pass attribute", "body": "To convert a model with TF-TRT dynamic shapes, one can provide profile information by inferring the segmented model a number of times. Currently the build mode uses [_profile_generation_mode](https://github.com/tensorflow/tensorflow/blob/6ce1270b038b7407bf6eefded34d543fea85a6ed/tensorflow/python/compiler/tensorrt/trt_convert.py#L1203) attribute of the graph to declare that we are in build mode. This requires two rewrites of the graph. Here is the current workflow\r\n1. Rewrite the graph to set `_profile_generation_mode=True`\r\n2. infer the model n_inputs times\r\n3. rewrite the graph to set `_profile_generation_mode=False` \r\n\r\nThis PR introduces a new attribute for TRTEngineOp: `_n_build_pass`, this is can be set by the rewriter config ([example](https://github.com/tfeher/tensorflow/blob/45680b70dc0a59cd5e7ef783b2b35a10a338d4e8/tf_trt_cpp_example/trt_convert.cc#L196)).  If we set this parameter during conversion time then we can avoid rewriting the graph to enable/disable build mode.\r\n\r\nThis PR is not essential to C++ conversion of TF-TRT, it just decreases the number of graph rewrite steps. Alternatively to this PR we could:\r\n- Mirror a the python side workflow on the C++ side: two rewrites of  `_profile_generation_mode`. No changes needed in the TRT optimization pass, but extra work in the C++ converter.\r\n- Set `_profile_generation_mode` to true already during graph conversion. We would need to read this value from the rewriter config (same way as this PR reads the `_n_build_pass`) attribute.\r\n\r\nTagging @bixia1 for discussing these points and for review.\r\n\r\nTracker: #52012", "comments": ["In the current PR, _n_build_pass is a value pass from a user facing config to the TRTEngineOp to help the implementation of a TF-TRT API  that takes n user inputs, performs shape profiling and builds cuda engines. Changing the user facing config for this purpose is not necessary, as the implementation of the TF-TRT API can do these instead:\r\n* Derives the value of _b_build_pass from the number of user provided inputs.\r\n* Sets attribute_profile_generation_mode=True and runs the graph for n_build_pass times, to collect shape profile information. \r\n* Set attribute _profile_generation_mode=False and runs the graph to build the engine.\r\n", "Thanks @bixia1 for the comment. Indeed, if the approach that you describe would work, then we do not need this PR and we can close. There is one complication though:\r\n- We create `session1` with `graph_def1`, which has  `_profile_generation_mode=True`.\r\n- We infer the graph in `session1` to collect the optimization profiles\r\n- In the last step we need to rewrite the graph_def to set `_profile_generation_mode=False`. Let us call the it `graph_def2`\r\n- We would create a new `session2` with `graph_def2`, similar to this step in the [converter example](https://github.com/tfeher/tensorflow/blob/45680b70dc0a59cd5e7ef783b2b35a10a338d4e8/tf_trt_cpp_example/main.cc#L115-L117). We want to infer the new graph to initiate building the TRT engines.\r\n- The newly created session seems to have new `Device` objects attached. We will also have a new `ResourceMgr`.\r\n- This way we do not have access to the `TRTEngineCacheResource` that [stored the profile](https://github.com/tensorflow/tensorflow/blob/55bcea4a34ff39e1039f9852c125e1f4c5a9f3d3/tensorflow/compiler/tf2tensorrt/utils/trt_lru_cache.h#L233) information collected in session1.\r\n\r\nWhat is the right way to overcome this problem?\r\n1. Could we create the`session2` in a way that keeps the `TRTEngineCacheResource` from the old `session1`?\r\n2. Is there a way to extend `session1` with  `graph_def2` and run it?\r\n3. A variante of the above, can we access the `Graph` object from `session1` and add  graph_def2 as a function?\r\n4. Shall we just manually copy over the `TRTEngineCacheResource` objects from `session1` to `session2`?\r\n\r\n", "We can modify the _profile_generation_mode attribute in the TRTEngineOp inside the SavedModelBundle graphdef, without creating a new session. I think we are already doing something similar to this in the Python converterV2.build method. Do you agree?\r\n\r\nAnother problem with having _n_build_pass attribute in the TRTEngineOp and relying on its the value to become 0 in order to trigger the build of the cuda engine is that, the number of inputs provided by users may not be the same as the number of time we execute the TRTEngineOp, in the presence of loops and if-stmt.", "I agree with you, that it is desirable to use the existing `_profile generation_mode` attribute.\r\n\r\n> We can modify the _profile_generation_mode attribute in the TRTEngineOp inside the SavedModelBundle graphdef, without creating a new session.\r\n\r\nHow to do that, could you point me to an example? \r\n\r\nIt is not clear to me which API to use. Below is one example where I rewrite the GraphDef, but that does not change the actual graph used by the session. I need to create a new session for the changes to take effect. \r\n\r\n```c++\r\n  // Create end execute a graph with a single const op\r\n  Scope root = Scope::NewRootScope();\r\n  auto c = Const(root.WithOpName(\"my_const\"), {{42.0f, 137.0f}});\r\n  ClientSession session(root);\r\n  std::vector<Tensor> outputs;\r\n\r\n  Status status = session.Run({c}, &outputs);\r\n  if (status.ok()) std::cout << outputs[0].DebugString() << std::endl;\r\n\r\n  // Get the graph def and rewrite the constant value\r\n  GraphDef gdef;\r\n  status = root.ToGraphDef(&gdef);\r\n  tensorflow::Tensor new_val(tensorflow::DT_FLOAT,\r\n                             tensorflow::TensorShape({1, 2}));\r\n  float *tensor_flat = new_val.flat<float>().data();\r\n  tensor_flat[0] = 31;\r\n  tensor_flat[1] = 41;\r\n\r\n  NodeDef *node = gdef.mutable_node(0);\r\n  TensorProto *tensor_attr = (*node->mutable_attr())[\"value\"].mutable_tensor();\r\n  new_val.AsProtoTensorContent(tensor_attr);\r\n  \r\n  // Changing the graph def has no effect on the results\r\n  status = session.Run({c}, &outputs);\r\n  if (status.ok()) std::cout << outputs[0].DebugString() << std::endl;\r\n\r\n  // Alternative: create a new session with the modified graph def:\r\n  std::unique_ptr<tensorflow::Session> session2(\r\n      tensorflow::NewSession(tensorflow::SessionOptions()));\r\n  status = session2->Create(gdef);\r\n  status = session2->Run({}, {\"my_const\"}, {}, &outputs);\r\n  std::cout << outputs[0].DebugString() << std::endl;\r\n```\r\n\r\n> I think we are already doing something similar to this in the Python converterV2.build method. Do you agree?\r\n\r\n- I agree. We use [function_from_graph_def](https://github.com/tensorflow/tensorflow/blob/294a0c6895553ea8f1e45e7e1ccb906da23d00c8/tensorflow/python/eager/wrap_function.py#L636) which relies on [import_graph_def_internal](https://github.com/tensorflow/tensorflow/blob/294a0c6895553ea8f1e45e7e1ccb906da23d00c8/tensorflow/python/framework/importer.py#L422). \r\n- The latter uses TF C API calls, like `TF_GraphImportGraphDefWithResults`. This function can apply a prefix to the names in the graph_def, and that way we could import that to existing session.\r\n\r\nIt is not clear to me how to get the Graph object from the current session, and which API to use to manage the session. Could you give suggestions for the above example, on how to import the modified GraphDef back into the original session?\r\n\r\n\r\n", "Closing this PR as it is not necessary. One can use `ImportGraphDef` to load the modified gdef with a prefix into the existing session.\r\n\r\nSome notes: \r\n- We cannot modify the `_profile_generation_mode` attribute of a node that is already constructed. We need to run the constructor to read the attribute. \r\n- When we execute the prefixed graph, it will create a new `TRTEngineOp` object, but that object willl use the same `TrtEngineCacheResource` object, because the [prefix is ignored](https://github.com/tensorflow/tensorflow/blob/6a41d5b2c95469e9c9b722948216fa3395287d5d/tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc#L906-L910) while looking op the cache resource. \r\n- An alternative solution could be to store the `_profile_generation_mode` as a state variable in the `TrtEngineCacheResoure`. One could have a special op to set its value. Alternatively, one could also access the cache resource directly through the `ResourceMgr` (but have to make sure that the `TrtEngineCacheResource` is already created in advance)."]}, {"number": 52032, "title": "Failed copying input tensor from CPU to GPU", "body": "### 1. System information\r\nWhen i try use bert pretrain model to do my downstream tasks(what i use to load bert pretrain model is keras-bert), but when i try load my model to CPU and use GPU to train, the mistake occured. I have two GPUs with memory 32g* 2 and CPU with 128g memory,who know this problem,please give me answer,thanks \r\n1. when i only use CPU, it will consumes about 112g to train;\r\n2. when i only use GPU, the OOM kill will occur;\r\n3. by the way, when i use bert-base pretrain model and bi-GRU net to do my classify task, what memory configuration can i have to do this task\r\n\r\n### 2. Code\r\nwith tf.device(\"/cpu:0\"):\r\n   myModel = create_model()\r\nparallel_model = multi_gpu_model(model, gpus=2)\r\nparallel_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\r\nparallel_model.fit(train_data, train_label, validation_data=(valid_data, valid_label), epochs=epoch, batch_size=8, shuffle=True, callbacks=[history])\r\n\r\n### 3. (optional) Any other info / logs\r\nthe error log is \r\n![image](https://user-images.githubusercontent.com/30098191/133622256-8970595f-5cd2-4e45-ae4e-0ee29241bf77.png)\r\n\r\n\r\n", "comments": ["Hi @zfeilongzaitian !Could you please share a complete stand alone  code with links to Datasets as a  Colab gist to replicate the above issue. It will help expedite the issue. ", "> Hi @zfeilongzaitian !Could you please share a complete stand alone code with links to Datasets as a Colab gist to replicate the above issue. It will help expedite the issue.\r\n\r\nsorry replay for so later, I found the reason,because i make a batch_size too big, so it use too much memory ", "ok @zfeilongzaitian , Thanks for replying! Please Feel free to close this one and create a new one if you face any problem. Thanks!"]}, {"number": 52031, "title": "Code completion of Keras Moudle no longer work since 2.6 (removed keras)", "body": "Code completion of Keras Module no longer work in VSCode after Keras code is remvoed from TensorFlow 2.6", "comments": ["@edwardyehuang \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here and provide some more details on this issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52031\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52031\">No</a>\n"]}, {"number": 52029, "title": "Implementation of a Copy Removal Pass.", "body": "This copy removal pass is based on the `UserangeAnalysis`. First, all `Operations` implementing the `CopyOpInterface` are found, then the useranges of the source and target of the `CopyOpInterface` are intersected. The only `Operation` in the intersection must be the `CopyOp` or  `Operations` that are within the nested region if the CopyOp implements a `Region`. If this is the case, we can remove the CopyOp.\r\n\r\nThis PR includes an addition to the `UserangeAnalysis` to intersect two useranges.", "comments": ["Since Alex will leave us today, we close this PR and we will reopen it using an other account again.", "We reopen the draft as #52299."]}, {"number": 52028, "title": "Reshape conversion produces invalid output shape (if one dim is -1)", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4 LTS\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0\r\n\r\n### 2. Code\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninputs = tf.keras.Input(shape=(1, 126))\r\noutputs = tf.keras.layers.Reshape((-1, 21))(inputs)\r\n#outputs = tf.reshape(inputs, (-1, 21))\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\r\n\r\ninputs = np.random.rand(126).reshape((1, 1, 126))\r\noutputs = np.random.rand(126).reshape((1, 6, 21))\r\n\r\nmodel.fit(x=inputs, y=outputs, epochs=1)\r\n\r\ndef representative_dataset_gen():\r\n  for input in inputs:\r\n    input = input.astype(np.float32)\r\n    yield [input]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\n\r\ntflite_model = converter.convert()\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\n### 3. Failure after conversion\r\nConversion produces a reshape operation (and all subsequent ops) with incorrect output shape, which makes TFLM fail (as it uses only the output shape as the reshape input parameter):\r\n![reshape](https://user-images.githubusercontent.com/52713197/133601423-e585f707-953e-4b20-9374-7453a5becd54.png)\r\nI tried both, tf.keras.layers.Reshape and tf.reshape, and the result is the same, except the branch in the blue box is generated only for the Keras operator. (The issue is extracted from a detection model with many reshapes.)\r\n", "comments": ["Hi @ymodak ,Could you please look at this issue ,Providing Gist for reference. in TF [2.5](https://colab.research.google.com/gist/mohantym/7c9682a7405c7ff8c544db1883ec2cfe/github_52028.ipynb#scrollTo=1mn-mL9rZUUQ),[2.6](https://colab.research.google.com/gist/mohantym/de0b92956820c5b1f1125923e408ec8f/github_52028_2-6.ipynb),[2.7](https://colab.research.google.com/gist/mohantym/9f6c61db5e2653d8ecfda0ffc4bb3eff/github_52028.ipynb#scrollTo=1mn-mL9rZUUQ)", "**TF Saved Model**\r\n![TF Saved Model](https://user-images.githubusercontent.com/8790823/137837200-21ca86df-70cd-4172-b505-ac1a0aeb9c3e.png)\r\n\r\n**TFLite model**\r\n![TFLite model](https://user-images.githubusercontent.com/8790823/137837204-da3cb255-5631-4fa8-a4f2-7419296e1924.png)\r\n\r\nLooks like either the Pack, or reshape op fail to parse the data correctly? Triaging this for further review.\r\n\r\n(I simply downloaded the models and viewed them on Netron - can be done using any of the 3 gists linked above)\r\n", "Sorry for the late reply.\r\nThis looks correct, the shape gets visualized is the default shape where any dynamic dimension uses '1'. If you printed the input/output details you will see that the output has shape_signature = [-1, -1, 21]\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52028\">No</a>\n"]}, {"number": 52026, "title": "cannot import name 'Adadelta' from 'keras.optimizers (Keras-Bidaf)", "body": "ImportError                               Traceback (most recent call last)\r\n<ipython-input-26-bc8a96d755fb> in <module>\r\n----> 1 from bidaf.models import BidirectionalAttentionFlow\r\n      2 #bidaf_model = BidirectionalAttentionFlow(400)\r\n      3 #keras_model = bidaf_model.model\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\bidaf\\models\\__init__.py in <module>\r\n----> 1 from .bidaf import BidirectionalAttentionFlow\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\bidaf\\models\\bidaf.py in <module>\r\n      1 from keras.layers import Input, TimeDistributed, LSTM, Bidirectional\r\n      2 from keras.models import Model, load_model\r\n----> 3 from keras.optimizers import Adadelta\r\n      4 from keras.callbacks import CSVLogger, ModelCheckpoint\r\n      5 from ..layers import Highway, Similarity, C2QAttention, Q2CAttention, MergedContext, SpanBegin, SpanEnd, CombineOutputs\r\n\r\nImportError: cannot import name **'Adadelta' from 'keras.optimizers' (C:\\Users\\vishd\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\optimizers.py)**\r\n\r\n****\r\nI have been trying to recreate the Keras-bidaf model in my python notebook and running this code in python **from bidaf.models import BidirectionalAttentionFlow** which keeps giving me the above error and saying Adadelta can't be imported from Keras. I have tried so many options to solve it but no luck. \r\n\r\nI am stuck here. Any suggestions and ideas are highly appreciated. ", "comments": ["@ReetuData ,\r\nCan you please try to execute as mentioned in the [gist](https://colab.research.google.com/gist/tilakrayal/593a1ecbf14b9a17bc12a8512a727c57/untitled77.ipynb).It helps. Thanks!", "@tilakrayal, \r\n\r\nHi, Thanks for the quick reply. I have tried the code mentioned in the gist. But still getting the same issue. \r\n\r\n**import tensorflow as tf**\r\n**from tensorflow import keras**\r\n**from tensorflow.keras.optimizers import Adadelta**\r\n\r\n**ImportError: cannot import name 'Adadelta' from 'keras.optimizers' (C:\\Users\\vishd\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\optimizers.py)**\r\n\r\nAny other idea how can we solve it? ", "@ReetuData ,\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset  and tensorflow version you are using. Thanks!", "Also please take a look at this [link](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta) for Adadelta.It helps.Thanks!", "@tilakrayal  \r\n\r\nThanks for your answer again. I have tried another solution as well. No luck again. I am sending you the google collab notebook where I have run the code. I have tried both python and google collab to run this code\r\n\r\n**https://colab.research.google.com/drive/10r52gvoSdwzWH9tIFosPwjQ37poY6oLy#scrollTo=f5b95a09**\r\n\r\nI am just trying to recreate the code provided by the below model of Parikh. \r\n\r\n**https://github.com/ParikhKadam/bidaf-keras**\r\n\r\nI want to train this model for hindi/tamil datasets of the question-answering model. \r\n\r\nPlease take a look if you can find some solution. ", "@ReetuData ,\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "@tilakrayal \r\n\r\nThanks again. Just posted this issue on keras-team/keras repo. here's [link ](https://github.com/keras-team/keras/issues/15395)\r\n", "@ReetuData ,\r\nPlease free feel to close this issue as it has been tracked in Keras repo.Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52026\">No</a>\n"]}, {"number": 52025, "title": "WARNING:tensorflow:AutoGraph could not transform <function canonicalize signatures.<locals>.signature wrapper at 0x7f0a44465cb0> and will run it as-is. Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'", "body": "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fdc2c0b8710> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: No module named 'tensorflow_core.estimator'\r\n using \r\ntf-nightly-gpu\r\n\r\n", "comments": ["Hi @kusumlata123 ! ,Could you please  fill the issue template ?  it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52025\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52025\">No</a>\n"]}, {"number": 52024, "title": "Nested tensor as a feature incompatible with tf.data.Dataset.from_tensor_slices", "body": "I'm trying to create a Dataset object with the following record structure(list):\r\n```\r\n['YoVfDbnISlW0f7abNQACIg', 'RA4V8pr014UyUbDvI-LW2A', 0.0499525, 0.6, 'Framingham', 7, \r\n['Department Stores', 'Optometrists', 'Home & Garden', 'Discount Store', 'Fashion', 'Furniture Stores', \r\n'Grocery', 'Food', 'Shopping', 'Drugstores', 'Electronics', 'Health & Medical', '<PAD>', '<PAD>', \r\n'<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', \r\n'<PAD>', '<PAD>', '<PAD>', '<PAD>']]\r\n```\r\n```\r\nValueError: Can't convert Python sequence with mixed types to Tensor.\r\n```\r\nI converted to NumPy and I got the following error:\r\n```\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).\r\n```\r\nWithout having the category list, the Dataset object is created.", "comments": ["@deshiyan1010 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose), Please provide the standalone code to reproduce the issue reported here.\r\nThanks!"]}, {"number": 52023, "title": "Error in lowering tf.ResizeBilinear and tf.ResizeNearestNeighbor using tf-mlir-translate and tf-opt ", "body": "\r\nModel for tf.ResizeNearestNeighbor looks like:\r\nmodel = tf.keras.Sequential([\r\ntf.keras.layers.Conv2D(4, (1, 1),input_shape = (10, 10, 3),batch_size=1, name='fpn_c5p5'),\r\ntf.keras.layers.UpSampling2D(size=(2, 2),interpolation='nearest', name=\"fpn_p5upsampled\")\r\n])\r\n\r\nModel for tf.ResizeBilinear looks like:\r\nmodel = tf.keras.Sequential([\r\ntf.keras.layers.Conv2D(4, (1, 1),input_shape = (10, 10, 3),batch_size=1, name='fpn_c5p5'),\r\ntf.keras.layers.UpSampling2D(size=(2, 2),interpolation='nearest', name=\"fpn_p5upsampled\")\r\n])\r\nError in lowering tf.ResizeBilinear and tf.ResizeNearestNeighbor to HLO.\r\n\r\nI have attached log file and python code to get saved_model.pb\r\n[codeandlog.zip](https://github.com/tensorflow/tensorflow/files/7174773/codeandlog.zip)\r\n\r\n1.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --savedmodel-objectgraph-to-mlir --tf-savedmodel-exported-names=predict  -tf-enable-shape-inference-on-import=true $PWD -o sample.mlir\r\n2.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt -canonicalize --tf-executor-to-functional-conversion --tf-shape-inference -xla-legalize-tf  --print-ir-before-all &>1  sample.mlir\r\n\r\nIf tf.ResizeNearestNeighbor if it has argument half_pixel_centers = false I able to lower to MHLO. But in the above case half_pixel_centers = true. and i am not able to lower to HLO.", "comments": ["@ArunaKote ,\r\nCan you please let us know the tensorflow version you are using to test the code.Thanks!", "@tilakrayal ,\r\n1.tensorflow version 2.7.0-dev20210831\r\n\r\n2.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-opt --version\r\nLLVM (http://llvm.org/):\r\n  LLVM version 14.0.0git\r\n  Optimized build.\r\n  Default target: x86_64-unknown-linux-gnu\r\n  Host CPU: cascadelake\r\n3.tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --version\r\nLLVM (http://llvm.org/):\r\n  LLVM version 14.0.0git\r\n  Optimized build.\r\n  Default target: x86_64-unknown-linux-gnu\r\n  Host CPU: cascadelake\r\n", "@ArunaKote,\r\n\r\nAs you have already added `tf.ResizeBilinear` and `tf.ResizeNearestNeighbor` in the issue #52030. Can you confirm if we can close this issue as duplicate and track everything at one place in #52030 ? Thanks!", "OK .Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52023\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52023\">No</a>\n"]}, {"number": 52022, "title": "Protobuf version conflict between tensorflow and grpc", "body": "**System information**\r\n- Linux Ubuntu 20.04\r\n- TensorFlow installed from source\r\n- TensorFlow version: C 2.5.1\r\n- Python version: 3.8.0\r\n- Bazel version: 3.7.2\r\n- GCC/Compiler version: 9.3.0\r\n\r\n\r\n**Describe the problem**\r\n\r\nI want to use C-GRPC to encapsulate the Tensorflow model\uff08TensorflowC\uff09 as a service, compiling is OK. But errors will occur when running server\uff1a\r\n\r\n> [libprotobuf FATAL /mnt/d/CODE/grpc/third_party/protobuf/src/google/protobuf/stubs/common.cc:87] This program was compiled against version 3.9.2 of the Protocol Buffer runtime library, which is not compatible with the installed version (3.17.3).  Contact the program author for an update.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"bazel-out/k8-opt/bin/tensorflow/core/framework/tensor_shape.pb.cc\".)\r\n> terminate called after throwing an instance of 'google::protobuf::FatalException'\r\n>   what():  This program was compiled against version 3.9.2 of the Protocol Buffer runtime library, which is not compatible with the installed version (3.17.3).  Contact the program author for an update.  If you compiled the program yourself, make sure that your headers are from the same version of Protocol Buffers as your link-time library.  (Version verification failed in \"bazel-out/k8-opt/bin/tensorflow/core/framework/tensor_shape.pb.cc\".)\r\n> Aborted (core dumped)\r\n\r\n**Any other info / logs**\r\n\r\nSo I want to know if I can specify a version of protobuf when compiling tensorflow C lib with bazel myself.\r\n\r\nIf so, which profile should be modified and how?\r\n\r\nThank you for your guidance and help \uff1a)\r\n", "comments": ["Hi @jvishnuvardhan ,Could you please look at this issue ?", "The answer is YES", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52022\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52022\">No</a>\n"]}, {"number": 52021, "title": "[oneDNN] Fixing couple of unit tests that were failing due to missing device setting", "body": "This PR fixes 2 unit tests (node_file_writer_test & nn_fused_batchnorm_deterministic_test) which were failing because device was not set for eager op after device is identified so oneDNN optimization was not able to rewrite the ops.", "comments": []}, {"number": 52020, "title": "ValueError: Could not find matching function to call loaded from the SavedModel. ", "body": "I ma facing the issue\r\nwords_pred = words_model.predict([data_text])\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 909, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 462, in predict\r\n    steps=steps, callbacks=callbacks, **kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 444, in _model_iteration\r\n    total_epochs=1)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 73, in distributed_function\r\n    per_replica_function, args=(model, x, y, sample_weights))\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 760, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1787, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2132, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 162, in _predict_on_batch\r\n    return predict_on_batch(model, x)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 370, in predict_on_batch\r\n    return model(inputs)  # pylint: disable=not-callable\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 847, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 57, in return_outputs_and_add_losses\r\n    outputs, losses = fn(inputs, *args, **kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 111, in wrap_with_training_arg\r\n    lambda: replace_training_and_call(False))\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\", line 59, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py\", line 56, in smart_cond\r\n    return false_fn()\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 111, in <lambda>\r\n    lambda: replace_training_and_call(False))\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\", line 106, in replace_training_and_call\r\n    return wrapped_call(*args, **kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 494, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1822, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/pardeep/anaconda3/envs/coref/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\", line 262, in restored_function_body\r\n    \"\\n\\n\".join(signature_descriptions)))\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (3 total):\r\n    * Tensor(\"inputs:0\", shape=(None, 40), dtype=int32)\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (3 total):\r\n    * TensorSpec(shape=(None, 40), dtype=tf.float32, name='input_1')\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (3 total):\r\n    * TensorSpec(shape=(None, 40), dtype=tf.float32, name='inputs')\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 3:\r\n  Positional arguments (3 total):\r\n    * TensorSpec(shape=(None, 40), dtype=tf.float32, name='inputs')\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 4:\r\n  Positional arguments (3 total):\r\n    * TensorSpec(shape=(None, 40), dtype=tf.float32, name='input_1')\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nHi @mrinalsardar \r\nI was facing a similar issue. What worked for me was changing the way I was saving and loading the model as suggested by another user. Instead of model.save(), I saved the weights and then loaded the model as per guidelines in Part II-Approach 1 here: https://www.tensorflow.org/guide/keras/save_and_serialize.\r\nSeems to be working now.\r\n\r\n_Originally posted by @hepbc in https://github.com/tensorflow/tensorflow/issues/35932#issuecomment-605558870_", "comments": ["@kusumlata123 \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52020\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52020\">No</a>\n"]}, {"number": 52019, "title": "[INTEL MKL] Introduce env variable to simulate frozen weights and filters", "body": "This PR introduces an env variable which can be used when running inference on unfrozen models to simulate frozen weights and filters. By setting the variable to one, we enable weight caching in the kernel and call the most performant oneDNN kernel  implementation with pre-packed weights.  ", "comments": ["@penpornk it looks like an internal CI failed. Can you please let me know if there is anything needed to be done.", "@Srini511 Apologies for the delay! I made slight changes to the build rule. Please check if this works for you."]}, {"number": 52018, "title": "libtensorflowlite target compilation failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): from source\r\n- TensorFlow version: 2.4.0, 2.4.1., latest (tried few)\r\n- Python version: Python 3.8.10\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): bazel 3.7.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to build from source Tensorflow Lite with Select Tensorflow Operation for ARM64 architecture using Bazel build system and I get the compilation error.\r\n```\r\n/home/anastasiia/.cache/bazel/_bazel_anastasiia/db1857d8efe24e737c8a02a07ecf55c7/external/boringssl/BUILD:147:11: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command /home/anastasiia/.cache/bazel/_bazel_anastasiia/db1857d8efe24e737c8a02a07ecf55c7/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 40 argument(s) skipped)\r\nIn file included from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/bio.h:687:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'\r\n DEPRECATEDIN_1_1_0(int BIO_get_port(const char *str, unsigned short *port_ptr))\r\n\r\n```\r\nFor ARM build I used tutorial: [build_arm](https://www.tensorflow.org/lite/guide/build_arm)\r\nFor Select TF operations: [ops_select](https://www.tensorflow.org/lite/guide/ops_select)\r\n\r\nLooks like an issue is in the dependency library _openssl_.\r\nWithout Select TF ops everything is ok.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1) git clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\n2) Add the TensorFlow ops delegate library dependency to the build dependencies. In the file `tensorflow/lite/BUILD` for target `libtensorflowlite` add dependency : `tensorflow/lite/delegates/flex:delegate.`\r\n3) bazel build --config=elinux_aarch64 --config=monolithic -c opt //tensorflow/lite:libtensorflowlite.so\r\n\r\n**Any other info / logs**\r\n\r\n```\r\nERROR: /home/anastasiia/.cache/bazel/_bazel_anastasiia/db1857d8efe24e737c8a02a07ecf55c7/external/boringssl/BUILD:147:11: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command /home/anastasiia/.cache/bazel/_bazel_anastasiia/db1857d8efe24e737c8a02a07ecf55c7/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 40 argument(s) skipped)\r\nIn file included from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/bio.h:687:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'\r\n DEPRECATEDIN_1_1_0(int BIO_get_port(const char *str, unsigned short *port_ptr))\r\n ^~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/openssl/asn1.h:23,\r\n                 from /usr/include/openssl/objects.h:15,\r\n                 from /usr/include/openssl/evp.h:28,\r\n                 from /usr/include/openssl/x509.h:18,\r\n                 from /usr/include/openssl/ssl.h:20,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/bn.h:183:43: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?\r\n int BN_abs_is_word(const BIGNUM *a, const BN_ULONG w);\r\n                                           ^~~~~~~~\r\n                                           SHA_LONG\r\n/usr/include/openssl/bn.h:186:39: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?\r\n int BN_is_word(const BIGNUM *a, const BN_ULONG w);\r\n                                       ^~~~~~~~\r\n                                       SHA_LONG\r\n/usr/include/openssl/bn.h:214:22: error: 'BN_ULONG' was not declared in this scope\r\n int BN_num_bits_word(BN_ULONG l);\r\n                      ^~~~~~~~\r\n/usr/include/openssl/bn.h:214:22: note: suggested alternative: 'SHA_LONG'\r\n int BN_num_bits_word(BN_ULONG l);\r\n                      ^~~~~~~~\r\n                      SHA_LONG\r\n/usr/include/openssl/bn.h:266:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?\r\n BN_ULONG BN_mod_word(const BIGNUM *a, BN_ULONG w);\r\n ^~~~~~~~\r\n SHA_LONG\r\n/usr/include/openssl/bn.h:267:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?\r\n BN_ULONG BN_div_word(BIGNUM *a, BN_ULONG w);\r\n ^~~~~~~~\r\n SHA_LONG\r\n/usr/include/openssl/bn.h:268:28: error: 'BN_ULONG' has not been declared\r\n int BN_mul_word(BIGNUM *a, BN_ULONG w);\r\n                            ^~~~~~~~\r\n/usr/include/openssl/bn.h:269:28: error: 'BN_ULONG' has not been declared\r\n int BN_add_word(BIGNUM *a, BN_ULONG w);\r\n                            ^~~~~~~~\r\n/usr/include/openssl/bn.h:270:28: error: 'BN_ULONG' has not been declared\r\n int BN_sub_word(BIGNUM *a, BN_ULONG w);\r\n                            ^~~~~~~~\r\n/usr/include/openssl/bn.h:271:28: error: 'BN_ULONG' has not been declared\r\n int BN_set_word(BIGNUM *a, BN_ULONG w);\r\n                            ^~~~~~~~\r\n/usr/include/openssl/bn.h:272:1: error: 'BN_ULONG' does not name a type; did you mean 'SHA_LONG'?\r\n BN_ULONG BN_get_word(const BIGNUM *a);\r\n ^~~~~~~~\r\n SHA_LONG\r\n/usr/include/openssl/bn.h:288:37: error: 'BN_ULONG' has not been declared\r\n int BN_mod_exp_mont_word(BIGNUM *r, BN_ULONG a, const BIGNUM *p,\r\n                                     ^~~~~~~~\r\n/usr/include/openssl/bn.h:323:24: error: variable or field 'BN_consttime_swap' declared void\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                        ^~~~~~~~\r\n/usr/include/openssl/bn.h:323:24: error: 'BN_ULONG' was not declared in this scope\r\n/usr/include/openssl/bn.h:323:24: note: suggested alternative: 'SHA_LONG'\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                        ^~~~~~~~\r\n                        SHA_LONG\r\n/usr/include/openssl/bn.h:323:46: error: expected primary-expression before '*' token\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                                              ^\r\n/usr/include/openssl/bn.h:323:47: error: 'a' was not declared in this scope\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                                               ^\r\n/usr/include/openssl/bn.h:323:57: error: expected primary-expression before '*' token\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                                                         ^\r\n/usr/include/openssl/bn.h:323:58: error: 'b' was not declared in this scope\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                                                          ^\r\n/usr/include/openssl/bn.h:323:61: error: expected primary-expression before 'int'\r\n void BN_consttime_swap(BN_ULONG swap, BIGNUM *a, BIGNUM *b, int nwords);\r\n                                                             ^~~\r\n/usr/include/openssl/bn.h:332:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_0_9_8'\r\n DEPRECATEDIN_0_9_8(int\r\n ^~~~~~~~~~~~~~~~~~\r\n/usr/include/openssl/bn.h:403:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_0_9_8'\r\n DEPRECATEDIN_0_9_8(int BN_get_params(int which)) /* 0, mul, 1 high, 2 low, 3\r\n ^~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/openssl/objects.h:15,\r\n                 from /usr/include/openssl/evp.h:28,\r\n                 from /usr/include/openssl/x509.h:18,\r\n                 from /usr/include/openssl/ssl.h:20,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/asn1.h:555:7: error: expected constructor, destructor, or type conversion before 'unsigned'\r\n const unsigned char *ASN1_STRING_get0_data(const ASN1_STRING *x);\r\n       ^~~~~~~~\r\nIn file included from /usr/include/openssl/x509.h:22,\r\n                 from /usr/include/openssl/ssl.h:20,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ec.h:274:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_2_0'\r\n DEPRECATEDIN_1_2_0(int EC_GROUP_get_curve_GFp(const EC_GROUP *group, BIGNUM *p,\r\n ^~~~~~~~~~~~~~~~~~\r\n/usr/include/openssl/ec.h:543:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_2_0'\r\n DEPRECATEDIN_1_2_0(int EC_POINT_get_affine_coordinates_GFp(const EC_GROUP *group,\r\n ^~~~~~~~~~~~~~~~~~\r\n/usr/include/openssl/ec.h:631:1: error: expected constructor, destructor, or type conversion before 'size_t'\r\n size_t EC_POINT_point2oct(const EC_GROUP *group, const EC_POINT *p,\r\n ^~~~~~\r\nIn file included from /usr/include/openssl/x509.h:25,\r\n                 from /usr/include/openssl/ssl.h:20,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/rsa.h:240:1: error: expected constructor, destructor, or type conversion before 'int'\r\n int RSA_generate_key_ex(RSA *rsa, int bits, BIGNUM *e, BN_GENCB *cb);\r\n ^~~\r\nIn file included from /usr/include/openssl/dsa.h:25,\r\n                 from /usr/include/openssl/x509.h:26,\r\n                 from /usr/include/openssl/ssl.h:20,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/dh.h:142:1: error: expected constructor, destructor, or type conversion before 'int'\r\n int DH_generate_parameters_ex(DH *dh, int prime_len, int generator,\r\n ^~~\r\nIn file included from /usr/include/openssl/x509.h:26,\r\n                 from /usr/include/openssl/ssl.h:20,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/dsa.h:103:1: error: expected constructor, destructor, or type conversion before 'int'\r\n int DSA_sign(int type, const unsigned char *dgst, int dlen,\r\n ^~~\r\n/usr/include/openssl/dsa.h:127:1: error: expected constructor, destructor, or type conversion before 'int'\r\n int DSA_generate_parameters_ex(DSA *dsa, int bits,\r\n ^~~\r\nIn file included from /usr/include/openssl/ssl.h:20,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/x509.h:728:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'\r\n DEPRECATEDIN_1_1_0(ASN1_TIME *X509_CRL_get_nextUpdate(X509_CRL *crl))\r\n ^~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/openssl/ssl.h:26,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/hmac.h:33:12: error: expected constructor, destructor, or type conversion before 'int'\r\n /*__owur*/ int HMAC_Init_ex(HMAC_CTX *ctx, const void *key, int len,\r\n            ^~~\r\nIn file included from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ssl.h:991:1: error: expected constructor, destructor, or type conversion before 'typedef'\r\n typedef enum {\r\n ^~~~~~~\r\n/usr/include/openssl/ssl.h:1042:3: error: 'OSSL_HANDSHAKE_STATE' does not name a type; did you mean 'SSL_CB_HANDSHAKE_START'?\r\n } OSSL_HANDSHAKE_STATE;\r\n   ^~~~~~~~~~~~~~~~~~~~\r\n   SSL_CB_HANDSHAKE_START\r\n/usr/include/openssl/ssl.h:1878:1: error: expected constructor, destructor, or type conversion before 'DEPRECATEDIN_1_1_0'\r\n DEPRECATEDIN_1_1_0(__owur const SSL_METHOD *TLSv1_server_method(void))\r\n ^~~~~~~~~~~~~~~~~~\r\n/usr/include/openssl/ssl.h:1997:8: error: 'OSSL_HANDSHAKE_STATE' does not name a type; did you mean 'SSL_CB_HANDSHAKE_START'?\r\n __owur OSSL_HANDSHAKE_STATE SSL_get_state(const SSL *ssl);\r\n        ^~~~~~~~~~~~~~~~~~~~\r\n        SSL_CB_HANDSHAKE_START\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'SSL* get_ssl(BIO*)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:16:37: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n   return reinterpret_cast<SSL *>(bio->ptr);\r\n                                     ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/bio.h:20,\r\n                 from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_read(BIO*, char*, int)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:40:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->retry_reason = BIO_RR_ACCEPT;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/bio.h:20,\r\n                 from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:45:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->retry_reason = BIO_RR_CONNECT;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/bio.h:20,\r\n                 from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_write(BIO*, const char*, int)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:80:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->retry_reason = BIO_RR_CONNECT;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/bio.h:20,\r\n                 from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'long int ssl_ctrl(BIO*, int, long int, void*)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:101:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->shutdown = num;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/bio.h:20,\r\n                 from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:102:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->ptr = ptr;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/bio.h:20,\r\n                 from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:103:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->init = 1;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/bio.h:20,\r\n                 from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:107:17: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       return bio->shutdown;\r\n                 ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/bio.h:20,\r\n                 from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:110:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->shutdown = num;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/bio.h:20,\r\n                 from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_free(BIO*)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:148:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n   if (bio->shutdown) {\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/bio.h:20,\r\n                 from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc: At global scope:\r\nexternal/boringssl/src/ssl/bio_ssl.cc:170:25: error: variable 'const BIO_METHOD ssl_method' has initializer but incomplete type\r\n static const BIO_METHOD ssl_method = {\r\n                         ^~~~~~~~~~\r\nIn file included from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\nexternal/boringssl/src/ssl/bio_ssl.cc:177:6: error: expected identifier before numeric constant\r\n long BIO_set_ssl(BIO *bio, SSL *ssl, int take_owership) {\r\n      ^~~~~~~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:177:6: error: expected ',' or '...' before numeric constant\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'long int BIO_ctrl(BIO*, int)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:178:39: error: 'take_owership' was not declared in this scope\r\n   return BIO_ctrl(bio, BIO_C_SET_SSL, take_owership, ssl);\r\n                                       ^~~~~~~~~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:178:54: error: 'ssl' was not declared in this scope\r\n   return BIO_ctrl(bio, BIO_C_SET_SSL, take_owership, ssl);\r\n                                                      ^~~\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 368.325s, Critical Path: 108.73s\r\nINFO: 1656 processes: 241 internal, 1415 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\nThanks", "comments": ["@AnastGerus \r\nplease refer to issue [with same](https://github.com/tensorflow/tensorflow/issues/48401) error and let us know.\r\nIssues for reference:\r\n[link](https://github.com/tensorflow/tensorflow/issues/25607), [link1](https://stackoverflow.com/questions/51352364/tensorflow-bazel-building-error-c-compilation-of-rule-boringssl-crypto-f)", "Hi @Saduf2019 ,\r\nyes, the proposed [issue](https://github.com/tensorflow/tensorflow/issues/48401) helped a lot. \r\nSorry, that I didn't manage to find it myself. Thanks a lot for your help!\r\n\r\nNote for those who looking for an answer:\r\nYou need to hide the system's _openssl_ library. For this go to `usr/include ` folder and make `sudo mv openssl openssl.original`.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52018\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52018\">No</a>\n"]}, {"number": 52017, "title": "Added shape inference for TFL to Tosa Lowering", "body": "Some TFLite operations need the rank/shape of operands to lower to the\r\nappropriate TOSA operations. By performing shape inference during the\r\nlowering, underspecified IR can be lowered without TFLite shapes known during\r\ncompilation.\r\n\r\nNote this splits lowering from quantized type removal as the Dialect Conversion\r\nframework does not agree with multiple sources of types.", "comments": ["It would seem you get all of what CreateAndInfer does if you just used effectively https://github.com/llvm/llvm-project/blob/24c8eaec9467b2aaf70b0db33a4e4dd415139a50/mlir/include/mlir/Interfaces/InferTypeOpInterface.td#L136. ", "> It would seem you get all of what CreateAndInfer does if you just used effectively [llvm/llvm-project@`24c8eae`/mlir/include/mlir/Interfaces/InferTypeOpInterface.td#L136](https://github.com/llvm/llvm-project/blob/24c8eaec9467b2aaf70b0db33a4e4dd415139a50/mlir/include/mlir/Interfaces/InferTypeOpInterface.td#L136).\r\n\r\nSo I am not entirely certain that is true. This works for the majority of operations however tosa.rescale depends on the result element type being specified directly by the constructor.\r\n\r\nOverall it would be nice to have an interface that supports this, however I don't think the current ones do. E.g. all the existing utilities are member functions as well, requiring a constructed op to reify the shapes.", "> > It would seem you get all of what CreateAndInfer does if you just used effectively [llvm/llvm-project@`24c8eae`/mlir/include/mlir/Interfaces/InferTypeOpInterface.td#L136](https://github.com/llvm/llvm-project/blob/24c8eaec9467b2aaf70b0db33a4e4dd415139a50/mlir/include/mlir/Interfaces/InferTypeOpInterface.td#L136).\r\n> \r\n> So I am not entirely certain that is true. This works for the majority of operations however tosa.rescale depends on the result element type being specified directly by the constructor.\r\n\r\nI see, so rescale can change the result element type and the target element type is not an attribute?\r\n\r\n> \r\n> Overall it would be nice to have an interface that supports this, however I don't think the current ones do. E.g. all the existing utilities are member functions as well, requiring a constructed op to reify the shapes.\r\n\r\nYou are doing construction time inference, so the producers are all created before a new op is. But you are correct that if you want to reify before creating an op, then you can't. But that feels weird to me to do: I see this as straddling line between tensor world and buffer world and should be a dedicated pass to go from one to the other.", "> orts this, however I don't think the current ones do. E.g. all the existing utilities are member functions as well, requiring a constructed op to reify the shapes.\r\n\r\nOverall I think the right choice is to affix the output element type as a type attr for tosa.rescale. It actually avoids some other issues as well (u8 needing to persist as a type until tosa-to-linalg).\r\n\r\nIt may be worth raising with the ARM folks.", "Everything should be cleaned up including some changes due to moving the quantization stripping to an optional pass. Verified manually but there may be integration issues needed internally."]}, {"number": 52016, "title": "[INTEL MKL] Enabling simple heuristic based tuning for innerproduct primitive", "body": "This PR enables a simple heuristic based approach to determine whether inner product primitive should run single threaded or not. If the matmul fits in L2 we have observed that single threaded operation gives better performance than multi-threaded. ", "comments": []}, {"number": 52015, "title": "DGN Tokens", "body": null, "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52015) for more info**.\n\n<!-- need_sender_cla -->", "@WhiteFeather27  Can you please sign CLA. Thanks!"]}, {"number": 52013, "title": "Cannot load saved tf model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 33\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.8\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`:\r\n`v2.6.0-rc2-32-g919f693420e 2.6.0`\r\n\r\n**Describe the current behavior**\r\n`tf.saved_model.load(model_path)` fails with the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"convert_to_onnx.py\", line 3, in <module>\r\n    tf.saved_model.load('/home/fmurdaca/work/aicoe/elyra-aidevsecops-tutorial/models/210915165333-7b04047d1220f5cd')\r\n  File \"/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 864, in load\r\n    result = load_internal(export_dir, tags, options)[\"root\"]\r\n  File \"/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 902, in load_internal\r\n    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\r\n  File \"/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 162, in __init__\r\n    self._load_all()\r\n  File \"/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 259, in _load_all\r\n    self._load_nodes()\r\n  File \"/home/fmurdaca/.local/share/virtualenvs/convert-onnx-KeTiB-gU/lib64/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 448, in _load_nodes\r\n    slot_variable = optimizer_object.add_slot(\r\nAttributeError: '_UserObject' object has no attribute 'add_slot'\r\n```\r\n\r\nThis also breaks tensorflow-onnx package: https://github.com/onnx/tensorflow-onnx/issues/1715\r\n\r\n**Describe the expected behavior**\r\nmethod is able to load model, so that other actions can be performed.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing): adjust attribute provided by that object\r\n\r\n**Standalone code to reproduce the issue**\r\n1. clone https://github.com/AICoE/elyra-aidevsecops-tutorial\r\n2. pip install tensorflow\r\n3. run python script with import tensoflow as tf and `tf.saved_model.load('./models/210124112759-d97fd1f46b13ee40')`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @pacospace ! Please use following code to load your model. \r\n`tf.keras.models.load_model(\"./elyra-aidevsecops-tutorial/models/210124112759-d97fd1f46b13ee40\")`", "Hi!\r\nI have the very same issue with TensorFlow 2.6\r\n\r\nI'm loading our upsample model and saving it as a saved model:\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow_addons.layers import InstanceNormalization, SpectralNormalization\r\n\r\nmodel = load_model('upsample---[_20]---[______3171]---[_____63420].h5', custom_objects={'InstanceNormalization': InstanceNormalization, 'SpectralNormalization': SpectralNormalization})\r\n\r\ntf.saved_model.save(model, 'upsample_saved_model')\r\n```\r\nThen, when I'm trying to load it:\r\n```py\r\nimport tensorflow as tf\r\n\r\nmodel = tf.saved_model.load('upsample_saved_model')\r\n```\r\nIt errors with:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/daniel/trt/2_find_layer_names.py\", line 4, in <module>\r\n    model = tf.saved_model.load('upsample_saved_model')\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\", line 864, in load\r\n    result = load_internal(export_dir, tags, options)[\"root\"]\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\", line 902, in load_internal\r\n    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\", line 162, in __init__\r\n    self._load_all()\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\", line 259, in _load_all\r\n    self._load_nodes()\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\", line 448, in _load_nodes\r\n    slot_variable = optimizer_object.add_slot(\r\nAttributeError: '_UserObject' object has no attribute 'add_slot'\r\n```\r\n\r\nThe same code works just fine in TensorFlow 2.5.\r\n\r\nIf you need the model to test: https://github.com/Sentdex/GANTheftAuto/tree/main/trained_models (it's the upsample one).\r\n\r\n\r\nRegards,\r\nDaniel", "Hi @daniel-kukiela ,Could not replicate above issue in TF 2.6 after putting complete path of model file as below.  \r\n`model = load_model('/content/GANTheftAuto/trained_models/upsample---[_20]---[______3171]---[_____63420].h5', custom_objects={'InstanceNormalization': InstanceNormalization, 'SpectralNormalization': SpectralNormalization})`\r\n\r\n", "Hi @pacospace ,Feel free to close this issue if it helped.", "> Hi @pacospace ,Feel free to close this issue if it helped.\r\n\r\nThanks @mohantym, your suggestion worked! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52013\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52013\">No</a>\n", "@mohantym I have no such path. Where did you get it from? I have a script and a folder with a model in the same folder, it's nothing like what you proposed. I'm not sure what your suggestion is supposed to do. Can you elaborate?\r\n\r\nAlso, it works the presented way in TF 2.5 and there's no reason it should not work in TF 2.6. It's like I have TF 2.6 and it does not work, then I'm downgrading to TF 2.5 and the same scripts work as expected.\r\n\r\nI still call it a bug and it should not be closed IMO.\r\n", "Oh wait @mohantym - in your reply to me, you mentioned the wrong line of code. Loading a model and saving it as a saved model works (the upper script). Loading the saved model is what does not (the lower script).", "Hi @pacospace ,\r\nCan I expect anything still here to happen (I can patiently wait), or do I have to open another issue?", "> Hi @pacospace ,\r\n> Can I expect anything still here to happen (I can patiently wait), or do I have to open another issue?\r\n\r\nHey @daniel-kukiela, from the initial issue/question, I would consider this solved, because using `tf.keras.models.load_model(\"./elyra-aidevsecops-tutorial/models/210124112759-d97fd1f46b13ee40\")` solved my problem. If you want specifically Tensorflow team to help with the issue when you use `tf.saved_model.load` I can reopen it, wdyt @ \r\nmohantym? Or you can open a new one as well @daniel-kukiela! wdyt?\r\n", "Hi @daniel-kukiela , Sorry for the late response! Actually I checked in Colab earlier.Tried to replicate and resolve the issue. Providing [GIST ](https://colab.research.google.com/gist/mohantym/187d7a2dc387998237211d7c590c6b1a/github_52013_sentdex.ipynb)for reference .For Further help! Please open a new issue. Thanks!", "@mohantym \r\nYou are running everything in a single runtime.\r\nAfter you downloaded/installed everything and converted a model to a saved model, please use Runtime -> Restart runtime and then try to load saved model (in a new runtime).\r\nI'm going to post it as a separate issue.\r\n\r\nThank you"]}, {"number": 52012, "title": "TF-TRT C++ conversion", "body": "TF-TRT C++ interface to convert models. \r\n\r\nDifferences compared to the Python API:\r\n- It is assumed that the input graph is frozen. This assumption will be removed in a follow up PR.\r\n- `convert_to_static_engine` conversion param allows to convert dynamic engines to static engines.\r\n- Currently we do not provide a way to save the engine files as assets. \r\n\r\nThe steps for model conversion by `ConvertAndBuild`\r\n1. Inline functions\r\n2. Freeze the graph - omitted since we assume frozen input graph\r\n3. Run Grappler with TRTOptimizer pass\r\n4. Infer the graph to provide shape information (only in dynamic shape mode).\r\n5. Infer the graph to build the engines\r\n6. Convert the graph_def to have static engines\r\n\r\n(On the Python side, steps 4-5 are done by a separate `build` function.)\r\n\r\nRelated PRs:\r\n- #52047 \r\n- Example code for the tensorflow/tensorrt repository https://github.com/tensorflow/tensorrt/pull/271\r\n", "comments": ["We can avoid the change to the utilities for freezing the graph by updating out SavedModelBundle with the converted GraphDef. Something like this:\r\n```\r\nMetaGraphDef* meta_graph_def = &saved_model_bundle->meta_graph_def;\u00a0\r\n*meta_graph_def->mutable_graph_def() = graph_def;\r\n```", "@bixia1 ready for review.", "@bixia1 I have removed the circular dependency, and added trt_convert_api as a dependency of trt_op_libs. Let me know if there are other issues that we want to address in this PR.", "Can you fix the PR description, such as to remove the information that is obsolete? The PR description will become part of the commit message."]}, {"number": 52011, "title": "C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n\r\n\r\n**Describe the problem**\r\nI followed instructions in https://www.tensorflow.org/install/source until building the pip package, when I got this error\r\n```\r\nERROR: C:/users/administrator/desktop/tensorflow/tensorflow/core/BUILD:1627:16: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/users/administrator/_bazel_administrator/56cwyie6/execroot/org_tensorflow\r\n````\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --local_ram_resources=8000 --config=opt  /tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nHere's the full log:\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=211\r\nINFO: Reading rc options for 'build' from c:\\users\\administrator\\desktop\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/Administrator/anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\administrator\\desktop\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from c:\\users\\administrator\\desktop\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Administrator/anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Administrator/anaconda3/lib/site-packages --python_path=C:/Users/Administrator/anaconda3/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\administrator\\desktop\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\administrator\\desktop\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file c:\\users\\administrator\\desktop\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --host_copt=/arch:AVX2\r\nINFO: Found applicable config definition build:windows in file c:\\users\\administrator\\desktop\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --features=compiler_param_file --distinct_host_configuration=false --config=no_tfrt\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\administrator\\desktop\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:no_tfrt in file c:\\users\\administrator\\desktop\\tensorflow\\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/users/administrator/desktop/tensorflow/tensorflow/core/BUILD:1627:16: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/users/administrator/_bazel_administrator/56cwyie6/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Administrator/anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Administrator/anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/BuildTools/VC/Tools/MSVC/14.29.30133/bin/HostX64/x64/cl.exe @bazel-out/x64_windows-opt/bin/tensorflow/core/_objs/framework_internal_impl/tensor_util.obj.params\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\ncl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\ntensorflow/core/framework/tensor_util.cc(284): error C2668: 'signbit': ambiguous call to overloaded function\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\\corecrt_math.h(303): note: could be 'bool signbit(float) throw()'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\\corecrt_math.h(308): note: or       'bool signbit(double) throw()'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\\corecrt_math.h(313): note: or       'bool signbit(long double) throw()'\r\ntensorflow/core/framework/tensor_util.cc(284): note: while trying to match the argument list '(T)'\r\n        with\r\n        [\r\n            T=tensorflow::EnumToDataType<tensorflow::DT_UINT8>::Type\r\n        ]\r\ntensorflow/core/framework/tensor_util.cc(329): note: see reference to function template instantiation 'bool tensorflow::tensor::internal::IsNegativeZero<T>(T)' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::EnumToDataType<tensorflow::DT_UINT8>::Type\r\n        ]\r\ntensorflow/core/framework/tensor_util.cc(372): note: see reference to function template instantiation 'bool tensorflow::tensor::internal::CompressRepeatedField<T>(float,const tensorflow::TensorShape &,tensorflow::TensorProto *)' being compiled\r\n        with\r\n        [\r\n            T=tensorflow::EnumToDataType<tensorflow::DT_UINT8>::Type\r\n        ]\r\ntensorflow/core/framework/tensor_util.cc(396): note: see reference to function template instantiation 'bool tensorflow::tensor::internal::CompressTensorProtoInPlaceImpl<tensorflow::EnumToDataType<tensorflow::DT_UINT8>::Type>(int64_t,float,tensorflow::TensorProto *)' being compiled\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 66.282s, Critical Path: 18.54s\r\nINFO: 27 processes: 8 internal, 19 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["@Harrelix Could you please refer to similar [issue1](https://github.com/tensorflow/tensorflow/issues/8973), [issue2](https://github.com/tensorflow/tensorflow/issues/29973) and [issue3](https://github.com/tensorflow/tensorflow/issues/31944) ,Please let us know if it helps?Thanks!", "Those don't seem to be of help. I also tried removing everything and starting from scratch but to no avail (same error).", "@Harrelix,\r\n\r\nAs you mentioned the OS as `Windows`, Can you clarify if you're following this [guide](https://www.tensorflow.org/install/source_windows) to install TF from source? \r\n\r\nFrom the issue description, I can see that you've mentioned that you followed this [link](https://www.tensorflow.org/install/source) which actually belongs to `Linux/MacOs`.", "Yeah my bad, I followed the windows guide", "@Harrelix,\r\n\r\nCan you take a look at this similar [issue](https://github.com/tensorflow/tensorflow/issues/36576) and let us know if it helps? Thanks! ", "No it didn't help...\r\nI gave up and installed the pre-built version at [here](https://github.com/fo40225/tensorflow-windows-wheel) instead", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52011\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52011\">No</a>\n"]}, {"number": 52010, "title": "tf.keras   TypeError: unsupported operand type(s) for +: 'NoneType' and 'float", "body": "<em>Please make sure that this is an issue related to keras.\r\ntag:keras_template</em>\r\n\r\n**Important Notice**\r\n\r\nPlease note that `tf.keras` code was moved entirely to\r\n[keras-team/keras](https://github.com/keras-team/keras) repository\r\n\r\nYou can open any code/doc bugs, performance issues, and feature requests\r\n in [keras-team/keras](https://github.com/keras-team/keras/issues) repository\r\n\r\n`tf.keras` related issues opened in\r\n[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) repository may\r\nnot get attention as [keras-team/keras](https://github.com/keras-team/keras)\r\nrepository is dedicated for the development of `keras` code\r\n", "comments": ["@Winndy ,\r\nPlease take a look at this issues with similar error.[1](https://github.com/tensorflow/tensorflow/issues/20867#issuecomment-422246723) and [2](https://github.com/AppliedDataSciencePartners/DeepReinforcementLearning/issues/12). It helps.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]