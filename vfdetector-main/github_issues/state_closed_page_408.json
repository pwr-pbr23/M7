[{"number": 41681, "title": "BroadcastTo", "body": "MacBook Pro\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, DIV, FULLY_CONNECTED, HARD_SWISH, MAXIMUM, MEAN, MINIMUM, MUL, PACK, RESHAPE, SHAPE, SOFTMAX, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: BroadcastTo.\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/z004njq/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/z004njq/Library/Python/3.7/lib/python/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/z004njq/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, DIV, FULLY_CONNECTED, HARD_SWISH, MAXIMUM, MEAN, MINIMUM, MUL, PACK, RESHAPE, SHAPE, SOFTMAX, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: BroadcastTo.```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["Could you try the conversion with the tomorrow's nightly build or the version of July 21?", "Sorry. We have an issue with delivering a broadcast to op legalization in the nightly version.", "Sure, I will give it a try. I will let you know how it goes.\n\nOn Thu, Jul 23, 2020 at 16:37 Jae sung Chung <notifications@github.com>\nwrote:\n\n> Could you try the conversion with the tomorrow's nightly build or the\n> version of July 21?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41681#issuecomment-663283364>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABZ4UC3C2UBO7QPIBHRPJOTR5DCR3ANCNFSM4PGGBBOQ>\n> .\n>\n-- \nDipendra\n", "If you are using tf 2.3 version, you can broadcast_to op without problems. However, you need to use the MLIR converter by specifying converter.experimental_new_converter=True.", "Hi, actually I removed the BroadcastTo operator from my graph since it was only being used in rescaling input in preprocessing. Could you please tell me if the inputs will be rescaled before feeding into the TFLite model or I have to include rescale input as a part of graph?\r\n\r\nAlso, the input of the tf-lite model has type int32 even if I am using input_inference_type=QUANTIZED_UINT8. This causes mismatch: Cannot convert between a TensorFlowLite buffer with 602112 bytes and a Java Buffer with 150528 bytes. Any suggestion on how to fix this? I tried using tf-lite command tool as well as the python script. Weirdly enough there is no tflite-convertor in python after I build Tensorflow Lite, so I am using tflite_convertor.py.", "This is the command I am using:\r\n\r\npython3 tensorflow/lite/python/tflite_convert.py --graph_def_file=~/Projects/save_models/mobilenetv4-1009/saved_model.pb --output_file=mobilenetv4-1009.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_shape=\"1,224,224,3\" --input_array=\"input_1\" --output_array=\"probs\" --mean_value=\"123.675,116.28,103.53\" --std_dev_value=\"58.395,57.12,57.375\" --saved_model_dir=/Users/z004njq/Projects/save_models/mobilenetv4-1009 --experimental_new_converter=true\r\n\r\nThis is the interpreter result for input details:\r\n\r\n[{'name': 'input_1', 'index': 0, 'shape': array([  1, 224, 224,   3], dtype=int32), 'shape_signature': array([ -1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n[[0.00023714 0.00018118 0.00022986 ... 0.00086142 0.0026318  0.00020374]]\r\n\r\n", "Hi, I need to use BroadCastTo operator. I installed tensorflow==2.3.0rc2, but still it does not support the operator. Any other suggestion?", "Could you try the conversion with tf-nightly version? Please share the full log of the conversion failure and provide the minimal steps for reproducing your case.", "Hi, I tried using the following command and looks like it is working. The accuracy is very low, what is the suggested mean and std to use? I have a normalization node for preprocessing in the graph. Command I used:\r\n\r\npython3 tensorflow/lite/python/tflite_convert.py --output_file=model.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_shape=\"1,224,224,3\" --input_array=\"serving_default_input_1\" --output_array=\"StatefulPartitionedCall\" --mean_value=0 --std_dev_value=1 --saved_model_dir=/Users/z004njq/Projects/save_models/mobilenetv4-exp-1004 --enable_v1_converter --experimental_new_converter=True\r\n", "Hi,\r\nThank you, I have not checked out the new version. I am currently working on some other project. I will let you know if I run into same issue in the future. ", "Closing this issue since the BroadcastTo op is supported under the TFLite builtin op set in the recent TF version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41681\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41681\">No</a>\n"]}, {"number": 41680, "title": "Added missing docstrings in dataset_utils.py module", "body": "This PR adds the missing docstrings to the following functions in `tensorflow/python/keras/preprocessing/dataset_utils.py` module:\r\n\r\n1. labels_to_dataset\r\n2. check_validation_split_arg", "comments": ["cc: @fchollet  @mihaimaruseac ", "cc:  @gbaned ."]}, {"number": 41679, "title": "bug in Using the SavedModel format Guide", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\nUbuntu 18.04, system with two GPUs, tensorflow 2.2\r\n\r\n### Describe the problem\r\n\r\nThe guide, near the top, includes the following:\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nif physical_devices:\r\n  tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\nand on a multi-GPU system, this will lead to problems later because memory growth will only be managed on one of the GPUs.  It should instead be:\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor device in physical_devices:\r\n  tf.config.experimental.set_memory_growth(device, True)\r\n\r\nI am aware that this is really a minor documentation issue, and would prefer to simply upload the fix myself.  Unfortunately, I am not willing to sign the Contributor License Agreement.  I have many friends at Google with whom I have many technical discussions.  I don't want those discussions to automatically grant licenses to Google if I forget to say \"not a contribution\" at the beginning of them.\r\n### Source code / logs\r\n\r\n", "comments": ["@gowthamkpr  Can you help me locate this piece of code.\r\nI have tried to find this but was unable to locate may be you can help me. ", "The page is at https://www.tensorflow.org/guide/saved_model and I've\nattached it as an ipynb file.  The lines in question are 128-130\n\nIn fact, this whole page is sort of terrible.  So on line 115, tmpdir is\nset to tempfile.mkdtemp() and then the model is saved to\nmobilenet_save_path, which is defined to be tmpdir/mobilenet on line 343.\nBut then on lines 471-475, the model server is invoked on /tmp/mobilenet,\nwhich is not where the model actually is on my system.  I fixed that as\nwell, but then the tensorflow_model_server call actually fails with a fatal\nerror (something about not being able to open the connection to port 8501\nwritten to the server log; let me know if you need me to go dig it up).  At\nthat point, I gave up.\n\nThanks for taking a look!\n\nMatt Ginsberg\n\n\nOn Fri, Jul 24, 2020 at 2:15 AM Aavishkar Mishra <notifications@github.com>\nwrote:\n\n> @gowthamkpr <https://github.com/gowthamkpr> Can you help me locate this\n> piece of code.\n> I have tried to find this but was unable to locate may be you can help me.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41679#issuecomment-663437186>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFKAGHKEKX27RLTNTQ6PV3DR5FGJHANCNFSM4PGFIGAQ>\n> .\n>\n", "Hi, may I submit a PR to fix this? @aavishkarmishra \r\nSorry, I didn't see the attached .ipynb file, could you please remind me where it locates? @mattginsberg", "Yes, certainly submit a PR.  I can't submit a PR because I'm not willing to\nsign the thing that says I have to curtail my conversations with all my\nfriends at Google!\n\nThe .ipynb file is what you get if you click \"Download notebook\" at\nhttps://www.tensorflow.org/guide/saved_model\n\nHopefully that is what you need -- thanks!\n\nMatt\n\nOn Fri, Jul 24, 2020 at 11:11 AM Yiwen Li <notifications@github.com> wrote:\n\n> Hi, may I submit a PR to fix this? @aavishkarmishra\n> <https://github.com/aavishkarmishra>\n> Sorry, I didn't see the attached .ipynb file, could you please remind me\n> where it locates?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41679#issuecomment-663666942>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFKAGHLX2AVS4IRF4I4TIZTR5HFG5ANCNFSM4PGFIGAQ>\n> .\n>\n", "Thank you so much Matt, fixing it now. @mattginsberg ", "Just tried out the notebook. There is no issue for me in the original notebook. Maybe that's a very specific memory issue for you?\r\nAnd both\r\n`physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nif physical_devices:\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)`\r\nand \r\n`physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor device in physical_devices:\r\ntf.config.experimental.set_memory_growth(device, True)`\r\nworks for me. @mattginsberg ", "Yes.  It will fail on a machine with multiple GPUs because only the first\ngets its memory growth managed.  This will eventually cause a problem\n(\"Memory growth cannot differ between GPU devices\") at\n\npretrained_model = tf.keras.applications.MobileNet()\n\nlater on the page.\n\nMatt\n\n\nOn Fri, Jul 24, 2020 at 11:53 AM Yiwen Li <notifications@github.com> wrote:\n\n> Just tried out the notebook. There is no issue for me in the original\n> notebook. Maybe that's a very specific memory issue for you?\n> And both\n> physical_devices = tf.config.experimental.list_physical_devices('GPU') if\n> physical_devices:\n> tf.config.experimental.set_memory_growth(physical_devices[0], True)\n> and\n> physical_devices = tf.config.experimental.list_physical_devices('GPU') for\n> device in physical_devices:\n> tf.config.experimental.set_memory_growth(device, True)\n> works for me. @mattginsberg <https://github.com/mattginsberg>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41679#issuecomment-663683649>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFKAGHKQNXGFJ2SXKQKB2FDR5HKELANCNFSM4PGFIGAQ>\n> .\n>\n", "@MarkDaoust will you please review this PR", "I am closing this as this was resolved. Please check the above commit. Thanks!"]}, {"number": 41678, "title": "Arg_def_case invariant fix", "body": "@mihaimaruseac ", "comments": []}, {"number": 41677, "title": "Fix incorrect mapping in fast tensor <=> np mapping", "body": "This PR fixes the incorrect mapping in fast tensor <=> np mapping\r\nSee diff below:\r\n```diff\r\n       dtypes.qint16.as_numpy_dtype:\r\n-          fast_tensor_util.AppendInt8ArrayToTensorProto,\r\n+          fast_tensor_util.AppendInt16ArrayToTensorProto,\r\n       dtypes.quint16.as_numpy_dtype:\r\n-          fast_tensor_util.AppendUInt8ArrayToTensorProto,\r\n+          fast_tensor_util.AppendUInt16ArrayToTensorProto,\r\n```\r\n\r\nThis PR is part of the effort to pass PR #41421's internal test.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 41676, "title": "Add Transactional API python bindings", "body": "This PR adds extra arguments with defaults to python bindings of FileSystem. Tokens will be forwarded to filesystems in a subsequent PR when #41615 is merged. Most line changes are due to clang-format. Main edits are addition of a new argument to methods and default arguments to pybind.", "comments": ["This still needs to wait for #41615 replacement to be fully done right?", "No. Since this just modifies the function signatures and don't forward it\nto Filesystem yet. A follow up PR will forward the token to C++ layer when\nit is ready.\n\nOn Wed, Jul 29, 2020, 10:58 AM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> This still needs to wait for #41615\n> <https://github.com/tensorflow/tensorflow/pull/41615> replacement to be\n> fully done right?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/41676#issuecomment-665812203>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACQNEFE2AS5JC3BEL57DYETR6BPMDANCNFSM4PGCHNSQ>\n> .\n>\n"]}, {"number": 41675, "title": "Allow slots to be initialized with different shapes + values than primary var in Keras optimizer", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): probably not\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently the `add_slot` method in the optimizer class [(link)](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L694-L730) only allows you to create slots which are the same shape as the original var, and you cannot set it to a custom value (i.e. it forces you to use an initializer).\r\n\r\n**Will this change the current api? How?**\r\nEither `add_slot` should be modified to include this functionality, or there should be a tf.keras equivalent to the old `_get_or_make_slots` method which gives the developer the option to pass in a value for the slot to be initialized with (which may be a different shape than the var). \r\n\r\n**Who will benefit with this feature?**\r\nAnyone implementing custom optimizers which require slots which may be custom values or shapes (i.e. the SM3 optimizer creates accumulators which are differently shaped than their corresponding vars).\r\n\r\n**Any Other info.**\r\nThis feature seems to be present in the base optimizer class via `_get_or_make_slot` which calls [create_slot()](https://github.com/tensorflow/tensorflow/blob/d8dcead44017aa0381ca16254a161099eeb7c2e4/tensorflow/python/training/slot_creator.py#L104), which allows you to pass in an initial value.", "comments": ["Maybe you can use hyper parameters, check [this question](https://stackoverflow.com/questions/62042342/what-are-get-hyper-and-set-hyper-in-tensorflow-optimizers).", "duplicate of https://github.com/tensorflow/tensorflow/issues/19457 (opened May 2018...)", "@mathemakitten Since it was opened long time back, I would like to know whether there is any actionable items like raising PR? If this is still an issue and want to contribute, please feel free to open this issue in keras-team/keras repo.\r\n\r\nPlease note that Keras development moved to another repository to focus entirely on only keras. Could you please repost this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues). Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41674, "title": "TensorFlow Lite for Microcontrollers sigaborts with a MobileNetV2 alpha=0.1 model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.5, Linux\r\n- GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.62) c++1\r\n\r\n**Describe the current behavior**\r\n\r\nI am using TensorFlow Lite for Microcontrollers at commit 4f69f62c61ecf3cd23286324af62d00643186ec2.\r\n\r\nI've trained two MobileNetV2 models in Keras with 48x48 input size and a single input channel, then converted to int8 quantized.\r\n\r\nI am attempting to run both models using TensorFlow Lite for Microcontrollers on x86, built with clang on MacOS and with gcc on Ubuntu.\r\n\r\nThe first model has a MobileNetV2 filter scaling factor (_a_) of 0.35. This model runs perfectly.\r\n\r\nThe second model has a scaling factor of 0.1. This model sigaborts during the `Invoke()` call.\r\n\r\nStrangely, both models run perfectly when executed from the OpenMV H7+ (Arm Cortex-M7), and the smaller model runs perfectly on the H7. It might be worth noting that on the OpenMV devices the model is stored in dynamic memory. That said, I've tried declaring the model without static on x86 and it has no impact.\r\n\r\nI've attached zips containing both model files, plus an example program that exhibits the sigabort.\r\n\r\nTo build and run the example with an empty input:\r\n\r\n```\r\nmake -f Makefile.tflite\r\n./build/edge-impulse-standalone \"\"\r\n```\r\n\r\nWithin the example code, the call to `Invoke()` is on line 260 of `edge-impulse-sdk/classifier/ei_run_classifier.h`.\r\n\r\nTo switch to the 0.35 model, which doesn't sigabort, replace `tflite-model`, `model-parameters`, and `edge-impulse-sdk` directories with the versions contained within `0.35 grayscale.zip`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe _a_=0.1 model should run successfully on x86, the same as the 0.35 does.\r\n\r\n**Standalone code to reproduce the issue**\r\nExample code here:\r\n\r\n[example-standalone-inferencing.zip](https://github.com/tensorflow/tensorflow/files/4968485/example-standalone-inferencing.zip)\r\n\r\nThe `.lite` model files are located here:\r\n[models.zip](https://github.com/tensorflow/tensorflow/files/4968496/models.zip)\r\n", "comments": ["The actual crash is here: tensorflow/lite/kernels/internal/reference/integer_ops/add.h in the Add function.\r\n\r\n```\r\nTFLITE_DCHECK_LE(params.input1_offset, int8_max_value);\r\n```\r\n\r\nHere `params.input1_offset` is 128, which is invalid (max is 127). I assume this is a parser or quantization bug.\r\n\r\nChanging this parameter to 127 solves the issue, but I'm not sure where it comes from.\r\n\r\n```\r\n  ArithmeticParams *p = (ArithmeticParams*)&params;\r\n\r\n  if (p->input1_offset > 127) {\r\n      p->input1_offset = 127;\r\n  }\r\n```", "Thank you Jan! Since this seems to perhaps be a converter issue, I'm attaching the SavedModel files for both models.\r\n\r\n[0.1-grayscale-savedmodel.zip](https://github.com/tensorflow/tensorflow/files/4973569/0.1-grayscale-savedmodel.zip)\r\n[0.35-grayscale-savedmodel.zip](https://github.com/tensorflow/tensorflow/files/4973570/0.35-grayscale-savedmodel.zip)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41674\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41674\">No</a>\n", "@dansitu \r\n@janjongboom \r\nHi, i have run into problems running similar NN to yours.\r\n`base_model = tf.keras.applications.MobileNetV2(input_shape=(48, 48, 1), alpha=0.35, weights=None, include_top=False)\r\n\r\nx = base_model.output\r\n\r\nx = tf.keras.layers.Flatten()(x)\r\n\r\nx = Dense(2)(x) #final layer with softmax activation for N classes\r\n\r\npreds = tf.keras.layers.Softmax()(x)\r\n\r\nmodel=Model(inputs=base_model.input,outputs=preds) #specify the inputs and outputs\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\ndef representative_dataset():\r\n  for i in range(500):\r\n    yield([np.random.rand(1,48,48,1).astype(np.float32)])\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.representative_dataset = representative_dataset\r\n\r\ntflite_model = converter.convert()\r\n\r\nopen(\"mobilenet_test.tflite\", \"wb\").write(tflite_model)\r\n`\r\nthe model crush on the microcontroller in assert (tflite::PreprocessSoftmaxScaling)\r\n\r\nthanks"]}, {"number": 41673, "title": "Add distribution strategy benchmark usages for antirectifier, bid_lstm, cifar10_cnn models.", "body": "Add some distribution strategy benchmarks and fix name typo.", "comments": []}, {"number": 41672, "title": "Clone rel/ directory structure", "body": "Some of these scripts are different from their 2.X counterparts and\ncouldn't be exactly copied, so I rebuilt the same structure with the\nsame files.\n\nThis assumes that the included files are the ONLY ones used for r1.15\ntests. If that's wrong, I can include the missing ones.", "comments": []}, {"number": 41671, "title": "Problems with Transformations when trying to model.fit() my 2D CNN Model, leading to two exceptions", "body": "**System information**\r\n- Have I written custom code [posted below]:\r\n- Microsoft Windows [Version 10.0.18363.959]\r\n- TensorFlow v2.2.0-rc4-8-g2b96f3662b 2.2.0 installed via pip. Cuda 10.1 with CUDNN 10.1. GPU enabled.\r\n- Python version: 3.7\r\n- GPU model and memory: GeForce GTX 1650 with Max-Q Design computeCapability: 7.5\r\ncoreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s\r\n-Done in Jupyter Notebook\r\n\r\nWhile training my model using the model.fit() method, my generator inputs the first image slice (with corresponding segmentation truth). During that process, I get 2 exceptions, which ill post after my code. I believe the issue has to do during the flatten method, but I do not know how to address that change.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport ctypes\r\n#hllDll = ctypes.WinDLL(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v10.1\\\\bin\\\\cudart64_101.dll\")\r\n#incase cudart64_101.dll is unable to be loaded\r\nimport os\r\nimport numpy as np\r\nfrom medpy.io import load\r\nimport tensorflow as tf\r\nimport tensorflow.keras as tfk\r\nfrom tensorflow.keras import layers\r\nimport keras\r\nfrom keras.utils.vis_utils import plot_model\r\nimport sklearn\r\nfrom scipy.ndimage import rotate\r\nimport scipy\r\nfrom sklearn import datasets, preprocessing, model_selection, metrics\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\n```\r\n\r\n\r\n```\r\n#Image dict is just a dictionary of paths to .mhd files for images and segmentation truths    \r\n\r\ndef image_preprocessing(array):\r\n    logging.info(\"Beginning Preprocessing\")\r\n    \r\n    logging.info(\"Rotating Image into axial plane\")\r\n    array = rotate(array, 90, axes=(0,2), reshape=True) #rotates to axial form sup to inf\r\n    \r\n    logging.info(\"Making image to uniform size\")\r\n    \r\n    blank_array = np.full((1,512,512),-3024) #-3024 is the \"background, out of zoom data point\"\r\n    \r\n    while len(array) < 69:\r\n        array = np.append(array, blank_array, axis = 0)\r\n        array = np.append(blank_array, array, axis = 0)\r\n        \r\n    if len(array) == 69:\r\n        array = np.append(array,blank_array, axis = 0)\r\n    else:\r\n        pass\r\n    \r\n    logging.info(\"Image Preprocessing is complete\")\r\n    \r\n    return array\r\n\r\ndef image_generator(img_dict, mask):\r\n    \r\n    include_mask = False\r\n    \r\n    logging.info(\"creating generator for {}.\".format(img_dict))\r\n    \r\n    if mask == True:\r\n        include_mask = True\r\n    \r\n    if include_mask == True:\r\n        \r\n        for path1, path2 in zip(img_dict['image'],img_dict['mask']):\r\n            logging.info(\"loading {} {} into generator\".format(\"image\",os.path.basename(path1)))\r\n            image, header_img = load(path1)\r\n            print(\"Image processing\")\r\n            image = image_preprocessing(image)\r\n            \r\n            \r\n            logging.info(\"loading {} {} into generator\".format(\"mask\",os.path.basename(path2)))\r\n            mask_img, header_mask = load(path2)\r\n            print(\"Mask Processing\")\r\n            mask_img = image_preprocessing(mask_img)\r\n        \r\n            logging.info(\"Generator is prepped\")\r\n            \r\n            for img_slice, mask_slice in zip(image,mask_img):\r\n                img_slice = np.reshape(img_slice,(1,512,512,1))\r\n                mask_slice = np.reshape(mask_slice,(1,512,512,1))\r\n                \r\n                yield img_slice, mask_slice\r\n            \r\n    elif include_mask == False:\r\n        for path in dict_obj[\"image\"]:\r\n            logging.info(\"loading {} {} into generator\".format(\"image\",os.path.basename(path)))\r\n            image, header  = load(path)\r\n            image = image_preprocessing(image)\r\n            logging.info(\"Generator is prepped\")\r\n            \r\n            for slice in image:\r\n                slice = np.reshape(slice,(512,512,1))\r\n                yield slice\r\n    else:\r\n        raise TypeError(\"Problem with inputs.\")\r\n        logging.error(\"Unable to load image into generator\")\r\n```\r\n\r\n`tr_data = image_generator(tr_dict,mask = True)\r\n`\r\n\r\n```\r\n''' Creating NN'''\r\n\r\n\r\nif os.path.isdir(Model_path+model_vers)==True:\r\n    logging.info(\"Loading Network\")\r\n    model=tfk.models.load_model(\"Model_path+model_vers\")\r\n    \r\nelse:\r\n    logging.info(\"Creating Network\")\r\n    model= tfk.Sequential()\r\n    model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=input_shape, data_format=\"channels_last\", padding='SAME'))\r\n    model.add(layers.MaxPooling2D(pool_size=(3, 3)))\r\n    model.add(layers.BatchNormalization(center=True, scale=True))\r\n    model.add(layers.Dropout(0.5))\r\n    model.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', padding ='SAME'))\r\n    model.add(layers.MaxPooling2D(pool_size=(3, 3)))\r\n    model.add(layers.BatchNormalization(center=True, scale=True))\r\n    model.add(layers.Dropout(0.5))\r\n    model.add(layers.Flatten())\r\n    model.add(layers.Dense(100, activation='relu', kernel_initializer='he_uniform'))\r\n    model.add(layers.Dense(50, activation='relu', kernel_initializer='he_uniform'))\r\n    model.add(layers.Dense(10, activation='relu'))\r\n\r\n\r\n'''Hyper Parameters'''\r\nlogging.info(\"Adding Hyperparameters\")\r\n\r\noptimizer=tfk.optimizers.Adadelta() #'Adam' \r\nloss= \"categorical_crossentropy\" #'sparse_categorical_crossentropy'\r\nmetrics = ['accuracy']\r\nmodel.compile(optimizer = optimizer, loss= loss, metrics= metrics )\r\n```\r\n\r\n\r\n\r\nWhen I run:\r\n\r\n```\r\n\"\"\"Training NN\"\"\"\r\nlogging.info(\"Training Neural Network\")\r\ntf.autograph.experimental.do_not_convert(func=model.fit(tr_data,epochs = 10,verbose=10))\r\n#model.save(\"Model_path+model_vers\")\r\n```\r\nMy error is:\r\n```\r\nImage processing\r\nMask Processing\r\nEpoch 1/10\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002785034A9D8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 4, expecting 3\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002785034A9D8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 4, expecting 3\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002785034A9D8> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 4, expecting 3\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n```\r\n\r\nTraceback attached in .txt file\r\nModel Summary in .txt file.\r\n[Tensorflow Traceback.txt](https://github.com/tensorflow/tensorflow/files/4967995/Tensorflow.Traceback.txt)\r\n[Model Summary.txt](https://github.com/tensorflow/tensorflow/files/4967996/Model.Summary.txt)\r\n\r\n\r\nThe 2 exceptions can be found in the tracebook, but summarized here:\r\n```\r\n\r\nAssertionError: Bad argument number for Name: 4, expecting 3\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError: Shapes (None, None, None, None) and (None, 10) are incompatible\r\n\r\n```\r\n\r\n\r\n**Have tried remedy without any change:**\r\n- reisntall tensorflow and keras, as well as gast. \r\n- restarted computer\r\n- altering input shapes\r\n- rewrote by hand in a new file, no change.\r\nEdit:\r\n- altered size of NN\r\n-altered data shape ", "comments": ["@zhilothebest \r\n\r\nI tried in colab with TF 2.2 and i am seeing the error (`NameError: name 'tr_dict' is not defined`).\r\nRequest you to provide colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "My code can be found at this repo: I have made a separate branch called tensorflow-debug that has unnecessary code commented out.  https://github.com/zhilothebest/Coronary_Calcium\r\n\r\nA sample of the dataset is uploaded to here: <Removed>\r\n\r\nJust edit the PATH_tr to where ever the folder that is holding the dataset  is located on your computer and the code should \"properly\" function.\r\n\r\nLet me know if there is anything else I can do to help!", "During some debugging, I found this might be related to the fact my computer cant load cudnn, and thus cant find the convolution algorithm. All the versions seem to match up to work properly, and my CUDA paths are appropriate. Ive tried limiting GPU memory space, and limiting to CPU only, but cudnn I still not being readable by tensorflow. Any thoughts on this?\r\n\r\n\r\nEDIT: Forgot to add the error here:\r\n```\r\n`UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node functional_1/conv2d/Conv2D (defined at <ipython-input-6-d54a4523ab21>:13) ]] [Op:__inference_train_function_791]\r\n\r\nFunction call stack:\r\ntrain_function`\r\n```", "@zhilothebest \r\n\r\nCan you go through the [link ](https://stackoverflow.com/questions/41402409/tensorflow-doesnt-seem-to-see-my-gpu), [link2](https://stackoverflow.com/questions/43658989/tensorflow-not-loading-cudnn) and see if it helps you.Thanks!\r\n", "Hi @ravikyram, with regards to the first link, I guess my GPU (Geforce GTX 1650) is not a CUDA enabled product according to that list? Which is weird, since CUDA recognizes it and runs \r\n`Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2907 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 7.5)`\r\n\r\nCould this be why Tensorflow doesn't load cudnn?\r\n\r\nWith regards to the second link, here are my PATH variables; I believe they are set up.\r\n![image](https://user-images.githubusercontent.com/21052148/88573934-5e58be00-cff6-11ea-9d91-68540662d236.png)\r\n![image](https://user-images.githubusercontent.com/21052148/88573635-e1c5df80-cff5-11ea-8331-3c606240a662.png)\r\n\r\nI've added the CUDA /include into the PATH, and this as well did not change anything.", "Here is my `nvidia-smi` as well:\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 442.19       Driver Version: 442.19       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 165... WDDM  | 00000000:02:00.0 Off |                  N/A |\r\n| N/A   40C    P8     3W /  N/A |   4076MiB /  4096MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     10124      C   ...cal\\Programs\\Python\\Python37\\python.exe N/A      |\r\n|    0     12672      C   ...cal\\Programs\\Python\\Python37\\python.exe N/A      |\r\n\r\n\r\nI currently have two Jupyter Kernels open.", "Your gpu is cuda enabled. See https://forums.developer.nvidia.com/t/cuda-enabled-geforce-1650/81010\r\nDid you try [limiting gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) after closing all the python sessions, terminating jupyter notebooks, exiting the interpreter/rebooting the system?\r\n```python\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\n# Your code here\r\n# . . .\r\n```", "Hi @ymodak, I was running my code in the Jupyter notebook session, but everything else was closed. Only the cmd prompt window and the Jupyter notebook with the one kernel was open. I have reset kernels. Restarted computer. All of that! \r\n\r\nI also tried limiting GPU memory growth as well, no luck.", "Hi @zhilothebest can you please attach the logs from your TF run?", "The traceback is posted above in the first post. Sadly, I don't quite know how to export any logging that tensorflow may have internally into a file?", "@sanjoy any update?", "> Sadly, I don't quite know how to export any logging\r\n\r\nBy \"logging\" I meant everything TensorFlow prints to stderr and stdout during execution.  That output might have a hint on what's going wrong with cuDNN.", "Hi, It appears my last comment didnt post; but everything that was printed in stderr and stdout is above.", "@zhilothebest Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41671\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41671\">No</a>\n"]}, {"number": 41670, "title": "Update version numbers for TensorFlow 2.3.0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 3 -> 3\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.3.0-rc2\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.3.0rc2\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 41669, "title": "2.3.0-rc2 cherry-pick request: Cherry pick library threadsafestatus ", "body": "TF serving is releasing 2.3.0-rc0 on top of TF 2.3.0-rc2. This library is needed to compile TF serving 2.3.0-rc0 (namely this cherry pick is a release blocker for TF serving)", "comments": ["This breaks builds, so will revert."]}, {"number": 41668, "title": "S3 get children , translate name ProvideFilesystemSupportFor", "body": "@mihaimaruseac \r\nWe almost finish `s3` now. I will start working on `hdfs` tomorrow. All 3 filesystems will be done around mid-August and we will use the remaining time to make sure all 3 filesystems work as expected.", "comments": []}, {"number": 41667, "title": "Can't install Tensorflow 1.x", "body": "I am trying to install Tensorflow 1.14 for a package that I am trying to use. I tried:\r\n`pip3 uninstall tensorflow`\r\n\r\nThen I tried to install Tensorflow 1.14 using:\r\n`pip3 install tensorflow==1.14`\r\n\r\nand I get the following error\r\n`ERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2)\r\nERROR: No matching distribution found for tensorflow==1.14`\r\n\r\nI also tried making a new virtual env and tried the following commands but it didn't work. Is there any way to install Tensorflow 1?\r\n", "comments": ["@NNDEV1,\r\nPlease upgrade pip to the latest version using the below command and check if you are facing the same issue.\r\n\r\n`python3 -m pip install --upgrade pip`\r\n\r\nAlso, please take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have all the required dependencies installed. Thanks!", "You have python3.8. The only TF versions with python3.8 support are versions after and including TF2.2.\r\n\r\nTF1.14 is no longer supported", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41667\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41667\">No</a>\n"]}, {"number": 41666, "title": "Can't predict bounding boxes with my own trained model, even though Tensorflow returns reduced loss during training.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nHave I written custom code (as opposed to using a stock example script\r\nprovided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\nhappens on a mobile device: No\r\nTensorFlow installed from (source or binary): Source\r\nTensorFlow version (use command below): 1.14.0\r\nPython version: 3.6.7\r\nBazel version (if compiling from source): Not using Bazel\r\nGCC/Compiler version (if compiling from source): Not using GCC\r\nCUDA/cuDNN version: CUDA/cuDNN: 10.0\r\nGPU model and memory: NVIDIA GeForce GTX 1080, 6G\r\nExact command to reproduce:\r\n\r\nvis_util.visualize_boxes_and_labels_on_image_array(\r\n    image,\r\n    np.squeeze(boxes),\r\n    np.squeeze(classes).astype(np.int32),\r\n    np.squeeze(scores),\r\n    category_index,\r\n    use_normalized_coordinates=True,\r\n    line_thickness=8,\r\n    min_score_thresh=0.20)\r\n\r\n**Describe the current behavior**\r\nWhen trying to predict bounding boxes with my own trained model with the Object Detection API, it won't predict any boxes and returns the original image. The problem is not with my prediction code, as I used the same as in the tutorial at \r\nhttps://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\r\nand with a different model, which returns the predicted bounding boxes.\r\n\r\n**Describe the expected behavior**\r\nI don't know why this is happening, as the Tensorboard report says that my loss is reducing as I train the model, but when actually using it to predict bounding boxes in the image, it won't work.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe model:\r\nhttps://drive.google.com/drive/folders/1SMwSHRR8RLX3RMhYXT_aI9gMAIRLHHlp?usp=sharing\r\n\r\nThe prediction code:\r\n```\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sys\r\n\r\nfrom utils import label_map_util\r\nfrom utils import visualization_utils as vis_util\r\n\r\nMODEL_NAME = 'league_model'\r\nIMAGE_NAME = 'test1.jpg'\r\n\r\n# Grab path to current working directory\r\nCWD_PATH = os.getcwd()\r\n\r\n# Path to frozen detection graph .pb file, which contains the model that is used\r\n# for object detection.\r\nPATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\r\n\r\n# Path to label map file\r\nPATH_TO_LABELS = os.path.join(CWD_PATH,'annotations','label_map.pbtxt')\r\n\r\n# Path to image\r\nPATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\r\n\r\n# Number of classes the object detector can identify\r\nNUM_CLASSES = 2\r\n\r\n# Load the label map.\r\n# Label maps map indices to category names, so that when our convolution\r\n# network predicts `5`, we know that this corresponds to `king`.\r\n# Here we use internal utility functions, but anything that returns a\r\n# dictionary mapping integers to appropriate string labels would be fine\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\n# Load the Tensorflow model into memory.\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.compat.v1.GraphDef()\r\n    with tf.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        od_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(od_graph_def, name='')\r\n\r\n    sess = tf.Session(graph=detection_graph)\r\n\r\n# Define input and output tensors (i.e. data) for the object detection classifier\r\n\r\n# Input tensor is the image\r\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n\r\n# Output tensors are the detection boxes, scores, and classes\r\n# Each box represents a part of the image where a particular object was detected\r\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n\r\n# Each score represents level of confidence for each of the objects.\r\n# The score is shown on the result image, together with the class label.\r\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n\r\n# Number of objects detected\r\nnum_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n\r\n# Load image using OpenCV and\r\n# expand image dimensions to have shape: [1, None, None, 3]\r\n# i.e. a single-column array, where each item in the column has the pixel RGB value\r\nimage = cv2.imread(PATH_TO_IMAGE)\r\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\nimage_expanded = np.expand_dims(image_rgb, axis=0)\r\n\r\n# Perform the actual detection by running the model with the image as input\r\n(boxes, scores, classes, num) = sess.run(\r\n    [detection_boxes, detection_scores, detection_classes, num_detections],\r\n    feed_dict={image_tensor: image_expanded})\r\n\r\n# Draw the results of the detection (aka 'visulaize the results')\r\n\r\nvis_util.visualize_boxes_and_labels_on_image_array(\r\n    image,\r\n    np.squeeze(boxes),\r\n    np.squeeze(classes).astype(np.int32),\r\n    np.squeeze(scores),\r\n    category_index,\r\n    use_normalized_coordinates=True,\r\n    line_thickness=8,\r\n    min_score_thresh=0.20)\r\n\r\n# All the results have been drawn on image. Now display the image.\r\ncv2.imshow('Object detector', image)\r\n\r\n# Press any key to close the image\r\ncv2.waitKey(0)\r\n\r\n# Clean up\r\ncv2.destroyAllWindows()\r\n\r\n\r\n```", "comments": ["**TensorBoard loss**\r\n![2020-07-23 14_08_49-TensorBoard](https://user-images.githubusercontent.com/35232794/88316369-1e3fc580-ccee-11ea-8c3d-7eca9f318b28.png)\r\n**Empty returned Image**\r\n![empty](https://user-images.githubusercontent.com/35232794/88316660-8db5b500-ccee-11ea-89d0-aebe77449d6f.png)\r\n**The original ssd_inception_v2 model predicting the bounding boxes on another image**\r\n![ssd](https://user-images.githubusercontent.com/35232794/88317853-403a4780-ccf0-11ea-95d5-f5beeccecf3c.png)\r\n", "**Update:**\r\nI got the boxes to appear, by using the Legacy _train.py_ file.\r\nThe problem has changed, as using the same image and the same steps as before, the bounding boxes get too big and don't actually match the number of objects in the image, as follows:\r\n![err](https://user-images.githubusercontent.com/35232794/88342952-c10c3a00-cd16-11ea-9201-9434bf6ca8be.png)\r\n![err1](https://user-images.githubusercontent.com/35232794/88343191-34ae4700-cd17-11ea-9d50-e8902df148a1.png)\r\n\r\n**Expected output: **\r\n![ok1](https://user-images.githubusercontent.com/35232794/88343432-c61db900-cd17-11ea-865a-5d11f4ec9727.png)\r\n\r\n", "### **Update: SOLVED**\r\nThe problem was in the way the tutorial documented the configuration for the pipeline.\r\nThe names of the folders were mixed up, so my model couldn't restore training, thus making me lose all progress whenever I stopped the training.\r\n\r\n_I will open a new issue to report this error in the tutorial._", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41666\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41666\">No</a>\n"]}, {"number": 41665, "title": "Fix messy formating in tf.linalg.inv's docstring", "body": "This PR fixes the incorrect formating of tf.linalg.inv's docstring\r\nwhere the summary span into description.\r\n\r\nThis PR fixes #41656.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 41664, "title": "Maybe incorrect Brier score calculation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow version (use command below): 1.15.0\r\n- TensorFlow Probability version: 0.8.0\r\n\r\n**Describe the current behavior**\r\n\r\nI believe that the way the Brier score calculation is implemented in tensorflow_probability.stats.brier_score is incorrect. The formula used is\r\n \r\nsum_i p[i]* p[i] - 2*p[k]\r\n  \r\nwhere p is the probability vector over all discrete outcomes, and k is the realized outcome. (Note: This gives element wise Brier scores, and to get the actual Brier score across the dataset, reduce_mean() must be called.)\r\n\r\n**Describe the expected behavior**\r\n\r\nThis formula does not match the reference cited nor [Wikipedia](https://en.wikipedia.org/wiki/Brier_score) nor the definition that [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html) uses. Also, it is stated in the tensorflow_probability docs that the Brier score can be negative, which is not true. \r\n\r\nThe reference cited is Brier\u2019s original paper, found [here](https://journals.ametsoc.org/mwr/article/78/1/1/96424/VERIFICATION-OF-FORECASTS-EXPRESSED-IN-TERMS-OF), which states the formula as \r\n\r\n<img width=\"182\" alt=\"Screen Shot 2020-07-23 at 09 48 34\" src=\"https://user-images.githubusercontent.com/41973500/88304392-3277c880-ccd6-11ea-84c3-237be5929f82.png\">\r\n\r\nwhere r is the number of possible classes, n is the number of forecasts, fij is the probability forecast of class j for instance i, and Eij is 0 or 1 depending if the event occurred in class j or not. This is the same formula that is used in sklearn and Wikipedia. By definition, this score cannot be negative.\r\n\r\nIf we consider the element wise Brier score from this formula, it is\r\n\r\nsum_j (p[j] - E[j])2\r\n\r\nwhich is not equivalent to sum_i p[i]* p[i] - 2*p[k]. Tensorflow\u2019s Brier score formula should be corrected to match the commonly accepted definition.\r\n\r\n", "comments": ["@maia-adar Are you referring to [this page](https://www.tensorflow.org/probability/api_docs/python/tfp/stats/brier_score) on TF website?\r\n\r\nAre you interested in raising a PR to update the docs? Thanks!", "@jvdillon, this looks like a `tfp` issue. Can you triage and/or transfer it over? (The transfer button is in the bottom of the right control panel)", "@maia-adar,\r\nSorry for the delayed response. Can you please confirm if the issue still persist. If so, request you to file an issue in [TFP Github Repository](https://github.com/tensorflow/probability/issues/new). Thanks! ", "Filed on tfp"]}, {"number": 41663, "title": "Sub-classes of keras.metrics.Metric do not inherit set_model function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Linux\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version: v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: Python 3.8.2\r\n\r\n**Describe the current behavior**\r\nWhen creating a custom callback function for use in training/evaluation I get an error:\r\n```python\r\nAttributeError: 'BinaryTruePositives' object has no attribute 'set_model'\r\n```\r\n\r\n**Describe the expected behavior**\r\nFitting/evaluation to continue normally, using custom callback.\r\n\r\n**Standalone code to reproduce the issue**\r\n``` python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nbatch_size = 25\r\nN = 10\r\n\r\n# From https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric\r\nclass BinaryTruePositives(tf.keras.metrics.Metric):\r\n    ...\r\n\r\n# Set up model\r\ninput = keras.layers.Input(shape=(1))\r\ndense = keras.layers.Dense(100, activation='relu')(input)\r\noutput = keras.layers.Dense(1, activation='tanh')(dense)\r\n\r\n# Create model\r\nmodel = keras.Model(inputs=input, outputs=output)\r\nmodel.compile(loss='MSE')\r\n\r\n# Following two lines give the error\r\nmodel.fit(BinaryTruePositives(), steps_per_epoch=N, callbacks=[BinaryTruePositives()])\r\nmodel.evaluate(BinaryTruePositives(), steps=N, callbacks=[BinaryTruePositives()])\r\n\r\n```\r\n\r\n**Other info / logs** \r\n```python\r\nTraceback (most recent call last):\r\n  File \"up_test.py\", line 47, in <module>\r\n    model.fit(my_generator(), steps_per_epoch=N, callbacks=[BinaryTruePositives()])\r\n  File \"~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 819, in fit\r\n    callbacks = callbacks_module.CallbackList(\r\n  File \"~/.local/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\", line 219, in __init__\r\n    self.set_model(model)\r\n  File \"~/.local/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\", line 281, in set_model\r\n    callback.set_model(model)\r\n```\r\n", "comments": ["Are `metric`s only intended for model training, and not for model prediction/evaluation? If so, then I misunderstood the role of `metric`s, and the documentation/guides are not very clear on when `metric`s can be used, and when callbacks can be used.", "@obtu,\r\nOn running the code, I am facing a different error stating `ValueError: Failed to find data adapter that can handle input: <class '__main__.BinaryTruePositives'>, <class 'NoneType'>`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3ee1278a43a2c1eba17717861f47e555/41663.ipynb#scrollTo=n6ZJiM33UFBi).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!\r\n\r\n", "My bad; I accidentally removed/changed the generator. This is the correct tail of the code:\r\n\r\n```python\r\n# Create data\r\nclass my_generator(keras.utils.Sequence):\r\n    def __len__(self): return N*N\r\n    def __getitem__(self,index):\r\n        return (np.random.random(batch_size), np.zeros(batch_size))\r\n\r\n# Fit model\r\nmodel.fit(my_generator(), steps_per_epoch=N, callbacks=[BinaryTruePositives()])\r\nmodel.evaluate(my_generator(), steps=N, callbacks=[BinaryTruePositives()])\r\n```\r\n\r\n\r\n", "@obtu,\r\nI was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/02f0e86424c61cf18154269a82dbb3ed/41663.ipynb#scrollTo=n6ZJiM33UFBi). \r\n\r\nCould you please check [this comment](https://stackoverflow.com/a/47791254) from a similar StackOverflow issue and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "`BinaryTruePositives` subclasses `tf.keras.metrics.Metric`in the example, thus the StackOverflow answer is insufficient.\r\n\r\nThe problem is that `metrics` cannot be used as `callbacks`, and vice versa, in model `compile` and `fit`, respectively."]}, {"number": 41662, "title": "NFC - minor spelling tweaks", "body": "This PR addresses minor spelling tweaks in documents", "comments": ["@ravi5175 the review you left had no effect, it does not trigger merging the PR. In fact, it can cause troubles in the CI."]}, {"number": 41661, "title": "Fix typo in optimizer_v2.py", "body": "Issue opened by FirefoxMetzger 3 hours ago: typo in optimizer_v2.py #41658\r\n\r\nFixed line in:\r\ntensorflow/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\r\nLine 285 in bd6b557\r\n\r\n", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac @chanshah"]}, {"number": 41660, "title": "Creating model in subprocess consume all memory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Keras version: 2.3.1\r\n- Python version: 3.7\r\n\r\nThe code is below.\r\nI expect the model would be created once. but instead it seems being created again and again. And it never got released.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport keras as k\r\nimport numpy as np \r\nimport multiprocessing as mp\r\n\r\ndef createmodel():\r\n    inputs = k.layers.Input(shape=(720,))\r\n    x = k.layers.Dense(256, activation=\"relu\")(inputs)\r\n    x = k.layers.Dense(256, activation=\"relu\")(x)\r\n    outputs = k.layers.Dense(1, activation=\"sigmoid\")(x)\r\n    model = k.models.Model(inputs, outputs)\r\n    model.compile(loss=\"binary_crossentropy\",\r\n                optimizer=\"adam\")\r\n    return model\r\n\r\nclass TRAINER:\r\n    def __init__(self):\r\n        self.model = createmodel()\r\n\r\n    def play(self, args):\r\n        return (1, np.zeros((720)))\r\n\r\n    def train(self):\r\n        mp.set_start_method('spawn')\r\n        p = mp.Pool(4)\r\n        while True:\r\n            res = p.map(self.play, [0] * 128)\r\n\r\ntrainer = TRAINER()\r\nif __name__ == '__main__':\r\n    trainer.train()\r\n```\r\n\r\n", "comments": ["@tangcxx I am not able to reproduce the exact same error but found different error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/2326da1679628ce05f9b6909a79f2a0f/untitled963.ipynb). Thanks!\r\n\r\n> TypeError: can't pickle _thread.RLock objects\r\n\r\n", "@jvishnuvardhan I am not sure what it is. Anyway, I 'd solved my problem by removing the class definition. Instead I'd made a global model."]}, {"number": 41659, "title": "Similar problem happened to me: \"tile_functor_gpu.cu.pic.o was not created\"", "body": "Similar problem happened to me: \"tile_functor_gpu.cu.pic.o was not created\"\r\nTensorflow r1.9\r\nUbuntu 16.04\r\ngcc 4.8\r\nCUDA 9.0\r\ncuDNN 7.0\r\n\r\n_Originally posted by @apepkuss in https://github.com/tensorflow/tensorflow/issues/18839#issuecomment-465826851_", "comments": ["I have the same problem. someone know how to solve it?Thank you very much!!!\r\nTensorflow r1.8\r\nubuntu16.04\r\ncuda9.0\r\ngcc 5.4.0\r\ncudnn7.0", "@nonlinear1 \r\n\r\nCan you please provide code snippet to reproduce the issue.\r\nAlso, TF version 1.8 ,1.9 is too old.Can you please upgrade to TF 1.15 and see if the issue still persists.Thanks!", "I have solve this problem. the version of cudnn must be 7.0.5\n\n\u5728 2020-07-23 23:27:28\uff0cravikyram <notifications@github.com> \u5199\u9053\uff1a\n\n\n@nonlinear1\n\nCan you please provide code snippet to reproduce the issue.\nAlso, TF version 1.8 ,1.9 is too old.Can you please upgrade to TF 1.15 and see if the issue still persists.Thanks!\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.", "@nonlinear1 \r\n\r\nGlad to know issue was resolved. Please, close this thread as your issue was resolved.Thanks!"]}, {"number": 41658, "title": "typo in optimizer_v2.py", "body": "https://github.com/tensorflow/tensorflow/blob/bd6b557c02a5cc1d094a7bb180b9779121a58520/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L285\r\n\r\nI was reading through the code to learn how to create a custom optimizer. I think the line should be\r\n\r\n```\r\nThis class is stateful and thread-compatible.\r\n```", "comments": ["Seems this [typo has been fixed.](https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L297) Closing this issue now.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41658\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41658\">No</a>\n"]}, {"number": 41657, "title": "NaN loss and accuracy when using MirroredStrategy on multiple GPUs (ROCm)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nNo, the code is from this example: https://www.tensorflow.org/tutorials/distribute/keras\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nLinux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\nNo\r\n- TensorFlow installed from (source or binary): \r\nROCm 3.5 \r\n- TensorFlow version (use command below): \r\nv2.2.0-30-g34c3143 2.2.0\r\n- Python version: \r\n3.6.9\r\n- Bazel version (if compiling from source): \r\nNo\r\n- GCC/Compiler version (if compiling from source): \r\nNo\r\n- CUDA/cuDNN version: \r\nROCm\r\n- GPU model and memory: \r\n2x AMD VEGA 10 XT (Vega 64) 8GB \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nMirrored strategy causes the following loss and accuracy values:\r\n\r\nEpoch 12/12\r\n468/469 [============================>.] - ETA: 0s - loss: nan - accuracy: nan\r\n469/469 [==============================] - 12s 26ms/step - loss: nan - accuracy: nan - lr: 1.0000e-05\r\n79/79 [==============================] - 1s 12ms/step - loss: nan - accuracy: nan\r\nEval loss: nan, Eval Accuracy: nan\r\n\r\n\r\nThis seems to happen on Nvidia cards too:\r\nhttps://github.com/tensorflow/tensorflow/issues/36224\r\n\r\nI have tried several other scripts and all do the same.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nRunning on a single card produces an expected output of loss and accuracy.\r\nI would like that to be the case in multiple GPUs too.\r\n\r\nNot sure whether it's just the loss and accuracy calculations screwed up, as the training goes about twice as fast on 2 GPUs.\r\n\r\n**This code seems to work flawlessly on both GPUs:**\r\n\r\n[mnistmultigpu.txt](https://github.com/tensorflow/tensorflow/files/4965502/mnistmultigpu.txt)\r\nHowever, it only seems to utilize 1 GPU, which is strange because it did utilize both before...\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nUnfortunately I cannot guarantee you will be able to reproduce this as this is tied to having multiple GPUs (Nvidia or not)\r\n\r\n<details><summary> The code I used </summary>\r\n# Import TensorFlow and TensorFlow Datasets\r\n\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\ntfds.disable_progress_bar()\r\n\r\nimport os\r\n\r\ndatasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n\r\n# You can also do info.splits.total_num_examples to get the total\r\n# number of examples in the dataset.\r\n\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\n\r\nBUFFER_SIZE = 10000\r\n\r\nBATCH_SIZE_PER_REPLICA = 64\r\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\ndef scale(image, label):\r\n  image = tf.cast(image, tf.float32)\r\n  image /= 255\r\n\r\n  return image, label\r\n\r\ntrain_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\neval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n\r\n  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\n\r\n# Define the checkpoint directory to store the checkpoints\r\n\r\ncheckpoint_dir = './training_checkpoints'\r\n# Name of the checkpoint files\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n\r\n# Function for decaying the learning rate.\r\n# You can define any decay function you need.\r\ndef decay(epoch):\r\n  if epoch < 3:\r\n    return 1e-3\r\n  elif epoch >= 3 and epoch < 7:\r\n    return 1e-4\r\n  else:\r\n    return 1e-5\r\n\r\n# Callback for printing the LR at the end of each epoch.\r\nclass PrintLR(tf.keras.callbacks.Callback):\r\n  def on_epoch_end(self, epoch, logs=None):\r\n    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\r\n                                                      model.optimizer.lr.numpy()))\r\n\r\ncallbacks = [\r\n    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\r\n    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\r\n                                       save_weights_only=True),\r\n    tf.keras.callbacks.LearningRateScheduler(decay),\r\n    PrintLR()\r\n]\r\n\r\nmodel.fit(train_dataset, epochs=12, callbacks=callbacks)\r\n\r\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n\r\neval_loss, eval_acc = model.evaluate(eval_dataset)\r\n\r\nprint('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\r\n\r\n</details>\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n[runlog.txt](https://github.com/tensorflow/tensorflow/files/4965388/runlog.txt)\r\n\r\nMy code in a text file:\r\n[mirroredstrategy.txt](https://github.com/tensorflow/tensorflow/files/4965443/mirroredstrategy.txt)\r\n\r\n\r\n#36224\r\n\r\nAny and all help would be appreciated, thank you!", "comments": ["> **This code seems to work flawlessly on both GPUs:**\r\n> \r\n> [mnistmultigpu.txt](https://github.com/tensorflow/tensorflow/files/4965502/mnistmultigpu.txt)\r\n> However, it only seems to utilize 1 GPU, which is strange because it did utilize both before...\r\n\r\n#30321\r\nIt seems that it may have worked properly when I experimented with an older version of TensorFlow.\r\n\r\nSo basically even that one doesn't work.\r\n\r\n", "Hi @Laggger164, just to make sure I'm clear on this, you are running the basic Distributed training with Keras MNIST example. When you have just one GPU with MirroredStrategy, everything works. However, when you add a second GPU you are seeing NaN loss and accuracy. Is that a correct summary?\r\n\r\nI'm not able to reproduce this error. So I'm wondering if maybe there's an issue with one of your GPUs? If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. Can you try and train with one GPU but enforce that it's GPU:1 instead of GPU:0? You can do that with tf.config.set_visible_devices to specify which devices are visible to the runtime.", "> Hi @Laggger164, just to make sure I'm clear on this, you are running the basic Distributed training with Keras MNIST example. When you have just one GPU with MirroredStrategy, everything works. However, when you add a second GPU you are seeing NaN loss and accuracy. Is that a correct summary?\r\nPretty much, yes. Thing is it goes about twice as fast with 2 GPUs so it is possible that it works correctly, just the loss and accuracy metrics are calculated incorrectly.\r\n\r\n> I'm not able to reproduce this error. So I'm wondering if maybe there's an issue with one of your GPUs? If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. Can you try and train with one GPU but enforce that it's GPU:1 instead of GPU:0? You can do that with tf.config.set_visible_devices to specify which devices are visible to the runtime.\r\n\r\nI tried with both GPUs, both seem to work identically well.\r\nHere are the logs:\r\n\r\n[mirroredstrategysinglegpu1.txt](https://github.com/tensorflow/tensorflow/files/4970727/mirroredstrategysinglegpu1.txt)\r\n[mirroredstrategysinglegpu2.txt](https://github.com/tensorflow/tensorflow/files/4970728/mirroredstrategysinglegpu2.txt)\r\n\r\nAnd I tried modifying the code to remove the mirrored strategy:\r\n\r\n[strategynomirror.txt](https://github.com/tensorflow/tensorflow/files/4970733/strategynomirror.txt)\r\nand the logs from that:\r\n[strategysinglegpu1.txt](https://github.com/tensorflow/tensorflow/files/4970735/strategysinglegpu1.txt)\r\n[strategysinglegpu2.txt](https://github.com/tensorflow/tensorflow/files/4970736/strategysinglegpu2.txt)\r\n\r\nSeemed to be just a little faster (9 seconds instead of 11) with just one GPU than the one with mirrored strategy.\r\n\r\nI used HID_VISIBLE_DEVICES=0 and 1, verifying the usage with rocm-smi on watch.\r\n\r\nSo I believe there is a problem in how either the code or the system is handling mirrored strategy.\r\nI could try reinstalling the OS with a different version of everything, then upgrading to the newer ones, however that seems like a shot in the dark. I have no idea what I am doing basically...\r\n\r\nAlso, you mentioned you aren't able to reproduce this, meaning you don't have the hardware or system, or did you try and it is working on your end?", "It seems that I managed to solve this by reinstalling everything.\r\n\r\nOne thing that I _did_ change was that I didn't install the AMDGPU-PRO drivers before installing ROCm.\r\n\r\nWhich I later found out was completely useless as ROCm is another type of driver, so it is possible I screwed something up that way.\r\n\r\nI still have a problem with the mirrorStrategy as it is only utilizing multiple GPUs to exactly half their power when using 2 and exactly a quarter when using 4.\r\n\r\nPretty sure this could be caused by some setting in Tensorflow that I don't yet know of.\r\nIf you do know then I would be thankful if you could tell me.\r\n\r\n\r\nNot sure if we should close this issue at this point and make another one. You decide \ud83d\udc4d ", "Glad to hear that reinstalling worked!\r\nAs for the performance issue, can you explain a bit more about what you mean by using half their power? Are you seeing a slowdown with the more GPUs you add?", "> Glad to hear that reinstalling worked!\r\n> As for the performance issue, can you explain a bit more about what you mean by using half their power? Are you seeing a slowdown with the more GPUs you add?\r\n\r\nYes, that's exactly what happens.\r\n\r\nHowever it did work better with both of the cards connected to full PCIe 16x slots... which improved the training time by about 2 seconds from 17 seconds.\r\nAs opposed to using PCIe 1x risers, which just slow it down.\r\n\r\nAs for the power I mean GPU utilization and GPU power draw, metric taken from rocm-smi on watch.\r\ni.e. when using a single GPU both are at 100%, but when using 2, both are at around 50%. VRAM being always full.\r\n\r\nIt's basically telling me that it _could work_ but doesn't right now.\r\n\r\nI also noticed the CPU utilization was almost always at 100% with 4 cards running, while being at around 70% with 2 cards.\r\nI am wondering whether there is a bandwidth issue, the CPU and motherboard are pretty cheap so I will try a much stronger setup at some point.\r\n\r\nBut if you have any ideas, feel free to tell me!", "If you're still facing this utilization problem, can you file a new issue? Any Tensorboard Profiler info you can provide as well will be helpful to debug. There are guides [here](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) and [here](https://www.tensorflow.org/guide/profiler) on how to use the tool.", "Closing this issue now since original problem was solved. If performance issues persist, please file a new issue and I'd be happy to help debug!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41657\">No</a>\n"]}, {"number": 41656, "title": "The first paragraph of the documentation for tf.linalg.inv is cut in half", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/inv\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe first paragraph says\r\n\r\n> Computes the inverse of one or more square invertible matrices or their\r\n\r\ninstead of \r\n\r\n> Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).\r\n\r\n![image](https://user-images.githubusercontent.com/552629/88276649-b2eef700-ccdf-11ea-9b32-1eb450ffc57a.png)\r\n", "comments": ["The issue is in:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.3.0-rc2/tensorflow/core/api_def/base_api/api_def_MatrixInverse.pbtxt#L18-L22\r\n\r\nwhere the summary spans across the description. Created a PR #41665 for the fix."]}, {"number": 41655, "title": "placeholder setting an array with a sequence", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@inksci,\r\nCould you please elaborate your concern and describe the issue you are facing?\r\n\r\nAlso in order to expedite the trouble-shooting process, please fill in the issue template. Thanks!\r\n", "there is a placeholder: shape=(None,None,3)\r\nfor the input: both x1 with shape (1,5,3) and x2 with shape (1,4,3) can feed into the network and get the output.\r\nIn fact, conv1d and global max_pooling is used in my networks to deal with these inputs with changed shape (at axis=1).\r\nHowever, when training the network, for example the batch_size is 2 and the input data \r\n[x1, x2 ]\r\nwhich feeded into the placeholder with shape of (None, None, 3)\r\nYou know, it will make a error: setting an array with a sequence.\r\nThe problem is that x1 and x2 have different domension. I know the problem but how can I train the network with datasets which contains data have different dimensions.\r\nYou know, if I feed these data one by one into networks, that is ok.\r\nBut for a batch training, these data with different domension can not accepted by placeholder except the batch_size is 1.\r\nhow can I training the networks with these irregular input data? will the tf.keras or tf2 can offer a solution?\r\nThanks a lot, and hope that I express my meaning clearly.", "> However, when training the network, for example the batch_size is 2 and the input data\r\n> [x1, x2 ]\r\n> which feeded into the placeholder with shape of (None, None, 3)\r\n> You know, it will make a error: setting an array with a sequence.\r\n\r\n@inksci,\r\nCould you please provide the complete code and the dataset you are using, to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41654, "title": "tensorflow Multi-GPU vs CPU mirroredstrategy  ", "body": "Ask everyone for help..\r\n\r\nI am so confused why gpu is slower than cpu on any codition i try...\r\nI want to use six GPU with mirroredstrategy to reduce the training time..\r\nI follow below steps.\r\nhttps://keras.io/guides/distributed_training/\r\n\r\nI got the bad performance with the machine \r\nGPU: 6*GeForce RTX 2080 TI (10GB) \r\nCPU: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\r\n\r\nrun on the Docker container:  \r\n(1)tensorflow version 2.0.0 \r\n(2)cuda 9.0\r\n(3)cudnn 7.6\r\n(4)nvidia-driver 410.78 - install on server\r\n\r\n====Test1=======\r\n\r\n**CPU**,batch=64,samples:575478\r\n\r\nEpoch 2/500 575478/575478 [===] - **16s** 28us/step - loss: 0.0735 - val_loss: \r\n\r\n====Test2=======\r\n\r\n**GPU**,batch=64,samples:575478\r\nEpoch 1/500\r\n1498/1498 [===] - 60s 40ms/step - loss: 0.0907 - val_loss: 0.0619\r\nEpoch 2/500\r\n1498/1498 [===] - **32s** 21ms/step - loss: 0.0592 - val_loss: 0.0522\r\n\r\n\r\nmy_mini_batch = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync(GPU)\r\n384 = 64*6\r\n[enter image description here][1]\r\n**Why so slower than CPU?**\r\n\r\n\r\n\r\n    enter code here\r\n    #import tensorflow_datasets  as ds #for debug\r\n    os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"       \r\n    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5'\t\r\n    gpu_table = ['/gpu:0','/gpu:1','/gpu:2','/gpu:3','/gpu:4','/gpu:5']\r\n   \r\n    strategy = tf.distribute.MirroredStrategy(devices=gpu_table)\r\n\r\n    TrainDataPath = './Train'\r\n    TestDataPath = './Test'\r\n\r\n    def Load_Data(InputPath):\r\n\t Data_Total = np.array([])\r\n\t Label_Total = np.array([])\r\n\t folder_content = glob.glob(InputPath+'/'+'*_Label*')\r\n\t print('folder content=',folder_content)\r\n\t for File in folder_content:\r\n\t\tLabel = np.load((File))\r\n\t\tLabel_Total = np.append(Label_Total, Label)\r\n\t\tFeature = np.load([F for F in glob.glob(InputPath + \"/\" + File.split('/')[-1].split('Label')[0] + '*') if 'Label' not in F][0])\r\n\t\tData_Total = np.append(Data_Total, Feature)\r\n\r\n\tLabel_Total = Label_Total.reshape(-1, 1)\r\n\tData_Total = Data_Total.reshape(-1, 1,3, 29)\r\n\treturn Data_Total, Label_Total\r\n\r\n    Train_Data, Train_Label = Load_Data(TrainDataPath)\r\n    Test_Data, Test_Label = Load_Data(TestDataPath)\r\n\r\n    def get_dataset():#for debug\r\n\r\n     print('get_dataset()')\r\n     print(\"Training_Data.shpae=\", Training_Data.shape)\r\n    \r\n     global my_mini_batch\r\n     my_mini_batch = 1\r\n     BATCH_SIZE_PER_REPLICA = 64 \r\n\r\n     my_mini_batch = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n     print('my_mini_batc=',my_mini_batch)\r\n     print('BATCH_SIZE_PER_REPLICA=',BATCH_SIZE_PER_REPLICA)\r\n     print('strategy.num_replicas_in_sync=',strategy.num_replicas_in_sync)\r\n\r\n     Train_Dataset_1 = tf.data.Dataset.from_tensor_slices((Training_Data[:, :,:,0:5], Training_Data[:, :,:,5:15], Training_Data[:, :,:,15:25], Training_Data[:,:,:,25:])).batch(my_mini_batch).repeat()\r\n     Train_Dataset_2 = tf.data.Dataset.from_tensor_slices(Training_Label).batch(my_mini_batch).repeat()\r\n\r\n     global train_steps\r\n     global valid_steps\r\n     global test_steps\r\n     train_steps = (int)( Training_Data.shape[0] / my_mini_batch)\r\n     print('train_steps='+str(train_steps))\r\n\r\n     valid_steps = (int)(Val_Data.shape[0] / my_mini_batch)\r\n     print('valid_steps=' + str(valid_steps))\r\n\r\n     test_steps = (int)(Test_Data.shape[0] / my_mini_batch)\r\n     print('test_steps=' + str(test_steps))\r\n\r\n     Valid_Dataset_1 = tf.data.Dataset.from_tensor_slices((Val_Data[:, :,: ,0:5], Val_Data[:, :,:,5:15], \r\n     Val_Data[:, :,:,15:25], Val_Data[:, :, :,25:])).batch(my_mini_batch).repeat()\r\n     Valid_Dataset_2 = tf.data.Dataset.from_tensor_slices(Val_Label).batch(my_mini_batch).repeat()\r\n\r\n     Test_Dataset_1 = tf.data.Dataset.from_tensor_slices((Test_Data[:, :,:,0:5], Test_Data[:, :,:,5:15], \r\n     Test_Data[:, :,:,15:25], Test_Data[:, :,:,25:])).batch(my_mini_batch).repeat()\r\n     Test_Dataset_2 = tf.data.Dataset.from_tensor_slices(Test_Label).batch(my_mini_batch).repeat()\r\n\r\n     print('Replicas: ', strategy.num_replicas_in_sync)\r\n     print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n\r\n\r\n     Train_Dataset_final = tf.data.Dataset.zip((Train_Dataset_1,Train_Dataset_2))\r\n     Valid_Dataset_final = tf.data.Dataset.zip((Valid_Dataset_1,Valid_Dataset_2))\r\n     Test_Dataset_final = tf.data.Dataset.zip((Test_Dataset_1,Test_Dataset_2))\r\n\r\n     return Train_Dataset_final, Valid_Dataset_final, Test_Dataset_final\r\n\r\n    with strategy.scope():\r\n\t PalmTh = 0.5\r\n\t A_layer1_Filters= 2\r\n\t B_layer1_Filters = 2\r\n\t C_layer1_Filters = 2\r\n\t D_layer1_Filters = 2\r\n\r\n\t L = 0.005\r\n\t F = 12\r\n\t Epochs = 500\r\n\t EarlyStopPatience = 10\r\n\t ChangeLrPatience = 8\r\n\t ChangeLrFactor = 0.9\r\n\r\n\t Test_Para = []\r\n\r\n\t A_IN_Cell = Input(shape=(1, 3, 5), name = \"Cell\")\r\n\t PX = Input(shape=(1, 3, 10), name = \"X\")\r\n\t PY= Input(shape=(1, 3, 10), name = \"Y\")\r\n\t PZ = Input(shape=(1, 3, 4), name = \"Z\")\r\n\r\n\r\n\t L1= Convolution2D(filters = A_layer1_Filters, \r\n\t\t     kernel_size = 3, \r\n\t\t     strides = 1, \r\n\t\t     padding = 'valid', \r\n\t\t     data_format = 'channels_first', \r\n\t\t     use_bias = True ,\r\n\t\t     name = 'Conv1_Height_Cell', activity_regularizer=regularizers.l2(0.00001)) \r\n     (A_IN_Cell)\r\n\r\n\t LH1 = Activation(custom_HardTanh)(L1)\r\n\t LH1_out= Flatten()(LH1)\r\n\r\n\t L2= Convolution2D(filters = B_layer1_Filters, \r\n\t\t     kernel_size = 3, \r\n\t\t     strides = 1, \r\n\t\t     padding = 'valid', \r\n\t\t     data_format = 'channels_first', \r\n\t\t     use_bias = True ,\r\n\t\t     name = 'Conv1_ProjectionX', activity_regularizer=regularizers.l2(0.00001)) \r\n     (PX)\r\n\r\n\t LH2= Activation(custom_HardTanh)(L2)\r\n\t LH2_out= Flatten()(LH2)\r\n\r\n\t A_Convolution1 = Convolution2D(filters = C_layer1_Filters, \r\n\t\t     kernel_size = 3, \r\n\t\t     strides = 1, \r\n\t\t     padding = 'valid', \r\n\t\t     data_format = 'channels_first', \r\n\t\t     use_bias = True ,\r\n\t\t     name = 'Conv1_ProjectionY', activity_regularizer=regularizers.l2(0.00001)) \r\n     (PY)\r\n\r\n\t A_Hidden1 = Activation(custom_HardTanh)(A_Convolution1)\r\n\t A_Out = Flatten()(A_Hidden1)\r\n\r\n\t Centroid_Convolution1 = Convolution2D(filters = D_layer1_Filters, \r\n\t\t     kernel_size = 3, \r\n\t\t     strides = 1, \r\n\t\t     padding = 'valid', \r\n\t\t     data_format = 'channels_first', \r\n\t\t     use_bias = True ,\r\n\t\t     name = 'Conv1_Centroid', activity_regularizer=regularizers.l2(0.00001))(PZ)\r\n\r\n\t Centroid_Hidden1 = Activation(custom_HardTanh)(Centroid_Convolution1)\r\n\t Centroid_Out = Flatten()(Centroid_Hidden1)\r\n\r\n\t ConcatentaLayer = concatenate([LH1_out, \r\n\t\t       LH2_out, \r\n\t\t       A_Out,\r\n\t\t       Centroid_Out]\r\n\t\t       )\r\n\r\n\t DenseLayer1 = Dense(F, use_bias = True, activation=None, \r\n     activity_regularizer=regularizers.l2(0.00001))(ConcatentaLayer)\r\n\t DenseLayer1 = Activation(custom_HardTanh)(DenseLayer1)\r\n\t Output = Dense(1, use_bias = True, activation='sigmoid')(DenseLayer1)\r\n\r\n\t model = Model(inputs=[A_IN_Cell , PX, PY, PZ], \r\n     outputs=[Output])\r\n\r\n\t adam = optimizers.Adam(lr=L)\r\n\r\n\t model.compile(optimizer = adam, loss = 'binary_crossentropy')\r\n\t model.summary()\r\n\r\n\t change_lr = ReduceLROnPlateau(monitor='val_loss', factor=ChangeLrFactor,\r\n\t\t\t\t      patience=ChangeLrPatience, min_lr=0.00005)\r\n\t EarlyStop = EarlyStopping(monitor='loss', patience = EarlyStopPatience, verbose=2, mode='min')\r\n\r\n\t Training_Data, Val_Data, Training_Label, Val_Label = train_test_split(Train_Data, Train_Label, \r\n     test_size=0.1)\r\n\t print('Train_Data follow=',Train_Data.shape)\r\n\t print('Training_Data follow=',Training_Data.shape)\r\n\r\n\t train_dataset, val_dataset, test_dataset = get_dataset()\r\n\t print('type(train_dataset)=', train_dataset)\r\n\r\n    #beside scope\r\n    history = model.fit(train_dataset, epochs=Epochs,\r\n                    steps_per_epoch=train_steps,\r\n                    validation_steps = valid_steps,\r\n                    validation_data=val_dataset) \r\n\r\n\r\n", "comments": ["@harrypotter02 \r\nI ran the code shared and face different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/df66ffbe3fdf33fb07213fc65f89e49b/untitled292.ipynb) , share a colab gist with the error faced.(select change run time and gpu)", "@Saduf2019  I Appreciate your help very much.\r\n----------------\r\nI want to ask some question.\r\n(1) I want to know whether tensorflow 'MirroredStrategy' can speed up the training speed.\r\nmy example is follow the link\r\nhttps://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy\r\n\r\n(2)How to speed up the training speed(double/triple) by  MirroredStrategy.\r\nI follow the link implement https://www.youtube.com/watch?v=bRMGoPqsn20\r\n---------------\r\n\r\nMy exsample code is word on colab or github gist,\r\nbut you need download the training data and put on the google drive space.\r\nPS:The colab is only support single GPU.. \r\n\r\n//colab\r\nhttps://colab.research.google.com/drive/1ldJvdk6wfu-fXBb2iBjKe0q1ZnExGG17?usp=sharing\r\n\r\n//github gist\r\nhttps://gist.github.com/harrypotter02/0cc6ffe3bf7c520207dc7be96b1e8b66\r\n--------------\r\nI got the bad performance with the machine\r\nGPU: 6*GeForce RTX 2080 TI (10GB)\r\nCPU: Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz\r\n\r\nrun on the Docker container:\r\n(1)tensorflow version 2.0.0\r\n(2)cuda 9.0\r\n(3)cudnn 7.6\r\n(4)nvidia-driver 410.78 - install on server\r\n\r\n====Test1=======\r\n\r\nCPU,batch=64,samples:575478\r\n\r\nEpoch 2/500 575478/575478 [===] - 16s 28us/step - loss: 0.0735 - val_loss:\r\n\r\n====Test2=======\r\n\r\nGPU,batch=64,samples:575478\r\nEpoch 1/500\r\n1498/1498 [===] - 60s 40ms/step - loss: 0.0907 - val_loss: 0.0619\r\nEpoch 2/500\r\n1498/1498 [===] - 32s 21ms/step - loss: 0.0592 - val_loss: 0.0522\r\n\r\nmy_mini_batch = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync(GPU)\r\n384 = 64*6\r\n[enter image description here][1]\r\nWhy so slower than CPU?\r\n\r\n\r\n", "Other experiment.\r\n\r\n-------Lib1 S-----------\r\nAdd mode samples for test.\r\nBut the six GPU slower than CPU..WHY..\r\n \r\nWindows **CPU** , batch=64 \r\ntraining samples 1726436\r\nEpoch 1/500 1726436/1726436 [=======] - 62s 36us/step - loss: 0.0888 - val_loss: 0.0685 \r\nEpoch 2/500 1726436/1726436 [=======] - **66s** 38us/step - loss: \r\n\r\n\r\nWindows GPU , batch=64 \r\ntraining samples 1726436\r\n4495/4495 [=====] - 122s 27ms/step - loss: 0.0644 - val_loss: 0.0518\r\nEpoch 2/500\r\n4495/4495 [=====] - **93s** 21ms/step - loss: 0.0466 - val_loss: 0.0442\r\nnvidia-smi\r\nhttps://imgur.com/svhbZsd\r\n-------Lib1 E-----------\r\n", "Hi @harrypotter02, in response to your first question, yes using `MirroredStrategy` can speed up training. However, sometimes you might need to do some performance debugging to make sure your program can make the best use of your accelerators. We are working on a guide to help with this type of debugging, but as a first step here are some things you can try:\r\n- Run an experiment to see if you are getting any speedup just using a single GPU (no MirroredStrategy) compared to just CPU\r\n- Run an experiment to see if you are getting any speedup using 2 GPUs and MirroredStrategy (it's possible that the overhead of synchronizing 6 GPUs is too high given your dataset/model size)\r\n- Follow this guide for using the [Tensorboard Profiler](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras), which can help you identify what could be causing the bottleneck. Often a slowdown like this indicates that your program in input bound.\r\n\r\nLastly, looking at your code I noticed you're using a combination of tensorflow and Keras. `tf.distribute.MirroredStrategy()` is tested with tf.keras and you'll notice that in the guide you are following imports `from tensorflow import keras` \r\nI would suggest modifying your code so that you are only importing from tf.keras and not native keras. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41654\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41654\">No</a>\n"]}, {"number": 41653, "title": "tf.lite.TFLiteConverter produces inconsistent converted model ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.5- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-37224-ga6cd18a133 2.4.0-dev20200722\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n`tf.lite.TFLiteConverter.from_saved_model` converts LSTM to  fused op, but `tf.lite.TFLiteConverter.from_concrete_functions` doesn't.\r\n\r\n**Describe the expected behavior**\r\nThe converted result should be the same for the same model.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n# produce fused LSTM\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Input(shape=(28, 28), name='input'),\r\n    tf.keras.layers.Bidirectional(\r\n        tf.keras.layers.LSTM(20, return_sequences=True)),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.summary()\r\n\r\nrun_model = tf.function(lambda x: model(x))\r\n# This is important, let's fix the input size.\r\nBATCH_SIZE = 1\r\nSTEPS = 28\r\nINPUT_SIZE = 28\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\r\n\r\n# model directory.\r\nMODEL_DIR = \"keras_lstm\"\r\n\r\n# converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('tflite_test.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\n```\r\n# produce original LSTM\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Input(shape=(28, 28), name='input'),\r\n    tf.keras.layers.Bidirectional(\r\n        tf.keras.layers.LSTM(20, return_sequences=True)),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.summary()\r\n\r\nrun_model = tf.function(lambda x: model(x))\r\n# This is important, let's fix the input size.\r\nBATCH_SIZE = 1\r\nSTEPS = 28\r\nINPUT_SIZE = 28\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\r\n\r\n# model directory.\r\nMODEL_DIR = \"keras_lstm\"\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n#converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('tflite_test.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n", "comments": ["This is an intended behavior by design. Do you have any problems because of this behavior?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41653\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41653\">No</a>\n", "It's strange that the dev described an unexpected program behavior as \"intended\". This unexpected behavior is not documented at all and will make users spend hours to debug it."]}, {"number": 41652, "title": "Unable to load graph from protobuf file on s390x arch (Big Endian)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): IBM Z (s390x arch), Ubuntu 18.04.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Installed Tensorflow C lib from source\r\n- TensorFlow version (use command below): 1.15.3\r\n- Python version:\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI installed the TensorFlow C library on an s390x machine. Then tried to do inferencing using a pre-trained model, when I encountered the following error. I'm loading the model graph from a protobuf file.\r\n\r\n```\r\nINFO[0000]/root/gopros/src/github.com/rai-project/tensorflow/vendor/github.com/rai-project/dlframework/framework/register.go:76 github.com/rai-project/tensorflow/vendor/github.com/rai-project/dlframework/framework.Register() skipping regitration of hidden model          pkg=dlframework/framework name=SSD_MobileNet_v2_Quantized_300x300_COCO\r\nINFO[0000] running predict urls                          model=MobileNet_v1_1.0_224 pkg=dlframework/framework/cmd/server\r\nError: unable to create tensorflow model graph: Invalid value in tensor used for shape: -385679360\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nTensorflow should support the loading of model graphs in protobuf format on a big-endian machine.\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nI suspected the error is because the protobuf file for the model graph was originally created on little endian machine and I'm trying to load it on a big-endian machine. In order to confirm that the error is due to the loading of protobuf graph in Tensorflow, I tried out **[this](https://gist.github.com/asimshankar/7c9f8a9b04323e93bb217109da8c7ad2)** tutorial. Using this tutorial, I created a tensorflow model and saved it in protobuf format on an x86 system. When I loaded the graph file on an x86 system, it works but when I tried loading the graph on s390x system, I got the following error -\r\n\r\n```\r\nLoading graph\r\nRead GraphDef of 27083 bytes\r\nERROR: Dimension 0 in both shapes must be equal, but are 1 and 16777216. Shapes are [1,1] and [16777216,16777216]. for 'dense/kernel/Assign' (op: 'Assign') with input shapes: [1,1], [16777216,16777216].\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nThe following tests are passing -\r\n```\r\n//tensorflow/c:c_api_test\r\n//tensorflow/c:c_api_function_test\r\n//tensorflow/c:c_test\r\n//tensorflow/c:ops_test\r\n//tensorflow/c:env_test\r\n//tensorflow/c:c_test_util\r\n//tensorflow/cc/saved_model:reader_test\r\n//tensorflow/cc/saved_model:loader_test\r\n```", "comments": ["This problem stems from a design flaw in TensorFlow's internal data structures.\r\n\r\nIn `tensorflow/core/framework/tensor.cc`, the method `Tensor::FromProto(Allocator*, const TensorProto&)`  reads the `tensor_content` field out of the generated C++ class corresponding to a `TensorProto` message and copies the contents of that field directly into the target `Tensor` object's backing buffer:\r\n\r\n```c++\r\n929    if (!proto.tensor_content().empty()) {\r\n930      const auto& content = proto.tensor_content();\r\n931      CASES_WITH_DEFAULT(proto.dtype(), p = Helper<T>::Decode(a, content, N),\r\n932                         dtype_error = true, dtype_error = true);\r\n933    }\r\n```\r\n([direct link to file in git](https://github.com/tensorflow/tensorflow/blob/66b1247f10bf37a33e5c495eb1523e0895a04ae9/tensorflow/core/framework/tensor.cc#L929))\r\n\r\n**The byte order of the `tensor_content` field is not defined.** The field was never intended to be used for on-disk serialization; see the comment [here](https://github.com/tensorflow/tensorflow/blob/f5fb417ebc18485b7e2493e766d658da539f007c/tensorflow/core/framework/tensor.proto#L35).  However, many different parts of TensorFlow currently store raw binary data in the `tensor_content` field, mostly for reasons of convenience.\r\n\r\nThe `Tensor` class also contains a method `Tensor::AsProtoTensorContent()` that writes the contents of the tensor's buffer directly into the `tensor_content` field of a `TensorProto` message.  One of these places that calls this method is `Input::AsTensorProto()` (see `tensorflow/cc/framework/ops.h`, line 182):\r\n\r\n```c++\r\n182    TensorProto AsTensorProto() {\r\n183      TensorProto tensor_proto;\r\n184      if (tensor.NumElements() > 1) {\r\n185        tensor.AsProtoTensorContent(&tensor_proto);\r\n186      } [...]\r\n```\r\n([direct link to file in git](https://github.com/tensorflow/tensorflow/blob/50caa9d728973399d8b5860dfb5c3a1748bfaf2f/tensorflow/cc/framework/ops.h#L182))\r\n\r\nAs a result of this code and others like it, `TensorProto` protocol buffer messages embedded inside `AttrValue` messages embedded inside `NodeDef` messages embedded inside `FunctionDef` messages embedded inside `FunctionDefLibrary` messages embedded inside `GraphDef` messages embedded inside `MetaGraphDef` messages embedded inside `SavedModel` messages embedded inside the `saved_model.pb` file inside of the root directory of a SavedModel sometimes contain raw binary data in the byte order of the host that produced the SavedModel.\r\n\r\nFixing this problem will be complicated.\r\n\r\nI don't think that tracking down all the places that abuse the `tensor_content` field is an option.  Modifying `Tensor::FromProto()` is likewise not an option, as there is no way to tell what is the byte order of a `TensorProto`'s `tensor_content` field at that point in the code.\r\n\r\nThe best fix I can think of would be to modify the Python code `tf.saved_model.load()` so that, when run on big-endian machines, that code will byte-swap the `tensor_content` fields of any `TensorProto` messages reachable from a traversal of the contents of the SaveModel's `saved_model.pb` file.\r\n\r\nThis change would also require adding similar byte-swapping functionality to `tf.saved_model.save()`. Otherwise, big-endian machines would not be able to import SavedModels that they themselves wrote.", "@frreiss Thank you for looking into the issue.\r\n\r\n@jvishnuvardhan Is the TensorFlow team planning to work on it?", "@frreiss I am running into similar issue on s390x with `TF serving` when it tries to serve models saved on x86 containing `TensorProto` serialized data.  I modified `tf.saved_model.save()` code on s390x as you indicated and it was able to store date in LE format.  Predictably, it failed to load the model on s390x as the bytes were now swapped.\r\n\r\nI am currently investigating if `tensor_content` can be modified right after `ReadMetaGraphDefFromSavedModel` is loaded by [loader.cc](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/cc/saved_model/loader.cc#L316).  Since `TF Serving` does not appear to call Python `tf.saved_model.load()` directly, any changes made there may not be picked up during model load.\r\n\r\nAnother issue I see here is that any models that were generated on s390x prior to  `tf.saved_model.save()` modifications will now fail as there is no way of determining endiness of the model.  I wonder if endiness information can be stored somewhere in  the `MetaGraph`?  The changes made by this [PR](https://github.com/tensorflow/tensorflow/pull/28490) appear to address `tensor bundles` only.\r\n\r\nI am still figuring out aspects of save/load so may be missing few key steps.\r\n\r\nLet me know your thoughts.\r\n\r\nThanks.", "Thanks for sharing that pointer to `loader.cc`, @rposts . It looks like there's a second, C++-only code path for loading SavedModels from non-Python applications, and that this second code path isn't currently reachable from the Python API but is TF Serving's primary way of loading models. I suspect that other languages beyond Python (Java in particular) are probably also exercising that C++-only loading code.\r\n\r\nThis C++ code path will also need to be updated to byte-swap tensors on loading. The change will be similar in nature to what I described above for the Python code. There is most likely a parallel C++-only code path for writing SavedModels that will need some byte-swapping inserted.\r\n\r\nThe good news here is that byte-swapping routines are already present on the C++ code base, at `tensorflow/core/util/tensor_bundle/byte_swap.cc`. The functions in that file would need to be moved to a different directory to be callable from `loader.cc`, but I don't think they would need to change at all.\r\n\r\nIf you need a short-term workaround, my colleagues at IBM have built a utility that byte-swaps the tensors in a SavedModel's serialized protocol buffers file.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41652\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41652\">No</a>\n"]}]