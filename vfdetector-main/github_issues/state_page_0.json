[{"number": 55706, "title": "r2.9 cherry-pick: Fix tf.config.set_visible_device for PluggableDevice", "body": "`tf.config.set_visible_device()` currently doesn't work for PluggableDevice. This cherry-pick fixes it.\r\n\r\nMore details in the original PR: https://github.com/tensorflow/tensorflow/pull/55552 (merged into master on 4/19)", "comments": []}, {"number": 55705, "title": "r2.9 cherry-pick: 39e53cf4f21 \"Remove deployment details from DTensor docstrings\"", "body": "Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/39e53cf4f212f53ac9d9f845d355304c8d596a1d", "comments": []}, {"number": 55704, "title": "TF-TRT Minor Cleaning", "body": "Minor Formatting Fixes for TF-TRT\r\nReview: @bixia1 ", "comments": []}, {"number": 55703, "title": "TypeError: expected str, bytes or os.PathLike object, not GFile", "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.8\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nMacOS 10.15.7\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.7\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\nWhenever I try to load any TensorFlow model, I get this error TypeError: expected str, bytes or os.PathLike object, not GFile\r\n\r\nLoading the model on google collab works without any error. I don't know if it is an OS-specific issue.\r\nI have searched online and I have not seen a case like mine. Please can you help me?\r\n```\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nI don't have any standalone code\r\n```\r\n\r\n\r\n### Relevant log output\r\n```shell\r\n(tf) kofi@Kofis-MacBook-Pro Masters Research % python main.py dataset/mypaperdataset.mp4 -d\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 26, in <module>\r\n    eyes_model = tf.keras.models.load_model('models/tf_eyes_model.h5')\r\n  File \"/Users/kofi/opt/anaconda3/envs/tf/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"/Users/kofi/opt/anaconda3/envs/tf/lib/python3.7/site-packages/h5py/_hl/files.py\", line 309, in __init__\r\n    name = filename_encode(name)\r\n  File \"/Users/kofi/opt/anaconda3/envs/tf/lib/python3.7/site-packages/h5py/_hl/compat.py\", line 111, in filename_encode\r\n    filename = fspath(filename)\r\nTypeError: expected str, bytes or os.PathLike object, not GFile\r\n\r\n```\r\n_No response_</details>", "comments": []}, {"number": 55702, "title": "TFLite doesn't build OOB on VS2019, requires /std:c++20", "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBuild/Install\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ngithub master 185c176\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nWindows 10 x64\r\n\r\n### Mobile device\r\n\r\nN/A\r\n\r\n### Python version\r\n\r\nN/A\r\n\r\n### Bazel version\r\n\r\nN/A\r\n\r\n### GCC/Compiler version\r\n\r\nVS2019 16.11.12\r\n\r\n### CUDA/cuDNN version\r\n\r\nN/A\r\n\r\n### GPU model and memory\r\n\r\nN/A (Intel UHD)\r\n\r\n### Current Behaviour?\r\n\r\nI followed the instructions here: https://www.tensorflow.org/lite/guide/build_cmake, making allowances for windows:\r\n1) CMake 3.23.0-rc2, using cmake-gui\r\n2) Chose tensorflow_src/tensorflow/lite as \"where is the source code\"\r\n3) Chose new folder, tensorflow_src/tflite_build as \"where to build the binaries\"\r\n4) Clicked \"Configure\", chose VS 2019 version in dropdown and left it with other options default.\r\n5) Clicked \"Generate\", then \"Open Project\", which correctly opened in VS\r\n\r\nIn CMake's output, the only warning was in abseil-cpp/CMakeLists.txt:74, \"A future Abseil release will default ABSL_PROPAGATE_CXX_STD to ON for CMake 3.8 and up. We recommend enabling this option to ensure your project still builds correctly\"\r\n\r\nOnce the project opened in VS, I chose \"Release\" build type, and left everything else default, then clicked \"Build Project\"\r\n\r\nThe build failed. Code C7555, \"use of designated initializers requires /srd:c++20\", in project tensorflow-lite, in external_delegate.cc:158.\r\n\r\nI'm unsure whether this is supposed to be a supported build configuration. If not, please close the issue.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nNo code necessary; instructions above.\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nSeverity   Code    Description    Project File    Line    Suppression State\r\nError    C7555    use of designated initializers requires at least '/std:c++20'    tensorflow-lite    C:\\Users\\gbriggs\\src\\tensorflow_src\\tensorflow\\lite\\delegates\\external\\external_delegate.cc    158\r\n```\r\n</details>", "comments": []}, {"number": 55701, "title": "Python Linux 3.10 CPU-only wheel not found", "body": "### Describe the problem\r\n[Link](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow_cpu-2.8.0-cp310-cp310-manylinux2010_x86_64.whl) to **Python Linux 3.10 CPU-only** is not found. \r\n\r\n", "comments": ["@brammerc,\r\n\r\nThank you for reporting."]}, {"number": 55700, "title": "Removes unnecessary MKL contraction kernel flag from mkl_aarch64 build", "body": null, "comments": ["@penpornk - here's a PR to avoid the...\r\n\r\n```\r\n//tensorflow/core/kernels:no_mkldnn_contraction_kernel\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\n```\r\n\r\n...error when running `bazel test` on a build with `--config=mkl_aarch64`.\r\n\r\nIs there any chance this could be cherry-picked for the next TF 2.9 RC?", "@elfringham - I've tested this fix (which @penpornk suggested) to the .bazelrc and it appears to work fine."]}, {"number": 55699, "title": "//tensorflow/tools/docs:tf_doctest fails on machines without GPU", "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nCentOS 7\n\n### Mobile device\n\nn/a\n\n### Python version\n\n3.7.13\n\n### Bazel version\n\n5.1.1\n\n### GCC/Compiler version\n\n10.2.1\n\n### CUDA/cuDNN version\n\nn/a\n\n### GPU model and memory\n\nn/a\n\n### Current Behaviour?\n\n```shell\nUnit test //tensorflow/tools/docs:tf_doctest fails when run on machine without GPU installed.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --noremote_accept_cached --config=nonccl --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --verbose_failures --build_tests_only -- //tensorflow/tools/docs:tf_doctest\n```\n\n\n### Relevant log output\n\n```shell\nMultiple instances similar to\r\n\r\n----------------------------------------------------------------------\r\nFile \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/types/distribute.py\", line 156, in tensorflow.python.types.distribute.DistributedValues\r\nFailed example:\r\n    strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"/opt/python/cp37-cp37m/lib/python3.7/doctest.py\", line 1337, in __run\r\n        compileflags, 1), test.globs)\r\n      File \"<doctest tensorflow.python.types.distribute.DistributedValues[22]>\", line 1, in <module>\r\n        strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/mirrored_strategy.py\", line 287, in __init__\r\n        self, devices=devices, cross_device_ops=cross_device_ops)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/mirrored_strategy.py\", line 342, in __init__\r\n        self._initialize_strategy(devices)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/mirrored_strategy.py\", line 367, in _initialize_strategy\r\n        self._collective_ops = self._make_collective_ops(devices)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/mirrored_strategy.py\", line 385, in _make_collective_ops\r\n        collective_keys=self._collective_keys)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/cross_device_ops.py\", line 1104, in __init__\r\n        group_key, group_size, self._collective_keys, device, options)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/distribute/cross_device_utils.py\", line 271, in __init__\r\n        self._ordering_token = resource_variable_ops.ResourceVariable(0.)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/util/traceback_utils.py\", line 141, in error_handler\r\n        return fn(*args, **kwargs)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/variables.py\", line 268, in __call__\r\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py\", line 1670, in __init__\r\n        validate_shape=validate_shape,\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/ops/resource_variable_ops.py\", line 1817, in _init_from_args\r\n        initial_value, name=\"initial_value\", dtype=dtype)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/profiler/trace.py\", line 183, in wrapped\r\n        return func(*args, **kwargs)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 1640, in convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/tensor_conversion_registry.py\", line 48, in _default_conversion_function\r\n        return constant_op.constant(value, dtype, name=name)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/constant_op.py\", line 268, in constant\r\n        allow_broadcast=True)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/constant_op.py\", line 279, in _constant_impl\r\n        return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/constant_op.py\", line 304, in _constant_eager_impl\r\n        t = convert_to_eager_tensor(value, ctx, dtype)\r\n      File \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/framework/constant_op.py\", line 102, in convert_to_eager_tensor\r\n        return ops.EagerTensor(value, ctx.device_name, dtype)\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0].\r\n----------------------------------------------------------------------\r\nFile \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/types/distribute.py\", line 158, in tensorflow.python.types.distribute.DistributedValues\r\nFailed example:\r\n    dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"/opt/python/cp37-cp37m/lib/python3.7/doctest.py\", line 1337, in __run\r\n        compileflags, 1), test.globs)\r\n      File \"<doctest tensorflow.python.types.distribute.DistributedValues[24]>\", line 1, in <module>\r\n        dataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))\r\n    NameError: name 'strategy' is not defined\r\n----------------------------------------------------------------------\r\nFile \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/types/distribute.py\", line 159, in tensorflow.python.types.distribute.DistributedValues\r\nFailed example:\r\n    per_replica_values = strategy.experimental_local_results(\r\n       distributed_values)\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"/opt/python/cp37-cp37m/lib/python3.7/doctest.py\", line 1337, in __run\r\n        compileflags, 1), test.globs)\r\n      File \"<doctest tensorflow.python.types.distribute.DistributedValues[25]>\", line 1, in <module>\r\n        per_replica_values = strategy.experimental_local_results(\r\n    NameError: name 'strategy' is not defined\r\n----------------------------------------------------------------------\r\nFile \"/root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/tools/docs/tf_doctest.runfiles/org_tensorflow/tensorflow/python/types/distribute.py\", line 161, in tensorflow.python.types.distribute.DistributedValues\r\nFailed example:\r\n    per_replica_values\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"/opt/python/cp37-cp37m/lib/python3.7/doctest.py\", line 1337, in __run\r\n        compileflags, 1), test.globs)\r\n      File \"<doctest tensorflow.python.types.distribute.DistributedValues[26]>\", line 1, in <module>\r\n        per_replica_values\r\n    NameError: name 'per_replica_values' is not defined\r\n\r\n\r\n----------------------------------------------------------------------\n```\n</details>", "comments": ["@cfRod @nSircombe ", "Introduced with commit https://github.com/tensorflow/tensorflow/commit/07e1274c3cd9c5cf52f6c6013c1205fc6e15473c"]}, {"number": 55698, "title": "TFLite Tensor name converted by 2.7.0 is much longer than 1.14.0", "body": "### System information\r\n\r\n-   **OS Platform and Distribution**: Linux Ubuntu 18.04\r\n-   **Mobile device**: No\r\n-   **TensorFlow installed from (source or binary)**: binary, pip install tensorflow==2.7.0\r\n-   **TensorFlow version**: 2.7.0\r\n-   **Python version**: 3.8.13\r\n-   **Bazel version (if compiling from source)**: None\r\n-   **GCC/Compiler version (if compiling from source)**: None\r\n-   **CUDA/cuDNN version**: None\r\n-   **GPU model and memory**: None\r\n-   **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\nUsing tensorflow 1.14.0 convert .pb model to tflite as follows:\r\n```python\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = '/path/to/mobilenet_v1.pb'\r\ninput_arrays = ['input']\r\noutput_arrays = ['MobilenetV1/Predictions/Reshape_1']\r\ninput_shapes = {'input': [1, 224, 224, 3]}\r\n\r\nconvert = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes)\r\ntflite_model = convert.convert()\r\nwith open('mobilenet_v1_tf1.14.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\nAfter update tensorflow to 2.7.0 and modify scripts as follows:\r\n```python\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = '/path/to/mobilenet_v1.pb'\r\ninput_arrays = ['input']\r\noutput_arrays = ['MobilenetV1/Predictions/Reshape_1']\r\ninput_shapes = {'input': [1, 224, 224, 3]}\r\n\r\nconvert = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes)\r\ntflite_model = convert.convert()\r\nwith open('mobilenet_v1_tf2.7.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\nI found tensor name in `mobilenet_v1_tf2.7.tflite` is much longer than `mobilenet_v1_tf1.14.tflite`. Such a long name caused a lot of trouble when comparing the tflite model with the original model.\r\n\r\nHowever, I also noticed that there will be a new `Attributes` in TFLiteConverter.\r\n```python\r\nconvert = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes)\r\nconvert.experimental_new_convert = False\r\ntflite_model = convert.convert()\r\n```\r\nBut the log as follows warning me that old converter is deprecated:\r\n```shell\r\nWARNING:absl:Please consider switching to the new converter by setting experimental_new_converter=True. The old converter (TOCO) is deprecated.\r\n```\r\nIs is possible to add an attribute in the new version, so that the model conversion name can remain unchanged, and the tensor of folder const can be removed.\r\n\r\n", "comments": ["Hi @jelly0o0 ! I could not see any difference between the names of tensors from lite files procured from 1.x converter and 2.x converter . Could you post a gist to reproduce the issue? Attaching gist in [1.14 ](https://colab.sandbox.google.com/gist/mohantym/d03b95a958aa2f04fb27f011f79aaa98/tflite.ipynb#scrollTo=v5Zf6G4M-sZz)and [2.8](https://colab.sandbox.google.com/gist/mohantym/4a880ccc5b166fb88b6c1c2a24320165/tflite.ipynb#scrollTo=iE0vSfMXumKI)  for reference. ", "Download [mobilenet_v1.pb](https://github.com/jelly0o0/learngit/blob/master/mobilenet_v1.pb)\r\nUsing tensorflow 1.14.0 convert to [mobilenet_v1_tf1.14.tflite](https://github.com/jelly0o0/learngit/blob/master/mobilenet_v1_tf1.14.tflite):\r\n![mobilenet_v1_tf1 14 tflite](https://user-images.githubusercontent.com/33118067/164487323-8bbecf3b-2931-4b95-9cdb-7f1d324e3127.svg)\r\n\r\nUsing tensorflow 2.7.0 convert to [mobilenet_v1_tf2.7.tflite](https://github.com/jelly0o0/learngit/blob/master/mobilenet_v1_tf2.7.tflite):\r\n![mobilenet_v1_tf2 7 tflite](https://user-images.githubusercontent.com/33118067/164487335-34f737c6-bbe2-4179-8d26-78bf429f02b0.svg)\r\n"]}, {"number": 55697, "title": "Fix error in tf.linalg.triangular_solve docstring", "body": "The description for adjoint=True vs False was the wrong way around. I've also added a (perhaps more natural) explanation using matmul directly.", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55697/checks?check_run_id=6108097412).", "@gbaned Hi, I am not able to sign the Google CLA because the form claims that my GitHub username is invalid!\r\n(It may be because I've got a very old GitHub account from before they changed it to disallow final dashes...)", "@st--  Can you please make sure to use same GitHub username and email-id associated with it.  Thank you.", "The problem might be because you use a github-noreply private email.\r\n\r\nUntil CLA is signed, we cannot review the PR"]}, {"number": 55696, "title": "tf.nn.embedding_lookup_sparse doesn't work with @tf.function(jit_compile=True)", "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.8.0\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nColab Notebook\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nTried to use `tf.nn.embedding_lookup_sparse` within a function decorated with `@tf.function(jit_compile=True)` and it failed.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\ntry:\r\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\r\nexcept ValueError:\r\n  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\r\n\r\ntf.config.experimental_connect_to_cluster(tpu)\r\ntf.tpu.experimental.initialize_tpu_system(tpu)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n\r\n\r\n@tf.function(jit_compile=True)\r\ndef run_embedding_bag(params, sp_ids):\r\n  return tf.nn.embedding_lookup_sparse(\r\n    params, sp_ids, None, combiner='sum', max_norm=None, name=None\r\n  )\r\n\r\n\r\nwith tf.device('/TPU:0'):\r\n  params = tf.random.uniform([1000, 64])\r\n  sp_ids = tf.sparse.SparseTensor([[0, 0], [0, 2]], values=[1, 2], dense_shape=[3, 4])\r\n  res = run_embedding_bag(params, sp_ids)\r\n\r\nprint(res.shape)\r\nprint(run_embedding_bag.experimental_get_compiler_ir(params, sp_ids)(stage='hlo'))\n```\n\n\n### Relevant log output\n\n```shell\nInvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_run_embedding_bag_978[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263] on XLA_TPU_JIT: SparseSegmentSum (No registered 'SparseSegmentSum' OpKernel for XLA_TPU_JIT devices compatible with node {{node embedding_lookup_sparse}}){{node embedding_lookup_sparse}}\r\nOne approach is to outside compile the unsupported ops to run on CPUs by enabling soft placement `tf.config.set_soft_device_placement(True)`. This has a potential performance penalty.\n```\n</details>", "comments": []}, {"number": 55695, "title": "Non-deterministic behavior when Run on Different GPU in different Machines", "body": "Hello everyone, I would like to ask a determinism Issue with **Tensorflow version 1.5**\r\n\r\nI tried to run some ML-model without tf.data.Dataset. (As i know there is some random in tf.data.Dataset) I also tried to add:\r\nos.environ['TF_CUDNN_DETERMINISTIC'] = '1'\r\nrandom.seed(1)\r\nnp.random.seed(1)\r\ntf.set_random_seed(1)\r\n\r\nThe results is that I can reproduce the results with 100% similarity in the **SAME** machine. However, I cannot reproduce the results with 100% similarity in other machine.\r\nDue to some issue in my old GPU, I cannot upgrade the **tensorflow** version, so we keep the version of 1.5\r\nIn our case, we try to run ML-model (Machine Learning - with Convolutional Neural Networks model) in two different Machines, one with the **GPU**:\r\n**GeForce RTX 2070\r\nNVIDIA-SMI 440.33.01    \r\nDriver Version: 440.33.01    \r\nCUDA Version: 10.2**\r\n\r\nAnother machine with the **GPU**:\r\n**GeForce RTX 2080\r\nNVIDIA-SMI 460.91.03    \r\nDriver Version: 460.91.03    \r\nCUDA Version: 11.2**  \r\n\r\nThe ML-results shows differently with these two machine. I hear about the difference of hard-ware version/**CUDA** version might cause the random results. May I ask that can anyone fix these issue? OR if the issue really cannot fix, which one has a bigger impact, difference of **GPU** hardware or **CUDA** version?\r\n\r\nIn other words, if the **GPUs** are the same, but **CUDAs** version are not the same, is it better than same **CUDA** with different **GPU**?\r\n\r\nThank you very much.\r\n", "comments": ["Hello @hontimzam, \r\nWe see that you are using tf version 1.5.0, 1.x is not actively supported, I request you,please update to latest v2.x and let us know if you are facing same issue.\r\nAlso please refer this [doc link](https://github.com/tensorflow/docs/blob/master/site/en/install/source.md#gpu) for the compatible CUDA and cudNN configurations for v1.5.0.Thanks!"]}, {"number": 55693, "title": "Replacing Add Layer with Linear Combination of Learnable Weights", "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nFeature Request\r\n\r\nThe Add() Layer can be expressed as a linear combination of inputs where each coefficient value is 1. \r\n\r\nI was trying to make a custom layer that performs the same as Add, except uses trainable coefficients as we assume a coefficient of 1 is optimal. Unfortunately, I was unable to do so on my own, specifically the gradients. \r\n\r\nI do not think this would be too hard to implement as all the code is kind of already there, just need to calculate gradients and then update.\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.7\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\nThe Add() Layer can be expressed as a linear combination of inputs where each coefficient value is 1. \r\n\r\nI was trying to make a custom layer that performs the same as Add, except uses trainable coefficients as we assume a coefficient of 1 is optimal. Unfortunately, I was unable to do so on my own, specifically the gradients. \r\n\r\nI do not think this would be too hard to implement as all the code is kind of already there, just need to calculate gradients and then update.\r\n```\r\n\r\n\r\n### Standlone code to reproduce the issue\r\n\r\n```shell\r\nx\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_</details>", "comments": ["@OUStudent ,\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!"]}, {"number": 55692, "title": "GPT-J saved_model conversion to fp16 TF-TRT errors with ``Message tensorflow.GraphDef exceeds maximum protobuf size of 2GB: 23499914257\"", "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.8\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nUbuntu 20.04.3 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n11.4/8.1.0\r\n\r\n### GPU model and memory\r\n\r\nA100 40GB\r\n\r\n### Current Behaviour?\r\n\r\nI tried converting a saved_model of GPT-J using TF-TRT to an FP16 version. I ran into a problem with the protobuf size limit (log below), it seems the model is too large even though saving works correctly. The standalone code was executed using the tensorflow:latest-gpu image. The saved_model can be produced with the following snippet\r\n```shell\r\nfrom transformers import TFGPTJForSequenceClassification\r\n\r\nmodel = TFGPTJForSequenceClassification.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", from_pt=True)\r\n# the saved_model parameter is a flag to create a SavedModel version of the model in same time than the h5 weights\r\nmodel.save_pretrained(\"sequence_fp16_model\", saved_model=True)\r\n```\r\nExpected Behaviour:\r\nThe saved_model should be converted into a TF-TRT version.\r\n\r\n\r\n### Standlone code to reproduce the issue\r\n\r\n```shell\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir='<home_dir>/sequence_model/saved_model/1', precision_mode='FP16', use_dynamic_shape=True)\r\nconverter.convert()\r\nconverter.save('<home_dir>/seqfp16_model/saved_model/1')\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2022-04-20 21:10:56.139648: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-04-20 21:10:56.733368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\r\n2022-04-20 21:11:38.182109: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2022-04-20 21:11:38.182325: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\r\n2022-04-20 21:11:38.184816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\r\n2022-04-20 21:11:39.502737: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: Graph size after: 18132 nodes (17815), 21036 edges (20719), time = 829.118ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.075ms.\r\n\r\nTraceback (most recent call last):\r\n  File \"convert_to_fp16.py\", line 3, in <module>\r\n    converter.convert()\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1201, in convert\r\n    frozen_func = convert_to_constants.convert_variables_to_constants_v2(func)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 1151, in convert_variables_to_constants_v2\r\n    return _construct_concrete_function(func, output_graph_def,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 1076, in _construct_concrete_function\r\n    new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 655, in function_from_graph_def\r\n    wrapped_import = wrap_function(_imports_graph_def, [])\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 619, in wrap_function\r\n    func_graph.func_graph_from_py_func(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\", line 1161, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 83, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 89, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 649, in _imports_graph_def\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py\", line 548, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py\", line 403, in import_graph_def\r\n    return _import_graph_def_internal(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py\", line 497, in _import_graph_def_internal\r\n    with c_api_util.tf_buffer(graph_def.SerializeToString()) as serialized:\r\nValueError: Message tensorflow.GraphDef exceeds maximum protobuf size of 2GB: 23499914257\r\n```\r\n</details>", "comments": []}, {"number": 55690, "title": "Fix Windows build break in mhlo", "body": "`ShapedType::getRank()` returns an `int64_t`, so no reason to use `uint` which is not a supported type on Windows.", "comments": ["@PatriceVignola Can you please resolve conflicts? Thank you!", "> @PatriceVignola Can you please resolve conflicts? Thank you!\r\n\r\nHmm looks like the commit that introduced the build break got reverted, so there's no need for this fix anymore."]}, {"number": 55689, "title": "Enable gpu compat memory on for output allocation dataset ops", "body": "This MR is a follow up to [55594](https://github.com/tensorflow/tensorflow/pull/55594), and the potential merge should happen afterwards. The goal is to activate the use_gpu_compat_allocator attribute to allocate gpu compatible memory directly prior to the prefetch_to_device op. The list of affected ops are:\r\nRepeatDataset, MapDataset, ParallelMapDataset, ParallelMapDatasetV2, InterleaveDataset, ParallelInterleaveDatasetV2, ParallelInterleaveDatasetV3, ParallelInterleaveDatasetV4, FilterDataset, ParallelFilterDataset, BatchDataset, BatchDatasetV2, ParallelBatchDataset, ShardDataset, PaddedBatchDataset, PaddedBatchDatasetV2, RangeDataset\r\nAttn: @changhuilin", "comments": []}, {"number": 55687, "title": "[oneDNN] Blocking MatMul+Add(bias) fusion until it is properly tested", "body": "This PR blocks the fusion of Matmul + Add when bias dims are more than one. This fusion was enabled earlier by a change that was meant to enable Conv3D + Add (PR https://github.com/tensorflow/tensorflow/pull/53299) which updated common code [here](https://github.com/tensorflow/tensorflow/pull/53299/files#diff-65c38c4abfc2cbfc01ab06f743b296203cc20a630919f65fd003b4e74632370cR518-R537) that enabled fusion for MatMul as well. we are blocking that fusion until it is properly tested.", "comments": []}, {"number": 55686, "title": "[TF-TRT] Adding Graph Visualization Export Instrumentation", "body": "This PR introduces a  TF-TRT graph visualization export tool in `graphviz` format.\r\n\r\nThe user can then convert the `graphviz` file in `.dot` format to a more commonly readable format like `PNG`.\r\n\r\nExample `Electra TF2`:\r\n\r\n![image](https://user-images.githubusercontent.com/10923599/164325039-0f3872e1-399a-4acc-83b1-109812257e9f.png)\r\n\r\n\r\nUsage: `TF_TRT_EXPORT_GRAPH_VIZ_PATH=/path/to/mymodel.dot python main.py`", "comments": ["@bixia1 for review\r\nCC: @tfeher @christopherbate "]}, {"number": 55685, "title": "Integrating cuBLASLt into TF", "body": "Adds support for the cuBLASLt library for GEMM operations to TF native. The library can be activated by setting the environment variable `TF_USE_CUBLASLT=1`.\r\n\r\nContinuation of PR #55518 which integrated cuBLASLt into XLA.", "comments": []}, {"number": 55682, "title": "XLA: softmax with numerically masked inputs does not match its non-XLA counterpart on CPU", "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.8\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.10\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n11.4\n\n### GPU model and memory\n\nT4 (16 GB)\n\n### Current Behaviour?\n\n```shell\nTL;DR if a function contains a softmax after a numerically masked input (i.e. after adding a very large negative penalty), and that function is compiled with `tf.function(jit_compile=True)`, its CPU output is very different from its non compiled counterpart. On GPU, the two outputs are very close.\r\n\r\nLong description:\r\nOn NLP models, like `T5`, it is common to have multiple inputs in the same batch with different lengths. To handle them without `RaggedTensors`, we pad the shortest entries in the batch to the longest length, and send the padded tokens along an attention mask (containing 0s where padding happened, 1s otherwise). Internally, in the model, we convert this binary attention mask to a numerical one, containing a large penalty (e.g. `-1e9` in Hugging Face's `T5`) that will be added to the input of the attention layers' softmax, such that no attention is given to padding. As such, this numerically masked softmax is a common operation in large language models.\r\n\r\nAs part of our efforts to speed up text generation at Hugging Face, we struggled to reproduce forward passes of some models when they were compiled with `tf.function(jit_compile=True)` (https://github.com/huggingface/transformers/issues/16838). Upon further inspection, we noticed that:\r\n- The problematic behavior was CPU-only;\r\n- The issue came from the softmax operation, but only when masking was present;\r\n- It is present even when the attention mask contains all 1s, meaning that the large negative penalty doesn't get applied at all to the inputs;\r\n- Reducing the large penalty to a not so large penalty, like `-100`, still results in noticable mismatches.\r\n\r\nThe snippet below gives a simple example where the problem described above can be seen -- it passes on GPU but fails on CPU. Because it is easily reproducible, I'm not including the XLA compilation files (as suggested [here](https://www.tensorflow.org/xla#reproducible_bug_reports)), but feel free to request them :)\n```\n\n\n### Standlone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n\r\n# same outcome for values smaller than -1e3\r\nLARGE_PENALTY = -1e9\r\n\r\n\r\ndef simple_softmax(x):\r\n    return tf.nn.softmax(x)\r\n\r\n\r\ndef masked_softmax(x, boolean_mask):\r\n    numerical_mask = (1. - tf.cast(boolean_mask, dtype=tf.float32)) * LARGE_PENALTY\r\n    masked_x = x + numerical_mask\r\n    return tf.nn.softmax(masked_x)\r\n\r\n\r\nxla_masked_softmax = tf.function(masked_softmax, jit_compile=True)\r\nxla_simple_softmax = tf.function(simple_softmax, jit_compile=True)\r\nx = tf.random.normal((1, 10))\r\n\r\n# same outcome regardless of the boolean mask here\r\nboolean_mask = tf.convert_to_tensor([[1] * 9 + [0] * 1], dtype=tf.int32)\r\n\r\n# masks input outside of the compiled softmax -> works correctly on CPU and GPU\r\nnumerical_mask = (1. - tf.cast(boolean_mask, dtype=tf.float32)) * LARGE_PENALTY\r\nmasked_x = x + numerical_mask\r\nxla_out = xla_simple_softmax(masked_x)\r\nout = simple_softmax(masked_x)\r\nprint(tf.math.reduce_max(tf.math.abs(xla_out - out)).numpy())\r\nassert tf.experimental.numpy.allclose(xla_out, out)\r\n\r\n# masked_softmax -> fails regardless of the mask on CPU, works correctly on GPU\r\nxla_out = xla_masked_softmax(x, boolean_mask)\r\nout = masked_softmax(x, boolean_mask)\r\nprint(tf.math.reduce_max(tf.math.abs(xla_out - out)).numpy())\r\nassert tf.experimental.numpy.allclose(xla_out, out)\n```\n\n\n### Relevant log output\n\n_No response_</details>", "comments": ["cc @Rocketknight1\r\n\r\nRelated issue: https://github.com/huggingface/transformers/issues/16838"]}, {"number": 55681, "title": "Improper expression in \u201ctf.keras.layers.Embedding\u201d \u2018s function introduction", "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nOthers\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\ntf 2.8\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\ntf.keras.layers.Embedding Turns positive integers (indexes) into dense vectors of fixed size.I think \"positive integers\" should be revised to \"non-negative integers\",because index \"0\" is not a positive integer. \"positive integers\" is a little improper from this perspective.\n```\n\n\n### Standlone code to reproduce the issue\n\n```shell\ninputs for tf.keras.layers.Embedding could have \"0\",but it's not a positive integer.\n```\n\n\n### Relevant log output\n\n_No response_</details>", "comments": ["@Kimtaehung123 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!"]}, {"number": 55680, "title": "build tflite-2.7.1 android_arm64 shared library failed", "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.7\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\nAndroid\r\n\r\n### Python version\r\n\r\n 3.8.10\r\n\r\n### Bazel version\r\n\r\n5.1.0\r\n\r\n### GCC/Compiler version\r\n\r\nclang version 9.0.9\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\n./build_tflite-2.7.1_android_arm64_so.sh \r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nINFO: Elapsed time: 1727.329s, Critical Path: 629.53s\r\nINFO: 1906 processes: 108 internal, 1798 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n### Standlone code to reproduce the issue\r\n\r\n```shell\r\nModifying tensorflow/lite/BUILD as follows:\r\ntflite_cc_shared_object(\r\n    name = \"libtensorflowlite.so\",\r\n    linkopts = select({\r\n        \"//tensorflow:macos\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/lite:tflite_exported_symbols.lds)\",\r\n            \"-Wl,-install_name,@rpath/libtensorflowlite.so\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)\",\r\n        ],\r\n    }),\r\n    deps = [\r\n        \":framework\",\r\n        \":tflite_exported_symbols.lds\",\r\n        \":tflite_version_script.lds\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n        \"//tensorflow/lite/delegates/flex:delegate\",\r\n    ],\r\n)\r\n\r\nRunning the following command in Ubuntu 20.04 shell.\r\nbazel build --jobs 6 --verbose_failures -c opt --config=monolithic --config=android_arm64 --cpu=arm64-v8a --define=tflite_convert_with_select_tf_ops=true --define=with_select_tf_ops=true //tensorflow/lite:libtensorflowlite.so\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n./build_tflite-2.7.1_android_arm64_so.sh \r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=101\r\nINFO: Reading rc options for 'build' from /home/andyueng/samba/workspece_TensorFlow2/tensorflow-2.7.0_android_arm64/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/andyueng/samba/workspece_TensorFlow2/tensorflow-2.7.0_android_arm64/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /home/andyueng/samba/workspece_TensorFlow2/tensorflow-2.7.0_android_arm64/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env ANDROID_NDK_HOME=/home/andyueng/Android/Sdk/ndk/21.4.7075529 --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=30.0.3 --action_env ANDROID_SDK_API_LEVEL=30 --action_env ANDROID_SDK_HOME=/home/andyueng/Android/Sdk\r\nINFO: Reading rc options for 'build' from /home/andyueng/samba/workspece_TensorFlow2/tensorflow-2.7.0_android_arm64/.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /home/andyueng/samba/workspece_TensorFlow2/tensorflow-2.7.0_android_arm64/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/andyueng/samba/workspece_TensorFlow2/tensorflow-2.7.0_android_arm64/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file /home/andyueng/samba/workspece_TensorFlow2/tensorflow-2.7.0_android_arm64/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:android_arm64 in file /home/andyueng/samba/workspece_TensorFlow2/tensorflow-2.7.0_android_arm64/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nINFO: Found applicable config definition build:android in file /home/andyueng/samba/workspece_TensorFlow2/tensorflow-2.7.0_android_arm64/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --define=with_xla_support=false\r\nINFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (156 packages loaded, 13239 targets configured).\r\nINFO: Found 1 target...\r\n[1,739 / 3,099] 6 actions, 2 running\r\n    Compiling tensorflow/core/common_runtime/process_function_library_runtime.cc; 7s local\r\n    Compiling tensorflow/core/platform/scanner.cc; 0s local\r\n    [Sched] Compiling tensorflow/core/c[1,739 / 3,099] 6 actions, 2 running\r\n    Compiling tensorflow/core/common_runtime/process_function_library_runtime.cc; 8s local\r\n    Compiling tensorflow/core/platform/scanner.cc; 1s local\r\n    [Sched] Compiling tensorflow/core/c[1,740 / 3,099] 6 actions, 2 running\r\n    Compiling tensorflow/core/common_runtime/process_function_library_runtime.cc; 8s local\r\n    Compiling tensorflow/core/common_runtime/process_state.cc; 0s local\r\n    [Sched] Compiling tensorflow/core/p[1,740 / 3,099] 6 actions, 2 running\r\n    Compiling tensorflow/core/common_runtime/process_function_library_runtime.cc; 9s local\r\n    Compiling tensorflow/core/common_runtime/process_state.cc; 0s local\r\nERROR: /home/andyueng/samba/workspece_TensorFlow2/tensorflow-2.7.0_android_arm64/tensorflow/core/kernels/BUILD:6804:11: C++ compilation of rule '//tensorflow/core/kernels:portable_tensorflow_kernels' failed (Exit 254): clang failed: error executing command \r\n  (cd /home/andyueng/.cache/bazel/_bazel_andyueng/927465b164a8f43413ec0d8510052d3f/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=30.0.3 \\\r\n    ANDROID_NDK_API_LEVEL=21 \\\r\n    ANDROID_NDK_HOME=/home/andyueng/Android/Sdk/ndk/21.4.7075529 \\\r\n    ANDROID_SDK_API_LEVEL=30 \\\r\n    ANDROID_SDK_HOME=/home/andyueng/Android/Sdk \\\r\n    PATH=/home/andyueng/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android '-D__ANDROID_API__=21' -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig '-Werror=return-type' '-Werror=int-to-pointer-cast' '-Werror=pointer-to-int-cast' '-Werror=implicit-function-declaration' -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/gather_op.pic.d '-frandom-seed=bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/gather_op.pic.o' -fPIC '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' '-DS_IEXEC=S_IXUSR' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DSUPPORT_SELECTIVE_REGISTRATION -iquote . -iquote bazel-out/arm64-v8a-opt/bin -iquote external/gif -iquote bazel-out/arm64-v8a-opt/bin/external/gif -iquote external/eigen_archive -iquote bazel-out/arm64-v8a-opt/bin/external/eigen_archive -iquote external/com_google_absl -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/arm64-v8a-opt/bin/external/nsync -iquote external/libjpeg_turbo -iquote bazel-out/arm64-v8a-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/arm64-v8a-opt/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/arm64-v8a-opt/bin/external/double_conversion -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm64-v8a-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm64-v8a-opt/bin/external/farmhash_archive -iquote external/png -iquote bazel-out/arm64-v8a-opt/bin/external/png -iquote external/highwayhash -iquote bazel-out/arm64-v8a-opt/bin/external/highwayhash -iquote external/icu -iquote bazel-out/arm64-v8a-opt/bin/external/icu -iquote external/fft2d -iquote bazel-out/arm64-v8a-opt/bin/external/fft2d -iquote external/gemmlowp -iquote bazel-out/arm64-v8a-opt/bin/external/gemmlowp -isystem external/gif -isystem bazel-out/arm64-v8a-opt/bin/external/gif -isystem external/eigen_archive -isystem bazel-out/arm64-v8a-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/arm64-v8a-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/arm64-v8a-opt/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/arm64-v8a-opt/bin/external/double_conversion -isystem external/farmhash_archive/src -isystem bazel-out/arm64-v8a-opt/bin/external/farmhash_archive/src -isystem external/png -isystem bazel-out/arm64-v8a-opt/bin/external/png -isystem external/icu/icu4c/source/common -isystem bazel-out/arm64-v8a-opt/bin/external/icu/icu4c/source/common -w '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions -DTENSORFLOW_MONOLITHIC_BUILD -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64' -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/core/kernels/gather_op.cc -o bazel-out/arm64-v8a-opt/bin/tensorflow/core/kernels/_objs/portable_tensorflow_kernels/gather_op.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nclang: error: unable to execute command: Killed\r\nclang: error: clang frontend command failed due to signal (use -v to see invocation)\r\nAndroid (7019983 based on r365631c3) clang version 9.0.9 (https://android.googlesource.com/toolchain/llvm-project a2a1e703c0edb03ba29944e529ccbf457742737b) (based on LLVM 9.0.9svn)\r\nTarget: aarch64-none-linux-android\r\nThread model: posix\r\nInstalledDir: external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin\r\nclang: note: diagnostic msg: PLEASE submit a bug report to https://github.com/android-ndk/ndk/issues and include the crash backtrace, preprocessed source, and associated run script.\r\nclang: note: diagnostic msg: \r\n********************\r\n\r\nPLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:\r\nPreprocessed source(s) and associated run script(s) are located at:\r\nclang: note: diagnostic msg: /tmp/gather_op-af1fc4.cpp\r\nclang: note: diagnostic msg: /tmp/gather_op-af1fc4.sh\r\nclang: note: diagnostic msg: \r\n\r\n********************\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nINFO: Elapsed time: 1727.329s, Critical Path: 629.53s\r\nINFO: 1906 processes: 108 internal, 1798 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n</details>", "comments": ["Hi @chunduriv ! Could you please look at this issue? It is replicating in [2.6.3](https://colab.sandbox.google.com/gist/mohantym/7efb22a34d3c7fa62df488941dc93922/git_55680.ipynb#scrollTo=zrSNPkicxMXH) , [2.7 ](https://colab.sandbox.google.com/gist/mohantym/cfa03ededb7130ee628fc20091c21ab2/git_55680.ipynb#scrollTo=2-ABAXDTvlJ3) and[ 2.8](https://colab.sandbox.google.com/gist/mohantym/b5aa9c41f39c7421177367f66c932f4b/git_55680.ipynb#scrollTo=UJZG2dEzvTzO) . Thanks!"]}, {"number": 55679, "title": "Android - Does not encode a valid TensorFlow Lite model.", "body": "Previously I was using TensorFlowInferenceInterface to read the model. but I was facing a lot of crashes. As per one of the moderator's recommendation, i replaced the method with the following:\r\n\r\n**Code:**\r\n```\r\n private var sTFInterface: Interpreter? = null\r\n private const val MODEL_FILE = \"file:///android_asset/auto.pb\"\r\n\r\nsTFInterface = Interpreter(File(MODEL_FILE))\r\n```\r\nBut now i see a very strange error and it is unable to read tensorflowlite model. Still \"abort\" crashes are on spike.\r\n\r\n**Error:**\r\n> java.lang.IllegalArgumentException: Contents of /file:/android_asset/auto.pb does not encode a valid TensorFlow Lite model: Could not open '/file:/android_asset/auto.pb'.\r\n> The model allocation is null/empty\r\n\r\nPlease help\r\n\r\nAttached model:\r\n[auto.pb.zip](https://github.com/tensorflow/tensorflow/files/8520578/auto.pb.zip)\r\n\r\n", "comments": ["Hi @ahmadbajwa8282 ! You should use below command to convert frozen graph files to Tflite model. \r\n```\r\n converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file, input_arrays, output_arrays)\r\ntflite_model = converter.convert()\r\n```\r\nBut I recommend creating the model in V2 api's and load TF 2.x Lite converter to load the model and convert it smoothly. \r\nAttaching [TF 1.x Lite converter ](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter), [2.x Lite converter ](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter#from_saved_model)guide  for reference.\r\n\r\nThanks!"]}, {"number": 55678, "title": "Add a validate_shape argument to `TF_AssignVariable`", "body": "The `TF_AssignVariable` function is missing a `validate_shape` argument that is available for native devices. Adding this argument breaks backward compatibility of the API, but this is necessary since C doesn't support function overloads. Since the header is still in an experimental state, I believe now is the right time to add those kind of arguments.", "comments": []}, {"number": 55677, "title": "Return an error code for `TF_GetInputTensorFromVariable`", "body": "Currently, because of `OP_REQUIRES_OK` calls, we always return early from the function before setting the error code when something goes wrong. This is non intuitive as it makes the `status` argument to always be `Status::OK()`, while the status of the context will have an error. This also forces the user to check the status of the context after returning from this function, but checking the value of the status argument itself should be enough.", "comments": []}, {"number": 55676, "title": "Would like to build for ppc64el, is this possible?", "body": "<details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBuild/Install\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.8\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nRedhat Enterprise Linux 8\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\nbazel4.2.1\n\n### GCC/Compiler version\n\n8.5\n\n### CUDA/cuDNN version\n\n11.2/8.4\n\n### GPU model and memory\n\nNvidia P100 x4 16GB\n\n### Current Behaviour?\n\n```shell\nTrying to build on IBM Power8 Server (8335-GTB) using ppc64el arch.\n```\n\n\n### Standlone code to reproduce the issue\n\n```shell\nbazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\n\n### Relevant log output\n\n```shell\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: /home/theskaz/.cache/bazel/_bazel_theskaz/70be98ca39496e399681fb572266510d/external/cpuinfo/BUILD.bazel:100:11: Configurable attribute \"srcs\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:\r\n @cpuinfo//:linux_x86_64\r\n @cpuinfo//:linux_arm\r\n @cpuinfo//:linux_armhf\r\n @cpuinfo//:linux_armv7a\r\n @cpuinfo//:linux_armeabi\r\n @cpuinfo//:linux_aarch64\r\n @cpuinfo//:linux_mips64\r\n @cpuinfo//:linux_riscv64\r\n @cpuinfo//:linux_s390x\r\n @cpuinfo//:macos_x86_64\r\n @cpuinfo//:macos_arm64\r\n @cpuinfo//:windows_x86_64\r\n @cpuinfo//:android_armv7\r\n @cpuinfo//:android_arm64\r\n @cpuinfo//:android_x86\r\n @cpuinfo//:android_x86_64\r\n @cpuinfo//:ios_x86_64\r\n @cpuinfo//:ios_x86\r\n @cpuinfo//:ios_armv7\r\n @cpuinfo//:ios_arm64\r\n @cpuinfo//:ios_arm64e\r\n @cpuinfo//:watchos_x86_64\r\n @cpuinfo//:watchos_x86\r\n @cpuinfo//:watchos_armv7k\r\n @cpuinfo//:watchos_arm64_32\r\n @cpuinfo//:tvos_x86_64\r\n @cpuinfo//:tvos_arm64\r\n @cpuinfo//:emscripten_wasm\n```\n</details>", "comments": ["@hicotton02 Could you please have a look at this [link](https://www.tensorflow.org/install/source) and  check the tested build configurations.Please let us know if it helps?\r\nThanks!"]}, {"number": 55675, "title": "[TRT] TF 2.8.0 EfficientDet D0 TRT conversion failed - Unable to save gradient functions", "body": "<details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.8.0\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux Ubuntu 18.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.7\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN/TensorRT version\r\n\r\n11.2 / 8.1.1.33 / 7.2.3-1\r\n\r\n### GPU model and memory\r\n\r\nNvidia Tesla T4 16GB (aws g4dn.2xlarge)\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\nTF 2.8.0\r\nTRT converter can convert EfficientDet D0 model BUT failed to save it.\r\n\r\n# Error\r\nValueError: Unable to save gradient functions when exporting a _DefinedFunction (generally created through graph freezing utils or through V1 graph importers). Please save with `options=tf.SaveOptions(experimental_custom_gradients=False)`\r\n\r\nTRT conversion and saving work fine in TF 2.4.4, 2.7.1 and 2.9.0-rc0\r\n```\r\n\r\n\r\n### Standlone code to reproduce the issue\r\n\r\nDownload EfficientDet D0 model from [TensorFlow Hub](https://tfhub.dev/tensorflow/efficientdet/d0/1)\r\nExtract model tar.gz to efficientdet_d0 folder\r\n```python\r\n# Convert the model\r\nimport tensorflow as tf\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nmmm=\"efficientdet_d0\"\r\nconversion_params = trt.TrtConversionParams()\r\nconverter = trt.TrtGraphConverterV2(\r\n    input_saved_model_dir=mmm,\r\n    conversion_params=conversion_params)\r\nconverter.convert()\r\nconverter.save(mmm + \"_trt_fp32\")\r\n```\r\n```shell\r\n# Error\r\nValueError: Unable to save gradient functions when exporting a _DefinedFunction (generally created through graph freezing utils or through V1 graph importers). Please save with `options=tf.SaveOptions(experimental_custom_gradients=False)`\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2022-04-20 04:39:57.385230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:39:57.391168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:39:57.393240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:39:57.395527: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-04-20 04:39:57.396021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:39:57.398117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:39:57.400194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:39:58.084849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:39:58.086283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:39:58.087478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:39:58.088632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13598 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\r\n2022-04-20 04:40:19.154659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:19.156146: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2022-04-20 04:40:19.156320: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\r\n2022-04-20 04:40:19.156736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:19.158083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:19.159358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:19.160688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:19.161965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:19.163204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13598 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\r\n2022-04-20 04:40:19.873601: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: Graph size after: 11949 nodes (11247), 19488 edges (18779), time = 418.884ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.03ms.\r\n\r\n2022-04-20 04:40:29.021558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:29.023022: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2022-04-20 04:40:29.023118: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\r\n2022-04-20 04:40:29.023521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:29.024914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:29.026207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:29.027569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:29.028843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-20 04:40:29.030082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13598 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\r\n2022-04-20 04:40:30.639612: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:385] Calibration with FP32 or FP16 is not implemented. Falling back to use_calibration = False.Note that the default value of use_calibration is True.\r\n2022-04-20 04:40:30.639665: I tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:403] [TF-TRT] not using explicit QDQ mode\r\n2022-04-20 04:40:30.950302: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:884]\r\n\r\n################################################################################\r\nTensorRT unsupported/non-converted OP Report:\r\n\t- Placeholder -> 513x\r\n\t- GatherV2 -> 390x\r\n\t- Reshape -> 325x\r\n\t- StridedSlice -> 273x\r\n\t- Fill -> 273x\r\n\t- Shape -> 246x\r\n\t- Sub -> 213x\r\n\t- ConcatV2 -> 187x\r\n\t- Pack -> 167x\r\n\t- Mul -> 112x\r\n\t- Select -> 102x\r\n\t- Slice -> 101x\r\n\t- AddV2 -> 100x\r\n\t- Minimum -> 97x\r\n\t- Identity -> 95x\r\n\t- Less -> 95x\r\n\t- Range -> 93x\r\n\t- ZerosLike -> 92x\r\n\t- NonMaxSuppressionV5 -> 90x\r\n\t- Switch -> 27x\r\n\t- Merge -> 26x\r\n\t- Enter -> 25x\r\n\t- NextIteration -> 25x\r\n\t- DataFormatVecPermute -> 22x\r\n\t- Exit -> 20x\r\n\t- Cast -> 20x\r\n\t- NoOp -> 15x\r\n\t- ExpandDims -> 14x\r\n\t- Greater -> 12x\r\n\t- Unpack -> 9x\r\n\t- Pad -> 9x\r\n\t- TensorListReserve -> 9x\r\n\t- TensorListSetItem -> 9x\r\n\t- TensorListFromTensor -> 8x\r\n\t- TensorListGetItem -> 8x\r\n\t- TensorListStack -> 8x\r\n\t- Tile -> 6x\r\n\t- Split -> 5x\r\n\t- Maximum -> 4x\r\n\t- Round -> 4x\r\n\t- RealDiv -> 4x\r\n\t- Transpose -> 3x\r\n\t- TopKV2 -> 2x\r\n\t- LoopCond -> 2x\r\n\t- Assert -> 2x\r\n\t- Const -> 2x\r\n\t- StopGradient -> 2x\r\n\t- Squeeze -> 2x\r\n\t- Size -> 2x\r\n\t- Equal -> 2x\r\n\t- ResizeBilinear -> 2x\r\n\t- Reciprocal -> 2x\r\n\t- Exp -> 2x\r\n\t- LogicalAnd -> 2x\r\n\t- Sum -> 1x\r\n\t- GreaterEqual -> 1x\r\n\t- Where -> 1x\r\n--------------------------------------------------------------------------------\r\n\t- Total nonconverted OPs: 3883\r\n\t- Total nonconverted OP Types: 57\r\nFor more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.\r\n################################################################################\r\n\r\n2022-04-20 04:40:31.806241: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:799] Number of TensorRT candidate segments: 33\r\n2022-04-20 04:40:31.918157: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 0 consisting of 6 nodes by TRTEngineOp_0_0.\r\n2022-04-20 04:40:31.918285: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 1 consisting of 38 nodes by TRTEngineOp_0_1.\r\n2022-04-20 04:40:31.918497: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 2 consisting of 29 nodes by TRTEngineOp_0_2.\r\n2022-04-20 04:40:31.918707: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 3 consisting of 166 nodes by TRTEngineOp_0_3.\r\n2022-04-20 04:40:31.919229: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 4 consisting of 38 nodes by TRTEngineOp_0_4.\r\n2022-04-20 04:40:31.919419: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 5 consisting of 38 nodes by TRTEngineOp_0_5.\r\n2022-04-20 04:40:31.919642: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 6 consisting of 139 nodes by TRTEngineOp_0_6.\r\n2022-04-20 04:40:31.920011: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 7 consisting of 29 nodes by TRTEngineOp_0_7.\r\n2022-04-20 04:40:31.920264: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 8 consisting of 29 nodes by TRTEngineOp_0_8.\r\n2022-04-20 04:40:31.920410: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 9 consisting of 29 nodes by TRTEngineOp_0_9.\r\n2022-04-20 04:40:31.920691: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 10 consisting of 663 nodes by TRTEngineOp_0_10.\r\n2022-04-20 04:40:31.922440: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 11 consisting of 29 nodes by TRTEngineOp_0_11.\r\n2022-04-20 04:40:31.922600: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 12 consisting of 29 nodes by TRTEngineOp_0_12.\r\n2022-04-20 04:40:31.922745: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 13 consisting of 29 nodes by TRTEngineOp_0_13.\r\n2022-04-20 04:40:31.922881: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 14 consisting of 22 nodes by TRTEngineOp_0_14.\r\n2022-04-20 04:40:31.923039: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 15 consisting of 42 nodes by TRTEngineOp_0_15.\r\n2022-04-20 04:40:31.923218: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 16 consisting of 42 nodes by TRTEngineOp_0_16.\r\n2022-04-20 04:40:31.923401: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 17 consisting of 44 nodes by TRTEngineOp_0_17.\r\n2022-04-20 04:40:31.923585: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 18 consisting of 42 nodes by TRTEngineOp_0_18.\r\n2022-04-20 04:40:31.923770: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 19 consisting of 44 nodes by TRTEngineOp_0_19.\r\n2022-04-20 04:40:31.923947: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 20 consisting of 42 nodes by TRTEngineOp_0_20.\r\n2022-04-20 04:40:31.924331: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 21 consisting of 44 nodes by TRTEngineOp_0_21.\r\n2022-04-20 04:40:31.924552: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 22 consisting of 44 nodes by TRTEngineOp_0_22.\r\n2022-04-20 04:40:31.924764: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 23 consisting of 42 nodes by TRTEngineOp_0_23.\r\n2022-04-20 04:40:31.924981: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 24 consisting of 44 nodes by TRTEngineOp_0_24.\r\n2022-04-20 04:40:31.925201: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 25 consisting of 44 nodes by TRTEngineOp_0_25.\r\n2022-04-20 04:40:31.925432: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 26 consisting of 42 nodes by TRTEngineOp_0_26.\r\n2022-04-20 04:40:31.925639: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 27 consisting of 44 nodes by TRTEngineOp_0_27.\r\n2022-04-20 04:40:31.925847: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 28 consisting of 44 nodes by TRTEngineOp_0_28.\r\n2022-04-20 04:40:31.926047: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 29 consisting of 44 nodes by TRTEngineOp_0_29.\r\n2022-04-20 04:40:31.926231: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 30 consisting of 9 nodes by TRTEngineOp_0_30.\r\n2022-04-20 04:40:31.926323: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 31 consisting of 9 nodes by TRTEngineOp_0_31.\r\n2022-04-20 04:40:31.926450: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:910] Replaced segment 32 consisting of 5 nodes by TRTEngineOp_0_32.\r\n2022-04-20 04:40:32.725324: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: tf_graph\r\n  constant_folding: Graph size after: 9106 nodes (-2558), 15479 edges (-2877), time = 532.691ms.\r\n  layout: Graph size after: 9384 nodes (278), 15757 edges (278), time = 442.72ms.\r\n  constant_folding: Graph size after: 9368 nodes (-16), 15741 edges (-16), time = 380.424ms.\r\n  TensorRTOptimizer: Graph size after: 7417 nodes (-1951), 12053 edges (-3688), time = 1359.01294ms.\r\n  constant_folding: Graph size after: 7362 nodes (-55), 12053 edges (0), time = 249.141ms.\r\nOptimization results for grappler item: TRTEngineOp_0_27_native_segment\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 2.382ms.\r\n  layout: Graph size after: 50 nodes (0), 52 edges (0), time = 3.47ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 2.043ms.\r\n  TensorRTOptimizer: Graph size after: 50 nodes (0), 52 edges (0), time = 0.291ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 2.02ms.\r\nOptimization results for grappler item: TRTEngineOp_0_13_native_segment\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.788ms.\r\n  layout: Graph size after: 33 nodes (0), 36 edges (0), time = 0.828ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.787ms.\r\n  TensorRTOptimizer: Graph size after: 33 nodes (0), 36 edges (0), time = 0.062ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.799ms.\r\nOptimization results for grappler item: TRTEngineOp_0_17_native_segment\r\n  constant_folding: Graph size after: 49 nodes (0), 51 edges (0), time = 1.077ms.\r\n  layout: Graph size after: 49 nodes (0), 51 edges (0), time = 1.207ms.\r\n  constant_folding: Graph size after: 49 nodes (0), 51 edges (0), time = 1.083ms.\r\n  TensorRTOptimizer: Graph size after: 49 nodes (0), 51 edges (0), time = 0.098ms.\r\n  constant_folding: Graph size after: 49 nodes (0), 51 edges (0), time = 1.06ms.\r\nOptimization results for grappler item: TRTEngineOp_0_22_native_segment\r\n  constant_folding: Graph size after: 49 nodes (0), 51 edges (0), time = 1.313ms.\r\n  layout: Graph size after: 49 nodes (0), 51 edges (0), time = 1.614ms.\r\n  constant_folding: Graph size after: 49 nodes (0), 51 edges (0), time = 1.269ms.\r\n  TensorRTOptimizer: Graph size after: 49 nodes (0), 51 edges (0), time = 0.131ms.\r\n  constant_folding: Graph size after: 49 nodes (0), 51 edges (0), time = 1.286ms.\r\nOptimization results for grappler item: TRTEngineOp_0_14_native_segment\r\n  constant_folding: Graph size after: 25 nodes (0), 26 edges (0), time = 0.593ms.\r\n  layout: Graph size after: 25 nodes (0), 26 edges (0), time = 0.635ms.\r\n  constant_folding: Graph size after: 25 nodes (0), 26 edges (0), time = 0.595ms.\r\n  TensorRTOptimizer: Graph size after: 25 nodes (0), 26 edges (0), time = 0.048ms.\r\n  constant_folding: Graph size after: 25 nodes (0), 26 edges (0), time = 0.582ms.\r\nOptimization results for grappler item: TRTEngineOp_0_31_native_segment\r\n  constant_folding: Graph size after: 14 nodes (0), 15 edges (0), time = 0.362ms.\r\n  layout: Graph size after: 14 nodes (0), 15 edges (0), time = 0.252ms.\r\n  constant_folding: Graph size after: 14 nodes (0), 15 edges (0), time = 0.347ms.\r\n  TensorRTOptimizer: Graph size after: 14 nodes (0), 15 edges (0), time = 0.012ms.\r\n  constant_folding: Graph size after: 14 nodes (0), 15 edges (0), time = 0.341ms.\r\nOptimization results for grappler item: TRTEngineOp_0_18_native_segment\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.056ms.\r\n  layout: Graph size after: 47 nodes (0), 49 edges (0), time = 1.178ms.\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.052ms.\r\n  TensorRTOptimizer: Graph size after: 47 nodes (0), 49 edges (0), time = 0.092ms.\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.063ms.\r\nOptimization results for grappler item: TRTEngineOp_0_3_native_segment\r\n  constant_folding: Graph size after: 181 nodes (0), 192 edges (0), time = 3.86ms.\r\n  layout: Graph size after: 181 nodes (0), 192 edges (0), time = 4.5ms.\r\n  constant_folding: Graph size after: 181 nodes (0), 192 edges (0), time = 3.769ms.\r\n  TensorRTOptimizer: Graph size after: 181 nodes (0), 192 edges (0), time = 0.32ms.\r\n  constant_folding: Graph size after: 181 nodes (0), 192 edges (0), time = 3.804ms.\r\nOptimization results for grappler item: TRTEngineOp_0_28_native_segment\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 2.284ms.\r\n  layout: Graph size after: 50 nodes (0), 52 edges (0), time = 3.438ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 2.111ms.\r\n  TensorRTOptimizer: Graph size after: 50 nodes (0), 52 edges (0), time = 0.29ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 2.09ms.\r\nOptimization results for grappler item: TRTEngineOp_0_8_native_segment\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.826ms.\r\n  layout: Graph size after: 33 nodes (0), 36 edges (0), time = 0.843ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.827ms.\r\n  TensorRTOptimizer: Graph size after: 33 nodes (0), 36 edges (0), time = 0.061ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.802ms.\r\nOptimization results for grappler item: TRTEngineOp_0_32_native_segment\r\n  constant_folding: Graph size after: 12 nodes (0), 11 edges (0), time = 0.338ms.\r\n  layout: Graph size after: 12 nodes (0), 11 edges (0), time = 0.227ms.\r\n  constant_folding: Graph size after: 12 nodes (0), 11 edges (0), time = 0.299ms.\r\n  TensorRTOptimizer: Graph size after: 12 nodes (0), 11 edges (0), time = 0.01ms.\r\n  constant_folding: Graph size after: 12 nodes (0), 11 edges (0), time = 0.313ms.\r\nOptimization results for grappler item: TRTEngineOp_0_6_native_segment\r\n  constant_folding: Graph size after: 154 nodes (0), 165 edges (0), time = 3.207ms.\r\n  layout: Graph size after: 154 nodes (0), 165 edges (0), time = 3.662ms.\r\n  constant_folding: Graph size after: 154 nodes (0), 165 edges (0), time = 3.273ms.\r\n  TensorRTOptimizer: Graph size after: 154 nodes (0), 165 edges (0), time = 0.331ms.\r\n  constant_folding: Graph size after: 154 nodes (0), 165 edges (0), time = 3.227ms.\r\nOptimization results for grappler item: TRTEngineOp_0_29_native_segment\r\n  constant_folding: Graph size after: 49 nodes (0), 51 edges (0), time = 2.206ms.\r\n  layout: Graph size after: 49 nodes (0), 51 edges (0), time = 3.379ms.\r\n  constant_folding: Graph size after: 49 nodes (0), 51 edges (0), time = 2.069ms.\r\n  TensorRTOptimizer: Graph size after: 49 nodes (0), 51 edges (0), time = 0.295ms.\r\n  constant_folding: Graph size after: 49 nodes (0), 51 edges (0), time = 2.029ms.\r\nOptimization results for grappler item: TRTEngineOp_0_2_native_segment\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.862ms.\r\n  layout: Graph size after: 33 nodes (0), 36 edges (0), time = 0.852ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.812ms.\r\n  TensorRTOptimizer: Graph size after: 33 nodes (0), 36 edges (0), time = 0.061ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.816ms.\r\nOptimization results for grappler item: TRTEngineOp_0_4_native_segment\r\n  constant_folding: Graph size after: 42 nodes (0), 45 edges (0), time = 1.028ms.\r\n  layout: Graph size after: 42 nodes (0), 45 edges (0), time = 1.183ms.\r\n  constant_folding: Graph size after: 42 nodes (0), 45 edges (0), time = 1.018ms.\r\n  TensorRTOptimizer: Graph size after: 42 nodes (0), 45 edges (0), time = 0.079ms.\r\n  constant_folding: Graph size after: 42 nodes (0), 45 edges (0), time = 1.072ms.\r\nOptimization results for grappler item: TRTEngineOp_0_30_native_segment\r\n  constant_folding: Graph size after: 12 nodes (0), 13 edges (0), time = 0.335ms.\r\n  layout: Graph size after: 12 nodes (0), 13 edges (0), time = 0.23ms.\r\n  constant_folding: Graph size after: 12 nodes (0), 13 edges (0), time = 0.295ms.\r\n  TensorRTOptimizer: Graph size after: 12 nodes (0), 13 edges (0), time = 0.01ms.\r\n  constant_folding: Graph size after: 12 nodes (0), 13 edges (0), time = 0.305ms.\r\nOptimization results for grappler item: TRTEngineOp_0_11_native_segment\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.806ms.\r\n  layout: Graph size after: 33 nodes (0), 36 edges (0), time = 0.82ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.807ms.\r\n  TensorRTOptimizer: Graph size after: 33 nodes (0), 36 edges (0), time = 0.062ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.806ms.\r\nOptimization results for grappler item: TRTEngineOp_0_15_native_segment\r\n  constant_folding: Graph size after: 46 nodes (0), 48 edges (0), time = 1.016ms.\r\n  layout: Graph size after: 46 nodes (0), 48 edges (0), time = 1.088ms.\r\n  constant_folding: Graph size after: 46 nodes (0), 48 edges (0), time = 1.025ms.\r\n  TensorRTOptimizer: Graph size after: 46 nodes (0), 48 edges (0), time = 0.083ms.\r\n  constant_folding: Graph size after: 46 nodes (0), 48 edges (0), time = 1.05ms.\r\nOptimization results for grappler item: TRTEngineOp_0_20_native_segment\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.245ms.\r\n  layout: Graph size after: 47 nodes (0), 49 edges (0), time = 1.482ms.\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.256ms.\r\n  TensorRTOptimizer: Graph size after: 47 nodes (0), 49 edges (0), time = 0.118ms.\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.24ms.\r\nOptimization results for grappler item: TRTEngineOp_0_12_native_segment\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.788ms.\r\n  layout: Graph size after: 33 nodes (0), 36 edges (0), time = 0.822ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.811ms.\r\n  TensorRTOptimizer: Graph size after: 33 nodes (0), 36 edges (0), time = 0.062ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.814ms.\r\nOptimization results for grappler item: TRTEngineOp_0_19_native_segment\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.126ms.\r\n  layout: Graph size after: 50 nodes (0), 52 edges (0), time = 1.256ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.133ms.\r\n  TensorRTOptimizer: Graph size after: 50 nodes (0), 52 edges (0), time = 0.094ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.121ms.\r\nOptimization results for grappler item: TRTEngineOp_0_9_native_segment\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.802ms.\r\n  layout: Graph size after: 33 nodes (0), 36 edges (0), time = 0.823ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.859ms.\r\n  TensorRTOptimizer: Graph size after: 33 nodes (0), 36 edges (0), time = 0.064ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.799ms.\r\nOptimization results for grappler item: TRTEngineOp_0_10_native_segment\r\n  constant_folding: Graph size after: 692 nodes (0), 761 edges (0), time = 15.29ms.\r\n  layout: Graph size after: 692 nodes (0), 761 edges (0), time = 18.143ms.\r\n  constant_folding: Graph size after: 692 nodes (0), 761 edges (0), time = 15.126ms.\r\n  TensorRTOptimizer: Graph size after: 692 nodes (0), 761 edges (0), time = 1.447ms.\r\n  constant_folding: Graph size after: 692 nodes (0), 761 edges (0), time = 15.151ms.\r\nOptimization results for grappler item: TRTEngineOp_0_1_native_segment\r\n  constant_folding: Graph size after: 44 nodes (0), 46 edges (0), time = 2.195ms.\r\n  layout: Graph size after: 44 nodes (0), 46 edges (0), time = 3.041ms.\r\n  constant_folding: Graph size after: 44 nodes (0), 46 edges (0), time = 1.848ms.\r\n  TensorRTOptimizer: Graph size after: 44 nodes (0), 46 edges (0), time = 0.264ms.\r\n  constant_folding: Graph size after: 44 nodes (0), 46 edges (0), time = 1.887ms.\r\nOptimization results for grappler item: TRTEngineOp_0_21_native_segment\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.372ms.\r\n  layout: Graph size after: 50 nodes (0), 52 edges (0), time = 1.707ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.368ms.\r\n  TensorRTOptimizer: Graph size after: 50 nodes (0), 52 edges (0), time = 0.133ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.348ms.\r\nOptimization results for grappler item: TRTEngineOp_0_16_native_segment\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.066ms.\r\n  layout: Graph size after: 47 nodes (0), 49 edges (0), time = 1.159ms.\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.096ms.\r\n  TensorRTOptimizer: Graph size after: 47 nodes (0), 49 edges (0), time = 0.095ms.\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.097ms.\r\nOptimization results for grappler item: TRTEngineOp_0_23_native_segment\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.465ms.\r\n  layout: Graph size after: 47 nodes (0), 49 edges (0), time = 1.878ms.\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.417ms.\r\n  TensorRTOptimizer: Graph size after: 47 nodes (0), 49 edges (0), time = 0.15ms.\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.446ms.\r\nOptimization results for grappler item: TRTEngineOp_0_24_native_segment\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.551ms.\r\n  layout: Graph size after: 50 nodes (0), 52 edges (0), time = 2.083ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.488ms.\r\n  TensorRTOptimizer: Graph size after: 50 nodes (0), 52 edges (0), time = 0.169ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.547ms.\r\nOptimization results for grappler item: TRTEngineOp_0_7_native_segment\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.826ms.\r\n  layout: Graph size after: 33 nodes (0), 36 edges (0), time = 0.835ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.823ms.\r\n  TensorRTOptimizer: Graph size after: 33 nodes (0), 36 edges (0), time = 0.061ms.\r\n  constant_folding: Graph size after: 33 nodes (0), 36 edges (0), time = 0.808ms.\r\nOptimization results for grappler item: TRTEngineOp_0_5_native_segment\r\n  constant_folding: Graph size after: 42 nodes (0), 45 edges (0), time = 1.003ms.\r\n  layout: Graph size after: 42 nodes (0), 45 edges (0), time = 1.102ms.\r\n  constant_folding: Graph size after: 42 nodes (0), 45 edges (0), time = 1.03ms.\r\n  TensorRTOptimizer: Graph size after: 42 nodes (0), 45 edges (0), time = 0.086ms.\r\n  constant_folding: Graph size after: 42 nodes (0), 45 edges (0), time = 1.002ms.\r\nOptimization results for grappler item: TRTEngineOp_0_0_native_segment\r\n  constant_folding: Graph size after: 8 nodes (0), 7 edges (0), time = 0.265ms.\r\n  layout: Graph size after: 8 nodes (0), 7 edges (0), time = 0.186ms.\r\n  constant_folding: Graph size after: 8 nodes (0), 7 edges (0), time = 0.234ms.\r\n  TensorRTOptimizer: Graph size after: 8 nodes (0), 7 edges (0), time = 0.008ms.\r\n  constant_folding: Graph size after: 8 nodes (0), 7 edges (0), time = 0.243ms.\r\nOptimization results for grappler item: TRTEngineOp_0_25_native_segment\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.566ms.\r\n  layout: Graph size after: 50 nodes (0), 52 edges (0), time = 2ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.503ms.\r\n  TensorRTOptimizer: Graph size after: 50 nodes (0), 52 edges (0), time = 0.164ms.\r\n  constant_folding: Graph size after: 50 nodes (0), 52 edges (0), time = 1.532ms.\r\nOptimization results for grappler item: TRTEngineOp_0_26_native_segment\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.988ms.\r\n  layout: Graph size after: 47 nodes (0), 49 edges (0), time = 2.847ms.\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.775ms.\r\n  TensorRTOptimizer: Graph size after: 47 nodes (0), 49 edges (0), time = 0.245ms.\r\n  constant_folding: Graph size after: 47 nodes (0), 49 edges (0), time = 1.851ms.\r\n\r\n2022-04-20 04:40:38.881680: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_30)\r\n2022-04-20 04:40:38.882029: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_31)\r\n2022-04-20 04:40:38.882302: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_14)\r\n2022-04-20 04:40:38.882531: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_0)\r\n2022-04-20 04:40:38.882749: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_15)\r\n2022-04-20 04:40:38.882961: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_16)\r\n2022-04-20 04:40:38.883192: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_17)\r\n2022-04-20 04:40:38.883404: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_18)\r\n2022-04-20 04:40:38.883624: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_19)\r\n2022-04-20 04:40:38.883835: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_20)\r\n2022-04-20 04:40:38.884032: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_21)\r\n2022-04-20 04:40:38.884223: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_22)\r\n2022-04-20 04:40:38.884408: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_23)\r\n2022-04-20 04:40:38.884594: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_24)\r\n2022-04-20 04:40:38.884777: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_25)\r\n2022-04-20 04:40:38.884962: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_26)\r\n2022-04-20 04:40:38.885146: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_27)\r\n2022-04-20 04:40:38.885331: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_28)\r\n2022-04-20 04:40:38.885530: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_29)\r\n2022-04-20 04:40:38.885716: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_1)\r\n2022-04-20 04:40:38.885902: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_2)\r\n2022-04-20 04:40:38.886088: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_4)\r\n2022-04-20 04:40:38.886271: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_5)\r\n2022-04-20 04:40:38.886455: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_3)\r\n2022-04-20 04:40:38.886639: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_9)\r\n2022-04-20 04:40:38.886819: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_8)\r\n2022-04-20 04:40:38.887015: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_7)\r\n2022-04-20 04:40:38.887202: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_6)\r\n2022-04-20 04:40:38.887387: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_13)\r\n2022-04-20 04:40:38.887571: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_12)\r\n2022-04-20 04:40:38.887755: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_11)\r\n2022-04-20 04:40:38.887937: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_10)\r\n2022-04-20 04:40:38.888158: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at trt_engine_resource_ops.cc:198 : NOT_FOUND: Container TF-TRT does not exist. (Could not find resource: TF-TRT/TRTEngineOp_0_32)\r\nWARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 318). These functions will not be directly callable after loading.\r\nTraceback (most recent call last):\r\n  File \"./convert.py\", line 14, in <module>\r\n    converter.save(mmm + \"_trt_fp32\")\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1408, in save\r\n    save.save(self._saved_model, output_saved_model_dir, signatures)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py\", line 1334, in save\r\n    save_and_return_nodes(obj, export_dir, signatures, options)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py\", line 1369, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py\", line 1536, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py\", line 1492, in _build_meta_graph_impl\r\n    options.namespace_whitelist, options.experimental_custom_gradients)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py\", line 944, in _fill_meta_graph_def\r\n    _trace_gradient_functions(exported_graph, saveable_view)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py\", line 799, in _trace_gradient_functions\r\n    for op_type, op in _iterate_op_types(fn):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/save.py\", line 762, in _iterate_op_types\r\n    \"Unable to save gradient functions when exporting a \"\r\nValueError: Unable to save gradient functions when exporting a _DefinedFunction (generally created through graph freezing utils or through V1 graph importers). Please save with `options=tf.SaveOptions(experimental_custom_gradients=False)`\r\n```\r\n</details>", "comments": ["Adding `options=tf.saved_model.SaveOptions(experimental_custom_gradients=False)` to `saved_model.save()` method in [trt_convert.py](https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/compiler/tensorrt/trt_convert.py#L1408) helped. But why it is not needed in 2.7.1 and 2.9.0-rc0?", "The reason why `save()` works in `2.9.0-rc0` is because [trt_convert.py](https://github.com/tensorflow/tensorflow/blob/v2.9.0-rc0/tensorflow/python/compiler/tensorrt/trt_convert.py#L318) uses new list of graph optimizers in `2.9.0-rc0`. Inserting `\"pruning\"` optimization before the original list helps to solve the issue with `Unable to save gradient functions` in TF 2.8.0\r\n```\r\n# TF 2.8.0\r\nrewriter_config_with_trt.optimizers.extend(\r\n  [\"constfold\", \"layout\", \"constfold\"]\r\n\r\n# TF 2.9.0-rc0\r\nrewriter_config_with_trt.optimizers.extend(\r\n  [\"pruning\", \"debug_stripper\", \"layout\", \"dependency\", \"constfold\", \"common_subgraph_elimination\"]\r\n```\r\n", "https://github.com/NVIDIA/TensorRT/tree/master/samples%2Fpython%2Fefficientdet\nI suggest use above transfer script instead of tf-trt."]}, {"number": 55674, "title": "Converter for Range operation", "body": "Similar to [`tf.range`](https://www.tensorflow.org/api_docs/python/tf/range) this converter creates a sequence of numbers defined by **(start, limit, delta)** any of which could be DT_FLOAT, DT_HALF or DT_INT32. Like  [`tf.range`](https://www.tensorflow.org/api_docs/python/tf/range) the output is a tensor of type DT_FLOAT unless all three   **(start, limit, delta)** are of type  DT_INT32. In this case, the output will also be a tensor of type DT_INT32.\r\n", "comments": ["Converted to draft because passing **(start, limit, delta)** parameters as tensors is not yet implemented."]}, {"number": 55673, "title": "[oneDNN] Fixing a failure in node_file_writer_test", "body": "This PR fixes a failure in //tensorflow/python/framework/node_file_writer_test by removing [this workaround ](https://github.com/tensorflow/tensorflow/commit/fdf2cc845fdfbafe0395bf9b389e4a056dea4f84)and setting the op device properly. Test was failing because it expected different op name to be logged (_MklNativeConv2D vs Conv2D) based on whether the op gets rewritten. As this test runs multiple times with and without eager_op_as_function feature, op was not going through the rewrite in the first case.", "comments": []}, {"number": 55672, "title": "[oneDNN] Fixes inconsistency in semantics of check for oneDNN opts in python and C++", "body": "After oneDNN was made default [ https://github.com/tensorflow/tensorflow/commit/0a63b9da132f2160411312af72de9e733e73dd65 ], the semantics of the check for oneDNN optimizations in python no longer matched the C++ checks. This fix will make sure python and c++ have the same semantics. The fix requires reshuffling the code and moving the IsMKLEnabled() implementation into onednn_env_vars.cc file.\r\n\r\nThis PR fixes following unit test failures\r\n\r\n//tensorflow/python/client:timeline_test_cpu,\r\n//tensorflow/c/eager:c_api_distributed_test_cpu,\r\n//tensorflow/python/eager:context_test_cpu,\r\n//tensorflow/python/framework:config_test_cpu,\r\n//tensorflow/python/framework:config_test_tpu,\r\n//tensorflow/python/framework:node_file_writer_test_cpu", "comments": []}]