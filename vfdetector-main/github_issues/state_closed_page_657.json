[{"number": 33902, "title": "Hopefully can solve Issue #33811", "body": "Hopefully can solve Issue #33811.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33902) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 33901, "title": "Is there any method to calculate batch linear regression in GPU efficiently???", "body": "I have written a cpu version of such calculate problem, but cpu version is still slow.\r\n\r\n```\r\nfrom numba import jit \r\nimport numpy as np \r\n\r\n@jit(nopython=True)\r\ndef resid_lstsq(x,y):\r\n\tret = np.linalg.lstsq(x,y)\r\n\tcoef = ret[0]\r\n\tyHat = np.dot(x,coef)\r\n\tresiduls = y - yHat\r\n\r\n\treturn residuls\r\n\r\n\r\n@jit(nopython=True)\r\ndef resid_matrixReverse(x,y):\r\n\t\r\n\tcoef = np.dot(np.dot(np.linalg.inv(np.dot(x.T,x)),x.T),y)\r\n\tyHat = np.dot(x,coef)\r\n\tresiduls = y - yHat\r\n\r\n\treturn residuls\r\n\r\n\r\n@jit(nopython=True)\r\ndef resid_matrixSolve(x,y):\r\n\t\r\n\tcoef = np.linalg.solve(np.dot(x.T, x), np.dot(x.T, y))\r\n\tyHat = np.dot(x,coef)\r\n\tresiduls = y - yHat\r\n\r\n\treturn residuls\r\n\r\n\r\ndef residCalculatorVector(x,y,method='matrixReverse'):\r\n\t'''\r\n\tget the residuls of regression for one period\r\n\tx: the independent variables\r\n\ty: the dependent variables\r\n\t'''\r\n\tif method == 'lstsq':\r\n\t\treturn resid_lstsq(x,y)\r\n\r\n\telif method == 'matrixReverse':\r\n\t\treturn resid_matrixReverse(x,y)\r\n\r\n\telif method == 'matrixSolve':\r\n\t\treturn resid_matrixSolve(x,y)\r\n\r\n\telif method == 'sklearn':\r\n\t\tpass \r\n\r\ndef residCalculatorMatrix(newFactorMatrix,targetMatrix,oldFactorTensor,method='matrixReverse'):\r\n\t'''\r\n\tcalculate the residuals between one new factor with several old factors\r\n\tnewFactorMatrix: (i,j)(i:trade date, j: stock ID)\r\n\toldFactorTensor (i,j,k)(i:stock ID, j: trade date, k:factors )\r\n\t'''\r\n\tshape0 = oldFactorTensor.shape[0]\r\n\tshape2 = oldFactorTensor.shape[2]\t\r\n\t\r\n\tif shape0 < shape2:\r\n\t\toldFactorTensor_ = oldFactorTensor.T\r\n\telse:\r\n\t\toldFactorTensor_ = oldFactorTensor  #(j,i,k)\r\n\r\n\trowNum = newFactorMatrix.shape[0]\r\n\tcolNum = newFactorMatrix.shape[1]\r\n\tfactorResidMatrix = np.full(fill_value=np.nan,shape=(rowNum,colNum))\r\n\ttargetResidMatrix = np.full(fill_value=np.nan,shape=(rowNum,colNum))\r\n\r\n\tfor i in range(rowNum):\r\n\t\ty_i = newFactorMatrix[i,:]\r\n\t\tretY_i = targetMatrix[i,:]\r\n\t\tx_i = oldFactorTensor_[:,i,:]\r\n\r\n\t\tidx = np.where(~np.isnan(y_i))[0]\t#the index which is NAs\r\n\t\t#idx_ = np.where(np.isnan(y_i))[0]\t#the index which is NAs\r\n\r\n\t\ty_i_ = y_i[idx]\r\n\t\tx_i_ = x_i[idx]\r\n\t\tretY_i_ = retY_i[idx]\r\n\r\n\t\tfactorResid_i = residCalculatorVector(x_i_,y_i_,method=method)\r\n\t\tretResid_i = residCalculatorVector(x_i_,retY_i_,method=method)\r\n\t\tfactorResidMatrix[i,idx] = factorResid_i\r\n\t\ttargetResidMatrix[i,idx] = retResid_i\r\n\r\n\treturn factorResidMatrix,targetResidMatrix\r\n\r\n\r\nnewF_01 = np.random.randn(500,3000)\r\ntarget_01 = np.random.randn(500,3000)\r\noldFTen_01 = np.random.randn(3000,500,100)\r\n\r\n%timeit residCalculatorMatrix(newF_01,target_01,oldFTen_01,method='matrixReverse')\r\n1.46 s \u00b1 104 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n%timeit residCalculatorMatrix(newF_01,target_01,oldFTen_01,method='matrixSolve')\r\n973 ms \u00b1 98.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\n%timeit residCalculatorMatrix(newF_01,target_01,oldFTen_02,method='lstsq')\r\n4.45 s \u00b1 116 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nThe np.linalg.solve is most efficient method in cpu version.\r\nBut If I have a Nvidia 2080Ti\r\nAnd how can I broadcast the loop of massive regressions into GPU.\r\nOr is there any method to calculate massive regressions in GPU ???\r\nThank you so much.", "comments": ["@isaac-you, Looks like issue is not related to Tenosrflow. Please post this issue in relevant repo. Thanks! ", "@isaac-you, Closing the issue as it is not related to Tensorflow. Please feel free to reopen if it is Tensorflow issue. Thanks!", "@gadagashwini  is there any function in tensorflow which can calculate massive OLS(linear regression) simultaneously in GPU? thank you ."]}, {"number": 33900, "title": "Address problems with use_deterministic_cudnn test decorator", "body": "There is something not right here, and I'm trying to figure out how best to fix it.\r\n\r\n`use_deterministic_cudnn` in `tensorflow/python/framework/test_util.py` is a test decorator that was added in [this commit](https://github.com/tensorflow/tensorflow/commit/c27909ea80e8823dbf4f7176ab69991a630356a1) on 2019-03-13.\r\n\r\nThe only place it's used is to decorate `layer_test` in `tensorflow/python/keras/testing_utils.py`, which is called (only) from a bunch of test files in `tensorflow/python/keras/layers`.\r\n\r\nI initially noticed that the docstring for `use_deterministic_cudnn` does not match what the decorator actually does, and I intended to fix that. The decorator sets `TF_CUDNN_DETERMINISTIC=true`, and that not only disables cuDNN auto-tuning (from an end-user perspective), but also ensures the deterministic selection of deterministic algorithms for the back-prop of cuDNN convolution and max-pooling.\r\n\r\nBut the problems go deeper. This decorator intends to enable `TF_CUDNN_DETERMINISTIC` only for the decorated test. However, `TF_CUDNN_DETERMINISTIC` is currently implemented so that its value is cached in a static variable the first time it's used. This means that the implied promise of the decorator code to restore the `TF_CUDNN_DETERMINISTIC` setting cannot be fulfilled. Worse still, if an earlier-running test causes `TF_CUDNN_DETERMINISTIC` to be cached (as \"0\" or \"false\") then the decorator will have no effect whatsoever.\r\n\r\nAdditionally, the underlying `TF_CUDNN_DETERMINISTIC` functionality is currently implemented in a way that leads to the caching of the chosen algorithms (for any given layer configuration), even if the environment variable itself were not cached.\r\n\r\nIn this current pull request, I'm proposing changing the decorator to use `TF_CUDNN_USE_AUTOTUNE=false` (instead of `TF_CUDNN_DETERMINISTIC=true`), which both makes the code match the docstring and works in terms of environment variable caching: I believe that `TF_CUDNN_USE_AUTOTUNE` is freshly re-evaluated every time a cuDNN convolution is launched.\r\n\r\nWhether this proposed change is appropriate or not depends on what is meant by \"Some tests want to base assertions on a graph being _isomorphic with a copy_.\" (from the docstring). What is meant by the graph being isomorphic with a copy? Is this requiring bit-exact determinism from the cuDNN convolution and max-pooling back-prop algorithms? If yes, then this per-test-decorator-based solution is probably not appropriate and `TF_CUDNN_DETERMINISTIC` should be set to \"true\" in a broader scope. An example approach is shown [here](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/kernel_tests/cudnn_determinism_test.py#L102).\r\n\r\nI would like to work with the right person/people to help resolve this issue.", "comments": ["@chsigg: You might be a better person for this PR", "> What is meant by the graph being isomorphic with a copy?\r\n\r\nThis is poorly phrased, but it just refers to the fact that inside of layer_test, there is a test that copying the model and running the model returns approximately the same result. With a different auto-tuning algorithm, the result could change by a large amount that caused some tests to fail.\r\n\r\n> Is this requiring bit-exact determinism from the cuDNN convolution and max-pooling back-prop algorithms? If yes, then this per-test-decorator-based solution is probably not appropriate and TF_CUDNN_DETERMINISTIC should be set to \"true\" in a broader scope.\r\n\r\nIt's requiring determinism of the algorithm chosen to limit variance (though at the time still not bit-exact determinism probably because not all reductions are deterministic for example), during the execution of all graphs during this function, relative to each other, but not relative to some baseline across executions of the test case.\r\n\r\nTensorflow has begun to have a policy with 2.0 not to use environment variables for configuring tests. In this case, it's okay to fix a bug without updating it to the new way of doing things, but the new ideal way of doing this would be similar to what is done here: https://github.com/tensorflow/tensorflow/commit/91a5a7318f77a7e6c7198d12b20c5414de29664d with TF_SetTfXlaCpuGlobalJit. It's also not ideal to rely on the implementation not caching the environment variable, when this optimization has been done in other areas, but for a test that's mostly fine. There is one blocker issue though, which is that XLA runs of this test would become flaky because the TF:XLA bridge does not know of TF_CUDNN_USE_AUTOTUNE; this is a shortcoming (bug) in the tf-xla bridge that it didn't know of this switch.\r\n\r\nI'd rather avoid the broader scope setting of a variable. While it is more proper than what is there right now and will have less surprises, it might lead people to having bad tests when they are creating additional test cases in the same file. It also doesn't fix the issue with `layer_test` because that function might be imported and called by many different test files. \r\n\r\nI'm giving you only a partial answer now, so you know you're not being ignored. I'll see tomorrow if it's trivial to add bridge support for TF_CUDNN_USE_ATOTUNE. The alternative that is more appropriate but would be more work for you is if you're willing to make the TF_CUDNN_DETERMINISTIC a variable that can be programatically set rather than just static, is initially set with the environment variable, and then adding the TF_CAPI_EXPORT related code like in the linked change.", "Hi @tpopp, Thanks so much for such a comprehensive reply.\r\n\r\nYou've provided some very compelling arguments for controlling this in the decorator used by `layer_test`, rather than by setting a variable at a larger scope. I agree with you on that.\r\n\r\nEven though XLA currently doesn't know about `TF_CUDNN_DETERMINISTIC` explicitly, I think that `TF_CUDNN_DETERMINISTIC` is currently working in this test decorator (on XLA) because of the way that `TF_CUDNN_DETERMINISTIC` functionality is currently implemented. `TF_CUDNN_DETERMINISTIC` currently works (in `tensorflow/stream_executor/cuda/cuda_dnn.cc`) by restricting the forward and two backward algorithms available to both the XLA and non-XLA auto-tuning code to a single, deterministic algorithm in each case, via the whitelists of algorithms returned by `CudnnSupport::GetConvolve*Algorithms()` in the stream-executor.\r\n\r\nI'm about to issue a pull request to fix a bug related to `TF_CUDNN_DETERMINISTIC` that requires those returned whitelists to contain all deterministic algorithms and for the auto-tuning code (both XLA and non-XLA) to operate deterministically when `TF_CUDNN_DETERMINISTIC=true`. At this point, I have not implemented that using the `TF_CAPI_EXPORT` mechanism, which I had not known about. The way I have implemented it is that the XLA auto-tuning code just calls a function in the main TF object, which holds the cached setting of `TF_CUDNN_DETERMINISTIC`.\r\n\r\nWith the current implementation of `TF_CUDNN_DETERMINISTIC` and with the upcoming bug fix, caching of cuDNN algorithm selection is part of the default functionality of the deterministic mode. This means that adding the ability to enable and disable it on a layer-by-layer basis would require additional algorithm-cache invalidation functionality which, as far as I can tell, would only be useful for the testing case that is the focus of this current PR.\r\n\r\nI am also planning on a later, higher-performance implementation of `TF_CUDNN_DETERMINISTIC` that will cache the deterministic algorithm selections separately from the auto-tuned algorithm selections. That implementation of `TF_CUDNN_DETERMINISTIC` will be more amenable to being enabled and disabled between test cases.\r\n\r\nGiven that the default functionality of `TF_CUDNN_USE_AUTOTUNE` lends itself to being enabled and disabled on-the-fly (and this is how it's currently implemented for non-XLA) and given that the `TF_CUDNN_AUTOTUNE` environment variable is also currently not cached, I think that it's preferable that we use `TF_CUDNN_AUTOTUNE`.\r\n\r\nIf you agree, then in this current pull request I could attempt to add `TF_CUDNN_AUTOTUNE` functionality to the XLA auto-tune code in `tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc` and pass the setting through from the environment variable using the `TF_CAPI_EXPORT` mechanism (as you suggested in your above comment for `TF_CUDNN_DETERMINISTIC`).\r\n\r\nAnother option would be to hope that we don't run into issues with the current (and bug-fix) implementations of `TF_CUDNN_DETERMINISTIC` and convert this current pull request to improve the docstring and comments on `use_deterministic_cudnn` (including a reference to the current pull request). Then the later, improved implementation of `TF_CUDNN_DETERMINISTIC` can be sure to make `use_deterministic_cudnn` more robust.", "Hi @duncanriach,\r\n\r\nI'm very sorry for not responding. I don't do PRs normally, so I completely forgot.\r\n\r\nEither of your suggestions in the last 2 paragraphs is good in my opinion, and I will sign off on either as long as it would not result in broken tests running with XLA (Running the test with TF_XLA_FLAGS=\"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=1\" should be enough to know).\r\n\r\nGiven this decorator is only used for a few tests, and the use of autotuning in general for these tests is debatable (both sides having valid points), I think any forward progress is good, and we shouldn't stress too much (but you're right about the current scheme being completely broken and us being lucky).\r\n\r\nAgain, I apologize for ignoring you, and I'll do better about checking github.", "No problem, @tpopp. Thanks for reviewing this PR.\r\nI have some other things that I'm more urgently working on and I will return to this PR in the next few weeks and decide how to proceed then.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "The direction of this PR is dependent on the outcome of [PR 34951](https://github.com/tensorflow/tensorflow/pull/34951), which I'm also dealing with at a higher priority than this one, and which is currently waiting for a review response.", "I'm going to close this pull request and open it again when I have the time and clarity to push it through to a merge.", "I'm re-opening this pull-request so that I can submit some additional changes for review, discussion, and potential merge.", "@tpopp, I have discovered that the XLA code does already have a mechanism for disabling cuDNN auto-tuning. Auto-tuning is disabled by adding `--xla_gpu_disable_autotune` to `XLA_FLAGS`. It takes effect in [xla/.../gpu_conv_algorithm_picker.cc](https://github.com/tensorflow/tensorflow/blob/240497c2b2058ed25b09308dd906441882ca7611/tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc#L721) and in [xla/.../gemm_algorithm_picker.cc](https://github.com/tensorflow/tensorflow/blob/3b71976900cb1bdb7016441076c70dcc3296b8b3/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc#L319).\r\n\r\nI have submitted an additional commit to this current PR which controls `--xla_gpu_disable_autotune` from the `disable_cudnn_autotune` decorator. I have tested this locally only using `//tensorflow/python/keras:convolutional_test`, both with and without `TF_XLA_FLAGS=\"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=1\"` These tests pass.\r\n\r\nI'm not totally confident that `XLA_FLAGS` changes will take effect between test-cases, as this current solution expects and implies.\r\n\r\nIf this current solution is **not** acceptable to you, then another option might be to add an equivalent `pywrap_tensorflow` switch and then get it to be visible in those same locations in `xla/.../gpu_conv_algorithm_picker.cc` and `xla/.../gemm_algorithm_picker.cc`.", "@gbaned or @rthadur, please will you remove the following tags: `stalled` and `stat:awaiting response`. This PR is now waiting for review from @tpopp.", "Adding reference to [PR 35621](https://github.com/tensorflow/tensorflow/pull/35621) (Replace --xla_gpu_disable_autotune option with --xla_gpu_autotune_level) which intends to change an XLA flag that I have proposed to utilize in this current PR.", "Unless it is hidden further down, it looks like the flags will be parsed multiple times for XLA_FLAGS, so I'm going to approve this."]}, {"number": 33898, "title": "tf.GradientTape training much slower than keras.fit", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 1*.04\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX 1080\r\n\r\nI am currently trying to get a hold of the **TF2.0** api, but as I compared the [**GradientTape**][1] to a regular **keras.Model.fit** I noticed: \r\n\r\n1. It ran slower(probably due to the Eager Execution)\r\n\r\n2. It converged much slower (and I am not sure why).\r\n\r\n```\r\n+--------+--------------+--------------+------------------+\r\n|  Epoch | GradientTape | GradientTape | keras.Model.fit  |\r\n|        |              |  shuffling   |                  |\r\n+--------+--------------+--------------+------------------+\r\n|    1   |     0.905    |     0.918    |      0.8793      |\r\n+--------+--------------+--------------+------------------+\r\n|    2   |     0.352    |     0.634    |      0.2226      |\r\n+--------+--------------+--------------+------------------+\r\n|    3   |     0.285    |     0.518    |      0.1192      |\r\n+--------+--------------+--------------+------------------+\r\n|    4   |     0.282    |     0.458    |      0.1029      |\r\n+--------+--------------+--------------+------------------+\r\n|    5   |     0.275    |     0.421    |      0.0940      |\r\n+--------+--------------+--------------+------------------+\r\n```\r\n\r\nHere is the training loop I used with the **GradientTape**:\r\n```python\r\n\r\noptimizer = keras.optimizers.Adam()\r\nglove_model = GloveModel(vocab_size=len(labels))\r\ntrain_loss = keras.metrics.Mean(name='train_loss')\r\n\r\n@tf.function\r\ndef train_step(examples, labels):\r\n\twith tf.GradientTape() as tape:\r\n\t\tpredictions = glove_model(examples)\r\n\t\tloss = glove_model.glove_loss(labels, predictions)\r\n\r\n\tgradients = tape.gradient(loss, glove_model.trainable_variables)\r\n\toptimizer.apply_gradients(zip(gradients, glove_model.trainable_variables))\r\n\r\n\ttrain_loss(loss)\r\n\r\n\r\n\r\ntotal_step = 0\r\nfor epoch in range(epochs_number):\r\n\r\n\tpbar = tqdm(train_ds.enumerate(), total=int(len(index_data) / batch_size) + 1)\r\n\r\n\tfor ix, (examples, labels) in pbar:\r\n\r\n\t\ttrain_step(examples, labels)\r\n\r\n\r\n\tprint(f\"Epoch {epoch + 1}, Loss {train_loss.result()}\")\r\n\r\n\t# Reset the metrics for the next epoch\r\n\ttrain_loss.reset_states()\r\n```\r\n\r\nAnd here is the **Keras.Model.fit** training:\r\n```python\r\nglove_model.compile(optimizer, glove_model.glove_loss)\r\nglove_model.fit(train_ds, epochs=epochs_number)\r\n```\r\nHere is the **tf.data.Dataset** source \r\n\r\n```python\r\ntrain_ds = data.Dataset.from_tensor_slices(\r\n\t(np.hstack([index_rows.reshape(-1, 1), index_cols.reshape(-1, 1)]), index_data)\r\n).shuffle(100000).batch(batch_size, drop_remainder=True)\r\n```\r\n\r\nAnd Here is the model.\r\n\r\n```python\r\nclass GloveModel(keras.Model):\r\n\r\n    def __init__(self, vocab_size, dim=100, a=3/4, x_max=100):\r\n        super(GloveModel, self).__init__()\r\n\r\n        self.vocab_size = vocab_size\r\n        self.dim = dim\r\n        self.a = a\r\n        self.x_max = x_max\r\n\r\n        self.target_embedding = layers.Embedding(\r\n            input_dim=self.vocab_size, output_dim=self.dim, input_length=1, name=\"target_embedding\"\r\n        )\r\n        self.target_bias = layers.Embedding(\r\n            input_dim=self.vocab_size, output_dim=1, input_length=1, name=\"target_bias\"\r\n        )\r\n\r\n        self.context_embedding = layers.Embedding(\r\n            input_dim=self.vocab_size, output_dim=self.dim, input_length=1, name=\"context_embedding\"\r\n        )\r\n        self.context_bias = layers.Embedding(\r\n            input_dim=self.vocab_size, output_dim=1, input_length=1, name=\"context_bias\"\r\n        )\r\n\r\n        self.dot_product = layers.Dot(axes=-1, name=\"dot\")\r\n\r\n        self.prediction = layers.Add(name=\"add\")\r\n        self.step = 0\r\n\r\n    def call(self, inputs):\r\n\r\n        target_ix = inputs[:, 0]\r\n        context_ix = inputs[:, 1]\r\n\r\n        target_embedding = self.target_embedding(target_ix)\r\n        target_bias = self.target_bias(target_ix)\r\n\r\n        context_embedding = self.context_embedding(context_ix)\r\n        context_bias = self.context_bias(context_ix)\r\n\r\n        dot_product = self.dot_product([target_embedding, context_embedding])\r\n        prediction = self.prediction([dot_product, target_bias, context_bias])\r\n\r\n        return prediction\r\n\r\n    def glove_loss(self, y_true, y_pred):\r\n\r\n        weight = tf.math.minimum(\r\n            tf.math.pow(y_true/self.x_max, self.a), 1.0\r\n        )\r\n        loss_value = tf.math.reduce_mean(weight * tf.math.pow(y_pred - tf.math.log(y_true), 2.0))\r\n\r\n        return loss_value\r\n\r\n\r\n\r\n```\r\nI tried multiple configurations and optimizers but nothing seems to change the convergence rate.\r\nI tried to **reshuffle** the dataset but with no luck. I also consulted the different issues on GitHub regarding this problem but they all seem to relate to a `tf.function` miss-use.\r\nI also posted this issue on [**StackOverflow**][2] but I had no luck either.\r\n\r\n  [1]: https://www.tensorflow.org/tutorials/quickstart/advanced\r\n  [2]: https://stackoverflow.com/questions/58584359/gradientape-convergence-much-slower-than-keras-model-fit", "comments": ["The problem came from the **shuffling** using the **tf.Dataset** method. It only shuffled through the dataset one bucket at the time. Using the **Keras.Model.fit** yielded better results because it probably adds another shuffling. \r\n\r\nI added a shuffling with `numpy.random.shuffle` and it improved the performance with both training methods:\r\n\r\nThe generation of the dataset is now:\r\n```python\r\nnumpy_data = np.hstack([index_rows.reshape(-1, 1), index_cols.reshape(-1, 1), index_data.reshape(-1, 1)])\r\n\r\nnp.random.shuffle(numpy_data)\r\n\r\nindexes = np.array(numpy_data[:, :2], dtype=np.uint32)\r\nlabels = np.array(numpy_data[:, 2].reshape(-1, 1), dtype=np.float32)\r\n\r\ntrain_ds = data.Dataset.from_tensor_slices(\r\n\t(indexes, labels)\r\n).shuffle(100000).batch(batch_size, drop_remainder=True)\r\n``` \r\n\r\nAnd the results are:\r\n\r\n```\r\n+--------+--------------+------------------+\r\n|  Epoch | GradientTape |  keras.Model.fit |\r\n+--------+--------------+------------------+\r\n|    1   |     0.294    |      0.294       |\r\n+--------+--------------+------------------+\r\n|    2   |     0.111    |      0.110       |\r\n+--------+--------------+------------------+\r\n|    3   |     0.089    |      0.089       |\r\n+--------+--------------+------------------+\r\n|    4   |     0.074    |      0.075       |\r\n+--------+--------------+------------------+\r\n|    5   |     0.063    |      0.063       |\r\n+--------+--------------+------------------+\r\n```\r\n\r\nThe training type per epoch is roughly the same at **2minutes per epoch**.\r\n", "how many/much the buffer_size is good for training ?\r\nthx  "]}, {"number": 33897, "title": "TF Real Op Not Supported for TFLite when generating MFCCs", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source): 2.1.0-dev20191023\r\n\r\nCode throwing error is from TF2.0 docs about generating MFCCs from audio. Using the SELECT_TF_OPS flag helps with RFFT and ComplexAbs, but now Real (assuming tf.math.real) is not able to convert. Is there any way to add a custom op for this or is there something I am missing?\r\n\r\n```python\r\ndef generate_mfcc_features(audio_tensor):\r\n    sample_rate = 16000.0\r\n\r\n    # A 1024-point STFT with frames of 64 ms and 75% overlap.\r\n    stfts = tf.signal.stft(audio_tensor, frame_length=1024, frame_step=256,fft_length=1024)\r\n    spectrograms = tf.cast(tf.abs(stfts), tf.float32)\r\n\r\n    # Warp the linear scale spectrograms into the mel-scale.\r\n    num_spectrogram_bins = stfts.shape[-1]\r\n    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\r\n    linear_to_mel_weight_matrix = tf.cast(tf.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz, upper_edge_hertz), tf.float32)\r\n    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\r\n    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\r\n\r\n    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\r\n    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\r\n\r\n    # Compute MFCCs from log_mel_spectrograms and take the first 13.\r\n    return tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :13]\r\n```\r\nwhen converting like this:\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([enc_to_save])\r\n# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.target_spec.supported_ops = set([tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS])\r\ntflite_model_enc = converter.convert()\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-275-096221475907> in <module>()\r\n      6 # converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n      7 converter.target_spec.supported_ops = set([tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS])\r\n----> 8 tflite_model_enc = converter.convert()\r\n      9 \r\n     10 converter = tf.lite.TFLiteConverter.from_concrete_functions([dec_to_save])\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    460         input_tensors=input_tensors,\r\n    461         output_tensors=output_tensors,\r\n--> 462         **converter_kwargs)\r\n    463 \r\n    464     if self._is_calibration_quantize():\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    450       input_data.SerializeToString(),\r\n    451       debug_info_str=debug_info_str,\r\n--> 452       enable_mlir_converter=enable_mlir_converter)\r\n    453   return data\r\n    454 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    201       stdout = _try_convert_to_unicode(stdout)\r\n    202       stderr = _try_convert_to_unicode(stderr)\r\n--> 203       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    204   finally:\r\n    205     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2019-10-31 16:49:07.859294: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: RFFT\r\n2019-10-31 16:49:07.859376: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: ComplexAbs\r\n2019-10-31 16:49:07.859438: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: RFFT\r\n2019-10-31 16:49:07.859470: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Real\r\n2019-10-31 16:49:07.879329: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 403 operators, 705 arrays (0 quantized)\r\n2019-10-31 16:49:07.897088: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 403 operators, 705 arrays (0 quantized)\r\n2019-10-31 16:49:08.185116: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 351 operators, 648 arrays (0 quantized)\r\n2019-10-31 16:49:08.199468: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 350 operators, 647 arrays (0 quantized)\r\n2019-10-31 16:49:08.213695: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 350 operators, 647 arrays (0 quantized)\r\n2019-10-31 16:49:08.224572: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 350 operators, 647 arrays (0 quantized)\r\n2019-10-31 16:49:08.307894: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 384 bytes, theoretical optimal value: 192 bytes.\r\n2019-10-31 16:49:08.309673: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 3677275\r\n2019-10-31 16:49:08.310492: W tensorflow/lite/toco/tflite/operator.cc:2024] Op Real is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-31 16:49:08.311448: W tensorflow/lite/toco/tflite/operator.cc:2024] Op Real is a valid TensorFlow op but has not been whitelisted for the TensorFlow Lite flex op set.\r\n2019-10-31 16:49:08.312363: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXPAND_DIMS, FLOOR_DIV, FULLY_CONNECTED, GATHER, LEAKY_RELU, LOG, LOGISTIC, MAXIMUM, MAX_POOL_2D, MUL, PACK, PAD, RANGE, RESHAPE, SHAPE, SPLIT, SPLIT_V, SQUEEZE, STRIDED_SLICE, SUB, TANH, TRANSPOSE, UNPACK. Here is a list of operators for which you will need custom implementations: Real.\r\nTraceback (most recent call last):\r\n  File \"/home/mattc/anaconda3/envs/main/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXPAND_DIMS, FLOOR_DIV, FULLY_CONNECTED, GATHER, LEAKY_RELU, LOG, LOGISTIC, MAXIMUM, MAX_POOL_2D, MUL, PACK, PAD, RANGE, RESHAPE, SHAPE, SPLIT, SPLIT_V, SQUEEZE, STRIDED_SLICE, SUB, TANH, TRANSPOSE, UNPACK. Here is a list of operators for which you will need custom implementations: Real.\r\n```\r\n", "comments": ["Able to get tf lite converter to convert mfcc function. I was not able to before, but I can now. But now, an error occurs when I try to invoke the model. I am using the simplest model I can think of to test this.\r\n\r\n```python\r\ndef extract_features(inputs):\r\n    sample_rate = 16000.0\r\n\r\n    # A 1024-point STFT with frames of 64 ms and 75% overlap.\r\n    stfts = tf.signal.stft(inputs, frame_length=1024, frame_step=256, fft_length=1024)\r\n    spectrograms = tf.abs(stfts)\r\n\r\n    # Warp the linear scale spectrograms into the mel-scale.\r\n    num_spectrogram_bins = stfts.shape[-1]\r\n    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\r\n    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,upper_edge_hertz)\r\n    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\r\n    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\r\n\r\n    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\r\n    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\r\n\r\n    # Compute MFCCs from log_mel_spectrograms and take the first 13.\r\n    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :40]\r\n    \r\n    return log_mel_spectrograms\r\n\r\nclass NN(tf.keras.Model):\r\n    def __init__(self):\r\n        super(NN, self).__init__()\r\n        \r\n        self.mfcc = tf.keras.layers.Lambda(extract_features)\r\n\r\n    def call(self, x):    \r\n        return self.mfcc(x)\r\n    \r\nnn = NN()\r\nnn.build((None,40000))\r\n\r\n@tf.function\r\ndef evaluate(inp):\r\n    return nn(inp)\r\n\r\ninput_shape = tf.TensorSpec([None,40000], tf.float32)\r\nto_save = evaluate.get_concrete_function(input_shape)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])\r\n# converter.experimental_new_converter = True\r\n# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\r\nconverter.target_spec.supported_ops = set([tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS])\r\ntflite_model = converter.convert()\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninterpreter.allocate_tensors()\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], tf.convert_to_tensor(np.expand_dims(audio_data, 0), dtype=tf.float32))\r\n\r\ninterpreter.invoke()\r\n\r\noutput = interpreter.get_tensor(output_details[0]['index'])\r\n```\r\n\r\nError:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-614-9e00ca7d4c6c> in <module>()\r\n     56 interpreter.set_tensor(input_details[0]['index'], tf.convert_to_tensor(np.expand_dims(audio_data, 0), dtype=tf.float32))\r\n     57 \r\n---> 58 interpreter.invoke()\r\n     59 \r\n     60 output = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter.py in invoke(self)\r\n    491     \"\"\"\r\n    492     self._ensure_safe()\r\n--> 493     self._interpreter.Invoke()\r\n    494 \r\n    495   def reset_all_variables(self):\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in Invoke(self)\r\n    111 \r\n    112     def Invoke(self):\r\n--> 113         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\r\n    114 \r\n    115     def InputIndices(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (14400 != 39936)Node number 17 (RESHAPE) failed to invoke.\r\n```", "Couldn't reproduce the latest error based on your code. Could you explain how did you get it working yourself? Which version of TF? Did you use a custom build? I still got the same error as you did in your first comment:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-7-815c57e120e7> in <module>\r\n     61 interpreter.set_tensor(input_details[0]['index'], tf.convert_to_tensor(np.expand_dims(audio_data, 0), dtype=tf.float32))\r\n     62 \r\n---> 63 interpreter.invoke()\r\n     64 \r\n     65 output = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n/opt/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter.py in invoke(self)\r\n    451     \"\"\"\r\n    452     self._ensure_safe()\r\n--> 453     self._interpreter.Invoke()\r\n    454 \r\n    455   def reset_all_variables(self):\r\n\r\n/opt/anaconda/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in Invoke(self)\r\n    107 \r\n    108     def Invoke(self):\r\n--> 109         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)\r\n    110 \r\n    111     def InputIndices(self):\r\n\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you invoke the Flex delegate before inference.Node number 23 (FlexRFFT) failed to prepare.\r\n```\r\n\r\n\r\nWith regards to your last problem (num_input_elements != num_output_elements), I'm currently debugging the same problem in my code. I'm also using `tf.stft` and `abs` functions for preprocessing. \r\n\r\nBy `interpreter.get_tensor_details()` you can list all the nodes in the model. What is your 17th node? The one that seems to cause problems in my case is `StridedSlice`:\r\n```\r\n{'name': 'conv/PreProc/audio/stft/frame/StridedSlice',\r\n  'index': 17,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n```\r\n", "I am using version `2.1.0-dev20191108`, a tf-nightly build.\r\n\r\nTensor details:\r\n```\r\n[{'name': 'ConstantFolding/nn_10/lambda_10/stft/frame/concat/values_1_const_axis_0',\r\n  'index': 0,\r\n  'shape': array([1, 1], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'Identity',\r\n  'index': 1,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'inp',\r\n  'index': 2,\r\n  'shape': array([    1, 40000], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/Abs',\r\n  'index': 3,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/Tensordot',\r\n  'index': 4,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/Tensordot/Const_2',\r\n  'index': 5,\r\n  'shape': array([1], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/Tensordot/GatherV2',\r\n  'index': 6,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/Tensordot/MatMul',\r\n  'index': 7,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/Tensordot/MatMul_bias',\r\n  'index': 8,\r\n  'shape': array([80], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/Tensordot/Reshape_1/transpose',\r\n  'index': 9,\r\n  'shape': array([ 80, 513], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/Tensordot/Shape',\r\n  'index': 10,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/Tensordot/concat_1',\r\n  'index': 11,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/Tensordot/free',\r\n  'index': 12,\r\n  'shape': array([2], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/add',\r\n  'index': 13,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/add/y',\r\n  'index': 14,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/GatherV2',\r\n  'index': 15,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Maximum',\r\n  'index': 16,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Maximum/x',\r\n  'index': 17,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Reshape',\r\n  'index': 18,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Reshape/shape',\r\n  'index': 19,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Reshape_1',\r\n  'index': 20,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Reshape_2',\r\n  'index': 21,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Reshape_2/shape',\r\n  'index': 22,\r\n  'shape': array([2], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Reshape_2/shape/1',\r\n  'index': 23,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Reshape_3',\r\n  'index': 24,\r\n  'shape': array([1, 4], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Reshape_4',\r\n  'index': 25,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/Shape',\r\n  'index': 26,\r\n  'shape': array([2], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/StridedSlice',\r\n  'index': 27,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/add_1',\r\n  'index': 28,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/add_1/x',\r\n  'index': 29,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/add_2',\r\n  'index': 30,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/concat',\r\n  'index': 31,\r\n  'shape': array([2], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/concat/values_1',\r\n  'index': 32,\r\n  'shape': array([1], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/concat_1',\r\n  'index': 33,\r\n  'shape': array([3], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/concat_1/values_1',\r\n  'index': 34,\r\n  'shape': array([2], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/concat_2',\r\n  'index': 35,\r\n  'shape': array([3], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/concat_2/values_1',\r\n  'index': 36,\r\n  'shape': array([2], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/floordiv',\r\n  'index': 37,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/floordiv_3',\r\n  'index': 38,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/gcd/Const',\r\n  'index': 39,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/mul',\r\n  'index': 40,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/ones_like',\r\n  'index': 41,\r\n  'shape': array([2], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/packed',\r\n  'index': 42,\r\n  'shape': array([3], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/range_1',\r\n  'index': 43,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/range_1/delta',\r\n  'index': 44,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/range_1/start',\r\n  'index': 45,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/split',\r\n  'index': 46,\r\n  'shape': array([1], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/split/split_dim',\r\n  'index': 47,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/split:1',\r\n  'index': 48,\r\n  'shape': array([1], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/split:2',\r\n  'index': 49,\r\n  'shape': array([0], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/sub_2',\r\n  'index': 50,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame/zeros_like',\r\n  'index': 51,\r\n  'shape': array([2], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame_length',\r\n  'index': 52,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/frame_step',\r\n  'index': 53,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/hann_window/sub_2',\r\n  'index': 54,\r\n  'shape': array([1024], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/mul',\r\n  'index': 55,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/rfft',\r\n  'index': 56,\r\n  'shape': array([], dtype=int32),\r\n  'dtype': numpy.complex64,\r\n  'quantization': (0.0, 0)},\r\n {'name': 'nn_10/lambda_10/stft/rfft/packed',\r\n  'index': 57,\r\n  'shape': array([1], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0)}]\r\n```", "I face the same problem", "I solve the problem. I need set_tensor to two params .", "hey @fmbao how exactly you solve the problem? you set_tensor with what? thank you", "hi, does the original issue resolve? ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33897\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33897\">No</a>\n"]}, {"number": 33896, "title": "[ROCm] Fix for the broken ROCm CSB.", "body": "The following commit breaks the `--config=rocm` build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/a2323c1b1f0f857fd40fff7a481dfa300b8f7e02\r\n\r\nThe commit above adds the \"builtin_sysroot\" and \"cuda_path\" arguments to the \"cc_toolchain_config\" rule within the crosstool/BUILD.tpl template. That template generates the BULD file which is used to setup both ROCm and CUDA configurations. The commit above updates the cuda_configure.bzl file to pass in the new arguments when calling the \"cc_toolchain_config\" rule, but does not make the same updates to the rocm_configure.bzl file, leading to the breakage in the ROCm build.\r\n\r\nThis commit simply makes the corresponding updates in the rocm_configure.bzl file.\r\n\r\n----------------------------------\r\n\r\n@whchung @chsigg \r\n\r\nNote this PR is in addition PR #33806 , which addresses different breakages in the ROCm CSB. \r\n\r\nBoth PRs are needed to get the ROCm CSB passing again. Please review this PR and PR #33806. \r\n\r\nthanks.", "comments": ["Closing out this PR as the changes in the following commit, supersede the ones being proposed in this PR\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/72d0facc37e377d89d8034bb825bf0e93a4f810d"]}, {"number": 33895, "title": "Fix ValueError with tf.data.Dataset and model.fit", "body": "This fix, like its predecessor PR tensorflow#24522, tries to address issue\r\nno. tensorflow#24520, where passing tf.data.Dataset into model.fit may result\r\nin `ValueError: Cannot take the length of Shape with unknown rank.`.\r\n\r\nThis fixes the error by using `shape.rank` rather than checking\r\n`len(shape)`. This PR's predecessor tensorflow#24522 did this incorrectly,\r\nand was never merged.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33895) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> memo **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> \r\n>     * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> \r\n> ##### Corporate signers\r\n> \r\n>     * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> \r\n>     * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n>     * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \r\n> information_source **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33895) for more info**.\r\n\r\n@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33895) for more info**.\n\n<!-- ok -->", "Unfortunately we don't merge PRs on the release branches after the final release, except if we plan to do a patch release for fixing security vulnerabilities and only PRs that are related to that.\r\n\r\nCan you please make this on master, instead? Thank you.\r\n\r\nPS: You can add me as a reviewer.", "Closing as it is not against master and rebasing requires more work from the author's side.\r\n\r\nPlease reopen against master", "While this bug likely still exists in master (as this same fix isn't implemented there), I'm only able to verify that my fix works in `r1.14` and `r1.15`. I don't have the resources right now to rebuild from master and compare the expected behaviours.", "You don't necessarily need to compile from source. In a virtual environment, install the pip package then edit the file from `.../dist-packages/tensorflow_core/python/keras/engine/training_utils.py` and test that this fix works."]}, {"number": 33894, "title": "Eager context device issue (Segmentation Fault) after context (re-)setting ServerDef ", "body": "\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary whl\r\n- TensorFlow version (use command below): `tensorflow-gpu==2.0.0`\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 7.6.4\r\n- GPU model and memory: GeForce GTX 1080 Ti\r\n\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf.tensorflow_server_pb2 import ServerDef\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.python.training.server_lib import ClusterSpec\r\n\r\n\r\ncluster_def = ClusterSpec({'worker': ['127.0.0.1:15293']}).as_cluster_def()\r\n# 15293 is just some random available port\r\n\r\nserver_def = ServerDef(\r\n    cluster=cluster_def,\r\n    job_name='worker',\r\n    task_index=0,\r\n    protocol='grpc'\r\n)\r\n\r\nv = tf.Variable(3)\r\n\r\nprint(v.device)\r\n# > /job:localhost/replica:0/task:0/device:CPU:0\r\n\r\ncontext.set_server_def(server_def)\r\n\r\n####################################\r\nprint(v.device)\r\n# > Segmentation fault (core dumped)\r\n####################################\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n* Should API users expect the Variable re-placed and re-initialized on the new Server?\r\n\r\n**Code to reproduce the issue**\r\nSee above\r\n", "comments": ["@pw2393 ,\r\nWhen tried running the given code, session crashed all the time. Is the same error faced ? Find the [gist](https://colab.sandbox.google.com/gist/oanush/9a232909b3ae54c70913a3df6491cab3/33894.ipynb) of colab.Thanks!", "> @pw2393 ,\r\n> When tried running the given code, session crashed all the time. Is the same error faced ? Find the [gist](https://colab.sandbox.google.com/gist/oanush/9a232909b3ae54c70913a3df6491cab3/33894.ipynb) of colab.Thanks!\r\n\r\n@oanush Yes. It crashes. If running it in simply Python Interpreter, one may see `Segmentation fault (core dumped)`. (In colab case, that error sends a sys signal which leads to a session crash.)", "Hello @pw2393 , this happens because `v.variable` refers to a device which is recreated before cluster initialization. `set_server_def` essentially recreates the devices and device managers (and also destroy existing ones) so `v.device` points to invalid memory and accessing it would cause segfault. This for now is intended behavior.\r\n\r\nIn the current codebase (you can install the latest nightly release by `pip install tf-nightly`), we provide an experimental `update_server_def` API that users can call to add/remove workers in a cluster, while still able to access ops/tensors on unchanged devices. Note that the variable devices must still be valid after the update for them to be accessed successfully. For example,\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf.tensorflow_server_pb2 import ServerDef\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.python.training import server_lib\r\nfrom tensorflow.python.training.server_lib import ClusterSpec\r\n\r\ncluster_def = ClusterSpec({'localhost': ['localhost:15293']}).as_cluster_def()\r\nserver_def = ServerDef(cluster=cluster_def, job_name='localhost', task_index=0, protocol='grpc')\r\n\r\ncontext.set_server_def(server_def)\r\nv = tf.Variable(3)\r\nprint(v.device)\r\n\r\ncontext.update_server_def(server_def)\r\nprint(v.device)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33894\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33894\">No</a>\n", "@haoyuz Thanks a lot! BTW, any chance you might know how to deal with #34242 ?"]}, {"number": 33893, "title": "Support INT8 quantisation for RESIZE_NEAREST_NEIGHBOR with TFLITE_BUILTINS_INT8 OpsSet", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.0.0\r\n\r\nTFLiteConverter post-training quantisation flow does not support RESIZE_NEAREST_NEIGHBOR op\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nRuntimeError: Quantization not yet supported for op: RESIZE_NEAREST_NEIGHBOR\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nSource code (as in [guide](https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations)):\r\n\r\n```\r\nmodel = tf.keras.models.load_model('mobilenetv2.h5')\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = calibrate\r\ntflite_model = converter.convert()\r\n```\r\n\r\nFull traceback:\r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-29-0a875b86673a> in <module>()\r\n      5 converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n      6 converter.representative_dataset = calibrate\r\n----> 7 tflite_model = converter.convert()\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    467       result = self._calibrate_quantize_model(\r\n    468           result, constants.FLOAT, constants.FLOAT,\r\n--> 469           self.experimental_new_quantizer)\r\n    470 \r\n    471     return result\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type, enable_mlir_quantizer)\r\n    241     return calibrate_quantize.calibrate_and_quantize(\r\n    242         self.representative_dataset.input_gen, inference_input_type,\r\n--> 243         inference_output_type, allow_float, enable_mlir_quantizer)\r\n    244 \r\n    245   def _get_base_converter_args(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float, enable_mlir_quantizer)\r\n     79         np.dtype(input_type.as_numpy_dtype()).num,\r\n     80         np.dtype(output_type.as_numpy_dtype()).num, allow_float,\r\n---> 81         enable_mlir_quantizer)\r\n     82 \r\n     83   def calibrate_and_quantize_single(self, dataset_gen, input_type, output_type,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in QuantizeModel(self, *args)\r\n    113 \r\n    114     def QuantizeModel(self, *args):\r\n--> 115         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, *args)\r\n    116 CalibrationWrapper_swigregister = _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_swigregister\r\n    117 CalibrationWrapper_swigregister(CalibrationWrapper)\r\n\r\nRuntimeError: Quantization not yet supported for op: RESIZE_NEAREST_NEIGHBOR\r\n```", "comments": ["Hi @xadrianzetx \r\n\r\nHave you tried tf-nightly? This bug was fixed a few days ago. But that is not in 2.0.0 release.", "Hi @wuhy08 \r\n\r\nJust tried [latest nightly](https://pypi.org/project/tf-nightly/2.1.0.dev20191106/) and indeed RESIZE_NEAREST_NEIGHBOR is now supported. This solved my issue. Thanks!"]}, {"number": 33892, "title": "tf.image.per_image_standardization return wrong values", "body": "Tensorflow version = 1.15\r\ntf.image.per_image_standardization does not standardize value to [-1,1] \r\ni test in tensorflow 1.12 it work correctly\r\n \r\n```python\r\nimport tensorflow as tf \r\nimport numpy as np \r\nprint(tf.__version__)\r\na = tf.constant([[[1,2,3],[4,5,6]]])\r\nb = tf.image.per_image_standardization(a)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run(b))\r\nx = np.asarray([[[1,2,3],[4,5,6]]], dtype=np.float64)\r\n\r\ndef normalize_meanstd(a, axis=None): \r\n    # axis param denotes axes along which mean & std reductions are to be performed\r\n    mean = np.mean(a, axis=axis, keepdims=True)\r\n    std = np.sqrt(((a - mean)**2).mean(axis=axis, keepdims=True))\r\n    return (a - mean) / std\r\n\r\nprint(normalize_meanstd(x))\r\n```\r\n```\r\n1.15.0\r\n[[[-6 -3 -1]\r\n  [ 1  3  6]]]\r\n[[[-1.46385011 -0.87831007 -0.29277002]\r\n  [ 0.29277002  0.87831007  1.46385011]]]\r\n```", "comments": ["It's because a is interpreted as an int tensor instead of a float tensor.  If you define \r\na = tf.constant([[[1.0,2.0,3.0],[4.0,5.0,6.0]]])\r\nyou get the correct behavior.", "thanks! it should handle int tensor because image normally in uint8 and since when we are using \"tf.image.per_image_standardization\"  we will not change the data type as we expect  the function handle it. ", "@skyap Looks like there is some issue with converting dtype from `int` to `float` with this `convert_image_dtype` function in the source code of `tf.image.per_image_standardization`. We will take a look deeper at the source. Source code is [here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/image_ops_impl.py#L1477-L1519). Thanks!", "The same issue still exists in TensorFlow version 2.3.0. \r\n`x = [[[0.8, 1., 1.],\r\n      [1., 1., 1.]],\r\n    [[0., 0., 0.],\r\n      [0.8, 0.8, 1.]]]`\r\n`y = tf.image.convert_image_dtype(x, dtype=tf.uint8, saturate=False)`\r\n`normal_y = tf.image.per_image_standardization(y)`\r\n`print(normal_y)` -> `tf.Tensor(\r\n[[[ 61 185 185]\r\n  [185 185 185]]\r\n [[  0   0   0]\r\n  [ 61  61 185]]], shape=(2, 2, 3), dtype=uint8)`\r\nThis is slightly inconvenient, as @skyap mentioned that a lot of times images are stored in uint8 format, including the returned image from `tf.io.decode_jpeg`. This also conflicts with the documentation of `tf.image.convert_image_dtype` https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization, where the description says \"Linearly scales each image in image to have mean 0 and variance 1.\" \r\n\r\nAnd as @jvishnuvardhan mentioned, the problem exists in the use of` convert_image_dtype`. According to https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype, images in floating point values are expected to have values in the range [0,1) and images in integer values are expected to have values in the range [0,MAX]. When converting the dtype from float into uint, `convert_image_dtype` rescales the standardized image back into the expected range of [0, MAX]. But it doesn't rescale the standardized floating point values into its expected range of [0,1), so the `tf.image.per_image_standardization` can still work as expected if the original image is in floating point values. \r\n\r\nI wonder if the documentation of `image.per_image_standardization` could be revised to add a reminder of dtype , or if the convert_image_dtype function can be revised to handle these cases appropriately. Based on the current documentation, as a user I'd be expecting the `image.per_image_standardization` to return to me me pixel values with mean 0 and variance 1 directly without having to converting the image dtype.", "We have been dealing with inconsistencies with training for a while now, and have found the root cause to be related to this issue.\r\n\r\nHere is the offending commit:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/906e0e3bc0dfe12db19afa261e4d793b73cb64ec\r\n\r\nIn version 1.14, per_image_standardization internally converted integer images to float and returned the standardized image as a float.\r\n\r\nIn version 1.15+, per_image_standardization internally converts the integer images to a float then divides by the max dtype value, and converts it back to an integer image.\r\n\r\nThis is especially bad for users of 1.14 and earlier who upgraded to 1.15 or 2.0+ as their integer image data would be quantized and/or truncated (e.g. unsigned int) by per_image_standardization.\r\n\r\nSuggest updating the documentation and/or throwing an exception if the input type is unsigned int. See example below:\r\n\r\n``` python\r\nimport tensorflow as tf \r\nimport numpy as np \r\nprint(tf.__version__)\r\n# unsigned int example (works in 1.14, but really, really, bad in 1.15+)\r\na = tf.constant([[[1,2,3],[4,5,6]]], tf.uint16)\r\nb = tf.image.per_image_standardization(a)\r\nx = np.asarray([[[1,2,3],[4,5,6]]], dtype=np.float64)\r\n\r\ndef normalize_meanstd(a, axis=None): \r\n    # axis param denotes axes along which mean & std reductions are to be performed\r\n    mean = np.mean(a, axis=axis, keepdims=True)\r\n    std = np.sqrt(((a - mean)**2).mean(axis=axis, keepdims=True))\r\n    return (a - mean) / std\r\n\r\nprint(b)\r\nprint(normalize_meanstd(x))\r\n```\r\n```\r\n2.2.0\r\ntf.Tensor(\r\n[[[0 0 0]\r\n  [1 3 6]]], shape=(1, 2, 3), dtype=uint16)\r\n[[[-1.46385011 -0.87831007 -0.29277002]\r\n  [ 0.29277002  0.87831007  1.46385011]]]\r\n```\r\n", "Related github issue: #44983 \r\n\r\nThe changes in https://github.com/tensorflow/tensorflow/commit/906e0e3bc0dfe12db19afa261e4d793b73cb64ec unintentionally modified the behavior of `tf.image.per_image_standardization` and is reverted in https://github.com/tensorflow/tensorflow/commit/b6be9714e878a7dd0d1405bd7a83e021ba4b561a. \r\n\r\nWith `tf-nightly`, the function should return expected values for `uint` data types as before. Here is a [colab](https://colab.research.google.com/drive/1nqzU22tZ0RVAaDm6drWPkfGV8cc7A_3x?usp=sharing) verifying the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33892\">No</a>\n"]}, {"number": 33891, "title": "'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory", "body": "Ubuntu 16.04, Python 2.7.12\r\n\r\n~$ python\r\nPython 2.7.12 (default, Oct  8 2019, 14:14:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n2019-11-01 02:23:07.614115: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-01 02:23:07.640540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-01 02:23:07.641857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65\r\npciBusID: 0000:0a:00.0\r\n2019-11-01 02:23:07.641907: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n2019-11-01 02:23:07.641942: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n2019-11-01 02:23:07.641975: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\r\n2019-11-01 02:23:07.642007: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\r\n2019-11-01 02:23:07.642040: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\r\n2019-11-01 02:23:07.642072: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\r\n2019-11-01 02:23:07.644213: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-01 02:23:07.644227: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2019-11-01 02:23:07.644455: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-01 02:23:07.666762: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 4000070000 Hz\r\n2019-11-01 02:23:07.667387: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fa0e80 executing computations on platform Host. Devices:\r\n2019-11-01 02:23:07.667399: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-01 02:23:07.739881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-01 02:23:07.740582: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5003bc0 executing computations on platform CUDA. Devices:\r\n2019-11-01 02:23:07.740595: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-11-01 02:23:07.740645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-01 02:23:07.740651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \r\n>>> \r\n\r\n", "comments": ["**Please provide system information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33891\">No</a>\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Also experiencing this issue:\r\n\r\nUbuntu 20.04\r\nKernel: 5.4.0-47-generic\r\n\r\nPython 3.6.10 :: Anaconda, Inc.\r\n\r\n```\r\ntensorflow==2.3.0\r\ntensorflow-estimator==2.3.0\r\ntensorflow-gpu==2.3.0\r\n```\r\n\r\nInstalled using `pip` inside conda environment\r\n\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Wed_Jul_22_19:09:09_PDT_2020\r\nCuda compilation tools, release 11.0, V11.0.221\r\nBuild cuda_11.0_bu.TC445_37.28845127_0\r\n```\r\n\r\n```\r\n$ cat /usr/local/cuda/version.txt\r\nCUDA Version 11.0.228\r\n```\r\n\r\n```\r\nFri Sep 18 18:56:46 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 750     On   | 00000000:01:00.0  On |                  N/A |\r\n| 39%   31C    P8     1W /  52W |    526MiB /  1995MiB |     17%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n"]}, {"number": 33890, "title": "[TF 2.0] Using Keras custom layers, cannot learn on Colaboratory on TPU.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Colaboratory on TPU\r\n- TensorFlow version (use command below):2.0.0\r\n- Python version:Python 3.6.8\r\n\r\n**Describe the current behavior**\r\nTansfomer, which uses Keras custom layers, cannot learn on Colaboratory's TPU.\r\nIt shows the error below.\r\n```\r\nUnimplementedError:  Compilation failure: Asked to propagate a dynamic dimension from hlo %scatter.14694 = f32[8333,256]{1,0} scatter(f32[8333,256]{1,0} %broadcast.14689, s32[320]{0} %reshape.2569, f32[320,256]{1,0} %reshape.14686), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.14690, metadata={op_type=\"UnsortedSegmentSum\" op_name=\"Adam/CrossReplicaSum/input\"}@{}@0 to hlo %all-reduce.14699 = f32[8333,256]{1,0} all-reduce(f32[8333,256]{1,0} %scatter.14694), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.14695, metadata={op_type=\"CrossReplicaSum\" op_name=\"Adam/CrossReplicaSum\"}, which is not implemented.\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_16451521731088977986/_6}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572537805.656904528\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\" Compilation failure: Asked to propagate a dynamic dimension from hlo %scatter.14694 = f32[8333,256]{1,0} scatter(f32[8333,256]{1,0} %broadcast.14689, s32[320]{0} %reshape.2569, f32[320,256]{1,0} %reshape.14686), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.14690, metadata={op_type=\"UnsortedSegmentSum\" op_name=\"Adam/CrossReplicaSum/input\"}@{}@0 to hlo %all-reduce.14699 = f32[8333,256]{1,0} all-reduce(f32[8333,256]{1,0} %scatter.14694), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.14695, metadata={op_type=\"CrossReplicaSum\" op_name=\"Adam/CrossReplicaSum\"}, which is not implemented.\\n\\tTPU compilation failed\\n\\t [[{{node tpu_compile_succeeded_assert/_16451521731088977986/_6}}]]\",\"grpc_status\":12} [Op:__inference_distributed_function_41790]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```\r\nThe code I used is from the following site and I wanted it to work with TPU.\r\n(https://medium.com/tensorflow/a-transformer-chatbot-tutorial-with-tensorflow-2-0-88bf59e66fe2)\r\n\r\n**Describe the expected behavior**\r\nThe Model can be learned on Colaboratory's TPU using custom layers.\r\n\r\n**Code to reproduce the issue**\r\nHere is a notebook to reproduce the problem. Look at this.\r\nhttps://colab.research.google.com/github/july1997/transformer_chatbot_tpu/blob/master/transformer_chatbot_tf2_fix_tpu.ipynb\r\n\r\n**Other info / logs**\r\nHere all Logs.\r\n```\r\nINFO:tensorflow:Initializing the TPU system: 10.8.123.210:8470\r\nINFO:tensorflow:Initializing the TPU system: 10.8.123.210:8470\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nTrain on 689 steps\r\nEpoch 1/20\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-23-234b607b615c> in <module>()\r\n     34   model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\r\n     35 \r\n---> 36   model.fit(create_dataset(questions, answers), epochs=EPOCHS)\r\n\r\n11 frames\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    683         validation_steps=validation_steps,\r\n    684         validation_freq=validation_freq,\r\n--> 685         steps_name='steps_per_epoch')\r\n    686 \r\n    687   def evaluate(self,\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    297           else:\r\n    298             actual_inputs = ins()\r\n--> 299           batch_outs = f(actual_inputs)\r\n    300         except errors.OutOfRangeError:\r\n    301           if is_dataset:\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/distribute/distributed_training_utils.py in execution_function(input_fn)\r\n    876       def execution_function(input_fn):\r\n    877         # `numpy` translates Tensors to values in Eager mode.\r\n--> 878         return [out.numpy() for out in distributed_function(input_fn)]\r\n    879     else:\r\n    880       execution_function = distributed_function\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    518         # Lifting succeeded, so variables are initialized and we can run the\r\n    519         # stateless function.\r\n--> 520         return self._stateless_fn(*args, **kwds)\r\n    521     else:\r\n    522       canon_args, canon_kwds = \\\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n   1825   @property\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnimplementedError:  Compilation failure: Asked to propagate a dynamic dimension from hlo %scatter.14694 = f32[8333,256]{1,0} scatter(f32[8333,256]{1,0} %broadcast.14689, s32[320]{0} %reshape.2569, f32[320,256]{1,0} %reshape.14686), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.14690, metadata={op_type=\"UnsortedSegmentSum\" op_name=\"Adam/CrossReplicaSum/input\"}@{}@0 to hlo %all-reduce.14699 = f32[8333,256]{1,0} all-reduce(f32[8333,256]{1,0} %scatter.14694), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.14695, metadata={op_type=\"CrossReplicaSum\" op_name=\"Adam/CrossReplicaSum\"}, which is not implemented.\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_16451521731088977986/_6}}]]\r\nAdditional GRPC error information:\r\n{\"created\":\"@1572537805.656904528\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\" Compilation failure: Asked to propagate a dynamic dimension from hlo %scatter.14694 = f32[8333,256]{1,0} scatter(f32[8333,256]{1,0} %broadcast.14689, s32[320]{0} %reshape.2569, f32[320,256]{1,0} %reshape.14686), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%scatter-combiner.14690, metadata={op_type=\"UnsortedSegmentSum\" op_name=\"Adam/CrossReplicaSum/input\"}@{}@0 to hlo %all-reduce.14699 = f32[8333,256]{1,0} all-reduce(f32[8333,256]{1,0} %scatter.14694), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.14695, metadata={op_type=\"CrossReplicaSum\" op_name=\"Adam/CrossReplicaSum\"}, which is not implemented.\\n\\tTPU compilation failed\\n\\t [[{{node tpu_compile_succeeded_assert/_16451521731088977986/_6}}]]\",\"grpc_status\":12} [Op:__inference_distributed_function_41790]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```\r\n", "comments": ["It is a known issue https://github.com/tensorflow/tensorflow/issues/33517 . According to @yunxing (https://github.com/tensorflow/tensorflow/issues/33517#issuecomment-546105978 ), the fix should have been applied in the tf-nightly release.\r\n", "Thank you for telling me! I try tf-nightly release.", "I have the same issue with TF 2.6.0:\r\n```\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py in sync_executors(self)\r\n    672     \"\"\"\r\n    673     if self._context_handle:\r\n--> 674       pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\r\n    675     else:\r\n    676       raise ValueError(\"Context is not initialized.\")\r\n\r\nUnimplementedError: 9 root error(s) found.\r\n  (0) Unimplemented: {{function_node __inference_train_function_94596}} Asked to propagate a dynamic dimension from hlo convolution.31970@{}@2 to hlo %all-reduce.31975 = f32[3,3,<=3,40]{3,2,1,0} all-reduce(f32[3,3,<=3,40]{3,2,1,0} %convolution.31970), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.31971, metadata={op_type=\"CrossReplicaSum\" op_name=\"Adam/CrossReplicaSum\"}, which is not implemented.\r\n\t [[{{node TPUReplicate/_compile/_3223262300518379275/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_4226575268796567461/_6/_145]]\r\n  (1) Unimplemented: {{function_node __inference_train_function_94596}} Asked to propagate a dynamic dimension from hlo convolution.31970@{}@2 to hlo %all-reduce.31975 = f32[3,3,<=3,40]{3,2,1,0} all-reduce(f32[3,3,<=3,40]{3,2,1,0} %convolution.31970), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.31971, metadata={op_type=\"CrossReplicaSum\" op_name=\"Adam/CrossReplicaSum\"}, which is not implemented.\r\n\t [[{{node TPUReplicate/_compile/_3223262300518379275/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_4226575268796567461/_6/_159]]\r\n  (2) Unimplemented: {{function_node __inference_train_function_94596}} Asked to propagate a dynamic dimension from hlo convolution.31970@{}@2 to hlo %all-reduce.31975 = f32[3,3,<=3,40]{3,2,1,0} all-reduce(f32[3,3,<=3,40]{3,2,1,0} %convolution.31970), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.31971, metadata={op_type=\"CrossReplicaSum\" op_name=\"Adam/CrossReplicaSum\"}, which is not implemented.\r\n\t [[{{node TPUReplicate/_compile/_3223262300518379275/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_4226575268796567461/_6/_215]]\r\n  (3) Unimplemented: {{function_node __inference_train_function_94596}} Asked to propagate a dynamic dimension from hlo convolution.31970@{}@2 to hlo %all-reduce.31975 = f32[3,3,<=3,40]{3,2,1,0} all-reduce(f32[3,3,<=3,40]{3,2,1,0} %convolution.31970), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.31971, metadata={op_type=\"CrossReplicaSum\" op_name=\"Adam/CrossReplicaSum\"}, which is not implemented.\r\n\t [[{{node TPUReplicate/_compile/_3223262300518379275/_5}}]]\r\n\t [[TPUReplicate/_compile/_3223262300518379275/_5/_240]]\r\n  (4) Unimplemented: {{function_node __inference_train_function_94596}} Asked to propagate a dynamic dimension from hlo convolution.31970@{}@2 to hlo %all-reduce.31975 = f32[3,3,<=3,40]{3,2,1,0} all-reduce(f32[3,3,<=3,40]{3,2,1,0} %convolution.31970), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.31971, metadata={op_type=\"CrossReplicaSum\" op_name=\"Adam/CrossReplicaSum\"}, which is not implemented.\r\n\t [[{{node TPUReplicate/_compile/_3223262300518379275/_5}}]]\r\n\t [[NoOp/_279]]\r\n  (5) Unimplemented: {{function_node __inference_train_function_94596}} Asked to propagate a dynamic dimension from hlo convolution.31970@{}@2 to hlo %all-reduce.31975 = f32[3,3,<=3,40]{3,2,1,0} all-reduce(f32[3,3,<=3,40]{3,2,1,0} %convolution.31970), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.31971, metadata={op_type=\"CrossReplicaSum\" op_name=\"Adam/CrossReplicaSum\"}, which is not implemented.\r\n\t [[{{node TPUReplicate/_compile/_3223262300518379275/_ ... [truncated]\r\n```\r\n\r\nThe model is compiled as:\r\n`model.compile(loss='mse', metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()], optimizer='adam')`"]}, {"number": 33889, "title": "Update README.md", "body": "added hyperlink to CUDA GPU", "comments": ["@mihaimaruseac can we take this change ?", "I'd say no, there is not a real information gain from the hyper-link to something that is so commonly known. Looks like last attempt to submit 4 PRs for Hacktoberfest.", "sure, thank you "]}, {"number": 33888, "title": "Bug in saving model in hdf5 format", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs Mojave version 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: \r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen try to save the below model in keras format, we get the following error:\r\nValueError: Unable to create group (name already exists)\r\n\r\nThis happens as this model has three layers with name as below:\r\ntf_op_layer_Pad/paddings/0\r\ntf_op_layer_Pad/paddings\r\ntf_op_layer_Pad\r\n\r\nSuch name causes error in keras as described here - https://github.com/keras-team/keras/issues/12195\r\n\r\n**Describe the expected behavior**\r\nModel saving should not fail.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nx = keras.Input(shape=(None,10), dtype=\"int32\", name=\"input\")\r\nT = tf.shape(x)[0]\r\nto_pad = -T % 2\r\ny = tf.pad(x, [[0, to_pad], [0, 0], [0, 0]])\r\nmodel = keras.Model(inputs=[x,], outputs=[y,])\r\nmodel.save(\"model.h5\")\r\n```\r\n\r\n**Other info / logs**\r\nThis fix for this has been checked into keras few days ago it seems - https://github.com/keras-team/keras/pull/13477/commits/7dee298ebec503c6b0e1727dfd49b89a3fb002d7\r\n\r\nBut it seems TF has its own copy of this hdf5 saving, so it seems this fix will also have to be made there -\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/hdf5_format.py#L624\r\n", "comments": ["Saving the model to TensorFlow format (tf) works where as hdf5 saving fails with TF nightly version '2.1.0-dev20191101'\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nx = keras.Input(shape=(None,10), dtype=\"int32\", name=\"input\")\r\nT = tf.shape(x)[0]\r\nto_pad = -T % 2\r\ny = tf.pad(x, [[0, to_pad], [0, 0], [0, 0]])\r\nmodel = keras.Model(inputs=[x,], outputs=[y,])\r\nmodel.save(\"models.tf\")\r\n```", "We need to backport https://github.com/keras-team/keras/pull/13477", "I believe this is a duplicate of https://github.com/tensorflow/tensorflow/issues/33565", "I have the same problem using TF 2.1.0. Calling keras from tensorflow will lead to the same error. As mentioned by @sonu1-p , TF hasn't updated its own copy of this hdf5 saving.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33888\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33888\">No</a>\n"]}, {"number": 33887, "title": "Fix out-of-sync issue in ignore_errors with tf.data.Dataset.zip", "body": "This PR tries to address the issue raised in #33383 where ignore_errors combined with tf.data.Dataset.zip will be out-of-sync for component.\r\n\r\nThe issue was that, in case of zip, remaining components were\r\nnot flushed out when end-of-sequence of error encountered.\r\n\r\nThis PR fixes the isuse.\r\n\r\nThis PR fixes #33383.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@jsimsa Thanks for the review. The PR has been updated. Please take a look and let me know if there are any other issues.", "@jsimsa Thanks for the detailed review! The PR has been updated.", "@jsimsa It looks like the status changed back to `not approved` as I push the update to PR. Would you mind re-approve again? Thanks a lot for the review and help!"]}, {"number": 33886, "title": "Problems with setting initial state in custom RNN cell", "body": "Currently I am trying to reimplement a publication in TF2.0. For the architecture presented in said publication, I started to tweak and extend the ```SimpleRNNCell```class. Next to some changes in the call method, the main addition is to have an additional matrix that is saved in the cell state. Thus next to default hidden state (```shape=(units)```), I also carry around a matrix (```shape=(units, units)```) in the cell state. I worked out how to do this using the ```TensorShape```class, and the cell works fine, but I would now like to set custom initiale state to the matrix; to be more specific I want to initialise it as an identity matrix.  \r\nI tried different method of supplying a ```initial_state```to the call method, but every time it seems there is a problem with how I supply it and an error of the type\r\n```\r\nValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=ListWrapper([InputSpec(shape=(50,), ndim=1), InputSpec(shape=(50, 50), ndim=2)]); however `cell.state_size` is [TensorShape([50]), TensorShape([50, 50])]\r\n```\r\nis thrown.\r\n\r\nHere is how I set up the cell state:\r\n```python\r\nself.state_size = NoDependency([TensorShape([self.units]),\r\n                                        TensorShape([self.units, self.units])])\r\n\r\n```\r\nand this is how I try to initialise them when calling:\r\n```python\r\nx = rnn_layer(x, initial_state = [tf.zeros(self.units), tf.eye(self.units)]).\r\n```\r\n\r\nI believe this is due to the fact of using ```TensorShape```, but I did not find any other method to create a matrix (in contrast to a vector) in the cell state. Thank you in advance.", "comments": ["Usually you don't need the TensorShape, just the raw value is good enough. So:\r\n\r\n```python\r\nself.state_size = NoDependency([self.units, [self.units, self.units])\r\n```", "Initializing the ```self_state``` like so, results in the following error:\r\n```\r\nValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=ListWrapper([InputSpec(shape=(50,), ndim=1), InputSpec(shape=(50, 50), ndim=2)]); however `cell.state_size` is [50, [50, 50]]\r\n```\r\nI started off like this, but the problem is that the second element is not interpreted as the shape of a matrix, but instead as a tuple of vector shapes.", "Can u provide me a short code snippet to reproduce your issue?", "Hey,\r\n\r\nsure here you go (not really short, but hope good enough):\r\n\r\n```python\r\nimport os\r\nimport random\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.layers import Layer, Dense, LayerNormalization\r\nfrom tensorflow.keras.activations import softmax, relu\r\nfrom tensorflow.python.keras.layers.recurrent import RNN, SimpleRNN, LSTM, GRU\r\nfrom tensorflow.python.keras import activations, initializers\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow.python.training.tracking.data_structures import NoDependency\r\nfrom tensorflow.python.framework.tensor_shape import TensorShape\r\n\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.losses import CategoricalCrossentropy\r\nfrom tensorflow.keras.metrics import CategoricalAccuracy\r\n\r\nfrom tensorflow.keras.datasets import mnist\r\n\r\n# set random seeds\r\nRANDOM_SEED = 1\r\ntf.random.set_seed(RANDOM_SEED)\r\nnp.random.seed(RANDOM_SEED)\r\nrandom.seed(RANDOM_SEED)\r\nos.environ['PYTHONHASHSEED'] = '0'\r\n\r\nclass BasicConfig(object):\r\n    def __init__(self):\r\n        self.num_epochs = 10\r\n        self.batch_size = 128\r\n        self.units = 50\r\n\r\n        self.output_dim = 10\r\n\r\n        self.ac_alpha = 60\r\n        self.ac_lambda = 0.105\r\n\r\n        self.learning_rate = 0.01\r\n        self.optimizer = Adam(self.learning_rate)\r\n        self.loss_function = CategoricalCrossentropy()\r\n        self.metric = CategoricalAccuracy()\r\n\r\n\r\n\r\nclass Model(Layer):\r\n    def __init__(self, c, **kwargs):\r\n        super(Model, self).__init__(**kwargs)\r\n\r\n        self.rnn_layer = RNN(AutoconceptorCell(c.units, alpha=c.ac_alpha,\r\n                            lamb=c.ac_lambda))\r\n        self.dense = Dense(c.output_dim, activation=softmax)\r\n\r\n    def call(self, x):\r\n        # TODO GITHUB line below causes error\r\n        # x = rnn_layer(x, initial_state = [tf.zeros(self.units), tf.eye(self.units)])\r\n        x = self.rnn_layer(x)\r\n        output = self.dense(x)\r\n        return output\r\n\r\n\r\nclass AutoconceptorCell(Layer):\r\n    \"\"\"\r\n    Cell, very very similar to SimpleRNNCell, but added a matrix to the\r\n    state, that is used in the call\r\n    \"\"\"\r\n\r\n    def __init__(self,\r\n               units,\r\n               alpha,\r\n               lamb,\r\n               activation='tanh',\r\n               kernel_initializer='glorot_uniform',\r\n               recurrent_initializer='identity',\r\n               bias_initializer='glorot_uniform',\r\n               **kwargs):\r\n        super(AutoconceptorCell, self).__init__(**kwargs)\r\n\r\n        self.units = units\r\n\r\n        self.aperture_fact = alpha**(-2)\r\n        self.l = lamb \r\n\r\n        self.activation = activations.get(activation)\r\n\r\n        self.kernel_initializer = initializers.get(kernel_initializer)\r\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\r\n        self.bias_initializer = initializers.get(bias_initializer)\r\n\r\n        # IMPORTANT here i declare the state_size\r\n        self.state_size = NoDependency([self.units,\r\n                                            TensorShape([self.units, self.units])])\r\n\r\n        self.output_size = self.units\r\n\r\n        self.layer_norm = LayerNormalization(epsilon=1e-6)\r\n\r\n\r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(\r\n            shape=(input_shape[-1], self.units),\r\n            name='kernel',\r\n            initializer=self.kernel_initializer)\r\n\r\n        self.recurrent_kernel = self.add_weight(\r\n            shape=(self.units, self.units),\r\n            name='recurrent_kernel',\r\n            initializer=self.recurrent_initializer)\r\n\r\n        self.bias = self.add_weight(\r\n            shape=(self.units,),\r\n            name='bias',\r\n            initializer=self.bias_initializer)\r\n\r\n        self.built = True\r\n\r\n\r\n    def call(self, inputs, states):\r\n        # hidden state, and Conceptor matrix\r\n        prev_h, C = states\r\n\r\n        # input influence\r\n        input_infl = K.bias_add(K.dot(inputs, self.kernel), self.bias)\r\n        # recurrent influence\r\n        recurrent_infl = K.dot(prev_h, self.recurrent_kernel)\r\n\r\n        # h i.e. typical hidden layer update\r\n        h = self.activation(input_infl + recurrent_infl)\r\n\r\n        # update conceptor\r\n        C = C + self.l * (K.batch_dot(tf.expand_dims(h - K.batch_dot(h, C, axes=(1,1)), 2), tf.expand_dims(h, 1)) - self.aperture_fact * C)\r\n\r\n        # apply filter, then layer norm, then AGAIN activation\r\n        h = self.activation(self.layer_norm(K.batch_dot(h, C, axes=(1,1))))\r\n      \r\n        return h, [h, C]\r\n\r\n\r\n\r\ndef get_mnist_data(batch_size=128, input_dim=28):\r\n    (x_train, y_train), (x_test, y_test) = mnist.load_data(path='mnist.npz')\r\n\r\n    # transform target digits into one-hot-vectors\r\n    y_train = tf.keras.utils.to_categorical(y_train)\r\n    y_test = tf.keras.utils.to_categorical(y_test)\r\n\r\n    # normalize input TODO why?\r\n    x_train = x_train.astype('float32') / 255\r\n    x_test = x_test.astype('float32') / 255\r\n\r\n    # for single pixel mnist (i.e. minst minst_784) reshape input\r\n    if (input_dim==1):\r\n        x_train = x_train.reshape(x_train.shape[0], 28, 28)\r\n        x_test = x_test.reshape(x_test.shape[0], 28, 28)\r\n\r\n     # bring into tf.data.Dataset\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\r\n\r\n    # bring into proper batch size\r\n    train_dataset = train_dataset.batch(config.batch_size)\r\n    test_dataset = test_dataset.batch(x_test.shape[0]) # test if full batch\r\n\r\n    return (train_dataset, test_dataset)\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    # config\r\n    config = BasicConfig()\r\n    # model with config\r\n    model = Model(config)\r\n\r\n    # data - minst\r\n    train_dataset, test_dataset = get_mnist_data(batch_size=config.batch_size)\r\n\r\n    for epoch_indx in range(config.num_epochs):\r\n        # train\r\n        for batch_indx, (x,t) in enumerate(train_dataset):\r\n            with tf.GradientTape() as tape:\r\n                output = model(x)\r\n\r\n                gradients = tape.gradient(config.loss_function(t, output), model.trainable_variables)\r\n            \r\n            config.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n        # test\r\n        for (x_test, t_test) in test_dataset:\r\n\r\n            output_test = model(x_test)\r\n\r\n        print(\"[E:{:3d}] Test Accuracy: {:04.5f}!\".format(epoch_indx,\r\n                                                    config.metric(t_test, output_test)))\r\n```", "I see. The issue is that the initial_state should have a shape of [batch, state_size]. In your case, you should pass in tf.zeros([batch_size, self.units]), tf.eye(self.units, batch_shape=[batch_size]), and the model will train correctly.", "You can also try cell.get_initial_state(inputs) and see what's the shape of returned initial state. The input here is purely used to get the batch size.", "Works perfectly, thank you very much!"]}, {"number": 33885, "title": "Tflite v1.10.1", "body": "i like this ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33885) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 33884, "title": "GRU layer fails on single GPU when using MirroredStrategy", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows and linux\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\n\r\nUsing single GPU and tf.keras.layers.GRU layer and model.fit with MirroredStrategy fails on error \r\n\r\n`Invalid argument:  var and delta do not have the same shape`\r\n\r\ntf.keras.layers.LSTM works fine on one GPU.\r\n\r\nOn machine with more GPU, the GRU layers works fine.\r\n\r\nI know it does not make much sense to run Mirrored Strategy on only one GPU but it is good for testing your code before uploading it to multi gpu machine.\r\n\r\nIt works in TF 1.5.\r\n\r\n**Code to reproduce the issue**\r\n\r\nOn Google Colab, use GPU runtime and run this code\r\n```\r\n%tensorflow_version 2.x\r\n# %tensorflow_version 1.x <- here it works\r\n\r\nimport tensorflow as tf\r\n\r\nprint(f'TensorFlow ver. {tf.__version__}')\r\nprint(f\"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\r\n\r\nds = tf.data.Dataset.from_tensor_slices({\r\n  'input': tf.zeros([64, 4]),\r\n  'target': tf.zeros([64, 5])\r\n})\r\nds = ds.batch(3)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nwith strategy.scope():\r\n\r\n  p_input = tf.keras.Input(shape=[4], name='input')\r\n  p_target = tf.keras.Input(shape=[5], name='target')\r\n\r\n  # gru = tf.keras.layers.LSTM(8) # <- this works\r\n  gru = tf.keras.layers.GRU(8)\r\n\r\n  x = p_input\r\n  x = tf.expand_dims(x, axis=-1)\r\n  x = gru(x)\r\n  x = tf.keras.layers.Dense(5, activation='tanh')(x)\r\n\r\n  model = tf.keras.Model([p_input, p_target], x)\r\n  model.add_loss(tf.keras.losses.MSE(p_target, x))\r\n\r\n  model.compile(optimizer=tf.keras.optimizers.SGD())\r\n  model.fit(ds)\r\n```\r\n\r\nOther info / logs\r\n```\r\nTensorFlow ver. 2.0.0\r\nNum GPUs Available: 1\r\nWARNING:tensorflow:Output dense_9 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_9.\r\n      1/Unknown - 3s 3s/step\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-10-e5114f02da9a> in <module>()\r\n     32 \r\n     33   model.compile(optimizer=tf.keras.optimizers.SGD())\r\n---> 34   model.fit(ds)\r\n\r\n11 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  var and delta do not have the same shape[8,24] [2,24]\r\n\t [[node SGD/SGD/update_1/update_0/ResourceApplyGradientDescent (defined at /tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/ops.py:1751) ]]\r\n  (1) Invalid argument:  var and delta do not have the same shape[8,24] [2,24]\r\n\t [[node SGD/SGD/update_1/update_0/ResourceApplyGradientDescent (defined at /tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/ops.py:1751) ]]\r\n\t [[ArithmeticOptimizer/ReorderCastLikeAndValuePreserving_int64_Cast_3/_18]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_24603]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```\r\n\r\n\r\n\r\n", "comments": ["I could reproduce the issue with `!pip install tf-nightly-gpu`. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/4331c4a13bf6983eb0359d81cec85f19/untitled617.ipynb) is the gist. Thanks!", "Thanks for reporting the issue. We also found a similar issue internally, it is caused by the graph rewrite between cpu kernel and gpu kernel.\r\n\r\nFor now, if you want to walk around the issue with some performance loss, you can add \"gru.could_use_cudnn = False\" to disable the cudnn path, which will allow the model to run, but just not use the fused cudnn GRU kernel.\r\n\r\nWe will fix the graph rewrite in future release.", "@reedwm for visibility.", "Thanks for the workaround.\r\n\r\nQuestion:\r\nConsidering the fact that it does work on multiple gpu, does it mean that the cuDNN path is not used  when more than one gpu is used?", "In the colab, TF 2.0 is still being used instead of the nightly, despite installing tf-nightly-gpu. I am not familiar with colab so I am not sure why this is happening. The colab prints the version is 2.0.0, but the nightly version will look similar to to 2.1.0-dev20191023. I confirmed the issue does not occur in the TF nightlies by running outside colab.\r\n\r\nThe fix will be in TF 2.1, so you might want to just wait until that is out. Closing this issue since this is fixed in the nightlies.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33884\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33884\">No</a>\n", "> Considering the fact that it does work on multiple gpu, does it mean that the cuDNN path is not used when more than one gpu is used?\r\n\r\nI am not sure, but since this will be fixed in 2.1, I do not think it's worth investigating."]}, {"number": 33883, "title": "I am trying to edit tensorflow lite docs but not able to find source code", "body": "link which I want to edit: [https://www.tensorflow.org/community/contribute/docs](https://www.tensorflow.org/community/contribute/docs)\r\nThe description of Digit classifier is incorrect", "comments": ["@mr-yamraj This is the file you are looking for: https://github.com/tensorflow/docs/blob/master/site/en/community/contribute/docs.md\r\n\r\nThe TensorFlow website code in stored in the `/site/` directory within the [`docs` repository](https://github.com/tensorflow/docs/tree/master/site). ", "I would be glad to have this issue assigned if any more help is required. \r\nOtherwise, this issue can be closed.  :)", "Thanks @nikochiko . @mr-yamraj Please close the issue if it was resolved by @nikochiko. Thanks!"]}, {"number": 33882, "title": "object detection tutorial not working", "body": "Hi guys,\r\nwhat is the error do you help\r\npython 3.8.0\r\ncuda 10.0\r\nprotoc 3.4.0 x32 \r\n\r\nERROR: Command errored out with exit status 1:\r\n   command: 'C:\\Users\\mertk\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\mertk\\\\AppData\\\\Local\\\\Temp\\\\pip-install-764_pxzy\\\\pycocotools\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\mertk\\\\AppData\\\\Local\\\\Temp\\\\pip-install-764_pxzy\\\\pycocotools\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\mertk\\AppData\\Local\\Temp\\pip-wheel-_obwf5qn' --python-tag cp37\r\n       cwd: C:\\Users\\mertk\\AppData\\Local\\Temp\\pip-install-764_pxzy\\pycocotools\\\r\n  Complete output (13 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build\\lib.win-amd64-3.7\r\n  creating build\\lib.win-amd64-3.7\\pycocotools\r\n  copying pycocotools\\coco.py -> build\\lib.win-amd64-3.7\\pycocotools\r\n  copying pycocotools\\cocoeval.py -> build\\lib.win-amd64-3.7\\pycocotools\r\n  copying pycocotools\\mask.py -> build\\lib.win-amd64-3.7\\pycocotools\r\n  copying pycocotools\\__init__.py -> build\\lib.win-amd64-3.7\\pycocotools\r\n  running build_ext\r\n  building 'pycocotools._mask' extension\r\n  error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for pycocotools\r\n    ERROR: Command errored out with exit status 1:\r\n     command: 'C:\\Users\\mertk\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\mertk\\\\AppData\\\\Local\\\\Temp\\\\pip-install-764_pxzy\\\\pycocotools\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\mertk\\\\AppData\\\\Local\\\\Temp\\\\pip-install-764_pxzy\\\\pycocotools\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\mertk\\AppData\\Local\\Temp\\pip-record-m7t3yhyc\\install-record.txt' --single-version-externally-managed --compile\r\n         cwd: C:\\Users\\mertk\\AppData\\Local\\Temp\\pip-install-764_pxzy\\pycocotools\\\r\n    Complete output (13 lines):\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build\\lib.win-amd64-3.7\r\n    creating build\\lib.win-amd64-3.7\\pycocotools\r\n    copying pycocotools\\coco.py -> build\\lib.win-amd64-3.7\\pycocotools\r\n    copying pycocotools\\cocoeval.py -> build\\lib.win-amd64-3.7\\pycocotools\r\n    copying pycocotools\\mask.py -> build\\lib.win-amd64-3.7\\pycocotools\r\n    copying pycocotools\\__init__.py -> build\\lib.win-amd64-3.7\\pycocotools\r\n    running build_ext\r\n    building 'pycocotools._mask' extension\r\n    error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: 'C:\\Users\\mertk\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\mertk\\\\AppData\\\\Local\\\\Temp\\\\pip-install-764_pxzy\\\\pycocotools\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\mertk\\\\AppData\\\\Local\\\\Temp\\\\pip-install-764_pxzy\\\\pycocotools\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\mertk\\AppData\\Local\\Temp\\pip-record-m7t3yhyc\\install-record.txt' --single-version-externally-managed --compile Check the logs for full command output.\r\n", "comments": ["@mertkislakci Can you please follow this [template](https://github.com/tensorflow/tensorflow/issues/new/choose) as it is too vague for us to understand the root cause of the issue. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue as it has been inactive for more than 14 days. Please add additional comments and we can open this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33882\">No</a>\n"]}, {"number": 33881, "title": "Setting Log Level via Java API?", "body": "**System information**\r\n- TensorFlow version: 1.14 (via Java API)\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using the current Java bindings for TensorFlow, the log gets filled with output from every operation when a model is evaluated. I was unable to find a way to set the log level via the Java API. Is this at all possible?\r\n", "comments": ["Active development on the TF Java API is happening on https://github.com/tensorflow/java, might be worth opening the issue there.", "Hey, thanks, but the code in that repository is not yet available as a maven artifact. The relevant code in my case is hosted in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java", "Yes, but there isn't any development happening on the API in the main tensorflow tree. I don't think it's possible to set the log level from Java at the moment. Open the issue on the Java API and we'll get to it.", "Ok, done: https://github.com/tensorflow/java/issues/3", "Can we close issue here and follow up in [tensorflow/java#3](https://github.com/tensorflow/java/issues/3) .Please, confirm.Thanks!", "Whatever is easiest for you.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "I don't think this has been resolved in tensorflow 2.X, or am I missing something?"]}, {"number": 33879, "title": "ERROR: An error occurred during the fetch of repository 'local_config_cuda': Traceback (most recent call last): File \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266 _create_local_cuda_repository(repository_ctx) File \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1199, in _create_local_cuda_repository _tpl(repository_ctx, \"crosstool:BUILD\", c...) File \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 745, in _tpl repository_ctx.template(out, Label((\"//third_party/gpus/%s...)), ...) class com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')", "body": "bazel version -- [bazel release 0.26.1]\r\ncuda 10.0\r\ncudnn 7\r\nubuntu 18.04\r\ntensorflow r1.14\r\n\r\nwhile building tensorflow, getting this error:\r\nINFO: Call stack for the definition of repository 'local_config_cuda' which is a cuda_configure (rule definition at /home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl:1268:18):\r\n - /home/shaurya/Downloads/tensorflow/tensorflow/workspace.bzl:63:5\r\n - /home/shaurya/Downloads/tensorflow/WORKSPACE:94:1\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1199, in _create_local_cuda_repository\r\n\t\t_tpl(repository_ctx, \"crosstool:BUILD\", c...)\r\n\tFile \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 745, in _tpl\r\n\t\trepository_ctx.template(out, Label((\"//third_party/gpus/%s...)), ...)\r\nclass com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1199, in _create_local_cuda_repository\r\n\t\t_tpl(repository_ctx, \"crosstool:BUILD\", c...)\r\n\tFile \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 745, in _tpl\r\n\t\trepository_ctx.template(out, Label((\"//third_party/gpus/%s...)), ...)\r\nclass com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1199, in _create_local_cuda_repository\r\n\t\t_tpl(repository_ctx, \"crosstool:BUILD\", c...)\r\n\tFile \"/home/shaurya/Downloads/tensorflow/third_party/gpus/cuda_configure.bzl\", line 745, in _tpl\r\n\t\trepository_ctx.template(out, Label((\"//third_party/gpus/%s...)), ...)\r\nclass com.google.devtools.build.lib.syntax.SkylarkList$MutableList cannot be cast to class java.lang.String (com.google.devtools.build.lib.syntax.SkylarkList$MutableList is in unnamed module of loader 'app'; java.lang.String is in module java.base of loader 'bootstrap')\r\nINFO: Elapsed time: 0.485s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n", "comments": ["Did you run `./configure` before running the build, as instructed in https://www.tensorflow.org/install/source#configure_the_build", "Yes, i followed the exact same step. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33879\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33879\">No</a>\n"]}, {"number": 33878, "title": "Strange behaviour of tf.sigmoid", "body": "**System information**\r\n- OS Platform and Distribution: Manjaro Linux testing\r\n- TensorFlow installed from: binary (pip tf-nightly)\r\n- TensorFlow version: v1.12.1-17077-ge9a3aa1 2.1.0-dev20191030\r\n- Python version: 3.7.4\r\n- CPU Model: Intel m3-6Y30\r\n\r\n**Describe the current behavior**\r\nIt seems that `tf.sigmoid` returns different values for identical inputs.\r\n\r\n**Describe the expected behavior**\r\nReturn identical outputs for identical inputs, regardless of the error scale.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.convert_to_tensor(\r\n    [[0.2, 0.2],\r\n     [0.2, 0.2],\r\n     [0.2, 0.2],\r\n     [0.2, 0.2],\r\n     [0.2, 0.2],],\r\n    dtype=tf.float32)\r\n\r\nsigm = tf.sigmoid(a)\r\n\r\nif tf.__version__.startswith('1'):\r\n  tf.InteractiveSession()\r\n  sigm = sigm.eval()\r\nelif tf.__version__.startswith('2'):\r\n  sigm = sigm.numpy()\r\nelse:\r\n  raise ValueError()\r\n\r\nassert((sigm[0] == sigm[1]).all()); print(\"*\"*1)\r\nassert((sigm[0] == sigm[2]).all()); print(\"*\"*2)\r\nassert((sigm[0] == sigm[3]).all()); print(\"*\"*3)\r\nassert((sigm[0] == sigm[4]).all()); print(\"*\"*4)\r\n```\r\n\r\n**Other info / logs**\r\nI can reproduce this in TensorFlow 1.x, 2.0, on GPU and CPU, on my laptop or in Google Colab.\r\n`1 / (1 + tf.exp(-a))` seems to return the expected output.\r\n\r\nTensorFlow appears to use the sigmoid implementation from the Eigen library, so I am trying to notify the contributors listed in the source files @benoitsteiner @ebrevdo @ggael \r\nI know that the differences are very small, yet the sigmoid function is at the core of some stateful operations such as LSTM, posing the risk of error accumulation. Besides that, I am simply curious to find out what causes this difference.", "comments": ["See: https://eigen.tuxfamily.org/bz/show_bug.cgi?id=1777", "+Alexandre Passos <apassos@google.com> +Rasmus Larsen\n<rmlarsen@google.com> this\nis a performance optimization I think.  Perhaps we should add a precision\nattr or graph config like xla does?\n\nOn Thu, Nov 14, 2019, 4:32 AM Gael Guennebaud <notifications@github.com>\nwrote:\n\n> See: https://eigen.tuxfamily.org/bz/show_bug.cgi?id=1777\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33878?email_source=notifications&email_token=AANWFG2WJCOAVTJA34737U3QTVOQ3A5CNFSM4JHJMTK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEECAWGY#issuecomment-553913115>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFGYC2NCJELAOBB32VOTQTVOQ3ANCNFSM4JHJMTKQ>\n> .\n>\n", "This particular optimization is problematic, since it gives values that are\nvery far off for gradients of values below -14. I have a TODO to remove it\nand just rely on the vectorized version of division, addition and exp.\n\nOn Thu, Nov 14, 2019 at 11:16 AM Eugene Brevdo <ebrevdo@google.com> wrote:\n\n> +Alexandre Passos <apassos@google.com> +Rasmus Larsen\n> <rmlarsen@google.com> this is a performance optimization I think.\n> Perhaps we should add a precision attr or graph config like xla does?\n>\n> On Thu, Nov 14, 2019, 4:32 AM Gael Guennebaud <notifications@github.com>\n> wrote:\n>\n>> See: https://eigen.tuxfamily.org/bz/show_bug.cgi?id=1777\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/33878?email_source=notifications&email_token=AANWFG2WJCOAVTJA34737U3QTVOQ3A5CNFSM4JHJMTK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEECAWGY#issuecomment-553913115>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AANWFGYC2NCJELAOBB32VOTQTVOQ3ANCNFSM4JHJMTKQ>\n>> .\n>>\n>\n", "Googlers: This is internal bug b/143090143.\n\nOn Thu, Nov 14, 2019 at 11:25 AM Rasmus Munk Larsen <rmlarsen@google.com>\nwrote:\n\n> This particular optimization is problematic, since it gives values that\n> are very far off for gradients of values below -14. I have a TODO to remove\n> it and just rely on the vectorized version of division, addition and exp.\n>\n> On Thu, Nov 14, 2019 at 11:16 AM Eugene Brevdo <ebrevdo@google.com> wrote:\n>\n>> +Alexandre Passos <apassos@google.com> +Rasmus Larsen\n>> <rmlarsen@google.com> this is a performance optimization I think.\n>> Perhaps we should add a precision attr or graph config like xla does?\n>>\n>> On Thu, Nov 14, 2019, 4:32 AM Gael Guennebaud <notifications@github.com>\n>> wrote:\n>>\n>>> See: https://eigen.tuxfamily.org/bz/show_bug.cgi?id=1777\n>>>\n>>> \u2014\n>>> You are receiving this because you were mentioned.\n>>> Reply to this email directly, view it on GitHub\n>>> <https://github.com/tensorflow/tensorflow/issues/33878?email_source=notifications&email_token=AANWFG2WJCOAVTJA34737U3QTVOQ3A5CNFSM4JHJMTK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEECAWGY#issuecomment-553913115>,\n>>> or unsubscribe\n>>> <https://github.com/notifications/unsubscribe-auth/AANWFGYC2NCJELAOBB32VOTQTVOQ3ANCNFSM4JHJMTKQ>\n>>> .\n>>>\n>>\n", "Apologies for the noob question, not familiar with Eigen: are the results different for some inputs because they are processed by different implementations of sigmoid, a fast one for the biggest chunk that is a multiple of the acceptable sizes, and a slower one for the remainder ?\r\n", "@georgesterpu Is this still an issue. I ran your code with `tf-nightly` and found results were asserted for the first three conditions and only failed for the last condition. However, this could be related to numerical error as the numerical mismatch is in 5th decimal.\r\n```\r\nprint(sigm[0]) #[0.549834 0.549834]\r\nprint(sigm[2]) #[0.549834 0.549834]\r\nprint(sigm[4]) #[0.54983395 0.54983395]\r\n```\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/2f27b3eb5cb8921d82d0494327e5b97a/untitled53.ipynb). Thanks!", "@jvishnuvardhan thank you very much for the follow up.\r\nAs far as I understand, this may be a side effect of an optimisation.\r\nOne of the eigen developers wrote that `Having (even slightly) different results for the same input is not good.`\r\n\r\nLooking at the commit history in ` Eigen/src/Core//functors/UnaryFunctors.h`, I think that @rmlarsen is the most qualified to clarify this.\r\n\r\n\r\n\r\n", "@rmlarsen Can you please take a look at this issue. Thanks!", "@georgesterpu I think this was resolved in recent `tf-nightly`. I couldn't reproduce the issue with `tf-nightly` and [here](https://colab.research.google.com/gist/jvishnuvardhan/d6c4e9fa4849020ae89101d1f45cce8b/untitled53.ipynb) is the gist for your reference. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Thank you @jvishnuvardhan.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33878\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33878\">No</a>\n"]}, {"number": 33877, "title": "Refactor network tester code and Readme", "body": "", "comments": ["@ninja18  Can you please resolve conflicts? Thanks!", "@ninja18 Still, conflicts appearing. Could you please resolve those? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@ninja18 gentle ping to resolve conflicts. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 33876, "title": "How to add an optimizer to tesorflow simple speech recognition", "body": "I wanna know how to add an optimizer such as Adam, or Nadam optimizer to the simple speech recognition example in this repository? Because I don't know where to or how to add it in the code such as to \"model.py\" file or where and how?", "comments": ["@chathuravithakshana Please post this question in [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) as github is only meant for bug/performance, build/install, feature requests or doc changes. Thanks!"]}, {"number": 33875, "title": "slim.conv2d_transpose()", "body": "**System information**\r\n- OS Platform and Distribution: arm aarch64 cortex A A54:\r\n- TensorFlow installed from (source or binary):  1.12.0 cross-compiled from source\r\n- TensorFlow version (or github SHA if from source): tensorflow-lite/benchmark_model\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nusing command 'TOCO' converted model sucessfully, but occur segment fault when run benchmark_model --graph=*.tflite(* is my model name)\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["sorry, type error, my plantform is cortex A53", "@SophieChang66 ,\r\nCan you please provide a standalone code to reproduce the issue ?Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@SophieChang66 ,\r\nAny update on the issue?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 33873, "title": "Add tests in TFLite micro for Logistic Int8", "body": "", "comments": ["gentle ping @petewarden for review ?", "@giorgio-arenarm can you please resolve conflicts ?", "Done, sorry about the delay", "@petewarden @advaitjain, gentle ping for review", "Rebased. @petewarden @advaitjain gentle ping for review", "@giorgio-arenarm Can you please check reviewer comments and keep us posted. Thanks!", "There is an internal discussion on whether the lookup table implementation is needed for ensuring bit-accurateness. Will update the issue once this is resolved. Sorry for the delay!", "Removed LuT implementation and rebased", "Removed LuT implementation and rebased", "@petewarden gentle ping for review", "@petewarden gentle ping for review", "@giorgio-arenarm can you please check this internal errors \r\n`third_party/tensorflow/lite/micro/kernels/logistic.cc:51:47: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n    TF_LITE_ENSURE_EQ(context, output->params.scale, 1. / 256);\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\r\n./third_party/tensorflow/lite/c/common.h:192:10: note: expanded from macro 'TF_LITE_ENSURE_EQ'\r\n    if ((a) != (b)) {                                                      \\\r\n         ^  ~~\r\nthird_party/tensorflow/lite/micro/kernels/logistic.cc:51:47: error: implicit conversion increases floating-point precision: 'float' to 'double' \r\n    TF_LITE_ENSURE_EQ(context, output->params.scale, 1. / 256);\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\r\n./third_party/tensorflow/lite/c/common.h:194:45: note: expanded from macro 'TF_LITE_ENSURE_EQ'\r\n                         __LINE__, #a, #b, (a), (b));                      \\\r\n                         ~~~~~~~~~~~~~~~~~~~^~~~~~~~\r\n./third_party/tensorflow/lite/c/common.h:152:39: note: expanded from macro 'TF_LITE_KERNEL_LOG'\r\n    (context)->ReportError((context), __VA_ARGS__); \\\r\n    ~                                 ^~~~~~~~~~~\r\nthird_party/tensorflow/lite/micro/kernels/logistic.cc:55:23: error: implicit conversion increases floating-point precision: 'const float' to 'double' \r\n        input->params.scale *\r\n        ~~~~~~~~~~~~~~^~~~~ ~\r\nthird_party/tensorflow/lite/micro/kernels/logistic.cc:133:62: error: missing field 'profiling_string' initializer \r\n      activations::LogisticPrepare, activations::LogisticEval};`", "Done rebasing and resolving conflicts"]}, {"number": 33872, "title": "Failed to load native Tensorflow runtime. Rasbian buster.", "body": "**System information**\r\n- Rasbian buster\r\n- TensorFlow installed whl file.\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7\r\n- Installed using Pip\r\n\r\n**I was having trouble installing it as per issue #33838 . I was able to install it by renaming the file but I'm not getting a new error when trying to import the package. No, installing a newer version of tensorflow is not an option. Neither for python. I going to stick to these versions.**\r\n\r\n```>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _PyThreadState_Current\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _PyThreadState_Current\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["Did you go through this thread on stackoverflow? https://stackoverflow.com/questions/56002315/undefined-symbol-pythreadstate-current-when-importing-tensorflow", "That is because you installed TF built from one toolchain/architecture on a different toolchain/architecture.\r\n\r\nYes, you can install it (that is, unzip the files in the wheel). But you can't use it without crashes and errors.", "closing this issue because it was my error that I installed an improper package. reopening #33838 until I am able to compile a proper package, as I have been having troubles with that too."]}, {"number": 33871, "title": "how to run test case in ring_reducer_test.cc", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS Linux release 7.6.1810 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.14.0\r\n- Python version:3.6.8\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source): 0.25.0\r\n- GCC/Compiler version (if compiling from source):7.3.1\r\n- CUDA/cuDNN version:cuda:10.0, cudnn:7.5.0.56\r\n- GPU model and memory:Telas v100, 32GB\r\n\r\nI want to run test case in tensorflow/core/common_runtime/ring_reducer_test.cc, so I try this:\r\n```\r\nbazel test -c opt --config=cuda //tensorflow/core:ring_reducer_test\r\n```\r\nbut bazel give me a error messages:\r\n```\r\ndebian ~/collective/tensorflow $ bazel test -c opt --config=cuda //tensorflow/core:ring_reducer_test\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: Skipping '//tensorflow/core:ring_reducer_test': no such target '//tensorflow/core:ring_reducer_test': target 'ring_reducer_test' not declared in package 'tensorflow/core' defined by /home/zxy/collective/tensorflow/tensorflow/core/BUILD\r\nERROR: no such target '//tensorflow/core:ring_reducer_test': target 'ring_reducer_test' not declared in package 'tensorflow/core' defined by /home/zxy/collective/tensorflow/tensorflow/core/BUILD\r\nINFO: Elapsed time: 0.212s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\nbazel thinks that `ring_reducer_test` is not exist in `/home/zxy/collective/tensorflow/tensorflow/core/BUILD`, but I do find it. So can you help me to run test case in `ring_reducer_test.cc`?", "comments": ["I think you did not run `./configure`? Please look at issue #4279", "the same error even run `./configure`, below is content of `.tf_configure.bazelrc`:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/zxy/collective/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/opt/rh/llvm-toolset-7.0/root/usr/lib/python2.7/site-packages\"\r\nbuild --python_path=\"/home/zxy/collective/bin/python\"\r\nbuild --action_env PYTHONPATH=\"/opt/rh/llvm-toolset-7.0/root/usr/lib/python2.7/site-packages:/opt/rh/devtoolset-7/root/usr/lib64/python2.7/site-packages:/opt/rh/devtoolset-7/root/usr/lib/python2.7/site-packages\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env TF_NEED_TENSORRT=\"0\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.0,7.0,7.0,7.0\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/opt/rh/llvm-toolset-7.0/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/opt/rh/devtoolset-7/root/usr/bin/gcc\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\n```\r\nIs there some wrong in `.tf_configure.bazelrc`?", "Tensorflow is built with bazel version 0.24.1. Can you please try it with bazel 0.24.1 and let me know if you are able to build successfully. For more info, go through this [link](https://www.tensorflow.org/install/source).", "The same error even with bazel 0.24.1:\r\n```\r\n(collective) gpu7 ~/collective/tensorflow $ bazel test -c opt --config=cuda //tensorflow/core:ring_reducer_test\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nDEBUG: /home/zxy/.cache/bazel/_bazel_zxy/398544b8d2045ac5982abb4c34aae05c/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nERROR: Skipping '//tensorflow/core:ring_reducer_test': no such target '//tensorflow/core:ring_reducer_test': target 'ring_reducer_test' not declared in package 'tensorflow/core' defined by /home/zxy/collective/tensorflow/tensorflow/core/BUILD\r\nERROR: no such target '//tensorflow/core:ring_reducer_test': target 'ring_reducer_test' not declared in package 'tensorflow/core' defined by /home/zxy/collective/tensorflow/tensorflow/core/BUILD\r\nINFO: Elapsed time: 8.180s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n```\r\nversion of bazel is:\r\n```\r\n(collective) gpu7 ~/collective/tensorflow $ bazel version\r\nBuild label: 0.24.1\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Apr 2 16:29:26 2019 (1554222566)\r\nBuild timestamp: 1554222566\r\nBuild timestamp as int: 1554222566\r\n```", "@Keepmoving-ZXY This is the configuration of Tensorflow-gpu 1.14\r\n\r\n\r\nVersion - tensorflow_gpu-1.14.0 \r\nPython version - 2.7, 3.3-3.7\r\nCompiler - GCC 4.8 \r\nBuild tools - Bazel 0.24.1\r\ncuDNN - 7.4 \r\nCUDA - 10.0\r\n\r\nIf you see above some of the configurations i.e., compiler and cuDNN versions are not matching.\r\nFor more information, you can refer to the following [guide](https://www.tensorflow.org/install/source#gpu)\r\n\r\nThanks!\r\n\r\n", "@Keepmoving-ZXY Were you able to solve this issue?", "No, I think there is no related to configuration of TensorFlow, I think the target `ring_reducer_test` is defined as private, not public, so I can't build the `ring_reducer_test` independently. I can see `test_main.cc` in `tensorflow/core/platform` but there is no instructions about how and when to run it. Could you tell me how to run tensorflow's c++ testcase?", "Can you try `bazel test -c opt --config=cuda //tensorflow/core:common_runtime_ring_reducer_test`?", "Yes, this can build `ring_reducer_test.cc` and it's dependency, and export binary file in `bazel-bin/tensorflow/core/common_runtime_ring_reducer_test`, thank you very much. And could you tell me why I must add `common_runtime` to `ring_reducer_test`? ", "`ring_reducer_test` is defined in `core/BUILD` using [`tf_cc_tests_gpu`](https://github.com/tensorflow/tensorflow/blob/1e65730120aafc413e8c3dcddcf19cd8d184fe1b/tensorflow/core/BUILD#L3718).  If you follow the definition of `tf_cc_tests_gpu` in `tensorflow.bzl`, you will eventually arrive at [`src_to_test_name`](https://github.com/tensorflow/tensorflow/blob/1e65730120aafc413e8c3dcddcf19cd8d184fe1b/tensorflow/tensorflow.bzl#L82), which contains the logic for which you're looking.", "ok, thank you.", "thanks! figure it out finally..."]}, {"number": 33870, "title": "[tf-nightly] unable to load saved functional model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macos mohave**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tf-nightly\r\n- TensorFlow version (use command below): **tf-nightly-2.1.0.dev20191029**\r\n- Python version: **3.7**\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nLoading functional model should pass in latest version of tf2. Everything works great in tensorflow==2.0.0.\r\n\r\n**Describe the expected behavior**\r\n\r\nLoading saved functional model raises exception.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nshape = (224, 224, 3)\r\n\r\n# sequential model\r\nmodel1 = tf.keras.Sequential(\r\n            [\r\n                tf.keras.Input(shape=shape, name=\"input\"),\r\n                tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=shape),\r\n                tf.keras.layers.GlobalAveragePooling2D(),\r\n                tf.keras.layers.Dense(256, activation=\"relu\", name=\"descriptor\"),\r\n                tf.keras.layers.Dense(2, activation=\"softmax\", name=\"probs\"),\r\n            ]\r\n        )\r\n\r\n# functional model\r\nbase_model2 = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=shape)\r\ninputs = tf.keras.Input(shape=shape, name=\"input\")\r\nx = base_model2(inputs)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = tf.keras.layers.Dense(256, activation=\"relu\", name=\"descriptor\")(x)\r\noutputs = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"probs\")(x)\r\nmodel2 = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\ntf.saved_model.save(model1, \"test1\")\r\ntf.saved_model.save(model2, \"test2\")\r\n#model2.save(\"test2\", include_optimizer=False, save_format=\"tf\")\r\n\r\nmodel_1 = tf.keras.models.load_model('test1')\r\n\r\n# THIS RAISES exception \r\nmodel_2 = tf.keras.models.load_model('test2')\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\n```\r\n2019-10-31 09:13:04.371282: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-31 09:13:04.384631: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcafda7be50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-10-31 09:13:04.384655: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-10-31 09:13:17.924297: W tensorflow/python/util/util.cc:309] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1785: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nTraceback (most recent call last):\r\n  File \"save.py\", line 32, in <module>\r\n    model_2 = tf.keras.models.load_model('test2')\r\n  File \"/Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 89, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"/Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 543, in load_internal\r\n    export_dir)\r\n  File \"/Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 119, in __init__\r\n    self._finalize()\r\n  File \"/Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 157, in _finalize\r\n    created_layers={layer.name: layer for layer in node.layers})\r\n  File \"/Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1885, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1833, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 773, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/Users/michallukac/env/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 712, in call\r\n    raise NotImplementedError('When subclassing the `Model` class, you should'\r\nNotImplementedError: When subclassing the `Model` class, you should implement a `call` method.\r\n```", "comments": ["```tf.keras.models.load_model``` loads a model saved via ```save_model. ```\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model\r\nIn your code snippet you may try ```tf.saved_model.load``` since you are saving your model with ```tf.saved_model.save```\r\nFollowing snippet executes successfully;\r\n```python\r\nimport tensorflow as tf\r\n\r\nshape = (224, 224, 3)\r\n\r\n# sequential model\r\nmodel1 = tf.keras.Sequential(\r\n            [\r\n                tf.keras.Input(shape=shape, name=\"input\"),\r\n                tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=shape),\r\n                tf.keras.layers.GlobalAveragePooling2D(),\r\n                tf.keras.layers.Dense(256, activation=\"relu\", name=\"descriptor\"),\r\n                tf.keras.layers.Dense(2, activation=\"softmax\", name=\"probs\"),\r\n            ]\r\n        )\r\n\r\n# functional model\r\nbase_model2 = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=shape)\r\ninputs = tf.keras.Input(shape=shape, name=\"input\")\r\nx = base_model2(inputs)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = tf.keras.layers.Dense(256, activation=\"relu\", name=\"descriptor\")(x)\r\noutputs = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"probs\")(x)\r\nmodel2 = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\ntf.saved_model.save(model1, \"test1\")\r\ntf.saved_model.save(model2, \"test2\")\r\n#model2.save(\"test2\", include_optimizer=False, save_format=\"tf\")\r\n\r\n#model_1 = tf.keras.models.load_model('test1')\r\nmodel_1 = tf.saved_model.load('test1')\r\n\r\n# THIS RAISES exception \r\n#model_2 = tf.keras.models.load_model('test2')\r\nmodel_2 = tf.saved_model.load('test2')\r\n```", "Thank you @ymodak , yes your example is working.\r\n\r\nHowever tf.keras.models.load_model and tf.keras.models.save_model should support (according to documentation) both formats (SavedModel/TF and H5 file/Keras).\r\n\r\nThis is passing:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nshape = (224, 224, 3)\r\n\r\n# sequential model\r\nmodel1 = tf.keras.Sequential(\r\n    [\r\n        tf.keras.Input(shape=shape, name=\"input\"),\r\n        tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=shape),\r\n        tf.keras.layers.GlobalAveragePooling2D(),\r\n        tf.keras.layers.Dense(256, activation=\"relu\", name=\"descriptor\"),\r\n        tf.keras.layers.Dense(2, activation=\"softmax\", name=\"probs\"),\r\n    ]\r\n)\r\n\r\n# functional model\r\nbase_model2 = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=shape)\r\ninputs = tf.keras.Input(shape=shape, name=\"input\")\r\nx = base_model2(inputs)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = tf.keras.layers.Dense(256, activation=\"relu\", name=\"descriptor\")(x)\r\noutputs = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"probs\")(x)\r\nmodel2 = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\ntf.keras.models.save_model(model1, \"test1.h5\", include_optimizer=False)\r\ntf.keras.models.save_model(model2, \"test2.h5\", include_optimizer=False)\r\n\r\nmodel_1 = tf.keras.models.load_model(\"test1.h5\")\r\n\r\n# THIS is passing\r\nmodel_2 = tf.keras.models.load_model(\"test2.h5\")\r\n```\r\n\r\nHowever this fails as it is working with saved model format:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nshape = (224, 224, 3)\r\n\r\n# sequential model\r\nmodel1 = tf.keras.Sequential(\r\n    [\r\n        tf.keras.Input(shape=shape, name=\"input\"),\r\n        tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=shape),\r\n        tf.keras.layers.GlobalAveragePooling2D(),\r\n        tf.keras.layers.Dense(256, activation=\"relu\", name=\"descriptor\"),\r\n        tf.keras.layers.Dense(2, activation=\"softmax\", name=\"probs\"),\r\n    ]\r\n)\r\n\r\n# functional model\r\nbase_model2 = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=shape)\r\ninputs = tf.keras.Input(shape=shape, name=\"input\")\r\nx = base_model2(inputs)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nx = tf.keras.layers.Dense(256, activation=\"relu\", name=\"descriptor\")(x)\r\noutputs = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"probs\")(x)\r\nmodel2 = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\r\ntf.keras.models.save_model(model1, \"test1\", include_optimizer=False)\r\ntf.keras.models.save_model(model2, \"test2\", include_optimizer=False)\r\n\r\nmodel_1 = tf.keras.models.load_model(\"test1\")\r\n\r\n# FAILS\r\nmodel_2 = tf.keras.models.load_model(\"test2\")\r\n\r\n```\r\n", "I'm using a `ModelCheckpoint` callback to save intermediate steps as `saved_model.pb`. With tf 2.0 it works, but with the nightly 2.1.0-dev20191104 I also get the same loading error.\r\n\r\nIf I replace `keras.load_model()` with `tf.saved_model.load()` I get \r\n```\r\nAttributeError: '_UserObject' object has no attribute 'summary'\r\n```", "This is also related to tensorflow==2.1.0rc.", "Thanks for reporting this Cospel. Confirming that this is a bug, I am in the middle of redoing SavedModel loading behavior which will address this issue.", "Thank you @k-w-w  for awesome work!\r\n\r\nThis is passing in the tf-nightly right now.\r\n\r\n-----", "This is strange it worked for several days in tf-nightly however in **tf2.1.0** and tf-nightly from today, it is not working again.\r\n", "This is fixed with latest tf nightly version '2.2.0-dev20200121'. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33870\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33870\">No</a>\n", "I save the model using the below on Google Colab\r\n`\r\n  cp_callback = tf.keras.callbacks.ModelCheckpoint(  filepath=checkpoint_path,    verbose=1,    save_weights_only=False,  period=5)\r\n`\r\n`\r\n training_model.save(checkpoint_path.format(epoch=0))\r\n`\r\nThen i load the model and put it inside a class and get the summary and it works\r\n`\r\nmodel = tf.keras.models.load_model('15867364372951858/cp-0005.ckpt')\r\n`\r\n`\r\nclass dic_model: \r\n`\r\n`\r\n  def __init__(self, model, dictionary):\r\n `\r\n`\r\n   self.model = model\r\n `\r\n`\r\n   self.dic = dictionary\r\n`\r\n`\r\nmymodel = dic_model(model, features_list)\r\n`\r\n`\r\nmymodel.model.summary()\r\n`\r\nbut loading the model using the same tf.keras.models.load_model and same model gets the below error \r\n\r\n> \"TypeError: The added layer must be an instance of class Layer. Found: <tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x0000026E77B2C788>\"", "@QuantumNinja92  When loading checkpoints, you should call `model.load_weights` instead of `tf.keras.models.load_model`.", "> @QuantumNinja92 When loading checkpoints, you should call `model.load_weights` instead of `tf.keras.models.load_model`.\r\n\r\nIt only worked after using the Nightly build"]}]