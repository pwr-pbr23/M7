[{"number": 34796, "title": "bazel build tensorflow lite with with_select_tf_ops Error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:r1.14\r\n- Bazel version (if compiling from source):0.24.1\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\ni set a crosstool for bazel to build tensorflow lite, it work well when build `libtensorflowlite.so`.\r\nbut when i add the tensorflow ops delegate library dependency (`tensorflow/lite/delegates/flex:delegate`) to the build dependencies, i got an error:\r\n\r\n```\r\nERROR:/home/wang/.cache/bazel/_bazel_wang/4b298145d4306eeb9a64ca7837a4593b/external/nasm/BUILD.bazel:8:1: C++ compilation of rule '@nasm//:nasm' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/wang/.cache/bazel/_bazel_wang/4b298145d4306eeb9a64ca7837a4593b/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/wang/.local/bin:/home/wang/bin:/home/wang/android-ndk-r17c:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/wang/.local/bin:/home/wang/bin:/usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=2' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -isystem /usr/include -g0 -O3 -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/host/bin/external/nasm/_objs/nasm/assemble.d '-frandom-seed=bazel-out/host/bin/external/nasm/_objs/nasm/assemble.o' -DHAVE_SNPRINTF -DHAVE_SYS_TYPES_H -iquote external/nasm -iquote bazel-out/host/genfiles/external/nasm -iquote bazel-out/host/bin/external/nasm -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -isystem external/nasm/asm -isystem bazel-out/host/genfiles/external/nasm/asm -isystem bazel-out/host/bin/external/nasm/asm -isystem external/nasm/include -isystem bazel-out/host/genfiles/external/nasm/include -isystem bazel-out/host/bin/external/nasm/include -isystem external/nasm/output -isystem bazel-out/host/genfiles/external/nasm/output -isystem bazel-out/host/bin/external/nasm/output -isystem external/nasm/x86 -isystem bazel-out/host/genfiles/external/nasm/x86 -isystem bazel-out/host/bin/external/nasm/x86 -g0 '-march=native' -w '-std=c99' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/nasm/asm/assemble.c -o bazel-out/host/bin/external/nasm/_objs/nasm/assemble.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/nasm/asm/assemble.c: In function 'assert_no_prefix':\r\nexternal/nasm/asm/assemble.c:265:20: error: 'ERR_NONFATAL' undeclared (first use in this function)\r\n         nasm_error(ERR_NONFATAL, \"invalid %s prefix\",\r\n                    ^\r\nexternal/nasm/asm/assemble.c:265:20: note: each undeclared identifier is reported only once for each function it appears in\r\nexternal/nasm/asm/assemble.c: In function 'warn_overflow':\r\nexternal/nasm/asm/assemble.c:295:16: error: 'ERR_WARNING' undeclared (first use in this function)\r\n     nasm_error(ERR_WARNING | ERR_PASS2 | ERR_WARN_NOV,\r\n                ^\r\nexternal/nasm/asm/assemble.c:295:30: error: 'ERR_PASS2' undeclared (first use in this function)\r\n     nasm_error(ERR_WARNING | ERR_PASS2 | ERR_WARN_NOV,\r\n                              ^\r\nexternal/nasm/asm/assemble.c:295:42: error: 'ERR_WARN_NOV' undeclared (first use in this function)\r\n     nasm_error(ERR_WARNING | ERR_PASS2 | ERR_WARN_NOV,\r\n                                          ^\r\nexternal/nasm/asm/assemble.c: In function 'out':\r\nexternal/nasm/asm/assemble.c:402:24: error: 'ERR_NONFATAL' undeclared (first use in this function)\r\n             nasm_error(ERR_NONFATAL,\r\n                        ^\r\nexternal/nasm/asm/assemble.c:406:24: error: 'ERR_WARNING' undeclared (first use in this function)\r\n             nasm_error(ERR_WARNING | ERR_WARN_ZEXTRELOC,\r\n                        ^\r\nexternal/nasm/asm/assemble.c:406:38: error: 'ERR_WARN_ZEXTRELOC' undeclared (first use in this function)\r\n             nasm_error(ERR_WARNING | ERR_WARN_ZEXTRELOC,\r\n                                      ^\r\nexternal/nasm/asm/assemble.c: In function 'out_reladdr':\r\nexternal/nasm/asm/assemble.c:473:20: error: 'ERR_NONFATAL' undeclared (first use in this function)\r\n         nasm_error(ERR_NONFATAL, \"invalid use of self-relative expression\");\r\n                    ^\r\nexternal/nasm/asm/assemble.c: In function 'jmp_match':\r\nexternal/nasm/asm/assemble.c:527:20: error: 'ERR_WARNING' undeclared (first use in this function)\r\n         nasm_error(ERR_WARNING | ERR_WARN_BND | ERR_PASS2 ,\r\n                    ^\r\nexternal/nasm/asm/assemble.c:527:34: error: 'ERR_WARN_BND' undeclared (first use in this function)\r\n         nasm_error(ERR_WARNING | ERR_WARN_BND | ERR_PASS2 ,\r\n```\r\nwhat should i do?\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi,\r\n\r\nNot sure how your build is configured, could you post your crosstool config?", "the directory structure like this:\r\n```\r\n\u2514\u2500\u2500tensorflow\r\n       \u251c\u2500\u2500 aarch64_compiler\r\n       \u2502   \u251c\u2500\u2500 BUILD\r\n       \u2502   \u2514\u2500\u2500CROSSTOOL\r\n       \u2502   \u2514\u2500\u2500  cross_toolchain_target_aarch64.BUILD\r\n       \u2514\u2500\u2500 build_tflite_aarch64.sh\r\n       \u2514\u2500\u2500 WORKSPACE\r\n```\r\nThe BUILD,CROSSTOOL,WORKSPACE , cross_toolchain_target_aarch64.BUILD,  build_tflite_aarch64.sh, files i am using are:\r\n[BUILD.txt](https://github.com/tensorflow/tensorflow/files/3925380/BUILD.txt)\r\n[build_tflite_aarch64.sh.txt](https://github.com/tensorflow/tensorflow/files/3925381/build_tflite_aarch64.sh.txt)\r\n[cross_toolchain_target_aarch64.BUILD.txt](https://github.com/tensorflow/tensorflow/files/3925382/cross_toolchain_target_aarch64.BUILD.txt)\r\n[CROSSTOOL.txt](https://github.com/tensorflow/tensorflow/files/3925383/CROSSTOOL.txt)\r\n[WORKSPACE.txt](https://github.com/tensorflow/tensorflow/files/3925384/WORKSPACE.txt)\r\n\r\ni use this crosstool successfully compiled the libtensorflowlite.so, but it doesn't work for `framework` and the library `with_select_tf_ops`. when compile the `framework`, i got this error:\r\n```\r\nERROR: /home/wang/work/arm64/tensorflow-r1.14/tensorflow/lite/BUILD:149:1: Linking of rule '//tensorflow/lite:framework' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command \r\n  (cd /home/wang/.cache/bazel/_bazel_wang/4b298145d4306eeb9a64ca7837a4593b/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/wang/.local/bin:/home/wang/bin:/home/wang/android-ndk-r17c:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/wang/.local/bin:/home/wang/bin:/usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-gcc -shared -o bazel-out/aarch64-opt/bin/tensorflow/lite/libframework.so -L/usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/lib -L/usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/aarch64-linux-gnu/lib -L/usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/aarch64-linux-gnu/lib64 -L/usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/aarch64-linux-gnu/libc/lib -L/usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/aarch64-linux-gnu/libc/usr/lib '-Wl,--dynamic-linker=/usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/aarch64-linux-gnu/libc/lib/ld-linux-aarch64.so.1' -no-canonical-prefixes -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,@bazel-out/aarch64-opt/bin/tensorflow/lite/libframework.so-2.params)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n/usr/ARM/toolchain/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/bin/../aarch64-linux-gnu/libc/usr/lib/Scrt1.o: In function `_start':\r\n/home/tcwg-buildslave/workspace/tcwg-make-release_1/snapshots/glibc.git~release~2.25~master/csu/../sysdeps/aarch64/start.S:64: undefined reference to `main'\r\n/home/tcwg-buildslave/workspace/tcwg-make-release_1/snapshots/glibc.git~release~2.25~master/csu/../sysdeps/aarch64/start.S:65: undefined reference to `main'\r\ncollect2: error: ld returned 1 exit status\r\n\r\n```\r\nim really hope you can help me in your free time,Thanks!\r\n", "I see you are trying to build for linux aarch64, @terryheo, could you take a look?", "> I see you are trying to build for linux aarch64, @terryheo, could you take a look?\r\n\r\nyes, i set this toolchain build tensorflow lite for linux aarch64 at linux x86_64", "Hi @Yooong-W \r\nI'm see the following error with your configuration.\r\n\r\nERROR: /usr/local/google/home/terryheo/work/tensorflow/aarch64_compiler/BUILD:25:1: //aarch64_compiler:cc-compiler-local: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule\r\nERROR: /usr/local/google/home/terryheo/work/tensorflow/aarch64_compiler/BUILD:38:1: //aarch64_compiler:cc-compiler-aarch64: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule\r\n\r\nI'm using Bazel version 1.2.1. Could you provide a proper toolchain_config ?", "Hi, @terryheo \r\nsorry, im new to bazel. i dont know what's you mean of `toolchain_config`, i add these four files only,`BUILD`\u3001`cross_toolchain_target_aarch64.BUILD` \u3001`CROSSTOOL `\u3001`WORKSPACE`.  maybe you can give me some suggestions?", "> \u55e8\uff0c@terryheo\r\n> \u5bf9\u4e0d\u8d77\uff0c\u6211\u662fbazel\u7684\u65b0\u624b\u3002\u6211\u4e0d\u77e5\u9053\u4ec0\u4e48\u662f\u4f60\u7684\u610f\u601d\u7684`toolchain_config`\uff0c\u6211\u53ea\u6dfb\u52a0\u8fd9\u56db\u4e2a\u6587\u4ef6`BUILD`\uff0c`cross_toolchain_target_aarch64.BUILD`\uff0c`CROSSTOOL `\uff0c \uff0c`WORKSPACE`\u3002\u4e5f\u8bb8\u60a8\u53ef\u4ee5\u7ed9\u6211\u4e00\u4e9b\u5efa\u8bae\uff1f\r\n\r\nmaybe i know what's you mean about the `toolchain_config`, i use the [guide ](https://docs.bazel.build/versions/0.23.0/tutorial/crosstool.html) is for bazel 0.23.0 or the before. the bazel version later 0.23.0 should use this [guide](https://docs.bazel.build/versions/1.2.0/tutorial/cc-toolchain-config.html). \r\nand in my `BUILD` file , it does not contain the attribute `toolchain_config` in `cc_toolchain` rule.", "For now, the official way of building TFLite library for aarch64 is https://www.tensorflow.org/lite/guide/build_arm64 which doesn't support flex delegate.\r\n ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34796\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34796\">No</a>\n", "Now we have a toolchain for aarch64. You can use it as the following.\r\n\r\n```\r\nbazel build -c opt --config=elinux_aarch64 //tensorflow/lite/delegates/flex:delegate\r\n```"]}, {"number": 34795, "title": "AttributeError: 'ImageDataGenerator' object has no attribute 'shape'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): KDE neon, based on Ubuntu 18.04 LTS\r\n- TensorFlow installed from (source or binary): via Anaconda 3\r\n- TensorFlow version (use command below): 2.0.0 \r\n- Python version: 3.7.5\r\n\r\n**Anaconda environment**\r\nName                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main  \r\n_tflow_select             2.3.0                       mkl  \r\nabsl-py                   0.8.1                    py37_0  \r\nastor                     0.8.0                    py37_0  \r\nblas                      1.0                         mkl  \r\nc-ares                    1.15.0            h7b6447c_1001  \r\nca-certificates           2019.10.16                    0  \r\ncertifi                   2019.9.11                py37_0  \r\ngast                      0.2.2                    py37_0  \r\ngoogle-pasta              0.1.8                      py_0  \r\ngrpcio                    1.16.1           py37hf8bcb03_1  \r\nh5py                      2.9.0            py37h7918eee_0  \r\nhdf5                      1.10.4               hb1b8bf9_0  \r\nintel-openmp              2019.4                      243  \r\nkeras-applications        1.0.8                      py_0  \r\nkeras-preprocessing       1.1.0                      py_1  \r\nlibedit                   3.1.20181209         hc058e9b_0  \r\nlibffi                    3.2.1                hd88cf55_4  \r\nlibgcc-ng                 9.1.0                hdf63c60_0  \r\nlibgfortran-ng            7.3.0                hdf63c60_0  \r\nlibprotobuf               3.10.1               hd408876_0  \r\nlibstdcxx-ng              9.1.0                hdf63c60_0  \r\nmarkdown                  3.1.1                    py37_0  \r\nmkl                       2019.4                      243  \r\nmkl-service               2.3.0            py37he904b0f_0  \r\nmkl_fft                   1.0.15           py37ha843d7b_0  \r\nmkl_random                1.1.0            py37hd6b4f25_0  \r\nncurses                   6.1                  he6710b0_1  \r\nnumpy                     1.17.3           py37hd14ec0e_0  \r\nnumpy-base                1.17.3           py37hde5b4d6_0  \r\nopenssl                   1.1.1d               h7b6447c_3  \r\nopt_einsum                3.1.0                      py_0  \r\npip                       19.3.1                   py37_0  \r\nprotobuf                  3.10.1           py37he6710b0_0  \r\npython                    3.7.5                h0371630_0  \r\nreadline                  7.0                  h7b6447c_5  \r\nscipy                     1.3.1            py37h7c811a0_0  \r\nsetuptools                41.6.0                   py37_0  \r\nsix                       1.13.0                   py37_0  \r\nsqlite                    3.30.1               h7b6447c_0  \r\ntensorboard               2.0.0              pyhb230dea_0  \r\ntensorflow                2.0.0           mkl_py37h66b46cc_0  \r\ntensorflow-base           2.0.0           mkl_py37h9204916_0  \r\ntensorflow-estimator      2.0.0              pyh2649769_0  \r\ntensorflow-mkl            2.0.0                h4fcabd2_0  \r\ntermcolor                 1.1.0                    py37_1  \r\ntk                        8.6.8                hbc83047_0  \r\nwebencodings              0.5.1                    py37_1  \r\nwerkzeug                  0.16.0                     py_0  \r\nwheel                     0.33.6                   py37_0  \r\nwrapt                     1.11.2           py37h7b6447c_0  \r\nxz                        5.2.4                h14c3975_4  \r\nzlib                      1.2.11               h7b6447c_3\r\n\r\n**Describe the current behavior**\r\nUsing only Keras methods I get the error:\r\n> AttributeError: 'ImageDataGenerator' object has no attribute 'shape'\r\n\r\nThis seems to indicate that Keras code is expecting other data scructures in an object that it created itself.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error message.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import MaxPooling2D, Conv2D, Dense\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\nmodel = Sequential([\r\n    Conv2D(filters=32,\r\n           input_shape=(10,10,3),\r\n           kernel_size=(4,4),\r\n           activation='relu'),\r\n\r\n    MaxPooling2D(pool_size=(2, 2)),\r\n\r\n\r\n    Dense(5),\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nimageGenerator = ImageDataGenerator()\r\n\r\nimageGenerator.flow_from_directory(\r\n    directory='training data',\r\n    target_size=(10, 10),\r\n    batch_size=32,\r\n    class_mode='categorical',\r\n)\r\n\r\nmodel.fit_generator(imageGenerator, steps_per_epoch=55)\r\n```\r\n\r\nThe folder *training data* contains three folders each containing JPG images for the respective class.\r\n\r\n**Output**\r\n```\r\n/home/user/anaconda3/envs/project/bin/python3.7 /home/user/.PyCharm2019.3/config/scratches/scratch_2.py\r\n2019-12-03 14:11:23.807238: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2019-12-03 14:11:23.834757: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593900000 Hz\r\n2019-12-03 14:11:23.835094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558c62e752d0 executing computations on platform Host. Devices:\r\n2019-12-03 14:11:23.835114: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-12-03 14:11:23.835399: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nFound 6126 images belonging to 3 classes.\r\nTraceback (most recent call last):\r\n  File \"/home/user/.PyCharm2019.3/config/scratches/scratch_2.py\", line 31, in <module>\r\n    model.fit_generator(imageGenerator, steps_per_epoch=55)\r\n  File \"/home/user/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1297, in fit_generator\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/user/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\", line 144, in model_iteration\r\n    shuffle=shuffle)\r\n  File \"/home/user/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\", line 477, in convert_to_generator_like\r\n    num_samples = int(nest.flatten(data)[0].shape[0])\r\nAttributeError: 'ImageDataGenerator' object has no attribute 'shape'\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["PEBKAC\r\n\r\n```\r\nimageGenerator.flow_from_directory.....\r\n```\r\nshould be\r\n```\r\ngenerator = imageGenerator.flow_from_directory.....\r\n```\r\nand `generator` must be the paremeter for `fit_generator()`"]}, {"number": 34794, "title": "@local_config_cc//:toolchain' does not contain a toolchain for cpu 'ios_x86_64'", "body": "I am trying to import mediapipe project using Bazel. But when try to sync the project by clicking on bazel->sync->sync bazel  I get following issues. I am using ubuntu LTS 18 version and bazel version is 0.29.1. I am new to mediapipe so please provide detail answer for the issue.\r\n   \r\n     Error:Error:line: 47in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite \r\n     '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'ios_x86_64'\r\n\r\n    Error:Error:While resolving toolchains for target @bazel_tools//tools/objc:dummy_lib: no \r\n    matching toolchains found for types @bazel_tools//tools/cpp:toolchain_type \r\n  ", "comments": ["Looks like the issue is not related to Tensorflow. Can you please elaborate about the issue & the context.Will it be possible to provide related code.Thanks!", "\r\nI am working on mediapipe Handtracking application and I am trying to import the project using bazel but I try to sync it \r\nit gives me that issue.I think I will have to add some configuration in some file but I really do not know what and where I have to add. \r\nAs you know that mediapipe uses tensorflow and other developers are also facing the similar issue and they mention that it is a tensorflow issue \r\nand becuse of it I added it to tensorflow. \r\n\r\n\r\n\r\n\r\n", "Please, let us know which version of Tensorflow you are using. Is it possible to provide related code?. It helps us in localizing the issue faster.Thanks!", "\nTensorflow version 1.13.2 and the example can be accessed from mediapipe/examples/android/handtrackinggpu \n", "@suraj-0387 Can you please post the issue in the `mediapipe` repo [here](https://github.com/google/mediapipe/issues). The resolution should be faster there. Thanks!", "close it", "It still doesn't solve the problem "]}, {"number": 34793, "title": "Sequence pad support for Ragged Tensor when call to_tensor", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRight now, we can get a dense tensor by calling `to_tensor` of a RaggedTensor and the num_cols of the dense tensor is the max length of the tensors in the RaggedTensor. However, the max length may vary if we get the RaggedTensor by `tf.string.splits` for different batch data. So, we can not use the dense tensor for `tf.keras.layers.Embedding` and `tf.keras.layers.Dense` layers because the num_cols is not fixed.\r\n\r\n**Will this change the current api? How?**\r\nIn order to support padding for `to_tensor`, we need add three arguments for `to_tensor` API, like:\r\n```python\r\ndef to_tensor(self, default_value=None, name=None, \r\n    maxlen=None, padding='pre', truncating='pre'):\r\n```\r\nmaxlen: Int, maximum length of all sequences where the sequence is a tensor in the RaggedTensor .\r\npadding: String, 'pre' or 'post':\r\n    pad either before or after each sequence.\r\ntruncating: String, 'pre' or 'post':\r\n    remove values from sequences larger than\r\n    `maxlen`, either at the beginning or at the end of the sequences.\r\n\r\nThose arguments are the same as `tf.keras.preprocessing.sequence.pad_sequences` in https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\r\n\r\n**Who will benefit with this feature?**\r\nIn NLP and time series modeling, the format of feature is the string with many values separated by a separator like \"19,39,94,30\" or \"there, is, a, dog\" and the number of values of different samples is different. In this case, we need to use `tf.strings.split()` to get a RaggedTensor and convert it to a dense tensor with fixed num_cols.\r\n\r\n**Any Other info.**\r\nHere is a demo with maxlen.\r\n```python\r\nimport numpy as np\r\n\r\nfrom tensorflow.python import tf2\r\nfrom tensorflow.python.client import session\r\nfrom tensorflow.python.framework import composite_tensor\r\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.framework import sparse_tensor\r\nfrom tensorflow.python.framework import tensor_shape\r\nfrom tensorflow.python.framework import tensor_spec\r\nfrom tensorflow.python.framework import tensor_util\r\nfrom tensorflow.python.framework import type_spec\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import check_ops\r\nfrom tensorflow.python.ops import control_flow_ops\r\nfrom tensorflow.python.ops import gen_ragged_conversion_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.ops.ragged import ragged_config\r\nfrom tensorflow.python.ops.ragged import ragged_tensor_value\r\nfrom tensorflow.python.ops.ragged import ragged_util\r\nfrom tensorflow.python.ops.ragged import segment_id_ops\r\nfrom tensorflow.python.util.tf_export import tf_export\r\n\r\ndef is_ragged(value):\r\n  \"\"\"Returns true if `value` is a ragged tensor or ragged tensor value.\"\"\"\r\n  return isinstance(value,\r\n                    (tf.RaggedTensor, ragged_tensor_value.RaggedTensorValue))\r\n\r\ndef to_tensor(ragged_tensor, default_value=None, name=None, maxlen=None):\r\n    self = ragged_tensor\r\n    print(\"self : \", self)\r\n    with ops.name_scope(name, \"RaggedToTensor\", [self, default_value]):\r\n      if default_value is not None:\r\n        default_value = ops.convert_to_tensor(\r\n            default_value, name=\"default_value\", dtype=self.dtype)\r\n\r\n      # If ragged_rank > 1, then recursively convert the ragged values into a\r\n      # `Tensor` before we proceed.\r\n      values = self.values\r\n      if is_ragged(values):\r\n        values = values.to_tensor(default_value)\r\n\r\n      # Tile the default value, if necessary.\r\n      if default_value is not None:\r\n        if values.shape.ndims is not None:\r\n          default_value.shape.with_rank_at_most(values.shape.ndims - 1)\r\n        if (values.shape.ndims is None or default_value.shape.ndims is None or\r\n            values.shape.ndims != default_value.shape.ndims + 1):\r\n          value_shape = array_ops.shape(values)[1:]\r\n          default_value = array_ops.broadcast_to(default_value, value_shape)\r\n        default_value.shape.assert_is_compatible_with(values.shape[1:])\r\n\r\n      # Get the expected dense shape ([nrows, ncols] + value_shape).\r\n      rt_row_lengths = [self.row_splits[1:] - self.row_splits[:-1]]\r\n      print('rt_row_lengths : ', rt_row_lengths)\r\n      nrows = array_ops.shape(self.row_splits,\r\n                              out_type=self._row_splits.dtype)[0] - 1\r\n      ncols = math_ops.maximum(math_ops.reduce_max(rt_row_lengths), 0) \r\n      if maxlen is not None:\r\n        ncols = tf.constant(maxlen, dtype=tf.int64)\r\n        \r\n      values_shape = array_ops.shape(values, out_type=self._row_splits.dtype)\r\n      value_shape = values_shape[1:]\r\n      nvals = values_shape[0]\r\n\r\n      # Build a default value if none was supplied.\r\n      if default_value is None:\r\n        default_value = array_ops.zeros(value_shape, dtype=values.dtype)\r\n      default_value.shape.assert_is_compatible_with(values.shape[1:])\r\n      default_value.set_shape(values.shape[1:])\r\n\r\n      # Get the row start indices, and expand to shape=[nrows, 1].\r\n      starts = array_ops.expand_dims(self.row_splits[:-1], 1)\r\n\r\n      # Get the row limit indices, and expand to shape=[nrows, 1].\r\n      limits = array_ops.expand_dims(self.row_splits[1:], 1)\r\n\r\n      # Get the column indices, and expand to shape=[1, ncols].\r\n      columns = array_ops.expand_dims(math_ops.range(0, ncols), 0)\r\n\r\n      # Build a list containing the values plus the default value.  We will use\r\n      # tf.gather to collect values from this list for the `Tensor` (using\r\n      # nvals as the index for the default value).\r\n      values_and_default = array_ops.concat(\r\n          [values, array_ops.stack([default_value])], axis=0)\r\n\r\n      # Construct a matrix \"indices\" pointing into values_and_default.  I.e.,\r\n      # output[r, c] = values_and_default[indices[r, c].\r\n      nondefault_index = starts + columns\r\n      has_value = nondefault_index < limits\r\n      default_index = array_ops.fill(array_ops.stack([nrows, ncols]), nvals)\r\n        \r\n      indices = array_ops.where(has_value, nondefault_index, default_index)\r\n      tensor = array_ops.gather(values_and_default, indices)\r\n\r\n      # Gather the results into a `Tensor`.\r\n      return tensor\r\n    \r\nrt = tf.ragged.constant([[9, 8, 7], [], [6, 5], [4]])\r\nprint(to_tensor(rt, maxlen=10))\r\n```\r\n", "comments": ["@gadagashwini @ymodak  Can you assign the issue to me? I have implemented the feature and test it. ", "RaggedTensor.to_tensor has a shape parameter (as of 0efc9f4) which you can use to pad/truncate to a specified shape. Partial shapes (ie shapes where some dimensions have unknown size) are supported. ", "Nice, Thanks!  Can you add a padding argument which we can use to pad either before or after each sequence? So we can pad the ragged tensor like `tf.keras.preprocessing.sequence.pad_sequences` https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences.\r\nFor example:\r\n```\r\n>>> rt = tf.ragged.constant([[9, 8, 7], [], [6, 5], [4]])\r\n>>> print(rt.to_tensor(shape=(4,3), padding=\"pre\"))\r\n[[9 8 7] [0 0 0] [6 5 0] [4 0 0]], shape=(4, 3), dtype=int32)\r\n\r\n>>> print(rt.to_tensor(shape=(4,3), padding=\"post\"))\r\n[[9 8 7] [0 0 0] [0 6 5] [0 0 4]], shape=(4, 3), dtype=int32)\r\n```", "This thread was useful but seems incomplete. Can someone please help me out here.\r\n\r\nI also have text data(sentences of varying length). Can I use to_tensor to pad each sentence to a max length of 200 words ?\r\n", "@anantvir If you have a 2D ragged tensor `sentences` with shape `[num_sentences, tokens_per_sentence]`, and you want to pad it exactly 200 tokens in each row, you can just use `sentences.to_tokens(shape=[None, 200])`.  E.g.:\r\n\r\n```python\r\n>>> rt = tf.ragged.constant([[1, 2], [], [3, 4, 5, 6]])\r\n>>> print(rt.to_tensor(shape=[None, 200]))\r\ntf.Tensor(\r\n[[1 2 0 0 0 0 ...]\r\n [0 0 0 0 0 0 ...]\r\n [3 4 5 6 0 0 ...]], shape=(3, 200), dtype=int32)\r\n```\r\n\r\nIf you want to pad it to have at most 200 tokens in each row (but less if all sentences are shorter than 200 tokens), then you could use `sentences[:, :200].to_tensor()`.  E.g.:\r\n\r\n```\r\n>>> print(rt[:, :200].to_tensor())\r\ntf.Tensor(\r\n[[1 2 0 0]\r\n [0 0 0 0]\r\n [3 4 5 6]], shape=(3, 4), dtype=int32)\r\n```\r\n\r\n@workingloong If you want to put padding before each row (rather than after), then you can't currently do that with `RaggedTensor.to_tensor`.  But you can write a fairly simple function to do it:\r\n\r\n```\r\n>>> def left_pad_2d_ragged(rt, width):\r\n...   rt = rt[-width:]  # Truncate rows to have at most `width` items\r\n...   pad_row_lengths = width - rt.row_lengths()\r\n...   pad_values = tf.zeros([(width * rt.nrows()) - tf.size(rt, tf.int64)], rt.dtype)\r\n...   padding = tf.RaggedTensor.from_row_lengths(pad_values, pad_row_lengths)\r\n...   return tf.concat([padding, rt], axis=1).to_tensor()\r\n>>> print(left_pad_2d_ragged(rt, 10))\r\ntf.Tensor(\r\n[[0 0 0 0 0 0 0 0 1 2]\r\n [0 0 0 0 0 0 0 0 0 0]\r\n [0 0 0 0 0 0 3 4 5 6]], shape=(3, 10), dtype=int32)\r\n```\r\n\r\nIf you'd be interested in putting together a PR that updates `to_tensor` to support pre/post padding, I'd be happy to take a look.  But I'd want it to be sufficiently general -- e.g., if there are multiple ragged dimensions, then we should probably be able to specify whether each dimension should have adding placed at the beginning or end of each slice."]}, {"number": 34792, "title": "Update systemlibs protobuf vars", "body": "Small patch to enable latest master builds using external ```protoc```\r\n\r\n```REPOSITORY_NAME``` and ```PACKAGE_NAME``` are no longer valid in newer bazel.\r\n\r\nBringing the attention of @perfinion .\r\n", "comments": []}, {"number": 34791, "title": "variable scope is changed by force when reopened (with regard to use Adam) (with new example)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 1080ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe same issue with #30813\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nPlease compare the results of following codes\r\n\r\ncode 1:\r\n```\r\nimport tensorflow as tf\r\nclass generator:\r\n    def __init__(self):\r\n        with tf.variable_scope('myscope'):\r\n            self.a = tf.get_variable('a', [4,3,2], tf.float32)\r\n            self.b = tf.get_variable('b', [4,3,2], tf.float32)\r\n            self.c = tf.reduce_sum(self.a + self.b)\r\n            self.d = tf.train.AdamOptimizer(name = 'adamopt')\r\n            self.e = self.d.compute_gradients(self.c, tf.trainable_variables())\r\n            self.f = self.d.apply_gradients(self.e)\r\n        for item in tf.global_variables():\r\n            if 'beta' in item.name:\r\n                print item\r\n    \r\n    def compute(self):\r\n        with tf.variable_scope('myscope'):\r\n            self.e1 = self.d.compute_gradients(self.c, tf.trainable_variables())\r\n            self.f1 = self.d.apply_gradients(self.e1)\r\n        print '------------------------'\r\n        for item in tf.global_variables():\r\n            if 'beta' in item.name:\r\n                print item\r\n    \r\n    def compute1(self):\r\n        with tf.variable_scope('myscope'):\r\n            self.e2 = self.d.compute_gradients(self.c, tf.trainable_variables())\r\n            self.f2 = self.d.apply_gradients(self.e2)\r\n        print '---------------------------'\r\n        for item in tf.global_variables():\r\n            if 'beta' in item.name:\r\n                print item\r\n\r\ng = generator()\r\ng.compute()\r\ng.compute1()\r\n```\r\n\r\nThe printed result will be:\r\n```\r\n<tf.Variable 'myscope/beta1_power:0' shape=() dtype=float32_ref>\r\n<tf.Variable 'myscope/beta2_power:0' shape=() dtype=float32_ref>\r\n------------------------\r\n<tf.Variable 'myscope/beta1_power:0' shape=() dtype=float32_ref>\r\n<tf.Variable 'myscope/beta2_power:0' shape=() dtype=float32_ref>\r\n---------------------------\r\n<tf.Variable 'myscope/beta1_power:0' shape=() dtype=float32_ref>\r\n<tf.Variable 'myscope/beta2_power:0' shape=() dtype=float32_ref>\r\n```\r\n\r\nHowever\r\n\r\ncode 2:\r\n```\r\nimport tensorflow as tf\r\nclass generator:\r\n    def __init__(self):\r\n        with tf.variable_scope('myscope'):\r\n            self.a = tf.get_variable('a', [4,3,2], tf.float32)\r\n            self.b = tf.get_variable('b', [4,3,2], tf.float32)\r\n            self.c = tf.reduce_sum(self.a + self.b)\r\n            self.d = tf.train.AdamOptimizer(name = 'adamopt')\r\n    \r\n    def compute(self):\r\n        with tf.variable_scope('myscope'):\r\n            self.e1 = self.d.compute_gradients(self.c, tf.trainable_variables())\r\n            self.f1 = self.d.apply_gradients(self.e1)\r\n        print '------------------------'\r\n        for item in tf.global_variables():\r\n            if 'beta' in item.name:\r\n                print item\r\n    \r\n    def compute1(self):\r\n        with tf.variable_scope('myscope'):\r\n            self.e2 = self.d.compute_gradients(self.c, tf.trainable_variables())\r\n            self.f2 = self.d.apply_gradients(self.e2)\r\n        print '---------------------------'\r\n        for item in tf.global_variables():\r\n            if 'beta' in item.name:\r\n                print item\r\n\r\ng = generator()\r\ng.compute()\r\ng.compute1()\r\n```\r\n\r\nresults in:\r\n```\r\n------------------------\r\n<tf.Variable 'myscope_1/beta1_power:0' shape=() dtype=float32_ref>\r\n<tf.Variable 'myscope_1/beta2_power:0' shape=() dtype=float32_ref>\r\n---------------------------\r\n<tf.Variable 'myscope_1/beta1_power:0' shape=() dtype=float32_ref>\r\n<tf.Variable 'myscope_1/beta2_power:0' shape=() dtype=float32_ref>\r\n```\r\n\r\nThe only difference between two code is that the first code block doesn't operate 'apply_gradients' operation within the first opening of the variable scope. And I found that this results from the fact that the `beta` is made with `variable_scope.variable` operation. Is this a bug? or an intended results?\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 1.12,1.15 and was able to reproduce the issue.Please, find the gists of [code1](https://colab.sandbox.google.com/gist/ravikyram/8da27df4cb68029c2dae23a95fb01d34/untitled439.ipynb) and [code2.](https://colab.sandbox.google.com/gist/ravikyram/2c0fffd2fae199f489869cf3bf8ae6f6/untitled440.ipynb) Thanks!", "@yanghoonkim Is this still an issue for you? Can you please try latest versions and let us know how it progresses. Thanks!\r\n\r\nPlease close the issue if this was already resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34791\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34791\">No</a>\n"]}, {"number": 34790, "title": "Dynamic scatter of TensorArray not working in tf.function + eager", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nThis may be related to #34683, but this is explicitly about the scatter method, I believe it is independent.\r\n\r\nThe following code creates a TensorArray in a while loop. Every time, a variable-length piece is added. Rules: I _need_ to use scatter and tf.Range like this, the example is a simplification.\r\n\r\nThe following works with:\r\n - Graph mode\r\n - Graph mode with tf.function (no autograph)\r\n - Eager mode\r\n\r\nIt breaks in Eager Mode with tf.function (no autograph)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nIt fails saying that the shape could not be determined, since the scatter unstacks the values internally.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nuse_eager = True  # switch to make it run\r\n\r\nif not use_eager:\r\n    tf.compat.v1.disable_eager_execution()\r\n\r\nstop_at = 1000\r\n\r\nempty_sample = tf.TensorArray(dtype=tf.float32, size=stop_at, dynamic_size=True,\r\n                              clear_after_read=True,  # we read only once at end to tensor\r\n                              element_shape=()\r\n                              )\r\n\r\n@tf.function(autograph=False)  # if removed, works\r\ndef body(sample, length):\r\n    n_to_draw = tf.cast(tf.random.poisson(shape=(), lam=30), dtype=tf.int32)\r\n    rnd = tf.random.uniform(shape=(n_to_draw,), dtype=tf.float32, maxval=1) + 3.\r\n    new_length = length + n_to_draw\r\n    indices = tf.range(length, new_length)\r\n    new_sample = sample.scatter(indices=indices, value=rnd)\r\n    return new_sample, new_length\r\n\r\n\r\ndef cond(sample, length):\r\n    return tf.less(length, stop_at)\r\n\r\n\r\nsampled = tf.while_loop(cond=cond, body=body, loop_vars=[empty_sample, 0], back_prop=False)[0]\r\nreshaped_sample = tf.reshape(sampled.stack(), shape=(-1,))  # make a read\r\nprint(reshaped_sample)\r\n\r\nif not use_eager:\r\n    with tf.compat.v1.Session() as sess:\r\n        print(sess.run(reshaped_sample))\r\n```\r\n**Other info / logs**\r\nThis is the actual error log\r\n```\r\n  File \"/home/jonas/.PyCharm2019.3/config/scratches/scratch_27.py\", line 21, in body\r\n    new_sample = sample.scatter(indices=indices, value=rnd)\r\n  File \"/home/jonas/anaconda3/envs/zfit37tf2/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/home/jonas/anaconda3/envs/zfit37tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 1168, in scatter\r\n    return self._implementation.scatter(indices, value, name=name)\r\n  File \"/home/jonas/anaconda3/envs/zfit37tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 873, in scatter\r\n    for index, val in zip(indices, array_ops.unstack(value)):\r\n  File \"/home/jonas/anaconda3/envs/zfit37tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 1333, in unstack\r\n    raise ValueError(\"Cannot infer num from shape %s\" % value_shape)\r\nValueError: Cannot infer num from shape (None,)\r\n```", "comments": ["Issue replicating for TF-2.0 and tf-nightly,kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/d513c8ebc713b5c725e55789587f4e7e/34790.ipynb) of colab.Thanks!", "Yes, it's the same issue as #34683 - because the \"eager\" implementation of TensorArray always expects a shape, it errors out in `scatter` before the tf.function verification takes place.\r\n\r\nIf we avoid the use of scatter, we get the error expected from #34683, in tf-nightly: \"NotImplementedError: Attempting to build a graph-mode TF2-style TensorArray from either an eager-mode TensorArray or a TF1-style TensorArray.\"\r\n\r\nI'm not sure if you can run the entire sampling process in graph, as below, but that clears the error:\r\n\r\n```\r\n@tf.function\r\ndef full_sample(stop_at):\r\n    sample = tf.TensorArray(dtype=tf.float32, size=stop_at, dynamic_size=True)\r\n    length = tf.constant(0)\r\n    while length < stop_at:\r\n        n_to_draw = tf.cast(tf.random.poisson(shape=(), lam=30), dtype=tf.int32)\r\n        rnd = tf.random.uniform(shape=(n_to_draw,), dtype=tf.float32, maxval=1) + 3.\r\n        new_length = length + n_to_draw\r\n        indices = tf.range(length, new_length)\r\n        sample = sample.scatter(indices=indices, value=rnd)\r\n        length = new_length\r\n\r\n    return sample.stack()\r\n\r\nsample = full_sample(stop_at)\r\nprint(sample)\r\n```", "That resolves the issue, true. Just putting `tf.function(autograph=False)` around the `TensorArray` creation is fine.\r\n\r\nThe improved error message is already in the nightlies as I see, so closing but feel free to reopen.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34790\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34790\">No</a>\n", "Bit of a caution that wrapping just the `TensorArray` creation in a tf.function no longer works in the nightly, and will most likely break in 2.1 or 2.2, due to the newly-added verification. It will only work reliably once #34683 is fully fixed."]}, {"number": 34789, "title": "GRUCell is not compatible with its own initial state", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.1.0rc0\r\n- Python version: 3.5.2\r\n\r\n**Describe the current behavior**\r\n\r\nThe initial state returned by `tf.keras.layers.GRUCell.get_initial_state()` can not be passed to the first cell call without error. It raises an `InvalidArgumentError` error.\r\n\r\n**Describe the expected behavior**\r\n\r\nRNN cells should accept their own initial states.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nbatch_size = 4\r\ncell = tf.keras.layers.GRUCell(20)\r\ninitial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\r\noutput, state = cell(tf.random.uniform([batch_size, 10]), initial_state)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test/gru_incompat.py\", line 5, in <module>\r\n    output, state = cell(tf.random.uniform([batch_size, 10]), initial_state)\r\n  File \"/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 822, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/recurrent.py\", line 1846, in call\r\n    matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\r\n  File \"/lib/python3.5/site-packages/tensorflow_core/python/keras/backend.py\", line 1678, in dot\r\n    out = math_ops.matmul(x, y)\r\n  File \"/lib/python3.5/site-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py\", line 2797, in matmul\r\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n  File \"/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 5631, in mat_mul\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py\", line 6598, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: In[0] is not a matrix. Instead it has shape [20] [Op:MatMul] name: gru_cell/MatMul/\r\n```", "comments": ["@guillaumekln hi, I met this error too, and I think here is a bug. I changed two line of codes, everything is ok.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4235c011cf48daf64953895e844dba7b72b3edc0/tensorflow/python/keras/layers/recurrent.py#L1773\r\nshould be `h_tm1 = states  # previous memory`\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4235c011cf48daf64953895e844dba7b72b3edc0/tensorflow/python/keras/layers/recurrent.py#L1871\r\nshould be `return h, h`\r\n", "Sorry for the breakage, will fix it very soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34789\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34789\">No</a>\n"]}, {"number": 34788, "title": "Inconsistent cpu/gpu results of gather_nd", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: 11G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen I set device to gpu, the code runs correctly. When I set device to cpu, the code raises error: tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[4] = [4, -1] does not index into param shape [5,1] [Op:GatherNd].\r\n**Describe the expected behavior**\r\nConsistent results on cpu and gpu.\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(2222222)\r\n\r\nwith tf.device('cpu:0'):\r\n  a = np.random.rand(5, 1)\r\n  print(a)\r\n\r\n  b = tf.gather_nd(\r\n      a,\r\n      [[0, -1], [1, -1], [2, -1], [3, -1], [4, -1]])\r\n\r\n  print(b.numpy())\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 2.0, 2.1.0-dev20191203 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/cccb91f1714c050ea544e6ada5861cb3/untitled441.ipynb). Thanks!", "@ravikyram Looking forward to your fix. Thx!", "This is a known issue in TF we cannot change without breaking backward compatibility; gather with invalid indices fails on the CPU but does not on the GPU :-/", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34788\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34788\">No</a>\n"]}, {"number": 34787, "title": "Bug: tensors built by tf.keras layers cannot use numpy() to obtain the array. ", "body": "Bug: tensors built by tf.keras layers cannot use numpy() to obtain the array. So can i get the value of such tensors??", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nRequest you to provide simple standalone code to reproduce the issue in our environment. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Operating system is windows 10, and TensorFlow version s 2.0.0. \r\nI want to obtain the value of my custom layer, and fail by using numpy() which raises 'Tensor' object has no attribute 'numpy' error. \r\nEager execution is used automatically. \r\n\r\nThe code is here, using an example in the tensorflow tutorials.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import *\r\n\r\nprint(\"TensorFlow version: {}\".format(tf.__version__))\r\nprint(\"Eager execution: {}\".format(tf.executing_eagerly()))\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nimport tensorflow.keras.backend as K\r\n\r\nclass MyDenseLayer(tf.keras.layers.Layer):\r\n    def __init__(self, num_outputs):\r\n        super(MyDenseLayer, self).__init__()\r\n        self.num_outputs = num_outputs\r\n        self.a = None\r\n  \r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(\"kernel\",\r\n                                        shape=[int(input_shape[-1]), self.num_outputs])\r\n  \r\n    def call(self, inputs):\r\n        self.a = K.dot(inputs, self.kernel)\r\n        return K.dot(inputs, self.kernel)\r\n\r\nlayer1 = MyDenseLayer(64)\r\ninput_tensor = Input(shape=(28, 28))\r\nx = layers.Flatten()(input_tensor)\r\nx = layer1(x)\r\noutput_tensor = layers.Dense(10, activation='softmax')(x)\r\n\r\nmodel = Model(input_tensor,  output_tensor)\r\n\r\nmodel.summary()\r\n\r\nadam = tf.keras.optimizers.Adam()\r\nmodel.compile(optimizer=adam,\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nfor epoch in range(2):\r\n    model.fit(x_train, y_train, batch_size=32, epochs=1)\r\n\r\nlayer = model.layers[2]\r\nprint('--------------------------------------------------')\r\nprint(layer.a)\r\n\r\nlayer.a.numpy()\r\n```\r\n\r\n\r\n\r\n", "I have tried on colab with TF version 2.0 ,2.1.0-dev20191203 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/de006b8bfcd8c04b89f483a0eaececac/untitled446.ipynb). Thanks!", "Even though the boolean flag \"tf.executing_eagerly()\" is True, while using the Keras model you are essentially creating a graph(DAGs) of layers. So in the flow through the graph, tensors created are of the type \"tensorflow.python.framework.ops.Tensor\" and not \"tensorflow.python.framework.ops.EagerTensor\". \r\nHence the error \"'Tensor' object has no attribute 'numpy' error.\".\r\n", "> Even though the boolean flag \"tf.executing_eagerly()\" is True, while using the Keras model you are essentially creating a graph(DAGs) of layers. So in the flow through the graph, tensors created are of the type \"tensorflow.python.framework.ops.Tensor\" and not \"tensorflow.python.framework.ops.EagerTensor\".\r\n> Hence the error \"'Tensor' object has no attribute 'numpy' error.\".\r\n\r\nHow to change tensorflow.python.framework.ops.Tensor into numpy?", "You have to explicitly declare as EagerMode to run it eagerly\r\ntf.config.experimental_run_functions_eagerly(True).\r\n\r\nPlease note that there will be significant performance hits when this is set to true. This is suggested only in cases like debugging etc.\r\nEven after decorating the __call__ method of  MyDenseLayer with tf.function. there is only slight performance gain.\r\nPlease find the gist here https://colab.research.google.com/drive/16hPlAHon5Lwr2uy1Nl3OSAOiBXLqSe4A", "@Tenyn thanks for the issue!\r\n\r\nAs @Athul8raj mentioned, this is expected behavior for Tensor created inside a Functional Model. These Tensors are created during the execution of a `tf.function`, and as such are Symbolic Tensors, which cannot be converted to a value\r\n\r\nYou could assign `layer.a` to a `Variable` if you need to inspect it's value after execution, the `Variable`s have values that live on after `tf.function` execution. Something like this should work:\r\n\r\n```python\r\nclass MyDenseLayer(tf.keras.layers.Layer):\r\n    def __init__(self, num_outputs):\r\n        super(MyDenseLayer, self).__init__()\r\n        self.num_outputs = num_outputs\r\n        self.a = tf.Variable([0]*num_outputs, dtype=tf.dtypes.float32)\r\n  \r\n    def build(self, input_shape):\r\n        self.kernel = self.add_weight(\"kernel\",\r\n                                        shape=[int(input_shape[-1]), self.num_outputs])\r\n  \r\n    def call(self, inputs):\r\n        self.a.assign(K.dot(inputs, self.kernel))\r\n        return K.dot(inputs, self.kernel)\r\n\r\nlayer1 = MyDenseLayer(64)\r\ninput_tensor = Input(shape=(28, 28))\r\nx = layers.Flatten()(input_tensor)\r\nx = layer1(x)\r\noutput_tensor = layers.Dense(10, activation='softmax')(x)\r\n\r\nmodel = Model(input_tensor,  output_tensor)\r\n\r\nmodel.summary()\r\n\r\nadam = tf.keras.optimizers.Adam()\r\nmodel.compile(optimizer=adam,\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nfor epoch in range(2):\r\n    model.fit(x_train, y_train, batch_size=32, epochs=1)\r\n\r\nlayer = model.layers[2]\r\nprint('--------------------------------------------------')\r\nprint(layer.a)\r\n\r\nlayer.a.numpy()\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34787\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34787\">No</a>\n"]}, {"number": 34786, "title": "memory leak on tf2.0 with tf.signal.frame ", "body": "The memory doubles every time I run the tape.gradients part.  I think it's connected to the timedistributedlayers...?\r\n\r\n```\r\n###### create model \r\n\r\n    inputs = tf.keras.Input(shape=(6, *data_loader_train[0][0][0].shape), name='img') ## (108, 192, 3)\r\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(inputs)\r\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(x)\r\n    block_1_output = layers.TimeDistributed(layers.MaxPooling2D(2))(x)\r\n\r\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu', padding='same'))(block_1_output)\r\n    block_3_output = layers.add([x, block_1_output])\r\n    block_3_output = layers.TimeDistributed(layers.MaxPooling2D(2))(block_3_output)\r\n\r\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(block_3_output)\r\n    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x)\r\n\r\n    x = layers.Flatten()(x)\r\n    x = layers.Dense(16, activation='relu')(x)\r\n    x = layers.Dense(1)(x)\r\n    counts = tf.keras.activations.softplus(x)\r\n\r\n    model = tf.keras.Model(inputs, counts, name='toy_resnet')\r\n    model.summary()\r\n\r\n    ### run model\r\n```\r\n\r\n```\r\n####### running this part doubles memory every two times ##########\r\nfor x_ in batch(np.random.uniform(size=(100,6,108,192,3)).astype(np.float32), 10):\r\n     with tf.GradientTape() as tape:\r\n             count_ = tf.reduce_sum(model(x_))\r\n```", "comments": ["@johnpjust ,\r\nWhen tried running the given code `NameError: name 'data_loader_train' is not defined` error was faced, can you provide the standalone code to replicate the issue reported above ?Thanks!", "@oanush \r\n```\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n \r\ndef batch(iterable, n=1):\r\n    l = len(iterable)\r\n    for ndx in range(0, l, n):\r\n        yield iterable[ndx:min(ndx + n, l)]\r\n \r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n    except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n                        \r\n \r\nwith tf.device('/gpu:0'):\r\n    inputs = tf.keras.Input(shape=(6, 108, 192, 3), name='img') ## (108, 192, 3)\r\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(inputs)\r\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(x)\r\n    block_1_output = layers.TimeDistributed(layers.MaxPooling2D(2))(x)\r\n \r\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu', padding='same'))(block_1_output)\r\n    # x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\r\n    block_3_output = layers.add([x, block_1_output])\r\n    block_3_output = layers.TimeDistributed(layers.MaxPooling2D(2))(block_3_output)\r\n \r\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(block_3_output)\r\n    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x)\r\n \r\n    x = layers.Flatten()(x)\r\n    x = layers.Dense(16, activation='relu')(x)\r\n    x = layers.Dense(1)(x)\r\n    counts = tf.keras.activations.softplus(x)\r\n    # x = layers.Dropout(0.5)(x)\r\n    # outputs = layers.Dense(10, activation='softmax')(x)\r\n \r\n    model = tf.keras.Model(inputs, counts, name='toy_resnet')\r\n    model.summary()\r\n \r\n            ### everytime this is run, gpu memory grows\r\nfor x_ in batch(np.random.uniform(size=(100,6,108,192,3)).astype(np.float32), 10):\r\n    temp = model(x_) \r\n\r\n```", "@johnpjust ,\r\nI tried running the code in colab for TF-2.0 and i didn't face any issue,kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/dc54aed45ba16f16cb96f1e3aa27d1e2/34786.ipynb) for your reference. Please provide us gist if the issue is replicating from your end.", "@oanush I'm not familiar with this gist your are referring to, but I have checked this on two different machines, using both Linux and Windows, and with TF2.0 and TF1.13, so i know the issue persists.  If you have a way to watch your GPU memory growth, then please use that and you will see what I'm talking about.  I use \"nvidia-smi  -l 2\".", "@oanush  Note that I've found a way around this issue by only using the timedistributedlayer once.  For this i just create the CNN model separately and then do a sequential model with the first layer being a timedistributedlayer using the CNN as input.", "@oanush  OK so I've been messing with this some more, and there is definitely a VRAM (GPU) memory leak associated with tf.signal.frame that interacts with the model.  Simply exchanging it with an itertools function eliminates the issue...\r\n\r\n```\r\ndef batch(iterable, n=1):\r\n    l = len(iterable)\r\n    for ndx in range(0, l, n):\r\n        yield iterable[ndx:min(ndx + n, l)]\r\n\r\n####### here's an example model #############\r\nwith tf.device('/gpu:0'):\r\n    inputs = tf.keras.Input(shape=(108, 192, 3), name='img') ## (108, 192, 3)\r\n    x = layers.Conv2D(16, 3, activation='relu')(inputs)\r\n    x = layers.Conv2D(16, 3, activation='relu')(x)\r\n    block_1_output = layers.MaxPooling2D(2)(x)\r\n\r\n    x = layers.Conv2D(16, 3, activation='relu', padding='same')(block_1_output)\r\n    # x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\r\n    x = layers.add([x, block_1_output])\r\n    block_2_output = layers.MaxPooling2D(2)(x)\r\n\r\n    x = layers.Conv2D(16, 3, activation='relu', padding='same')(block_2_output)\r\n    x = layers.add([x, block_2_output])\r\n    x = layers.MaxPooling2D(2)(x)\r\n    block_3_output = layers.GlobalAveragePooling2D()(x)\r\n\r\n    # x = layers.Flatten()(x)\r\n    # x = layers.Dense(16, activation='relu')(x)\r\n    # x = layers.Dense(1)(x)\r\n    # counts = tf.keras.activations.softplus(x)\r\n\r\n    cnn = tf.keras.Model(inputs, block_3_output, name='toy_resnet')\r\n    # model = tf.keras.Sequential()\r\n    # model.add(layers.TimeDistributed(cnn, input_shape=(6, 108, 192, 3)))\r\n    # model.add(layers.Dense(16, activation='relu'))\r\n    # model.add(layers.Dense(1))\r\n\r\n    input_sequences = tf.keras.Input(shape=(6, 108, 192, 3)) ## (108, 192, 3)\r\n    x = layers.TimeDistributed(cnn)(input_sequences)\r\n    x = layers.Flatten()(x)\r\n    x = layers.Dense(16, activation='relu')(x)\r\n    x = layers.Dense(1)(x)\r\n    counts = tf.keras.activations.softplus(x)\r\n    model = tf.keras.Model(input_sequences, counts, name='toy_resnet')\r\n    model.summary()\r\n\r\n###### run code below using tf.signal.frame and watch the VRAM memory on the GPU.  You will see it grow before throwing an error.  Then do the same with \"more_itertools\" and you'll see it's fine.\r\n\r\nfor n in range(50):\r\n    ##### exchange more_itertools with tf.signal.frame to get memory leak\r\n    x_mb = tf.signal.frame(np.random.uniform(size=(200,108,192,3)).astype(np.float32), args.num_frames, 1, axis=0)\r\n    for x_ in batch(x_mb, 10):\r\n    ################# no memory leak with more_itertools for sliding window framing ###############\r\n    # for x_ in batch(np.array(list(more_itertools.windowed(np.random.uniform(size=(100, 108, 192, 3)).astype(np.float32), n=6, step=1))),10):\r\n    #########################################\r\n        temp = model(x_)\r\n```", "update:  further testing -- in addition to the above repeatable memory leak.   I've found using the above model and batch loader, and then a toy training loop below, that I'm still seeing the memory leak but it's much more delayed.  As it stands the model will train using about 9gb of VRAM up to incount =~8000 or so, after which it will jump up to to the max value of available VRAM (for me that's ~11gb).  Then if I keep training it will eventually throw an OOM as it leaks more memory.\r\n\r\nAll in all there are definitely some glitchy things with some of these less-used features in TF like tf.signal.frame and timedistributedlayer, and these are the types of things that we are trying to make use of on the cutting edge front of applications.  Thus it is well worth the time to fix some of these issues.\r\n\r\nThanks.\r\n\r\n```\r\ndata_loader_train = np.random.uniform(size=(100, 120, 108, 192, 3)).astype(np.float32)\r\n\r\nindcount = 0\r\nfor epoch in range(args.epochs):\r\n    train_loss = 0\r\n    for ind in range(len(data_loader_train)):\r\n    # for ind in np.random.permutation(len(data_loader_train)):\r\n    #     print(r'epoch:  %i,   index:  %i' % (epoch, ind), end=\"\\r\")\r\n        print(r'indcount:  %i' % indcount, end=\"\\r\")\r\n        # x_mb = np.array(list(more_itertools.windowed(data_loader_train[ind][0], n=args.num_frames, step=1)))\r\n        # y_mb = data_loader_train[ind][1]\r\n        x_mb = np.array(list(more_itertools.windowed(data_loader_train[ind], n=args.num_frames, step=1)))\r\n        y_mb = np.random.randint(20,40)\r\n        count = 0\r\n        grads = [np.zeros_like(x) for x in model.trainable_variables]\r\n        # print(\"index:  \" + str(ind))\r\n        for x_ in batch(x_mb, args.batch_size):\r\n            indcount += 1\r\n            with tf.GradientTape() as tape:\r\n                count_ = tf.reduce_sum(model(x_))\r\n            count += count_\r\n            grads_ = tape.gradient(count_, model.trainable_variables)\r\n            grads = [x1 + x2 for x1, x2 in zip(grads, grads_)]\r\n\r\n        # grads = [None if grad is None else tf.clip_by_norm(grad, clip_norm=args.clip_norm) for grad in grads]\r\n        loss = count-y_mb\r\n        globalstep = optimizer.apply_gradients(zip([2*loss*x for x in grads], model.trainable_variables))\r\n\r\n        tf.summary.scalar('loss/train', loss**2, globalstep)\r\n## after 5 epochs\r\n```", "Apologies for the delay in response. The second last code snippet where you are using tf.signal.frame looks incomplete. It will be great if you can complete it. Also feel free to close this issue if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34786\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34786\">No</a>\n"]}, {"number": 34785, "title": "How does TensorFlow calculate the gradients of an FFT layer?", "body": "Hi, if I insert the function, i.e., tf.fft(input, name=None), into a neural network, how does TensorFlow calculate the gradients in backpropagation?\r\n\r\nI didn't find a documentation about this. I am using TensorFlow 1.0.\r\n\r\nDoes anyone know\uff1f", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 34784, "title": "Boosted trees using Estimators codes crash", "body": "When I test the codes in https://www.tensorflow.org/tutorials/estimator/boosted_trees, it crashed. and I see the error of \"malloc(): memory corruption (fast): 0x00007f4f84066f50 ***\". The detailed error messages can be found here(https://gist.github.com/fancyerii/c805604e94b76988771e4ff045aeb303)\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.0\r\n- GPU model and memory: Quadro P3000\r\n\r\n\r\n\r\n**Code to reproduce the issue**\r\nhttps://gist.github.com/fancyerii/538d4033bc94115c561a146b78beeac9\r\n\r\n**Other info / logs**\r\nhttps://gist.github.com/fancyerii/c805604e94b76988771e4ff045aeb303\r\n", "comments": ["@fancyerii ,\r\nWhen tried running the given code in TF-2.0, I did not face any error. Kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/6b66743fe92a533bd9f0a35e47976b87/34784.ipynb) of colab for your reference.Thanks!", "@fancyerii ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34783, "title": "Added usage examples for multiple API symbols", "body": "Added usage examples for image.resize_with_crop_or_pad, image.resize_with_pad, image.random_brightness, image.random_contrast, image.random_crop, math.multiply_no_nan, and math.reciprocal_no_nan.", "comments": ["@Jake-Short Looks good to me now \ud83d\udc4d ", "@Ayush517 @Jake-Short , I think it would be better to use the triple left-angle bracket (`>>>`) syntax for examples as recommended in the [contributing guidelines](https://www.tensorflow.org/community/contribute/docs_ref#testable_docstrings). It would also make the examples compatible with `DocTest`. For testing the return values, Google Colab notebooks with `TensorFlow 2` can be used.", "@Jake-Short  thank you, it is still failing doctest can you please check here for [logs](https://source.cloud.google.com/results/invocations/a9741221-910e-457d-9c81-7c18b00dca03/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests).\r\nPlease run the doctest locally as mentioned here in the [contributor guidelines](https://www.tensorflow.org/community/contribute/docs_ref).\r\n\r\nAlso, can you please help to fix Ubuntu Sanity errors? Thanks!", "It looks like a lot of these have been added in later pull requests (I.E. #34798 and #35332). I will close this PR in favor of those."]}, {"number": 34782, "title": "[tf.keras] Mixed precision policy \"mixed_bfloat16\" not supported in Keras compile", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Co-lab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TF 2.1-rc0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nWe are porting a GPU based model to CloudTPU. We are using Keras **mixed_float16** mixed-precision policy to enable TensorCore on GPU. Without any code change, we are trying to use **mixed_bfloat16** for CloudTPU for maximal performance.\r\n\r\n**Describe the expected behavior**\r\n\r\n``model.compile`` with **mixed_bfloat16** policy to enable mixed-precision training on CloudTPU.\r\n\r\n**Code to reproduce the issue**\r\n\r\nColab link: https://colab.research.google.com/drive/1-SBnqhsmyjVNJxntB8ZZdqRUk7h7tRs8\r\n\r\n```\r\ndef compile_keras_model(dtype):\r\n  policy = tf.keras.mixed_precision.experimental.Policy(dtype)\r\n  tf.compat.v2.keras.mixed_precision.experimental.set_policy(policy)\r\n\r\n  optimizer = tf.optimizers.SGD(learning_rate=0.1, momentum=0.9)\r\n\r\n  model = tf.keras.applications.resnet50.ResNet50(weights=None)\r\n  model.compile(loss='sparse_categorical_crossentropy',\r\n                optimizer=optimizer, \r\n                metrics=['sparse_categorical_accuracy'])\r\n  return model\r\n  \r\ngpu_model = compile_keras_model('mixed_float16')\r\ntpu_model = compile_keras_model('mixed_bfloat16')\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-5-ad9cf91bd793> in <module>()\r\n     12 \r\n     13 gpu_model = compile_keras_model('mixed_float16')\r\n---> 14 tpu_model = compile_keras_model('mixed_bfloat16')\r\n\r\n12 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)\r\n     59           \"allowed values: %s\" %\r\n     60           (param_name, dtypes.as_dtype(dtype).name,\r\n---> 61            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n     62 \r\n     63 \r\n\r\nTypeError: Value passed to parameter 'x' has DataType bfloat16 not in list of allowed values: float16, float32, float64, complex64, complex128\r\n```", "comments": ["I have tried on colab with TF version 2.1.0-rc0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/0d0639c5e7a5dae0937d7aa16b042d59/untitled444.ipynb). Thanks!", "Gently ping @reedwm; is this behavior intended?", "The issue is that we are calling an op that does not support bfloat16 in the Loss. We should do all Losses in float32, but I haven't implemented that yet. I regret not doing this for 2.1, but this will definitely be fixed in 2.2.\r\n\r\nBecause of this issue, in 2.1, models should end in float32. For models which you cannot modify, like `tf.keras.applications.resnet50.ResNet50`, you can wrap the model with a version that has a float32 output, as follows:\r\n\r\n```python\r\ndef compile_keras_model(dtype):\r\n  policy = tf.keras.mixed_precision.experimental.Policy(dtype)\r\n  tf.compat.v2.keras.mixed_precision.experimental.set_policy(policy)\r\n\r\n  optimizer = tf.optimizers.SGD(learning_rate=0.1, momentum=0.9)\r\n\r\n  model = tf.keras.applications.resnet50.ResNet50(weights=None)\r\n\r\n  # Create new model that is `model` except the output is float32\r\n  inp = tf.keras.layers.Input(batch_shape=model.input_shape)\r\n  out = model(inp)\r\n  # This layer simply casts to float32\r\n  out = tf.keras.layers.Activation('linear', dtype='float32')(out)\r\n  model = tf.keras.Model(inp, out)\r\n\r\n  model.compile(loss='sparse_categorical_crossentropy',\r\n                optimizer=optimizer,\r\n                metrics=['sparse_categorical_accuracy'])\r\n  return model\r\n\r\ngpu_model = compile_keras_model('mixed_float16')\r\ntpu_model = compile_keras_model('mixed_bfloat16')\r\n```\r\n\r\nAdditionally, if your model ends in softmax, you should also do the softmax in float32, as a few models require this for numeric stability and it has very little impact on performance. Only softmaxes at the end of the model need to be float32; if a softmax is in the middle of the model, doing it in float16 is fine. For example, if your model ends with the following\r\n```python\r\nout = tf.keras.layers.Dense(32, activation='softmax')(out)\r\n```\r\n\r\nReplace it with the following:\r\n\r\n```python\r\nout = tf.keras.layers.Dense(32)(out)\r\nout = tf.keras.layers.Activation('softmax', dtype='float32')(out)\r\n```\r\nThis will cause the softmax to be in float32, and the output of the model in float32.\r\n\r\nUnfortunately, none of the keras application models currently do the last softmax in float32, but I will fix in a future release. If feasible, I will try to have this done automatically. Luckily, for most models, including resnet50, doing the softmax at the end of the model in float16 is fine", "@reedwm that\u2019s very helpful, thanks Reed!\r\n\r\nI\u2019ll leave this issue as a marker and close it once it\u2019s fixed in master.", "@byronyi Looks like this was already implemented in the recent `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/12876c8f6f6e2e185b6952ebea37a4da/untitled444.ipynb). Thanks.\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Ah yeah, this was fixed fairly recently.\r\n\r\nIt turns out doing softmax + loss in fp16 or bf16 is fine in most cases. For this particular issue, the error came from the div_no_nan op, which Keras will now always do in fp32 to avoid the error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34782\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34782\">No</a>\n"]}, {"number": 34781, "title": "obfuscate_names encrypts inputs", "body": "I'm try work on with Graph Transform Tool. \r\nAnd found with the flag `obfuscate_names`, he inputs the encrypting. Outputs not encrypting.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7.2\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 1060\r\n\r\n", "comments": ["@sonfiree Can you please provide a simple reproducible case for us to reproduce the issue. Thanks!", "@sonfiree Can you please respond to the above question. Thanks!", "Closing this issue as it has been inactive for more than 3 weeks. Please add additional comments and we can open the issue again. Thanks!"]}, {"number": 34780, "title": "[FusedBatchNormGradOp] Actual shapes of outputs NOT consistent with the shapes derived by shapeFn", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n> NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n> Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):\r\n> Binary\r\n- TensorFlow version (use command below):\r\n> 1.12.0\r\n- Python version:\r\n> 3.6.3\r\n\r\n**Describe the current behavior**\r\nFor the operator FusedBatchNormGradOp, the `shape derived by shapeFn` is inconsistent with the `run-time shape of the actual output` when set `is_training = False` , and this difference is caused by the obvious code. I wonder if this is a bug? In fact, I rely on the shape derived by Tensorflow to build my network...\r\n\r\n**Describe the expected behavior**\r\nThe shapefn derivation gives the same result as the runtime, unless it's an so called `unknown shape operator`.\r\n\r\n**Code to reproduce the issue**\r\n- Just as an example:\r\n> the comment line illustrates the problem\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import gen_nn_ops\r\n\r\ny_backprop=np.random.random((32,32,2,1024)).astype(np.float32)\r\nx=np.random.random((32,32,2,1024)).astype(np.float32)\r\nscale=np.random.random((1024)).astype(np.float32)\r\nreserve_space_1=np.random.random((1024)).astype(np.float32)\r\nreserve_space_2=np.random.random((1024)).astype(np.float32)\r\nfused_batch_norm_grad_result = gen_nn_ops.fused_batch_norm_grad(y_backprop, x, scale, reserve_space_1, reserve_space_2, epsilon=0.0001, data_format=\"NHWC\", is_training=False)\r\n\r\nprint(fused_batch_norm_grad_result)\r\n# printed: (32, 32, 2, 1024), (1024,), (1024,), (1024,), (1024,)\r\nfused_batch_norm_grad_result=tf.Session().run(fused_batch_norm_grad_result)\r\nfor t in fused_batch_norm_grad_result:\r\n    print(np.shape(t))\r\n# printed: (32, 32, 2, 1024), (1024,), (1024,), (), ()\r\n```\r\n\r\n**Other info / logs**\r\nThe code that causes this problem is easily found in the Compute method of FusedBatchNormGradOp as follows:\r\n```c\r\n    ctx->SetOutput(1, scale_backprop);\r\n    ctx->SetOutput(2, offset_backprop);\r\n    ctx->SetConstantOutput(3, Tensor());\r\n    ctx->SetConstantOutput(4, Tensor());\r\n```\r\nHowever, the following derivation is made in shapefn:\r\n```c\r\n  // Set the correct shapes for reserve_spaces\r\n  // so that gradients can be performed when\r\n  // the op is in a symbolic condition.\r\n  if (is_training) {\r\n    c->set_output(3, c->Vector(0));\r\n    c->set_output(4, c->Vector(0));\r\n  } else {\r\n    c->set_output(3, c->Vector(channel_dim));\r\n    c->set_output(4, c->Vector(channel_dim));\r\n  }\r\n```\r\nI noticed the comment above that says this branch is to set the proper shape for reserve_spaces, but I can't understand what that really means...\r\n\r\n", "comments": ["Issue replicating for Tf-1.12 and TF-1.15 also, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/5c84d5aabb47b85a792e7c0d474336bd/34780.ipynb) of colab.Thanks!", "@dMokaMoka  It seems the issue is fixed now.Could you please confirm still if you are facing the same issue?.Thanks!", "@dMokaMoka  Any updates regarding on this issue?.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34780\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34780\">No</a>\n"]}, {"number": 34779, "title": "Return new instance of AutoCastVariable after assignment", "body": "Assignments and sparse updates will now return a new instance of `AutoCastVariable`, wrapping the [`_UnreadVariable`](https://github.com/tensorflow/tensorflow/blob/2692ea8ec1953e42952597adb5b5099181a679b2/tensorflow/python/ops/resource_variable_ops.py#L1806) returned from the assignment op, so that the `dtype` is preserved.\r\n\r\nCloses #34332\r\n\r\n/cc @reedwm @alextp", "comments": ["I cannot reproduce the [GPU test failures](https://source.cloud.google.com/results/invocations/33649cdd-6664-42f9-b28a-3d76db537aba/targets) locally because I don't have a GPU in my machine @reedwm would you be able to take a look at this?", "On Tue, Dec 3, 2019 at 3:45 PM Reed <notifications@github.com> wrote:\n\n> *@reedwm* requested changes on this pull request.\n> ------------------------------\n>\n> In\n> tensorflow/python/keras/mixed_precision/experimental/autocast_variable.py\n> <https://github.com/tensorflow/tensorflow/pull/34779#discussion_r353481035>\n> :\n>\n> > @@ -185,46 +185,60 @@ def constraint(self):\n>      return self._variable.constraint\n>\n>    def assign(self, value, use_locking=None, name=None, read_value=True):\n> -    return self._variable.assign(value, use_locking, name, read_value)\n> +    assign_op = self._variable.assign(value, use_locking, name, read_value)\n> +    return create_autocast_variable(assign_op) if read_value else assign_op\n>\n> This will fail with DistributionStrategy with at least two devices. The\n> issue is that MirroredVariable.assign will (incorrectly) return a Mirrored\n> value instead of a MirroredVariable. Following the logic, I think\n> MirroredVariable eventually returns a Mirrored here\n> <https://github.com/tensorflow/tensorflow/blob/147de48ad973a6a05e8113af815988014652caf2/tensorflow/python/distribute/values.py#L1637>.\n> I'm guessing that with one device, it simply will return a normal\n> non-mirrored variable.\n>\n> To reproduce, on this line\n> <https://github.com/tensorflow/tensorflow/blob/147de48ad973a6a05e8113af815988014652caf2/tensorflow/python/keras/mixed_precision/experimental/autocast_variable_test.py#L64>,\n> change ['cpu:0'] to ['cpu:0', 'gpu:0']. Since you do not have a GPU, also\n> add the following two lines to the top of the file:\n>\n> from google3.third_party.tensorflow.python.framework import config\n> config.set_soft_device_placement(True)\n>\n> Unfortunately, soft placement seems to be broken in Eager mode (I'll ask\n> internally to get this resolved), but the graph test is running first so\n> you'll be able to reproduce the issue. If you can get it to pass in graph\n> mode, chances are it will pass in eager.\n>\n\nInstead of soft placement use tf.config.experimental.set_visible_devices to\nmake a fake cluster with 2 cpu devices.\n\n\n> I can't think of an elegant way of fixing this, unfortunately. I think for\n> now, you should simply return assign_op if it is Mirrored (or perhaps if not\n> resource_variable_ops.is_resource_variable(assign_op)). This means #34332\n> <https://github.com/tensorflow/tensorflow/issues/34332> won't be fixed in\n> the distributed case for now, but that can be addressed later. Let me know\n> if you can think of a better solution.\n>\n> In the long term, we can fix this by having DistributionStrategy return a\n> MirroredVariable instead of Mirrored from MirroredVariable.assign().\n> @guptapriya <https://github.com/guptapriya> can you work on that?\n> ------------------------------\n>\n> In\n> tensorflow/python/keras/mixed_precision/experimental/autocast_variable_test.py\n> <https://github.com/tensorflow/tensorflow/pull/34779#discussion_r353481496>\n> :\n>\n> >            if not context.executing_eagerly():\n>              if not distribute:\n>                # These functions are not supported for DistributedVariables\n>                x.load(9)\n>                self.assertEqual(x.eval(), 9)\n> -            self.assertEqual(evaluate(x.initial_value), 7)\n> +            self.assertEqual(self.evaluate(x.initial_value), 7)\n>              self.assertEqual(x.op, x._variable.op)\n>              self.assertEqual(x.graph, x._variable.graph)\n>            if not distribute:\n>\n> You should also test that you can assign the return value of x.assign(8).\n> And that if you pass read_value=False, the return value is an Operation\n> or None instead of a variable.\n>\n> I would move the assign, assign_add, and assign_sub calls to a new test\n> method as well.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34779?email_source=notifications&email_token=AAABHRMQ6HYNXMCTVOFF53DQW3VTBA5CNFSM4JUQOAZKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCNWHI3Y#pullrequestreview-325874799>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRK3XM2RBFJLBZX4QVTQW3VTBANCNFSM4JUQOAZA>\n> .\n>\n\n\n-- \n - Alex\n", "But that's what it has to do now, and we have tests validating it.\n\nOn Thu, Dec 5, 2019 at 3:01 PM guptapriya <notifications@github.com> wrote:\n\n> *@guptapriya* commented on this pull request.\n> ------------------------------\n>\n> In\n> tensorflow/python/keras/mixed_precision/experimental/autocast_variable.py\n> <https://github.com/tensorflow/tensorflow/pull/34779#discussion_r354591826>\n> :\n>\n> > @@ -185,46 +185,60 @@ def constraint(self):\n>      return self._variable.constraint\n>\n>    def assign(self, value, use_locking=None, name=None, read_value=True):\n> -    return self._variable.assign(value, use_locking, name, read_value)\n> +    assign_op = self._variable.assign(value, use_locking, name, read_value)\n> +    return create_autocast_variable(assign_op) if read_value else assign_op\n>\n> @reedwm <https://github.com/reedwm> are you saying var.assign(..) should\n> return a variable? that's not what tf.Variable docs say though:\n> https://www.tensorflow.org/api_docs/python/tf/Variable#assign\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/34779?email_source=notifications&email_token=AAABHRIHYWRA23CUJSQTCBLQXGB25A5CNFSM4JUQOAZKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCOFX76A#discussion_r354591826>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKWW4RAVTAQTQLNNS3QXGB25ANCNFSM4JUQOAZA>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 34778, "title": "Can't get a tape.gradient when tf.signal.frame is used.", "body": "Tensorflow 2.0.\r\nI believe this is related to the tf.gather problems that have been noted already.  \r\n\r\n```\r\n    inputs = tf.keras.Input(shape=data_loader_train[0][0][0].shape, name='img') ## (108, 192, 3)\r\n    x = layers.Conv2D(32, 3, activation='relu')(inputs)\r\n    x = layers.Conv2D(16, 3, activation='relu')(x)\r\n    block_1_output = layers.MaxPooling2D(2)(x)\r\n\r\n    x = layers.Conv2D(16, 3, activation='relu', padding='same')(block_1_output)\r\n    # x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\r\n    block_2_output = layers.add([x, block_1_output])\r\n    block_2_output = layers.MaxPooling2D(2)(block_2_output)\r\n\r\n    x = layers.Conv2D(16, 3, activation='relu', padding='same')(block_2_output)\r\n    # x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\r\n    block_3_output = layers.add([x, block_2_output])\r\n    block_3_output = layers.MaxPooling2D(2)(block_3_output)\r\n\r\n    x = layers.Conv2D(32, 3, activation='relu')(block_3_output)\r\n    x = layers.GlobalAveragePooling2D()(x)\r\n\r\n### if this part is included we get an error ##########\r\n    # x = layers.Flatten()(x)\r\n    x = tf.signal.frame(x,args.num_frames,1, axis=0)\r\n    x = layers.Flatten()(x)\r\n##############################\r\n    x = layers.Dense(16, activation='relu')(x)\r\n    x = layers.Dense(1)(x)\r\n    counts = tf.keras.activations.softplus(x)\r\n    # x = layers.Dropout(0.5)(x)\r\n    # outputs = layers.Dense(10, activation='softmax')(x)\r\n\r\n    model = tf.keras.Model(inputs, counts, name='toy_resnet')\r\n```\r\nNow if we do this....\r\n```\r\n            with tf.GradientTape() as tape:\r\n                loss = tf.math.squared_difference(tf.reduce_sum(model(x_mb)), y_mb)\r\n\r\n            grads = tape.gradient(loss, model.trainable_variables)\r\n```\r\nwe get the following error at \"grads = ...\"\r\nError = AssertionError: Expected all args to be Tensors or Variables; but got CompositeTensor: [<tensorflow.python.framework.indexed_slices.IndexedSlices object\r\n", "comments": ["@johnpjust \r\n\r\nLooks like code is incomplete. Request you to provide complete code snippet or colab link to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@ravikyram  Thanks -- after more examination I found it's not an interaction with the tape.gradient, but isolated to VRAM (GPU) memory leaks in two separate functions in TF (timedistributedlayer & tf.signal.frame).  I'm closing this and focusing on a separate thread.  Thanks."]}, {"number": 34777, "title": "Build issue \"None of the libraries match their SONAME\" (libcudnn)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nSlackware Linux 64 -current (14.2+) 2019-12-02\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version:\r\nHEAD 882a1c8ed7\r\n- Python version:\r\nPython 3.7.5\r\n- Installed using virtualenv? pip? conda?:\r\nDirectly from OS\r\n- Bazel version (if compiling from source):\r\nBinary download bazel 1.1.0\r\n- GCC/Compiler version (if compiling from source):\r\ngcc (GCC) 8.3.1 20191031\r\n- CUDA/cuDNN version:\r\ncuda_10.2.89_440.33.01_linux.run\r\ncudnn-10.2-linux-x64-v7.6.5.32.tgz\r\nnccl_2.5.6-2+cuda10.2_x86_64.txz\r\n- GPU model and memory:\r\nMSI Graphic Cards GT 1030 2G LP OC\r\n\r\n**Describe the problem**\r\nTrying to build TF shows error `Cuda Configuration Error: None of the libraries match their SONAME: /opt/nvidia/cuda/lib64/libcudnn.so.7`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n- Installed nvidia kernel modules and driver 440.31\r\n- Installed cuda, cudnn, nccl, tensorrt to `/opt/nvidia/cuda  /opt/nvidia/nccl  /opt/nvidia/tensorrt`\r\n- Installed bazel binary as `/usr/bin/bazel`\r\n- Downloaded latest TF source from GitHub\r\n- Used TF's `./configure` command to config the build\r\n- Ran build `bazel build //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n[.tf_configure.bazelrc](https://github.com/tensorflow/tensorflow/files/3913908/tf_configure.bazelrc.txt)\r\n[tf-build-err.txt](https://github.com/tensorflow/tensorflow/files/3913909/tf-build-err.txt)\r\n", "comments": ["@edrozenberg,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "@amahendrakar thanks, closing. No such problems building `2.4.1` recently.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34777\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34777\">No</a>\n"]}, {"number": 34776, "title": "StackedRNNCells has an invalid example", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells\r\n\r\n## Description of issue (what needs changing):\r\n\r\nDocumentation example does not actually use `StackedRNNCells`. There is no example for the class being documented. Ideally there would be both an example of the class and an example showing how the same behaviour would be implemented without the class.\r\n", "comments": ["Resolved internally."]}, {"number": 34775, "title": "tf.keras.models.load_model from saved model not loading as keras model?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0rc0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.1/7.6.4\r\n- GPU model and memory: RTX Titan\r\n2.1.0-rc0\r\nv1.12.1-17734-gc6daad3\r\n\r\n**Describe the current behavior**\r\nThe documents from [official-tensorflow-website](https://www.tensorflow.org/tutorials/keras/save_and_load) written as when using ```tf.keras.models.load_model```  on a saved model format folder will be loaded as Keras model.\r\n\r\nMy model was trained on 2.0.0b0\r\n\r\n**Describe the expected behavior**\r\nHowever, when I loaded the model, it loaded as saved_model.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\n```\r\n>>> new_model = tf.keras.models.load_model('saved_model/01')\r\n2019-12-03 08:13:44.584942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-03 08:13:44.681183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:17:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2019-12-03 08:13:44.683339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties:\r\npciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2019-12-03 08:13:44.683357: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-03 08:13:44.683380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-03 08:13:44.684694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-03 08:13:44.684963: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-03 08:13:44.686456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-03 08:13:44.687384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-03 08:13:44.687422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-03 08:13:44.695355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n2019-12-03 08:13:44.695566: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-12-03 08:13:44.727235: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\r\n2019-12-03 08:13:44.730301: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6317a70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-12-03 08:13:44.730343: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-12-03 08:13:45.044582: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x637d2f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-12-03 08:13:45.044634: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5\r\n2019-12-03 08:13:45.044649: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): TITAN RTX, Compute Capability 7.5\r\n2019-12-03 08:13:45.047700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:17:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2019-12-03 08:13:45.050560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties:\r\npciBusID: 0000:65:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.62GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2019-12-03 08:13:45.050603: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-03 08:13:45.050625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-03 08:13:45.050653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-03 08:13:45.050680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-03 08:13:45.050712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-03 08:13:45.050746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-03 08:13:45.050765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-03 08:13:45.061373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n2019-12-03 08:13:46.263485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-03 08:13:46.263516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1\r\n2019-12-03 08:13:46.263521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N Y\r\n2019-12-03 08:13:46.263524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   Y N\r\n2019-12-03 08:13:46.266356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22629 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:17:00.0, compute capability: 7.5)\r\n2019-12-03 08:13:46.267761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 22629 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:65:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:From /misc/home/usr16/cheesiang_leow/.virtualenvs/tensorflow-2.1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n>>> new_model\r\n<tensorflow.python.training.tracking.tracking.AutoTrackable object at 0x7f9c3775b198>\r\n>>> new_model.summary()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'AutoTrackable' object has no attribute 'summary'\r\n```\r\n\r\nIs this because my model uses the Functional API ? The model API from ", "comments": ["@luvwinnie could you please provide a reproducible code snippet preferably in colab format", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@luvwinnie \r\n\r\nAny update on the issue please. Thanks!", "Sorry for late reply. I have created a reproducible procedure.\r\n\r\nWhat I have done is, I used tensorflow 2.0.0 beta0 to create my model, then the released of tensorflow 2.1.0rc0 , I upgrade my tensorflow. However when I try to load the model it shows the above errors. This link below is a Colab.\r\n\r\nWhat I do is, I use tensorflow beta0 create a model, export the model to saved model with tf.keras.experimental.export_saved_model function, which mostly stable during beta version. Then i Installed tensorflow2.1.0rc0 restart the runtime, straighth load the model.\r\n\r\nhttps://drive.google.com/open?id=1tJebhW0arF7oXYNIDFRjlrlRR91_3K9B", "@luvwinnie In tensorflow 2.1rco `tensorflow.keras.experimental.export_saved_model` has been depricated so I think this might be the reason you are facing an error. Thanks!", "@luvwinnie Did the above comment help you in solving this issue. Thanks!", "@gowthamkpr Thank you for replying! Yes this has solved my issues. If there is any problem i will make a new comment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34775\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34775\">No</a>\n", "hi @luvwinnie did you manage to load the model without the 'AutoTrackable' object has no attribute 'summary' error? if so, please show your solution, thanks!\r\n", "Hey, was this issue reolved ? I am currently facing the same issue with the AutoTrackable object\r\n", "Same issue", "Same issue...", "Same issue", "Same issue"]}, {"number": 34774, "title": "DenseToDenseSetOperation: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): conda -c anaconda\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: cuda10.1/cudnn7.6.4\r\n- GPU model and memory: Quadro P5000\r\n\r\n**Describe the current behavior**\r\ntf.data.Dataset throws a error since I pass to tf114 from tf113. My following code can reproduce this error. \r\n\r\n**Describe the expected behavior**\r\nI explain my code here. The Dataset takes a list of .h5 file names in string then shuffle this list.  A wrapper written with tf.py_func() will load the .h5 file (I know it's deprecated in TF2, but since it works well in my previous code). It's also my question. Is this bug exists only in TF1 or it does in TF2? Should I move to TF2?\r\n\r\n**Code to reproduce the issue**\r\nI wrote a small Seg-Net network to to reproduce the same bug. Everything goes well until I change the output and input of the NN by tf.data.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nfrom tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\r\nfrom tensorflow.python.framework import dtypes\r\nimport os\r\n\r\ndef up_2by2_ind(input_layer, ind, name=''):\r\n    with tf.name_scope(name):\r\n        in_shape = input_layer.get_shape().as_list()\r\n        out_shape = [tf.cast(tf.shape(input_layer), dtype=tf.int64)[0], in_shape[1] * 2, in_shape[2] * 2, in_shape[3]]\r\n\r\n        # prepare\r\n        _pool = tf.reshape(input_layer, [-1])\r\n        _range = tf.reshape(tf.range(out_shape[0], dtype=ind.dtype), [out_shape[0], 1, 1, 1])\r\n        tmp = tf.ones_like(ind) * _range\r\n        tmp = tf.reshape(tmp, [-1, 1])\r\n        _ind = tf.reshape(ind, [-1, 1])\r\n        _ind = tf.concat([tmp, _ind], 1)\r\n\r\n        # scatter\r\n        unpool = tf.scatter_nd(_ind, _pool, [out_shape[0], out_shape[1] * out_shape[2] * out_shape[3]])\r\n\r\n        # reshape\r\n        unpool = tf.reshape(unpool, out_shape)\r\n        return unpool\r\n\r\n###################### input pipeline\r\ndef wrapper(a, b):\r\n    return tf.py_func(\r\n        wrawrapper,\r\n        [a, b],\r\n        [tf.float32, tf.float32],\r\n    )\r\n\r\ndef wrawrapper(a, b):\r\n    return np.ones((1, 50, 50, 1), dtype=np.float32), np.ones((1, 50, 50, 1), dtype=np.float32)\r\n\r\na_ph = tf.placeholder(tf.string, shape=[None], name='a_ph')\r\nb_ph = tf.placeholder(tf.int32, shape=[None], name='b_ph')\r\n\r\nbatch = tf.data.Dataset.from_tensor_slices((a_ph, b_ph))\r\nbatch = batch.shuffle(tf.cast(tf.shape(a_ph)[0], tf.int64))\r\nbatch = batch.map(wrapper).prefetch(10).repeat()\r\nit = tf.data.Iterator.from_structure(batch.output_types, batch.output_shapes)\r\niter_init_op = it.make_initializer(batch, name='iter_init_op')\r\nX_it, y_it = it.get_next()\r\n\r\ndropout = tf.placeholder(tf.float32, [], name='dropout')\r\nBN_phase = tf.placeholder(tf.bool, [], name='BN_phase')\r\nsave_summary_step = 20\r\nsave_model_step = 100\r\n\r\n##################### train graph on gpu1\r\nwith tf.device('/device:GPU:0'):\r\n    with tf.name_scope('model'):\r\n        with tf.name_scope('conv'):\r\n            with tf.variable_scope('conv', reuse=False):\r\n                w1 = tf.get_variable('w', shape=[3, 3, 1, 1], initializer=tf.initializers.glorot_normal())\r\n            out1 = tf.nn.conv2d(X_it, w1, strides=[1, 1, 1, 1], padding='SAME', name='conv')\r\n            with tf.variable_scope('conv', reuse=False):\r\n                out1 = tf.layers.batch_normalization(out1, training=BN_phase, name='batch_norm')\r\n            out1 = tf.nn.relu(out1, 'relu')\r\n            out1, ind1 = tf.nn.max_pool_with_argmax(out1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='maxpool')\r\n        with tf.name_scope('dnn'):\r\n            flat = tf.reshape(out1, [1, 625])\r\n            with tf.variable_scope('dnn2', reuse=False):\r\n                w2 = tf.get_variable('w2', shape=[625, 625], initializer=tf.initializers.glorot_normal())\r\n            dnn_out = tf.matmul(flat, w2)\r\n            dnn_out = tf.nn.dropout(dnn_out, keep_prob=dropout, name='do')\r\n            dnn_out = tf.nn.relu(dnn_out, name='relu')\r\n            dnn_out = tf.reshape(dnn_out, shape=[1, 25, 25, 1], name='dnn')\r\n\r\n        with tf.name_scope('deconv'):\r\n            out1 = up_2by2_ind(dnn_out, ind1, 'up1')\r\n            with tf.variable_scope('deconv', reuse=False):\r\n                w3 = tf.get_variable('w', shape=[3, 3, 1, 1], initializer=tf.initializers.glorot_normal())\r\n            out1 = tf.nn.conv2d(out1, w3, strides=[1, 1, 1, 1], padding='SAME', name='deconv')\r\n            with tf.variable_scope('deconv', reuse=False):\r\n                out1 = tf.layers.batch_normalization(out1, training=BN_phase, name='batch_norm')\r\n            logits = tf.nn.relu(out1, 'logits')\r\n\r\n        #todo: here with tabulation with tf.name_scope('operation'):\r\n    with tf.name_scope('operation'):\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        with tf.control_dependencies(update_ops):\r\n            mse = tf.losses.mean_squared_error(labels=y_it, predictions=logits)\r\n        opt = tf.train.AdamOptimizer(learning_rate=0.0001, name='Adam')\r\n\r\n        grads = opt.compute_gradients(mse)\r\n        train_op = opt.apply_gradients(grads, name='apply_grad')\r\n\r\nwith tf.name_scope('train_metrics'):\r\n    acc_val_op, acc_update_op = tf.metrics.accuracy(labels=y_it, predictions=logits)\r\n    summ_acc = tf.summary.merge([tf.summary.scalar('accuracy', acc_val_op)])\r\n    grad_sum = tf.summary.merge([tf.summary.histogram('{}/grad'.format(g[1].name), g[0]) for g in grads])\r\n\r\nwith tf.name_scope('train_summary'):\r\n    merged = tf.summary.merge([summ_acc, grad_sum, tf.summary.histogram(\"weights\", w1)])\r\n\r\n###################### test graph on gpu2\r\nwith tf.device('/device:GPU:1'):\r\n    with tf.name_scope('model'):\r\n        with tf.name_scope('conv'):\r\n            with tf.variable_scope('conv', reuse=True):\r\n                w1 = tf.get_variable('w', shape=[3, 3, 1, 1], initializer=tf.initializers.glorot_normal())\r\n            out1 = tf.nn.conv2d(X_it, w1, strides=[1, 1, 1, 1], padding='SAME', name='conv')\r\n            with tf.variable_scope('conv', reuse=True):\r\n                out1 = tf.layers.batch_normalization(out1, training=BN_phase, name='batch_norm')\r\n            out1 = tf.nn.relu(out1, 'relu')\r\n            out1, ind1 = tf.nn.max_pool_with_argmax(out1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='maxpool')\r\n        with tf.name_scope('dnn'):\r\n            flat = tf.reshape(out1, [1, 625])\r\n            with tf.variable_scope('dnn2', reuse=True):\r\n                w2 = tf.get_variable('w2', shape=[625, 625], initializer=tf.initializers.glorot_normal())\r\n            dnn_out = tf.matmul(flat, w2)\r\n            dnn_out = tf.nn.dropout(dnn_out, keep_prob=dropout, name='do')\r\n            dnn_out = tf.nn.relu(dnn_out, name='relu')\r\n            dnn_out = tf.reshape(dnn_out, shape=[1, 25, 25, 1], name='dnn')\r\n\r\n        with tf.name_scope('deconv'):\r\n            out1 = up_2by2_ind(dnn_out, ind1, 'up1')\r\n            with tf.variable_scope('deconv', reuse=True):\r\n                w3 = tf.get_variable('w', shape=[3, 3, 1, 1], initializer=tf.initializers.glorot_normal())\r\n            out1 = tf.nn.conv2d(out1, w3, strides=[1, 1, 1, 1], padding='SAME', name='deconv')\r\n            with tf.variable_scope('conv', reuse=True):\r\n                out1 = tf.layers.batch_normalization(out1, training=BN_phase, name='batch_norm')\r\n            logits = tf.nn.relu(out1, 'logits')\r\n\r\n\r\nwith tf.name_scope('test_metrics'):\r\n    acc_val_op2, acc_update_op2 = tf.metrics.accuracy(labels=y_it, predictions=logits)\r\n    summ_acc2 = tf.summary.merge([tf.summary.scalar('accuracy', acc_val_op2)])\r\n\r\nwith tf.name_scope('test_summary'):\r\n    merged2 = tf.summary.merge([summ_acc2, tf.summary.histogram(\"weights\", w1), tf.summary.histogram(\"weights\", w2)])\r\n\r\n\r\n##############################################\r\nwith tf.Session() as sess:\r\n    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer(), iter_init_op],\r\n             feed_dict={a_ph: ['a'], b_ph: [10]})\r\n    model_saver = tf.train.Saver(max_to_keep=100000)\r\n    train_writer = tf.summary.FileWriter('./dummy/gpus/train/', sess.graph)\r\n    test_writer = tf.summary.FileWriter('./dummy/gpus/test/', sess.graph)\r\n    for i in tqdm(range(1000)):\r\n        if i % save_summary_step == 0:\r\n            _, rlt, summary, _ = sess.run([train_op, out1, merged, acc_update_op], feed_dict={\r\n                dropout: 1,\r\n                BN_phase: True,\r\n            }\r\n                                          )\r\n            # print('train:', rlt)\r\n            train_writer.add_summary(summary, global_step=i)\r\n        else:\r\n            _, rlt = sess.run([train_op, out1], feed_dict={dropout: 1, BN_phase: True})\r\n            # print('train:', rlt)\r\n        if i % save_model_step == 0:\r\n            model_saver.save(sess, './dummy/ckpt/step{}'.format(i))\r\n            if i != 0:\r\n                for j in tqdm(range(5)):\r\n                    rlt, summary = sess.run([logits, merged2], feed_dict={\r\n                        dropout: 1,\r\n                        BN_phase: False,\r\n                    }\r\n                                            )\r\n                    # print('test:', rlt)\r\n                    test_writer.add_summary(summary, global_step=j)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1339, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1374, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]\r\nDenseToDenseSetOperation: CPU \r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation (DenseToDenseSetOperation) /device:GPU:0\r\n\r\nOp: DenseToDenseSetOperation\r\nNode attrs: T=DT_INT32, validate_indices=true, set_operation=\"a-b\"\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_STRING]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_INT8]\r\n\r\n\t [[{{node operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/media/snippet.py\", line 1930, in <module>\r\n    feed_dict={a_ph: ['a'], b_ph: [10]})\r\n  File \"/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation: Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]\r\nDenseToDenseSetOperation: CPU \r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation (DenseToDenseSetOperation) /device:GPU:0\r\n\r\nOp: DenseToDenseSetOperation\r\nNode attrs: T=DT_INT32, validate_indices=true, set_operation=\"a-b\"\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_STRING]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_INT8]\r\n\r\n\t [[node operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation (defined at media/snippet.py:1875) ]]\r\n\r\nOriginal stack trace for 'operation/mean_squared_error/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation':\r\n  File \"media/snippet.py\", line 1875, in <module>\r\n    mse = tf.losses.mean_squared_error(labels=y_it, predictions=logits)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py\", line 646, in mean_squared_error\r\n    losses, weights, scope, loss_collection, reduction=reduction)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py\", line 181, in compute_weighted_loss\r\n    weights_broadcast_ops.assert_broadcastable(weights, losses),)):\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 132, in assert_broadcastable\r\n    name=\"is_valid_shape\")\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1988, in cond\r\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1814, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 131, in <lambda>\r\n    weights_rank, weights_shape, values_rank, values_shape),\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 57, in _has_valid_nonscalar_shape\r\n    name=scope)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1977, in cond\r\n    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1814, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 55, in <lambda>\r\n    lambda: _has_valid_dims(weights_shape, values_shape),\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 40, in _has_valid_dims\r\n    invalid_dims = sets.set_difference(weights_shape_2d, valid_dims)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/sets_impl.py\", line 273, in set_difference\r\n    return _set_operation(a, b, \"a-b\" if aminusb else \"b-a\", validate_indices)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/sets_impl.py\", line 132, in _set_operation\r\n    a, b, set_operation, validate_indices)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/ops/gen_set_ops.py\", line 98, in dense_to_dense_set_operation\r\n    name=name)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"home/anaconda3/envs/tf114/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n```", "comments": ["Resolved the issue, it's not a bug. I will close the issue. One shouldn't put `tf.losses.MSE(); tf.summary.merge()` inside GPU `tf.device('/device:GPU:X')` since means, update ops, or variables are in the RAM and operated by CPU. In contrast, gradient, loss and model are in GPU."]}, {"number": 34773, "title": "tf.math.tanh produces values strictly smaller than -1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab default environment\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 2.4.6\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.enable_v2_behavior()\r\n\r\nx = np.float(-8.51089)\r\ntf.math.tanh(x)\r\n=> <tf.Tensor: shape=(), dtype=float32, numpy=-1.0000001>\r\n```\r\nNote the result is less than -1.0.\r\n\r\n**Describe the expected behavior**\r\n\r\nExpected tanh to stay within the interval [-1.0, 1.0].\r\n", "comments": ["@axch \r\n\r\nI tried in colab with TF 1.15 and i am seeing output (`<tf.Tensor: id=1, shape=(), dtype=float32, numpy=-1.0>`). I tried with TF 2.0, 2.1.0-dev20191202 by commenting (`tf.enable_v2_behavior()`)\r\ni am seeing output(<`tf.Tensor: shape=(), dtype=float32, numpy=-1.0000001>`)Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/76dbd5a25368435a7beed6ac76152b04/untitled437.ipynb) Is this the expected behavior?. Thanks!", "Interesting.  I had tried it in the internal colab and got -1.0000001 in TF 1.15, but in any case, getting an answer strictly less than -1.0 isn't great in any TF version.", "@axch  I see different results than yours. TF 1.15 gives me ```-1.0``` [GitHub gist](https://colab.sandbox.google.com/gist/ymodak/3b1c3949119b9c7bc06d4a0fce8d370b/untitled16.ipynb)\r\nTF 1.15 with GPU ```<tf.Tensor: id=1, shape=(), dtype=float32, numpy=-0.99999994>```\r\n\r\nAnd TF 2.0 outputs ```-1.0```as well [GitHub gist](https://colab.sandbox.google.com/gist/ymodak/e03d2ed7b2bcbd7bdefd2b524b7b29b1/untitled18.ipynb)\r\nTF 2.0 with GPU ```<tf.Tensor: id=1, shape=(), dtype=float32, numpy=-0.99999994>```\r\nCan you please confirm? Thanks!", "I am able to reproduce the issue with TensorFlow 2.1.0-dev20191203 at https://colab.research.google.com/drive/1sGTDyv5K6fNEmX_-j9L89Cb-kUeUi-3F?usp=sharing :\r\n```\r\n!pip install tf-nightly\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n# ==> 2.1.0-dev20191203\r\n\r\nx = np.float32(-8.51089)\r\ntf.math.tanh(x)\r\n# ==> <tf.Tensor: shape=(), dtype=float32, numpy=-1.0000001>\r\n```\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This is fixed in tf-nightly version tested with '2.1.0-dev20191226'", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34773\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34773\">No</a>\n"]}, {"number": 34772, "title": "tf.pad is limited to six dimensions", "body": "```\r\n> A = tf.zeros([1] * 7)\r\n> tf.pad(A, [[0, 0]] * 6 + [[0, 1]])\r\n...\r\nUnimplementedError: inputs rank not in [0,6]: 7 [Op:Pad]\r\n> tf.pad(A[0], [[0, 0]] * 5 + [[0, 1]])\r\n<tf.Tensor: id=50, shape=(1, 1, 1, 1, 1, 2), dtype=float32, numpy=array([[[[[[0., 0.]]]]]], dtype=float32)>\r\n```", "comments": ["@terhorst \r\nI am not seeing any issue with recent nightly version (`!pip install tf-nightly==2.1.0dev20191202`). Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/5d638694576b69a824eff8b0ba308469/untitled436.ipynb) Thanks!", "Alright, thanks!\n\nOn Tue, Dec 3, 2019 at 5:39 AM ravikyram <notifications@github.com> wrote:\n\n> @terhorst <https://github.com/terhorst>\n> I am not seeing any issue with recent nightly version (!pip install\n> tf-nightly==2.1.0dev20191202). Please, find the gist here.\n> <https://colab.sandbox.google.com/gist/ravikyram/5d638694576b69a824eff8b0ba308469/untitled436.ipynb>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34772?email_source=notifications&email_token=AAAEOHBLTCHEUC23VG7ISS3QWYZPDA5CNFSM4JUJJKNKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEFY5K4Q#issuecomment-561108338>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAAEOHGNOHJDW4QKAFN3KWTQWYZPDANCNFSM4JUJJKNA>\n> .\n>\n-- \nJonathan\nterhorst@gmail.com\n", "@terhorst \r\nCan i close this thread as issue got resolved?. Thanks!", "Yep. Thanks."]}, {"number": 34771, "title": "tf.keras uses Eager execution or Graph execution in tf 2.0 ?", "body": "Hello, i'm doing a research work and i'd be glad to know if in tf 2.0, tensorflow.keras uses eager execution vs graph execution instead.", "comments": ["@LuchoTangorra ,\r\nHi, by default it is Eager execution in Tensorflow 2.0,irrespective of usage of tf.keras or not. \r\nKindly refer this [link](https://www.tensorflow.org/guide/effective_tf2#eager_execution).Thanks!", "Thank you!\r\nWhy is this ? I think that if you are using a Keras model, you don't really need to debug, but to speed up your project, and Eager executions do the opposite. Am i wrong? \r\n\r\nI don't understand the the reason of Eager execution using Keras.\r\n\r\nThank you in advance!", "@LuchoTangorra Eager execution is by default in TF2.0. This is more intuitive and useful to starters as well as experts to see what a variable holds at any time (more like pythonic). Once you checks everything running without a bug, then you can add @tf.function to run time intensive functions in graph mode. Some of the functions like model.fit runs in graph mode. There are several ways to speed up your TF2.0 and you don't lose the performance. \r\n\r\nPlease checks the [guides and tutorials](https://www.tensorflow.org/guide) on TF website and also take  a look at the tensorflow [channel on Youtube](https://www.youtube.com/watch?v=5ECD8J3dvDQ).\r\n\r\nHaving said that, This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "I believe this thread is incorrect, and tf.keras actually uses graph execution inside of layers. See the issue [here ](https://github.com/tensorflow/tensorflow/issues/36715) where we discuss this.\r\n\r\n Can anyone confirm?"]}, {"number": 34770, "title": "[ROCm] eigen patch needed for HIP header changes", "body": "This patch is needed to fix the ROCm TensorFlow build for changes currently in HIP master.", "comments": ["@jeffdaily can you please check build failures ?", "@rthadur Eigen moved its repo from bitbucket to gitlab.  All references to fetching from bitbucket will likely need to be updated.\r\n\r\nhttp://eigen.tuxfamily.org/index.php?title=News:Migration_to_GitLab.com_scheduled_on_the_December_4th", "@jeffdaily will this require a new PR ?", "@whchung could you restart CI for this PR?  I think the eigen fetch is now working correctly upstream."]}, {"number": 34769, "title": "[ROCm] eigen patch needed for HIP header changes in r1.15 branch", "body": "This patch is needed to fix the ROCm TensorFlow build for changes currently in HIP master.  Since this is the last V1 release branch, it is important to have this feature here.  This PR depends on #34532.", "comments": ["This patch is similar to #34770 but differs slightly due to the earlier eigen version used in r1.15 versus master.", "@jeffdaily can you please check the build failures ?", "Build failures are likely due to the dependent PR https://github.com/tensorflow/tensorflow/pull/34532 not yet being merged."]}, {"number": 34768, "title": "Error when saving a stacked LSTM model to .h5 file", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI have a simple and functional custom code to reproduce the issue.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n\r\n- TensorFlow installed from (source or binary):\r\nconda install tensorflow-gpu=2.0.0\r\n\r\n- TensorFlow version (use command below):\r\n\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n\r\nv2.0.0-rc2-26-g64c3d38 2.0.0\r\n\r\n- Python version:\r\nPython 3.7.4\r\n\r\n- CUDA/cuDNN version:\r\n CUDA Version: 10.1\r\n#define CUDNN_MAJOR 7\r\n\r\n- GPU model and memory:\r\nnvidia Quadro GV100 - 32478MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n\r\noutput `env_info.sh` attached. \r\n\r\n**Describe the current behavior**\r\n\r\nWhen saving a stacked LSTM model to .h5 file, the following error is encountered:\r\n\r\n`OSError: Unable to create link (name already exists)`\r\n\r\n**Describe the expected behavior**\r\n\r\nExpect to be able to save a stacked LSTM model to .h5 file with no error.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef Model_Functional_API():\r\n\r\n    inputs = tf.keras.Input(shape=(3, 2))\r\n    encoder_cell = [\r\n        tf.keras.layers.LSTMCell(10),\r\n        tf.keras.layers.LSTMCell(10),\r\n        tf.keras.layers.LSTMCell(10)\r\n    ]\r\n    encoder = tf.keras.layers.RNN(encoder_cell, return_sequences=True)\r\n    encoder_outputs = encoder(inputs)\r\n    projection_layer = tf.keras.layers.Dense(2)\r\n    preds = projection_layer(encoder_outputs)\r\n    model = tf.keras.Model(inputs,preds)\r\n\r\n    return model\r\n\r\nmodel = Model_Functional_API()\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n            loss='mean_squared_error')\r\n\r\ndata_x = np.random.random([64,3,2])\r\ndata_y = np.random.random([64,3,2])\r\n\r\nmodel.fit(data_x,data_y,batch_size=64,epochs=2)\r\n\r\nmodel.save('saved_model_h5/my_model.h5')\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n`tf_env.txt` and full error log are attached:\r\n[error_log.txt](https://github.com/tensorflow/tensorflow/files/3912957/error_log.txt)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3912958/tf_env.txt)\r\n\r\n", "comments": ["The issue I found here is caused by the default eager execution. tf.keras runs in eager execution model by default, so when each of the three LSTMCell is initialized, they are not aware of the existence of others so that all their weights are named exactly the same, which will then throw error in the model saving stage due to the naming conflicts. \r\nYou can check by printing out the weight names:\r\n```\r\nfor w in model.weights:\r\n    print(w.name)\r\n```\r\noutputs:\r\n```\r\nrnn/kernel:0\r\nrnn/recurrent_kernel:0\r\nrnn/bias:0\r\nrnn/kernel_1:0\r\nrnn/recurrent_kernel_1:0\r\nrnn/bias_1:0\r\nrnn/kernel_2:0\r\nrnn/recurrent_kernel_2:0\r\nrnn/bias_2:0\r\ndense/kernel:0\r\ndense/bias:0\r\n```\r\n\r\nSolution to this specific issue is to run tf.keras in graph mode instead of eager mode:\r\n```\r\nwith tf.Graph().as_default(): \r\n    \r\n    model = Model_Functional_API(seq_len)\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n                loss='mean_squared_error')\r\n\r\n    data_x = np.random.random([64,seq_len,2])\r\n    data_y = np.random.random([64,seq_len,2])\r\n\r\n    model.fit(data_x,data_y,batch_size=64,epochs=2)\r\n    model.save('saved_model_h5/my_model.h5')\r\n```\r\nThis way model will be saved successfully. In graph mode, there is no conflicts in weight names:\r\n```\r\nfor w in model.weights:\r\n    print(w.name)\r\n```\r\noutputs:\r\n```\r\nrnn/kernel:0\r\nrnn/recurrent_kernel:0\r\nrnn/bias:0\r\nrnn/kernel:0\r\nrnn/recurrent_kernel:0\r\nrnn/bias:0\r\nrnn/kernel:0\r\nrnn/recurrent_kernel:0\r\nrnn/bias:0\r\ndense/kernel:0\r\ndense/bias:0\r\n```\r\nI guess this particular issue is then solved thus I am closing it."]}, {"number": 34767, "title": "Error when retraining a saved LSTM model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI have a simple and functional custom code to reproduce the issue.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04\r\n\r\n- TensorFlow installed from (source or binary):\r\nconda install tensorflow-gpu=2.0.0\r\n\r\n- TensorFlow version (use command below):\r\n\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n\r\nv2.0.0-rc2-26-g64c3d38 2.0.0\r\n\r\n- Python version:\r\nPython 3.7.4\r\n\r\n- CUDA/cuDNN version:\r\n CUDA Version: 10.1\r\n#define CUDNN_MAJOR 7\r\n\r\n- GPU model and memory:\r\nnvidia Quadro GV100 - 32478MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n\r\noutput `env_info.sh` attached. \r\n\r\n**Describe the current behavior**\r\n\r\n1. Train a simple LSTM model using tf.keras API\r\n2. save the model to file as `SavedModel` format. `model.save('saved_model', save_format='tf')`\r\n3. In a separate script (without access to the model definition), load the saved model: `model = tf.keras.models.load_model('saved_model1')`\r\n4. continue training the reloaded model: `model.fit(data_x,data_y,batch_size=64,epochs=2)`\r\nThe following error encountered:\r\n\r\n`LookupError: No gradient defined for operation 'while' (op type: While)`\r\n\r\n**Describe the expected behavior**\r\n\r\nexpect the LSTM `SavedModel` can be loaded and retrained in a separated script without access to the model definition.\r\n\r\n**Code to reproduce the issue**\r\nScript to train and save the model:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef Model_Functional_API():\r\n\r\n    inputs = tf.keras.Input(shape=(3, 2))\r\n    encoder = tf.keras.layers.LSTM(10,return_sequences=True)\r\n    encoder_outputs = encoder(inputs)\r\n    projection_layer = tf.keras.layers.Dense(2)\r\n    preds = projection_layer(encoder_outputs)\r\n    model = tf.keras.Model(inputs,preds)\r\n\r\n    return model\r\n\r\ndef Model_Sequence():\r\n\r\n    model = tf.keras.Sequential()\r\n    model.add(tf.keras.layers.LSTM(10,return_sequences=True))\r\n    model.add(tf.keras.layers.Dense(2))\r\n\r\n    return model\r\n\r\n# model = Model_Functional_API()\r\nmodel = Model_Sequence()\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n            loss='mean_squared_error')\r\n\r\ndata_x = np.random.random([64,3,2])\r\ndata_y = np.random.random([64,3,2])\r\n\r\nmodel.fit(data_x,data_y,batch_size=64,epochs=2)\r\n\r\nmodel.save('saved_model', save_format='tf')\r\n# model.save('saved_model.h5')\r\n```\r\nScript to load and retrain the model (this is where the error encountered):\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nmodel = tf.keras.models.load_model('saved_model')\r\n# model = tf.keras.models.load_model('saved_model.h5')\r\n\r\ndata_x = np.random.random([64,3,2])\r\ndata_y = np.random.random([64,3,2])\r\n\r\nmodel.fit(data_x,data_y,batch_size=64,epochs=2)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n`tf_env.txt` and the ouput error message are attached.\r\n[error_log.txt](https://github.com/tensorflow/tensorflow/files/3912802/error_log.txt)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3912803/tf_env.txt)\r\n\r\n ", "comments": ["@wkelongws ,\r\nWhen tried running the code for `TF-gpu-2.0 `I did not face any error, kindly find the [link](https://colab.sandbox.google.com/gist/oanush/435a707fb7889d90926df9b79596f7f2/34767.ipynb) of colab gist.Thanks!", "@oanush \r\n\"In a separate script (without access to the model definition)\", load the saved model for retraining, then you would have the error.\r\nIn your colab link, after finishing the second cell, please restart runtime then try running JUST the third cell, that is where the aforementioned error occurred. \r\n\r\nI expect the LSTM SavedModel can be loaded and retrained in a separated script without access to the model definition. In your colab link, restart runtime is for simulating that case.", "@wkelongws ,\r\nThank you, was able to reproduce the error. I have updated the link of gist.", "Potentially duplicate of #34211", "Is there any update on this ??", "@wkelongws On GPU, I can still reproduce the issue with `tf-nightly-gpu`. However, with CPU, there is no issue. Please check the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/762bea845c3ce926e8dc93863f1b1b3e/untitled825.ipynb) with CPU.\r\nPlease check the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/20a72b25a5c16f98438d30b08351d1fe/34767.ipynb) with GPU.\r\n\r\nWith GPU, the error is different from the reported error. Please check below for full error trace with `tf-nightly-gpu`. Thanks!\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-9ba863f98283> in <module>()\r\n      2 import numpy as np\r\n      3 \r\n----> 4 model = tf.keras.models.load_model('saved_model')\r\n      5 # model = tf.keras.models.load_model('saved_model.h5')\r\n      6 \r\n\r\n11 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _set_input_attrs(self, inputs)\r\n   2479         input_shape = (None,)\r\n   2480       else:\r\n-> 2481         input_shape = (None,) + tuple(inputs.shape[1:])\r\n   2482       self._build_input_shape = input_shape\r\n   2483 \r\n\r\nAttributeError: 'list' object has no attribute 'shape' \r\n```", "@jvishnuvardhan  Thanks for the update. \r\nBy commenting/uncommenting the corresponding lines in the GPU gist that you provided above I got below findings:\r\n\r\nscenario 1: model = Model_Functional_API()  +  model.save('saved_model', save_format='tf')\r\nPASS\r\n\r\nscenario 2: model = Model_Sequence()  +  model.save('saved_model', save_format='tf')\r\nFAIL\r\n\r\nscenario 3: model = Model_Functional_API()  +  model.save('saved_model.h5')\r\nPASS\r\n\r\nscenario 4: model = Model_Sequence()  +  model.save('saved_model.h5')\r\nPASS\r\n\r\n(Note: In each scenario, restart runtime then run JUST the third cell. Again, this is for mimicking a situation where the inference script has no access to the model definition.)\r\n\r\nSo looks like the `tf-nightly-gpu` fixed the previous `No gradient defined for operation 'while'` error but still have some bugs remaining with aforementioned scenario 2.", "This issue still exists with `scenario 2` mentioned above. The trace with recently `tf-nightly` is as follows. This trace is for our reference. Thanks!\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in make_shape(v, arg_name)\r\n    210   try:\r\n--> 211     shape = tensor_shape.as_shape(v)\r\n    212   except TypeError as e:\r\n\r\n21 frames\r\nTypeError: Dimension value must be integer or None or have an __index__ method, got TensorShape([None, None, 2])\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in make_shape(v, arg_name)\r\n    211     shape = tensor_shape.as_shape(v)\r\n    212   except TypeError as e:\r\n--> 213     raise TypeError(\"Error converting %s to a TensorShape: %s.\" % (arg_name, e))\r\n    214   except ValueError as e:\r\n    215     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\r\n\r\nTypeError: Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got TensorShape([None, None, 2]).\r\n```", "@wkelongws Can you please check this issue. I think this was resolved for `scenario 2`. I was not able to reproduce the issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/20a72b25a5c16f98438d30b08351d1fe/34767.ipynb). Thanks!\r\n\r\nPlease close the issue if this was resolved for you. thanks! ", "@wkelongws Can you please confirm whether it is resolved for you. Thanks!", "Closing this issue as this was resolved in recently `tf-nightly`. Please feel free to reopen if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34767\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34767\">No</a>\n", "Hi, I am having the same issue while trying to load a model with multiple inputs.\r\n![model](https://user-images.githubusercontent.com/33988623/80512979-a415d400-897e-11ea-945f-0fcf4d4e7af4.png)\r\n1.  I save the whole model with a ModelCheckpoint callback\r\n```\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\r\n                                                  save_weights_only=False,\r\n                                                  verbose=0,\r\n                                                  save_freq=4542*10)\r\n```\r\n2. In a new colab session I try to load it with\r\n```\r\ncheckpoint_dir = os.path.dirname(checkpoint_path)\r\nmodel = tf.keras.models.load_model(checkpoint_dir)\r\n```\r\n\r\nThe trace: \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in make_shape(v, arg_name)\r\n    210   try:\r\n--> 211     shape = tensor_shape.as_shape(v)\r\n    212   except TypeError as e:\r\n\r\n22 frames\r\nTypeError: Dimension value must be integer or None or have an __index__ method, got TensorShape([None, 2048])\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in make_shape(v, arg_name)\r\n    211     shape = tensor_shape.as_shape(v)\r\n    212   except TypeError as e:\r\n--> 213     raise TypeError(\"Error converting %s to a TensorShape: %s.\" % (arg_name, e))\r\n    214   except ValueError as e:\r\n    215     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\r\n\r\nTypeError: Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got TensorShape([None, 2048]).\r\n```", "@pabloi09 Can you please open a new issue and provide a simple standalone code to reproduce the error. Thanks!", "Done ! https://github.com/tensorflow/tensorflow/issues/39013", "> @jvishnuvardhan Thanks for the update.\r\n> By commenting/uncommenting the corresponding lines in the GPU gist that you provided above I got below findings:\r\n> \r\n> scenario 1: model = Model_Functional_API() + model.save('saved_model', save_format='tf')\r\n> PASS\r\n> \r\n> scenario 2: model = Model_Sequence() + model.save('saved_model', save_format='tf')\r\n> FAIL\r\n> \r\n> scenario 3: model = Model_Functional_API() + model.save('saved_model.h5')\r\n> PASS\r\n> \r\n> scenario 4: model = Model_Sequence() + model.save('saved_model.h5')\r\n> PASS\r\n> \r\n> (Note: In each scenario, restart runtime then run JUST the third cell. Again, this is for mimicking a situation where the inference script has no access to the model definition.)\r\n> \r\n> So looks like the `tf-nightly-gpu` fixed the previous `No gradient defined for operation 'while'` error but still have some bugs remaining with aforementioned scenario 2.\r\n\r\nI installed \"tf-nightly-gpu\" and checked scenario 3: model = Model_Functional_API() + model.save('saved_model.h5').\r\nIt is really work, thanks!\r\n\r\nBut, fit after loading model became very slow.\r\nWhen i create model like \"modelC = Model(inputs=[if1,if2,if3,if4], outputs=out)\", fit from cold start was about 4-5 min, on next fit it was 3 min.\r\n\r\nFit on loaded model show ETA about 30 min.\r\nRecompiliing after load has no effect.", "I also had issue descibed by @pabloi09  (TypeError when trying to save/load Sequential model starting with concat layer, and taking list of tensors as input). But then I noticed new warning (it seems that this warning has appeared only in tf 2.2):\r\n`WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: ... Consider rewriting this model with the Functional API.`\r\nI have replaced Sequential model with Functional API Model and TypeError during model_load was solved."]}]