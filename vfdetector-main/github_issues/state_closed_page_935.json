[{"number": 25394, "title": "Enabling integer data types in autograd", "body": "Refer #25386\r\n\r\nAdds all of (u)int(8,16,32,64) dtypes to IsTrainable\r\n\r\n(reopened to point CLA to correct email address)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@jvmancuso  Please sign CLA in order to proceed with this PR. Thanks !", "Working on a corporate CLA, thanks!", "This is not safe to submit since we don't support integer gradients in the gradient functions, please see https://github.com/tensorflow/tensorflow/issues/25386#issuecomment-459847536."]}, {"number": 25393, "title": "\"reuse\" argument in tf.layers.Dense documentation should be renamed to \"_reuse\"", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/layers/Dense\r\n\r\n\r\n**Describe the documentation issue**\r\n`reuse` is no longer a valid argument, and should be replaced with `_reuse` instead. Passing reuse to Dense will cause the following error:\r\n\r\n```python\r\nTypeError: ('Keyword argument not understood:', 'reuse')\r\n```\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["@HanGuo97 Would you like creating a PR to fix the issue?", "@HanGuo97 and @facaiy is anyone of you working on this issue? I am trying to start contributing to Tensorflow and this looks like a good starting point. I can work on this if it has not been assigned yet.", "Welcome, feel free to take a try, @Sudeepam97 . Ping me if you need any help later :-)", "My bad for the delayed response, was offline for the last couple of days due to the lunar new year, I\u2019ll make a PR sometime today.", "Looks like @HanGuo97 has fixed this issue. I suppose it can be closed now. ", "Yes, thank @HanGuo97 for your contribution :-)\r\n\r\n@Sudeepam97 I think #25564 is also a good issue for starter if you're interested in it.", "@facaiy Thank you for pointing it out. I'll take a look at it."]}, {"number": 25392, "title": "Enabling integer data types in autograd", "body": "Refer #25386 \r\n\r\nAdds all of (u)int(8,16,32,64) dtypes to `IsTrainable`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 25391, "title": "tf.linalg.solve segfaults on invalid matrix dimensions", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6 (17G4015)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-rc2-3-ga6d8ffae09 1.12.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nCalling `tf.linalg.solve(A, b)` with `b.shape == (n,)` causes a segfault.\r\n\r\n**Describe the expected behavior**\r\n\r\nInvalid shape error expected. This seems to happen on the GPU version I've tested on another system, but crashes on the CPU version on my Mac.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n# This is ok\r\ntf.linalg.solve([[.1, .0], [.0, .1]], [[.1], [.1]])\r\n# This line causes segfault\r\ntf.linalg.solve([[.1, .0], [.0, .1]], [.1, .1])\r\n```\r\n\r\n**Other info / logs**\r\n\r\nSame thing happens on the latest nightly build, that is `b'v1.12.0-6726-g5522d670af' 1.13.0-dev20190126`", "comments": ["@darthdeus I see the following error in CPU\r\nInvalidArgumentError: All input tensors must have the same rank. [Op:MatrixSolve]\r\n\r\nOn TF GPU, I see the following error \r\nValueError: Shape must be at least rank 2 but is rank 1 for 'MatrixSolve_3' (op: 'MatrixSolve') with input shapes: [2,2], [2].\r\n\r\nI think in both the cases it is clearly describing that the error is due to shape or Rank. Thanks!\r\n\r\n", "Closing due to lack of recent activity and type of issue (not build/install and not bug/performance). Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I encounter the same issue on TF 1.13.1 on Ubuntu 18.04.2 LTS. Code to reproduce:\r\n\r\n> import tensorflow as tf\r\n> import numpy as np\r\n> tf.enable_eager_execution()\r\n> \r\n> N = 10\r\n> A = tf.Variable(np.random.normal(size=[N,N]))\r\n> b = tf.Variable(np.random.normal(size=[N]))\r\n> tf.linalg.solve(A, b)\r\n\r\nThis results in a segfault.\r\n\r\nReplacing the b assignment with \r\n> b = tf.Variable(np.random.normal(size=[N,1]))\r\n\r\nprevents the issue from occurring.\r\n\r\nEdit: perhaps I should mention I'm using python 3.6.7 via ipython 7.4.0\r\nEdit2: Remove unnecessary tfp import in code example.", "@jvishnuvardhan It's probably related to a specific version/OS, because in some cases you do get the shape error, but as @NathanWycoff just reporetd, I'm not the only one getting a segfault.", "@darthdeus I agree with you. This could be an issue specific to environment. I am not seeing this segfault error but seeing the following error on Mac with TF1.13.1\r\n\r\nValueError: Shape must be at least rank 2 but is rank 1 for 'MatrixSolve' (op: 'MatrixSolve') with input shapes: [10,10], [10].\r\n\r\nI get the following error with google colab with TF1.13.1\r\nInvalidArgumentError: Input and right-hand side must have same rank, got 2 != 1 [Op:MatrixSolve]\r\n\r\nWe will try to resolve the root cause. Thanks!", "This is still a problem on latest docker image for cpu on py3 at least:\r\n\r\n```\r\njloper@risks3:~$ docker run --rm -it -u 1000 tensorflow/tensorflow:latest-py3\r\n\r\n________                               _______________                \r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / \r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nYou are running this container as user with ID 1000 and group 0,\r\nwhich should map to the ID and group for your user on the Docker host. Great!\r\n\r\ntf-docker / > python\r\nPython 3.6.8 (default, Aug 20 2019, 17:12:48) \r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.0.0'\r\n>>> import numpy.random as npr\r\n>>> mtx = tf.convert_to_tensor(npr.randn(5,4,4))\r\n2019-11-04 19:14:49.503214: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-04 19:14:49.506655: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3392445000 Hz\r\n2019-11-04 19:14:49.506900: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44745b0 executing computations on platform Host. Devices:\r\n2019-11-04 19:14:49.506916: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n>>> vec = tf.convert_to_tensor(npr.randn(1,4,8))\r\n>>> tf.linalg.solve(mtx,vec)\r\nSegmentation fault (core dumped)\r\ntf-docker / > date\r\nMon Nov  4 19:15:23 UTC 2019\r\n```", "@darthdeus,\r\nI ran your code in `Google Colab` and in `Jupyter Notebook` in `Linux` Machine and I could see the error, \r\n\r\n> InvalidArgumentError: All input tensors must have the same rank. [Op:MatrixSolve] \r\n\r\nPlease find the [Gist of the Google Colab](https://colab.research.google.com/gist/rmothukuru/bb4173404c62ce4b3cf5784dc7afb1b4/gh_25391.ipynb). \r\n\r\nCan you please confirm if we can close this issue as the expected Error is being raised? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25391\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25391\">No</a>\n"]}, {"number": 25390, "title": "Failed in v1.13.0-rc0 docker file build", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\nJust used the tensorflow/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile in v1.13.0-rc0 tag.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nHere is the bug:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/a8e5c41c5bbe684a88b9285e07bd9838c089e83b/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile#L71\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nIf the condition is false, bash return 1. Thus, it breaks docker build image process.\r\n\r\nPLEASE PLEASE PLEASE find a guy who knows dockerfile and bash to write a build script. The quality of build process is absolutely unacceptable.\r\n\r\nI tried last stable release v1.12.0 and recent release candidate. They all failed.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Looks like you're right. Thanks! I've prepared an internal change that will resolve this, so I'm closing the issue.\r\n\r\nHowever, in future bug reports, please do not be so rude."]}, {"number": 25389, "title": "Add keras.utils.model_to_dot and print_summary", "body": "These functions appear on keras.io.\r\nFixes https://github.com/tensorflow/tensorflow/issues/24639 (which was not actually fixed by my previous PR, unfortunately it seems that I also needed to add some `@keras_export` decorators).\r\n", "comments": ["Hi there, I'm not sure why some checks are unsuccessful?", "@ageron can you please resolve conflicts ?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 25388, "title": "Added One TC for the custom ops", "body": "Added one error condition check for the allow_custom_ops set to False", "comments": ["@gargn can you pls review the same", "@gargn  can you pls review the PR and let me know your comments", "@gargn , thanks alot for your comments, i have updated the code as per your comments. Also the TC file is very well written and i just found a missing condition which was not captured in the TC, so added the same. Kindly approve.\r\n\r\nRegards\r\nAmit", "@gargn & @rthadur , I have updated the code as per your comments, kindly check, if everything is ok, kindly approve the PR.", "@rthadur , can you pls update the label to review requested.", "@gargn , thanks for approving the PR, can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "@rthadur , can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "@gargn thanks for approving the PR.\r\n@rthadur , can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "@rthadur , can you please help to get this merged.\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 25387, "title": "TF 2.0 Documents how BatchNormalization update is handle in TF 2.0", "body": "When training model that need specific update such as batch norm mean and variance, it is not clear \r\nfrom the current version of documentations how we can handle this. \r\n\r\nThis docs https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model suggests that we have to call `model.updates`\r\n\r\nWhile here https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/dcgan.ipynb there's nothing about `model.updates`\r\n\r\n**System information**\r\n- TensorFlow version: 2.0-preview\r\n", "comments": ["model.updates is not in TF2 now. Thanks"]}, {"number": 25385, "title": "TF 2.0: Summaries are not written to the disk", "body": "**Describe the current behavior**\r\n\r\nThe example from [tf2_overview](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/beginner/tf2_overview.ipynb) doesn't write the summaries onto the disk and nothing happen in the tensorboard.\r\n\r\nTensorflow installed with `pip install tf-nightly-2.0-preview`. Same thing happened with the GPU version.\r\n\r\nBelow is the code we use to reproduce this behaviour\r\n\r\n```\r\nimport os\r\nimport time\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.ops import summary_ops_v2\r\n\r\nlayers = tf.keras.layers\r\n\r\n\r\ndef create_model():\r\n    max_pool = layers.MaxPooling2D((2, 2), (2, 2), padding='same')\r\n    # The model consists of a sequential chain of layers, so tf.keras.Sequential\r\n    # (a subclass of tf.keras.Model) makes for a compact description.\r\n    return tf.keras.Sequential([\r\n        layers.Reshape(\r\n            target_shape=[28, 28, 1],\r\n            input_shape=(28, 28,)),\r\n        layers.Conv2D(2, 5, padding='same', activation=tf.nn.relu),\r\n        max_pool,\r\n        layers.Conv2D(4, 5, padding='same', activation=tf.nn.relu),\r\n        max_pool,\r\n        layers.Flatten(),\r\n        layers.Dense(32, activation=tf.nn.relu),\r\n        layers.Dropout(0.4),\r\n        layers.Dense(10)])\r\n\r\n\r\n# Define a loss function and accuracy function\r\ndef compute_loss(logits, labels):\r\n    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, logits))\r\n\r\n\r\ndef compute_accuracy(logits, labels):\r\n    return tf.keras.metrics.categorical_accuracy(labels, logits)\r\n\r\n\r\n# Set up datasets\r\ndef mnist_datasets():\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n    # Numpy defaults to dtype=float64; TF defaults to float32. Stick with float32.\r\n    x_train, x_test = x_train / np.float32(255), x_test / np.float32(255)\r\n    y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\r\n    return train_dataset, test_dataset\r\n\r\n\r\n@tf.function\r\ndef train_step(model, optimizer, images, labels):\r\n    # Record the operations used to compute the loss, so that the gradient\r\n    # of the loss with respect to the variables can be computed.\r\n    with tf.GradientTape() as tape:\r\n        logits = model(images, training=True)\r\n        loss = compute_loss(logits, labels)\r\n        accuracy = compute_accuracy(logits, labels)\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n    return loss, accuracy\r\n\r\n\r\ndef train(model, optimizer, dataset, log_freq=50):\r\n    \"\"\"Trains model on `dataset` using `optimizer`.\"\"\"\r\n    start = time.time()\r\n    # Metrics are stateful. They accumulate values and return a cumulative\r\n    # result when you call .result(). Clear accumulated values with .reset_states()\r\n    avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\r\n    avg_accuracy = tf.keras.metrics.Mean('accuracy', dtype=tf.float32)\r\n    # Datasets can be iterated over like any other Python iterable.\r\n    for images, labels in dataset:\r\n        loss, accuracy = train_step(model, optimizer, images, labels)\r\n        avg_loss(loss)\r\n        avg_accuracy(accuracy)\r\n        if tf.equal(optimizer.iterations % log_freq, 0):\r\n            summary_ops_v2.scalar('loss', avg_loss.result(), step=optimizer.iterations)\r\n            summary_ops_v2.scalar('accuracy', avg_accuracy.result(), step=optimizer.iterations)\r\n            avg_loss.reset_states()\r\n            avg_accuracy.reset_states()\r\n            rate = log_freq / (time.time() - start)\r\n            print('Step #%d\\tLoss: %.6f (%d steps/sec)' % (optimizer.iterations, loss, rate))\r\n            start = time.time()\r\n\r\n\r\ndef test(model, dataset, step_num):\r\n    \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\r\n    avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\r\n    avg_accuracy = tf.keras.metrics.Mean('accuracy', dtype=tf.float32)\r\n\r\n    for (images, labels) in dataset:\r\n        logits = model(images, training=False)\r\n        avg_loss(compute_loss(logits, labels))\r\n        avg_accuracy(compute_accuracy(logits, labels))\r\n    print('Model test set loss: {:0.4f} accuracy: {:0.2f}%'.format(\r\n        avg_loss.result(), avg_accuracy.result() * 100))\r\n    summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\r\n    summary_ops_v2.scalar('accuracy', avg_accuracy.result(), step=step_num)\r\n\r\n\r\nMODEL_DIR = '/tmp/mnist2'\r\n\r\nif __name__ == '__main__':\r\n    model = create_model()\r\n\r\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5)\r\n\r\n    train_ds, test_ds = mnist_datasets()\r\n    train_ds = train_ds.shuffle(60000).batch(100)\r\n    test_ds = test_ds.batch(100)\r\n\r\n    train_dir = os.path.join(MODEL_DIR, 'summaries', 'train')\r\n    test_dir = os.path.join(MODEL_DIR, 'summaries', 'eval')\r\n\r\n    train_summary_writer = summary_ops_v2.create_file_writer(\r\n        train_dir, flush_millis=10000)\r\n    test_summary_writer = summary_ops_v2.create_file_writer(\r\n        test_dir, flush_millis=10000, name='test')\r\n\r\n    for epoch in range(10):\r\n        start = time.time()\r\n        with train_summary_writer.as_default():\r\n            train(model, optimizer, train_ds)\r\n        end = time.time()\r\n        print('\\nTrain time for epoch #{} ({} total steps): {}'.format(epoch + 1, optimizer.iterations, end - start))\r\n        with test_summary_writer.as_default():\r\n            test(model, test_ds, optimizer.iterations)\r\n```\r\n", "comments": ["Interesting enough, that I use model subclassing, that allows pretty similar flexibility and there tensorboard shows data.\r\n\r\nIn this example I've got similar result as OP\r\n```\r\ntotal 12\r\ndrwxr-xr-x 2 root root 4096 Jan 31 23:34 .\r\ndrwxr-xr-x 4 root root 4096 Jan 31 23:34 ..\r\n-rw-r--r-- 1 root root   40 Jan 31 23:34 events.out.tfevents.1548977685.1c8286c5866d.v2\r\n```", "For now, can you try adding `always_record_summaries()` as in:\r\n\r\n```python\r\nwith train_summary_writer.as_default(), summary_ops_v2.always_record_summaries():\r\n    train(model, optimizer, train_ds)\r\n...\r\nwith test_summary_writer.as_default(),  summary_ops_v2.always_record_summaries():\r\n    test(model, test_ds, optimizer.iterations)\r\n```\r\n\r\nThis shouldn't be necessary once the TF 2.0 summary API is slightly further along.", "Closing this since the original tf2_overview.ipynb has been removed.  In the new code samples, we should show the new TF 2.0 summary API which doesn't require any use of `always_record_summaries()` to work."]}, {"number": 25384, "title": "[Intel MKL] Fuse filter + random_uniform < rate into sampling dataset", "body": "This PR optimize two forms of filter with random sampling predicate:\r\nForm1:\r\n`dataset.filter(lambda x: tf.less(tf.random_uniform([1]), rate)[0])`\r\nForm2:\r\n`dataset.filter(lambda _: tf.random_uniform([]) < rate)`\r\n\r\nIn these two forms, the predicate function is called to decide whether the next data should be included in the output dataset.   When rate is low and there is a large data buffer, for example, followed by a `shuffle` with large buffer size, it takes a long while to call the predicate function.  For example, to fill a shuffle buffer size=1000 with 0.01 sampling rate, the predicate needs to be called 100,000 times on average.\r\n\r\nThis optimization fuse filter and the random_uniform < rate predicate of these two forms into a sampling dataset.  In the sampling dataset, a random number generator is checked quickly to decide whether current data should be added to the sample.  This makes sampling a dataset much faster.\r\n\r\nThis PR address the issue in the following link, @jsimsa suggested the method used in this PR as a solution to the low performance of shuffle buffer filling with filter+random_uniform combination.\r\nhttps://github.com/tensorflow/tensorflow/issues/23179", "comments": ["> Thank you for the contribution. I did an initial pass and left some comments throughout the PR.\r\n> \r\n> I also have two high-level comments:\r\n> \r\n> 1. You will need to provide a mechanism for enabling this optimization. Take a look at how other tf.data optimizations are surfaced through https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/optimization_options.py.\r\n> 2. You will need to add Python tests that test that the optimization is in fact taking place and test the end-to-end functionality of your SamplingDatasetOp kernel.\r\n\r\nHi, thanks for a comments, I'm working on a patch regarding on your comments.  There is one question.  The optimization itself is supposed to be transparent on Python level.   How to check an optimization is actually taking place in Python?  I see there is a C++ counter record how many times the optimization is taken, but that is not exposed on Python level.  If you can point me to an example I could follow the example to develop a Python test.", "> > Thank you for the contribution. I did an initial pass and left some comments throughout the PR.\r\n> > I also have two high-level comments:\r\n> > \r\n> > 1. You will need to provide a mechanism for enabling this optimization. Take a look at how other tf.data optimizations are surfaced through https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/optimization_options.py.\r\n> > 2. You will need to add Python tests that test that the optimization is in fact taking place and test the end-to-end functionality of your SamplingDatasetOp kernel.\r\n> \r\n> Hi, thanks for a comments, I'm working on a patch regarding on your comments. There is one question. The optimization itself is supposed to be transparent on Python level. How to check an optimization is actually taking place in Python? I see there is a C++ counter record how many times the optimization is taken, but that is not exposed on Python level. If you can point me to an example I could follow the example to develop a Python test.\r\n\r\nYou can use the `assert_next` transformation to check that the optimized pipeline contains `Sample` transformation similar to how https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/kernel_tests/optimization/map_and_batch_fusion_test.py checks whether the rewritten pipeline contains `MapAndBatch` transformation.", "> Thank you for the contribution. I did an initial pass and left some comments throughout the PR.\r\n> \r\n> I also have two high-level comments:\r\n> \r\n> 1. You will need to provide a mechanism for enabling this optimization. Take a look at how other tf.data optimizations are surfaced through https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/optimization_options.py.\r\n> 2. You will need to add Python tests that test that the optimization is in fact taking place and test the end-to-end functionality of your SamplingDatasetOp kernel.\r\n\r\n\r\n\r\n> Thank you for the contribution. I did an initial pass and left some comments throughout the PR.\r\n> \r\n> I also have two high-level comments:\r\n> \r\n> 1. You will need to provide a mechanism for enabling this optimization. Take a look at how other tf.data optimizations are surfaced through https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/optimization_options.py.\r\n> 2. You will need to add Python tests that test that the optimization is in fact taking place and test the end-to-end functionality of your SamplingDatasetOp kernel.\r\n\r\nA new test ad been added and enabling/disabling also supported following the suggestion.  See\r\nhttps://github.com/tensorflow/tensorflow/pull/25384/commits/fa11069c3e85ffbcae19994bdba09c30eee0a6de#diff-7ea11d87b21c32b24a525b31789b29b7R1", "Hi @jsimsa , the change request had been responded by https://github.com/tensorflow/tensorflow/pull/25384/commits/fa11069c3e85ffbcae19994bdba09c30eee0a6de, can you take a look?  Thanks!", "> @delock thank you for addressing my comments ... I have one last comment, otherwise this PR looks good.\r\n\r\n@jsimsa Thanks for your comments.  The last two comments had been responded.", "@delock please fix the failing presubmit tests", "> @delock please fix the failing presubmit tests\r\n\r\nSanity CI test failure had been responded in\r\nhttps://github.com/tensorflow/tensorflow/pull/25384/commits/1af9cd33396595b035b02cc9ae288cc4682a4ae9\r\n\r\nThe rest failure test is for API changes and is not marked as required.  I can reproduce them locally.  I think they are expected, because this PR change C++ API.  Let me know if I need to fix for this test.\r\n`bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True`", "yes, please run `bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True`", "> yes, please run `bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True`\r\n\r\napi compatibility test failure is responded by\r\nhttps://github.com/tensorflow/tensorflow/pull/25384/commits/fad8724feef4d0ad4fa023e5b40f023855067b84", "Internal tests fails with:\r\n\r\n```\r\nthird_party/tensorflow/core/grappler/optimizers/data/filter_with_random_uniform_fusion.cc:253:5: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n    graph.UpdateFanouts(filter_node.name(), fused_sampling->name());\r\n    ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nthird_party/tensorflow/core/grappler/optimizers/data/filter_with_random_uniform_fusion.cc:260:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n  graph.DeleteNodes(nodes_to_delete);\r\n  ^~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~\r\n```", "The internal test failure is responded with\r\nhttps://github.com/tensorflow/tensorflow/pull/25384/commits/443e7cf2e11fb757935b808ba116525765d4e86f\r\n\r\n> Internal tests fails with:\r\n> \r\n> ```\r\n> third_party/tensorflow/core/grappler/optimizers/data/filter_with_random_uniform_fusion.cc:253:5: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n>     graph.UpdateFanouts(filter_node.name(), fused_sampling->name());\r\n>     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> third_party/tensorflow/core/grappler/optimizers/data/filter_with_random_uniform_fusion.cc:260:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n>   graph.DeleteNodes(nodes_to_delete);\r\n>   ^~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~\r\n> ```\r\n\r\n", "Internal tests fail with:\r\n\r\n```\r\nmodule tensorflow/core/grappler/optimizers/data:filter_with_random_uniform_fusion_test does not depend on a module exporting 'tensorflow/core/framework/function_testlib.h'\r\n```", "The failure of missing dependence is responded by\r\nhttps://github.com/tensorflow/tensorflow/pull/25384/commits/a26198c1b1c9ced529fe6ffd68ee9192b351ec68\r\n> Internal tests fail with:\r\n> \r\n> ```\r\n> module tensorflow/core/grappler/optimizers/data:filter_with_random_uniform_fusion_test does not depend on a module exporting 'tensorflow/core/framework/function_testlib.h'\r\n> ```\r\n\r\n", "Internal tests fail with:\r\n\r\n```\r\ntensorflow/core/kernels/data/experimental/sampling_dataset_op.cc:136:28: error: calling function 'Random' requires holding mutex 'mu_' exclusively [-Werror,-Wthread-safety-analysis]\r\n          float rand_val = Random();\r\n```", "This internal failure is responded with \r\nhttps://github.com/tensorflow/tensorflow/pull/25384/commits/ced09392cb27d2ec4b1d85fb34ebeb01c9907725\r\n\r\n> Internal tests fail with:\r\n> \r\n> ```\r\n> tensorflow/core/kernels/data/experimental/sampling_dataset_op.cc:136:28: error: calling function 'Random' requires holding mutex 'mu_' exclusively [-Werror,-Wthread-safety-analysis]\r\n>           float rand_val = Random();\r\n> ```\r\n\r\n", "@delock can you please resolve conflicts", "Conflicts in test function had been resolved by\r\nhttps://github.com/tensorflow/tensorflow/pull/25384/commits/ce2746d3313b649257f9c6c6d7bc9e1896bbbc35\r\n> @delock can you please resolve conflicts\r\n\r\n", "api_compatibility generation rerun with \r\nhttps://github.com/tensorflow/tensorflow/pull/25384/commits/be63dabaec8c76a39c824a5f9cb3c0378e684dfe\r\n> Please re-run the api_compability generation, the respective test is failing internally.\r\n\r\n", "Internally, the test fails with:\r\n\r\ntensorflow/python/data/experimental/ops/optimization_options.py:104: [C0301(line-too-long), ] Line too long (85/80)\r\n\r\nEDIT: Looking at the PR it is not clear to me why though. @rthadur could you please re-run internal tests for the latest version of the PR? Thanks.", "> Internally, the test fails with:\r\n> \r\n> tensorflow/python/data/experimental/ops/optimization_options.py:104: [C0301(line-too-long), ] Line too long (85/80)\r\n> \r\n> EDIT: Looking at the PR it is not clear to me why though. @rthadur could you please re-run internal tests for the latest version of the PR? Thanks.\r\n\r\n@jsimsa there are 4 lint errors which were reported internally , please check internal CL for more information , @delock can you please fix Ubuntu Sanity checks", "> > Internally, the test fails with:\r\n> > tensorflow/python/data/experimental/ops/optimization_options.py:104: [C0301(line-too-long), ] Line too long (85/80)\r\n> > EDIT: Looking at the PR it is not clear to me why though. @rthadur could you please re-run internal tests for the latest version of the PR? Thanks.\r\n> \r\n> @jsimsa there are 4 lint errors which were reported internally , please check internal CL for more information , @delock can you please fix Ubuntu Sanity checks\r\n\r\nI didn't see the detail in Ubuntu Sanity checks.  How do I reproduce the lint errors locally?\r\n\r\ntensorflow/python/data/experimental/ops/optimization_options.py:104 does not exceed 80 characters, I don't know why it is reported.", "I will take care of fixing the issue myself to expedite the process of getting this PR submitted.", "> I will take care of fixing the issue myself to expedite the process of getting this PR submitted.\r\n\r\nThanks very much!", "Hi, I see this optimizer is disabled with this comment.  Can I know more detail about why this optimizer is disabled and what situation needs to be handled to make it more robust to input ordering?  @jsimsa \r\n\r\n// TODO(b/131229793): The current implementation of the optimization is brittle\r\n // as it depends on the order of inputs to commutative nodes. Make the\r\n // optimization robust to the input ordering before re-enabling it.", "IIRC, some upstream graph optimizations result in the rewrite to not handle the scenario tested in `python/data/experimental/kernel_tests/optimization/filter_with_random_uniform_fusion_test.py` correctly. It would be great if you could look into why (by re-enabling the optimizer and using the test to understand what is going on)."]}, {"number": 25383, "title": "TF Framework registry_test missing test case add", "body": "1-list api test case\r\n2-register method empty name parameter test case", "comments": ["@vrv \r\n\r\nThanks for quick review, update the PR as per changes requested, please review and thanks in advance."]}, {"number": 25382, "title": "TF 2.0: eager.function.defun (or tf.function) Cancelled Op Error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): '2.0.0-dev20190126'\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\n\r\nAssume that there is tf.Assert inside of decorated function, which is independent of input argument (then defun get same graph function from graph cache, right?)\r\n\r\nHowever, after AssertOp invoked once, following function calls are not be called. It directly raise `CancelledError`, which is described as below log information.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nWhether previous function call raised assertion error or not,\r\nfunction result should be independent of previous function call\r\n\r\n**Code to reproduce the issue**\r\n\r\nI think this is one of the most simplest example to reproduce error.\r\nSince it's stochastic, it might be run without error in single run.\r\nHowever, you can get `CancelledError` in few trials\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef prob_assert():\r\n  x = tf.random.uniform(shape=(1,))\r\n  tf.print('sampled value', x)\r\n  tf.assert_greater(x, 0.5, message='test')\r\n  return x\r\n\r\ntry:\r\n  prob_assert()\r\n  prob_assert()\r\n  prob_assert()\r\n  prob_assert()\r\nexcept:\r\n  pass\r\nprob_assert()\r\n```\r\n**Other info / logs**\r\n\r\n```\r\nsampled value [0.622857213]\r\nsampled value [0.417618513]  // Assertion Op Invoked\r\n2019-01-31 17:51:14.223845: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Invalid argument: assertion failed: [test] [Condition x > y did not hold element-wise:x (random_uniform:0) = ] [0.417618513] [y (assert_greater/y:0) = ] [0.5]\r\n\t [[{{node assert_greater/Assert/AssertGuard/else/_1/Assert}}]]\r\n2019-01-31 17:51:14.223876: E tensorflow/core/common_runtime/process_function_library_runtime.cc:735] Component function execution failed: Invalid argument: assertion failed: [test] [Condition x > y did not hold element-wise:x (random_uniform:0) = ] [0.417618513] [y (assert_greater/y:0) = ] [0.5]\r\n\t [[{{node assert_greater/Assert/AssertGuard/else/_1/Assert}}]]\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 19, in <module>\r\n    (random_assert())\r\n  File \"/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 376, in __call__\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1072, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 460, in _filtered_call\r\n    (t for t in nest.flatten((args, kwargs))\r\n  File \"/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 507, in _call_flat\r\n    outputs = self._inference_function.call(ctx, args)\r\n  File \"/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 312, in call\r\n    ctx=ctx)\r\n  File \"/Users/taebum/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.CancelledError:  [Op:__inference_random_assert_38]\r\n```\r\n", "comments": ["I'm a little confused. What's the issue you have here? You see a CancelledError because the function call as a whole is cancelled. Is the issue because you wanted to get an AssertionError?", "yes I thought that it should raise AssertionError if sampled x is less than 0.5 or return just x if x is greater than 0.5 whether previous function call raised AssertionError or not. Is it that I am misunderstanding?", "Right, but it's currently raising cancellederror if x is less than 0.5 and\nreturning x otherwise, right? So the only issue is which exception we raise?\n\nOn Wed, Feb 6, 2019 at 1:02 AM KIM TAE BUM <notifications@github.com> wrote:\n\n> yes I thought that it should raise AssertionError if sampled x is less\n> than 0.5 or return just x if x is greater than 0.5 whether previous\n> function call raised AssertionError or not. Is it that I am\n> misunderstanding?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25382#issuecomment-460946868>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQMx5qBiYDnK_4JEsoac4eLety1zks5vKpoTgaJpZM4ab2_->\n> .\n>\n\n\n-- \n - Alex\n", "Yes it's currently raising `CancelledError` and I think that's the issue.\r\nIn the logic of `defun`, it calls ` _maybe_define_function` in class `Function`.\r\nIt returns callable GraphFunction, `ConcreteFunction` class.\r\nInside of `_ maybe_define_function` it tries to retrieve graph function via cache key, made from function args and kwargs.\r\nThus in above case, after first `prob_assert()` function called, converted graph function is on the cache.\r\nFollowing function calls retrieve graph function from cache and just run it.\r\nHowever, after `AssertOp` occurred, following function calls always raise `CancelledError` since it gets same graph function via same cache key, right?\r\nI think it should be independent of previous execution, whether previous executions raised Assert or not.\r\nIn other words, I think following function calls raise AssertError if sampled x is less than 0.5 or return just x normally, not `CancelledError` (not affected by previous function calls)\r\n\r\n\r\nSorry for your confusion"]}, {"number": 25381, "title": "add AutoMixedPrecision graph optimization pass", "body": "This pull request is to add an AutoMixedPrecision graph optimization pass, which automatically transforms a normal tensorflow graph to mixed-precision graph, in order to take advantage of hardware accelerators like TensorCore of Volta GPU.\r\nAlthough NVIDIA provided documents and sample codes on how to write tensorflow models in mixed-precision (https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html#tensorflow), but it's not convenient for most users, especially that most existing tensorflow models are written in fp32 and the effort to manually convert them to mixed precision models is too high.\r\nWith AutoMixedPrecision graph optimization pass, running mixed-precision training is as easy as adding a single configuration like \"session_config.graph_options.optimizer_options.auto_mixed_precision=True\" (In some cases, loss scaling e.g. tf.contrib.mixed_precision.LossScaleOptimizer is needed to avoid underflow of fp16 gradients).\r\nThe AutoMixedPrecision graph optimization pass has been widely used by various production groups on Alibaba-PAI (Platform of AI) for half years.\r\n", "comments": ["This AutoMixedPrecision feature has already been successfully deployed onto Alibaba PAI platform running for more than half a year  and lots of our internal workload benefits from this feature since it significantly release the laboring work for manually converting fp32 models to fp16 version, which is both time-consuming and error-prone.  \r\nThis feature has been applied for diversified workloads, including image recognition, image detection, Transformer-based machine translation model, BERT, language model, large-scale embedding model and graph embedding models. The performance speed-up ranges from 1.3X to 3X with the same convergence trend which truly help exploit the computing power of NV's TensorCore accelerator.\r\n\r\nAlso there is a related PR regarding to [this one](https://github.com/tensorflow/tensorflow/pull/23244) which implements loss scaling manager, inside Alibaba, the AutoMixedPrecision graph optimization pass actually work together with our loss scaling manager to ensure our users have transparent experience using TensorCore feature.  Due to that the loss scaling manager PR involves changes of the core gradient processing logic, several months ago we have discussed with the TF community and there are still some different design considerations between Google and us, so [this PR](https://github.com/tensorflow/tensorflow/pull/23244) is still pending.\r\n\r\nSince the AutoMixedPrecision graph optimization pass can be decoupled from the automatic loss scaling manager feature, we make this separate PR wishing community could benefit from our internal work earlier. The AutoMixedPrecision graph optimization pass within this PR actually could cooperate with the tf.contrib.mixed_precision.LossScaleOptimizer to ensure both performance speed-up as well as convergence guarantee. Although personally we think what is implemented in [this PR](https://github.com/tensorflow/tensorflow/pull/23244) might be a better choice than tf.contrib.mixed_precision.LossScaleOptimizer since it make users' life easier:).\r\n\r\nAny feedbacks from community are highly welcome. ", "@rthadur \r\n\r\nThanks for the prompt follow-up, any comments and feedback are highly welcome. \r\n", "I am not the right reviewer for this PR, please redirect.", "@rthadur @mrry @reedwm \r\nFolks,\r\n\r\nAny comments or feedback from your side?\r\n\r\nThanks\r\n\r\nJun", "> @rthadur @mrry @reedwm\r\n> Folks,\r\n> \r\n> Any comments or feedback from your side?\r\n> \r\n> Thanks\r\n> \r\n> Jun\r\n\r\n@rthadur @mrry  @reedwm \r\nHi, any update from your side? This PR has been pending for more than one weeks and we would like to know any kinds of feedback from TF community.\r\n\r\nRegards.", "I'm definitely not the right reviewer for this PR.\r\n\r\nCC'ing @rmlarsen ... even though this isn't Grappler-related, he might know of a better reviewer.", "Sorry for the delayed response. @nluehr from Nvidia has also been working on a Grappler pass that enables mixed precision by making certain nodes fp16. We do not want two Grappler passes that have the same functionality. I hear you are planning on talking with @nluehr soon, and so if he can make his PR meet all your requirements, we will submit only his PR.\r\n\r\nFeel free email me at <my username> AT google if you want to talk to me privately.\r\n", "> Sorry for the delayed response. @nluehr from Nvidia has also been working on a Grappler pass that enables mixed precision by making certain nodes fp16. We do not want two Grappler passes that have the same functionality. I hear you are planning on talking with @nluehr soon, and so if he can make his PR meet all your requirements, we will submit only his PR.\r\n> \r\n> Feel free email me at AT google if you want to talk to me privately.\r\n\r\nWe will have communication with NV folks tomorrow about the collaboration. And personally I think it should be clarified about \"only submit NV's PR\" since our work certainly started much earlier than NV folks. It should be trivial for us to port our code from graph optimization to grappler pass, as what we have done for [the LINM grapper pass](https://github.com/tensorflow/tensorflow/commit/84fe908258550e1ce27e8725de1e2af279479c9d).  For the details I will communicate with you privately. ", "Nvidia and Alibaba are collaborating to combine our graph pass algorithms into a single PR, for the initial version, which we expect will be ready within a few weeks.", "@rmlarsen @meteorcloudy can you please help review this PR", "I believe Alibaba has collaborated with Nvidia to create #26342, which contains the functionality of this PR. So I'll close this PR.", "How can I get the graph for tensorboard after mixed precision? @yangjunpro @minminsun ", "Looks like you figured it out in tensorflow/models#7480. We can discuss the problem in that issue."]}, {"number": 25380, "title": "update", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "It says there are no files that were changed.. maybe you just need to rebase rather than manually merge from ```tensorflow/tensorflow```?", "> It says there are no files that were changed.. maybe you just need to rebase rather than manually merge from `tensorflow/tensorflow`?\r\n\r\n@leekinpo gentle ping to rebase ", "Nagging Reviewer : You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 45 days with no activity and the `awaiting review` label has been applied.", "I don't think this is a useful PR, there are no actual changes to TensorFlow code and the commits are just merges of old PRs. Thus, closing."]}, {"number": 25378, "title": "how to export tensorrt int8  model with savedmodel format after calibration\uff1f", "body": "Is it possible to export a savedModel int8 model after calibration?\r\nIf it is possible, is there any demo could be offered\uff1f\r\nIf it is impossible\uff0cwhen will it maybe possible\uff1f", "comments": ["You can save the frozen graph that convert_after_calib returns and use it like a normal frozen graph. Not sure about how SavedModel works.", "@trevor-m \uff0c do you mean calib_graph_to_infer_graph()?\r\nI know this api. I want to export a model for tf-serving and it seems tf-serving only support SavedModel and the  calib_graph_to_infer_graph() api export frozen model only. So i do not know how to do it.", "Looks like it is possible to convert a frozen graph to a SavedModel: https://stackoverflow.com/questions/44329185/convert-a-graph-proto-pb-pbtxt-to-a-savedmodel-for-use-in-tensorflow-serving-o/44329200#44329200", "@trevor-m \r\nIs the frozen model from calib_graph_to_infer_graph the same as a normal frozen model?  No difference\uff1f", "Yup, it is a normal frozen model. The only difference is that some of the nodes will be TRTEngineOp.", "@trevor-m \r\nI do conversion with the script from [stackflow](https://stackoverflow.com/questions/44329185/convert-a-graph-proto-pb-pbtxt-to-a-savedmodel-for-use-in-tensorflow-serving-o/44329200#44329200).\r\nI found that the `logits` tensor lose shape info. And the export model could run neither in TFTIS nor in tf-serving.\r\n\r\nthe detail info you can reference to [#issue 25417](https://github.com/tensorflow/tensorflow/issues/25417)\r\n\r\nThe error message from TRTIS is as bellow:\r\n` E0202 01:40:52.288811 1 aspired_versions_manager.cc:358] Servable {name: resnet_v1_50_graphdef version: 1} cannot be loaded: Invalid argument: unable to load model 'resnet_v1_50_graphdef', output 'logits' dims [] don't match configuration dims [1001]`\r\n\r\nLoading the model with tf-serving is OK, but when do request for test, the server will crashed with a Segmentation fault.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "@ymodak it's really a bug as i mentioned before, losing shape while do tensorrt transformation.", "@samikama Can you PTAL? Thanks!", "@trevor-m @pooyadavoodi  Can you PTAL", "@pooyadavoodi @trevor-m What is the status on here?", "Hi @rankeey! \r\nWe are checking to see if you still need help in this issue , Have you checked these threads yet [Link1](https://blog.tensorflow.org/2019/06/high-performance-inference-with-TensorRT.html),[Link2](https://colab.research.google.com/github/vinhngx/tensorrt/blob/vinhn-tf20-notebook/tftrt/examples/image-classification/TFv2-TF-TRT-inference-from-Keras-saved-model.ipynb?hl=en)? Please post this on [Tf forum](https://discuss.tensorflow.org/) if you need further assistance.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25378\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25378\">No</a>\n"]}, {"number": 25377, "title": "Added missing test case for invalid range usage", "body": "Added missing test case for the invalid default_ranges_stats usage", "comments": ["@gargn Can you pls review the same.", "@gargn can you pls review the PR and let me know your comments", "@gargn thanks for your review, this TC just tell that if you do not inference_type as QUANTIZED_UINT8\r\nthen the convertion does not happen to the qunatization format.I have made the changes requested by you, kindly approve", "@gargn & @rthadur , I have updated the code as per your comments, kindly check, if everything is ok, kindly approve the PR.", "@rthadur i have resolved the merge conflicts, can you please update the label to review requested.", "I don't think this test case is necessary so I'm closing this PR."]}, {"number": 25376, "title": "TF Framework ops_test missing test case add", "body": "1-has_default_graph api test case\r\n2-get_all_collection_keys api test case", "comments": ["Can someone help me get this in? @rthadur ?"]}, {"number": 25375, "title": "TF 2.0: API symbol renames design review.", "body": "TensorFlow 1.x has over 2000 endpoints total including over 500 endpoints in the root namespace. As number of symbols grows, it is important to maintain a clear structure to aid discoverability.\r\n\r\nCertain API symbol placements could be improved:\r\n\r\n- Some namespaces were created recently and might not contain all the corresponding symbols. For e.g. `tf.math` namespace was added recently. Symbols such as `tf.round` are not in `tf.math` namespace even though logically they belong in that namespace.\r\n- Some symbols are included in the root namespace even though they are rarely used (for e.g. `tf.zeta`).\r\n- Some symbols currently start with a prefix that could really be replaced by introducing a subnamespace (for e.g. `tf.string_strip` vs `tf.strings.strip`, `tf.sparse_maximum` vs `tf.sparse.maximum`).\r\n- Certain deep hierarchies seem redundant and could be flattened (for e.g. `tf.saved_model.signature_constants.CLASSIFY_INPUTS` could be moved to `tf.saved_model.CLASSIFY_INPUTS`).\r\n- To keep clear structure and reduce duplication, we want to collect all layers, losses and metrics under the `tf.keras` namespace.\r\n- In general, we want to balance flatness and browsability. Flat hierarchies allow for shorter endpoint names that are easy to remember (for e.g. `tf.add` vs `tf.math.add`). At the same time subnamespaces support easier browsability (for e.g. `tf.math` namespace would contain all math functions making it easier to discover available symbols).\r\n\r\nAdditional information about API symbol renames in TensorFlow 2.0 can be found in the RFC [here](https://github.com/tensorflow/community/blob/master/rfcs/20180827-api-names.md).", "comments": ["This has been done a while back. RFC: https://github.com/tensorflow/community/pull/16"]}, {"number": 25374, "title": "TF 2.0: Remove QueueRunners in favor of tf.data.Dataset.", "body": "**[`tf.train.QueueRunner`](https://www.tensorflow.org/api_docs/python/tf/train/QueueRunner)** is not compatible with eager execution, and will be deprecated in TF 2.0.\r\n\r\nAs an alternative, **[`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)** can be used to represent an input pipeline as a collection of elements (nested structures of tensors) and a \"logical plan\" of transformations that act on those elements.\r\n\r\n", "comments": ["Implemented! See _[What's coming in TensorFlow 2.0](https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8)_ for additional details."]}, {"number": 25373, "title": "TF 2.0: Move TensorFlow Lite out of tf.contrib.", "body": "**[TensorFlow Lite](https://www.tensorflow.org/lite/)** is the official solution for running machine learning models on mobile and embedded devices. It enables on\u2011device machine learning inference with low latency and a small binary size on Android, iOS, and other operating systems.\r\n\r\nIn TensorFlow 1.x, TF Lite was housed in `tf.contrib`, a somewhat volatile location. With TF 2.0, we intend to migrate TF Lite to the main API, under [`tf.lite`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/lite).\r\n\r\n", "comments": ["This was completed at the end of October. Here's a copy of the relevant email on [tflite@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!searchin/tflite/moving%7Csort:date/tflite/iIIXOTOFvwQ/EwfiRFtvAQAJ):\r\n* * * \r\nTL;DR TensorFlow Lite will move from tensorflow/contrib/lite to tensorflow/lite on Wednesday, Oct. 31. This should be a no-op for Python package users (but please update your usage) and will break Bazel users.\r\n\r\nHello everyone,\r\n\r\nTensorFlow Lite is moving from //tensorflow/contrib/lite to //tensorflow/lite. The move is scheduled for this Wednesday, October 31, and will take effect on the master TensorFlow branch, in upcoming nightly packages, and in TensorFlow 1.13+. This email describes what you can expect from the move.\r\n\r\nWe previously announced that the migration would happen on Wednesday Oct. 10, but we discovered blockers that needed a significant change in direction. Sorry about the confusion.\r\n\r\nFor anyone who depends on tf.contrib.lite in TensorFlow's Pip package, we\u2019ve added import helpers so that tf.contrib.lite points to tf.lite. This will affect upcoming nightly packages and TF 1.13+. tf.contrib will not be present in TF 2.0. Please update your imports as soon as you can.\r\n\r\nFor anyone who depends on TF Lite\u2019s OSS Bazel BUILD structure or Makefiles, you will need to update packages that reference tensorflow/contrib/lite so that they point to tensorflow/lite instead."]}, {"number": 25372, "title": "ImportError: DLL load failed: The specified procedure could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new/choose). Please provide all the information it asks. Thank you.\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25371, "title": "TF 2.0: Create conversion tool from TensorFlow 1.x.", "body": "", "comments": ["We have tf_upgrade_v2 script so this issue can be resolved:\r\nhttps://www.tensorflow.org/alpha/guide/upgrade"]}, {"number": 25370, "title": "TF 2.0: Create nightly CI builds for the TF 2.0 target.", "body": "", "comments": ["Goldie, I think you have setup all the nightly builds we need."]}, {"number": 25369, "title": "Feature: End-to-end TensorFlow Extended (TFX) example for TF 2.0.", "body": "**[TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx/)** is an end-to-end, TensorFlow-based, general-purpose machine learning platform implemented at Google. We've already open sourced some TFX libraries with the rest of the system to come. For an overview of TFX, read our [KDD\u20192017 paper](https://dl.acm.org/citation.cfm?id=3098021) and watch the [Dev Summit talk](https://www.youtube.com/watch?v=vdG7uKQ2eKk).\r\n\r\nTFX includes:\r\n- **[TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation)**: a library for exploring and validating machine learning data.\r\n- **[TensorFlow Transform](https://www.tensorflow.org/tfx/transform)**: full-pass analyses over data to create transformation graphs that are consistently applied during training and serving.\r\n- **[TensorFlow Model Analysis](https://www.tensorflow.org/tfx/model_analysis)**: libraries and visualization components to compute full-pass and sliced model metrics over large datasets, and analyze them in a notebook.\r\n- **[TensorFlow Serving](https://www.tensorflow.org/serving)**: a flexible, high-performance serving system for machine learning models, designed for production environments.\r\n\r\nThe purpose of this issue is to track the creation of an end-to-end TFX example using TensorFlow 2.0: a Keras model working with Transform, TFMA, data validation, and Serving.", "comments": ["What's the latest status?", "Nice intro at GoogleNext19 but will a full demo be available soon? ", "how it is done in the project tracker and still open in github? it is a bit confusing to track the progress.\r\n\r\nand please if it is done, how can we access this example?\r\n", "here is a link to the end-to-end example using the Chicago taxi dataset - \r\nhttps://github.com/tensorflow/tfx/tree/master/tfx/examples/chicago_taxi_pipeline ", "@shubham-samant this link is sending me to the old example, still in TF 1.X. Do you have a reference for an example using TF 2.0 and TFX?", "as @ahmedanis03 mentioned, it is a bit confusing that this is open here but the task is marked as done. Is there already material or is it under development?", "@dynamicwebpaige, are we to interpret https://github.com/tensorflow/tfx/issues/542 and the current status of \"Done\" here to mean that this will not be a part of the TF 2.0 release?", "I wonder if there are any news regarding that, and specifically regarding TF Transform together with the new Tensorflow 2.0. The functionality in TFT is quite important to produce well-structured preprocessing steps without leaking info between train and validation/test set.\r\n\r\nOn the other hand, anyone who know about a nice approach to replace TFT for the while?\r\n\r\nThanks", "> anyone who know about a nice approach to replace TFT for the while\r\n\r\n@Efaq i might be able to help but first, could you please clarify what you meant by the following statement and what is the failure condition under which this is expected?\r\n\r\n> without leaking info between train and validation/test set\r\n\r\n\r\nThanks..", "Closing old issue, we have several e2e examples now."]}, {"number": 25368, "title": "Fix bug in hadoop_file_system.cc when reading big variable from hdfs", "body": "The parameter tSize of the libhdfs's hdfsPread function has type int32, casting a size_t of int64 to tSize is not safe in hadoop_file_system.cc. \r\n\r\nFor example, we have a big user embedding of [250000000,28] in model, it will throw exception when reading from hdfs like this:\r\n\r\nInvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\nhdfs://appcluster-cdh/user/tftest/model/model.ckpt-0.data-00003-of-00004; Invalid argument\r\n         [[node save/RestoreV2 (defined at /data/home/tftest/python1.13rc/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py:1530) ]]", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "> I signed it!\r\n\r\n", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 25367, "title": "Testing: TF 2.0 with TensorFlow Datasets (TFDS).", "body": "TFDS is now ready for testing, sans the LMDB dataset!\r\n\r\nhttps://github.com/tensorflow/datasets/issues/34", "comments": ["TFDS will begin testing against the TF 2.0 preview in the next week or two. And the TF tutorials that use TFDS will also be tested. Tomer Kaftan (does he have a github handle?) was looking into adding tests for them. ", "I was not aware of this ticket wanted to just experiment, and seems like there are still dependencies on contrib like https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/__init__.py#L90", "Yup, will update here when TFDS is 2.0-ready. ", "I've tested tf2.0 and TFDS in #25414 ", "TFDS now supports TF 2.0. Woohoo!"]}, {"number": 25366, "title": "Documentation: TF 2.0 docs and tutorials.", "body": "Documentation and tutorials are a critical piece of every open-source project, and a developer's first impression of the product. The purpose of this issue will be to track the creation, migration, and translations for TensorFlow 2.0 docs.\r\n\r\nFor more information, check out the example notebooks we have available [here](https://github.com/tensorflow/docs/tree/master/site/en/r2), as well as the [TF 2.0 upgrade guide](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md) and the [Effective 2.0 Style Guide](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/effective_tf2.md).\r\n\r\n", "comments": ["#25587 ", "Hey @dynamicwebpaige can i know about any specific functions which needs some docs!?", "@dynamicwebpaige  in this documentation, do we have to implement the high-level APIs without using the graph execution. Since sessions are not being used anymore.?", "Tensorflow 2.x resources such as corresponding tutorials, blogs, codes and videos:\r\nhttps://github.com/Amin-Tgz/Awesome-TensorFlow-2\r\nI hope to be useful", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25366\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25366\">No</a>\n"]}, {"number": 25365, "title": "Feature: Google Colab compatibility with TF 2.0.", "body": "**[Colaboratory](https://colab.research.google.com/)** is a free research tool for machine learning education and research. It\u2019s a Jupyter notebook environment that requires no setup to use. Colab works with most major browsers, and is most thoroughly tested with desktop versions of Chrome and Firefox.\r\n\r\nThe purpose of this issue will be to track TF 2.0 compatibility with Colab.", "comments": ["Looks like [CUDA 10 support has landed for Google Colab](https://github.com/tensorflow/tensorflow/issues/25081#issuecomment-460912076)! ", "I am closing the tracking issue as it was resolved. `TF2.0` is completely compatible with Colab. Thanks!"]}, {"number": 25364, "title": "Documentation: Updating tutorials to use TensorFlow Datasets (TFDS).", "body": "**[TensorFlow Datasets (TFDS)](https://www.tensorflow.org/datasets/api_docs/python/tfds)** is a collection of datasets ready-to-use with TensorFlow. Each dataset is defined as a `tfds.core.DatasetBuilder`, which encapsulates the logic to download the dataset and construct an input pipeline, as well as contains the dataset documentation (version, splits, number of examples, etc.).\r\n\r\nThe purpose of this issue is to migrate all tutorials to use TensorFlow Datasets, and to be compliant with the TF 2.0 API. Each migrated tutorial must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["I want to contribute. \r\nIs this referring to imports like https://github.com/tensorflow/tensorflow/blob/226398bd70d6369dd97fa9cf4bee94af9a05f1d0/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py#L33 where we would maybe want to import data from Tensorflow Datasets:\r\n```\r\nimport tensorflow_datasets as tfds\r\nds_train, ds_test = tfds.load(name=\"mnist\", split=[\"train\", \"test\"])\r\n``` \r\nPlease correct me if I'm wrong.", "Hi, @piyush-kgp! Thanks for the offer to help - we appreciate it! \ud83d\ude0a\r\n\r\nFor more information on `tf.data.Datasets`, check [here](https://github.com/tensorflow/datasets).\r\n\r\n@rsepassi would be the best contact, and is working with @yashk2810 + several other members of the TensorFlow team to get tutorials ready for TF 2.0. Also make sure to follow progress on these two issues for TFDS support and testing: [#31](https://github.com/tensorflow/datasets/issues/31), #25367 .\r\n\r\n**Ryan, Yash** - are there any tutorials on GitHub right now that Piyush could help migrate?", "That\u2019s great, thanks @piyush-kgp. TFDS is now TF 2.0-ready and can be used in the tutorials. @yashk2810 and his colleagues can coordinate work on the tutorials.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25364\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25364\">No</a>\n"]}, {"number": 25363, "title": "Feature: TF Serving compatibility with TF 2.0.", "body": "**[TensorFlow Serving](https://www.tensorflow.org/serving/)** is a flexible, high-performance serving system for machine learning models, designed for production environments. Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.\r\n\r\nThe purpose of this issue is to upgrade Serving to be TF 2.0 compliant. The system must be eager and distribution compatible, with tests, and all associated engineering artifacts.\r\n", "comments": ["Is this still current?  I've seen some presos that suggest that TF-serving does support TF 2.0.", "Its on the done column in https://github.com/orgs/tensorflow/projects/4#card-17112802 so I would assume its complete, right?", "Tensorflow2.0 training model\uff0cHas compatibility been fully achieved on tf-serving?", "Any update on this? Thanks!", "Would appreciate any update on this if possible!", "**EDIT: never mind, heard from my team that it is working.** For anyone else running into the issue described below, you'll want your saved model to be under another folder with a version number. Other than that, this issue should be marked as closed if it's done so there's no confusion.\r\n\r\nOLD ------------\r\nWe trained a model with the `tf.keras` API and created a SavedModel with Keras. We then tried to deploy it to tf-serving, but we're getting:\r\n```\r\nNo versions of servable XX found under base path /models/XX\r\n```\r\nI verified that the SavedModel exists in that path. It was exported like so:\r\n```\r\nmodel = tf.keras.models.load_model(\"model.h5\", custom_objects={'KerasLayer':hub.KerasLayer})\r\nmodel.save(some_path_here, save_format=\"tf\")\r\n```\r\nCan someone confirm whether this issue was resolved or not?", "This is already been supported.Closing the request."]}]