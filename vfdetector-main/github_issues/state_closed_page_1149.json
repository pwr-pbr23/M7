[{"number": 18739, "title": "Undefined References building example_trainer.cc with dynamic linking and Tensorflow r1.7 as external repository and CUDA9/CuDNN7", "body": "/.bazelrc\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --force_python=py2\r\nbuild --host_force_python=py2\r\nbuild --python_path=\"/usr/bin/python\"\r\nbuild --define with_jemalloc=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_VERSION=\"9.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda-9.0/lib64:/usr/local/cuda/lib64:\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild --define grpc_no_ares=true\r\nbuild:opt --copt=-march=native\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\nbuild --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\r\n```\r\n/tools/bazel.rc\r\n```\r\nbuild:monolithic --define framework_shared_object=false\r\nbuild --define framework_shared_object=true\r\nbuild:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nbuild:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\r\nbuild:mkl --define=using_mkl=true\r\nbuild --define=use_fast_cpp_protos=true\r\nbuild --define=allow_oversize_protos=true\r\nbuild --define=grpc_no_ares=true\r\nbuild --spawn_strategy=standalone\r\nbuild --genrule_strategy=standalone\r\nbuild -c opt\r\n```\r\n/WORKSPACE:\r\n```\r\nworkspace(name = \"proj\")\r\ngit_repository(\r\n    name = \"org_tensorflow\",\r\n    commit = \"024aecf414941e11eb643e29ceed3e1c47a115ad\",\r\n    remote = \"git@github.com:tensorflow/tensorflow.git\",\r\n)\r\n\r\nhttp_archive(\r\n    name = \"io_bazel_rules_closure\",\r\n    sha256 = \"6691c58a2cd30a86776dd9bb34898b041e37136f2dc7e24cadaeaf599c95c657\",\r\n    strip_prefix = \"rules_closure-08039ba8ca59f64248bb3b6ae016460fe9c9914f\",\r\n    urls = [\r\n        \"https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/08039ba8ca59f64248bb3b6ae016460fe9c9914f.tar.gz\",\r\n        \"https://github.com/bazelbuild/rules_closure/archive/08039ba8ca59f64248bb3b6ae016460fe9c9914f.tar.gz\",  # 2018-01-16\r\n    ],\r\n)\r\n\r\nload(\"//src:workspace.bzl\", \"proj_workspace\")\r\nproj_workspace()\r\n```\r\nSource code: copied `@org_tensorflow//tensorflow/cc/tutorial/example_trainer.cc` to `//src/example_trainer.cc`.\r\n/src/BUILD:\r\n```\r\ncc_binary(\r\n    name = \"example\",\r\n    srcs = [\"example_trainer.cc\"],\r\n    deps = [\r\n        \"@org_tensorflow//tensorflow/cc:cc_ops\",\r\n        \"@org_tensorflow//tensorflow/cc:client_session\",\r\n        \"@org_tensorflow//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n```\r\nBuild command:\r\n```\r\nbazel build src:example\r\n```\r\nError:\r\nTons of \"undefined reference\" errors pointing to functions like stringprintf().\r\n```\r\nnaming.cc:(.text._ZN10tensorflow12DataFilenameB5cxx11ENS_11StringPieceEii+0x1f): undefined reference to `tensorflow::strings::Printf[abi:cxx11](char const*, ...)'\r\n```\r\nAdding ```--monolithic``` does make it compile and run properly:\r\n```bazel build --monolithic src:example```\r\n", "comments": ["Ubuntu 16.04LTS\r\nBazel 0.11.1\r\n```\r\nBuild label: 0.11.1- (@non-git)\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n```\r\nOpenJDK 9\r\n```\r\nopenjdk version \"9-internal\"\r\nOpenJDK Runtime Environment (build 9-internal+0-2016-04-14-195246.buildd.src)\r\nOpenJDK 64-Bit Server VM (build 9-internal+0-2016-04-14-195246.buildd.src, mixed mode)\r\n```", "Some error messages:\r\n```\r\ngif_io.cc:(.text._ZN10tensorflow3gif6DecodeEPKviRKSt8functionIFPhiiiiEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x43f): undefined reference to `tensorflow::strings::StrCat[abi:cxx11](tensorflow::strings::AlphaNum const&)'\r\ngif_io.cc:(.text._ZN10tensorflow3gif6DecodeEPKviRKSt8functionIFPhiiiiEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x4d2): undefined reference to `tensorflow::strings::StrCat[abi:cxx11](tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\ngif_io.cc:(.text._ZN10tensorflow3gif6DecodeEPKviRKSt8functionIFPhiiiiEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x558): undefined reference to `tensorflow::strings::StrCat[abi:cxx11](tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\nbazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/core/util/tensor_bundle/libnaming.a(naming.o): In function `tensorflow::MetaFilename[abi:cxx11](tensorflow::StringPiece)':\r\nnaming.cc:(.text._ZN10tensorflow12MetaFilenameB5cxx11ENS_11StringPieceE+0x19): undefined reference to `tensorflow::strings::Printf[abi:cxx11](char const*, ...)'\r\nbazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/core/util/tensor_bundle/libnaming.a(naming.o): In function `tensorflow::DataFilename[abi:cxx11](tensorflow::StringPiece, int, int)':\r\nnaming.cc:(.text._ZN10tensorflow12DataFilenameB5cxx11ENS_11StringPieceEii+0x1f): undefined reference to `tensorflow::strings::Printf[abi:cxx11](char const*, ...)'\r\n```", "Unfortunately Bazel does not yet allow transitive dependencies on shared objects (cc_binary with linkshared). So that dependency gets stripped out, leading to undefined symbols.\r\n\r\nA workaround is to use `tf_cc_binary` ([example](https://github.com/tensorflow/tensorflow/blob/a3aa84f28c09b86e4961632d0e2581d06bb4a47e/tensorflow/core/profiler/BUILD#L9)), which will add [//tensorflow:libtensorflow_framework.so](https://github.com/tensorflow/tensorflow/blob/a3aa84f28c09b86e4961632d0e2581d06bb4a47e/tensorflow/BUILD#L423) to your `srcs` and add some rpaths. It should work with TF as an external repository, but let me know if it doesn't :)", "Thanks for info! Now I know what to do."]}, {"number": 18738, "title": "tensorflow problem", "body": "how to solve the following error? ImportError: No module named '_pywrap_tensorflow_internal'\r\nI have python  3.5 installed, Jupyter Notebook along with Ananconda also installed.\r\n\r\n", "comments": ["Hi @tbousole, can you provide more information? At minimum, tensorflow version and a piece of code that reproduces the error would be needed.", "import tensorflow as tf\r\n\r\nreset_graph()\r\n\r\nx = tf.Variable(3, name=\"x\")\r\ny = tf.Variable(4, name=\"y\")\r\nf = x*x*y + y + 2\r\n\r\ni am using tensorflow version 1.5", "@tboousele, if you're using the CPU version on Windows, can you please try installing version 1.4. This turned out to be a good workaround for a friend of mine.", "@tbousole what is `reset_graph()`? The following code works fine for me:\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.reset_default_graph()\r\n\r\nx = tf.Variable(3, name=\"x\")\r\ny = tf.Variable(4, name=\"y\")\r\nf = x * y + y + 2\r\n```\r\n", "Perhaps also take a look at #11571.", "This is a duplicate of #11571 (thanks for the link, @cchan). Please move discussion over there so that advice is not spread across the tracker. Thanks!"]}, {"number": 18737, "title": "TF hangs with distributed mode", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.5\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.5.4\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\n9.0/7\r\n- **GPU model and memory**:\r\nTitanXP, 12G\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen I run skip_thoughts model with distributed mode(2 workers in a machine, 2 GPUs per worker), tf hangs with linux condition variable error.\r\n\r\n### Source code / logs\r\nBelow is the gdb log.\r\n`Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\npthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n185\t../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S: No such file or directory.\r\n(gdb) bt\r\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\r\n#1  0x00007fd44d2e891c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#2  0x00007fd45b021ecb in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fd45b0217a1 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fd45b01ec12 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fd45b01f0f3 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) ()\r\n   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fd45879da1b in tensorflow::(anonymous namespace)::WaitForNotification(tensorflow::CallOptions*, long long, tensorflow::Notification*) ()\r\n   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fd45879e313 in tensorflow::LocalMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) ()\r\n   from /home/soojeong/expdir_parallax/venv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so`\r\n", "comments": ["I found master sessions are created more than # workers + #ps, is it okay?\r\n`2018-04-21 00:34:14.953222: I tensorflow/core/distributed_runtime/master_session.cc:1017] Start master session b6a0c9c95637b2bf with config: allow_soft_placement: true operation_timeout_in_ms: 600000\r\n2018-04-21 00:34:16.310345: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x427e8e0\r\n2018-04-21 00:34:44.031831: I tensorflow/core/distributed_runtime/master_session.cc:1017] Start master session 76c3e3934cf937ae with config: allow_soft_placement: true operation_timeout_in_ms: 600000\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: encoder/gru_cell/gates/w_h, encoder/gru_cell/candidate/u, decoder_pre/gru_cell/gates/w_h, decoder_pre/gru_cell/candidate/u, decoder_post/gru_cell/gates/w_h, decoder_post/gru_cell/candidate/u, ready: None\r\n2018-04-21 00:35:15.087387: I tensorflow/core/distributed_runtime/master_session.cc:1017] Start master session c4f0640d9b337f3b with config: allow_soft_placement: true operation_timeout_in_ms: 600000\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: encoder/gru_cell/gates/w_h, encoder/gru_cell/candidate/u, decoder_pre/gru_cell/gates/w_h, decoder_pre/gru_cell/candidate/u, decoder_post/gru_cell/gates/w_h, decoder_post/gru_cell/candidate/u, ready: None\r\n2018-04-21 00:35:46.098996: I tensorflow/core/distributed_runtime/master_session.cc:1017] Start master session 9369768b7b043c22 with config: allow_soft_placement: true operation_timeout_in_ms: 600000\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: encoder/gru_cell/gates/w_h, encoder/gru_cell/candidate/u, decoder_pre/gru_cell/gates/w_h, decoder_pre/gru_cell/candidate/u, ready: None\r\n2018-04-21 00:36:17.038228: I tensorflow/core/distributed_runtime/master_session.cc:1017] Start master session 7efeddd92fe6182f with config: allow_soft_placement: true operation_timeout_in_ms: 600000\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: encoder/gru_cell/gates/w_h, encoder/gru_cell/candidate/u, decoder_pre/gru_cell/gates/w_h, ready: None\r\n2018-04-21 00:36:48.031089: I tensorflow/core/distributed_runtime/master_session.cc:1017] Start master session 6969f6e188a265b0 with config: allow_soft_placement: true operation_timeout_in_ms: 600000\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: encoder/gru_cell/gates/w_h, encoder/gru_cell/candidate/u, decoder_pre/gru_cell/gates/w_h, ready: None\r\n2018-04-21 00:37:19.015507: I tensorflow/core/distributed_runtime/master_session.cc:1017] Start master session ac9ea7dbf55378a3 with config: allow_soft_placement: true operation_timeout_in_ms: 600000\r\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: encoder/gru_cell/gates/w_h, ready: None\r\n2018-04-21 00:37:50.067835: I tensorflow/core/distributed_runtime/master_session.cc:1017] Start master session 22291f96c1f27e60 with config: allow_soft_placement: true operation_timeout_in_ms: 600000\r\n2018-04-21 00:37:50.692820: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x4ead290\r\nINFO:140062530406144:RDAG:RunOp: Created MonitoredTrainingSession on grpc://localhost:55501\r\nINFO:140062530406144:RDAG:iteration 0 start`", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 35 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 50 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 65 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 80 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 95 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please provide a pointer to the code you're running so that I can try and reproduce it.", "I think this issue is gone. I'll close it. ", "how do you solve it?", "@sj6077 how do you solve it?", "@lxn179208 I don't remember the issue exactly, but if you suffer similar problems you can check device of your variables by `log_device_placement` option in SessionConfig. Sometimes, some variables are assigned to each worker, then global init is not working."]}, {"number": 18736, "title": "tensorflow/core/framework/allocator.cc:101] Allocation of X exceeds 10% of system memory. #18735", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nMy server has 32gb of RAM, but tensorflow uses only 10% of that memory.\r\nIt returns the message:\r\n<<tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.>>\r\n\r\nIs it possible to increase this percentage?\r\n\r\n### Source code / logs\r\n### SOURCE CODE:\r\nimport keras\r\nfrom keras import regularizers\r\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, add\r\nfrom keras.models import Model\r\nfrom keras.layers.core import Layer, Dense, Dropout, Activation, Flatten, Reshape\r\nfrom keras.regularizers import l2\r\nfrom keras.utils import np_utils\r\nfrom keras.callbacks import TensorBoard\r\nfrom sklearn.model_selection import train_test_split\r\nimport numpy as np\r\nimport h5py\r\nimport time\r\n\r\nfile_train = 'features/files_train_test/train_inceptionv3_doc2vec.npy'\r\nfile_test = 'features/files_train_test/test_inceptionv3_doc2vec.npy'\r\nx_train = np.load(file_train)\r\nx_test = np.load(file_test)\r\n\r\nprint('shape train: ',x_train.shape,'shape test: ', x_test.shape)\r\nprint('size train: ', len(x_train), 'size test: ', len(x_test))\r\nprint('prod train: ',np.prod(x_train.shape[1:]), 'prod test: ', np.prod(x_test.shape[1:]))\r\n\r\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\r\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\r\n\r\nprint('shape train: ',x_train.shape,'shape test: ', x_test.shape)\r\n\r\nepochs = 10\r\nbatch_size = 32\r\ninput_size = x_train.shape[1]\r\noutput_size = x_train.shape[1]\r\nhidden_size = x_train.shape[1]\r\nfile_name = 'features/files_reduce/sparse/autoencoder_inceptionv3_doc2vec_'\r\n\r\nx = Input(shape=(input_size,))\r\nh = Dense(hidden_size, activation='relu', activity_regularizer=regularizers.l1(10e-5))(x)\r\nr = Dense(output_size, activation='sigmoid')(h)\r\nautoencoder = Model(inputs=x, outputs=r)\r\nautoencoder.compile(optimizer='adam', loss='mse')\r\nautoencoder.summary()\r\nautoencoder.fit(x_train\r\n                ,x_train\r\n                ,batch_size=batch_size\r\n                ,epochs=epochs\r\n                ,verbose=1\r\n                ,validation_data=(x_test, x_test))\r\n\r\nautoencoder.save(file_name+name_full)\r\nprint('Save encode' + file_name+name_full)\r\n\r\n===============================================================================\r\nOUTPUT: \r\n\r\nUsing TensorFlow backend.\r\nshape train:  (21248, 49452) shape test:  (5313, 49452)\r\nsize train:  21248 size test:  5313\r\nprod train:  49452 prod test:  49452\r\nshape train:  (21248, 49452) shape test:  (5313, 49452)\r\n\r\nLayer (type)                      | Output Shape              | Param    \r\n- - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - -  - - - - - - - -  - - - - - -\r\ninput_1 (InputLayer)         | (None, 49452)             | 0         \r\ndense_1 (Dense)              | (None, 49452)             | 2445549756\r\ndense_2 (Dense)              | (None, 49452)             | 2445549756\r\n- - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - -  - - - - - - - -  - - - - - -\r\n\r\nTotal params: 4,891,099,512\r\nTrainable params: 4,891,099,512\r\nNon-trainable params: 0\r\n\r\nTrain on 21248 samples, validate on 5313 samples\r\nEpoch 1/10\r\n2018-04-20 11:22:08.069764: W tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.\r\n2018-04-20 11:22:12.089390: W tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.", "comments": ["https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/logging.cc#L117\r\n\r\nTry setting the environment variable\r\n\r\nTF_CPP_MIN_LOG_LEVEL=2\r\nto filter out INFO and WARNING logs.\r\n\r\n", "I set environment variable \"TF_CPP_MIN_LOG_LEVEL=2\", the message is not show, but the problem persists. After a long time running the die process.\r\n\r\n**OUTPUT**:\r\n\r\n from ._conv import register_converters as _register_converters\r\nUsing TensorFlow backend.\r\nshape train:  (21248, 49452) shape test:  (5313, 49452)\r\nsize train:  21248 size test:  5313\r\nprod train:  49452 prod test:  49452\r\nshape train:  (21248, 49452) shape test:  (5313, 49452)\r\n_____________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\ninput_1 (InputLayer)         (None, 49452)             0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 49452)             2445549756\r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 49452)             2445549756\r\n\r\nTotal params: 4,891,099,512\r\nTrainable params: 4,891,099,512\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTrain on 21248 samples, validate on 5313 samples\r\n**Epoch 1/10\r\nMorto**\r\n", "Besides the warning message, does it affect the speed or the accuracy? I'm trying to figure out what is the exact problem.\r\n", "I need to increase the memory that tensorflow use. This is necessary for my application run. Do you know any alternative? \r\n\r\nThe problem is, my server have 32GB, but tensorflow is using only 10%.", "I am currently having the exact same issue.\r\n2018-04-27 12:00:03.250952: W tensorflow/core/framework/allocator.cc:101] Allocation of 439808000 exceeds 10% of system memory.\r\nKilled", "Exact same issue:\r\n2018-04-28 23:29:06.477982: W tensorflow/core/framework/allocator.cc:101] Allocation of 18874368 exceeds 10% of system memory.", "Same issue:\r\n2018-05-01 08:05:49.156289: W tensorflow/core/framework/allocator.cc:101] Allocation of 179437568 exceeds 10% of system memory.", "> 2018-05-02 01:08:14.865905: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-05-02 01:08:15.343656: W tensorflow/core/framework/allocator.cc:101] Allocation of 1061683200 exceeds 10% of system memory.\r\n", "The same problem with ssd_mobilenet_v1_coco and ssd_mobilenet_v2_coco. I do not have any problem with faster_rcnn_inception_v2.\r\n\r\nEDIT: Ok my solution was to set batch_size: 1 (line 141 of config file) instead of batch_size: 37 in  ssd_mobilenet_v1_coco.config file or other config file.\r\n\r\n**I suppose that solution for this problem in the main post is to change batch_size = 32, to some smaller value. :)**", "I tried this abordage (decrease value batch_size) previously, but not resolved problem.", "I have the same issue when I'm using conv1d", "Well, I wished something like this feature because my laptop often freezed when I did something stupid in tensorflow.\r\nHaving an option to limit max mem usage in TF would be really cool, but currently I'm working with 32GB RAM and I obviously do not want to be limited to 3.2GB RAM usage :)\r\n", "It's just a warning, not a limit on TF mem usage. You can easily verify it by looking at your system memory monitor.", "From your output:\r\n\"Total params: 4,891,099,512\"\r\nEven each param is only one byte. TensorFlow was using at least 4,891,099,512 bytes, or 4.6 GB, which is larger than 10% of 32 GB.\r\n\r\n", "I'm getting the warnings even when allocating amounts much smaller than 10% of available RAM.\r\n\r\nReason: `freeram` is not a reliable indicator for the amount of available free ram. After a while, most of it is used as buffer/cache by the OS.\r\n\r\nInstead, the true free ram should include bufferram.\r\nhttps://github.com/tensorflow/tensorflow/blob/f27033fb1212d7031a359c913d0f59e976b14c14/tensorflow/core/platform/posix/port.cc#L180\r\n```\r\nint64 AvailableRam() {\r\n#if defined(__linux__) && !defined(__ANDROID__)\r\n  struct sysinfo info;\r\n  int err = sysinfo(&info);\r\n  if (err == 0) {\r\n    return info.freeram + bufferram;\r\n  }\r\n#endif\r\n  return INT64_MAX;\r\n}\r\n```", "This problem occurs even for two layer net with batch size of 64 for 32x32x3 images, seems like nothing to do with RAM ", "\u4e00\u6837\u7684\u95ee\u9898\uff0c\u5728\u8fd0\u884ctf.global_variables_initializer()\u7684\u65f6\u5019\u5c31\u4f1a\u51fa\u73b0\uff0c\u7136\u540e\u7a0b\u5e8f\u5c31\u5f02\u5e38\u88abkill\u4e86", "so this alert is just for warn ? don't we neccesary to nothing to do ? ", "NO\uff0cI just found what's the problem is. May be your model contains too much variables , such as a big Dense layer (FC) , and check out your data type, float64 is lager than float32, and last , some optimizer will make this problem too, such sa adam , it will allocate double memory of model size, because it will save the history grads.", "Same error here, even though my model only has 144.340 parameters. I have 10 GB of GPU RAM and I get the error message:\r\n\r\nW tensorflow/core/framework/allocator.cc:101] Allocation of 589824000 exceeds 10% of system memory", "@plaffitte Care of your date type, float32 or float 64, and if your optimizer need to save history grad, such as adam, it also make the need of RAM double.", "Same issue here, tried reducing the batch_size to a smaller number worked for me.  Training time may be extended but at least it's enabling training and preventing core dump. Credit to @Swordancer for the suggestion. ", "It seems like it isjust a warning and the sysetm will still allocate the same amount of memory. here's the code...\r\n\r\nvoid* AlignedMalloc(size_t size, int minimum_alignment) {\r\n#if defined(__ANDROID__)\r\n  return memalign(minimum_alignment, size);\r\n#else  // !defined(__ANDROID__)\r\n  void* ptr = nullptr;\r\n  // posix_memalign requires that the requested alignment be at least\r\n  // sizeof(void*). In this case, fall back on malloc which should return\r\n  // memory aligned to at least the size of a pointer.\r\n  const int required_alignment = sizeof(void*);\r\n  if (minimum_alignment < required_alignment) return Malloc(size);\r\n#ifdef TENSORFLOW_USE_JEMALLOC\r\n  int err = jemalloc_posix_memalign(&ptr, minimum_alignment, size);\r\n#else\r\n  int err = posix_memalign(&ptr, minimum_alignment, size);\r\n#endif\r\n  if (err != 0) {\r\n    return nullptr;\r\n  } else {\r\n    return ptr;\r\n  }\r\n#endif\r\n}\r\n\r\nCALLED BY....\r\n\r\nvoid* AllocateRaw(size_t alignment, size_t num_bytes) override {\r\n    if (num_bytes > LargeAllocationWarningBytes() &&\r\n        single_allocation_warning_count_ < kMaxSingleAllocationWarnings) {\r\n      ++single_allocation_warning_count_;\r\n      **LOG(WARNING) << \"Allocation of \" << num_bytes << \" exceeds \"\r\n                   << 100 * kLargeAllocationWarningThreshold\r\n                   << \"% of system memory.\";\r\n    }**\r\n\r\n    void* p = port::AlignedMalloc(num_bytes, alignment);\r\n    if (cpu_allocator_collect_stats) {\r\n      const std::size_t alloc_size = port::MallocExtension_GetAllocatedSize(p);\r\n      mutex_lock l(mu_);\r\n      ++stats_.num_allocs;\r\n      stats_.bytes_in_use += alloc_size;\r\n      stats_.max_bytes_in_use =\r\n          std::max<int64>(stats_.max_bytes_in_use, stats_.bytes_in_use);\r\n      stats_.max_alloc_size =\r\n          std::max<int64>(stats_.max_alloc_size, alloc_size);\r\n\r\n      if (stats_.bytes_in_use > TotalAllocationWarningBytes() &&\r\n          total_allocation_warning_count_ < kMaxTotalAllocationWarnings) {\r\n        ++total_allocation_warning_count_;\r\n        LOG(WARNING) << \"Total allocated memory \" << stats_.bytes_in_use\r\n                     << \"exceeds \" << 100 * kTotalAllocationWarningThreshold\r\n                     << \"% of system memory\";\r\n      }\r\n    }\r\n    return p;\r\n}", "Tf:1.9 , I have the same issue.    Total: prams: 367.352.  256G mem.  1) When I use tf.keras.layers, it's OK, but I changed to tf.layers to address  BN layer accuracy issue.  this issue pop up 2) Batch size 128 work when using tf.keras layer, still exist when batch size is set to 8    3) from the detail, the clue is on Adam initilization, but it shoud not. \r\n\r\n2018-08-06 06:54:03.420400: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 61.04GiB.  Current allocation summary follows.\r\n\r\n\r\n2018-08-06 06:54:03.422814: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *___________________________________________________________________________________________________\r\n2018-08-06 06:54:03.423826: W tensorflow/core/framework/op_kernel.cc:1295] OP_REQUIRES failed at constant_op.cc:75 : Resource exhausted: OOM when allocating tensor of shape [32000000,512] and type float\r\n2018-08-06 06:54:45.899764: E tensorflow/core/common_runtime/executor.cc:696] Executor failed to create kernel. Resource exhausted: OOM when allocating tensor of shape [32000000,512] and type float\r\n [[Node: block_output_fc1/kernel/Adam/Initializer/zeros = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [32000000,512] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]", "@IvanZhangDoIt \r\nHi, Ivan\r\n\r\nI have solved my problem, you can try the following:\r\n1. Kill all the running process, to make sure GPU have suffice memory. use command \"nvidia-smi\" see the running process, and use command \"kill -9 id\" to kill it.\r\n2. Make sure your network not very large,  Check if there is an oversized fully connected layer.\r\n3. Checking if float64 is used will double your memory.\r\n4. Check if the adam/RMSprop optimization algorithm is used. These optimization algorithms will record historical gradients and will double the memory usage.\r\n\r\nYaohua Guo\r\nwechat: guoyaohua167", "I have the same issue.the reason is that I didn't install CPU acceleration. Don't use \"pip install tensorflow\", use \"conda install tensorflow\". @reinaldocmendes ", "@guoyaohua \r\n\u6211\u4e5f\u9047\u5230\u4e86\u7c7b\u4f3c\u95ee\u9898\u3002\u4e0d\u8fc7\u662f\u81ea\u5df1\u642d\u7684\u7f51\u7edc\uff1a\u7528\u7684\u662ftf.layers\uff0ctf.float32 \u7f51\u7edc\u6df1\u5ea6150\u591a\u5c42\uff0c\u4f46freeze\u4e86\u524d100+\u5c42\uff0c\u53eatrain\u6700\u540e21\u5c42\uff0c\u4f18\u5316\u5668\u662fGradientDescentOptimizer\uff0c\u4e00\u8fd0\u884ctrain\u5c31\u5361\u6b7b\uff0c\u88ab\u8fd9\u4e2a\u95ee\u9898\u56f0\u6270\u4e86\u5f88\u4e45\u4e86\u3002\u7f51\u4e0a\u6709\u7528tf.keras\u6784\u5efa\u7c7b\u4f3c\u6df1\u5ea6\u5377\u79ef\u5c42\uff0c\u6211\u8bd5\u8fc7\u53ef\u4ee5\u6b63\u5e38\u8bad\u7ec3\u3002\r\n\r\n-------------------------------------------------------------------------------------------------------------------\r\none year later, add translation\uff1a\r\nsame issue. I rewrite a network with tf.layers and tf.float32. The framework totally has 150 layers, and during training, the backbone is freezed(about 120 layers). Only the remaining 21 layers are trained. However, when i started to train, it came to OOM.  However, the same framework implemented by tf.keras works well. \r\n\r\nadd solution:\r\nI re-implemented the framework, with tf.nn.layer. It work well. The tips may be as following: \r\n1.Try to aviod adding op in graph  \r\n2.Try to pre-process dataset with beautiful format.\r\n3. Also, try to implent by other package methods, such as tf.keras ", "> I have the same issue.the reason is that I didn't install CPU acceleration. Don't use \"pip install tensorflow\", use \"conda install tensorflow\". @reinaldocmendes\r\n\r\nHi @elihuan1990, I installed the tensorflow from the source code, I am not using the precompiled version of the tensorflow. I have a very large feature vector such as input in my neural network (autoencoder).", "It is just a warning, and it does not matter, so just be easy. At last, you still can get normal output.\r\n", "Same problem here.\r\nAnd my program got killed soon after.", "I had a similar problem: the message about exceeding 10% of system memory and then `Killed` message and the program exiting. I was running tensorflow program on a very low-end machine (Ubuntu 18.04 on t2.micro on AWS with 1GB RAM). What helped in my case was realizing that it had no swap file configured by default. After adding the swap file the problem was gone. I'm not sure if that's connected to everyone else's problems but maybe it'll help someone.", "If you are using the tensorflow with Docker containers, then I was able to fix this problem by increasing the container memory. \r\n\r\nI have posted the answer here on stack overflow: \r\nhttps://stackoverflow.com/a/55246936/565223 \r\n\r\nHope this helps\r\n", "> I had a similar problem: the message about exceeding 10% of system memory and then `Killed` message and the program exiting. I was running tensorflow program on a very low-end machine (Ubuntu 18.04 on t2.micro on AWS with 1GB RAM). What helped in my case was realizing that it had no swap file configured by default. After adding the swap file the problem was gone. I'm not sure if that's connected to everyone else's problems but maybe it'll help someone.\r\n\r\nHey @lendrom, I am a beginner in using EC2. I happen to be using the same configuration as yours and i am getting the same error. Could you tell where and how did you add the swap file? And what to add in it?  Thanks in advance.", "I believe `Killed` is being experienced due to kernel's Out of Memory Killer. Resolving this via introducing swap may yield major slowdown (as I/O from disk is slower than from memory), so consider resizing your EC2 to have more main memory. But if you still want to introduce swap, you can do that by [following the steps here](https://linuxize.com/post/create-a-linux-swap-file/).\r\n\r\n> Swap is a space on a disk that is used when the amount of physical RAM memory is full. When a Linux system runs out of RAM, inactive pages are moved from the RAM to the swap space.", "> The same problem with ssd_mobilenet_v1_coco and ssd_mobilenet_v2_coco. I do not have any problem with faster_rcnn_inception_v2.\r\n> \r\n> EDIT: Ok my solution was to set batch_size: 1 (line 141 of config file) instead of batch_size: 37 in ssd_mobilenet_v1_coco.config file or other config file.\r\n> \r\n> **I suppose that solution for this problem in the main post is to change batch_size = 32, to some smaller value. :)**\r\n\r\nIn my case, batch_size was 128 and i reduce it to 20 and it seems running now. Hope i wouldn't have this problem again till my training ends", "same issue in my windows os environment,but OK in my mac,this is odd", "> same issue in my windows os environment,but OK in my mac,this is odd\r\n\r\nmy windows computer have 16GB RAM and my mac just have 8GB RAM", "@reinaldocmendes  I think you should try to reduce your batch size, because after some time probably  the process will be killed ", "- have the same issue with log ``` W tensorflow/core/framework/allocator.cc:108] Allocation of 1966080000 exceeds 10% of system memory```\r\n- how to solve it ?\r\n- **need help**\r\n", "> I am currently having the exact same issue.\r\n> 2018-04-27 12:00:03.250952: W tensorflow/core/framework/allocator.cc:101] Allocation of 439808000 exceeds 10% of system memory.\r\n> Killed\r\n\r\nI do have the same issue", "@LIUHUANUCAS Tensorflow uses too much memory, you have to find a way to reduce it. You can reduce your batch size for example. If it doesn't work, try @guoyaohua method.", "I think I found what might have caused the problem. \r\nI recently messed with a variational autoencoder I found online that was using the MNIST dataset (implemented in Keras). Running the script worked perfectly, but I got the warning when I changed it to fit my own data. My image data (which are League of Legends champion splash art) has dimensions of 300 x 210 pixels, whereas the MNIST dataset has dimensions of 28 x 28 pixels. It could be that your dataset is too big, which is why Tensorflow is using a lot of memory. As the answers stated above, try to get Tensorflow to use more than 10% of your memory (batch size or otherwise). ", "So much information and no mention of TF version..\r\nI tried to move from TF 1.13 to 1.14 and I get multiple problems. Nothing what worked in 1.13 works in 1.14..\r\nFor instance - i get the discussed warning on 10% memory use in conjunction with various errors when I try model.fit.. It complains on data types, i change data types it says Session bla-bla not found and some other error.\r\n\r\nTF 2.0 is in beta and has no TPU support..\r\n\r\nIdk - having billions of pounds in cash and not able to release a working version for more than a year.. tut tut google..", "I have the same issue \"2019-10-01 16:56:36.080697: W tensorflow/core/framework/allocator.cc:107] Allocation of 135475200 exceeds 10% of system memory.\"", "> NO\uff0cI just found what's the problem is. May be your model contains too much variables , such as a big Dense layer (FC) , and check out your data type, float64 is lager than float32, and last , some optimizer will make this problem too, such sa adam , it will allocate double memory of model size, because it will save the history grads.\r\n\r\nGot it. Thanks!", "> @reinaldocmendes I think you should try to reduce your batch size, because after some time probably the process will be killed\r\n\r\nmy batch size is 1 but still i faces this problem.\r\n", "same issue and after trying all the way below, i found it work if I close Chrome...", "> I had a similar problem: the message about exceeding 10% of system memory and then `Killed` message and the program exiting. I was running tensorflow program on a very low-end machine (Ubuntu 18.04 on t2.micro on AWS with 1GB RAM). What helped in my case was realizing that it had no swap file configured by default. After adding the swap file the problem was gone. I'm not sure if that's connected to everyone else's problems but maybe it'll help someone.\r\n\r\nThank you, this worked wonders", "The solution quoted by @tash149 works like a charm! I'm training YOLOv3 with TF2 on AWS, using a NVIDIA T4.\r\n\r\nDetails on how to set it up here: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/", "you have to reduce the number of parameters", "@lendrom message quoted below helped me:\r\n\r\n> I had a similar problem: the message about exceeding 10% of system memory and then `Killed` message and the program exiting. I was running tensorflow program on a very low-end machine (Ubuntu 18.04 on t2.micro on AWS with 1GB RAM). What helped in my case was realizing that it had no swap file configured by default. After adding the swap file the problem was gone. I'm not sure if that's connected to everyone else's problems but maybe it'll help someone.\r\n\r\nI had the same `exceeds 10% of system memory.` warning after which my application was crashing. \r\nI followed this instructions: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/ to add a swap file to the t2.micro instance I was using.\r\nNow I still get the same warning, but the application does not crash.\r\n\r\n\r\n", "For me, this fixed the issue:\r\n\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n\r\nPut it at the beginning of your code, before you use any tensorflow.", "Im having this issue running a jupyter notebook, the kernel connection then breaks. Is there any way to increase the memory allocated to tensorflow?\r\nIm already at batch_size=1 and enabled memory growth on the GPU. \r\nBTW, is this the GPU memory that is exceeded or is it the RAM memory?\r\n\r\n**EDIT:** this always happens when I try to enumerate through a tensorflow Dataset object (a huge one though, holding millions of samples). How can I prevent this?\r\n\r\n**EDIT 2:** I always ran into this issue while trying to enumerate/fit a tf dataset holding more than a certain amount of samples (in my case 2 million). This always cuased my jupyter kernel to break. Once I reduced, I didn't occur this issue anymore.", "> same issue and after trying all the way below, i found it work if I close Chrome...\r\n\r\nit works for me too. I wonder why...", "> > same issue and after trying all the way below, i found it work if I close Chrome...\r\n> \r\n> it works for me too. I wonder why...\r\n\r\nChrome eats up a lot of memory, so maybe that's the reason. If you close Chrome, you have more memory left for Tensorflow.\r\n", "Try this https://github.com/OzzyProjects/Tensorflow", "> > > same issue and after trying all the way below, i found it work if I close Chrome...\r\n> > \r\n> > \r\n> > it works for me too. I wonder why...\r\n> \r\n> Chrome eats up a lot of memory, so maybe that's the reason. If you close Chrome, you have more memory left for Tensorflow.\r\n\r\nbut my device monitor says only 38% memory is used", "> > same issue in my windows os environment,but OK in my mac,this is odd\r\n> \r\n> my windows computer have 16GB RAM and my mac just have 8GB RAM\r\n\r\nat my Mac 2021-04-02 12:45:20.785361: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-04-02 12:45:20.788755: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2904000000 Hz\r\nKilled\r\n\r\nmy Mac has 16G RAM\r\nwould you mind sharing your envs?\r\nmy tf is 2.4\r\n\r\nthx", "I have 64G RAM and i9 10900k and RTX 2080 Ti but despite it all, i had this problem with tensorflow before i use the script. Weird too because on my portable i have an i7, 8go ram and i'v never had any issue with tensorflow, running the same script.\r\ntry installing tensorflow 1.14", "This is a huge problem for me right now, I cant even finish a single epoch. ", "Facing this problem in Linux and not in Windows 10.", "OK, it just a warning which notice you that the memory tensorflow try to allocate is bigger than 10% of your free memory.\r\nIt doesn't mean tensorflow failed to get the memory it want.  When this warning happened , check your free memory, if it is bigger than the number which tensorflow warning, you don't need to care about it.  If you really care, close other application or add physical memory.\r\n\r\nAt lots of time, problem is caused by arrogance. Those 'smart' guys feel shame to communication with others equally.", "swapfile worked for me, thanks (had to add 10G!!)", "\u60a8\u597d\uff0c\u6211\u5df2\u6536\u5230\u60a8\u7684\u90ae\u4ef6\u5e76\u4f1a\u5c3d\u5feb\u5904\u7406\uff0c\u795d\u60a8\u5de5\u4f5c\u987a\u5229\uff0c\u751f\u6d3b\u6109\u5feb\uff01\u2014\u2014\u5218\u6e90", "I have the same issue on ubuntu desktop 20, tensorflow 2, 8gb ram, 4 cores", "Same problem here in jupyter notebook, even after increasing swap size to 10GB (I checked and it does not use it all), it's sometimes fixed if I restart the kernel, it feels like there might be some \"garbage\" left/cached or something that gets cleaned after restarting, does that make sense?", "@raquelhortab I'm also getting the issue in jupyter notebook.", "\u60a8\u597d\uff0c\u6211\u5df2\u6536\u5230\u60a8\u7684\u90ae\u4ef6\u5e76\u4f1a\u5c3d\u5feb\u5904\u7406\uff0c\u795d\u60a8\u5de5\u4f5c\u987a\u5229\uff0c\u751f\u6d3b\u6109\u5feb\uff01\u2014\u2014\u5218\u6e90", "Thanks you. I'll stay in touch. ", "In my case, I was running jupyter notebooks inside of a docker container, where the docker host machine was configured to allow only up to 2GB of memory usage. I did a vanilla install of jupyter via `pip` and was able to circumvent the error."]}, {"number": 18735, "title": "tensorflow/core/framework/allocator.cc:101] Allocation of X exceeds 10% of system memory.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nMy server has 32gb of RAM, but tensorflow uses only 10% of that memory.\r\nIt returns the message:\r\n<<tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.>>\r\n\r\nIs it possible to increase this percentage?\r\n\r\n### Source code / logs\r\n### SOURCE CODE:\r\nimport keras\r\nfrom keras import regularizers\r\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, add\r\nfrom keras.models import Model\r\nfrom keras.layers.core import Layer, Dense, Dropout, Activation, Flatten, Reshape\r\nfrom keras.regularizers import l2\r\nfrom keras.utils import np_utils\r\nfrom keras.callbacks import TensorBoard\r\nfrom sklearn.model_selection import train_test_split\r\nimport numpy as np\r\nimport h5py\r\nimport time\r\n\r\nfile_train = 'features/files_train_test/train_inceptionv3_doc2vec.npy'\r\nfile_test = 'features/files_train_test/test_inceptionv3_doc2vec.npy'\r\nx_train = np.load(file_train)\r\nx_test = np.load(file_test)\r\n\r\nprint('shape train: ',x_train.shape,'shape test: ', x_test.shape)\r\nprint('size train: ', len(x_train), 'size test: ', len(x_test))\r\nprint('prod train: ',np.prod(x_train.shape[1:]), 'prod test: ', np.prod(x_test.shape[1:]))\r\n\r\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\r\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\r\n\r\nprint('shape train: ',x_train.shape,'shape test: ', x_test.shape)\r\n\r\nepochs = 10\r\nbatch_size = 32\r\ninput_size = x_train.shape[1]\r\noutput_size = x_train.shape[1]\r\nhidden_size = x_train.shape[1]\r\nfile_name = 'features/files_reduce/sparse/autoencoder_inceptionv3_doc2vec_'\r\n\r\nx = Input(shape=(input_size,))\r\nh = Dense(hidden_size, activation='relu', activity_regularizer=regularizers.l1(10e-5))(x)\r\nr = Dense(output_size, activation='sigmoid')(h)\r\nautoencoder = Model(inputs=x, outputs=r)\r\nautoencoder.compile(optimizer='adam', loss='mse')\r\nautoencoder.summary()\r\nautoencoder.fit(x_train\r\n                ,x_train\r\n                ,batch_size=batch_size\r\n                ,epochs=epochs\r\n                ,verbose=1\r\n                ,validation_data=(x_test, x_test))\r\n\r\nautoencoder.save(file_name+name_full)\r\nprint('Save encode' + file_name+name_full)\r\n\r\n===============================================================================\r\nOUTPUT: \r\n\r\nUsing TensorFlow backend.\r\nshape train:  (21248, 49452) shape test:  (5313, 49452)\r\nsize train:  21248 size test:  5313\r\nprod train:  49452 prod test:  49452\r\nshape train:  (21248, 49452) shape test:  (5313, 49452)\r\n\r\nLayer (type)                      | Output Shape              | Param    \r\n- - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - -  - - - - - - - -  - - - - - -\r\ninput_1 (InputLayer)         | (None, 49452)             | 0         \r\ndense_1 (Dense)              | (None, 49452)             | 2445549756\r\ndense_2 (Dense)              | (None, 49452)             | 2445549756\r\n- - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - - - -  - - - - - - - -  - - - - - -\r\n\r\nTotal params: 4,891,099,512\r\nTrainable params: 4,891,099,512\r\nNon-trainable params: 0\r\n\r\nTrain on 21248 samples, validate on 5313 samples\r\nEpoch 1/10\r\n2018-04-20 11:22:08.069764: W tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.\r\n2018-04-20 11:22:12.089390: W tensorflow/core/framework/allocator.cc:101] Allocation of 9782001216 exceeds 10% of system memory.", "comments": []}, {"number": 18734, "title": "tensorflow::ops::SoftmaxCrossEntropyWithLogits has apparently backwards order to its public attributes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS 10.13.3 clang 900.0.39.2 and CentOS Linux 7 gcc-4.8.5\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nmaster\r\n\r\n- **TensorFlow version (use command below)**:\r\nI have not actually installed the python pip package, but the source tree is a current checkout of master.\r\n\r\n- **Python version**: \r\nN/A using the C++ API\r\n\r\n- **Bazel version (if compiling from source)**:\r\nmacOS Build label: 0.11.1-homebrew and Centos Linux 7 Build label: 0.11.1- (@non-git)\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nmacOS clang 900.0.39.2 and CentOS Linux 7 gcc-4.8.5\r\n\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n\r\n- **GPU model and memory**:\r\nN/A\r\n\r\n- **Exact command to reproduce**:\r\nPlease read the description below\r\n\r\n### Describe the problem\r\nWhen trying to track down why code in pull request #14727 is failing an issue came to light. The gradient testing code in the C++ API would not correctly test C++ gradient code that had been shown to be a faithful reproduction of the python equivalent. It seems that the public attributes \"loss\" and \"backprop\" are reversed from their operation output order. @suharshs asked that a new issue be created to track this problem.  \r\n\r\n### Source code / logs\r\nTake a look at the comments in pull request #14727 \r\n", "comments": ["Can we close this?", "@bhack Yes! You can close this as it was addressed in #20763"]}, {"number": 18733, "title": "How can we handle resource exhaust error coming while allocating a tensor of a particular size.", "body": "I am currently using single GPU : GEFORCE GTX1080 8GB and I am unable to handle an error coming while I am trying to train a network. Error is :\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[131072,2048]\r\n[[Node: training/Adam/mul_3 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Adam/beta_1/read, training/Adam/Variable/read)]]\r\n[[Node: loss/mul/_179 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_697_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\r\n\r\nHowever, I understand that I have limited memory in GPU which has to be utilized. I currently just wanting to know if there is any workaround of this by which I can train the network without reducing the dimensions of the network.\r\n\r\nModel :\r\n\r\n    model = Sequential()\r\n    model.add(TimeDistributed(Flatten(), input_shape = (24,8,8,2048)))\r\n    model.add(Dropout(0.5))\r\n    model.add(LSTM(512, return_sequences=True, dropout=0.5))\r\n    model.add(LSTM(512, return_sequences=False, dropout=0.5))\r\n    model.add(Dense(512, activation='relu'))\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(256, activation='relu'))\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(5, activation='softmax'))\r\nInput to this network is actually image embedding I am getting from InceptionV3 without top.\r\n\r\nAny help in this regard is appreciated.\r\n\r\nThanks in advance.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It's a huge tensor you try to allocate. If we assume that you use a data type which needs 32bit for each value, your complete tensor needs 131072*2048*32bit/1024/1024/1024=8GB of memory on your GPU. Can you reduce some dimension like the embedding size or the time steps?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18731, "title": "TfLite toco: fail kTensorflowMerge", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 18.04 Beta\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nMaster\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n0.10.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc 4.8.5\r\n- **CUDA/cuDNN version**:\r\n9.1 / 7.0.5\r\n- **GPU model and memory**:\r\nGTX 1070 Ti 6go & 16Go RAM\r\n- **Exact command to reproduce**:\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \r\n--input_file=frozen_graph.pb \r\n--input_format=TENSORFLOW_GRAPHDEF \r\n--output_format=TFLITE \r\n--output_file=/tmp/test.tflite \r\n--inference_input_type=FLOAT\r\n--input_arrays=input_image\r\n--output_arrays=output_corner,output_mask\r\n--input_shapes=1,512,512,3\r\n\r\n### Describe the problem\r\n\r\nI'm working on a FCN. I want to export my frozen graph to TfLite graph but it's not working. I'm wondering why, but I guess it's just due to the lack of maturity of TfLite (and probably some operator compatibility). In my case, it's about the operator \"Merge\". Someone can explain me if there is a way/trick to make it work ?\r\n\r\n### Source code / logs\r\n\r\n```\r\n2018-04-20 12:57:33.287154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: LogicalOr\r\n2018-04-20 12:57:33.302765: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 518 operators, 965 arrays (0 quantized)\r\n2018-04-20 12:57:33.312501: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 477 operators, 902 arrays (0 quantized)\r\n2018-04-20 12:57:33.325133: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 477 operators, 902 arrays (0 quantized)\r\n2018-04-20 12:57:33.325335: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge \r\nAborted (core dumped)\r\n```\r\n\r\nI also make a summarize_graph for you, maybe it help\r\n\r\n```\r\nFound 1 possible inputs: (name=input_image, type=int32(3), shape=[1,?,?,3]) \r\nNo variables spotted.\r\nFound 2 possible outputs: (name=output_corner, op=Tile) (name=output_mask, op=Tile) \r\nFound 1471309 (1.47M) const parameters, 0 (0) variable parameters, and 51 control_edges\r\nOp types used: 239 Const, 168 Switch, 36 FusedBatchNorm, 27 Add, 22 Identity, 21 Merge, 18 Conv2D, 15 Relu, 13 StridedSlice, 12 Shape, 7 Mul, 6 Cast, 5 MaxPool, 5 Pack, 5 RealDiv, 3 Slice, 3 Conv2DBackpropInput, 2 Tile, 2 Reshape, 2 RandomUniform, 2 PlaceholderWithDefault, 2 Greater, 2 Floor, 2 ConcatV2, 1 Placeholder, 1 Maximum, 1 LogicalOr, 1 ResizeBilinear, 1 Sigmoid, 1 Softmax, 1 Squeeze, 1 ExpandDims, 1 Sub, 1 ArgMax\r\n```", "comments": ["TOCO is expecting a Switch node to be followed by a Merge node, but that doesn't seem to be the case in your model.", "Hi, \r\nThank you for your answer @andrehentz but I don't understand. You say that when I have a Switch node, I necessarily need to have a Merge node ?\r\nSo it's not good to have more Switch (168) than Merge (21) ?\r\n\r\nFor solve this, do I need to add manually a Merge node after every Switch node in my model ? I don't really know how to do that. Why TfLite don't do it by itself ?\r\n\r\nThx again for your time!", "Hi Kayoku,\r\n\r\nI encountered the same issue, would want to know how you later solved the issue.\r\n\r\nThanks", "Hi @adekunleba ,\r\nI didn't. I'm always stuck haha (I work on an other project waiting for a solution here). But if you find something, don't forget to tell me!\r\nSorry :(", "Hi @Kayoku ,\r\n\r\nTried to check the source code and found out it's one of the assertions causing the error. \r\n```\r\n// There remains to handle the edges that were pointing to the nonselected\r\n  // output. We will just discard those edges. Concretely, at the moment,\r\n  // our only examples of graphs with Switch nodes have them feeding into Merge\r\n  // nodes, so what we're saying here is that we'll make the convention,\r\n  // in our toco internal representation, that Merge nodes with only 1 input\r\n  // are Merge nodes that have been resolved already and should be have as\r\n  // Identity nodes, simply forwarding their input.\r\n  //\r\n  for (const auto& other_op : model->operators) {\r\n    auto input_it = other_op->inputs.begin();\r\n    while (input_it != other_op->inputs.end()) {\r\n      if (*input_it == switch_op->outputs[nonselected_output_index]) {\r\n        // Let us guard our assumption that only Merge nodes consume the outputs\r\n        // of Switch nodes:\r\n        CHECK(other_op->type == OperatorType::kTensorFlowMerge);  //ASSERTION CAUSING THE BOUNCE\r\n        input_it = other_op->inputs.erase(input_it);\r\n      } else {\r\n        ++input_it;\r\n      }\r\n    }\r\n  }\r\n\r\n```\r\nOne approach I have tried is to comment that out the line, but seems it's an important check for another codeline, hence it's required to pass. For now, I worked using the tfmobile approach instead, as I look futher to how my architecture can be refactored to meet the merge requirements.", "Getting the same fail. Using LSTM.\r\n\r\n2018-06-01 21:14:40.418639: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge \r\n", "Hello, getting the same fail with next models: \"ssd_inception_v2_coco_2017_11_17\", \"ssd_mobilenet_v1_coco_2017_11_17\", \"ssd_mobilenet_v2_coco_2018_03_29\" .\r\n\r\n2018-06-06 14:08:10.450529: I tensorflow/contrib/lite/toco/graph_transformations=/graph_transformations.cc:39] Before Removing unused ops: 4331 operators, 7153 arrays (0 quantized)\r\n2018-06-06 14:08:10.776917: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4276 operators, 7046 arrays (0 quantized)\r\n2018-06-06 14:08:11.215557: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4276 operators, 7046 arrays (0 quantized)\r\n2018-06-06 14:08:11.639476: F tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge \r\n", "Any solutions?", "I saw the same issue. Our models are created by keras.\r\nIt looks TOCO assumes that merge node is immediately after switch node, but in our models, there are several nodes between switch and merge.\r\n\r\nI fixed as below and it works for our models. I am not sure it also works for others' models.\r\n\r\n[resolve_tensorflow_switch.patch.zip](https://github.com/tensorflow/tensorflow/files/2097006/resolve_tensorflow_switch.patch.zip)\r\n", "I am running into the same problem with a retrained Inception V3 model. Seems like Google's own model should be able to be converted into TFLite. Hoping a solution comes from this. [This](https://github.com/tf-coreml/tf-coreml/issues/200) also happens when trying to convert from frozen pb to `.mlmodel`. ", "I'm trying to convert a custom model on inception V3 + SSD and running into the same issue. Any workarounds to get the model to tflite format until this is officially fixed?", "Same issue here... fail to convert a pretty simple mobile network.", "I have this issue when I convert networks with a placeholder as input:\r\n\r\n`is_training = tf.placeholder_with_default(False, shape=(), name=\"is_training\")`\r\n\r\nReplacing this with\r\n\r\n`is_training = False`\r\n\r\nfor inference lets me convert the model successfully. Making this manual change is not ideal though.", "@david-haber your w/a worked for me", "As I built my model with Keras and converted it into a .pb file @david-haber 's workaround doesn't apply here. Is there any other solution yet?\r\n", "Hi,\r\nThe workaround provided by @david-haber doesn't work for me. (Actually I have 4/5 tf.placeholder_with_default, and it not work even when I replace them by constants values)\r\nI try it with the new tensorflow (1.9).. Still bugged.\r\n\r\nEdit 03/08: Well, I'm a bit confused, I was looking to the file \"tensorflow/contrib/lite/toco/model.h\" for find \"kTensorflowMerge\", but it doesn't even exist... :D ", "Nagging Assignee @andrehentz: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "TensorFlow Lite doesn't support control flow ops yet (e.g. Switch, Merge, Cond...etc). The team is looking into broadening the support.\r\n\r\nIt's not possible to make Switch/Merge work properly with dynamic RNNs.\r\n\r\nTFLite already implemented fused RNN / LSTM ops. However, there is no easy way to produce these TFLite ops through our converter. The team is currently working on a better way to convert these ops. Stay tuned.\r\n\r\n====\r\n\r\n> TOCO is expecting a Switch node to be followed by a Merge node, but that doesn't seem to be the case in your model.\r\n\r\nIn addition, TOCO can only resolve Switch/Merge in a very specific case (the predicate of Switch need to be constant). \r\nTherefore dynamic RNN won't work, because it requires real control flow when running the model. \r\n\r\n> I was looking to the file \"tensorflow/contrib/lite/toco/model.h\" for find \"kTensorflowMerge\", but it doesn't even exist... :D\r\n\r\nIt seems kTensorflowMerge was renamed to kMerge. \r\n", "Anyone got a workaround for this? I am having this problem with mobilenet v2 keras", "@arpitkh96 The recommended option is to avoid using Merge in your graph. As far as I know, MobileNetV2 (classification) works with TFLite. Can you file a separate bug with the reproducible steps that are leading to this error. Include the command you are using for converting the model and attach the model.\r\n\r\nI am closing the original bug due to lack of recent activity in relation to the original user's problem. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@gargn This should suffice. https://github.com/arpitkh96/KerasMobilnet/blob/master/Untitled.ipynb\r\nModel is simple enough. Focus on the last two cells.\r\nI'll try to open separate issue when I'll have time", "@arpitkh96 One quick suggestion is to make sure that you are not passing in a training model by setting `keras.backend.set_learning_phase(False)`.\r\n\r\nAdditionally, `TocoConverter` has a `from_keras_model_file` function that makes it easier to do what you are doing, so I suggest using that. It is broken in 1.10, however, it has been fixed on the tf-nightly and in the 1.11 release.", "@gargn  learning phase and tf nightly finally fixed it. Thanks. BTW I was not loading from file because relu6 isn't recognized even after using CustomObjectScope", "I don't think this issue has been resolved yet. The current version on master does not allow me to convert models with placeholder_with_default ops in the graph.\r\n\r\n> TOCO is expecting a Switch node to be followed by a Merge node, but that doesn't seem to be the case in your model.\r\n\r\n@andrehentz Do we still want to impose this restriction? Otherwise I could implement a fix that allows us to convert placeholder_with_default ops.\r\n", "I think there is partial support in TOCO for converting placeholder_with_default. The SwitchMerge restriction has to do with TF Lite not supporting control flow, right?", "initially i was getting this error \" Check failed: IsConstantParameterArray(*model, bn_op->inputs[1]) && IsConstantParameterArray(*model, bn_op->inputs[2]) && IsConstantParameterArray(*model, bn_op->inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.\" \r\n\r\nThen i changed is_training = tf.placeholder_with_default(False, shape=(), name = \"batchnormswitch\")  prameter to False. Since then i am getting \" tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_switch.cc:95] Check failed: other_op->type == OperatorType::kTensorFlowMerge \r\nAborted (core dumped)\r\n\r\nNone\"\r\n\r\nKindly help. Thanks in advance.", "Anybody have any luck with this?", "@sanoguch Where exactly should we add the patch you have given? Which files specifically?", "I think it is tensorflow/lite/toco/graph_transformations/resolve_tensorflow_switch.cc could be replaced with this one.\r\n\r\n[resolve_tensorflow_switch.zip](https://github.com/tensorflow/tensorflow/files/3210585/resolve_tensorflow_switch.zip)\r\n\r\nActually we gave up using TOCO or even TensorFlow Lite, so I am not actively following this issue. I am not sure it works or not.", "After applying the patch (https://github.com/tensorflow/tensorflow/issues/18731#issuecomment-396843328) I get the error below:\r\n\r\n```\r\nTOCO failed. See console for info.\r\n2019-09-02 16:05:31.485705: F tensorflow/lite/toco/graph_transformations/propagate_array_data_types.cc:257] Check failed: op->inputs.size() > 0 (0 vs. 0)\r\nAborted (core dumped)\r\n```\r\n\r\nAlso when compiling I had to change `kTensorFlowMerge` to `kMerge`\r\n", "Has someone  resolved this issue?, I want to convert the ssd_resnet50_v1 and ssd_inception_v2_coco and both of them throw **the OperatorType::kMerge Found StridedSlice as non-selected output from Switch, but only Merge supported.**"]}, {"number": 18730, "title": "[tf.data] Check in a strictly faster rejection resampling transformation.", "body": "This transformation is faster because it rejects fewer data. This\r\nis done by occasionally sampling from the original data distribution\r\nin an efficient way.\r\n\r\nTested:\r\nbazel test :resample_test", "comments": ["@joel-shor Could you address the comments and resolve the conflicts?", "Applied changes.\r\n\r\nNote that I also added one bit of extra logic, which short-circuits the interleaving (making it faster) and avoiding a bug in one of those edge cases.", "Done! PTAL\n\nOn Fri, Apr 27, 2018, 2:41 AM Derek Murray <notifications@github.com> wrote:\n\n> *@mrry* requested changes on this pull request.\n>\n> Just a couple of minor changes left.\n> ------------------------------\n>\n> In tensorflow/contrib/data/python/ops/resampling.py\n> <https://github.com/tensorflow/tensorflow/pull/18730#discussion_r184559610>\n> :\n>\n> >\n>    return _apply_fn\n>\n>\n> -def _calculate_acceptance_probs(initial_probs, target_probs):\n> -  \"\"\"Calculate the per-class acceptance rates.\n> +def _get_prob_original_static(initial_dist_t, target_dist_t):\n> +  \"\"\"Returns the static probability of sampling from the original.\n> +\n> +  For some reason, `tensor_util.constant_value(prob_of_original)` of a ratio\n>\n> For some reason, [...].\n>\n> The reason is that tensor_util.constant_value() only implements a handful\n> of ops, and returns None when it encounters an op that it hasn't\n> implemented. Please update the comment to make it seem less mysterious!\n> ------------------------------\n>\n> In tensorflow/contrib/data/python/kernel_tests/resample_test.py\n> <https://github.com/tensorflow/tensorflow/pull/18730#discussion_r184559789>\n> :\n>\n> >\n> -  def _testDistribution(self, initial_known):\n> +  with test_obj.test_session() as sess:\n> +    start_time = time.time()\n> +    for _ in xrange(num_to_sample):\n> +      sess.run(get_next)\n> +    end_time = time.time()\n> +\n> +  return end_time - start_time\n> +\n> +\n> +class ResampleTest(test.TestCase, parameterized.TestCase):\n> +\n> +  @parameterized.named_parameters(\n> +      (\"InitialnDistributionKnown\", True),\n>\n> Typo: \"Initialn\" -> \"Initial\"\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/18730#pullrequestreview-115782670>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFvffP_gwOxIho9ESWrlP3KG9xJD2nJWks5tslsbgaJpZM4TdR7l>\n> .\n>\n", "Looks like a legitimate failure on the Windows build. The usual cause for errors like that is the fact that `np.array([1]).dtype` (for any list of Python integers) is `int64` on Linux and `int32` on Windows. That probably means that some integer array created in the code needs to be explicitly cast to `int64`.\r\n\r\nI\u2019d start by looking at arguments to `sample_from_datasets()` although it\u2019s possible that the bug is in the Python implementation of that function.", "@mrry I think the problem was that the interleave dataset op wasn't properly added to the cmake file. PTAL when you get a chance, once the tests are completed.", "Ah, good catch! I guess the `interleave_dataset_op_test.py` was [disabled due to an unrelated bug](https://github.com/tensorflow/tensorflow/blob/d0f5bc17560fc97bcc7de9164aa3b237a8d5221d/tensorflow/contrib/cmake/tf_tests.cmake#L285) and this left a hole in the test coverage."]}, {"number": 18729, "title": "convert from .pb to .lite failed on operation conv_transpose", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS 10.12.6 (16G29)\r\n- **TensorFlow installed from (source or binary)**:\r\ntoco is built form source\r\n- **TensorFlow version (use command below)**:\r\nmaster\r\n- **Python version**: \r\nPython 3.6.1\r\n- **Bazel version (if compiling from source)**:\r\nbazel release 0.12.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**:\r\nVersion 9.2 (9C40b)\r\n- **CUDA/cuDNN version**:\r\nNO\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --input_file=model.pb \\\r\n  --output_format=TFLITE \\\r\n  --output_file=model.lite \\\r\n  --inference_type=FLOAT \\\r\n  --inference_input_type=FLOAT \\\r\n  --input_arrays=data \\\r\n  --output_arrays=prob \\\r\n  --input_shapes=1,480,480,3\r\n### Describe the problem\r\nI want to convert a trained model to lite and run it on iPhone,I use command above to convert but it faided:\r\n\r\n```\r\n2018-04-20 17:33:52.204327: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-20 17:33:52.204482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: SparseTensorDenseAdd\r\n2018-04-20 17:33:52.204515: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: GatherNd\r\n2018-04-20 17:33:52.204623: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: SparseTensorDenseAdd\r\n2018-04-20 17:33:52.227224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: ScatterNd\r\n2018-04-20 17:33:52.238603: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1268] Converting unsupported operation: ScatterNd\r\n2018-04-20 17:36:13.632280: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 63715 operators, 127007 arrays (0 quantized)\r\n2018-04-20 17:38:38.230277: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 63715 operators, 127007 arrays (0 quantized)\r\n2018-04-20 17:41:43.014435: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:252] Check failed: weights_shape.dims(0) == 1 && weights_shape.dims(3) == 1 TransposeConv weights dimensions must begin and end with 1. Input weights \"bottle4_1_conv/weights/read_transposed\" had shape [ 16, 4, 4, 16 ].\r\n```\r\n\r\nthe error information is confused, bottle4_1_conv is a layer using conv_transpose, but it's weights has shape [4,4,16,16] and I run the .pb model in python ,the result is correct, so any idea?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nGPU model and memory", "I have updated the GPU model and memory information,  it is not relevant", "the same problem...", "@petewarden can you comment or reassign? Thanks!", "Is there some process? thx!", "@asdemon I had the same issue, but it seems to be resolved in the latest master.", "@asdemon, did @maksimgusarovsc's suggestion of using the latest solve your problem?", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18728, "title": "Typo?", "body": "I think it's Typo...right?", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 18727, "title": "[tflite] add profiling to label_image for tflite ", "body": "use the profiling mechanism in dfae914b\r\n\r\nbuild with something like:\r\n```\r\nbazel build --config android_arm64 \\\r\n  --cxxopt=-std=c++11 \\\r\n  --cxxopt=-DTFLITE_PROFILING_ENABLED \\\r\n  //tensorflow/contrib/lite/examples/label_image:label_image\r\n```\r\n\r\nrun `label_image` will get something like:\r\n```\r\n./label_image -p 1\r\nLoaded model ./mobilenet_quant_v1_224.tflite\r\nresolved reporter\r\ninvoked\r\naverage time: 67.227 ms\r\n    13.349, Node   0, OpCode   3, CONV_2D\r\n     6.024, Node   1, OpCode   4, DEPTHWISE_CONV_2D\r\n    11.847, Node   2, OpCode   3, CONV_2D\r\n     3.927, Node   3, OpCode   4, DEPTHWISE_CONV_2D\r\n     1.905, Node   4, OpCode   3, CONV_2D\r\n     3.573, Node   5, OpCode   4, DEPTHWISE_CONV_2D\r\n     2.344, Node   6, OpCode   3, CONV_2D\r\n     0.964, Node   7, OpCode   4, DEPTHWISE_CONV_2D\r\n     1.224, Node   8, OpCode   3, CONV_2D\r\n     1.846, Node   9, OpCode   4, DEPTHWISE_CONV_2D\r\n     2.181, Node  10, OpCode   3, CONV_2D\r\n     0.454, Node  11, OpCode   4, DEPTHWISE_CONV_2D\r\n     0.997, Node  12, OpCode   3, CONV_2D\r\n     0.865, Node  13, OpCode   4, DEPTHWISE_CONV_2D\r\n     1.844, Node  14, OpCode   3, CONV_2D\r\n     0.753, Node  15, OpCode   4, DEPTHWISE_CONV_2D\r\n     1.724, Node  16, OpCode   3, CONV_2D\r\n     0.803, Node  17, OpCode   4, DEPTHWISE_CONV_2D\r\n     1.698, Node  18, OpCode   3, CONV_2D\r\n     0.794, Node  19, OpCode   4, DEPTHWISE_CONV_2D\r\n     1.754, Node  20, OpCode   3, CONV_2D\r\n     0.798, Node  21, OpCode   4, DEPTHWISE_CONV_2D\r\n     1.704, Node  22, OpCode   3, CONV_2D\r\n     0.204, Node  23, OpCode   4, DEPTHWISE_CONV_2D\r\n     0.983, Node  24, OpCode   3, CONV_2D\r\n     0.373, Node  25, OpCode   4, DEPTHWISE_CONV_2D\r\n     1.791, Node  26, OpCode   3, CONV_2D\r\n     0.067, Node  27, OpCode   1, AVERAGE_POOL_2D\r\n     0.388, Node  28, OpCode   3, CONV_2D\r\n     0.001, Node  29, OpCode  22, RESHAPE\r\n     0.035, Node  30, OpCode  25, SOFTMAX\r\n0.600: 458 bow tie\r\n0.365: 653 military uniform\r\n0.008: 835 suit\r\n0.008: 611 jersey\r\n0.004: 514 cornet\r\n```", "comments": ["Not sure this is an expected error for this change?\r\n\r\n```\r\n2018-04-26 14:22:17.797851: F tensorflow/python/lib/core/bfloat16.cc:664] Check failed: PyBfloat16_Type.tp_base != nullptr\r\n```", "@martinwicke: I didn't have such problem on Android devices. How to produce that?"]}, {"number": 18726, "title": "[INTEL MKLDNN]Enable mkldnn build under MacOs and upgrade mkldnn version to v0.13", "body": "Signed-off-by: shaohua <shaohua.zhang@intel.com>", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@opencici2006 can you check your CLA status? I believe Intel has a corporate CLA, but the email address on your commits and your GitHub account must match that list.", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "@martinwicke Could you please help with the merge, if everything looks good. Thank you very much!\r\n", "Please do not merge\r\nThis fails on macos with \r\n```\r\nclang: error: unsupported option '-fopenmp'\r\n```"]}, {"number": 18725, "title": "InvalidArgumentError : restore the weight of Inception_v4", "body": "   \r\n\r\n### System information \r\n- OS Platform and Distribution : windows10\r\n- TensorFlow installed from : Source\r\n- TensorFlow version : 1.7.0\r\n- Python version: Python 3.6.3\r\n- Bazel version: 0.11.1\r\n- GCC/Compiler version : 5.4.0\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n- Exact command to reproduce:\r\n \r\n\r\n### Describe the problem\r\n\r\n  I'm training a deep neural network.I want to restore the weight of Inception_v4 ,but there are some errors.I think the define of network is right but I can't restore the weight of model.Is it a bug of different version.\r\n\r\n### The errors:\r\n\r\n InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3,3,128,768] rhs shape= 5,5,128,768, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](InceptionV4/AuxLogits/Conv2d_2a/weights, save/RestoreV2:9)]]\r\n\r\n### Source code\r\n\r\n    import scipy.misc as misc\r\n    import pickle\r\n    import tensorflow as tf\r\n    from tqdm import tqdm\r\n    import numpy as np\r\n    import argparse\r\n    import fnmatch\r\n    import sys\r\n    import os\r\n    \r\n    from nets.inception_v4 import *\r\n    sys.path.insert(0, 'nets/')\r\n    slim = tf.contrib.slim\r\n    \r\n    def getPaths(data_dir):\r\n        image_paths = []\r\n        # add more extensions if need be\r\n        ps = ['jpg', 'jpeg', 'JPG', 'JPEG', 'bmp', 'BMP', 'png', 'PNG']\r\n        for p in ps:\r\n            pattern = '*.' + p\r\n            for d, s, fList in os.walk(data_dir):\r\n                for filename in fList:\r\n                    if fnmatch.fnmatch(filename, pattern):\r\n                        fname_ = os.path.join(d, filename)\r\n                        image_paths.append(fname_)\r\n        return image_paths\r\n    \r\n    if name == 'main':\r\n        parser = argparse.ArgumentParser()\r\n        parser.add_argument('--data_dir', required=True, type=str, help='Directory images are in. Searches recursively.')\r\n        parser.add_argument('--model', required=True, type=str, help='Model to use')\r\n        parser.add_argument('--checkpoint_file', required=True, type=str, help='Model file')\r\n        a = parser.parse_args()\r\n        data_dir = a.data_dir\r\n        model = a.model\r\n        checkpoint_file = a.checkpoint_file\r\n        # I only have these because I thought some take in size of (299,299), but maybe not\r\n        height, width, channels = 224, 224, 3 \r\n        x = tf.placeholder(tf.float32, shape=(1, height, width, channels))\r\n        \r\n        arg_scope = inception_v4_arg_scope()\r\n        with slim.arg_scope(arg_scope):\r\n            logits, end_points = inception_v4(x, is_training=False, num_classes=1001)\r\n            features = end_points['PreLogitsFlatten']\r\n         \r\n        sess = tf.Session()\r\n        saver = tf.train.Saver()\r\n        saver.restore(sess, checkpoint_file)\r\n        \r\n        feat_dict = {}\r\n        paths = getPaths(data_dir)\r\n        print('Computing features...')\r\n        for path in tqdm(paths):\r\n            image = misc.imread(path)\r\n            image = misc.imresize(image, (height, width))\r\n            image = np.expand_dims(image, 0)\r\n            feat = np.squeeze(sess.run(features, feed_dict={x: image}))\r\n            feat_dict[path] = feat\r\n        \r\n        try:\r\n            os.makedirs('features/')\r\n        except:\r\n            pass\r\n        exp_pkl = open('features/' + model + '_features.pkl', 'wb')\r\n        data = pickle.dumps(feat_dict)\r\n        exp_pkl.write(data)\r\n        exp_pkl.close()\r\n    \r\n\r\n### Log\uff1a\r\n\r\n    D:\\PythonProject\\Compute-Features-WithModel>python compute_features.py --data_dir=jaffe/ --checkpoint_file=inception_v4.ckpt --model=inception_v4\r\n    Use the retry module or similar alternatives.\r\n    Traceback (most recent call last):\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\r\n        return fn(*args)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1312, in _run_fn\r\n        options, feed_dict, fetch_list, target_list, run_metadata)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1420, in _call_tf_sessionrun\r\n        status, run_metadata)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in exit\r\n        c_api.TF_GetCode(self.status.status))\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [3,3,128,768] rhs shape= [5,5,128,768]\r\n    \t\t[[Node: save/Assign_9 = Assign[T=DT_FLOAT, _class=[\"loc:@InceptionV4/AuxLogits/Conv2d_2a/weights\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](InceptionV4/AuxLogits/Conv2d_2a/weights, save/RestoreV2:9)]]\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    Traceback (most recent call last):\r\n      File \"compute_features.py\", line 120, in <module>\r\n        saver.restore(sess, checkpoint_file)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1775, in restore\r\n        {self.saver_def.filename_tensor_name: save_path})\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\r\n        run_metadata_ptr)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1140, in _run\r\n        feed_dict_tensor, options, run_metadata)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n        run_metadata)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n        raise type(e)(node_def, op, message)\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [3,3,128,768] rhs shape= [5,5,128,768]\r\n             [[Node: save/Assign_9 = Assign[T=DT_FLOAT, _class=[\"loc:@InceptionV4/AuxLogits/Conv2d_2a/weights\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](InceptionV4/AuxLogits/Conv2d_2a/weights, save/RestoreV2:9)]]\r\n    Caused by op 'save/Assign_9', defined at:\r\n      File \"compute_features.py\", line 119, in <module>\r\n        saver = tf.train.Saver()\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1311, in init\r\n        self.build()\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1320, in build\r\n        self.build(self.filename, build_save=True, build_restore=True)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1357, in _build\r\n        build_save=build_save, build_restore=build_restore)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 809, in _build_internal\r\n        restore_sequentially, reshape)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 470, in _AddRestoreOps\r\n        assign_ops.append(saveable.restore(saveable_tensors, shapes))\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 162, in restore\r\n        self.op.get_shape().is_fully_defined())\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 281, in assign\r\n        validate_shape=validate_shape)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 64, in assign\r\n        use_locking=use_locking, name=name)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n        op_def=op_def)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_opop_def=op_def)\r\n      File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in init\r\n        self.traceback = self.graph._extract_stack()  # pylint: disable=protected-access\r\n    InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3,3,128,768] rhs shape= [5,5,128,768]\r\n             [[Node: save/Assign_9 = Assign[T=DT_FLOAT, _class=[\"loc:@InceptionV4/AuxLogits/Conv2d_2a/weights\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](InceptionV4/AuxLogits/Conv2d_2a/weights, save/RestoreV2:9)]]\r\n    \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have added this information\u3002", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18724, "title": "Executor failed to create kernel Snapshot, compiled from source using contrib/makefile", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: slightly modified code of label_image/main.cc\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: linux ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: compiled from source using contrib/makefile just for CPU\r\n- **TensorFlow version (use command below)**: the latest r1.7\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**:  5.4.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nI created a c++ project slightly modified from label_image/main.cc, and compiled the code in qtcreator against -ltensorflow_cc -ltensorflow_framework (compiled from source using bazel) with no problems. Then I added all necessary ops to tf_op_files.txt and successfully compiled libtensorflow_core.a using contrib/makefile. Linking this static lib -ltensorflow_core to my code, my code returned at \r\nDiv(root, Sub(root, resized, ...), std ) (pseudo code)\r\nwith error\r\n\r\n2018-04-19 19:12:07.554566: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Not found: No registered 'Snapshot' OpKernel for CPU devices compatible with node Subtract = Snapshot[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"] (ResizeBilinear)\r\n. Registered: < no registered kernels >\r\n\r\n[[Node: Subtract = Snapshot[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"] (ResizeBilinear)]]\r\n\r\n\r\nI found a similar post https://github.com/tensorflow/tensorflow/issues/17752 to this problem, but branch r1.7 with the fix https://github.com/tensorflow/tensorflow/commit/d392b1c9ebf131b9ac64ff289d26e43afea21c10 for windows doesn't solve my problem. \r\n \r\nMy own PB model file was created a while ago using tools compiled from r1.5, will this be the problem? \r\n \r\n### Source code / logs\r\n\r\n", "comments": ["recompiled the code, code crushed at other places. close this ticket for now"]}, {"number": 18723, "title": "1", "body": "Ok", "comments": []}, {"number": 18722, "title": "How to use C++ in java", "body": "How to compile android/jni c + + files, their own so files? Beginner Android my language makes you confused, please understand", "comments": ["How to use Tracking C++ files below in my test project"]}, {"number": 18721, "title": "modify initial value of scale to zero", "body": "In Android NN's validation function, it will check operand type and scale\r\nvalue. Here is sources of validation implementation.\r\nswitch (operand.type) {\r\n     case OperandType::FLOAT32:\r\n     case OperandType::INT32:\r\n     case OperandType::UINT32:\r\n     case OperandType::TENSOR_FLOAT32:\r\n         if (operand.scale != 0.f) {\r\n             LOG(ERROR) << \"Operand \" << index << \": Operand of type \"\r\n                        << getOperandTypeName(operand.type) << \" with a non-zero scale (\"\r\n                        << operand.scale << \")\";\r\n             return false;\r\n         }\r\n         break;\r\n\r\nSigned-off-by: Tix Lo <tixlo.tw@gmail.com>", "comments": ["Ping for review @aselle ", "Nagging Reviewer @aselle: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @aselle: It has been 29 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "already fixed"]}, {"number": 18720, "title": "Problem:  Cannot merge devices with incompatible types: '/job:localhost/replica:0/task:0/device:CPU:0' and '/job:localhost/replica:0/task:0/device:GPU:0'", "body": "I get a problem. Here is the description of the error:\r\n\r\nCaused by op u'block_stack_0/block_0/short_cut/weight/Initializer/random_uniform/RandomUniform', defined at:\r\n  File \"train.py\", line 133, in <module>\r\n    train()\r\n  File \"train.py\", line 84, in train\r\n    model.get_loss(image, gt, args.block_num_channel_list)\r\n  File \"model.py\", line 145, in get_loss\r\n    self.logits = self.inference(images, block_num_channel_list=block_num_channel_list)\r\n  File \"model.py\", line 126, in inference\r\n    output = self.stack(inputs=output, output_channel=bottel_neck_channel * 4, stack_num=stack_num, strides=stride, scope='block_stack_%d' % stack_block_index, bottel_neck_channel=bottel_neck_channel)\r\n  File \"model.py\", line 118, in stack\r\n    output = self.block(inputs, bottel_neck_channel=bottel_neck_channel, output_channel=output_channel, strides=strides, is_projection=True, scope='block_0')\r\n  File \"model.py\", line 106, in block\r\n    short_cut = self.conv(output, kernel_size=1, strides=strides, output_channel=output_channel, scope='short_cut', use_bias=False, is_activate=False)\r\n  File \"model.py\", line 48, in conv\r\n    name='weight')\r\n  File \"model.py\", line 36, in get_variable\r\n    trainable=trainable)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 367, in get_variable\r\n    validate_shape=validate_shape, use_resource=use_resource)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\r\n    use_resource=use_resource)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\r\n    validate_shape=validate_shape)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 277, in _init_from_args\r\n    initial_value(), name=\"initial_value\", dtype=dtype)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 701, in <lambda>\r\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/initializers.py\", line 144, in _initializer\r\n    dtype, seed=seed)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 240, in random_uniform\r\n    shape, dtype, seed=seed1, seed2=seed2)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 247, in _random_uniform\r\n    seed=seed, seed2=seed2, name=name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot colocate nodes 'block_stack_0/block_0/short_cut/weight/Initializer/random_uniform/RandomUniform' and 'block_stack_0/block_0/short_cut/weight: Cannot merge devices with incompatible types: '/job:localhost/replica:0/task:0/device:CPU:0' and '/job:localhost/replica:0/task:0/device:GPU:0'\r\n\t [[Node: block_stack_0/block_0/short_cut/weight/Initializer/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, _class=[\"loc:@block_stack_0/block_0/short_cut/weight\"], dtype=DT_FLOAT, seed=0, seed2=0](block_stack_0/block_0/short_cut/weight/Initializer/random_uniform/shape)]]\r\n\r\n\r\n('add regulariztion, regularization decay:', 0.003)\r\nset learning rate\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 133, in <module>\r\n    train()\r\n  File \"train.py\", line 91, in train\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'block_stack_0/block_0/short_cut/cond/Switch_1': Unknown input node 'block_stack_0/block_0/short_cut/weight/Initializer/random_uniform'\r\n\r\n\r\n\r\nit seems that the error happen on the op \"tf.global_variables_initializer()\"\r\nsome code may be related to this error is : \r\n\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n\r\nwith tf.device('/cpu:0'):\r\n            var = tf.get_variable(shape=shape,\r\n                                  name=name,\r\n                                  dtype=dtype,\r\n                                  initializer=initializer,\r\n                                  regularizer=regularizer,\r\n                                  trainable=trainable)\r\n            moving_average_opt = self.moving_averager.apply([var])\r\n            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, moving_average_opt)\r\n            moving_average_variable = self.moving_averager.average(var)\r\n            tf.summary.histogram(var.name, var)\r\n            tf.summary.histogram('moving_'+ var.name, moving_average_variable)\r\n\r\n\r\nhow can i solve this problem? Thanks. \r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "From the error message, it seems there are some unsatisfiable constraints on the placement of the operations being provided.\r\n\r\nIs it possible to provide enough information to reproduce the problem so that we can determine if there is a bug in some library code, or an error in some usage of it?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I am facing the same error. \r\n\r\nTF = 2.3.0.\r\nNumber of GPUs = 3"]}, {"number": 18719, "title": "Won't build with latest version of ComputeCpp", "body": "I was just supposed to install Keras. But it failed on building Tensorflow.\r\nThe command I ran is 'bazel build --local_resources 2048,.5,1.0 -c opt --config=sycl //tensorflow/tools/pip_package:build_pip_package', as in https://www.codeplay.com/portal/03-30-17-setting-up-tensorflow-with-opencl-using-sycl .\r\nOS: Linux bckpkol-ashatan 4.13.0-38-generic #43 16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nFrom source. Git master version. ec2c66356b450fe52ec4c9a8b8c2e0e89e69f271 commit.\r\nPackage: python3\r\nVersion: 3.5.1-3\r\nPackage: bazel\r\nVersion: 0.12.0\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)\r\nNo CUDA, because I have RX 550 with working OpenCL\r\namdgpu-pro-17.40.2712-510357 driver, because latest wasn't working\r\nhttps://pastebin.com/gx67YjTU clinfo log, also gpu model and memory\r\nhttps://pastebin.com/HqgE3PG3 computecpp_info log\r\nhttps://pastebin.com/8nMXuPh2 build log (last lines from terminal)\r\n\r\nThe problem is 'error: no template named 'map_allocator' in namespace 'cl::sycl''.\r\nIt also constantly appears while running testsuite.\r\nForgot to mention - it is said that I need ComputeCpp 1.2. But minimum version on their site is 0.2.0, and I'm using 0.7.0. That might be the problem, however I don't know which version is supported at the moment.", "comments": ["Hi,\r\nAt the moment we (Codeplay) are upstreaming a lot of the changes we have been making to TensorFlow and Eigen to add the OpenCL support for AMD processors.\r\nThis means you will need to use a specific branch to try out the OpenCL support.\r\nThe instructions below are more up to date than the blog post you reference, in particular you will need to use the \"dev/amd_gpu\" branch to get the latest codebase.\r\n\r\nhttps://developer.codeplay.com/computecppce/latest/getting-started-with-tensorflow\r\n\r\nThanks,\r\nRod.", "Thanks, I'll try it.", "https://pastebin.com/mRGDxLP7 I have that error. What's wrong?\r\nADD: that https://github.com/tensorflow/tensorflow/issues/18450", "https://drive.google.com/open?id=1GJiN3JtVNBBeItLfnza-c7IYzBxH8LNL built, so closing.\r\nhttps://drive.google.com/open?id=1BusauChEAdRsBnqKrjmqSWN79w4jNvHu"]}, {"number": 18718, "title": "Bug: CPU/multinomial_op_test fails on AVX512 systems most likely due to bug in Eigen AVX512 log implementation ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n('v1.8.0-rc0-561-g075fbb59d7', '1.8.0-rc0')\r\n- **Python version**: \r\nPython 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\nNA\r\n- **GPU model and memory**:\r\nNA\r\n- **Exact command to reproduce**:\r\nbazel test --config=opt //tensorflow/python/kernel_tests/random:multinomial_op_test\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nOn machines with AVX512 instruction set, multinomial_op_test fails on CPU device. Most likely this is an issue with Eigen's AVX512 log implementation.\r\n\r\nTest passes if I disable the AVX512 plog implementation https://bitbucket.org/eigen/eigen/src/3215c06819b99ce52d5a8d6939d072024e1e3fa0/Eigen/src/Core/arch/AVX512/MathFunctions.h?fileviewer=file-view-default#MathFunctions.h-36:125 \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n  2 -----------------------------------------------------------------------------\r\n  3 ..2018-04-20 03:28:51.833924: W tensorflow/core/framework/op_kernel.cc:1290] CtxFailure at multinomial_op.cc:165: Invalid argument: num_classes should be positive, got 0\r\n ...\r\n/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/random/multinomial_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/random/multinomial_op_test.py:194: RuntimeWarning: divide by zero encountered in true_divide\r\n 12   chi2 = np.sum(diff * diff / expected, axis=0)\r\n 13 F....\r\n 14 ======================================================================\r\n 15 FAIL: testSamplingCorrectness (__main__.MultinomialTest)\r\n\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/random/multinomial_op_test.py#L44\r\n\r\ndef composed_sampler(logits, num_samples):\r\n  unif = random_ops.random_uniform(logits.get_shape().concatenate(\r\n      tensor_shape.TensorShape([num_samples])))\r\n      noise = -math_ops.log(-math_ops.log(unif)) \r\n\r\nCurrent tensorflow (for nodes highlighted in the link above):\r\nunif: [[[0.824622393 0.0375790596 0.197375655]]...]\r\nnoise: [[[inf inf inf]]...]\r\n\r\nAfter disabling Eigen's AVX512 plog implementation:\r\nunif: [[[0.824622393 0.0375790596 0.197375655]]...]\r\nnoise: [[[1.64594793 -1.1882422 -0.48405844]]...]\r\n\r\n\r\n", "comments": ["Bug fixed in Eigen: https://bitbucket.org/eigen/eigen/pull-requests/385/fix-avx512-plog/diff\r\nThanks @benoitsteiner for the quick turnaround.\r\n\r\nNeed TF to update to new version of Eigen.", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 32 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 47 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 62 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This seems to have been fixed by https://github.com/tensorflow/tensorflow/pull/20254 which updated Eigen.  The test is now passing locally for me."]}, {"number": 18717, "title": "iOS: No OpKernel was registered to support Op 'Conv2d' with these attrs", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS 10.12.6 (16G29)\r\n- **TensorFlow installed from (source or binary)**:\r\nI only build the iOS library from source\r\nwhen I trained the model, I install it use pip\r\n- **TensorFlow version (use command below)**:\r\n1.7.0\r\n- **Python version**: \r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nNot use GPU\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nbuild_all_ios.sh -g mymodel.pb\r\n\r\n\r\n### Describe the problem\r\nI use command list below to generate the ops_to_register.h file. and build the iOS library use \"build_all_ios.sh -g mymodel.pb\"\r\n```\r\n$ bazel build tensorflow/python/tools/print_selective_registration_header\r\n$ bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\r\n  --graphs=graph.pb > tensorflow/core/framework/ops_to_register.h\r\n```\r\n\r\nAfter that , I config my Xcode and run the model on an iphone, but it issues error:\r\n\r\n\r\nCould not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Conv2D' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: init_conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 4, 4, 1], use_cudnn_on_gpu=false](Pad, init_conv/weights/read)]]\r\n\r\nbut the conv*.cc is in the tf_op_files.txt,  Is there some error on my usage?\r\n", "comments": ["Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@asdemon have you solved this problem? i am facing the same problem when try to use Conv2d in iOS", "If that's still an issue you can take a look at #9073 "]}, {"number": 18716, "title": "Add the API to set the session configuration when load the model in the JAVA", "body": "See the details in the issue:\r\n[https://github.com/tensorflow/tensorflow/issues/18143]<https://github.com/tensorflow/tensorflow/issues/18143>\r\n", "comments": ["Also, it seems this PR is aiming for a merge into the 1.6 release branch, I don't think that's what you want. The PR should be for the master branch, no?", "Nagging Assignee @martinwicke: It has been 34 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@asimshankar can you take a look?", "Closing PR to the release branch.", "@raintung - could you recreate the PR for the master branch?"]}, {"number": 18715, "title": "Support string tensors for `tf.count_nonzero`", "body": "This fix tries to address the issue raised in #18712 where `tf.count_nonzero` does not support string tensors.\r\n\r\nThe reason the string tensor does not work was because `tf.count_nonzero` uses `input_tensor.dtype.as_numpy_dtype()` to created a numpy type `zero` which is passed to `tf.not_equal`. However, `as_numpy_dtype()` will convert `tf.string` to `np.object` so an exception is thrown.\r\n\r\nBut passing a numpy type `zero` to `tf.not_equal` is unnecessary as we could use `tf.zeros` instead, which works for `tf.string`\r\n\r\nThis fix fixes the issue. This fix fixes #18712.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @martinwicke for the review. The PR has been updated and example/note for tf.count_nonzero with strings has been added. Please take a look and let me know if there are any issues."]}, {"number": 18714, "title": "Branch 193610264", "body": "", "comments": ["Pushes are in.\r\ncc: @fchollet @anj-s for keras eager cherry-picks\r\n@sb2nov @chrisying for checkpoint cherry-pick"]}, {"number": 18713, "title": "Fix typo", "body": "fix typo\r\n", "comments": []}, {"number": 18712, "title": "tf.count_nonzero should support string tensors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: tf.count_nonzero(tf.constant([\"test\"]))\r\n\r\n### Describe the problem\r\ntf.count_nonzero should support string tensors, since tf.zeros and tf.zeros_like work with string tensors.\r\n\r\n### Source code / logs\r\ntf.count_nonzero(tf.constant([\"test\"]))", "comments": ["Added a PR #18715 to support `tf.string` with `tf.count_nonzero`.", "Thanks for the quick fix!"]}, {"number": 18710, "title": "Branch 193594593", "body": "", "comments": ["fyi @jlebar is looking into the failure."]}, {"number": 18709, "title": "Update install_python3.5_pip_packages.sh", "body": "", "comments": []}, {"number": 18708, "title": "Placeholder shape checking inside tf.cond not properly enforced", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tensorflow 1.7 Docker under CentOS 7.3\r\n- **TensorFlow installed from (source or binary)**: Tensorflow 1.7 Docker\r\n- **TensorFlow version (use command below)**:  1.7\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:  N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Quadro M4000\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen a placeholder is defined in a tf.cond statement, shape checking is not enforced until the merge, and ends up being only rank checking.\r\n\r\n### Source code / logs\r\n Consider the following toy graph/session, developed in the pursuit of a larger issue.  I suspect this entire approach of trying to have differently shaped placeholders is doomed to failure.  But, the graph produced clearly shows placeholders of appropriate shape inside the conditional; shows appropriately sized tensors flowing into the merge, but shows the merge itself as shape (?,?) and that is where the shape checking happens.  As a result, you can put just about any rank two tensor in and set the boolean to anything you like and tensorflow doesn't care. \r\n\r\nHeck, you can make another op completely outside the conditional to compute matmul (a, matrix_inverse(a)) and it'll chug merrily along. \r\n\r\nIs this the desired behavior?  \r\n\r\n\r\n```\r\nwith graph.as_default():\r\n    pred   = tf.placeholder(tf.bool, shape=[])\r\n\r\n    def fnTrue():\r\n        a      = tf.placeholder(tf.float32, shape=[5, 1], name=\"column\")\r\n        return a\r\n\r\n    def fnFalse():\r\n        a      = tf.placeholder(tf.float32, shape=[1, 5], name=\"row\")\r\n        return a\r\n\r\n    a = tf.cond(pred, fnTrue, fnFalse, name=\"my_conditional\")\r\ngraphWriter = tf.summary.FileWriter(logdir='logdir', graph=graph)\r\ngraphWriter.flush()\r\n\r\nwith tf.Session(graph=graph) as session:\r\n    writer_graph         = tf.summary.FileWriter(LogPath , session.graph)\r\n    writer_graph.flush()\r\n    writer_graph.close()\r\n    plTrue       = np.ones((15, 2)) * 2     # column\r\n    plFalse      = np.ones((12, 15)) * 3     # row\r\n    feed_dict = {\r\n        a            : plTrue,\r\n        pred         : False\r\n    }\r\n\r\n    d = session.run( a , feed_dict = feed_dict)\r\n    print(d)\r\n    print(d.shape)\r\n```", "comments": ["Sorry, but I'm not completely sure what you are asking. I think it is the intended behavior that the two branches of a conditional may return differently-shaped tensors. This is in general supported in TF control flow, for example loop-carried state may be of a different shape every time around a loop, e.g., a loop may add a row to a tensor each time around.\r\n\r\nI think the shape inference is working as expected, in that given the two placeholders, the 'merge' of their shapes constraints is the constraint that the output is rank 2.\r\n\r\nIf you build a conditional with branches outputting different shapes, then shape checking becomes a dynamic property that the TensorFlow interpreter will enforce, not something that is checked before the graph is run.\r\n\r\nDoes that make sense? What are you hoping for here?", "Agreed, conditionals returning different possible shapes is desirable.  That's what I was trying to do. \r\n\r\nBut surely a placeholder of shape [1,5] should not accept a tensor of shape [12,15], inside a conditional or no. ", "Sorry that I hadn't looked carefully enough at your example.\r\n\r\nThe reason for the counterintuitive behavior is that you aren't feeding either of the placeholders in your example. You are feeding 'a' in the outer scope, which is returned by the tf.cond and is I guess a Merge node (you can verify this by printing its name and looking at the graph to find it).\r\n\r\nYou could feed the placeholders e.g., by capturing them while the lambdas execute, something like:\r\n\r\n```\r\n      p_list = []\r\n\r\n      def fnTrue():\r\n        a      = tf.placeholder(tf.float32, shape=[5, 1], name=\"column\")\r\n        p_list.append(a)\r\n        return a\r\n\r\n    def fnFalse():\r\n        a      = tf.placeholder(tf.float32, shape=[1, 5], name=\"row\")\r\n        p_list.append(a)\r\n        return a\r\n ...\r\n\r\n    feed_dict = {\r\n        p_list[0]    : plTrue,\r\n        p_list[1]    : plFalse,\r\n        pred         : False\r\n    }\r\n```\r\n\r\nI think if you do that you should get the shape error from the placeholders.\r\n\r\nInstead you are feeding the Merge, which (as you observe) is only constrained to have rank 2. The most precise shape constraint on the Merge is \"either [5,1] or [1,5]\" which our shape inference system is not rich enough to encode, and I don't think it's realistic to make it encode arbitrary expressions like that, so it's probably going to stay at this level of imprecision.\r\n\r\nI wonder from your example if you think that feeding False to p should constrain the Merge to have shape [1,5]. Unfortunately it doesn't, because of the way that graph pruning works in TensorFlow. The algorithm is to look at all the fetches (in this case a) and work backwards from them only as far as necessary to evaluate them given the supplied feeds (in this case a and p). Since a can be computed given a, the pruned graph that is evaluated by the session.run() call is just the singleton node a, and the value of p is ignored. You should also know that the pruning algorithm is based on the identity of the feed and fetch nodes, not the values sent to the feeds, so in the case I gave where I fed the placeholders it would be necessary to feed both placeholders in every run: the set of feeds required to evaluate the graph is determined before the value of p is known, so it has to include both branches.\r\n\r\nThere is no doubt that the algorithm for determining which nodes are evaluated given a set of feeds and fetches has confused a lot of people. At this point, however, it's probably not going to change given the amount of code that has been built, so I hope that you can solve your problem based on this explanation of the graph pruning process.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing for now but please reopen if there's a bug that needs to be looked at."]}]