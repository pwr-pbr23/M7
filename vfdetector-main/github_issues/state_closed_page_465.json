[{"number": 39865, "title": "Loading tf saved model throws error in Keras, but works in tf", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- Python version: 2.1\r\n- CUDA/cuDNN version: Unsure\r\n- GPU model and memory:  Unsure\r\n\r\nTF: v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n\r\n\r\n**Describe the current behavior**\r\nWhen loading a model with sub/nested models in Keras it throws an error saying the Model must contain a call method. However, when loading the model directly through TensorFlow it still works.\r\n\r\n**Describe the expected behavior**\r\nThe model should successfully load.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\n# assuming a model with submodels called \"saved_model\" exists\r\nimport tensorflow as tf\r\nmodel = tf.saved_model.load('saved_model') # works\r\nmodel = keras.models.load_model('saved_model') # doesnt work\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nNotImplementedError                       Traceback (most recent call last)\r\n in \r\n      3 import tensorflow as tf\r\n      4 model = tf.saved_model.load('saved_model')\r\n----> 5 tf.keras.models.load_model('saved_model')\r\n\r\n~/.local/share/virtualenvs/model-TBy5gsp1/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)\r\n    148   if isinstance(filepath, six.string_types):\r\n    149     loader_impl.parse_saved_model(filepath)\r\n--> 150     return saved_model_load.load(filepath, compile)\r\n    151 \r\n    152   raise IOError(\r\n\r\n~/.local/share/virtualenvs/model-TBy5gsp1/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py in load(path, compile)\r\n     87   # TODO(kathywu): Add saving/loading of optimizer, compiled losses and metrics.\r\n     88   # TODO(kathywu): Add code to load from objects that contain all endpoints\r\n---> 89   model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n     90 \r\n     91   # pylint: disable=protected-access\r\n\r\n~/.local/share/virtualenvs/model-TBy5gsp1/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py in load_internal(export_dir, tags, loader_cls)\r\n    550       loader = loader_cls(object_graph_proto,\r\n    551                           saved_model_proto,\r\n--> 552                           export_dir)\r\n    553       root = loader.get(0)\r\n    554     root.tensorflow_version = meta_graph_def.meta_info_def.tensorflow_version\r\n\r\n~/.local/share/virtualenvs/model-TBy5gsp1/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py in __init__(self, *args, **kwargs)\r\n    117   def __init__(self, *args, **kwargs):\r\n    118     super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n--> 119     self._finalize()\r\n    120 \r\n    121   def _finalize(self):\r\n\r\n~/.local/share/virtualenvs/model-TBy5gsp1/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py in _finalize(self)\r\n    155           inputs, outputs, _ = network_lib.reconstruct_from_config(\r\n    156               node.get_config(),\r\n--> 157               created_layers={layer.name: layer for layer in node.layers})\r\n    158           node._init_graph_network(\r\n    159               inputs, outputs,\r\n\r\n~/.local/share/virtualenvs/model-TBy5gsp1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in reconstruct_from_config(config, custom_objects, created_layers)\r\n   1901       if layer in unprocessed_nodes:\r\n   1902         for node_data in unprocessed_nodes.pop(layer):\r\n-> 1903           process_node(layer, node_data)\r\n   1904 \r\n   1905   input_tensors = []\r\n\r\n~/.local/share/virtualenvs/model-TBy5gsp1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in process_node(layer, node_data)\r\n   1849       if not isinstance(input_tensors, dict) and len(flat_input_tensors) == 1:\r\n   1850         input_tensors = flat_input_tensors[0]\r\n-> 1851       output_tensors = layer(input_tensors, **kwargs)\r\n   1852 \r\n   1853       # Update node index map.\r\n\r\n~/.local/share/virtualenvs/model-TBy5gsp1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    771                     not base_layer_utils.is_in_eager_or_tf_function()):\r\n    772                   with auto_control_deps.AutomaticControlDependencies() as acd:\r\n--> 773                     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    774                     # Wrap Tensors in `outputs` in `tf.identity` to avoid\r\n    775                     # circular dependencies.\r\n\r\n~/.local/share/virtualenvs/model-TBy5gsp1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)\r\n    710     \"\"\"\r\n    711     if not self._is_graph_network:\r\n--> 712       raise NotImplementedError('When subclassing the `Model` class, you should'\r\n    713                                 ' implement a `call` method.')\r\n    714 \r\n\r\nNotImplementedError: When subclassing the `Model` class, you should implement a `call` method.\r\n```\r\n", "comments": ["@sinclairnick,\r\nCould you please provide the complete code or the SavedModel file you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39865\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39865\">No</a>\n"]}, {"number": 39864, "title": "Recreate Variables in tf.function", "body": "I'll preface this issue with the following:\r\n- I've thought about this for a really long time (I've watched the talks and read the docs), and I can't find any solutions, which is why I'm creating this issue\r\n- I know this is a pretty big change to the fundamental concept of \"don't create `Variable`s in `tf.function`s\" (see https://youtu.be/Up9CvRLIIIw?t=478 and https://www.tensorflow.org/guide/function#variables)\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): no\u2014this seems like it'll take a lot of time\u2014I might be able to work on this in 2 years, when I'm in college :) \r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, creating a `tf.Variable` in a `tf.function` raises a `ValueError`, for good reason: https://youtu.be/Up9CvRLIIIw?t=431. \r\n\r\nSuppose I want to run a function multiple times for benchmarking purposes (I know `tf.test` exists). If this function creates Variables, I can't run it multiple times. Instead, I need to create the variables outside of `tf.function`, and pass them to the `tf.function`. This makes for unclear and illogical code. \r\n\r\nWhat I want is to be able to **create/delete/recreate Variables in a `tf.function`**. \r\n\r\n**Will this change the current API? How?**\r\nI don't think so; I don't expect any new functions or classes to be added to the external API. \r\n\r\n**Who will benefit with this feature?**\r\nPeople who wants to run a function (with new state) multiple times\u2014e.g., benchmarking, hyperparameter optimization, etc. I realize that \"with new state\" isn't the best word choice; what I mean is I want each run to be independent, and create new `Variables`.\r\n\r\n**Any Other info.**\r\nFor the use-cases I have in mind, I think being able to delete Variables is what we need. I have no problem deleting `tf.Variable`s at the end of the `tf.function`. The reason I mention being able to create `tf.Variable`s in `tf.function`s is because I might be missing a use-case. \r\n\r\nI'm also unsure if this will help: `tf.variable_creator_scope`", "comments": ["I'm sorry for the trouble but this was an explicit API decision not allowing variable recreation in tf.function.\r\n\r\nIf you don't want to create a variable outside, there are two workarounds:\r\n\r\n```python\r\nis_created = False\r\ndef foo(x):\r\n  if not is_created:\r\n    y = tf.Variable(3.)\r\n  return x + y\r\n\r\n# Creating a new @tf.function instance\r\nfoo_tf_function = tf.function(foo)\r\n```\r\n\r\n```python\r\nis_created = False\r\n@tf.function\r\ndef foo(x):\r\n  if not is_created:\r\n    y = tf.Variable(3.)\r\n  return x + y\r\n\r\n# Calling a concrete function\r\nfoo.get_concrete_function(tf.constant(5.))(tf.constant(5.))\r\n```", "Thanks for the reply! Is it possible to add a method called `tf.reset_all_variables` that `del`s `Variable`s and removes them under the hood? IMO this would make for much cleaner and readable code (although I do recall that TF 2 is aiming to do less Variable management on its own). The variable-creations would still be in the method graph, but after each function call, they would be appropriately deleted, allowing for them to be recreated. Maybe something like https://github.com/tensorflow/tensorflow/issues/859 for TF 2?\r\n\r\nTo anyone else who's come across this thread, this might help: https://github.com/tensorflow/tensorflow/issues/27120. \r\n\r\n", "Hi\u2014I know you all are very busy, but I'd like to bump this thread. If what I described earlier doesn't make sense, I'm really just asking for:\r\n\r\n- https://www.youtube.com/watch?v=MSXouZPyTrc&feature=youtu.be&t=983\r\n\r\nAt the time of the talk, Dr. Passos mentioned that both eager and graph semantics are easy to implement\u2014would it be possible to allow the user to choose which execution mode to go with (e.g., via `tf.config` or with a keyword argument in `tf.function`)? \r\n\r\nIn the talk, [Dr. Passos mentions, \"once there's no more TF v1 code out there, then I think we can flip the flag and allow all variable creation inside `tf.function` with fully eager semantics, but that's going to take a while.\"](https://youtu.be/MSXouZPyTrc?t=1153) If there hasn't been a change of heart, do you have an ETA on when TF 1.x will be fully out of business? \r\n- > In the future we may want to allow for function local `tf.Variable`s, which are created and destroyed each time the decorated function is invoked.\r\n\r\n(from the original [RFC](https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md), also see [\"Function-local state\"](https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md#function-local-state))\r\n\r\n---\r\n\r\nAlso: is `tf.init_scope` any help in recreating variables in functions? I tried and it still raised `ValueError`. Perhaps we could allow recreating Variables (e.g., eager semantics) as long as their creation is within a `tf.init_scope` context manager. \r\n\r\nI'm really sorry for consistently bugging you guys; I totally understand if this is something that you all don't want to work on. \r\n\r\n---\r\n\r\n- https://www.youtube.com/watch?v=51YtxSH-U3Y\r\n- https://www.youtube.com/watch?v=Up9CvRLIIIw\r\n- https://www.youtube.com/watch?v=MSXouZPyTrc\r\n- https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md", "@sumanthratna  Closing this issue since it was the intended behavior. Please feel free to re-open the issue if you have any concern.Thanks!", "@sumanthratna hi,have you solved this issue and by how? I think this is really the demand that tf should work on because I've seen many tf users who have raised this issue on stackoverflow and github. Although it's the rule the variable inside the tf.function should not be created twice, but this design may not be reasonable because it's not what users need.", "hi @sjtusmartboy, I think the TensorFlow team has taken a look at this (see https://github.com/tensorflow/tensorflow/issues/27120) and they have added an internal flag: \r\nhttps://github.com/tensorflow/tensorflow/blob/5cbc6f7e53c5a55753fcdf2877b278a8f48970d5/tensorflow/python/eager/def_function.py#L103\r\n\r\nI'm not sure on the best tf/python syntax to use this feature, but something like this should work: \r\n\r\n```python\r\nfrom tensorflow.python.eager import def_function  # def_function is the same as tf.function\r\n\r\ndef_function.ALLOW_DYNAMIC_VARIABLE_CREATION = True\r\n\r\n@def_function.function\r\ndef myfunc(val):\r\n    tf.Variable(val)\r\n\r\nmyfunc(0.3)\r\nmyfunc(0.9)\r\n```\r\n\r\nsee https://github.com/tensorflow/tensorflow/blob/9d97737008bae61a3a96abdac8cd676e025fd3c0/tensorflow/python/eager/def_function_test.py for more examples", "@sumanthratna  Thanks for your immediate answer. This is the first time for me to know this flag. I've tried your example, but somehow it gives out the error `ValueError: tf.function-decorated function tried to create variables on non-first call.` Any idea what happens?", "@sumanthratna For the testcase you mentioned above, I've tried it and found it not easy to understand the function of that flag because whether `def_function.ALLOW_DYNAMIC_VARIABLE_CREATION` is true of false, the following testcase would pass.\r\n\r\n```\r\nfrom tensorflow.python.eager import def_function  # def_function is the same as tf.function\r\nfrom tensorflow.python.ops import variables\r\nfrom absl.testing import parameterized\r\nfrom tensorflow.python.platform import test\r\n\r\nclass DefFunctionTest(test.TestCase, parameterized.TestCase):\r\n\r\n    def testMethodAllowDynamicVariable(self):\r\n        class Foo:\r\n\r\n            def __init__(self):\r\n                self._flag_keyed_vars = {}\r\n                self.trace_count = 0\r\n\r\n            def __call__(self, var_creation_flag):\r\n                self.compute(var_creation_flag)\r\n                return self._flag_keyed_vars[var_creation_flag]\r\n\r\n            @def_function.function\r\n            def compute(self, var_creation_flag):\r\n                self.trace_count += 1\r\n                if var_creation_flag not in self._flag_keyed_vars:\r\n                    if var_creation_flag:\r\n                        self._flag_keyed_vars[var_creation_flag] = variables.Variable(1.0)\r\n                    else:\r\n                        self._flag_keyed_vars[var_creation_flag] = variables.Variable(2.0)\r\n\r\n        def_function.ALLOW_DYNAMIC_VARIABLE_CREATION = False\r\n        foo = Foo()\r\n        self.assertAllEqual(foo(True), 1.0)\r\n        self.assertEqual(foo.trace_count, 2)\r\n        self.assertAllEqual(foo(True), 1.0)\r\n        self.assertEqual(foo.trace_count, 2)\r\n        self.assertAllEqual(foo(False), 2.0)\r\n        self.assertEqual(foo.trace_count, 3)\r\n```\r\nAlso, I cannot understand why it needs so many flags in the testcase. What's the minimum testcase?", "@sumanthratna Also would you please help me look at this demand, https://github.com/tensorflow/tensorflow/issues/51453. Is it possible to solve it with `def_function.ALLOW_DYNAMIC_VARIABLE_CREATION`. Thanks a million.", "@sjtusmartboy I think I may be understanding the purpose of that flag, then\u2014sorry for the misleading suggestion! I think for now it is safe to assume that there's no way to create `Variable`s in a `function` (I'm also confused by the test-case you sent)\r\n\r\nRegarding the issue that you linked, I'm not sure how to resolve it; I think you'll have to use eager execution if possible"]}, {"number": 39863, "title": "R1.8", "body": "Complie C++", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39863) for more info**.\n\n<!-- need_sender_cla -->", "@zhoumhWin Thank you for your contribution. Can you please sign CLA? Thanks!", "We don't merge release branches back into master"]}, {"number": 39861, "title": "close it please", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Issue template not filled. Marking as closed as per author's comment."]}, {"number": 39860, "title": "Enforce host memory for DT_INT32 inputs of functional ops", "body": "This PR amends the memory type (host/device) assignment logic.\r\n\r\nPrior to this fix, there were several situations where a GPU-allocated DT_INT32 tensor could arrive into a kernel via a HostMemory input (typically when that kernel was inside an If or a StatelessWhile subgraph). This _usually_ worked fine, because it's common for the GPU memory to be mapped into the host memory address space anyway, so the kernel could still access the data (which is why the problem was not noticed for so long). But, on some devices and some systems, memory mapping is partial or unavailable, and, on those, the kernel would crash. ", "comments": ["That's a pretty huge non-ROCm specific change. Could you show the test cases where it is crashing otherwise? I also thing the tag `[ROCm]` is somewhat misleading here.", "It crashes on some ROCm devices in the convert_to_constants test.", "Sure, but you are changing the logic for everything. Do you know why is it crashing only on ROCm?", "It crashes because some ROCm devices do not map all GPU memory into host space. It might be crashing with some NV devices as well but I don't have the hardware to test it.\r\nI don't think the change is all that huge, apart from formatting, it should only affect inputs to functional ops like StatelessWhile.", "@sanjoy for comments", "I think @agarwal-ashish is a better reviewer here.", "@agarwal-ashish Can you please review this PR ? Thanks!", "@saxenasaurabh Can you please review this PR ? Thanks!\r\n", "@ezhulenev  Can you please review this PR ? Thanks!", "@ekuznetsov139  Can you please check @alextp's comments and keep us posted ? Thanks!", "@ekuznetsov139  Can you please check @alextp's comments and resolve conflicts?. Thanks!", "@ekuznetsov139  Can you please resolve conflicts? Thanks!", "@ekuznetsov139  Can you please resolve conflicts? Thanks!", "@ekuznetsov139  Can you please check @allenlavoie's comments and keep us posted ? Thanks!", "@ekuznetsov139 Any update on this PR? Please. Thanks!\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It seems that we're waiting for further feedback from TF devs.", "> It seems that we're waiting for further feedback from TF devs.\r\n\r\n@allenlavoie Can you please take a look on above comments from @ekuznetsov139. Thanks!", "What more did you want feedback on?", "Mainly, do you want me to make any specific changes to this PR, and do you even want to merge it at all (as opposed to having someone at your end rewrite that entire code).", "Unless there's a pressing need I'd rather we didn't make int32 more special than it already is. I doubt users will run into this since control flow is typically inlined rather than these functional kernels running directly.\r\n\r\nSo if there's a great need for more int32 special casing please make that case, but otherwise I'd say we should implement a more satisfying solution (figure out where downstream kernels want inputs placed / leave them where they are). Of course you're free to leave the latter to someone else.", "@ekuznetsov139  Can you please check @allenlavoie's comments and keep us posted ? Thanks!"]}, {"number": 39859, "title": "TF-TRT Pack op conversion in dynamic shape mode", "body": "This PR improves the Pack (Stack) op converter to handle explicit batch and dynamic shape input data. Tagging @bixia1 for review.", "comments": ["@tfeher Can you please resolve conflicts? Thanks!", "@tfeher Can you please check @bixia1's comments and keep us posted. Thanks!"]}, {"number": 39858, "title": "Unable to calculate gradient -> AttributeError: 'Tensor' object has no attribute 'numpy'", "body": "**System information**\r\n- OS Platform and Distribution: macOS Catalina\r\n- TensorFlow installed from (source or binary): Google Colab\r\n- TensorFlow version: >=2.0\r\n- Python version: 3.6\r\n\r\n** I want to compute the gradient for KL Divergence ( between two distributions aka logits) with respect to noise (kind of input and not wrt weights). It was possible with Tensorflow 1.0 but using GradientTape seems to return me a None for \"grads\" which I am unable to comprehend**\r\n\r\nCode:\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model, Sequential\r\nfrom tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional\r\n\r\nMAX_VOCAB_SIZE = 10000 # maximum no of unique words\r\nMAX_DOC_LENGTH = 500 # maximum no of words in each sentence\r\nEMBEDDING_DIM = 300 # Embeddings dimension from Glove directory\r\ndef compute_kld(p_logit, q_logit): # computes difference between two distributions\r\n  p = tf.nn.softmax(p_logit)\r\n  q = tf.nn.softmax(q_logit)\r\n  kl_score = tf.reduce_sum( p * (tf.math.log(p+1e-16) - tf.math.log(q+1e-16)), axis = 1)\r\n  return kl_score # Scalar -> lower kl means closer the distributions are closer\r\n\r\n# Create Embeddings\r\ninputs2d = Input(shape=(MAX_DOC_LENGTH,)) # takes a sequence of words\r\nclean_embd= Embedding(input_dim=MAX_VOCAB_SIZE + 1, output_dim=EMBEDDING_DIM,\r\n                      input_length=MAX_DOC_LENGTH)(inputs2d) # converts each input to embedding matrix\r\nr_embd = tf.random.uniform(shape=tf.shape(clean_embd)) # random noise to be added to clean_embd\r\n\r\n# Create Model\r\ninputs3d = Input(shape=(MAX_DOC_LENGTH, EMBEDDING_DIM,)) # Embedding shall be my input to the model\r\nnetwork = Sequential()\r\nnetwork.add(LSTM(units=128,))\r\nnetwork.add(Dense(units=16, activation='relu'))\r\noutput = network(inputs3d)\r\nmodel = Model(inputs3d, output)\r\nmodel.summary()\r\n\r\n# Calculate gradient\r\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\r\n    tape.watch(r_embd)\r\n    r_embd_ = tf.math.add(clean_embd, r_embd) # Noised input\r\n    # Compute logits\r\n    p_logit = model(clean_embd) # True logit distribution\r\n    p_logit_r = model(r_embd_) # Perturbed logits distribution\r\n    # Find the Kl Score\r\n    kl_score = tf.reduce_mean(compute_kld(p_logit, p_logit_r))\r\n    kl_score = tf.convert_to_tensor(kl_score, dtype=tf.float32)\r\n**grads = tape.gradient(kl_score, r_embd) # I wish to differentiate kl_score wrt r_embd. So the Jacobian matrix should be of shape (None, 500, 300) similar to (clean_embd | r_embd_).**\r\n\r\n**# Error is that grads is returned as None; which I can't understand.**\r\n\r\n# Future implementation\r\nr_vadv_embd = tf.math.add(clean_embd, g)\r\n**q_logits = model(r_vadv_embd) # End goal is to be able to calculate q_logit**\r\n\r\n**Any other info / logs**\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-10-f19b613537c7> in <module>()\r\n     15     kl_score = tf.reduce_mean(compute_kld(p_logit, p_logit_r))\r\n     16     kl_score = tf.convert_to_tensor(kl_score, dtype=tf.float32)\r\n---> 17 grads = tape.gradient(kl_score, r_embd)\r\n     18 \r\n     19 # g = [grad if grad is not None else tf.zeros_like(var)for var, grad in zip(r_embd, grads)]\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n   1046         output_gradients=output_gradients,\r\n   1047         sources_raw=flat_sources_raw,\r\n-> 1048         unconnected_gradients=unconnected_gradients)\r\n   1049 \r\n   1050     if not self._persistent:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     75       output_gradients,\r\n     76       sources_raw,\r\n---> 77       compat.as_str(unconnected_gradients.value))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\r\n    155       gradient_name_scope = \"gradient_tape/\"\r\n    156     with ops.name_scope(gradient_name_scope):\r\n--> 157       return grad_fn(mock_op, *out_grads)\r\n    158   else:\r\n    159     return grad_fn(mock_op, *out_grads)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py in _SumGrad(op, grad)\r\n    210       # more sense.\r\n    211       output_shape_kept_dims = math_ops.reduced_shape(input_shape,\r\n--> 212                                                       op.inputs[1])\r\n    213     grad = array_ops.reshape(grad, output_shape_kept_dims)\r\n    214   return [array_ops.broadcast_to(grad, input_shape), None]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduced_shape(input_shape, axes)\r\n   3733   \"\"\"\r\n   3734   if context.executing_eagerly():\r\n-> 3735     input_shape = input_shape.numpy()\r\n   3736     axes = axes.numpy()\r\n   3737     input_shape[axes] = 1\r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n", "comments": ["@lokesharma-dev \r\nCan you please refer to these links and let us know if it helps:\r\n[link](https://github.com/tensorflow/tensorflow/issues/39817#issuecomment-633675341) [link1](https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy) [link2](https://www.reddit.com/r/learnmachinelearning/comments/c1j7i5/attributeerror_tensor_object_has_no_attribute/) [link3](https://www.kaggle.com/c/prostate-cancer-grade-assessment/discussion/146248) #39582 #35949 #36979", "Thanks a lot! This was of help and we can close the issue now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39858\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39858\">No</a>\n", "@lokesharma-dev so what was the solution for you? I refered all that links and didn't get the answer for me. With tf 2.0 eager execution is enabled, but I see the problem, as you had, in standart reduced_shape function, not in the custom one. As I can see, there is a check that succesfully passes: \"if context.executing_eagerly()\", but that doesn't help and the error raises while attempted to use to_numpy(). I'm confused what to do with this\r\n\r\n**UPD.** Ok, I've found out, that Jul 7, 2020 commit [7d2a2a6](https://github.com/tensorflow/tensorflow/commit/7d2a2a6a11dd2bf8f4ab95d62bad10d46b953c9f#diff-f0675f2568ff9470bcfc1f2bc79c5386) fixed the reduced_shape function and got rid of .numpy(). Hooray! Unfortunately, it is not included in release(", "@EvsanDlg Well TensorFlow 2.x is tricky at times especially the exceptions they provide.  \r\nMy issue was fixed when I was using an EagerTensor instead of the tensor object. In tf1.x we could compute gradient wrt to a tensor with no data inside it. I do not understand how I was getting an error relating to NumPy() conversion. However, in your case, I won't be able to comment unless have some more information.\r\nHope this helps.\r\n", "@lokesharma-dev i am facing the same issue...can you help as to how you solved the issue by using an eagertensor ??"]}, {"number": 39856, "title": "`tf.keras.models.clone_model` does not support custom model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **pip**\r\n- TensorFlow version (use command below): **2.2.0**\r\n- Python version: **Python 3.6.10 :: Anaconda, Inc.**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **CUDA 10.1**\r\n- GPU model and memory: **GeForce RTX 2080 Ti**\r\n\r\n**Describe the current behavior**\r\n\r\nIn TF2.2.0 release notes, it said \r\n> * You can now use custom training logic with `Model.fit` by overriding `Model.train_step`\r\n> * Easily write state-of-the-art training loops without worrying about all of the features `Model.fit` handles for you (distribution strategies, callbacks, data formats, looping logic, etc)\r\n\r\nI have implemented my own custom model whose `train_step` is overwrited, And I want to create an identical model by API `tf.keras.models.clone_model`.\r\n\r\nBut, the problem there is that my custom `train_step` is gone.\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.keras.models.clone_model` should copy not only model's layers but also `train_step`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\n#%%\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\n#%% \r\nclass Composite(tf.keras.Model):\r\n    def __init__(self, *args, **kwargs):\r\n\r\n        super(Composite, self).__init__(*args, **kwargs)\r\n\r\n    def train_step(self, data):\r\n\r\n        data_adapter = tf.python.keras.engine.data_adapter\r\n        data = data_adapter.expand_1d(data)\r\n        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\r\n\r\n        tf.print(\"HIHI! I'm in function train_step!\")\r\n\r\n        with tf.GradientTape() as tape:\r\n            y_pred = self(x, training=True)\r\n            loss = self.compiled_loss(\r\n                y, y_pred, sample_weight, regularization_losses=self.losses)\r\n\r\n        _minimize = tf.python.keras.engine.training._minimize\r\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\r\n                self.trainable_variables)\r\n\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\nin_ = tf.keras.layers.Input(shape=(10, ) )\r\nx = tf.keras.layers.Dense(1)(in_)\r\nmodel = Composite(inputs=in_, outputs=x)\r\nmodel.compile(loss='binary_crossentropy',optimizer='SGD', metrics=['accuracy'])\r\n\r\nX = np.zeros((10,10))\r\nY = np.zeros((10,1))\r\nmodel.fit(X,Y,verbose=2)\r\n\r\n# %%\r\nnew_model = tf.keras.models.clone_model(model)\r\nnew_model.compile(loss='binary_crossentropy',optimizer='SGD', metrics=['accuracy'])\r\nnew_model.fit(X,Y,verbose=2)\r\n```\r\n\r\n**Other info / logs** \r\n```\r\nthere is the original model\r\nHIHI! I'm in function train_step!\r\n1/1 - 0s - loss: 0.0000e+00 - accuracy: 1.0000\r\n<tensorflow.python.keras.callbacks.History at 0x7fa9603dd6a0>\r\nthere is the NEW model\r\n1/1 - 0s - loss: 0.0000e+00 - accuracy: 1.0000\r\n<tensorflow.python.keras.callbacks.History at 0x7fa96023bbe0>\r\n```\r\n\r\nFrom the console, you will see the output `HIHI! I'm in function train_step!` is gone when I run `new_model.fit(X,Y,verbose=2)`", "comments": ["An alternative solution is:\r\n```python\r\nwrap_model = Composite(inputs=new_model.input, outputs=new_model.output) \r\nwrap_model.compile(loss='binary_crossentropy',optimizer='SGD', metrics=['accuracy'])\r\nwrap_model.fit(X,Y,verbose=2)\r\n```\r\n\r\nI don't think this is good solution but it is an acceptable choice.\r\n\r\nIf `tf.keras.models.clone_model` can not support custom model. It should be emphasized in the document.", "Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/2dce07b4fdce8b8c64a159955602b76d/39856.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/94e7f9d3fea0c318f91ecbdaf7460861/39856-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Yes. Same problem here. ", "@CNOCycle When i executed the code in TF 2.6 I  am facing different error when called fit method. Please find the [gist](https://colab.research.google.com/gist/saikumarchalla/f1391267be62299c0d50eadff110e9e5/nighly.ipynb) here.Thanks!", "@saikumarchalla  because tf.python is hidden in newer TF. So I updated the testing code:\r\n```python\r\n#%%\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\n#%% \r\nclass Composite(tf.keras.Model):\r\n    def __init__(self, *args, **kwargs):\r\n\r\n        super(Composite, self).__init__(*args, **kwargs)\r\n\r\n    def unpack_x_y_sample_weight(self, data):\r\n        \"\"\"Unpacks user-provided data tuple.\"\"\"\r\n        if not isinstance(data, tuple):\r\n            return (data, None, None)\r\n        elif len(data) == 1:\r\n            return (data[0], None, None)\r\n        elif len(data) == 2:\r\n            return (data[0], data[1], None)\r\n        elif len(data) == 3:\r\n            return (data[0], data[1], data[2])\r\n\r\n    def train_step(self, data):\r\n\r\n        x, y, sample_weight = self.unpack_x_y_sample_weight(data)\r\n\r\n        tf.print(\"HIHI! I'm in function train_step!\")\r\n\r\n        with tf.GradientTape() as tape:\r\n            y_pred = self(x, training=True)\r\n            loss = self.compiled_loss(\r\n                y, y_pred, sample_weight, regularization_losses=self.losses)\r\n\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\nin_ = tf.keras.layers.Input(shape=(10, ) )\r\nx = tf.keras.layers.Dense(1)(in_)\r\nmodel = Composite(inputs=in_, outputs=x)\r\nmodel.compile(loss='binary_crossentropy',optimizer='SGD', metrics=['accuracy'])\r\n\r\nX = np.zeros((10,10))\r\nY = np.zeros((10,1))\r\n\r\nprint(\"the original model\")\r\nmodel.fit(X,Y,verbose=2)\r\n\r\n# %%\r\nprint(\"cloned model\")\r\nnew_model = tf.keras.models.clone_model(model)\r\nnew_model.compile(loss='binary_crossentropy',optimizer='SGD', metrics=['accuracy'])\r\nnew_model.fit(X,Y,verbose=2)\r\n\r\nprint(\"the original model again\")\r\nmodel.fit(X,Y,verbose=2)\r\n```\r\n\r\nHere is the output\r\n```2.6.0-dev20210601\r\nthe original model\r\nHIHI! I'm in function train_step!\r\n1/1 - 0s - loss: 0.0000e+00 - accuracy: 1.0000\r\ncloned model\r\n1/1 - 0s - loss: 0.0000e+00 - accuracy: 1.0000\r\nthe original model again\r\nHIHI! I'm in function train_step!\r\n1/1 - 0s - loss: 0.0000e+00 - accuracy: 1.0000\r\n\r\n<tensorflow.python.keras.callbacks.History at 0x7ff9a5852490>\r\n```\r\n\r\nObviously, this issue has not been fixed", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I think subclassed API model is not supported in the `clone_model` function, see the document \r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/models/clone_model\r\n\r\n```\r\nArgs \r\n--  model | Instance of Model (could be a Functional model or a Sequential model).\r\n....\r\n....\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39856\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39856\">No</a>\n", "I thought that this problem has not been solved yet.", "still has this issue in tf2.7, any progress in this bug?", "@groovemaxRong you can post a feature request in the keras Github repo regarding this. "]}, {"number": 39855, "title": "DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.", "body": "-windows 10\r\n-python 3.8.3\r\n-tensorflow 2.2.0\r\n\r\ni have installed it using\r\n`pip install tensorflow` \r\n\r\ni run this code\r\n`import tensorflow as tf`\r\n\r\ni got error\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\pratibha\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\pratibha\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\pratibha\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\pratibha\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\pratibha\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/pratibha/AppData/Roaming/JetBrains/PyCharmCE2020.1/scratches/textblob.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\pratibha\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\pratibha\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\pratibha\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\pratibha\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\pratibha\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\pratibha\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\pratibha\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\pratibha\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39855\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39855\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39855\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39855\">No</a>\n", "I don't understand I have the same issue with same exact environment. What was the fix?", "In my case my pc not having AVX, check for yours if it is having AVX. also check if u have gpu to run the latest version. Or u can switch to 1.5 version of this."]}, {"number": 39854, "title": "XLA compilation not working in Windows 10 Pro", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 7.6.5\r\n- GPU model and memory: RTX 2080 8GB\r\n\r\n**Describe the current behavior**\r\nXLA compilation in Windows doesn't seem to work even if in the changelog of TF 2.2.0 says XLA is now possible with Windows systems. Here is the output of the error:\r\n`2020-05-25 17:03:22.775625: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. \r\nSetting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas\r\nThis message will only be logged once.\r\n2020-05-25 17:03:23.131452: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\r\n2020-05-25 17:03:23.131584: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:\r\n2020-05-25 17:03:23.131652: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib\r\n2020-05-25 17:03:23.131702: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n2020-05-25 17:03:23.131769: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .\r\n2020-05-25 17:03:23.131814: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\r\n2020-05-25 17:03:23.132862: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:305] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\r\n2020-05-25 17:03:23.139801: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n2020-05-25 17:03:23.141241: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at xla_ops.cc:520 : Internal: libdevice not found at ./libdevice.10.bc\r\nTraceback (most recent call last):\r\n  File \"D:/Users/Alvaro/Documents/TFM/Project/eyesrgan/Resnet_MSE.py\", line 142, in <module>\r\n    callbacks=callbacks)\r\n  File \"D:\\Development\\Anaconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"D:\\Development\\Anaconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"D:\\Development\\Anaconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"D:\\Development\\Anaconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 644, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"D:\\Development\\Anaconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"D:\\Development\\Anaconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"D:\\Development\\Anaconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"D:\\Development\\Anaconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"D:\\Development\\Anaconda3\\envs\\TFM\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InternalError:  libdevice not found at ./libdevice.10.bc\r\n\t [[{{node cluster_4_1/xla_compile}}]] [Op:__inference_train_function_12671]\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\n\r\nProcess finished with exit code 1`\r\n\r\n\r\nI already tried to add the XLA_FLAGS environment variable, as well as CUDA_DIR environ, both with any success.\r\n\r\n**Describe the expected behavior**\r\nXLA should work with its implicit gain in performance.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe source code can be found in this [colab](https://drive.google.com/open?id=12sx2GQN5kwRke7fPkj5TkgtugN5y1X44).\r\n\r\n", "comments": ["I have tried in colab with TF 2.2  and i am seeing the below error message `NonMatchingChecksumError:`.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ff4a1e3d9ede6f0ce3497b6faa0444fb/untitled42.ipynb).Thanks!", "Hi,\r\n\r\nHere it is another [colab ](https://colab.research.google.com/drive/12sx2GQN5kwRke7fPkj5TkgtugN5y1X44?usp=sharing)without the issues mentioned above. Still getting the same error when running on Windows.\r\n\r\nRegards.", "@alvarobasi It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39854\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39854\">No</a>\n"]}, {"number": 39853, "title": "2 errors while building NodeDef 'tf_op_layer_Maximum_2/Maximum_2' using Op<name=Maximum; signature=x:T, y:T -> z:T ...Inconsistent values for attr 'T' DT_INT32 vs. DT_INT64", "body": "System information\r\nI am using colab to reproduce the issue and the ipynb is attached below.\r\n\r\nYou can collect some of this information using our environment capture\r\ntf.version.GIT_VERSION: v1.12.1-32511-g2cc80a74f2\r\ntf.version.VERSION:  2.3.0-dev20200525\r\n\r\nDescribe the current behavior\r\ncannot load the saved tf model\r\n\r\nDescribe the expected behavior\r\nsuccessifully save the model and serve it like this example: https://github.com/tensorflow/transform/blob/master/examples/census_example_v2_test.py\r\n\r\nStandalone code to reproduce the issue\r\nhttps://colab.research.google.com/drive/1h2QIX_QZetIzSuG0J6lNWkHoSa2nnIyS?usp=sharing\r\n\r\nOther info / logs Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nerror is show in the last cell of the colab notebook.\r\n\r\n```\r\nWARNING:tensorflow:Layer LSTM_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\nWARNING:tensorflow:Layer LSTM_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def)\r\n   1819   try:\r\n-> 1820     c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\n   1821   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: 2 errors while building NodeDef 'tf_op_layer_Maximum_2/Maximum_2' using Op<name=Maximum; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT16, DT_INT32, DT_INT64]>:\r\nInconsistent values for attr 'T' DT_INT32 vs. DT_INT64\r\nInconsistent values for attr 'T' DT_INT32 vs. DT_INT64\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n15 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def)\r\n   1821   except errors.InvalidArgumentError as e:\r\n   1822     # Convert to ValueError for backwards compatibility.\r\n-> 1823     raise ValueError(str(e))\r\n   1824 \r\n   1825   return c_op\r\n\r\nValueError: 2 errors while building NodeDef 'tf_op_layer_Maximum_2/Maximum_2' using Op<name=Maximum; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT16, DT_INT32, DT_INT64]>:\r\nInconsistent values for attr 'T' DT_INT32 vs. DT_INT64\r\nInconsistent values for attr 'T' DT_INT32 vs. DT_INT64\r\n```", "comments": ["Was able to reproduce the error. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/01a934336b1da585aac5a13eb980bb4a/39853.ipynb#scrollTo=9_otiTo2RbY6&line=6&uniqifier=1). Thanks!", "Thanks for the report @neilteng . The example is fairly complex, and it is hard for us to pinpoint the issue. Can you remove the dependencies + get the example down to the simplest possible repro? That will allow us to more easily determine whether this is an issue with TFT, Beam, feature columns, graph mode, etc.", "ok, I will try to reproduce the error with a simpler example.", "@neilteng Were you able to reproduce the error with a simpler example?\r\n", "> @neilteng Were you able to reproduce the error with a simpler example?\r\n\r\nI think I accidentally delete the notebook.. And I cannot recover it.. If we are luck and you guys have a copy, I will work on it again. Otherwise, I can only close this issue for now.", "@neilteng Please share us the notebook. Thanks!", "I think I accidentally delete the notebook.. And I cannot recover it.. If we are luck and you guys have a copy, I will work on it again. Otherwise, I can only close this issue for now."]}, {"number": 39852, "title": "tf.guarantee_const does not work with XLA compilation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nI am unable to use `tf.guarantee_const` in functions decorated with `tf.function(experimental_compile=True)`.\r\n\r\n**Describe the expected behavior**\r\nThe method, with XLA, should run the same as the method without XLA, with no errors.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function(experimental_compile=False)\r\ndef test_xla(x):\r\n    x = tf.guarantee_const(x)\r\n    return x**2\r\n\r\ntest_xla(3)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n2020-05-25 10:14:47.561576: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-25 10:14:47.621911: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fed5077ddc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-25 10:14:47.621943: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"tfxla.py\", line 10, in <module>\r\n    test_xla(3)\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 576, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 650, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\r\n    return self._call_flat(\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\r\n    outputs = execute.execute(\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_test_xla_8}} = __inference_test_xla_8[_XlaMustCompile=true, config_proto=\"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0002\\002J\\0008\\001\", executor_type=\"\"]().\r\nUncompilable nodes:\r\nGuaranteeConst: unsupported op: No registered 'GuaranteeConst' OpKernel for XLA_CPU_JIT devices compatible with node {{node GuaranteeConst}}\r\n\tStacktrace:\r\n\t\tNode: __inference_test_xla_8, function: \r\n\t\tNode: GuaranteeConst, function: __inference_test_xla_8\r\n [Op:__inference_test_xla_8]\r\n```\r\n", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/693825990d56e417ee3eb8fa800beae7/untitled197.ipynb)", "This is fixed with tf-nightly 2.4.0-dev20200828.\r\nSee colab [gist](https://colab.research.google.com/gist/ymodak/96dbb5488d4d6fb8a7c3d98da35aa2d1/github_issue39852.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I haven't had the chance to revisit this issue but I trust the colab notebook you've sent, so I'll close this issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39852\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39852\">No</a>\n"]}, {"number": 39851, "title": "Cannot Read From Google Storage with tf.io.gfile.GFile under intel-tensorflow==1.14.0", "body": "Support for Google Storage (`gs` protocol) seems to be missing from `intel-tensorflow==1.14.0`, which is unexpected. I'm aware that this is no core tensorflow issue but I've seen that some engineers from Intel are active in this repository.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): docker image `intelaipg/intel-optimized-tensorflow:1.14.0-mkl-py3` (Ubuntu 18.04.2 LTS)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): No\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nOutput: `v1.14.0-1-hardened-0-g340d16ee58 1.14.0`\r\n\r\n**Describe the current behavior**\r\n\r\nGoogle storage doesn't seem to be supported for some reason:\r\n\r\n```\r\n> python\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34)\r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf; tf.io.gfile.GFile(\"gs://some_bucket/test.txt\").read()\r\n[deprecation warning redacted]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 122, in read\r\n    self._preread_check()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 84, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512)\r\ntensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 'gs' not implemented (file: 'gs://some_bucket/test.txt')\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nVerified to work with 1.13.2:\r\n\r\n```\r\n> python\r\nPython 3.6.8 (default, Jan 14 2019, 11:02:34)\r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf; tf.io.gfile.GFile(\"gs://some_bucket/test.txt\").read()\r\n[deprecation warning redacted]\r\n'test\\n'\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee above.\r\n\r\n**Other info / logs**\r\nn/a\r\n", "comments": ["@pks \r\nCan you check with other TF versions like 1.15, 2.x and see the behavior.Also, request you to provide colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "I tried the image `intelaipg/intel-optimized-tensorflow:1.15.2-mkl-py3` and it works:\r\n\r\n```\r\n> python\r\nPython 3.6.9 (default, Nov  7 2019, 10:44:02)\r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf; tf.io.gfile.GFile(\"gs://some_bucket/test.txt\").read()\r\n'test\\n'\r\n```\r\n\r\nTensorflow version:\r\n```\r\n> python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.15.2-0-gb57a210 1.15.2\r\n```\r\nI noticed that the `hardened` substring is missing from here.\r\n\r\nThis code snippet should suffice for testing:\r\n```\r\n import tensorflow as tf; tf.io.gfile.GFile(\"gs://some_bucket/test.txt\").read()\r\n```\r\nJust note that the bucket and the file within it likely do not exist.", "@pks \r\n\r\nThe issue is resolved in latest TF version.And you can use TF 1.15 version. Can we close this issue?Thanks!", "This seems to be more like a packaging issue, somehow support for GCS was removed in this build.\r\n\r\nWe cannot use Tf 1.15 as we observed serious performance degradations with our models.", "@TensorFlow-MKL Could you please help look into this? Thank you very much!", "We are working on this issue. Will keep you posted", "> This seems to be more like a packaging issue, somehow support for GCS was removed in this build.\r\n> \r\n> We cannot use Tf 1.15 as we observed serious performance degradations with our models.\r\n\r\n@pks  TF 1.15 should has less performance gap than TF 1.14. \r\nCould try following script to disable eager mode, which would impact the first running performance?\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.disable_eager_execution()\r\n```\r\n\r\n", "@NeoZhangJianyu We found that the `_MklSoftmax` operation is quite a bit slower in Tf 1.15 than it was in Tf 1.13, about 2-2.5x worse. Tf 1.14 still works as expected, but the `gs://` protocol was removed.\r\n\r\n", "@pks Could you try the docker image built by Google:  gcr.io/deeplearning-platform-release/tf-cpu.1-14 ?\r\nIt also includes the Intel optimization for Tensorflow. \r\n\r\nI got following error in your case. Looks like it keep the \"gs\".\r\n```\r\npython -c \"import tensorflow as tf; tf.io.gfile.GFile(\\\"gs://some_bucket/test.txt\\\").read()\"\r\n2020-06-19 09:04:23.374934: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Failed precondition: Error executing an HTTP request: HTTP response code 302 with body '<HTML>\r\n<HEAD><TITLE>Redirection</TITLE></HEAD>\r\n<BODY><H1>Redirect</H1></BODY>\r\n</HTML>\r\n'\".\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/root/miniconda3/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 124, in read\r\n    length = self.size() - self.tell()\r\n  File \"/root/miniconda3/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 102, in size\r\n    return stat(self.__name).length\r\n  File \"/root/miniconda3/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 727, in stat\r\n    return stat_v2(filename)\r\n  File \"/root/miniconda3/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 744, in stat_v2\r\n    pywrap_tensorflow.Stat(compat.as_bytes(path), file_statistics)\r\ntensorflow.python.framework.errors_impl.PermissionDeniedError: Error executing an HTTP request: HTTP response code 403 with body '{\r\n  \"error\": {\r\n    \"code\": 403,\r\n    \"message\": \"The project to be billed is associated with a closed billing account.\",\r\n    \"errors\": [\r\n      {\r\n        \"message\": \"The project to be billed is associated with a closed billing account.\",\r\n        \"domain\": \"global\",\r\n        \"reason\": \"accountDisabled\",\r\n        \"locationType\": \"header\",\r\n        \"location\": \"Authorization\"\r\n      }\r\n    ]\r\n  }\r\n}\r\n'\r\n\t when reading metadata of gs://some_bucket/test.txt\r\n\r\n```\r\n\r\nUse following cmd to run it:\r\n```\r\ndocker run -d -p 8080:8080 -v /home:/home  gcr.io/deeplearning-platform-release/tf-cpu.1-14\r\nor\r\ndocker run -v /home:/home -it gcr.io/deeplearning-platform-release/tf-cpu.1-14 bash\r\n```", "> @NeoZhangJianyu We found that the `_MklSoftmax` operation is quite a bit slower in Tf 1.15 than it was in Tf 1.13, about 2-2.5x worse. Tf 1.14 still works as expected, but the `gs://` protocol was removed.\r\n\r\n@pks I will check it. Thank your feedback!", "@pks Could try with the suggestion above and feedback?", "Yes, it works with other images. But they serve another purpose -- the Intel one has minimal dependencies.\r\n\r\nAny update on the softmax issue? Should I open another issue for that? Unfortunately I cannot share specific code for this, but should be reproducable with other problems too.", "@pks \r\nIt's great that it works.\r\nCould you close this issue?\r\n\r\nFor the sofmax performance issue, I have reproduced it and report an internal issue.\r\nYes, you can open another issue to trace it.\r\n", "Ok, I'll close it in favor of https://github.com/tensorflow/tensorflow/issues/40989. But note that The the Tensorflow build in this image `intelaipg/intel-optimized-tensorflow:1.14.0-mkl-py3` does not support `GFile` and that there's no update for it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39851\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39851\">No</a>\n", "> Ok, I'll close it in favor of #40989. But note that The the Tensorflow build in this image `intelaipg/intel-optimized-tensorflow:1.14.0-mkl-py3` does not support `GFile` and that there's no update for it.\r\n\r\n@pks. Thank you!\r\nThe TF1.14 is out of maintain. But TF 1.15 will be maintained for more months."]}, {"number": 39850, "title": "array slice on tf.function is about 23 times slower than non tf function", "body": "tf version: 2.1.0\r\nHW: one gpu\r\n\r\n```python code\r\nimport time\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef test_tf_fun(inputs):\r\n    for i in range(16):\r\n        for j in range(128):\r\n            t = inputs[i, j, :]\r\n\r\ninputs = tf.random.uniform((16, 128, 4))\r\n\r\nt1 = time.time()\r\ntest_tf_fun(inputs)\r\nprint(\"duration for tf.fun: \", time.time() - t1)\r\n```\r\n![image](https://user-images.githubusercontent.com/731496/82810012-03a0ca00-9ec0-11ea-8ed5-778b3ed4ab66.png)\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/e52732cb63db90f1e641d2f6805766a4/39850-2-1.ipynb), [TF v2.2](https://colab.research.google.com/gist/amahendrakar/d52e2ebe334d840d95661cf1184b5560/39850.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/6ffd39d98243e58c00fc7f561d55c993/39850-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@ymodak any update for this issue?", "@w19787 \r\nThe reason for this is the first invocation will do lots of graph optimization and will add overhead. If you run the function multiple times, you could see the performance is actually better. For example, if you execute the function 5 times:\r\n```python\r\n# the function definition is the same as above\r\n# ...\r\n\r\nfor _ in range(5):\r\n    t1 = time.time()\r\n    test_tf_fun(inputs)\r\n    print(\"duration for tf.fun: \", time.time() - t1)\r\n```\r\nwith `tf.function` will have:\r\n```\r\nduration for tf.fun:  5.076590061187744\r\nduration for tf.fun:  0.00021696090698242188\r\nduration for tf.fun:  0.0001277923583984375\r\nduration for tf.fun:  0.00012183189392089844\r\nduration for tf.fun:  0.00015115737915039062\r\n```\r\nwithout will be:\r\n```\r\nduration for tf.fun:  0.12435412406921387\r\nduration for tf.fun:  0.12618684768676758\r\nduration for tf.fun:  0.1261131763458252\r\nduration for tf.fun:  0.1262199878692627\r\nduration for tf.fun:  0.1267070770263672\r\n```\r\nMy test environment is a macbook pro 2019.", "@mdanatg \r\nIs it possible for `tf.function` to add config like `RewriterConfig` to control the grappler?", "> @w19787\r\n> The reason for this is the first invocation will do lots of graph optimization and will add overhead. If you run the function multiple times, you could see the performance is actually better. For example, if you execute the function 5 times:\r\n> \r\n> ```python\r\n> # the function definition is the same as above\r\n> # ...\r\n> \r\n> for _ in range(5):\r\n>     t1 = time.time()\r\n>     test_tf_fun(inputs)\r\n>     print(\"duration for tf.fun: \", time.time() - t1)\r\n> ```\r\n> \r\n> with `tf.function` will have:\r\n> \r\n> ```\r\n> duration for tf.fun:  5.076590061187744\r\n> duration for tf.fun:  0.00021696090698242188\r\n> duration for tf.fun:  0.0001277923583984375\r\n> duration for tf.fun:  0.00012183189392089844\r\n> duration for tf.fun:  0.00015115737915039062\r\n> ```\r\n> \r\n> without will be:\r\n> \r\n> ```\r\n> duration for tf.fun:  0.12435412406921387\r\n> duration for tf.fun:  0.12618684768676758\r\n> duration for tf.fun:  0.1261131763458252\r\n> duration for tf.fun:  0.1262199878692627\r\n> duration for tf.fun:  0.1267070770263672\r\n> ```\r\n> \r\n> My test environment is a macbook pro 2019.\r\n\r\nthanks. it is very reasonable explanation.  ", "Yes, it's possible with `tf.config.optimizer.set_experimental_options`, but it's tricky to tweak these settings to get an apples-to-apples comparison.\r\n\r\nNote that if you only want to benchmark the performance of slice operations, you should simplify the code as much as possible. There are a few caveats:\r\n\r\n * in eager mode, you should add a `.numpy()` call at the end of the computation, to make sure you measure the total time from starting it to getting the results, otherwise you may only be benchmarking asynchronous launch costs\r\n * within tf.function, `for i in range` will multiply the ops in the graph, so the two nested loops will measure the performance of 16 * 128 slice ops executed with maximum parallelism; `for i in tf.range` does create a graph loop, but even that will be aggressively parallelized\r\n * the cost of calling tf.function alone is significant, higher than calling a single slice op; you should account for that when benchmarking\r\n * there are many caches involved - for example, tf.function compilations are cached, tf.constant is cached; you should always do dry-runs to get accurate results\r\n\r\nSo to make an apples-to-apples comparison, here's what I'd run:\r\n\r\n```\r\ndef test_no_tf_fun(inputs, i, j):\r\n    return inputs[i, j, :]\r\n\r\n\r\n@tf.function\r\ndef test_tf_fun(inputs, i, j):\r\n    return inputs[i, j, :]\r\n\r\n\r\n@tf.function\r\ndef test_tf_fun_empty(inputs, i, j):\r\n    return i\r\n\r\n\r\ninputs = tf.random.uniform((16, 128, 4))\r\n\r\n# Warm-up caches\r\ntest_tf_fun(inputs, tf.constant(0), tf.constant(0)).numpy()\r\ntest_tf_fun_empty(inputs, tf.constant(0), tf.constant(0)).numpy()\r\nfor c in range(128):\r\n  tf.constant(c).numpy()\r\n\r\n# Measure cost of calling tf.function\r\nt1 = time.time()\r\nfor i in range(16):\r\n    for j in range(128):\r\n      test_tf_fun_empty(inputs, tf.constant(i), tf.constant(j)).numpy()\r\nt_empty = time.time() - t1\r\nprint(\"duration for empty tf.fun: \", t_empty)\r\n\r\n# Measure cost of slice inside tf.function\r\nt1 = time.time()\r\nfor i in range(16):\r\n    for j in range(128):\r\n      test_tf_fun(inputs, tf.constant(i), tf.constant(j)).numpy()\r\nprint(\"duration for tf.fun: \", time.time() - t1 - t_empty)\r\n\r\n# Measure cost of slice outside tf.function\r\nt1 = time.time()\r\nfor i in range(16):\r\n    for j in range(128):\r\n      inputs[i, j, :].numpy()\r\nprint(\"duration without tf.function: \", time.time() - t1)\r\n```\r\n\r\nOn my machine, that printed:\r\n```\r\nduration for empty tf.fun:  0.4628314971923828\r\nduration for tf.fun:  0.02042388916015625\r\nduration without tf.function:  0.2617456912994385\r\n```\r\n"]}, {"number": 39849, "title": "Move -no-as-needed to the top of the linking command line in the cuda toolchain", "body": "`-no-as-needed` linker flag is position sensitive (it's only effecting\r\nfollowing -l flags), therefore we need to move it before libraries to\r\nlink.\r\n\r\nThis change uncovered that nccl doesn't properly declare it's dependency\r\non `-lrt`, which is fixed. I suspect this started to be a problem in\r\nhttps://github.com/tensorflow/tensorflow/commit/f819114a2d9d393a60e954d3a3e42d8700ff3b19.\r\n\r\nThis change also uncovered that some tests don't need to depend on nccl.\r\nWhile `-no-as-needed` wasn't taking effect, nccl was just left out as\r\nnot needed.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/38205", "comments": ["CC @r4nt @chsigg @gunan:\r\n\r\n1) I presume just passing `-lrt` on all platforms is not what we want at the end. Should I instead use `lrt_if_needed()` function? This would make `@nccl_archive` depend on `tensorflow.bzl`, but I guess that's fine.\r\n2) I haven't investigated if we need to depend on nccl in these tests at all. Do you know the answer right away, or should I look into it?"]}, {"number": 39848, "title": "TF-TRT GatherV2 op conversion dynamic shape mode", "body": "This PR improves the GatherV2 op converter to handle explicit batch and dynamic shape input data.\r\nAdditionally:\r\n- fix testing validation errors\r\n- enable defining implicit batch test with inconsistent batch size\r\n- improve converter helper routine for tensor/weight argument check\r\n\r\nTagging @bixia1 for review", "comments": ["@tfeher Can you please resolve conflicts? Thanks!"]}, {"number": 39847, "title": "tflite lstm", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Mrsyoung  \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39846, "title": "the gpu memory seem doesn't load well with crontab in centos7", "body": "I use crontab to start a timed task, in the task, it has a DL model service, which was write with tensorflow.\r\nbut when crontab to start this service, the model memory didn't load well on GPU, it should load with 1.5G, but in practice, it loaded 0.3G, and the compute speed is so low, is there something wrong with tensorflow in crontab.", "comments": ["Please provide following information relevant to your case.\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:", "os: centos 7.4\r\ntensorflow: tensorflow-gpu 1.14\r\npython: 3.6.8\r\npip: 20.1.1\r\nbazel: no\r\ngcc: 4.8.5\r\ncuda:10.0 cudnn:7.6.4\r\ngpu: V100-16G\r\n", "@ymodak hi, how about now, hope u can help me to figure out this problem. thx", "Do you have a reproducer to validate this behavior? A minimal reproducible code will be great.", "1. sorry, the code can't be reproduced, it need the model. \r\n2. the code logic is I use process to load a dl model, one process I represent a worker, the model load in my code as below, sorry, the code a little bit long.\r\n```     \r\n        # add placeholder\r\n        self.input_ids = self.tf.placeholder(self.tf.int32, (None, self.seq_len),\r\n                                             'input_ids')\r\n        self.input_mask = self.tf.placeholder(self.tf.int32, (None, self.seq_len),\r\n                                              'input_mask')\r\n        self.input_type_ids = self.tf.placeholder(self.tf.int32, (None, self.seq_len),\r\n                                                  'input_type_ids')\r\n\r\n        self.standard_input_ids = self.tf.placeholder(self.tf.int32, (None, self.seq_len),\r\n                                                      'standard_input_ids')\r\n        self.standard_input_mask = self.tf.placeholder(self.tf.int32, (None, self.seq_len),\r\n                                                       'standard_input_mask')\r\n        self.standard_input_type_ids = self.tf.placeholder(self.tf.int32, (None, self.seq_len),\r\n                                                           'standard_input_type_ids')\r\n        self._init_graph()\r\n\r\n    def _init_graph(self):\r\n\r\n        from bert_extend.bert_new import modeling\r\n        bert_config = modeling.BertConfig.from_json_file(os.path.join(self.model_dir, 'bert_config.json'))\r\n        all_input_ids = self.tf.concat([self.input_ids, self.standard_input_ids], axis=0)\r\n        all_input_mask = self.tf.concat([self.input_mask, self.standard_input_mask], axis=0)\r\n        all_input_type_ids = self.tf.concat([self.input_type_ids, self.standard_input_type_ids], axis=0)\r\n        self.model = modeling.BertModel(config=bert_config,\r\n                                        is_training=False,\r\n                                        input_ids=all_input_ids,\r\n                                        input_mask=all_input_mask,\r\n                                        token_type_ids=all_input_type_ids,\r\n                                        use_one_hot_embeddings=False)\r\n\r\n        # get output weights and output bias\r\n        ckpt = self.tf.train.get_checkpoint_state(self.model_dir).all_model_checkpoint_paths[-1]\r\n\r\n        final_hidden = self.model.get_sequence_output()\r\n        final_hidden = self.tf.split(final_hidden, axis=0, num_or_size_splits=2)\r\n\r\n        usr_final_hidden = final_hidden[0]\r\n        standard_final_hidden = final_hidden[1]\r\n\r\n        usr_final_hidden = self.atrous_conv1d(usr_final_hidden, self.input_mask, window=3, dilation=1,\r\n                                              scope='atrous_conv1d_1',\r\n                                              reuse=None)\r\n        usr_final_hidden = self.atrous_conv1d(usr_final_hidden, self.input_mask, window=3, dilation=2,\r\n                                              scope='atrous_conv1d_2',\r\n                                              reuse=None)\r\n        usr_final_hidden = self.atrous_conv1d(usr_final_hidden, self.input_mask, window=3, dilation=4,\r\n                                              scope='atrous_conv1d_4',\r\n                                              reuse=None)\r\n\r\n        standard_final_hidden = self.atrous_conv1d(standard_final_hidden, self.standard_input_mask, window=3,\r\n                                                   dilation=1,\r\n                                                   scope='atrous_conv1d_1', reuse=True)\r\n        standard_final_hidden = self.atrous_conv1d(standard_final_hidden, self.standard_input_mask, window=3,\r\n                                                   dilation=2,\r\n                                                   scope='atrous_conv1d_2', reuse=True)\r\n        standard_final_hidden = self.atrous_conv1d(standard_final_hidden, self.standard_input_mask, window=3,\r\n                                                   dilation=4,\r\n                                                   scope='atrous_conv1d_4', reuse=True)\r\n\r\n        from bert_extend.self_attention import multihead_attention\r\n        usr_final_hidden = multihead_attention(usr_final_hidden, standard_final_hidden, standard_final_hidden,\r\n                                               training=False, scope='multi_attention')\r\n\r\n        with self.tf.variable_scope('output'):\r\n            start_logits = self.tf.layers.dense(usr_final_hidden, 1, name='start_logits')\r\n            end_logits = self.tf.layers.dense(usr_final_hidden, 1, name='end_logits')\r\n\r\n            start_logits = self.tf.squeeze(start_logits, axis=-1)\r\n            end_logits = self.tf.squeeze(end_logits, axis=-1)\r\n\r\n            self.unstacked_logits = [start_logits, end_logits]\r\n\r\n        # restore model\r\n        sess_config = self.tf.ConfigProto(allow_soft_placement=True)\r\n        sess_config.gpu_options.allow_growth = True\r\n        graph = end_logits.graph\r\n        saver = self.tf.train.Saver()\r\n        self.sess = self.tf.Session(config=sess_config, graph=graph)\r\n        self.sess.run(self.tf.global_variables_initializer())\r\n        self.tf.reset_default_graph()\r\n        saver.restore(self.sess, ckpt)\r\n\r\n    def atrous_conv1d(self, X, mask, window=3, dilation=1, scope='atrous_conv1d', reuse=None):\r\n        \"\"\"\r\n        expansion of convolution\r\n        :param X: embedding\r\n        :param dilation: the size of expansion\r\n        :param window: the size of kernel length\r\n        \"\"\"\r\n        with self.tf.variable_scope(scope, reuse=reuse):\r\n            filters = X.shape.as_list()[-1]\r\n\r\n            # conv1\r\n            conv1 = self.tf.layers.conv1d(X,\r\n                                          filters,\r\n                                          window,\r\n                                          dilation_rate=dilation,\r\n                                          padding='SAME')\r\n            conv1 = self.tf.subtract(conv1, X)\r\n\r\n            # conv2\r\n            conv2 = self.tf.layers.conv1d(X,\r\n                                          filters,\r\n                                          window,\r\n                                          dilation_rate=dilation,\r\n                                          activation=self.tf.sigmoid,\r\n                                          padding='SAME')\r\n\r\n            conv = X + self.tf.multiply(conv1, conv2)\r\n\r\n            # mask\r\n            mask = self.tf.expand_dims(self.tf.cast(mask, dtype=self.tf.float32), axis=2)\r\n            conv = conv * mask\r\n\r\n            from bert_extend.self_attention import ln\r\n            conv = ln(conv, reuse=reuse, scope=scope)\r\n\r\n            return conv\r\n```", "Hi,\r\n\r\nIt is difficult to help debug this kind of an issue over github since the problem is likely very machine specific.\r\n\r\nAs a first step I would suggest using the [TF profiler](https://www.tensorflow.org/guide/profiler) or [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html) to see if you can spot any obvious differences between the two profiles (crontab and non-crontab execution).", "ok, I will try, thx a lot"]}, {"number": 42133, "title": "Graph visualization failed, in Graph mode [TF 2.2, TB 2.2.1]", "body": "Eager works, though partially - Graph doesn't work at all. [Suggested thread](https://github.com/tensorflow/tensorboard/issues/1961) doesn't reveal much.\r\n\r\n![image](https://user-images.githubusercontent.com/16495490/82794515-a5ee8c80-9e83-11ea-9a48-932c5eda13eb.png)\r\n\r\n\r\n```python\r\nimport shutil\r\nimport tempfile\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.callbacks import TensorBoard\r\n\r\n#%%############################################\r\nipt = Input(shape=(16,))\r\nout = Dense(16)(ipt)\r\nmodel = Model(ipt, out)\r\nmodel.compile('adam', 'mse')\r\n\r\nlogdir = tempfile.mkdtemp()\r\nprint('tensorboard --logdir=\"%s\"' % logdir)\r\ntb = TensorBoard(logdir, write_graph=True)\r\n\r\n#%%############################################\r\nx = y = np.random.randn(160, 16)\r\nmodel.fit(x, y, batch_size=32, callbacks=[tb])\r\n\r\n# shutil.rmtree(logdir)\r\n```\r\n\r\n<hr>\r\n\r\n**Environment info**:\r\n\r\n  - _Browser_: Google Chrome v83.0.4103.61 x64 [also tried Firefox Developer Edition]\r\n  - _System_: Windows 10 x64, GTX 1070, i7-7700HQ 2.8 GHz", "comments": ["I looked at it a bit and it looks like you are using a Functional style of Keras (https://github.com/tensorflow/tensorflow/blob/a3aec8d37ec6dcd7e82a68067162f492be00944f/tensorflow/python/keras/engine/training.py#L236-L237). \r\n\r\nWhile I am confident that Keras will generate op graph when using Sequential model (a special case of Functional), I am not too certain op graphs will be generated in all cases of Functional. It is inconclusive with my cursory reading of <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/functional.py> as it is quite dense. @omalleyt12 could you enlighten us a bit? ", "Also having the same issue. Did you solve this problem eventually?", "Greetings! I am very new to Python and Tensorflow. I am just trying to find out where is a good place to start. It seems Keras is a good starting point. However, I ran into the same issue. I can see the SCALARS' plots. However, I can't see the model in GRAPHS.  I just used a very simple Sequential model as given in a web page tutorial. I understand that Tensorflow Version 2.0 made some changes. I was hoping it's a simple fix in my code to your experts. Thank you in advance.\r\n\r\n \r\n[train_2b.txt](https://github.com/tensorflow/tensorboard/files/4749028/train_2b.txt)\r\n", "I also have the same issue. When I used sequential model and model.fit(), there is a tab for graph, but it shows\" Graph visualization failed\". Also, the example code for @tf.function does not works for me either. ", "Also having this issue with my sub-classed model. I'm using TF 2.4 and Python 3.7.", "Also having the same issue. Did you solve this problem eventually?", "Hi @OverLordGoldDragon, is there a particular reason you are disabling eager execution? In TF2 `tf.compat.v1.disable_eager_execution()` is generally not recommended.", "@nikitamaia Back then it was recommended for speed and memory advantages; I also ensured Graph & Eager compatibility of my packages for users.\r\n\r\nI've switched to Pytorch and no longer seek to resolve any TF bugs. Keeping issue open for others.", "Was able to reproduce the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/97fd01740ea47898f92a32c69d839a86/untitled100.ipynb)..Thanks !", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42133\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42133\">No</a>\n"]}, {"number": 39845, "title": "DLL issue", "body": "C:\\Users\\Admin\\Desktop\\nd\\Project>python\r\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\.virtualenvs\\Project-Ldxc-xwW\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Admin\\.virtualenvs\\Project-Ldxc-xwW\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Admin\\.virtualenvs\\Project-Ldxc-xwW\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Admin\\.virtualenvs\\Project-Ldxc-xwW\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Admin\\.virtualenvs\\Project-Ldxc-xwW\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Admin\\.virtualenvs\\Project-Ldxc-xwW\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\.virtualenvs\\Project-Ldxc-xwW\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Admin\\.virtualenvs\\Project-Ldxc-xwW\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Admin\\.virtualenvs\\Project-Ldxc-xwW\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\admin\\appdata\\local\\programs\\python\\python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>", "comments": ["@nakul1010,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from a similar issue and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39845\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39845\">No</a>\n"]}, {"number": 39844, "title": "Correct the year of the reference paper for CTC", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39844) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 39843, "title": "Differences between model.load_weights and checkpoint.restore", "body": "I can checkpoint.restore(ckfile) without any questions, but raise `Nothing except the root object matched a checkpointed value. Typically this means that the checkpoint does not match the Python program.` by model.load_weights(ckfile). I want to know what's the differences between these two api.\r\n\r\nThank you.", "comments": ["Hi @BSlience, tf.train.Checkpoint and model.load/save_weights are not compatible.\r\nWhen you use tf.train.Checkpoint, you will specific objects as kwargs. However, they are not recognized by the keras model. So you must use checkpoint save() and restore() together.\r\nSee the comment here: https://github.com/tensorflow/tensorflow/blob/2560d6fd31b20e81a5a98a73f325fb1dcf0c68a7/tensorflow/python/keras/engine/training.py#L1976\r\nBTW, this also confused me many times....", "Thanks a lot.@Saduf2019.  It's mean that two kinds of checkpoint in tf2. `Model` checkpoint only can be loaded by model_weight and also `Checkpoint` checkpoint only loaded by `Checkpoint.restore`. `CheckpointCallback` is Model checkpoint.\r\n```python\r\nEPOCHS = 10\r\ncheckpoint_filepath = '/tmp/checkpoint'\r\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_filepath,\r\n    save_weights_only=True,\r\n    monitor='val_acc',\r\n    mode='max',\r\n    save_best_only=True)\r\n\r\n# Model weights are saved at the end of every epoch, if it's the best seen\r\n# so far.\r\nmodel.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\r\n\r\n# The model weights (that are considered the best) are loaded into the model.\r\nmodel.load_weights(checkpoint_filepath)\r\n```\r\n"]}, {"number": 39842, "title": "Loaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.5.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.", "body": "\r\n\r\n**System information**\r\n- WIN10:\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version (2.2.0):\r\n```\r\ntensorboard               2.2.1                    pypi_0    pypi\r\ntensorboard-plugin-wit    1.6.0.post3              pypi_0    pypi\r\ntensorflow                2.2.0                    pypi_0    pypi\r\ntensorflow-estimator      2.2.0                    pypi_0    pypi\r\ntensorflow-gpu            2.2.0                    pypi_0    pypi\r\ntensorflow-gpu-estimator  2.2.0                    pypi_0    pypi\r\n```\r\n- Python    3.7.3   :\r\n- CUDA 10.1/cuDNN 7.6.5:\r\n- GPU model and memory:NVIDIA 1070Ti 8G \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n`v2.2.0-rc4-8-g2b96f3662b 2.2.0`\r\n\r\n**Describe the current behavior**\r\nwhen i use [keras_ocr](https://github.com/faustomorales/keras-ocr) project , cmd print `Loaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.5.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.` however the version of cuda and cudnn is good.\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport matplotlib.pyplot as plt\r\n\r\nimport keras_ocr\r\nimport cv2\r\n# keras-ocr will automatically download pretrained\r\n# weights for the detector and recognizer.\r\npipeline = keras_ocr.pipeline.Pipeline()\r\n\r\n# Get a set of three example images\r\nimages = [\r\n    keras_ocr.tools.read(url) for url in [\r\n        '.\\debug\\Army_Reserves_Recruitment_Banner_MOD_45156284.jpg',\r\n        '.\\debug\\EUBanana-500x112.jpg',\r\n        '.\\debug\\FseeG2QeLXo.jpg'\r\n    ]\r\n]\r\nfor image in images:\r\n    image = cv2.resize(image,(480 ,640))\r\n\r\n# Each list of predictions in prediction_groups is a list of\r\n# (word, box) tuples.\r\nprediction_groups = pipeline.recognize(images)\r\n\r\n# Plot the predictions\r\n#fig, axs = plt.subplots(nrows=len(images), figsize=(20, 20))\r\n#for ax, image, predictions in zip(axs, images, prediction_groups):\r\n#    keras_ocr.tools.drawAnnotations(image=image, predictions=predictions, ax=ax)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nC:\\Users\\\u6709\u5bf9\u8c61\u771f\u597d\\Desktop>python lal.python\r\n2020-05-25 10:54:31.190482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nLooking for C:\\Users\\\u6709\u5bf9\u8c61\u771f\u597d\\.keras-ocr\\craft_mlt_25k.h5\r\n2020-05-25 10:54:34.541182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-05-25 10:54:34.572573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:09:00.0 name: GeForce GTX 1070 Ti computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 19 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-05-25 10:54:34.583784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-05-25 10:54:34.598399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-05-25 10:54:34.617941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-05-25 10:54:34.630153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-05-25 10:54:34.653711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-05-25 10:54:34.670926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-05-25 10:54:34.710546: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-05-25 10:54:34.718154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-05-25 10:54:34.726651: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-05-25 10:54:34.747053: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1427ab75710 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-25 10:54:34.754351: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-05-25 10:54:34.767358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:09:00.0 name: GeForce GTX 1070 Ti computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 19 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-05-25 10:54:34.785846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-05-25 10:54:34.796338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-05-25 10:54:34.805291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-05-25 10:54:34.817470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-05-25 10:54:34.827060: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-05-25 10:54:34.836200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-05-25 10:54:34.845863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-05-25 10:54:34.856443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-05-25 10:54:35.395109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-05-25 10:54:35.401033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0\r\n2020-05-25 10:54:35.412130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\r\n2020-05-25 10:54:35.421922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6285 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\n2020-05-25 10:54:35.445885: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1421e063660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-05-25 10:54:35.454505: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070 Ti, Compute Capability 6.1\r\n2020-05-25 10:54:56.938514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-05-25 10:54:57.626411: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.5.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2020-05-25 10:54:57.662951: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.5.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\nTraceback (most recent call last):\r\n  File \"lal.python\", line 8, in <module>\r\n    boxes = detector.detect(images=[image])[0]\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_ocr\\detection.py\", line 679, in detect\r\n    boxes = getBoxes(self.model.predict(np.array(images), **kwargs),\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 88, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1268, in predict\r\n    tmp_batch_outputs = predict_function(iterator)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 650, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node model_1/basenet.slice1.0/Conv2D (defined at C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_ocr\\detection.py:679) ]] [Op:__inference_predict_function_3683]\r\n\r\nFunction call stack:\r\npredict_function\r\n```", "comments": ["@jackCplusplus \r\n\r\nI have tried in colab with TF version 2.2.0 and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a3c3b614df8dc7899a9ef71914b4d3d8/untitled38.ipynb).Thanks!", "Does this colab run on WIN10?\r\nHow can we see the runtime setup of this gist colab (e.g. cuda version, python version etc)?\r\n\r\nThanks.", "Hi\r\n    the project [keras-ocr](https://github.com/faustomorales/keras-ocr) can run in win10,but in anaconda not in google colab.\r\nin cmd console, input `python` check python version. cuda version check in Environment variables\u3002use 'pip freeze' show all.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39842\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39842\">No</a>\n", " I encounter this problem now.\r\nat the beginning, there are some dll missing .and then I download the dll and then ,encounter this problem"]}, {"number": 39841, "title": "Build tensorflow from source errors", "body": "**System information**\r\nPlatform: IBM power8 ppc64le machine\r\nOS: Red Hat Enterprise Linux Server 7.4 (Maipo)\r\nTensorFlow build from source:\r\n    Tensorflow source: v2.2.0 , commit: 2b96f36 - 05/05/2020\r\n     Bazel version: 2.0.0:\r\n    GCC/Compiler version: 7.3.0\r\n    CUDA/cuDNN version:  cuda -10.1.105/libcudnn.so.7.5.0\r\n   GPU: Tesla P100-SXM2-16GB\r\n\r\nThis is the command I used to build tensorflow from source:\r\n\r\n      bazel build --jobs 10 //tensorflow/tools/pip_package:build_pip_package\r\n\r\nThe following is the errors:\r\n\r\n    ERROR: /home/users/apps/ppc64le/tensorflow/tensorflow/lite/python/optimize/BUILD:50:1: SWIGing tensorflow/lite/python/optimize/calibration_wrapper.i failed (Exit 1)\r\n    bazel-out/host/bin/external/swig/swig: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by bazel-out/host/bin/external/swig/swig)\r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n    Use --verbose_failures to see the command lines of failed build steps.\r\n    ERROR: /home/users/apps/ppc64le/tensorflow/tensorflow/python/tools/BUILD:142:1 SWIGing tensorflow/lite/python/optimize/calibration_wrapper.i failed (Exit 1)\r\n\r\nThe system library /lib/libstdc++.so.6 points to libstdc++.so.6.0.19,  but gcc/7.3.0 has the version of libstdc++.so.6.0.25 which covers CXXABI_1.3.9. How to have the build link to libstdc++.so.6 of gcc/7.3.0  instead of system /lib64/libstdc++.so.6? Using LD_LIBRARY_PATH seems doesn't work.\r\n\r\nThanks in advance!\r\n\r\nBest,\r\nShelton\r\n    \r\n", "comments": ["@nayana-ibm could you take a look?", "Or maybe I confused with s390x? who would be the best person to talk to about ppc?", "@gunan Yes...this is ppc", "I am very sorry for the confusion, @jayfurmanek @wdirons I hope you are the correct people to ask about ppc64?", "Thanks @gunan !\r\n\r\nHi, What toolchain are you using? And did you build that version of bazel yourself with that same toolchain?", "I used gcc/7.3.0. Yes - use the same compiler to build bazel.\r\nThe generated executable \"bazel-out/host/bin/external/swig/swig\" during the build always was linked to system /lib64/libstdc++.so.6. It seems that there is no way to have it linked to its own libstdc++.so.6 from the compiler (gcc/7.3.0). \r\n", "I meet this issue and get it fixed with these methods:\r\n* For SWIGing: https://github.com/tensorflow/tensorflow/pull/32198#issuecomment-638437695\r\n* For google protoc: https://github.com/tensorflow/tensorflow/issues/40860#issuecomment-651629927\r\n\r\nHope it can help you.", "@sheltongeosx ,\r\nCan you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/39841#issuecomment-657732814) and also please try to install latest tf v2.5 or v2.6 and let us know if the issue still persists.Thanks", "@tilakrayal, @Zantares, ...\r\nThank you all for fixing the issues. Unfortunately I am not able to verify the fixes right now through tf installation because our ppc64le platform had been decommissioned from our data center. \r\nThank you again guys for your great helps!\r\n", "@sheltongeosx ,\r\nPlease feel free to move this issue to closed status as this issue has resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39841\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39841\">No</a>\n"]}, {"number": 39840, "title": "Fix typo in Core ML Delegate docs", "body": "This PR simply corrects a typo in the Core ML Delegate documentation.", "comments": []}, {"number": 39838, "title": "ValueError thrown in trivial Keras model using tf.split", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, pasted below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nAnaconda, \"tensorflow-gpu\"\r\n- TensorFlow version (use command below):\r\n2.1.0\r\n- Python version:\r\n3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nKeras model using tf.split (or tf.tile, or tf.zeros) hits the following exception, via \"assert_shallow_structure\" in nest.py, because one of the TensorSpecs is deeper in a sequence than the other (e.g. [[[TensorSpec]]] vs. [TensorSpec]). The actual trigger is tf.keras.models.load_model.\r\n\r\nValueError\r\nCould not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (1 total):\r\n    * Tensor(\"inputs:0\", shape=(None, 256, 8, 8), dtype=float32)\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 1 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (1 total):\r\n    * [TensorSpec(shape=(None, 256, 8, 8), dtype=tf.float32, name='inputs/0')]\r\n  Keyword arguments: {}\r\n\r\n**Describe the expected behavior**\r\nLoading the SavedModel in Keras succeeds without an exception.\r\n\r\n**Standalone code to reproduce the issue**\r\nimport tensorflow as tf\r\ninput = tf.keras.layers.Input(shape=(256, 8, 8), dtype=\"float32\")\r\noutput, y = **tf.split(input, 2, axis=1)**\r\nmodel = tf.keras.Model(input, output)\r\nmodel.save(\"C:\\\\Users\\\\Public\\\\repro\", save_format=\"tf\")\r\ntf.keras.models.load_model(\"C:\\\\Users\\\\Public\\\\repro\")\r\n\r\n**Workaround**\r\nWrapping the tf.split or other tf operation in a Keras Lambda layer works around the bug. Example code:\r\nimport tensorflow as tf\r\ninput = tf.keras.layers.Input(shape=(256, 8, 8), dtype=\"float32\")\r\noutput, y = **tf.keras.layers.Lambda(lambda tensor : tf.split(tensor, 2, axis=1))(input)**\r\nmodel = tf.keras.Model(input, output)\r\nmodel.save(\"C:\\\\Users\\\\Public\\\\repro\", save_format=\"tf\")\r\ntf.keras.models.load_model(\"C:\\\\Users\\\\Public\\\\repro\")\r\n\r\n**Other info / logs**\r\n", "comments": ["The problem is that even if operations like tf.*, \"output = input * 5\", \"output = input[:, :128, :, :]\" are meant to be wrapped in a Keras layer (e.g. subclassed or Lambda), without doing so (a) the model will happily compile and train for hours, and (b) the exception thrown gives no indication of what the developer has done wrong. Fast-failing and clearer doc examples may both help at a project level.", "@chrisbutner \r\n\r\nI am not seeing any issue with TF version 2.2.0.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ed12871c438ae1ab8e3fbc0a0af35e31/untitled37.ipynb).Please verify once and close the issue. Thanks!", "Thanks. Confirmed fixed on 2.2.0, can train and predict after loading.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39838\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39838\">No</a>\n"]}, {"number": 39837, "title": "Import Issue ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Traceback (most recent call last):\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>    \r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 6, in <module>\r\n    from warpgan import WarpGAN\r\n  File \"C:\\Users\\Swapnil Kadakia\\Downloads\\WarpGAN-master\\WarpGAN-master\\warpgan.py\", line 32, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>    \r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>    \r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Swapnil Kadakia\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'", "I am constantly getting this error, i have installed tensorflow and do not know how tomsolve this erroro", "@swapnilkadakia,\r\nCould you please check if you are using 64-bit Python on your machine. TensorFlow is tested and supported on the following 64-bit systems. For more information, please check [this document](https://www.tensorflow.org/install#install-tensorflow-2). \r\n\r\nAlso, please mention the make and model of your CPU along with the TensorFlow version you are trying to install. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39837\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39837\">No</a>\n"]}, {"number": 39836, "title": "Recall and Precision of binary classifciation not the same as manual calculated", "body": "i have a Keras Sequential Network and use the validation_data attribute to get an idea of the validation straight away. \r\n\r\nHowever when the model is trained and I predict on the validation set I dont get as high scores as keras is telling me through training.\r\n\r\nThis is the model and training:\r\n\r\n```\r\ndef getKerasModel(ndim):\r\n\r\n    model = Sequential()\r\n    model.add(Dense(100, activation='relu', input_shape=(ndim,)))\r\n    model.add(Dense(40, activation='relu'))\r\n    model.add(Dense(1, activation='sigmoid'))\r\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\r\n    return model\r\n```\r\nThen I train it:\r\n```\r\nfrom keras.wrappers.scikit_learn import KerasClassifier\r\nfrom keras.utils import to_categorical\r\n\r\n# pipeline is doing scales and one hot encoding\r\nX_train2 = full_pipeline.fit_transform(X_train)\r\nX_val2 = full_pipeline.transform(X_val)\r\n\r\nmodel = getKerasModel(ndim=X_train2.shape[1])\r\nmodel.fit(X_train2, y_train,epochs=5, batch_size=32, verbose=True, validation_data = (X_val2, y_val))\r\n```\r\nNow this is the result:\r\n```\r\nTrain on 265815 samples, validate on 38247 samples\r\nEpoch 1/5\r\n265815/265815 [==============================] - 12s 45us/step - loss: 0.2715 - precision: 0.6690 - recall: 0.4065 - val_loss: 0.2965 - val_precision: 0.6875 - val_recall: 0.4654\r\nEpoch 2/5\r\n265815/265815 [==============================] - 13s 50us/step - loss: 0.2300 - precision: 0.6956 - recall: 0.4959 - val_loss: 0.3735 - val_precision: 0.7000 - val_recall: 0.5192\r\nEpoch 3/5\r\n265815/265815 [==============================] - 13s 47us/step - loss: 0.2111 - precision: 0.7020 - recall: 0.5376 - val_loss: 0.4003 - val_precision: 0.7071 - val_recall: 0.5509\r\nEpoch 4/5\r\n265815/265815 [==============================] - 17s 63us/step - loss: 0.1988 - precision: 0.7113 - recall: 0.5627 - val_loss: 0.4695 - val_precision: 0.7113 - val_recall: 0.5735\r\nEpoch 5/5\r\n265815/265815 [==============================] - 16s 61us/step - loss: 0.1900 - precision: 0.7120 - recall: 0.5831 - val_loss: 0.4893 - val_precision: 0.7134 - val_recall: 0.5908\r\n```\r\nSo it definetly says after the last epoch :\r\n```\r\nval_precision: 0.7134 - val_recall: 0.5908\r\n```\r\nWhen I now use the model, do prediction and evaluate precision and recall seperatly it is much lower:\r\n```\r\npr = model.predict(X_val2)\r\n\r\nbinary_result = [1 if i[0] > 0.5 else 0 for i in pr]\r\nprint(precision_score(y_val,binary_result))\r\nprint(recall_score(y_val,binary_result))\r\n\r\n>> 0.23\r\n>> 0.38\r\n```\r\nI don't get why keras is doing much better in its evaluation", "comments": ["@tuxmania87  \r\nI ran the code shared by you and face this error as per this [gist](https://colab.sandbox.google.com/gist/Saduf2019/68df26553bf2eb4fb159b09f69f6720d/untitled197.ipynb), please share complete stand alone code or a colab gist to reproduce the issue faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39835, "title": "Help Changing Tensor Value", "body": "How can I change a particular tensor value? It seems eagertensors cannot be changed at all in either keras or tensorflow, but it seems to me that this should be a relatively straightforward task.\r\n\r\n\r\n**System information**\r\n- TensorFlow version 2+:\r\n- Are you willing to contribute it (No): I don't know how\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCannot change tensor values\r\n\r\n**Will this change the current api? How?**\r\nGive more flexibility to users\r\n\r\n**Who will benefit with this feature?**\r\nAll TF users\r\n", "comments": ["@ryanmaxwell96 \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "Hello, thank you for the quick reply. I'm currently working on an idea to train in stages and for this I need to lock some values coming from a Dense layer in place and then unlock them after a certain stage of training. I thought it would be a simple task to just assign the values to be what I want them to be, but apparently not.", "Keras backend exposes [get_value](https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_value) API which can be used to set the variables.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank you. I realized that I can lock vehicle appendages more directly with the inputs to the actual simulation itself instead, so I think I've got it figured out."]}, {"number": 39834, "title": "Flops calculation in tensorflow 2.x version.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\ntensorflow 2.2\r\n- Are you willing to contribute it (Yes/No):\r\nNo\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFlops calculation in tf1.x usually use  tf.compat.v1.profiler.ProfileOptionBuilder,but it doesn't work success in tf2.x,So,anyone can show me the official example to calculate flop in tf2.x version?\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nPeople who care about model efficiency.\r\n**Any Other info.**\r\n", "comments": ["> Flops calculation in tf1.x usually use tf.compat.v1.profiler.ProfileOptionBuilder,but it doesn't work success in tf2.x\r\n\r\n@weichen456,\r\nCould you please provide a minimal reproducible code regarding this so that we can look into it. \r\n\r\nAlso, please check this [documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/profiler/ProfileOptionBuilder) for ProfileOptionBuilder and let us know if it helps. Thanks!", "> > Flops calculation in tf1.x usually use tf.compat.v1.profiler.ProfileOptionBuilder,but it doesn't work success in tf2.x\r\n> \r\n> @weichen456,\r\n> Could you please provide a minimal reproducible code regarding this so that we can look into it.\r\n> \r\n> Also, please check this [documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/profiler/ProfileOptionBuilder) for ProfileOptionBuilder and let us know if it helps. Thanks!\r\n\r\nhello,\r\nmy code is here https://colab.research.google.com/drive/1pA_cYnMj5JXxCCsJcGISmaZjmXJ5BTf9?usp=sharing\r\nThe params is work well, but the flops is zero.\r\ncan you give me a official example in tf2.x version?", "Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/c9337fcd83ce34c7a8d968b2980070c9/39763.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/cccc06e3bdd090a88ba9ccfe805c07c0/39763-tf-nightly.ipynb). \r\n\r\nWorks as expected with [TF v1.15](https://colab.research.google.com/gist/amahendrakar/4c946d63385f289e796422607dc29f2f/39763-1-15.ipynb). Please find the attached gist. Thanks!", "It will be very valuable to have the issue fixed.", "ThIs Is a duplicate of https://github.com/tensorflow/tensorflow/issues/32809", "@weichen456 this ticket is a duplicate of #32809  ,please let us know if we can close this issue ..thanks !", "> @weichen456 this ticket is a duplicate of #32809 ,please let us know if we can close this issue ..thanks !\r\n\r\nOk", "@weichen456 Thank you for your response. closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39834\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39834\">No</a>\n"]}]