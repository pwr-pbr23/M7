[{"number": 24628, "title": "Fix typos", "body": "Fix typos\r\n\r\nso that that -> so that\r\n\r\noutptus -> outputs", "comments": []}, {"number": 24627, "title": "Keras model summary does not display \"Connected to\" correctly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=\"1.13.0-dev20181226\" (this is the TF 2.0-preview)\r\nGIT_VERSION=\"b'v1.12.0-5133-gc343196842'\"\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nThe `model.summary()` method does not display the **Connect to** column correctly when using the functional API (see the full output below). It looks like `<tensorflow.python.keras.engine.i` or `<tensorflow.python.keras.layers.c` or `<tensorflow.python.keras.layers.m`, which are probably truncated object names. Perhaps these objects are missing a `__str__()` method?\r\n\r\n**Describe the expected behavior**\r\nI expect the same nice output as in previous versions, such as `input_1[0][0]`, `dense_3[0][0]` or `concatenate[0][0]`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninput_data = keras.layers.Input(shape=[8])\r\nhidden1 = keras.layers.Dense(30, activation=\"relu\")(input_data)\r\nhidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\r\nconcat = keras.layers.concatenate([input_data, hidden2])\r\noutput = keras.layers.Dense(1)(concat)\r\nmodel = keras.models.Model(inputs=[input_data], outputs=[output])\r\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\r\n\r\nmodel.summary()\r\n```\r\n\r\n**Other info / logs**\r\nHere is the output:\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 8)]          0\r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 30)           270         <tensorflow.python.keras.engine.i\r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 30)           930         <tensorflow.python.keras.layers.c\r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 38)           0           <tensorflow.python.keras.engine.i\r\n                                                                 <tensorflow.python.keras.layers.c\r\n__________________________________________________________________________________________________\r\ndense_2 (Dense)                 (None, 1)            39          <tensorflow.python.keras.layers.m\r\n==================================================================================================\r\nTotal params: 1,239\r\nTrainable params: 1,239\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n```\r\n\r\nAnd here is the output in TF 1.12:\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput_1 (InputLayer)            (None, 8)            0\r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 30)           270         input_1[0][0]\r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 30)           930         dense[0][0]\r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 38)           0           input_1[0][0]\r\n                                                                 dense_1[0][0]\r\n__________________________________________________________________________________________________\r\ndense_2 (Dense)                 (None, 1)            39          concatenate[0][0]\r\n==================================================================================================\r\nTotal params: 1,239\r\nTrainable params: 1,239\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n```", "comments": ["Added a PR #24629 for the fix.", "Thanks @yongtang , looks much better. \ud83d\udc4d "]}, {"number": 24626, "title": "Master build crash with GPU support testing with OpenSeq2Seq ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.6 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Mobile\r\n- TensorFlow installed from (source or binary): Clone from github, master branch\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source):  gcc version 6.3.0 20170516 (Debian 6.3.0-18+deb9u1)\r\n- CUDA/cuDNN version: cuDNN 7.4.1 for CUDA 10.0\r\n- GPU model and memory: GTX2070, 8G\r\n\r\nTo reproduce:\r\n\r\n- Install latest CUDA and cuDNN and make sure CUDA samples and cuDNN samples run successfully. Make sure CUDA things are OK.\r\n- Checkout the latest tensorflow code, configure with cuda, bazel build with --config=opt --config=cuda\r\n- Waiting then create python package and install\r\n- ldd /usr/local/lib/python3.5/site-packages/tensorflow/libtensorflow_framework.so everything is linked correct.\r\n\r\n- Download OpenSeq2Seq git clone https://github.com/NVIDIA/OpenSeq2Seq.git\r\n- Run toy test\r\ncd OpenSeq2Seq; python3 run.py --config_file=example_files/speech2text/ds2_toy_config.py --mode=train\r\n\r\n**CRASH and generate core dump in libtensorflow_framework.so.**\r\n\r\n\r\n", "comments": ["Hello, Can you please step back to the following build: tensorflow_gpu-1.12.0 , gcc 4.8 , bazel 0.15.0 , cuDNN 7 , CUDA 9 , and let us know. Thanks.\r\n\r\n\r\n", "Hi, Thanks for your response.\r\n\r\nBut I do not have CUDA 9, only CUDA 10 for my RTX card.\r\nIf CUDA 10 is helpful, I can try.\r\nOne question is 'tensoflow_gpu-1.12.0` the latest release?\r\n\r\nThanks.\r\n", "Hi @dinguijin , yes, tensorflow_gpu-1.12.0 is latest stable official release. Only CUDA 9 is advised with cuDNN 7 currently for r1.12.0 ; Please check with the GPU vendor for a CUDA 9 library.", "I want to build tensorflow from the latest source not the latest release 1.12. Thanks though.", "@dingguijin , yes, tensorflow_gpu-1.12.0 is the latest official source to build from, with the above mentioned configuration of CUDA 9 / cuDNN 7. Thanks."]}, {"number": 24625, "title": "Fix TypeError when using tf.keras.utils.plot_model", "body": "This fix fixes the issue raised in #24622 where a TypeError was raised when using `tf.keras.utils.plot_model`.\r\n\r\nThis fix fixes #24622.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Reviewer @pavithrasv: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24624, "title": "Load weights from TensorFlow checkpoint to Keras model", "body": "I have trained a TensorFlow with Keras model and using `keras.callbacks.ModelCheckpoint` I've saved the weights as follows:\r\n\r\n    cp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path,\r\n                                              save_weights_only=True,\r\n                                              verbose=1)\r\n    model.fit(X_train, Y_train, callbacks=[cp_callback], epochs=50, batch_size=256)\r\n\r\nBut while trying to load the saved weights there is nothing changed to my model, after building the model architecture and compile it I load weights as follows:\r\n\r\n    model.load_weights('./checkpoints/cp.ckpt')\r\n\r\nBut nothing happens, the testing accuracy is as random guessing, while my real testing accuracy is 80.49%\r\n\r\nThe model consists of keras dence layers with l2 kernel_regularizer and glorot kernel_initializer, also I'm using TensorFlow version 1.12.0, any ideas?", "comments": ["Do you use `tf.keras` or `keras` standalone?\r\n \r\nAre you sure your `checkpoint_path` is correct?\r\n\r\nCould you run the following script? \r\n```Python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\n\r\ndef build_model():\r\n    model = tf.keras.models.Sequential()\r\n\r\n    model.add(tf.keras.layers.Flatten(input_shape=(28,28,)))\r\n    model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\r\n    model.add(tf.keras.layers.Dense(10, activation=\"sigmoid\"))\r\n    \r\n    return model\r\n\r\nmodel = build_model()\r\n\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(), \r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nprint(model.summary())\r\n\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\"./weights.{epoch:02d}.hdf5\",\r\n                                          save_weights_only=True,\r\n                                          verbose=1)\r\nmodel.fit(x_train, y_train, callbacks=[cp_callback], epochs=3, batch_size=256)\r\n\r\n\r\n# rebuild model as model2\r\n# simulating starting a new script\r\nmodel2 = build_model()\r\nmodel2.compile(optimizer=tf.train.AdamOptimizer(), \r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n# check that the new model does indeed have random weights\r\nprint (model2.evaluate(x=x_test, y=y_test))\r\n\r\n# load 3rd epoch weights\r\nmodel2.load_weights('./weights.03.hdf5')\r\n\r\n# these should be equal\r\nprint (model2.evaluate(x=x_test, y=y_test))\r\nprint (model.evaluate(x=x_test, y=y_test))\r\n```", "I'm working with `tf.keras` and the checkpoint_path is correct. Also, the output of the evaluation in your code is correct:\r\n\r\n```\r\n10000/10000 [==============================] - 0s 43us/step\r\n[9.01510094833374, 0.1127]\r\n10000/10000 [==============================] - 0s 39us/step\r\n[1.2179515656471251, 0.674]\r\n10000/10000 [==============================] - 0s 41us/step\r\n[1.2179515656471251, 0.674]\r\n```\r\n\r\nI think I figure the problem, after I load the weights I test it with the same testing data but shuffled (not in the same order) is this effects the accuracy? because if I evaluate using the same order it gives me the same results of the original model.\r\n\r\nAnother thing is when I load the weights I can't start training from the point I've stopped in, any idea?", "Since the accuracy metric is a reduce operation, the order of the data should not matter. \r\n\r\nIt's important, however, that when you're shuffling, that you're preserving the same order between the features and labels.\r\n\r\nWhen you compile the model, you lose the optimizer state meaning it will require a few iterations to dial the learning rate back in. This may be why you can't resume training from the point you've stopped.\r\n\r\nWhile I'm very new here, I think there's an official policy saying that Github issues are only for Tensorflow bugs, and since this doesn't appear to be a bug in Tensorflow, you may have a better experience asking subsequent questions on Stackoverflow using the tensorflow tag. ", "Actually I've asked the same question on Stackoverflow here: https://stackoverflow.com/questions/53971139/load-weights-from-tensorflow-checkpoint-to-keras-model but there is no answer, the shuffling should be ok, but still I don't know why the accuracy decreases after loading the model.", "Could you send a working code snippet that demonstrates this behavior? ", "This is the code, by now the code saves and restore the checkpoint weights correctly. But as I said before, the testing data should be in the same order and it is not working from the same point that ends in with the fitting process.\r\n\r\n```\r\nimport os\r\nimport math\r\nimport pickle\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.callbacks import Callback\r\n\r\ndef create_dense_layer(neurons):\r\n    return keras.layers.Dense(neurons, activation='relu',\r\n                                        kernel_initializer=keras.initializers.glorot_normal(seed=961),\r\n                                        kernel_regularizer=keras.regularizers.l2(0.0001))\r\n\r\ndef create_model():\r\n    model = tf.keras.models.Sequential([\r\n        keras.layers.Dense(200, activation=tf.nn.relu, input_shape=(200,)),\r\n        create_dense_layer(500),\r\n        create_dense_layer(500),\r\n        create_dense_layer(450),\r\n        create_dense_layer(400),\r\n        create_dense_layer(400),\r\n        create_dense_layer(350),\r\n        create_dense_layer(300),\r\n        create_dense_layer(300),\r\n        create_dense_layer(250),\r\n        create_dense_layer(200),\r\n        create_dense_layer(200),\r\n        create_dense_layer(150),\r\n        create_dense_layer(100),\r\n        create_dense_layer(100),\r\n        create_dense_layer(50),\r\n        create_dense_layer(25),\r\n        keras.layers.Dense(CLSS_NUM, activation=tf.nn.softmax)\r\n    ])\r\n    \r\n    model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=tf.train.AdagradOptimizer(learning_rate=0.0075), metrics=['accuracy'])\r\n    \r\n    return model\r\n\r\nmodel = create_model()\r\n\r\nclass TestCallback(Callback):\r\n    def __init__(self, test_data):\r\n        self.test_data = test_data\r\n\r\n    def on_epoch_end(self, epoch, logs={}):\r\n        x, y = self.test_data\r\n        loss, acc = self.model.evaluate(x, y, verbose=0, batch_size=256)\r\n        print('\\nTesting loss: {}, acc: {}'.format(loss, acc))\r\n\r\ncheckpoint_path = 'checkpoints/cp-{epoch:04d}.ckpt'\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=0)\r\n\r\nmodel.fit(X_train, Y_train, epochs=10, batch_size=256, callbacks=[cp_callback, TestCallback((X_test, Y_test))])\r\n\r\nloss, acc = model.evaluate(X_test, Y_test, batch_size=256)\r\nprint(loss, acc)\r\n```", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "> Actually I've asked the same question on Stackoverflow here: https://stackoverflow.com/questions/53971139/load-weights-from-tensorflow-checkpoint-to-keras-model but there is no answer, the shuffling should be ok, but still I don't know why the accuracy decreases after loading the model.\r\n\r\nI am facing this behavior but I am thinking that when the callback saves the model or model weights to the h5 file it uses the train acc value to save, which could be logically bigger than the validation accuracy, if I am right comment", "Hello @LeninGF, the problem I faced previously was about the construction of the vocabulary. I was using Python set without sorting it, so the vocabulary order within the training was different from the vocabulary order within the testing."]}, {"number": 24623, "title": "tf.keras.Model.load_weights failed", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Both on Ubuntu 16.04 / Windows 10 1803\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: Python 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44) \r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 9.0 & cuDNN 7.1.4\r\n- GPU model and memory: NVIDIA GeForce GTX 1080Ti 12Gb\r\n\r\n**Describe the current behavior**\r\n\r\nWhen load a trained keras model (`tf.keras.Model`):\r\n```python3\r\nmodel.load_weights(\"./test/trained.h5\")\r\n```\r\nIt will raise the bug\r\n`ValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.`\r\n\r\nIt's discussed in keras repo: https://github.com/keras-team/keras/issues/10417 and  https://github.com/keras-team/keras/issues/11683 and has not been fixed so far.\r\n\r\nIt still exists in latest keras version but it only happens on `keras >= 2.2.0` (reported by https://github.com/keras-team/keras/issues/10417#issuecomment-402696659). However, it's also exists in `2.1.6-tf` with tensorflow 1.12.\r\n\r\n**Describe the expected behavior**\r\nLoad the model from the `*.h5` file.\r\n\r\n**Code to reproduce the issue**\r\nTo reproduce the issue, run the demo:\r\n```python3\r\nimport numpy as np\r\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint\r\nfrom tensorflow.python.keras.layers import Dense\r\n\r\nfrom files_functions import *\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef measure(y_true, y_pred):\r\n    return tf.reduce_mean(y_pred - y_true)\r\n\r\n\r\nclass DemoNet(tf.keras.Model):\r\n    def __init__(self):\r\n        super(DemoNet, self).__init__()\r\n        self.encoder_layer = Dense(128, input_shape=(128,), activation='tanh')\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        \"\"\"Run the model.\"\"\"\r\n        encoded = self.encoder_layer(inputs)\r\n\r\n        return encoded\r\n\r\n\r\nmodel = DemoNet()\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(),\r\n              loss='mse',\r\n              metrics=[measure])\r\n\r\ntraining_input = np.ones((30000, 128)).astype(np.float32)\r\ncurrent_path = \"./test/\"\r\nsave_path = current_path\r\ncheckpoint = ModelCheckpoint(save_path + '/trained.h5', monitor='val_loss', verbose=0, save_best_only=True,\r\n                             mode='min',\r\n                             save_weights_only=True)\r\nmodel.fit(training_input, training_input, epochs=1, batch_size=512, verbose=2, \\\r\n          callbacks=[checkpoint], validation_split=0.2)\r\n\r\ndel model\r\n\r\nmodel = DemoNet()\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(),\r\n              loss='mse',\r\n              metrics=[measure])\r\n\r\ntestdata = np.ones((3000, 128))\r\n\r\nmodel.load_weights(\"./test/trained.h5\")\r\neval = model.evaluate(testdata, testdata, batch_size=512)\r\n```\r\nThe demo simpily build a network with one fully connected layer. It will raise the error:\r\n`ValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.`\r\n\r\nThe environment of python can be installed by `requrement.txt`:\r\n```\r\nPackage             Version   \r\n------------------- ----------\r\nabsl-py             0.6.1     \r\nastor               0.7.1     \r\ncertifi             2018.11.29\r\nchannelnorm-cuda    0.0.0     \r\nchardet             3.0.4     \r\ncorrelation-cuda    0.0.0     \r\ncycler              0.10.0    \r\ngast                0.2.0     \r\ngrpcio              1.16.1    \r\nh5json              1.1.3     \r\nh5py                2.8.0     \r\nidna                2.8       \r\nKeras               2.1.0     \r\nKeras-Applications  1.0.6     \r\nKeras-Preprocessing 1.0.5     \r\nkiwisolver          1.0.1     \r\nlxml                4.2.5     \r\nMarkdown            3.0.1     \r\nmatplotlib          3.0.2     \r\nnumpy               1.15.4    \r\nPillow              5.2.0     \r\npip                 18.1      \r\nprotobuf            3.6.0     \r\npyparsing           2.3.0     \r\npython-dateutil     2.7.5     \r\nPyYAML              3.13      \r\nrequests            2.21.0    \r\nresample2d-cuda     0.0.0     \r\nscipy               1.1.0     \r\nsetuptools          40.6.2    \r\nsix                 1.11.0    \r\ntensorboard         1.12.0    \r\ntensorflow-gpu      1.12.0    \r\ntermcolor           1.1.0     \r\nurllib3             1.24.1    \r\nWerkzeug            0.14.1    \r\nwheel               0.32.3\r\n```\r\n\r\n**Other info / logs**\r\nTrackback of the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/***/***/test.py\", line 51, in <module>\r\n    model.load_weights(\"./test/trained.h5\")\r\n  File \"/home/***/anaconda3/envs/transmitter/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1544, in load_weights\r\n    saving.load_weights_from_hdf5_group(f, self.layers)\r\n  File \"/home/***/anaconda3/envs/transmitter/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py\", line 784, in load_weights_from_hdf5_group\r\n    ' layers.')\r\nValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.\r\n```", "comments": ["You didn't initialize the weights when creating the 2nd model. \r\n\r\nWhen using the model class API, you need to initialize the weights manually. You can do this by training on just a single example. (don't worry the learned weights will be overwritten when you load your own weights) \r\n\r\nYou can do this by inserting the following line after you compile the model for the 2nd time. \r\n\r\n```Python\r\nmodel.fit(training_input[:1], training_input[:1], epochs=1)\r\n```\r\n\r\nAlternatively, you can replace your `DemoNet` model with one of the following schemes which automatically initializes the weights. \r\n\r\nFor example sequential,\r\n```Python\r\ndef DemoNet():\r\n    model = tf.keras.models.Sequential()\r\n\r\n    model.add(tf.keras.layers.Dense(128, input_shape=(128,), activation=\"tanh\"))\r\n    \r\n    return model\r\n```\r\n\r\nOr using the functional API,\r\n```Python\r\ndef DemoNet():\r\n    a = tf.keras.layers.Input(shape=(128,))\r\n    b = tf.keras.layers.Dense(128)(a)\r\n    \r\n    model = tf.keras.models.Model(inputs=a, outputs=b)\r\n    return model\r\n```\r\n\r\nModel class + functional\r\n\r\n```Python\r\nclass DemoModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(DemoModel, self).__init__()\r\n        self.encoder_layer = Dense(128, input_shape=(128,), activation='tanh')\r\n\r\n    def call(self, inputs):\r\n        \"\"\"Run the model.\"\"\"\r\n        encoded = self.encoder_layer(inputs)\r\n\r\n        return encoded\r\n\r\ndef DemoNet():\r\n    a = tf.keras.layers.Input(shape=(128,))\r\n    b = DemoModel()(a)\r\n    \r\n    model = tf.keras.models.Model(inputs=a, outputs=b)\r\n    return model\r\n```", "@kasperfred Thank you for your help. \r\nIs it intentional not to initialize the model automatically in model class API? It's confused to initialize weight with `fit()` method. Is it a bug or designed for some reason?\r\n", "I'm very new here. As such, I'm not very familiar with what behavior has been left intentional, and what is unintentional. \r\n\r\nI do agree that it's rather confusing having to call `fit` just to overwrite it afterwards. \r\n\r\nHowever, I think we need a regular contributor or a member who are more familiar with the architectural decisions to determine if this is considered a bug. If it is, I'd be happy to fix it. ", "One option is using the TensorFlow weight format, by using an extension that doesn't include `h5` (or [passing `save_format='tf'` explicitly to save_weights](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#save_weights)). That will do restore-on-create for variables when loading, so you can `load_weights` and then run `fit` and the variables will be restored before they get used.", "@allenlavoie So Keras `h5` model is not compatible with TensorFlow now? I am confused on initializing layers with `fit` method. Is this a bug?\r\n\r\nI prefer `ModelCheckpoint` class because it supports more features, like saving the best models, loggings, etc. ", "Not sure what you mean by \"not compatible with TensorFlow.\" The h5 format just doesn't include enough information to restore variables as soon as they're created.\r\n\r\nYou can use the TF format with ModelCheckpoint if you use `save_weights_only` and leave off the \".h5\" suffix.", "@kasperfred addressed this issue and @allenlavoie has provided a workaround. Hence closing the issue. Thanks."]}, {"number": 24622, "title": "tf.keras.utils.plot_model() raises TypeError: 'InputLayer' object is not iterable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=\"1.13.0-dev20181226\" (this is the TF 2.0-preview)\r\nGIT_VERSION=\"b'v1.12.0-5133-gc343196842'\"\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\ntf.keras.utils.plot_model() raises `TypeError: 'InputLayer' object is not iterable`\r\n\r\n**Describe the expected behavior**\r\nI expect it to save a png image of the model.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(3, input_shape=[2]))\r\nmodel.add(tf.keras.layers.Dense(1))\r\ntf.keras.utils.plot_model(model, to_file='my_model.png')\r\n```\r\n\r\n**Other info / logs**\r\nI ran the code in TF 1.12, no problem.\r\n\r\nHere is the traceback in TF 2.0-preview:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/vis_utils.py\", line 148, in plot_model\r\n    dot = model_to_dot(model, show_shapes, show_layer_names, rankdir)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/vis_utils.py\", line 123, in model_to_dot\r\n    for inbound_layer in node.inbound_layers:\r\nTypeError: 'InputLayer' object is not iterable\r\n```", "comments": ["Added a PR #24625 for the fix.", "Thanks @yongtang , I just tried your fix, works like a charm.", "Thanks @yongtang. \r\nClosing this out since it was resolved.", "I still get this error! Apparently, my error was due to the fact that I was using `keras.utils.plot_model` rather than `tf.keras.utils.plot_model` where `import tensorflow as tf`, while I am using Keras from `tf`.", "> tf.keras.utils.plot_model\r\n\r\nthanks, this worked for me"]}, {"number": 24621, "title": "Upgrading deprecated API cuDeviceGetProperties()", "body": "cuDeviceGetProperties() was deprecated and should be migrated to cudaGetDeviceProperties() which is good for all CUDA versions >= 8.0;\r\n\r\n```sh\r\n__CUDA_DEPRECATED CUresult CUDAAPI cuDeviceGetProperties(CUdevprop *prop, CUdevice dev);\r\n```\r\n\r\nSigned-off-by: CUI Wei <ghostplant@qq.com>\r\n", "comments": ["@ghostplant Can you please resolve the merge conflicts? Thanks!", "No, not needed, seems like another one did the similar job 4 days ago which had been merged in advance."]}, {"number": 24620, "title": " Does tf.contrib.signal.stft  do the right audio framing\uff1f", "body": "Hello\r\nWhen using  TensorFlow for audio analysis, I found an unreasonable results in **tf.contrib.signal.stft**.\r\nMy code is as follows:\r\n```\r\nsample_rate = 16000 #16kHz\r\nsegment_size = 2000 #ms\r\nwindow_size_ms = 40\r\nwindow_stride_ms = 20\r\nsegment_size_samples = int(sample_rate * segment_size / 1000)\r\nwindow_size_samples = int(sample_rate * window_size_ms / 1000)\r\nwindow_stride_samples = int(sample_rate * window_stride_ms / 1000)\r\naudio_sequence = tf.placeholder(tf.float32,\r\n                                [1,segment_size_samples],\r\n                                name = 'audio_sequence')\r\nstfts = tf.contrib.signal.stft(audio_sequence, \r\n                               frame_length=window_size_samples,\r\n                               frame_step=window_stride_samples,\r\n                               fft_length=window_size_samples,\r\n                               name = 'stfts')\r\nstfts.get_shape()\r\n```\r\nThe result is **TensorShape([Dimension(1), Dimension(99), Dimension(321)])**\r\nAs far as I know, the right should be **(1,100,321)**.\r\nSo, what is the underlying reason behind this incident?\r\nThx~", "comments": ["@yaxiongma please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/search?q=tensorflow). Github is mainly for addressing bugs in installation and performance. ", "No, this is probably a bug @jvishnuvardhan ", "@yaxiongma, as far as I know, tf.contrib.signal.stft performing as it was designed to. Second dimension is equal to ((len(audio_sequence)-frame_length)/frame_step)+1. So only 99 short-time-windows are available for the given window parameters. Please let me know if I am wrong. I have also checked the signals provided in an [article](https://medium.com/swlh/how-to-run-gpu-accelerated-signal-processing-in-tensorflow-13e1633f4bfb).\r\n\r\nPlease also check other sources such as Stockoverflow.", "Author of `tf.signal.stft` here -- @yaxiongma your signal isn't long enough for 100 full frames, and the default behavior is to truncate frames that fall outside the signal. You can set `pad_end=True` to pad the signal with zeros, which would produce 100 frames."]}, {"number": 24619, "title": "TensorFlow official devel docker cannot build TensorFlow", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):n/a\r\n- TensorFlow version:n/a\r\n- Python version:3\r\n- Installed using virtualenv? pip? conda?:n.a\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:n/a\r\n\r\nI'm trying to build tensorflow from latest source locally but failed. Then I found that I cannot even build it with the official docker. I ran the following command:\r\n\r\n```bash\r\nnvidia-docker run -it --rm tensorflow/tensorflow:devel-gpu-py3 bash\r\n# inside the docker:\r\ncd tensorflow_src    # it contains latest source code as of Dec 28\r\n./configure   # choose the default options for all questions\r\nbazel build //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\n\r\nErrors:\r\n```\r\nERROR: /tensorflow_src/tensorflow/contrib/seq2seq/BUILD:84:1: Linking of rule '//tensorflow/contrib/seq2seq:gen_beam_search_ops_py_wrappers_cc' fa$\r\nled (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64 \\\r\n    PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/contrib/seq2seq/gen_beam_sea$\r\nch_ops_py_wrappers_cc '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Ute$\r\nsorflow' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbaz$\r\nl-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow -Lbazel-out/host/bin/_solib$\r\nlocal/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Wl,-ldl '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rp$\r\nth,$ORIGIN/../..' -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-S -Wl,-no-as-ne$\r\nded -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-$\r\nections -Wl,@bazel-out/host/bin/tensorflow/contrib/seq2seq/gen_beam_search_ops_py_wrappers_cc-2.params)\r\n/usr/bin/ld: warning: libcuda.so.1, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrap$\r\ners_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemFree_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemsetD32Async'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuEventCreate'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuStreamAddCallback'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuModuleLoadFatBinary'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuCtxEnablePeerAccess'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemGetInfo_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuLaunchKernel'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuStreamSynchronize'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuEventQuery'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuEventElapsedTime'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDeviceCanAccessPeer'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuCtxSynchronize'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDeviceGetAttribute'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuFuncGetAttribute'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemcpyDtoH_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuStreamQuery'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDevicePrimaryCtxGetState'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuCtxSetCurrent'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuStreamWaitEvent'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuEventSynchronize'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuModuleUnload'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDeviceGet'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemsetD32_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemAllocManaged'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuCtxGetSharedMemConfig'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemFreeHost'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuFuncSetCacheConfig'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuStreamCreate'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemGetAddressRange_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuCtxGetDevice'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDeviceGetProperties'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemcpyDtoHAsync_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuGetErrorName'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDeviceGetCount'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuModuleGetFunction'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemHostRegister_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDevicePrimaryCtxRelease'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemcpyHtoDAsync_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemcpyDtoD_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuOccupancyMaxPotentialBlockSize'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuModuleLoadDataEx'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDevicePrimaryCtxRetain'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemHostAlloc'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuInit'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuGetErrorString'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDriverGetVersion'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDeviceGetPCIBusId'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuEventRecord'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuPointerGetAttribute'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDeviceTotalMem_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemsetD8_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDeviceComputeCapability'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemAlloc_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDeviceGetName'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuDevicePrimaryCtxSetFlags'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuMemHostUnregister'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.\r\nso: undefined reference to `cuModuleGetGlobal_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemcpyDtoDAsync_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuEventDestroy_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuOccupancyMaxActiveBlocksPerMultiprocessor'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxGetCurrent'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuStreamDestroy_v2'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuCtxSetSharedMemConfig'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemsetD8Async'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sseq2seq_Cgen_Ubeam_Usearch_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cuMemcpyHtoD_v2'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 575.519s, Critical Path: 282.39s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 7411 processes: 7411 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)", "I already provided the information at first.", "One workaround is:\r\n```\r\nln -sv /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1\r\n```\r\nHowever this is not ideal because root permission is not always available to a user.", "The author has closed this issue after some experiments. Thanks.", "@tfboyd Do you know if this was a surprise change? I thought CUDA had been working.", "The solution was copied from https://github.com/tensorflow/tensorflow/commit/be893ac19b13a77c645e168b6ab3f835062c4280, btw.", "**I will give the devel-docker a check** as that needs to work and I thought it did.  \r\n\r\nLocal build I normally install CUDA manually, I think if you install it via apt-get building can be messier and I do not test that scenario outside the docker working that way.  Not that I don't care it is just a scenario I do not use so I don't test it myself.  \r\n\r\n@angersson \r\nBuilding tensorflow should be part of the release process.  We discussed before that it use to build and install TensorFlow which was a nice test.  it should atleast do a full build with CUDA as part of the release.  ", "Yeah, agreed. It should be a fairly simple change; I'll be able to look into it early next week.", "I ran a build on the latest Nvidia drivers (**CUDA 10 requires the latest drivers which may not be marked \"stable\"**) and it looked good:\r\n\r\n```bash\r\nasm_images --release nightly --build_images --run_tests_path=$(realpath tests) --only_tags_matching=\"^devel-gpu-py3$\"\r\n```\r\n\r\n(`asm_images` from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dockerfiles)\r\n\r\nI'll close this issue since it seems to be resolved, although I'm still looking into running a build on Docker as part of our Docker release process."]}, {"number": 24618, "title": "tensor flow requires ffmpeg ... it was replaced by avconv", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["CI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\"     tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3     tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n\r\nfails with \r\n\r\nE: Package 'ffmpeg' has no installation candidate\r\n\r\nffmpeg packages was replaced by avconv in libav-tools\r\n", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24617, "title": "[INTEL MKL] Fix for conv_ops_test failure", "body": "The test has been failing with mkl build for two instances:\r\n\r\n1. No mkl kernel for `double` data type. [Already a PR #24532 is there]\r\n2. precision when filter size is same as input image size\r\n\r\n(1) is fixed by enabling mkl kernel only for `float` data type \r\n(2) is fixed using lowering the error tolerance to `1e-4` (motivation from\r\n[https://github.com/tensorflow/tensorflow/commit/f66961fe7987bbb2c41c8dec56bb80990c465701])", "comments": []}, {"number": 24616, "title": "Tensorflow 1.10", "body": "Hi . I have unstatement import issue while using Pycharm community edition. I\u2019m using this command \r\nimport tensorflow as tf \r\nPython : 3.6.0\r\nTensorflow : 1.10.1\r\n", "comments": ["Hi @deanmonnn, as it is not clear to find the root-cause of the issue, could you fill the following build/installation template.  Please report the installation process and commands used.  Here is the [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md)", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "There is no problem anymore\n\n22 Oca 2019 Sal, saat 21:54 tarihinde Alfred Sorten Wolf <\nnotifications@github.com> \u015funu yazd\u0131:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24616#issuecomment-456517611>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ApHfnDm2PdmOALfySPSCtawGWeZKMXGuks5vF15agaJpZM4ZkBrT>\n> .\n>\n"]}, {"number": 24615, "title": "Building from source on docker very slow (9 hours)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:  docker: latest-devel-gpu-py3 (image id: d6c139d2fdbf)\r\n- Python version: py3\r\n- Installed using virtualenv? pip? conda?: docker\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176\r\n- GPU model and memory: GTX650Ti, 1024MB\r\n- CPU: Intel(R) Core(TM)2 Duo CPU     E7500  @ 2.93GHz\r\n- Storage: SSD\r\n- RAM: 8GB\r\n\r\n\r\n**Describe the problem**\r\nI'm trying to compile GPU TF, but it seems to take forever. I'm now on 9 hours and counting. I'm expecting the compilation time to be much quicker.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nfollowing https://www.tensorflow.org/install/source#gpu_support_2 using\r\ndocker: latest-devel-gpu-py3 (image id: d6c139d2fdbf).\r\n\r\nCompilation finally ends after writing this at roughly 9 hours and 10 minutes\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nfinal output logs after completion:\r\n```\r\n[17,524 / 17,527] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/k_f|\r\n[17,524 / 17,527] 2 actions running                                             |\r\n    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/k_feature_gradien|\r\n[17,525 / 17,528] 2 actions, 1 running                                          |\r\n    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc|\r\n[17,526 / 17,529] 2 actions running                                             |\r\n    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc|\r\n[17,526 / 17,529] 2 actions running                                             |\r\n    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc|\r\n[17,526 / 17,529] 2 actions running                                             |\r\n    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc|\r\n[17,526 / 17,529] 2 actions running                                             |\r\n    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc|\r\n[17,533 / 17,536] 2 actions, 1 running                                          |\r\n    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/routing_gradient_|\r\n[17,533 / 17,536] 2 actions, 1 running                                          |\r\n    Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/routing_gradient_|\r\n[17,537 / 17,538] [-----] Linking tensorflow/contrib/tensor_forest/hybrid/python|\r\n[17,537 / 17,538] Linking tensorflow/contrib/tensor_forest/hybrid/python/ops/_tr|\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:             |\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package                      |\r\nINFO: Elapsed time: 75209.586s, Critical Path: 1426.82s                         |\r\nINFO: 14080 processes: 14080 local.                                             |\r\nINFO: Build completed successfully, 17538 total actions                         |\r\nroot@61c2126bd1ff:/tensorflow#              \r\n```                       ", "comments": ["Thank you for the report.\r\n\r\nOne reason for the slowness is that only 2 CPU jobs were used for building. Though that's mostly linear speedup if you increase to more (on 12 cores I'm seeing builds finish in ~3 hours)", "Ah I just realized from the log, that's 75209.586s (20 hours!)\r\nThank you @mihaimaruseac, I didn't realized the scale of the code. 3 hours on 12 cores corresponds to 18 hours on 2 cores, so I guess the numbers check out.", "The cpu is also outdated by now."]}, {"number": 24614, "title": "Tensorflow docker image doesn't set correct `LD_LIBRARY_PATH` variable for CUDA libraries", "body": "I ran the tensorflow docker with GPU support with the following command:\r\n```\r\ndocker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-devel-gpu-py3 python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\n```\r\n\r\nI got the following error message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nIt complains that the library `libcublas.so.9.0` cannot be found.\r\n\r\nI then started a bash session with the following command:\r\n```\r\ndocker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-devel-gpu-py3 bash\r\n```\r\nand checked the variable `LD_LIBRARY_PATH`:\r\n```\r\n$ echo $LD_LIBRARY_PATH\r\n/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n\r\n$ ls /usr/local/\r\nbin  cuda  cuda-9.0  etc  games  include  lib  man  sbin  share  src\r\n```\r\n\r\nAs we can see, there is no `/usr/local/nvidia` directory in the docker image. The `libcublas.so.9.0` library can be found at `/usr/local/cuda/lib64`.\r\n\r\nTo fix this problem, the `LD_LIBRARY_PATH` variable should be set to `/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib:/usr/local/cuda/lib64`.\r\n", "comments": ["@xulongwu4 This is really helpful for people who are facing similar issues. Could you mention your system configuration? Thanks! ", "I was running the tensorflow docker on Solus linux. The version of `docker` at the time I wrote the issue was 18.06.1. `nvidia-docker` was at version 2.0.3 (the latest release on github). The issue happened on both two laptops that I tried the tensorflow docker on. One laptop has an intel 7700HQ with NVIDIA GTX 1060. The other laptop has an intel 8700H with NVIDIA GTX 1060 MAX-Q.\r\n\r\nIs there any other information that I can provide?", "That's odd. @tfboyd, you had checked that these images were working, right?", "I ran the provided command with no issues. If the latest \"latest\" images still don't work, you may have an outdated version of the NVidia drivers (from `nvidia-smi`:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nOlder versions of the drivers don't support CUDA 10.", "@angersson This is weird. I still see the same problem. The following is the output of `nvidia-smi` on my laptop:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 415.27       Driver Version: 415.27       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 106...  On   | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   47C    P3    18W /  N/A |    243MiB /  6078MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       925      G   /usr/lib64/xorg-server/Xorg                  142MiB |\r\n|    0      1335      G   budgie-wm                                     86MiB |\r\n|    0      1572      G   ...quest-channel-token=1786131106816168071     8MiB |\r\n|    0      1691      G   ...-token=772756EC82DA66D9E1D850ED9DAEFD9B     4MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nDoes the docker actually use CUDA 10? It complains about `libcublas.so.9.0` which I thought is about CUDA 9.", "I tried the latest \"latest\" -- namely the \"nightly-gpu\" version. I still see the same problem -- this time it is complaining about `libcublas.so.10.0` though.\r\n\r\n```\r\n$ docker run --runtime=nvidia -it --rm tensorflow/tensorflow:nightly-gpu python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n```"]}, {"number": 24613, "title": "Fix bug in test_streaming_accuracy.cc", "body": "This fix fixes the issue raised in #24601 where `clip_duration_ms` should be `clip_duration_samples` to calculate `audio_data_end` in test_streaming_accuracy.cc. (Thanks Alireza89).\r\n\r\nThis fix #24601.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@petewarden You are the owner of speech commands in tensorflow/tensorflow. You want to take a look at this PR?", "Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24612, "title": "weird behavior in distributed training using partitioner", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): unknown, probably binary\r\n- TensorFlow version (use command below): 1.10.0\r\n- Python version: 2.7.5\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI wrote a custom estimator to implement FM algorithm and used train_and_evaluate to perform distributed training (4 ps and 100 workers). When I set partitioner to None, the training process quickly converged to an expected test AUC 0.87. However, when I either used fixed_size_partitioner or min_max_variable_partitioner with modified min_slice_size, the test AUC stayed below 0.64.  Different learning rates and batch sizes didn't help. Am I missing anything in distributed TF?\r\n\r\n**Describe the expected behavior**\r\n\r\nWith partitioner dividing embedding matrix to each PS, the training process should also converge quickly.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nclass FMEstimator:\r\n\tdef __init__(self, model_dir, config=None, params=None, optimizer=None, partitioner=None):\r\n\t\tself.model_dir = model_dir\r\n\t\tself.config = config\r\n\t\tself.params = params\r\n\t\tself.optimizer = optimizer\r\n\t\tself.partitioner = partitioner\r\n\r\n\tdef __embedding_lookup_square_sparse(self, params, sp_ids, partition_strategy=\"mod\", name=None, max_norm=None):\r\n\t\tif isinstance(params, variables.PartitionedVariable):\r\n\t\t\tparams = list(params)  # Iterate to get the underlying Variables.\r\n\t\tif not isinstance(params, list):\r\n\t\t\tparams = [params]\r\n\r\n\t\twith ops.name_scope(name, \"embedding_lookup_square_sparse\",\r\n\t\t                    params + [sp_ids]) as name:\r\n\t\t\tsegment_ids = sp_ids.indices[:, 0]\r\n\t\t\tif segment_ids.dtype != dtypes.int32:\r\n\t\t\t\tsegment_ids = math_ops.cast(segment_ids, dtypes.int32)\r\n\r\n\t\t\tids = sp_ids.values\r\n\t\t\tids, idx = array_ops.unique(ids)\r\n\r\n\t\t\tembeddings = tf.nn.embedding_lookup(params, ids, partition_strategy=partition_strategy, max_norm=max_norm)\r\n\r\n\t\t\tembeddings = tf.square(embeddings)\r\n\r\n\t\t\tassert idx is not None\r\n\r\n\t\t\tembeddings = math_ops.sparse_segment_sum(embeddings, idx, segment_ids, name=name)\r\n\r\n\t\t\treturn embeddings\r\n\r\n\tdef get_model_fn(self):\r\n\t\tdef custom_model_fn(features, labels, mode, params):\r\n\t\t\tlinear_bias = tf.get_variable(name='linear_bias',\r\n\t\t\t                              shape=[1],\r\n\t\t\t                              dtype=tf.float32,\r\n\t\t\t                              initializer=tf.random_normal_initializer(stddev=0.0001))\r\n\r\n\t\t\tlinear_w = tf.get_variable(name='linear_w',\r\n\t\t\t                           shape=[params['feature_size'], 1],\r\n\t\t\t                           dtype=tf.float32,\r\n\t\t\t                           initializer=tf.random_normal_initializer(stddev=0.0001),\r\n\t\t\t                           partitioner=self.partitioner)\r\n\r\n\t\t\t# wx\r\n\t\t\t# size: [batch_size, 1]\r\n\t\t\tlogits_wide = tf.nn.embedding_lookup_sparse(params=linear_w,\r\n\t\t\t                                            sp_ids=features['featureID'],\r\n\t\t\t                                            sp_weights=None,\r\n\t\t\t                                            combiner='sum')\r\n\t\t\t# wx + b\r\n\t\t\tlogits_linear = linear_bias + logits_wide\r\n\r\n\t\t\tcross_emb_w = tf.get_variable(name='cross_emb_w',\r\n\t\t\t                              shape=[params['feature_size'], params['cross_emb_size']],\r\n\t\t\t                              dtype=tf.float32,\r\n                                                      partitioner=self.partitioner,\r\n\t\t\t                              initializer=tf.random_normal_initializer(stddev=0.0001))\r\n\r\n\t\t\t# (a1,b1,c1) (a2,b2,c2) -> (a1+a2, b1+b2, c1+c2)\r\n\t\t\t# size: [batch_size, cross_emb_size]\r\n\t\t\tsummed_cross_emb = tf.nn.embedding_lookup_sparse(params=cross_emb_w,\r\n\t\t\t                                                 sp_ids=features['featureID'],\r\n\t\t\t                                                 sp_weights=None,\r\n\t\t\t                                                 combiner='sum')\r\n\t\t\t# ((a1+a2)^2, (b1+b2)^2, (c1+c2)^2)\r\n\t\t\t# size: [batch_size, cross_emb_size]\r\n\t\t\tsquared_summed_cross_emb = tf.square(summed_cross_emb)\r\n\r\n\t\t\t# (a1^2, b1^2, c1^2) (a2^2, b2^2, c2^2) -> (a1^2+a2^2, b1^2+b2^2, c1^2+c2^2)\r\n\t\t\t# size: [batch_size, cross_emb_size]\r\n\t\t\tsummed_squared_cross_emb = self.__embedding_lookup_square_sparse(params=cross_emb_w,\r\n\t\t\t                                                                 sp_ids=features['featureID'])\r\n\r\n\t\t\t# a1a2+b1b2+c1c2\r\n\t\t\t# size: [batch_size, 1]\r\n\t\t\tlogits_cross = tf.reduce_sum(0.5 * tf.subtract(squared_summed_cross_emb, summed_squared_cross_emb), 1, keepdims=True)\r\n\r\n\t\t\t# logits = tf.add(logits_linear, logits_cross)\r\n\t\t\tlogits = logits_linear\r\n\t\t\tlogits_adjusted = logits + tf.math.log(params['negative_sampling_rate'])\r\n\r\n\t\t\tif mode == tf.estimator.ModeKeys.PREDICT:\r\n\t\t\t\tpredictions = {\r\n\t\t\t\t\t'probabilities': tf.nn.sigmoid(logits_adjusted),\r\n\t\t\t\t\t'logits': logits,\r\n\t\t\t\t\t'logits_adjusted': logits_adjusted\r\n\t\t\t\t}\r\n\r\n\t\t\t\treturn tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n\t\t\telse:\r\n\t\t\t\tloss = tf.reduce_mean(\r\n\t\t\t\t\ttf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(labels, dtype=tf.float32),\r\n\t\t\t\t\t                                        logits=logits))\r\n\r\n\t\t\t\tif mode == tf.estimator.ModeKeys.EVAL:\r\n\t\t\t\t\tauc = tf.metrics.auc(\r\n\t\t\t\t\t\tlabels=labels,\r\n\t\t\t\t\t\tpredictions=1 / (1 + tf.math.exp(-logits_adjusted)),\r\n\t\t\t\t\t\tnum_thresholds=400,\r\n\t\t\t\t\t\tcurve='ROC',\r\n\t\t\t\t\t\tsummation_method='careful_interpolation')\r\n\t\t\t\t\tlogloss = tf.metrics.mean(tf.nn.sigmoid_cross_entropy_with_logits(\r\n\t\t\t\t\t\tlabels=tf.cast(labels, dtype=tf.float32),\r\n\t\t\t\t\t\tlogits=logits_adjusted))\r\n\t\t\t\t\ttf.summary.scalar('True_AUC', auc)\r\n\t\t\t\t\ttf.summary.scalar('True_Logloss', logloss)\r\n\t\t\t\t\tmetrics = {\r\n\t\t\t\t\t\t'True_AUC': auc,\r\n\t\t\t\t\t\t'True_Logloss': logloss\r\n\t\t\t\t\t}\r\n\r\n\t\t\t\t\tpredictions = {\r\n\t\t\t\t\t\t'probabilities': tf.nn.sigmoid(logits_adjusted),\r\n\t\t\t\t\t\t'logits': logits,\r\n\t\t\t\t\t\t'logits_adjusted': logits_adjusted\r\n\t\t\t\t\t}\r\n\r\n\t\t\t\t\treturn tf.estimator.EstimatorSpec(mode, loss=loss, predictions=predictions,\r\n\t\t\t\t\t                                  eval_metric_ops=metrics)\r\n\r\n\t\t\t\telif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t\t\t\ttrain_op = self.optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n\r\n\t\t\t\t\treturn tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n\t\treturn custom_model_fn\r\n\r\n\tdef get_estimator(self):\r\n\t\treturn tf.estimator.Estimator(model_fn=self.get_model_fn(),\r\n\t\t                              model_dir=self.model_dir,\r\n\t\t                              config=self.config,\r\n\t\t                              params=self.params)\r\n\r\n\r\n```\r\n**Other info / logs**\r\nThe input data is in sparse tensor of tf.int64. Feature size is around 480,000. FM cross_emb_size is 4.\r\nI have checked the graph from tensorboard and timeline from hooks. Both of them seemed correct. The embedding matrix was divided and placed on each ps with corresponding update ops. I used both the estimator.evaluate function and hand-made prediction calculation with exported embedding matrix, and got the same low AUC value. I also ask a question in stackoverflow ([stackoverflow_link](https://stackoverflow.com/questions/53870450/distributed-tf-multi-ps-version-lr-doesnt-converge)), but go no response by now.\r\n", "comments": ["Hi @karmel , this user is using the tf.estimator module to write a custom estimator and trying to set the Partitioner in the model. But it does not converge. Can you please see why this happens? Thanks.", "@xiejw , @alextp for the behavior of the current partitioners, and some insight as to the expected APIs in 2.0.", "The partitioner shouldn't affect convergence, other than it can change the relative speed of your workers and hence affect the level of parallelism requiring a retuning of the learning rate.", "> \r\n> \r\n> The partitioner shouldn't affect convergence, other than it can change the relative speed of your workers and hence affect the level of parallelism requiring a retuning of the learning rate.\r\n\r\nI had a similar guess about the parallelism and did some experiments under less scale setting using 2 ps and 5 workers on my local server. I also observed low AUC with fixed_size_partitioner. After reading your reply, I did another set of experiments with only 1 worker providing updating gradients, and still low AUC.\r\n\r\nI also tried to find some other clues from TensorBoard and noticed:\r\n1. the training average losses actually were similar whether using partitioner or not. The numbers fluctuated in the same region. That's why I tried several ways (estimator.evaluate, NewCheckpointReader and saver.restore) to do the evaluation cause I thought the training process with partitioner might acutally have converged and there might be problem in my evaluation process. But the results were the same of low AUC.\r\n\r\n2. Since the training process with partitioner gave acceptable average loss, I used the exact training data to the evaluation and expected similar loss. However, large loss number came with low AUC. \r\n\r\nCurrently, I decide to set evaluator in the cluster_spec to get runtime auc and loss of both training data and test data to see what will happen. If the runtime evalutor could give good AUC, I may have problem exporting weights, otherwise, I don't know what I can do...\r\n\r\n\r\n", "Hello @Zhiqiang-Ye , we hope @alextp was able to address your initial query. We shall close this issue now. Thanks.", "@alextp @msymp \r\nSorry for my late reply. I just got time to do more tests and finally solve this issue.\r\n\r\nOn one hand, the training process went well with partitioner, and it converged quickly;\r\nOn the other hand, the discrepancy between evaluation AUC and training AUC resulted from the initialization process of variables in the graph. To get the expected result, I had to provide a partitioner with **exactly the same** partitioner args when using tf.get_variable to make these variables as partitionedVariable, even though the evaluation process can only run in the non-distributed mode. Simple tensor or partitionedVariable with wrong partitioner info would gave wrong result.\r\n\r\nDo I miss any points in the doc about this initialization requirements, or is there any problem with my code about using partitioner? I didn't expect issues in this part, since there was no error when using saver.restore to initialize simple tensor variable and the tensor values were the same as their counterparts extracted from checkpoint.", "This does look like a bug. TF is supposed to be able to load partitioned variables as non-partitioned ones and vice versa.\r\n\r\nCan you have short instructions to reproduce this issue, and file another bug?"]}, {"number": 24611, "title": "performance issue in CollectiveAllReduceStrategy", "body": "Tried both `CollectiveAllReduceStrategy` and `MirroredStrategy` on a same 8 GPU machine. And `CollectiveAllReduceStrategy` is constantly 30% slower than MirroredStrategy no matter how many cards I am using.\r\n\r\nThe tensorflow version I am running is 1.12.0.\r\n\r\nI am using estimator API and tried many different models including resnet50, resnet34, vgg etc. \r\n\r\nIs this expected? Or I need some configuration to make CollectiveAllReduceStrategy faster? Thanks\r\n\r\n", "comments": ["You could take a look in this thread: https://groups.google.com/a/tensorflow.org/d/msg/discuss/7T05tNV08Us/gStt8n77CgAJ\r\n\r\nTL;DR: change `allreduce_merge_scope` to a larger value, e.g. 32.", "@byronyi thanks, may I know where could I change this parameter? I cannot find place to pass in this parameter and seems its default value is already 32.", "@byronyi tried change this line https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/distribute/python/cross_tower_ops.py#L782 and set `all_reduce_merge_scope` to various values but speed is still the same. Seems it has no effect at all.", "some update, I figured out why `CollectiveAllReduceStrategy` is slower than `MirroredStrategy` on a single machine. It is because the `CollectiveReduce` op is not enabled in rewrite_option when running on single machine. After fix this, it can run at same speed with MirroredStrategy now, but  `CollectiveAllReduceStrategy` on multiple workers is still slow, for example, when I split the 8 cards into two workers, speed will drop 50%.", "Case of multi-worker involves interprocess communication, which is a whole different story. If you have RDMA capable NIC, it\u2019s possible to utilize GPU Direct RDMA to improve performance in the case multi-worker.", "I am using Titan card, so there is no GPU Direct. But in this testcase these two workers are on the same machine, 50% drop in performance seems unreasonable. ", "I would not say it\u2019s unreasonable; it\u2019s certainly possible to achieve higher performance than gRPC via some other means, e.g. shared memory, regular RDMA, interprocess CUDA P2P, etc. Some of them are implemented in open source TF (contrib/verbs or uber/horovod), some are not, and maybe it\u2019s because we have not known a whole lot of valid use cases (single node multi-worker).\r\n\r\nSince you could actually reach expected performance with a single TF process, I do not understand what you are trying to accomplish here.", "Hi @byronyi , this is just an example. I am trying to use it for distributed training across several machines. I tried 64 cards training across 8 machines and the acceleration rate is around 20.", "@ustcyue That is good to know. May you give more specific configurations of your cluster? What is your network configuration? How does PS/mirrored strategy work with your jobs in the same environment? \r\n\r\n@poxvoculi @yanivbl6 @jbedorf @yuefengz may help you.", "@ustcyue What is the latest performance number? We've fixed the rewriter_options problem for single-worker.", "Closing due to inactivity; please reopen if you have updates or more questions."]}, {"number": 24610, "title": "Fix typo in the documentation of tf.function", "body": "\"`add_noise()` will return a different output every time it is invoked. However, `add_noise` will return the same value every time it is called...\"\r\n=> the second `add_noise` should be `traced`", "comments": []}, {"number": 24609, "title": "unable to convert the .pb to .tflite as tflite_convert is giving me a problem", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- I am working on tensorflow-for-poets-2 and trying to convert the trained model to tflite so that i can port my model to mobile device. which i was not able to do and i am following the hands on from the \r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#0\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" >> 1.12.0\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nmy log when executing the code:\r\n\r\n(tensorflow) C:\\Users\\H156759>python\r\nPython 3.6.7 |Anaconda, Inc.| (default, Dec 10 2018, 20:35:02) [MSC v.1915 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.12.0'\r\n>>> exit()\r\n\r\n(tensorflow) C:\\Users\\H156759>tflite_convert --help\r\nTraceback (most recent call last):\r\n  File \"C:\\Pavans\\Anaconda3\\envs\\tensorflow\\Scripts\\tflite_convert-script.py\", line 6, in <module>\r\n    from tensorflow.contrib.lite.python.tflite_convert import main\r\nModuleNotFoundError: No module named 'tensorflow.contrib.lite.python.tflite_convert'", "comments": ["If all the ops in your graph are supported by the current TFLite, you can do this successfully in linux environment, and you'd better choose  1.12.0. Don't forget to Check the [**TensorFlow Lite & TensorFlow Compatibility Guide**](https://www.tensorflow.org/lite/tf_ops_compatibility)\r\nGood luck~", "> If all the ops in your graph are supported by the current TFLite, you can do this successfully in linux environment, and you'd better choose 1.12.0. Don't forget to Check the [**TensorFlow Lite & TensorFlow Compatibility Guide**](https://www.tensorflow.org/lite/tf_ops_compatibility)\r\n> Good luck~\r\n\r\nHi yaxiongma,\r\n\r\nyou mean to say that i should try tflite on Linux and this will not work on windows ? because my execution environment that i am trying is on Windows currently.", "@pavanjava  \nYes.\nI haven't tried it in Windows. \ngood luck\n\n", "> @pavanjava\r\n> Yes.\r\n> I haven't tried it in Windows.\r\n> good luck\r\n\r\nThanks yaxiongma,\r\ni will try and let you know. i will keep this issue on hold", "until i try creating the model in linux. closing the issue"]}, {"number": 24608, "title": "Build did NOT complete successfully while  Fetching @com_google_protobuf", "body": "**System information**\r\n- OS Platform and Distribution: Mac OS Mojave 10.14.2 (18C54)\r\n- TensorFlow version: 1.12 (cloned from master)\r\n- Python version: 3.6.6\r\n- Icompiling in conda virtual env\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): /MacOSX10.14.sdk/usr/include/c++/4.2.1\r\n\r\nexception when executing `bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...`\r\n\r\nInternal error thrown during build. Printing stack trace: java.lang.RuntimeException: Unrecoverable error while evaluating node 'REPOSITORY_DIRECTORY:@remotejdk_macos' (requested by nodes 'REPOSITORY:@remotejdk_macos')\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:499)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)\r\n\tat java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)\r\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)\r\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.localPopAndExec(Unknown Source)\r\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)\r\nCaused by: java.lang.IllegalArgumentException: Invalid EvalException:\r\njava.lang.ExceptionInInitializerError\r\n\tat java.base/javax.crypto.JceSecurityManager.<clinit>(Unknown Source)\r\n\tat java.base/javax.crypto.Cipher.getConfiguredPermission(Unknown Source)\r\n\tat java.base/javax.crypto.Cipher.getMaxAllowedKeyLength(Unknown Source)\r\n\tat java.base/sun.security.ssl.CipherSuite$BulkCipher.isUnlimited(Unknown Source)\r\n\tat java.base/sun.security.ssl.CipherSuite$BulkCipher.<init>(Unknown Source)\r\n\tat java.base/sun.security.ssl.CipherSuite$BulkCipher.<clinit>(Unknown Source)\r\n\tat java.base/sun.security.ssl.CipherSuite.<clinit>(Unknown Source)\r\n\tat java.base/sun.security.ssl.SSLContextImpl.getApplicableSupportedCipherSuiteList(Unknown Source)\r\n\tat java.base/sun.security.ssl.SSLContextImpl.access$100(Unknown Source)\r\n\tat java.base/sun.security.ssl.SSLContextImpl$AbstractTLSContext.<clinit>(Unknown Source)\r\n\tat java.base/java.lang.Class.forName0(Native Method)\r\n\tat java.base/java.lang.Class.forName(Unknown Source)\r\n\tat java.base/java.security.Provider$Service.getImplClass(Unknown Source)\r\n\tat java.base/java.security.Provider$Service.newInstance(Unknown Source)\r\n\tat java.base/sun.security.jca.GetInstance.getInstance(Unknown Source)\r\n\tat java.base/sun.security.jca.GetInstance.getInstance(Unknown Source)\r\n\tat java.base/javax.net.ssl.SSLContext.getInstance(Unknown Source)\r\n\tat java.base/javax.net.ssl.SSLContext.getDefault(Unknown Source)\r\n\tat java.base/javax.net.ssl.SSLSocketFactory.getDefault(Unknown Source)\r\n\tat java.base/javax.net.ssl.HttpsURLConnection.getDefaultSSLSocketFactory(Unknown Source)\r\n\tat java.base/javax.net.ssl.HttpsURLConnection.<init>(Unknown Source)\r\n\tat java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.<init>(Unknown Source)\r\n\tat java.base/sun.net.www.protocol.https.Handler.openConnection(Unknown Source)\r\n\tat java.base/java.net.URL.openConnection(Unknown Source)\r\n\tat com.google.devtools.build.lib.bazel.repository.downloader.HttpConnector.connect(HttpConnector.java:93)\r\n\tat com.google.devtools.build.lib.bazel.repository.downloader.HttpConnectorMultiplexer.establishConnection(HttpConnectorMultiplexer.java:300)\r\n\tat com.google.devtools.build.lib.bazel.repository.downloader.HttpConnectorMultiplexer.connect(HttpConnectorMultiplexer.java:126)\r\n\tat com.google.devtools.build.lib.bazel.repository.downloader.HttpDownloader.download(HttpDownloader.java:246)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryContext.downloadAndExtract(SkylarkRepositoryContext.java:433)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:130)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:919)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:887)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:200)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:182)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:125)\r\n\tat com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:217)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:422)\r\n\tat com.google.devtools.build.lib.concurrent.AbstractQueueVisitor$WrappedRunnable.run(AbstractQueueVisitor.java:368)\r\n\tat java.base/java.util.concurrent.ForkJoinTask$AdaptedRunnableAction.exec(Unknown Source)\r\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(Unknown Source)\r\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.localPopAndExec(Unknown Source)\r\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)\r\nCaused by: java.lang.SecurityException: Can not initialize cryptographic mechanism\r\n\tat java.base/javax.crypto.JceSecurity.<clinit>(Unknown Source)\r\n\t... 51 more\r\nCaused by: java.lang.SecurityException: Couldn't parse jurisdiction policy files in: unlimited\r\n\tat java.base/javax.crypto.JceSecurity.setupJurisdictionPolicies(Unknown Source)\r\n\tat java.base/javax.crypto.JceSecurity.access$000(Unknown Source)\r\n\tat java.base/javax.crypto.JceSecurity$1.run(Unknown Source)\r\n\tat java.base/javax.crypto.JceSecurity$1.run(Unknown Source)\r\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\n\t... 52 more\r\n\r\n\tat com.google.devtools.build.lib.syntax.EvalException.<init>(EvalException.java:112)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:209)\r\n\tat com.google.devtools.build.lib.syntax.EvalException$EvalExceptionWithJavaCause.<init>(EvalException.java:217)\r\n\tat com.google.devtools.build.lib.syntax.MethodDescriptor.call(MethodDescriptor.java:163)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.invokeObjectMethod(FuncallExpression.java:919)\r\n\tat com.google.devtools.build.lib.syntax.FuncallExpression.doEval(FuncallExpression.java:887)\r\n\tat com.google.devtools.build.lib.syntax.Expression.eval(Expression.java:69)\r\n\tat com.google.devtools.build.lib.syntax.Eval.execDispatch(Eval.java:200)\r\n\tat com.google.devtools.build.lib.syntax.Eval.exec(Eval.java:182)\r\n\tat com.google.devtools.build.lib.syntax.UserDefinedFunction.call(UserDefinedFunction.java:91)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.callWithArgArray(BaseFunction.java:475)\r\n\tat com.google.devtools.build.lib.syntax.BaseFunction.call(BaseFunction.java:437)\r\n\tat com.google.devtools.build.lib.bazel.repository.skylark.SkylarkRepositoryFunction.fetch(SkylarkRepositoryFunction.java:125)\r\n\tat com.google.devtools.build.lib.rules.repository.RepositoryDelegatorFunction.compute(RepositoryDelegatorFunction.java:217)\r\n\tat com.google.devtools.build.skyframe.AbstractParallelEvaluator$Evaluate.run(AbstractParallelEvaluator.java:422)\r\n\t... 6 more\r\n\r\nINFO: Elapsed time: 12,581s\r\n", "comments": ["Hello,\r\nPlease try and compile / build the cloned source 1.12.0 (and python 3.6) with _clang from xcode_ and _bazel 0.15.0_ on the MacOS. Thanks.\r\n\r\n\r\n\r\n", "source does not work with bazel 0.15.0\r\n./configure requires min_version bazel 0.19.0\r\n`Please upgrade your bazel installation to version 0.19.0 or higher to build TensorFlow!`", "Hi,\r\n\r\nbazel version was not the problem. Sources could be compiled with 0.19.0 and the latest (0.21.0)\r\nI changed two things:\r\n\r\n`export CC=/usr/bin/clang`\r\n\r\nand according to following error:\r\n> ..in apple_cc_toolchain rule @local_config_cc//:cc-compiler-darwin_x86_64: Xcode version must be specified to use an Apple CROSSTOOL\r\n\r\nas well as proposed solution in\r\nhttps://stackoverflow.com/questions/45276830/xcode-version-must-be-specified-to-use-an-apple-crosstool\r\n\r\nAt the end you have to take care that you initially set the command line tool version in the preferences location tab of XCode.\r\n\r\nHowever there are a plenty of deprecated warnings during build time.", "Thank you @mauermbq , for the detailed feedback after your experiments and closing the issue. "]}, {"number": 24607, "title": "TFLite not support Dynamic input size ", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTry running tflite_convert again, with --input_shapes=1,none,none,3,  but these conversion cannot be supported.\r\nFor some networks requiring dynamic input, the static input of tflite is too limited, such as Superresolution Neural Network.\r\n\r\n", "comments": ["Can you provide a link/reference to an existing (trained) model that requires dynamic shape elements? That will help us focus support for this feature. Thanks.", "To add some context, support for conversion with explicitly dynamic dimensions is on our radar.", "hello,I'm trying to deploy Superresolution on android, and I found it seems not support Dynamic input size,\r\nhave you fix this problem now?\r\n@jdduke ", "We're actively working to improve this. Expect a change in the next few days which will improve the ability to dynamically resize at runtime. You'll still need to give it a static shape during conversion, but the graph should be much more robust to dynamic size changes.", "@jdduke Any updates in supporting dynamic size at runtime?", "We've landed several fixes, but they've been rolled back due to downstream clients depending on the current (somewhat broken) behavior in our converter. We're hoping to resolve this properly in Q3, and might expose an experimental flag in the converter to generate graphs that always preserve rank/shape operators.", "@jdduke any update?", "No concrete updates yet, but we are investing this quarter in improving the situation, and will be sure to address this specific case. Thanks for your patience.\r\n\r\nSome workarounds that folks use are 1) for images, do the resize outside the graph and use a fixed shape input, 2) if you do require actual resizing, but only for a small number of fixed shapes, create a separate interpreter instance for each shape.", "@jdduke tflite support multi-size image input. Is there some sample about how to do that?", "@jdduke Is plan to support multi-size image input on tflite?", "Is there any update on this? This feature would be very useful for supporting seq2seq models in TFLite.", "@jdduke any update?", "We're still actively working on this, and it remains a priority for Q1.", "We added support for unknown dimensions in TensorFlow Lite today (55912083e2f16087c2f29394acf8a6a4811a2ce0). \r\n\r\nCan you try converting your model again with tonight's (1/31) `tf-nightly` once it's released (`pip install tf-nightly`). Convert the model with `experimental_new_converter = True`.\r\n\r\nWhen you load the model it should have an additional field `shape_signature` that contains the shape with any unknown dimensions marked with `-1`. `shape` will have those dimensions marked with `1`.\r\n\r\nYou can then call `ResizeInputTensor` with the desired shape when running the interpreter. The generated model will only work on the latest TensorFlow version (i.e. the interpreter on the `tf-nightly` version you are running).\r\n\r\nIf it does not work, can you provide a detailed error and repro instructions?", "If I have model version tf **1.x**, exported as saved_model format.  Can I still use this functionality for dynamic input shapes?", "@harsh306 Yes. As long as the 1.X TensorFlow SavedModel takes in dynamic input shapes, then it will work with TFLite after reconverting the model. I recommend using `TFLiteConverter.from_saved-model` in the `tf-nightly` (which supports 1.X SavedModels) to try converting the model and testing this functionality.", "Thanks, Yes, I will try that. Currently I am stuck on different issue of `pip install tf-nightly` on MAC :) Will update results once I figure out. \r\nhttps://github.com/tensorflow/tensorflow/issues/27470", "ok, I am using tf-nightly==2.2.0-dev20200204\r\nStill getting the error during converting. \r\n\r\n```\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n    converter.experimental_new_converter = True\r\n    print(converter.experimental_new_converter) # outputs True\r\n    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n    tflite_quant_model = converter.convert()\r\n    file = open(TFLITE_MODEL+'model.tflite', 'wb')\r\n    file.write(tflite_quant_model)\r\n```\r\n\r\nError:\r\n```\r\nFile \"/Users/hpathak/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\", line 478, in convert\r\n    _get_tensor_name(tensor), shape_list))\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_img' has invalid shape '[None, None, None, 3]'.\r\n```", "Unfortunately, dynamic shapes are currently not supported with quantization.\r\n\r\nWe are hoping to enable dynamic shapes with weight-only quantization support in the near future (which seems to be sufficient for your use case). However, full integer post training quantization support won't be added in the immediate future.", "@gargn For speech recognition applications, as opposed to image, it is natural using dynamic shapes due to varying lengths of the input signal. Would you point to an example that deals with this dynamic aspect and uses quantization and TFLite? I am aware of [Command Recognition Android Demo](https://github.com/tensorflow/examples/blob/master/lite/examples/speech_commands/android/README.md), but the speech input is fixed to 1 second in that example. Thank you.", "@dusanmacho how about if u set fixed size for length and it large enough (let say around 1minutes). Then u can apply masking input on the begining of the code ?", "Thanks! Unfortunately, I am unable to run quantization on a fixed shape input either. See details below.\r\n\r\nTF inference model:\r\n```\r\ntf.__version__\r\n'2.2.0-dev20200212'\r\ninference_model = tf.keras.models.load_model('./inference_model.h5')\r\ninference_model.summary()\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nInput_feats (InputLayer)     [(None, None, 41, 1)]     0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, None, 21, 32)      3872      \r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, None, 21, 32)      128       \r\n_________________________________________________________________\r\nactivation (Activation)      (None, None, 21, 32)      0         \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, None, 11, 32)      78848     \r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (None, None, 11, 32)      128       \r\n_________________________________________________________________\r\nactivation_1 (Activation)    (None, None, 11, 32)      0         \r\n_________________________________________________________________\r\nreshape (Reshape)            (None, None, 352)         0         \r\n_________________________________________________________________\r\nlstm (LSTM)                  (None, None, 172)         361200    \r\n_________________________________________________________________\r\nlstm_1 (LSTM)                (None, None, 172)         237360    \r\n_________________________________________________________________\r\nlstm_2 (LSTM)                (None, None, 172)         237360    \r\n_________________________________________________________________\r\ndense (Dense)                (None, None, 29)          5017      \r\n=================================================================\r\nTotal params: 923,913\r\nTrainable params: 923,785\r\nNon-trainable params: 128\r\n_________________________________________________________________\r\n```\r\nThe following runs correctly, and the TFLite model inference results match those of the original TF model (@gargn thumbs up!!!):\r\n```\r\ninference_model = tf.keras.models.load_model('./inference_model.h5')\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n```\r\nThe following gives an error due to dynamic shapes not being supported in TFLite quantization yet:\r\n```\r\ninference_model = tf.keras.models.load_model('./inference_model.h5')\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\r\nconverter.experimental_new_converter = True\r\nconverter.experimental_new_quantizer = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\n*** ValueError: None is only supported in the 1st dimension. Tensor 'Input_feats' has invalid shape '[None, None, 41, 1]'.\r\n```\r\nSetting a fixed input shape gives a different error:\r\n```\r\ninference_model = tf.keras.models.load_model('./inference_model.h5')\r\ninference_model.input.set_shape((1, 100, 41, 1))\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\r\nconverter.experimental_new_converter = True\r\nconverter.experimental_new_quantizer = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\n... quantize_weights.cc:351] Quantize weights tool only supports tflite models with one subgraph.\r\nException: Quantize weights transformation failed.\r\n```\r\n", "@dusanmacho be carefull with rnn, at this time, lstm haven\u2019t fully support yet. Pls replace rnn to dilated convution (eg temporal convolution network).", "@gargn Is stateful LSTM supported with converter.experimental_new_converter? When using \r\n```\r\ntf.keras.layers.LSTM(..., return_state = False, stateful = False, ...) \r\n```\r\nconverter.convert() works. However when using  \r\n```\r\ntf.keras.layers.LSTM(..., return_state = False, stateful = True, ...)\r\n```\r\nconverter.convert()  gives the following error:\r\n```\r\nValueError: Input 0 of node model/lstm/AssignVariableOp was passed float from model/lstm/1991:0 incompatible with expected resource.\r\n```", "Hi everyone and @gargn, I am getting the same error in a slightly different use case as I am working with images:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/gixhx/.conda/envs/tf22_p37/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 655, in convert\r\n    _get_tensor_name(tensor), shape_list))\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_image' has invalid shape '[None, None, None, 3]'.\r\n```\r\nTensorflow nightly version: `2.2.0-dev20200411`\r\n\r\nMy code for conversion from SavedModel to tflite:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\r\nprint(converter.experimental_new_converter)  # outputs True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nmodel_tflite = converter.convert()\r\n```\r\n\r\nIf I convert slightly differently:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\r\nprint(converter.experimental_new_converter)  # outputs True\r\nconverter.optimizations = []  # this was the default behavior\r\nmodel_tflite = converter.convert()\r\n```\r\n\r\nI get the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/gixhx/.conda/envs/tf22_p37/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 704, in convert\r\n    **converter_kwargs)\r\n  File \"/home/gixhx/.conda/envs/tf22_p37/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 536, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/gixhx/.conda/envs/tf22_p37/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 241, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-04-11 14:59:11.353946: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-04-11 14:59:11.353969: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-04-11 14:59:12.413053: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:307] Ignored output_format.\r\n2020-04-11 14:59:12.413085: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:310] Ignored drop_control_dependency.\r\nloc(\"resize_image_with_pad/control_dependency\"): error: requires all operands to be either same as or ref type of results\r\nTraceback (most recent call last):\r\n  File \"/home/gixhx/.conda/envs/tf22_p37/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/gixhx/.conda/envs/tf22_p37/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/gixhx/.conda/envs/tf22_p37/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/gixhx/.conda/envs/tf22_p37/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/gixhx/.conda/envs/tf22_p37/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/gixhx/.conda/envs/tf22_p37/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(\"resize_image_with_pad/control_dependency\"): requires all operands to be either same as or ref type of results\r\n<unknown>:0: note: loc(\"resize_image_with_pad/control_dependency\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<?x?x?x3x!tf.quint8>) -> tensor<?x?x?x3xui8>\r\n```\r\n\r\n**More Information:**\r\nI have a classification usecase with ResNet50 model which I trained with transfer learning using tf.keras (tensorflow 2.x). Input image dimensions are 224x224x3. Input data is images and resized to 224x224x3 with the following function that maintains original image aspect ratio:\r\n`image = tf.image.resize_with_pad(image, target_height=224, target_width=224)`\r\nThis means, the model is trained on images with black padding on shorter dimension. Therefore, I need to give similar images at inference time, otherwise the model accuracy drops significantly. Therefore, I converted the tf.keras .h5 model into SavedModel format with the following serving function:\r\n```\r\ndef serving_input_receiver_fn():\r\n    \"\"\"Preprocess/resize images (like during tfrecord creation) and prepare for inference\"\"\"\r\n    input_image = tf.compat.v1.placeholder(dtype=tf.uint8,\r\n                                           shape=[None, None, None, 3],\r\n                                           name='input_image')\r\n    \r\n    image = tf.image.resize_with_pad(image=input_image,\r\n                                     target_height=224,\r\n                                     target_width=224,\r\n                                     method=tf.image.ResizeMethod.LANCZOS3,\r\n                                     antialias=True)\r\n    image = image/255.0\r\n    \r\n    return tf.estimator.export.ServingInputReceiver({'input_1': image},\r\n                                                    {'image_original': input_image})\r\n```\r\nCode to convert from h5 model to SavedModel model:\r\n```\r\n# code to convert from tf.keras h5 model to tf SaveModel .pb model\r\nestimator = tf.keras.estimator.model_to_estimator(keras_model=model_h5,\r\n                                                  model_dir=model_rootpath)\r\nexported_path = estimator.export_saved_model(sm_model_fp, serving_input_receiver_fn)\r\n```\r\nNow, this model performs as expected if I use the SavedModel for inference directly on my ubuntu cloud instance. The issue only arises when I try to convert this model from SavedModel to tflite model for deploying on iOS and Android, as I have shown on the top of my comment.\r\n\r\nAs you can see, my serving function takes randomly sized input images becauses it resizes them to required size while maintaining the aspect ration. Therefore, I need to have [None, None, None, 3] for input dimensions. \r\n\r\nCan anyone share if there's a way to do this and what I might be missing? Is there really this functionality missing at this point? I thought this would be a very common use case (it does work flawlessly using SavedModel).", "Hey @gargn @karimnosseir @miaout17 @dusanmacho  @Saduf2019 does it support dynamic input shape, while using frozen graphs for conversion to tflite in TF 1.x version. \r\n( i have a .ckpt model built in TF 1.14 , I converted it to a .pb model, in order to convert it to .tflite model) \r\nIs this how I should be converting a dynamic input shape .ckpt model to a tflite model?", "> Unfortunately, dynamic shapes are currently not supported with quantization.\r\n> \r\n> We are hoping to enable dynamic shapes with weight-only quantization support in the near future (which seems to be sufficient for your use case). However, full integer post training quantization support won't be added in the immediate future.\r\n\r\ni think this issue might be related: https://github.com/tensorflow/tensorflow/issues/40268\r\ntry use float32 instead of uint8 for input", "I am running into a similar issue where **input sizes are updating** to accomodate for a dynamic batch size, however the **output batch dimension appears to remain fixed at 1**  despite the batch size. \r\n\r\n`(1, 512, 512, 3)` as opposed to `(X, 512, 512, 3)`.\r\n\r\nI outlined all the details in another similar issue [here](https://github.com/tensorflow/tensorflow/issues/37012#issuecomment-652641044).", "@alfarok updated the other issue.\r\n\"You should have your model converted again with supporting dynamic batch size. Looks like you specified static size during conversion.\"\r\n\r\nThanks", "hi @gargn,  \r\nI find that dynamic input shape doesn't suppot GPU accelerate.\r\nAny updates in supporting GPU or NNAPI accelerate when using dynamic size input?", "It works, helps a lot!!", "@yuqiu1233 GPU and NNAPU delegates doesn't support handling dynamic batch size currently.\r\nDepending on your model, you might want to consider splitting your model so you can get the heavy part accelerated and post processing which requires dynamic batch on CPU.\r\n\r\n", "> @yuqiu1233 GPU and NNAPU delegates doesn't support handling dynamic batch size currently.\r\n> Depending on your model, you might want to consider splitting your model so you can get the heavy part accelerated and post processing which requires dynamic batch on CPU.\r\n\r\ndoes GPU and NNAPI delegates support handling dynamic input shape such as [1, None, 100] ?."]}, {"number": 24606, "title": "Can't run tensorflow on PyCharm at Win10", "body": "Hi, can anyone help me with this errors? I've tried varios Python installations an have installed Cuda 10.\r\nI tried to run TensorFlow-GPU...\r\n```\r\n\r\n\"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\Scripts\\python.exe\" \"C:/Users/Christoph Richter/PycharmProjects/NNV3/MNISTTest.py\"\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python35\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python35\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Christoph Richter/PycharmProjects/NNV3/MNISTTest.py\", line 1, in <module>\r\n    import keras\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Christoph Richter\\PycharmProjects\\NNV3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python35\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python35\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["Please provide following information asked by the template. Thanks!\r\n**System information**\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n**Describe the problem**\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "Hi @ymodak I am also facing the issue with installation of tensorflow (latest version 1.12.0). I tried installing the CPU version on my laptop whose configuration is: Windows 7 OS, 64 bit /core 2 duo processor. The error is:\r\n\r\nD:\\1projcode\\venv\\Scripts\\python.exe C:/Users/aakash/.PyCharmCE2018.1/config/scratches/scratch_1.py\r\nTraceback (most recent call last):\r\n  File \"D:\\1projcode\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\1projcode\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\1projcode\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\aakash\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\aakash\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/aakash/.PyCharmCE2018.1/config/scratches/scratch_1.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"D:\\1projcode\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"D:\\1projcode\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\1projcode\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\1projcode\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\1projcode\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\1projcode\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\aakash\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\aakash\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n", "Python version: 3.5.2 \r\nTensorflow installed from source using Pycharm (also tried using pip and got the same error)", "Duplicate #22794", "@ymodak How to resolve this? #22794 does not mention any solution. In that issue, many people have stated that they faced the same issue. But what's the solution? Mine is not GPU version, rather I am using CPU version of tensorflow. Still I am getting error related to pywrap. Please help.", "@CRichterLeipzig @AksBrijSandy Can you please try setting a virtual environment and install TensorFlow?\r\nApparently there's an issue with other ide's used for installation.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24605, "title": "[Feature Request] Indexing an array of functions", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently using a tensor to index a list of functions is inconsistent. In order to have the same functionality while training I have to chain together a series of tf.cond statements to accomplish the same task. Currently, this can be done in eager execution but not in outside of it.\r\n\r\nExample:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef mul(a):\r\n    return tf.multiply(a, 10)\r\n\r\ndef add(a):\r\n    return tf.add(a, 10)\r\n\r\ndef sub(a):\r\n    return tf.subtract(a, 10)\r\n\r\ndef div(a):\r\n    return tf.divide(a, 10)\r\n\r\nfunctions = [mul, add, sub, div]\r\na = tf.ones(shape=(), dtype=tf.float32) * 10\r\n\r\nfor i in range(10):\r\n    random_op = tf.random_uniform(shape=(), minval=0, maxval=len(functions), dtype=tf.int64)\r\n    result = functions[random_op](a)\r\n    print(random_op, result)\r\n```\r\n\r\nOutput:\r\n```\r\ntf.Tensor(1, shape=(), dtype=int64) tf.Tensor(20.0, shape=(), dtype=float32)\r\ntf.Tensor(1, shape=(), dtype=int64) tf.Tensor(20.0, shape=(), dtype=float32)\r\ntf.Tensor(1, shape=(), dtype=int64) tf.Tensor(20.0, shape=(), dtype=float32)\r\ntf.Tensor(2, shape=(), dtype=int64) tf.Tensor(0.0, shape=(), dtype=float32)\r\ntf.Tensor(0, shape=(), dtype=int64) tf.Tensor(100.0, shape=(), dtype=float32)\r\ntf.Tensor(0, shape=(), dtype=int64) tf.Tensor(100.0, shape=(), dtype=float32)\r\ntf.Tensor(1, shape=(), dtype=int64) tf.Tensor(20.0, shape=(), dtype=float32)\r\ntf.Tensor(1, shape=(), dtype=int64) tf.Tensor(20.0, shape=(), dtype=float32)\r\ntf.Tensor(2, shape=(), dtype=int64) tf.Tensor(0.0, shape=(), dtype=float32)\r\ntf.Tensor(0, shape=(), dtype=int64) tf.Tensor(100.0, shape=(), dtype=float32)\r\n```\r\n\r\n**Will this change the current api? How?**\r\nI suspect this will require the api to detect the output of the indexing the list to be a function as opposed to a new tensor.\r\n**Who will benefit with this feature?**\r\nThis should come as a large benefit to anyone who is performing online data augmentation.\r\n**Any Other info.**\r\n", "comments": ["I think this would basically be implementing [first-class functions](https://en.wikipedia.org/wiki/First-class_function) in TF, i.e. we'd need to represent functions as tensors, so you could use TF's operations with them. This would be pretty cool but unfortunately we don't have the bandwidth to tackle this kind of project right now. I recommend using [tf.case](https://www.tensorflow.org/api_docs/python/tf/case) instead.", "@nunezpaul,\r\nCan you please let us know if using [tf.case](https://www.tensorflow.org/api_docs/python/tf/case) as mentioned in [this comment](https://github.com/tensorflow/tensorflow/issues/24605#issuecomment-461177587) fulfills your request so that we can close this issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24604, "title": "How to create an tensor as vector C++ ?", "body": "Hey, there.\r\n\r\nI am trying to create an tensor as vector with 16 positions. However i cannot. Can you help me please?\r\nMy code:\r\n\r\n```\r\nTensor targetsValues(DT_FLOAT, TensorShape({16}));\r\ntargetsValues.scalar<float>()() = { 62, 62, 62, 62, 43, 55,  3, 78, 27, 78,  4, 78,  9, 78, 13, 78 };\r\n```\r\n\r\nThe error:\r\n\r\n![image](https://user-images.githubusercontent.com/18532570/50494950-08302d00-0a05-11e9-8eb3-1170f1c0c720.png)\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 24603, "title": "Retval[0] has already been set ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Modified Code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 28\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary pip\r\n- TensorFlow version (use command below):1.12\r\n- Python version:3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n I have a working SavedModel (ie. a saved model that works when restored in python) **that fails when run on tensorflow serving.**\r\nThe error message on the server is:\r\nOP_REQUIRES failed at function_ops.cc:68 : Internal: Retval[0] has already been set.\r\nError: W **external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1401]** OP_REQUIRES failed at function_ops.cc:68 : Internal: Retval[0] has already been set.\r\n\r\n \r\n### Exact Steps to Reproduce\r\n(https://drive.google.com/file/d/1at1CQ9iHgcPHCn-MkvSGcgtbVM2lrKJn/view) link to saved model. it can be restored and run in python successfully but will error if run on a model server. (Takes an image as input  sess.run(fetches=[\"loop/Exit_1:0\"],feed_dict={\"image_bytes:0\": image})\r\n\r\n\r\n### Source code / logs\r\nRelevant source code(I hope):\r\n(contains a while loop with a concat in the body)\r\n`\r\nval, idx =tf.nn.top_k(softmax ,name=\"topk\")\r\n    sentence = tf.Variable([vocab.start_id],False,name=\"sentence\",)\r\n    sentence = tf.concat([sentence, idx[0]], 0)#\r\n\r\n    def cond(sentence,state):\r\n        return tf.math.not_equal( sentence[-1],tf.constant(vocab.end_id))\r\n\r\n    def body(sentence,state):\r\n        input_seqs = tf.expand_dims([sentence[-1]], 1)\r\n\r\n        seq_embeddings = tf.nn.embedding_lookup(self.embedding_map, input_seqs)\r\n        embed = seq_embeddings\r\n\r\n        # In inference mode, use concatenated states for convenient feeding and\r\n        # fetching.\r\n        state_feed = tf.concat(axis=1, values=state, name=\"state\")\r\n\r\n        # Placeholder for feeding a batch of concatenated states.\r\n        # state_feed = tf.placeholder(dtype=tf.float32,\r\n        #                             shape=[None, sum(lstm_cell.state_size)],\r\n        #                             name=\"state_feed\")\r\n        state_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1)\r\n\r\n        # Run a single LSTM step.\r\n        lstm_outputs, new_state_tuple = lstm_cell(\r\n            inputs=tf.squeeze(embed, axis=[1]),\r\n            state=state_tuple)\r\n\r\n        # Concatentate the resulting state.\r\n        state = tf.concat(axis=1, values=new_state_tuple, name=\"state\")\r\n\r\n        # Stack batches vertically.\r\n\r\n        lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\r\n\r\n        with tf.variable_scope(\"logits\") as logits_scope:\r\n            logits = tf.contrib.layers.fully_connected(\r\n            inputs=lstm_outputs,\r\n            num_outputs=self.config.vocab_size,\r\n            activation_fn=None,\r\n            weights_initializer=self.initializer,\r\n            scope=logits_scope, reuse = True\r\n            )\r\n\r\n        softmax = tf.nn.softmax(logits, name=\"softmax\")\r\n        self.softmax = softmax\r\n        val, idx = tf.nn.top_k(softmax, name=\"topk\")\r\n\r\n        sentence = tf.concat([sentence,idx[0]],0)\r\n        self.output = sentence\r\n        return [sentence, state]\r\n    out = tf.while_loop(cond, body, [sentence, state],parallel_iterations=1,maximum_iterations=20,name=\"loop\",shape_invariants=[tf.TensorShape([None]),tf.TensorShape([None,None])])\r\n\r\n    return out\r\n`\r\nfails with error:\r\n`\r\n W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at function_ops.cc:68 : Internal: Retval[0] has already been set.\r\n\r\n`", "comments": ["Apologies for the delay in response. This issue is more suitable on TensorFlow Serving repo. Can you please post it on TensorFLow Serving repo from [here](https://github.com/tensorflow/serving/issues). Thanks!", ":) they sent me here because the error isn't in serving code.\r\nCan you explain what  Retval[0] has already been set may mean in the context of a while loop?\r\nGuesses welcome!"]}, {"number": 24602, "title": "Install documentation for pip install on PI not right / fails", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: unknown.\r\n- Doc Link: https://www.tensorflow.org/install/pip\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nfor // Linux rasputin 4.9.35-v7+ #1014 SMP Fri Jun 30 14:47:43 BST 2017 armv7l GNU/Linux\r\nthe directions for installing tensor flow fail after setting up the v environment.\r\n\r\n(venv) ebcdic@rasputin:~ $ pip install --upgrade tensorflow\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensor flow\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Is this still an issue? Can you check with the latest version of TensorFlow and try again?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24601, "title": "possible bug in examples/speech_commands/test_streaming_accuracy.cc", "body": "in tensorflow/tensorflow/examples/speech_commands/test_streaming_accuracy.cc line 247 the following code is not correct:\r\n\r\n`const int64 audio_data_end = (sample_count - clip_duration_ms);`\r\nit should be corrected as follows:\r\n`const int64 audio_data_end = (sample_count - clip_duration_samples);`\r\n\r\nit is wrong because the dimensions do not match. sample_count is in units [sample] but clip_duration_ms is in [ms] units.\r\nAdditionally it gives the following error:\r\n`Segmentation fault (core dumped)`\r\n\r\nbut if audio_data_end is set correctly as i mentioned above, this error would not occur.\r\n", "comments": ["@Alireza89 I think you are correct, do you want to create a PR for that?", "@yongtang  Well I don't have a plan to create a PR for now but if it is actually a bug, I hope it gets corrected in the master branch in the future.", "@Alireza89 I have created the PR #24613 for the fix. Thanks for your contribution!", "Thanks for the fix! The PR has been reviewed, so closing this now."]}, {"number": 24600, "title": "cudnn gru has bug", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source):0.19\r\n- GCC/Compiler version (if compiling from source):5.4.0\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI write a unit test to compare tf.CudnnGRU and native gru but they got different result, i think it's cudnn's problem....becase i don't find any problem in my code....\r\n\r\n**Describe the expected behavior**\r\nshould be same\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\r\n\r\nS_MIN=-40\r\nS_MAX=13\r\nEXP_MAX=40\r\n\r\ndef native_gru(input, w):\r\n\r\n        seq_len, batch_size, hidden_size=input.shape\r\n\r\n        offset = 0\r\n        wu = w[offset:offset + hidden_size*hidden_size].reshape(\r\n                (hidden_size,hidden_size)).transpose()\r\n        offset += hidden_size*hidden_size\r\n        wr = w[offset:offset + hidden_size*hidden_size].reshape(\r\n                (hidden_size,hidden_size)).transpose()\r\n        offset += hidden_size*hidden_size\r\n        wc = w[offset:offset + hidden_size*hidden_size].reshape(\r\n                (hidden_size,hidden_size)).transpose()\r\n        offset += hidden_size*hidden_size\r\n\r\n\r\n        ru = w[offset:offset + hidden_size*hidden_size].reshape(\r\n                (hidden_size,hidden_size)).transpose()\r\n        offset += hidden_size*hidden_size\r\n        rr = w[offset:offset + hidden_size*hidden_size].reshape(\r\n                (hidden_size,hidden_size)).transpose()\r\n        offset += hidden_size*hidden_size\r\n        rc = w[offset:offset + hidden_size*hidden_size].reshape(\r\n                (hidden_size,hidden_size)).transpose()\r\n        offset += hidden_size*hidden_size\r\n\r\n\r\n\r\n        bx_u = w[offset:offset + hidden_size]\r\n        offset += hidden_size\r\n        bx_r = w[offset:offset + hidden_size]\r\n        offset += hidden_size\r\n        bx_c = w[offset:offset + hidden_size]\r\n        offset += hidden_size\r\n\r\n\r\n        bh_u = w[offset:offset + hidden_size]\r\n        offset += hidden_size\r\n        bh_r = w[offset:offset + hidden_size]\r\n        offset += hidden_size\r\n        bh_c = w[offset:offset + hidden_size]\r\n        offset += hidden_size\r\n\r\n        def sigmod(x):\r\n                y = np.copy(x)\r\n                y[x < S_MIN] = S_MIN\r\n                y[x>S_MAX] = S_MAX\r\n                return 1./(1. + np.exp(-y))\r\n\r\n        def tanh(x):\r\n                y=-2.*x\r\n                y[y>EXP_MAX] = EXP_MAX\r\n                return (2./(1.+np.exp(y))) - 1\r\n\r\n        output = []\r\n        pre_h = np.zeros((batch_size, hidden_size), dtype=input.dtype)\r\n\r\n        for i in range(seq_len):\r\n                emb_1 = input[i]\r\n                update_gate = sigmod(np.matmul(emb_1, wu) + np.matmul(pre_h, ru)+bx_u+bh_u)\r\n                reset_gate = sigmod(np.matmul(emb_1, wr)+np.matmul(pre_h, rr)+bx_r+bh_r)\r\n                h_t_temp = tanh(np.matmul(emb_1, wc)+reset_gate*(np.matmul(pre_h,rc)+bh_c)+bx_c)\r\n                new_h = update_gate*pre_h + (1-update_gate)*h_t_temp\r\n                pre_h = new_h\r\n                output.append(new_h)\r\n\r\n        output = np.concatenate(output, -1)\r\n        output = output.reshape((batch_size, -1, hidden_size))\r\n        output = output.transpose((1,0,2))\r\n        return output\r\n\r\ndef TestGRU():\r\n        num_steps = 2\r\n        batch_size = 1\r\n        hidden_size = 1\r\n        input_w_size = 6*hidden_size*hidden_size + 6*hidden_size\r\n        x = np.random.uniform(low=-0.1, high=0.1, size=(num_steps, batch_size, hidden_size)).astype(np.float32)\r\n        #x = np.ones((num_steps, batch_size, hidden_size)).astype(np.float32)\r\n\r\n\r\n        flat_w = np.random.uniform(low=-0.1, high=0.1, size=(input_w_size)).astype(np.float32)\r\n        #flat_w = np.ones((input_w_size)).astype(np.float32)\r\n        out = native_gru(x, flat_w)\r\n\r\n        n_layer = 1\r\n        rnn_cudnn = cudnn_rnn_ops.CudnnGRU(n_layer, hidden_size, hidden_size, 'linear_input')\r\n        #param_cudnn = tf.Variable(tf.ones([rnn_cudnn.params_size()]), validate_shape=False)\r\n        param_cudnn = tf.Variable(flat_w)\r\n\r\n        y_cudnn, state_cudnn = rnn_cudnn(x,#tf.transpose(x, [1,0,2]),\r\n        tf.zeros([n_layer, batch_size, hidden_size]), param_cudnn)\r\n        print(\"native gru:\"+str(out))\r\n        with tf.Session() as sess:\r\n                sess.run(tf.global_variables_initializer())\r\n                print(\"cudnn gru:\" +str(sess.run([y_cudnn])))\r\n\r\nif __name__ == '__main__':\r\n        TestGRU()\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "comments": ["Thank for reporting this, and sorry for the very late reply.\r\n\r\nThe cudnn gru kernel is provide by Nvidia, and we don't think it has a bug. tf.keras.layers.GRU is an equivalent implementation as the Cudnn GRU. They should generate same math result if the inputs and kernel weights are the same. Due to the limit of resource, we don't have enough time to debug your code. You can refer to tf.keras.layers.GRUCell for the details the math, and compare the intermediate results. ", "@COthello  can you  put all your code into a gist and share with me? I want to try it.", "yes, it's not a bug, I misunderstand cudnn spec, sorry for the problem"]}, {"number": 24599, "title": "Documentation: User-friendly Bazel installation", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n**System information**\r\n- TensorFlow version: GPU 1.14.0\r\n- Doc Link: https://www.tensorflow.org/install/source_windows\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nThe documentation for installing Bazel on Windows is separated into two different pages with no indication that the second page is required after following the first.  This can lead to install issues and build errors that make absolutely no sense, but do after figuring out that you're missing 5 dependencies, mentioned only on the second page under \"Building C++ Projects\"\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nI have written a massive walkthrough for installing tensorflow <a href=\"http://aaronjencks.net/blog/8/#Bazel\">here</a>, but in reality, just a mention of the second page as an indication that there's more to set up than just the \"Install Bazel\" <a href=\"https://docs.bazel.build/versions/master/install-windows.html\">link</a> on the site, that the user must also install the dependencies for building C++ projects with Bazel, found <a href=\"https://docs.bazel.build/versions/master/windows.html\">here</a>, would solve the issue.", "comments": ["Hi @iggy12345 \r\n\r\nThanks for the report.\r\n\r\nI don't have a windows box handy to try this on. \r\nSo help me understand.\r\n\r\nThe install_source_windows file includes in the setup section [Install Visual C++ Build Tools 2015](https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2015)\r\n\r\nIsn't that what the \"C++ with Bazel\" page you linked to says?\r\n\r\nOr is it just not clear in [Setup](https://www.tensorflow.org/install/source_windows#setup_for_windows) that you need to do __all__ these things?\r\n\r\nEither way, you seem to have a clear idea of what's missing, if you want to propose a PR, here is the [source file](https://github.com/tensorflow/docs/blob/master/site/en/install/source_windows.md) for that page.", "I will do that, thank you for your response!", "Okay, I made the change, is there anything special I need to do to submit a PR?\r\n[Tensorflow_PR_24599.txt](https://github.com/tensorflow/tensorflow/files/2809427/Tensorflow_PR_24599.txt)\r\n", "Nothing special.\r\n\r\nJust follow the usual process.\r\n\r\n- Navigate to the document in the browser.\r\n- Click the edit button.\r\n- Paste in your edits.\r\n- Click the Commit changes button to save it onto a branch.\r\n- Send a PR.\r\n\r\n", "Okay, I think I did it then", "Wow, this is really easy, okay", "Closing this issue since its resolved. Feel free to reopen if otherwise. Thanks!"]}]