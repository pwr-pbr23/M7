[{"number": 38514, "title": "tf.keras.callbacks.ModelCheckpoint fails in distributed Parameter Server Strategy", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux RHEL 7\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1\r\n\r\n**Describe the current behavior**\r\n`tf.keras.callbacks.ModelCheckpoint` callback fails to save the model if the parameter server doesn't have access to the checkpoint location of other workers. And the location for workers other than chief worker, cannot be configured to a remote filesystem (e.g., hdfs)\r\n\r\n**Describe the expected behavior**\r\nThe callback should be able to save the model if running on different machines using shared storage. I am not sure if the shared storage should be a requirement either.\r\n\r\n**Standalone code to reproduce the issue** \r\n1. Create model.py provided below\r\n2. Run the model.py in two terminals.\r\n    - Terminal 1: Simply execute the model.py with the shown args.\r\n    - Terminal 2: Run the unshare command to separate the mount namespace.\r\noptional:\r\n  Instead of running on the same machine, you can also run the given script on two separate machines. All you'd have to do this is change TF_CONFIG in the model.py. Then you wouldn't need to separate the mount namespace.\r\n\r\n\r\nmodel.py\r\n```py\r\nimport os\r\nimport pprint\r\nimport sys\r\nimport json\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nnode_attr = sys.argv[1]\r\nname = node_attr[:-1]\r\nindex = node_attr[-1]\r\nos.environ['CUDA_VISIBLE_DEVICES']='-1'\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\"cluster\": {\"worker\": [\"localhost:5773\"], \"ps\": [\"localhost:5711\"]}, \"task\": {\"type\": name, \"index\": int(index)}})\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy()\r\n\r\n# Uncomment below for multi worker mirror strategy.\r\n#os.environ['TF_CONFIG'] = json.dumps({\"cluster\": {\"worker\": [\"localhost:5773\", \"localhost:6778\"]}, \"task\": {\"type\": name, \"index\": int(index)}})\r\n#strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\ndef input_fn(mode):\r\n    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n    mnist_dataset = (\r\n        datasets['train'] if mode == 'train' else datasets['test']\r\n    )\r\n    def scale(image, label):\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n        return image, label\r\n\r\n    return mnist_dataset.map(scale).cache().repeat(2).shuffle(10000).batch(4)\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10)\r\n    ])\r\n    model.compile(\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n        metrics=['accuracy'])\r\n    return model\r\n\r\ndef main():\r\n    tf_config = json.loads(os.environ['TF_CONFIG'])\r\n    job_name = tf_config['task']['type']\r\n    job_index = tf_config['task']['index']\r\n\r\n    if job_name == 'ps':\r\n        server = tf.distribute.Server(\r\n            tf_config['cluster'], job_name=job_name, task_index=job_index\r\n        )\r\n        server.join()\r\n    else:\r\n      train_dataset = input_fn('train')\r\n      ckpt_full_path = os.path.join(sys.argv[2], 'model.ckpt-{epoch:04d}')\r\n      callbacks = [\r\n          tf.keras.callbacks.ModelCheckpoint(ckpt_full_path, save_weights_only=True, verbose=1, save_freq=1),\r\n      ]\r\n\r\n      options = tf.data.Options()\r\n      options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\r\n      train_datasets_no_auto_shard = train_dataset.with_options(options)\r\n\r\n      with strategy.scope():\r\n        model = build_and_compile_cnn_model()\r\n      model.fit(x=train_datasets_no_auto_shard, epochs=3,steps_per_epoch=3, callbacks=callbacks)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nSteps to reproduce:\r\n1st terminal\r\n```\r\n$ python model.py worker0 /tmp\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2020-04-13 18:11:00.844880: I tensorflow/core/distributed_runtime/worker.cc:204] Cancellation requested for RunGraph.\r\nTrain on 3 steps\r\nEpoch 1/3\r\n\r\nEpoch 00001: saving model to /tmp/checkpoints/model.ckpt-0001\r\nTraceback (most recent call last):\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1367, in _do_call\r\n    return fn(*args)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1352, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1445, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: From /job:ps/replica:0/task:0:\r\n/tmp/checkpoints/model.ckpt-0001_temp_8b7417c3f79f449f87c2218bde68999d; No such file or directory\r\n\t [[{{node SaveV2}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"model.py\", line 87, in <module>\r\n    main()\r\n  File \"model.py\", line 83, in main\r\n    model.fit(x=train_datasets_no_auto_shard, epochs=3,steps_per_epoch=3, callbacks=callbacks)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 685, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 352, in model_iteration\r\n    callbacks._call_batch_hook(mode, 'end', step, batch_logs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\", line 239, in _call_batch_hook\r\n    batch_hook(batch, logs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\", line 528, in on_train_batch_end\r\n    self.on_batch_end(batch, logs=logs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\", line 977, in on_batch_end\r\n    self._save_model(epoch=self._current_epoch, logs=logs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\", line 1038, in _save_model\r\n    self.model.save_weights(filepath, overwrite=True)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1123, in save_weights\r\n    self._trackable_saver.save(filepath, session=session)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1177, in save\r\n    return session.run(save_path, feed_dict=feed_dict)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 960, in run\r\n    run_metadata_ptr)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1183, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1361, in _do_run\r\n    run_metadata)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1386, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: From /job:ps/replica:0/task:0:\r\n/tmp/checkpoints/model.ckpt-0001_temp_8b7417c3f79f449f87c2218bde68999d; No such file or directory\r\n\t [[node SaveV2 (defined at model.py:83) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node SaveV2:\r\n dense/kernel/Read/ReadVariableOp (defined at model.py:48)\t\r\n training/SGD/decay/Read/ReadVariableOp (defined at /export/apps/python/3.7/lib/python3.7/threading.py:926)\r\n\r\nOriginal stack trace for 'SaveV2':\r\n  File \"model.py\", line 87, in <module>\r\n    main()\r\n  File \"model.py\", line 83, in main\r\n    model.fit(x=train_datasets_no_auto_shard, epochs=3,steps_per_epoch=3, callbacks=callbacks)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 685, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 352, in model_iteration\r\n    callbacks._call_batch_hook(mode, 'end', step, batch_logs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\", line 239, in _call_batch_hook\r\n    batch_hook(batch, logs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\", line 528, in on_train_batch_end\r\n    self.on_batch_end(batch, logs=logs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\", line 977, in on_batch_end\r\n    self._save_model(epoch=self._current_epoch, logs=logs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\", line 1038, in _save_model\r\n    self.model.save_weights(filepath, overwrite=True)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 1123, in save_weights\r\n    self._trackable_saver.save(filepath, session=session)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1168, in save\r\n    file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py\", line 1116, in _save_cached_when_graph_building\r\n    save_op = saver.save(file_prefix)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py\", line 230, in save\r\n    sharded_saves.append(saver.save(shard_prefix))\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py\", line 72, in save\r\n    return io_ops.save_v2(file_prefix, tensor_names, tensor_slices, tensors)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1717, in save_v2\r\n    name=name)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 742, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3322, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1756, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nException ignored in: <function Server.__del__ at 0x7f4457191320>\r\nTraceback (most recent call last):\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/training/server_lib.py\", line 158, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'UnimplementedError'\r\n```\r\n\r\n2nd terminal\r\n```\r\n# isolate the mount namespace\r\n$ unshare --mount\r\n$ mkdir newtmp; mount --bind ./newtmp /tmp\r\n$ python model.py ps0 /tmp\r\n...\r\n...\r\n2020-04-13 18:04:37.871417: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job ps -> {0 -> localhost:5711}\r\n2020-04-13 18:04:37.871485: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> localhost:5773}\r\n2020-04-13 18:04:37.872330: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:5711\r\n2020-04-13 18:09:31.762295: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at save_restore_v2_ops.cc:109 : Not found: /tmp/checkpoints/model.ckpt-0001_temp_64dd8f8a454a4a28b378ce888c66219e; No such file or directory\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n- It works fine with MultiWorkerMirrorStrategy\r\n- Even if we wanted to give shared access through HDFS, It doesn't work. It tries to write to a /tmp location on workers other than chief.\r\n- It does work if all the workers and ps are running on the same machine with local filesystem access.\r\n", "comments": ["Are you saying that \r\n\r\npython model.py worker0 [shared localtion]\r\npython model.py ps0 [shared localtion]\r\n\r\ndoesn't work? For ParameterServer the location needs to be a shared one among all workers + ps.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@crccw My apologies for the late response on this. \r\n\r\nThank you for confirming that shared location is a requirement in Parameter Strategy. \r\n\r\nTo answer your question, if there's only 1 worker (worker0), the training works as expected. However, if there are more workers and the shared location is an hdfs location it doesn't work.\r\n\r\nReason being that worker1 is trying to write at a temporary location that is local to that worker.\r\n\r\nFor example:\r\n\r\nI setup hdfs locally and this is how I started the training:\r\n\r\n```\r\npython model.py ps0 hdfs://172.18.0.2/data/p1\r\n.... <truncated ps logs> ....\r\n```\r\n\r\nOn **worker0**, I see that the model is being saved to the hdfs location:\r\n```\r\npython model.py worker0 hdfs://172.18.0.2/data/p1\r\n...\r\nEpoch 00002: saving model to hdfs://172.18.0.2/data/p1/model.ckpt-0002\r\n...\r\n```\r\n\r\nHowever on **worker1**, the model is being saved to the tmp location:\r\n```\r\npython model.py worker1 hdfs://172.18.0.2/data/p1\r\n...\r\nEpoch 00007: saving model to /tmp/tmppcu_cwk9/temp.ckpt-{epoch:04d}\r\n...\r\n```\r\nSince worker1 is trying to write at a /tmp location and if the ps is running on a different machine, this location is not accessible. So it fails with an error.", "FWIW, it works as expected in the nightly release.\r\n\r\n`worker1`: seems to be writing at a temporary location in hdfs. At least based on the logs.\r\n```\r\nEpoch 00001: saving model to hdfs://172.18.0.2/data/workertemp_1/model.ckpt-0001\r\n```\r\nHowever, It'll take me some time to run this in our cluster to verify.\r\n\r\nThe version I tested:\r\n```\r\n(tf-2.2-venv) $ python\r\nPython 3.7.7 (default, Mar 10 2020, 23:11:38)\r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-04-22 15:31:12.809176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n>>>\r\n>>>\r\n>>> tf.__version__\r\n'2.2.0-dev20200422'\r\n```\r\nHowever, I am unable to point out the exact commit that fixed it. I'd be curious to learn what fixed the issue. ", "@goyalankit   Closing this issue since it is working as expected on tf-nightly. please reopen the issue if you have any concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38514\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38514\">No</a>\n"]}, {"number": 38512, "title": "Keras Callbacks `params` API change", "body": "In the current nightly there were changes to the `params` attribute in the Callbacks that is affecting custom callbacks code.\r\n\r\n**System information** \r\n- Yes\r\n- Colab\r\n\r\n**Describe the current behavior**\r\n\r\nThe minimal example is:\r\n\r\n```\r\nimport tensorflow as tf\r\nmodel = tf.keras.Sequential([tf.keras.layers.Dense(1)])\r\nmodel.compile(loss = \"mse\", optimizer = \"sgd\")\r\ncallback = tf.keras.callbacks.Callback()\r\nmodel.fit(\r\n    [1,2,3],\r\n    [5,6,7],\r\n    callbacks = [callback]\r\n)\r\ncallback.params\r\n```\r\n\r\nWith current 2.2-rc3 we see:\r\n\r\n```\r\n{'epochs': 1, 'steps': 1, 'verbose': 1}\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behavior is a dict with the following parameters:\r\n\r\n```\r\n{'batch_size': 32,\r\n 'do_validation': False,\r\n 'epochs': 1,\r\n 'metrics': ['loss'],\r\n 'samples': 3,\r\n 'steps': 1,\r\n 'verbose': 1}\r\n```\r\n\r\nHere are colab links for:\r\n\r\n2.1: https://colab.research.google.com/drive/1D0o-1J9StBPqnrgnOBH5NuqbxoLHodN4\r\n2.2-rc2: https://colab.research.google.com/drive/15c6gJSCL19xrx68fmgUtFsbXFDDYpg3_\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.2.0rc3](https://colab.research.google.com/gist/amahendrakar/c9d0d40e38c4c9f7adf7dcdd26af82ec/38512-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/e57fab07562fb0d93ca863af79d44ff1/38512-tf-nightly.ipynb). Works as expected with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/4d7d6b85c7f0614c9222770c910c0762/38512-2-1.ipynb). Please find the attached gist. Thanks!", "@dfalbel Thanks for the issue!\r\n\r\nThis change is intended, could you explain your use case and I can advise on how your code can be updated?", "Hi @omalleyt12 ! Thanks for your answer.\r\n\r\nIn [Keras for R](https://github.com/rstudio/keras) we have a default callback that will display metrics nicely in the RStudio IDE, and write logs to a [run tracker](https://github.com/rstudio/tfruns). \r\n\r\nThis is implemented here (in R, but accessing the Callbacks python class) : https://github.com/rstudio/keras/blob/master/R/metrics-callback.R#L18-L41\r\n\r\nWe basically use the `params` dictionary to get the name of the metrics that we should track as well as the `do_validation` field.\r\n\r\n\r\n\r\n\r\n", "@dfalbel Got it, thanks! The best way to access the name of the Model's metrics in a Callback IMO is to use `self.model.metrics_names`. This should only be accessed after the first batch has completed, since metrics can be created the first time the Model is called on actual data\r\n\r\nFor `do_validation`, I think it's probably better to make use of the `Callback.on_test_begin` endpoint. If this hook is called, then validation will be performed", "Thanks @omalleyt12, your suggestions were very helpful.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38512\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38512\">No</a>\n"]}, {"number": 38511, "title": "RuntimeError while trying to run with Parameter Server Strategy in Eager mode", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): **RHEL 7**\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): **2.0, 2.1**\r\n\r\n**Describe the current behavior**\r\nThe model fails to instantiate and fails with the following error:\r\n```\r\n  File \"model.py\", line 10, in <module>\r\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 116, in __init__\r\n    self.add(layer)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 185, in add\r\n    layer(x)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 748, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2116, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 158, in build\r\n    dtype=self.dtype)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 446, in add_weight\r\n    caching_device=caching_device)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 142, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1330, in creator_with_resource_vars\r\n    return self._create_variable(*args, **kwargs)\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/distribute/parameter_server_strategy.py\", line 446, in _create_variable\r\n    with ops.device(self._variable_device):\r\n  File \"/home/angoyal/ws/scratch/myvenv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 5032, in device\r\n    \"tf.device does not support functions when eager execution \"\r\nRuntimeError: tf.device does not support functions when eager execution is enabled.\r\n```\r\n**Describe the expected behavior**\r\nThe model should instantiate\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n(myvenv) [angoyal@angoyal-ld2 scratch]$ cat model.py\r\nimport os\r\nimport tensorflow as tf\r\n\r\nos.environ['TF_CONFIG'] = '{\"cluster\": {\"worker\": [\"localhost:5773\"], \"ps\": [\"localhost:5711\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}'\r\n\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy()\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n    ])\r\n````\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["ParameterServerStrategy is not supported in eager yet. We're actively working on it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38511\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38511\">No</a>\n"]}, {"number": 38510, "title": "Cannot find dependencies to use in bazel BUILD to compile tensorflow lite", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\nUnable to find documentation to compile a .cc file that will load and run the interpreter on a tflite model.  I have created a test folder which contains test.cc and BUILD  the build file is shown as below.  I have attempted to list deps but it fails to compile each time i run bazel build :test from within the test directory\r\n\r\ncc_binary(\r\n    name = \"catlite\",\r\n    srcs = [\"catlite.cc\"],\r\n    deps = []\r\n\r\n)\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Is this what you're looking for?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/BUILD", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38510\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38510\">No</a>\n"]}, {"number": 38509, "title": "List deterministic op functionality bug fixes in version 2.2 release notes", "body": "The changes in this current PR refer to the following commits and PR:\r\n\r\n* Commit [e31955](https://github.com/tensorflow/tensorflow/commit/e31955d9fb34ae7273354dc2347ba99eea8c5280): [XLA/GPU] Convert reduction into tree reduction using padding + bitcast-reshape\r\n* Commit [8b7a3](https://github.com/tensorflow/tensorflow/commit/8b7a3db0b6e09415b5640be4986fb4d7c6e5209a\r\n): [XLA] Respect TF_DETERMINISTIC_OPS environment variable for reductions\r\n* PR [34951](https://github.com/tensorflow/tensorflow/pull/34951): Add multi-algorithm deterministic cuDNN convolutions\r\n\r\nThanks to @goldiegadde for getting `RELEASE.md` committed in the `r2.2` branch so that I could submit this current PR.", "comments": []}, {"number": 38508, "title": "Tensorboard exception with non-ascii character username on windows", "body": "When there are non-ascii characters in the user name, tensorboard will have problems.\r\n\r\n```\r\n2020-04-14 04:28:35.993085: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2020-04-14 04:28:35.996519: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:217]  GpuTracer has collected 0 callback api events and 0 activity events.\r\n2020-04-14 04:28:36.013131: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: logs\\train\\plugins\\profile\\2020_04_13_20_28_35\r\n2020-04-14 04:28:36.027295: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to logs\\train\\plugins\\profile\\2020_04_13_20_28_35\\\ufffd\ufffd\ufffd\u01f1\ufffd\ufffd\ufffd\u013c\ufffd\ufffd\ufffd.trace.json.gz\r\n2020-04-14 04:28:36.031216: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0 ms\r\n\r\n2020-04-14 04:28:36.039011: I tensorflow/python/profiler/internal/profiler_wrapper.cc:91] Creating directory: logs\\train\\plugins\\profile\\2020_04_13_20_28_35Dumped tool data for overview_page.pb to logs\\train\\plugins\\profile\\2020_04_13_20_28_35\\\ufffd\ufffd\ufffd\u01f1\ufffd\ufffd\ufffd\u013c\ufffd\ufffd\ufffd.overview_page.pb\r\nDumped tool data for input_pipeline.pb to logs\\train\\plugins\\profile\\2020_04_13_20_28_35\\\ufffd\ufffd\ufffd\u01f1\ufffd\ufffd\ufffd\u013c\ufffd\ufffd\ufffd.input_pipeline.pb\r\nDumped tool data for tensorflow_stats.pb to logs\\train\\plugins\\profile\\2020_04_13_20_28_35\\\ufffd\ufffd\ufffd\u01f1\ufffd\ufffd\ufffd\u013c\ufffd\ufffd\ufffd.tensorflow_stats.pb\r\nDumped tool data for kernel_stats.pb to logs\\train\\plugins\\profile\\2020_04_13_20_28_35\\\ufffd\ufffd\ufffd\u01f1\ufffd\ufffd\ufffd\u013c\ufffd\ufffd\ufffd.kernel_stats.pb\r\n```", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Please, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Please, provide simple standalone code to reproduce the issue reported here.It helps us in localizing the issue faster. Thanks!\r\n", "WIN10\r\ntf-nightly-gpu\r\n\r\nmodel.fit(callbacks=[keras.callbacks.TensorBoard()])\r\n", "@liuxingbaoyu \r\n\r\nWill it be possible to share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Any code that uses `model.fit(callbacks = [keras.callbacks.TensorBoard()])` can be reproduced", "https://sourcegraph.com/github.com/tensorflow/tensorflow@b97c90c87fc433140bae47d93521a3b795645c12/-/blob/tensorflow/core/platform/windows/port.cc#L46:9", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38508\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38508\">No</a>\n"]}, {"number": 38507, "title": "building tfcompile.exe debug mode with Windows failed", "body": "I want to use the debug mode tfcompile to trace into one problem that XLA/AOT cannot compile my model. But I found problem in building the debug mode tfcompile file.\r\n\r\nI followed\u00a0https://www.tensorflow.org/install/source\u00a0(for windows part), installed msys2.With all nvidia drivers Cuda, cudnn installed.\r\n\r\nTensorflow 2.2.0 (latest on branch r2.2 on Apr 13, 2020), \r\nWin10 Pro, \r\nPython 3.6, \r\nBazel 2.0.0 (or 2.2.0) \r\nCuda 10.2 + Cudnn\r\nnvidia 1060\r\n\r\nVS2019 native console:\r\ncd d:/w/Git/tensorflowDebugpython configure.py\r\n`python configure.py`\r\n\r\nI can successfully build release mode tfcompile, using command\r\n`bazel build --config=opt --config=cuda //tensorflow/compiler/aot:tfcompile` \r\n\r\nBut never succeed for debug mode, with the following command under VS2019 native console:\r\n`bazel build -s --config=opt -c dbg --strip=never --config=cuda //tensorflow/compiler/aot:tfcompile` \r\n\r\nThere were some bugs  I had to fix before going to the following symbol link errors, basically the return value empty error. I have no idea why they did not affect the release version. I can't find any clue in C++ codes to differentiate the release and debug modes. Only bazel --config=opt -c dbg seems the switch to turn debug on. I traced into bazel files, but not found any suspicious errors.\r\n\r\nAnother bug I have to work around is I saw:\r\nC:/Users/yuefe/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmpdiauwif_/fill_functor.cu.compute_70.cudafe1.cpp: fatal error C1041: cannot open program database 'C:\\users\\yuefe\\_bazel_yuefe\\p5ggus4g\\execroot\\org_tensorflow\\vc140.pdb'; if multiple CL.EXE write to the same .PDB file, please use /FS\r\n\r\nI cannot find the root. My solution is to add --job 1 to the command line:\r\n`bazel build --jobs 1 -s --config=opt -c dbg --strip=never --config=cuda //tensorflow/compiler/aot:tfcompile` \r\n\r\nFinally, compiling can reach the following point:\r\n\r\nWhen failing, I observed:\r\nlibconcat_op.lo(concat_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<unsigned __int64>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<unsigned __int64,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@_K@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CB_K$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CB_K$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CB_K$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CB_K$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@_K$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,unsigned __int64,1>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@_K$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibconcat_op.lo(concat_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<unsigned int>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned int const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned int const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned int const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned int const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<unsigned int,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@I@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBI$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBI$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBI$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBI$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@I$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,unsigned int,1>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@I$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibconcat_op.lo(concat_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt32>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQInt32@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,struct Eigen::QInt32,1>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@UQInt32@2@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibpack_op.lo(pack_op.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt32>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQInt32@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibtensor_array_ops.lo(tensor_array_ops.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt32>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQInt32@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nliblist_kernels.lo(list_kernels.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt32>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt32,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQInt32@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt32@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibconcat_op.lo(concat_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt16>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQInt16@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,struct Eigen::QInt16,1>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@UQInt16@2@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nliblist_kernels.lo(list_kernels.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt16>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt16,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQInt16@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibconcat_op.lo(concat_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt16>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQUInt16@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,struct Eigen::QUInt16,1>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@UQUInt16@2@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nliblist_kernels.lo(list_kernels.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt16>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt16,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQUInt16@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt16@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibconcat_op.lo(concat_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,struct Eigen::QInt8,1>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@UQInt8@2@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibpack_op.lo(pack_op.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibtensor_array_ops.lo(tensor_array_ops.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nliblist_kernels.lo(list_kernels.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibconcat_op.lo(concat_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQUInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,struct Eigen::QUInt8,1>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@UQUInt8@2@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibpack_op.lo(pack_op.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQUInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibtensor_array_ops.lo(tensor_array_ops.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQUInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nliblist_kernels.lo(list_kernels.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<struct Eigen::QUInt8>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8 const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QUInt8,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@UQUInt8@Eigen@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBUQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@UQUInt8@Eigen@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibconcat_op.lo(concat_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<class tensorflow::tstring>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@Vtstring@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring,1>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibpack_op.lo(pack_op.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<class tensorflow::tstring>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@Vtstring@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibtensor_array_ops.lo(tensor_array_ops.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<class tensorflow::tstring>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@Vtstring@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nliblist_kernels.lo(list_kernels.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<class tensorflow::tstring>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@Vtstring@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibconcat_op.lo(concat_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<signed char>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<signed char,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@C@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@C$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,signed char,1>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@C$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibpack_op.lo(pack_op.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<signed char>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<signed char,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@C@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@C$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibtensor_array_ops.lo(tensor_array_ops.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<signed char>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<signed char,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@C@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@C$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nliblist_kernels.lo(list_kernels.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<signed char>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<signed char const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<signed char,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@C@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBC$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@C$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibconcat_op.lo(concat_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<unsigned short>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@G@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@G$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ConcatBaseOp<struct Eigen::ThreadPoolDevice,unsigned short,1>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ConcatBaseOp@UThreadPoolDevice@Eigen@@G$00@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibpack_op.lo(pack_op.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<unsigned short>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@G@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@G$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibtensor_array_ops.lo(tensor_array_ops.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<unsigned short>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@G@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@G$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nliblist_kernels.lo(list_kernels.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<unsigned short>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@G@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBG$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@G$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,struct Eigen::QInt8,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@UQInt8@2@$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::GpuDevice,struct Eigen::QInt8>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UGpuDevice@Eigen@@UQInt8@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,class tensorflow::Variant,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@VVariant@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,class tensorflow::Variant,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@VVariant@tensorflow@@$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,class tensorflow::ResourceHandle,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@VResourceHandle@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,class tensorflow::ResourceHandle,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@VResourceHandle@tensorflow@@$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,class tensorflow::tstring,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@Vtstring@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,class tensorflow::tstring,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@Vtstring@tensorflow@@$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,bool,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@_N$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,bool,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@_N$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,class std::complex<double>,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double>,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@V?$complex@N@std@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class std::complex<double> >::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@V?$complex@N@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,class std::complex<double>,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double>,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@V?$complex@N@std@@$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class std::complex<double> >::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@V?$complex@N@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,class std::complex<float>,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float>,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@V?$complex@M@std@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class std::complex<float> >::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@V?$complex@M@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,class std::complex<float>,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float>,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@V?$complex@M@std@@$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,class std::complex<float> >::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@V?$complex@M@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,double,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<double,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@N$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBN$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,double>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,double,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<double,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@N$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBN$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,double>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,struct tensorflow::bfloat16,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@Ubfloat16@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUbfloat16@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Ubfloat16@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,struct tensorflow::bfloat16>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Ubfloat16@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,struct tensorflow::bfloat16,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@Ubfloat16@tensorflow@@$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUbfloat16@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Ubfloat16@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,struct tensorflow::bfloat16>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@Ubfloat16@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,signed char,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<signed char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<signed char,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@C$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBC$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@C$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,signed char,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<signed char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<signed char,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@C$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBC$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@C$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,short,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<short,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@F$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBF$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@F$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,short>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@F@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,short,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<short,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@F$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBF$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@F$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,short>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@F@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,unsigned short,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@G$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBG$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@G$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned short>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@G@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,unsigned short,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@G$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBG$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@G$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,unsigned short>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@G@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,int,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<int const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<int,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@H$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBH$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@H$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,int>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@H@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,__int64,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<__int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<__int64,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@_J$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,__int64>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_J@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(spacetodepth_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpaceToDepthOpFunctor<struct Eigen::GpuDevice,__int64,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<__int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<__int64,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SpaceToDepthOpFunctor@UGpuDevice@Eigen@@_J$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SpaceToDepthOp<struct Eigen::ThreadPoolDevice,__int64>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$SpaceToDepthOp@UThreadPoolDevice@Eigen@@_J@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,struct Eigen::QInt8,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,5,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,5,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@UQInt8@2@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$04$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$04$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::GpuDevice,struct Eigen::QInt8>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UGpuDevice@Eigen@@UQInt8@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,struct Eigen::QInt8,0>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8 const ,5,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct Eigen::QInt8,5,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@UQInt8@2@$0A@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUQInt8@Eigen@@$04$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@UQInt8@Eigen@@$04$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::GpuDevice,struct Eigen::QInt8>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UGpuDevice@Eigen@@UQInt8@2@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,class tensorflow::Variant,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@VVariant@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,class tensorflow::ResourceHandle,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@VResourceHandle@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,class tensorflow::tstring,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::tstring,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@Vtstring@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBVtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Vtstring@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class tensorflow::tstring>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Vtstring@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,bool,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<bool const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<bool,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@_N$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,bool>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,class std::complex<double>,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<double>,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@V?$complex@N@std@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@N@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class std::complex<double> >::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@V?$complex@N@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,class std::complex<float>,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float> const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<class std::complex<float>,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@V?$complex@M@std@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBV?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@V?$complex@M@std@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,class std::complex<float> >::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@V?$complex@M@std@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,double,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<double,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@N$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBN$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@N$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,double>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@N@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,struct tensorflow::bfloat16,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@Ubfloat16@tensorflow@@$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBUbfloat16@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@Ubfloat16@tensorflow@@$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,struct tensorflow::bfloat16>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@Ubfloat16@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,signed char,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<signed char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<signed char,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@C$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBC$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@C$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,signed char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@C@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,unsigned char,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned char const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned char,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@E$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBE$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@E$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned char>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@E@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,short,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<short,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@F$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBF$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@F$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,short>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@F@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,unsigned short,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<unsigned short const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<unsigned short,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@G$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CBG$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@G$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,unsigned short>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@G@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibdepth_space_ops.lo(depthtospace_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DepthToSpaceOpFunctor<struct Eigen::GpuDevice,__int64,1>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<__int64 const ,4,1,__int64>,16,struct Eigen::MakePointer>,int,class Eigen::TensorMap<class Eigen::Tensor<__int64,4,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DepthToSpaceOpFunctor@UGpuDevice@Eigen@@_J$00@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@$$CB_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@HV?$TensorMap@V?$Tensor@_J$03$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::DepthToSpaceOp<struct Eigen::ThreadPoolDevice,__int64>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$DepthToSpaceOp@UThreadPoolDevice@Eigen@@_J@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibpack_op.lo(pack_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<class tensorflow::Variant>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@VVariant@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::PackOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$PackOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibtensor_array_ops.lo(tensor_array_ops.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<class tensorflow::Variant>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@VVariant@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nliblist_kernels.lo(list_kernels.o) : error LNK2001: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<class tensorflow::Variant>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@VVariant@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@VVariant@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z)\r\nlibpack_op.lo(pack_op.o) : error LNK2019: unresolved external symbol \"void __cdecl tensorflow::ConcatGPU<class tensorflow::ResourceHandle>(class tensorflow::OpKernelContext *,class std::vector<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,2,1,__int64>,16,struct Eigen::MakePointer> > >,class std::allocator<class std::unique_ptr<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,2,1,__int64>,16,struct Eigen::MakePointer>,struct std::default_delete<class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle const ,2,1,__int64>,16,struct Eigen::MakePointer> > > > > const &,class tensorflow::Tensor *,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::ResourceHandle,2,1,__int64>,16,struct Eigen::MakePointer> *)\" (??$ConcatGPU@VResourceHandle@tensorflow@@@tensorflow@@YAXPEAVOpKernelContext@0@AEBV?$vector@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@V?$allocator@V?$unique_ptr@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@U?$default_delete@V?$TensorMap@V?$Tensor@$$CBVResourceHandle@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@std@@@std@@@2@@std@@PEAVTensor@0@PEAV?$TensorMap@V?$Tensor@VResourceHandle@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@@Z) referenced in function \"public: virtual void __cdecl tensorflow::PackOp<struct Eigen::ThreadPoolDevice,class tensorflow::ResourceHandle>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$PackOp@UThreadPoolDevice@Eigen@@VResourceHandle@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\nlibself_adjoint_eig_v2_op.lo(self_adjoint_eig_v2_op_gpu.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::UnaryFunctor<struct Eigen::GpuDevice,struct tensorflow::functor::conj<double> >::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<double,1,1,__int64>,16,struct Eigen::MakePointer>,class Eigen::TensorMap<class Eigen::Tensor<double const ,1,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$UnaryFunctor@UGpuDevice@Eigen@@U?$conj@N@functor@tensorflow@@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@N$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@V?$TensorMap@V?$Tensor@$$CBN$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SelfAdjointEigV2OpGpu<double>::ComputeAsync(class tensorflow::OpKernelContext *,class std::function<void __cdecl(void)>)\" (?ComputeAsync@?$SelfAdjointEigV2OpGpu@N@tensorflow@@UEAAXPEAVOpKernelContext@2@V?$function@$$A6AXXZ@std@@@Z)\r\nlibself_adjoint_eig_v2_op.lo(self_adjoint_eig_v2_op_gpu.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::UnaryFunctor<struct Eigen::GpuDevice,struct tensorflow::functor::conj<float> >::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<float,1,1,__int64>,16,struct Eigen::MakePointer>,class Eigen::TensorMap<class Eigen::Tensor<float const ,1,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$UnaryFunctor@UGpuDevice@Eigen@@U?$conj@M@functor@tensorflow@@@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@M$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@V?$TensorMap@V?$Tensor@$$CBM$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::SelfAdjointEigV2OpGpu<float>::ComputeAsync(class tensorflow::OpKernelContext *,class std::function<void __cdecl(void)>)\" (?ComputeAsync@?$SelfAdjointEigV2OpGpu@M@tensorflow@@UEAAXPEAVOpKernelContext@2@V?$function@$$A6AXXZ@std@@@Z)\r\nlibresource_variable_ops.lo(resource_variable_ops.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::DenseUpdate<struct Eigen::GpuDevice,class tensorflow::Variant,2>::operator()(struct Eigen::GpuDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,1,1,__int64>,16,struct Eigen::MakePointer>,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant const ,1,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$DenseUpdate@UGpuDevice@Eigen@@VVariant@tensorflow@@$01@functor@tensorflow@@QEAAXAEBUGpuDevice@Eigen@@V?$TensorMap@V?$Tensor@VVariant@tensorflow@@$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@V?$TensorMap@V?$Tensor@$$CBVVariant@tensorflow@@$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"class tensorflow::Status __cdecl tensorflow::EnsureSparseVariableAccess<struct Eigen::GpuDevice,class tensorflow::Variant>(class tensorflow::OpKernelContext *,class tensorflow::Var *)\" (??$EnsureSparseVariableAccess@UGpuDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@YA?AVStatus@0@PEAVOpKernelContext@0@PEAVVar@0@@Z)\r\nbazel-out\\x64_windows-dbg\\bin\\tensorflow\\compiler\\aot\\tfcompile : fatal error LNK1120: 56 unresolved externals\r\nTarget //tensorflow/compiler/aot:tfcompile failed to build\r\nINFO: Elapsed time: 18423.485s, Critical Path: 947.85s\r\nINFO: 2594 processes: 2594 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@YLuc,\r\nCould you please provide the exact sequence of commands / steps that you executed before running into the problem? Thanks!", "Sure. I edited the ticket as the following. Thanks for respond.\n\nI want to use the debug mode tfcompile to trace into one problem that\nXLA/AOT cannot compile my model. But I found problem in building the debug\nmode tfcompile file.\n\nI followed https://www.tensorflow.org/install/source (for windows part),\ninstalled msys2.With all nvidia drivers Cuda, cudnn installed.\n\nTensorflow 2.2.0 (latest on branch r2.2 on Apr 13, 2020),\nWin10 Pro,\nPython 3.6,\nBazel 2.0.0 (or 2.2.0)\nCuda 10.2 + Cudnn\nnvidia 1060\n\nVS2019 native console:\ncd d:/w/Git/tensorflowDebugpython configure.py\n`python configure.py`\n\nI can successfully build release mode tfcompile, using command\n`bazel build --config=opt --config=cuda\n//tensorflow/compiler/aot:tfcompile`\n\nBut never succeed for debug mode, with the following command under VS2019\nnative console:\n`bazel build -s --config=opt -c dbg --strip=never --config=cuda\n//tensorflow/compiler/aot:tfcompile`\n\nThere were some bugs before going to the following symbol link errors I had\nto fix, basically the return value empty error. I have no idea why they did\nnot affect the release version, because I am new to bazel.\n\nAnother bug I have to work around is I saw:\nC:/Users/yuefe/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmpdiauwif_/fill_functor.cu.compute_70.cudafe1.cpp:\nfatal error C1041: cannot open program database\n'C:\\users\\yuefe\\_bazel_yuefe\\p5ggus4g\\execroot\\org_tensorflow\\vc140.pdb';\nif multiple CL.EXE write to the same .PDB file, please use /FS\n\nI cannot find the root. My solution is to add --job 1 to the command line:\n`bazel build --jobs 1 -s --config=opt -c dbg --strip=never --config=cuda\n//tensorflow/compiler/aot:tfcompile`\n\nFinally, compiling can reach the following point:\n\nWhen failing, I observed:....\n\nThen will see what I posted.\n\nOn Tue, Apr 14, 2020 at 7:04 AM amahendrakar <notifications@github.com>\nwrote:\n\n> @YLuc <https://github.com/YLuc>,\n> Could you please provide the exact sequence of commands / steps that you\n> executed before running into the problem? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38507#issuecomment-613374759>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAHB3U5RQXUZYINC7GGQ7BLRMQ7KPANCNFSM4MHGYKKQ>\n> .\n>\n", "I do not think on windows we were ever able to build on debug mode.\r\nI certainly was never able to.\r\n@mrry @meteorcloudy may know more about why.", "Yeah, the problem is we never have the debug build running on CI, so it could easily be broken.\r\nBut it's definitely possible to fix the source code to make it work.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38507\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38507\">No</a>\n"]}, {"number": 38506, "title": "Keras Assertion Error for TPU Strategy ", "body": "I get the following assertion error at .fit() when trying to use TPU distributed strategy. \r\n\r\n```\r\nModel: \"my_model_final\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nmy_model (MyModel)           multiple                  2098016   \r\n_________________________________________________________________\r\ndense_2 (Dense)              multiple                  3120      \r\n_________________________________________________________________\r\nlayer_normalization_9 (Layer multiple                  96        \r\n_________________________________________________________________\r\ndense_3 (Dense)              multiple                  4160      \r\n_________________________________________________________________\r\noutput_2 (Dense)             multiple                  65        \r\n_________________________________________________________________\r\noutput_1 (EmbeddingSimilarit multiple                  32000     \r\n=================================================================\r\nTotal params: 2,137,457\r\nTrainable params: 2,137,457\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\nTraceback (most recent call last):\r\n  File \"copy_train_lm.py\", line 91, in <module>\r\n    model.fit(x=gen(all_files,128), epochs=100, steps_per_epoch=100)#,\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 619, in fit\r\n    epochs=epochs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 2242, in _distribution_standardize_user_data\r\n    assert isinstance(x, dataset_ops.DatasetV2)\r\nAssertionError\r\n```\r\nThis is my code to reproduce the results:\r\n\r\n```\r\nimport numpy as np \r\nimport tensorflow as tf \r\nfrom keras_model import MyModelFinal\r\nfrom tensorflow import keras\r\ntf.compat.v1.disable_eager_execution()\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='rakshanda-agarwal')\r\ntf.config.experimental_connect_to_host(resolver.master())tf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\ndef gen(all_files, batch_size):\r\n    while True:\r\n        for file in all_files:\r\n            with np.load(file) as data:\r\n                for i in range(0,len(data[\"input_ids\"]),batch_size):\r\n\r\n                    input_ids = data[\"input_ids\"][i:(i+batch_size)] \r\n                    input_mask = data['input_mask'][i:(i+batch_size)]\r\n                    segment_ids = data[\"segment_ids\"][i:(i+batch_size)]\r\n                    masked_lm_positions = data[\"masked_lm_positions\"][i:(i+batch_size)]\r\n                    masked_lm_ids = data[\"masked_lm_ids\"][i:(i+batch_size)]\r\n                    masked_lm_weights = data[\"masked_lm_weights\"][i:(i+batch_size)]\r\n                    next_sentence_labels = data[\"next_sentence_labels\"][i:(i+batch_size)]\r\n\r\n                    # masked_lm_weights=tf.reshape(masked_lm_weights,[128*20])\r\n                    # masked_lm_ids=tf.reshape(masked_lm_ids, [128,20,1])\r\n                    yield ([input_ids, segment_ids, input_mask, masked_lm_positions],\r\n                        [[masked_lm_ids,masked_lm_weights], next_sentence_labels])\r\n                        # {'output_1':masked_lm_ids, 'output_2':next_sentence_labels}, \r\n                        # {'output_1':masked_lm_weights, 'output_2':np.ones(batch_size)})\r\n                        # [masked_lm_ids, next_sentence_labels],[masked_lm_weights,1])\r\n\r\n                        # [masked_lm_ids, next_sentence_labels],[masked_lm_weights,np.ones(batch_size)])\r\n\r\n\r\nall_files=[\"data1/train/1.npz\"]\r\n# val_files=[\"LM1/train/1.npz\"]\r\n\r\nbatch_size = 128\r\nout_filters = 64\r\nnum_layers = 4\r\n\r\ndef loss1(logits, y, vocab_size=32000):\r\n    print(y)\r\n    masked_lm_ids = y[0]\r\n    masked_lm_weights = y[1]\r\n    logits = tf.reshape(logits, [2560,32000])\r\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\r\n    masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\r\n    masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\r\n    one_hot_labels = tf.one_hot(masked_lm_ids, depth=vocab_size, dtype=tf.float32)\r\n    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\r\n    numerator = tf.reduce_sum(masked_lm_weights * per_example_loss)\r\n    denominator = tf.reduce_sum(masked_lm_weights) + 1e-5\r\n    loss=numerator / denominator\r\n    return loss\r\n\r\nwith strategy.scope():\r\n    model=MyModelFinal(out_filters=64, is_training=True, emb_size=48, \r\n        vocab_size=32000, max_seq_length=128, num_layers=4) \r\n\r\n    \r\n    with np.load(all_files[0]) as data:\r\n        for i in range(0,len(data[\"input_ids\"]),batch_size):\r\n\r\n            input_ids = data[\"input_ids\"][i:(i+batch_size)] \r\n            input_mask = data['input_mask'][i:(i+batch_size)]\r\n            segment_ids = data[\"segment_ids\"][i:(i+batch_size)]\r\n            masked_lm_positions = data[\"masked_lm_positions\"][i:(i+batch_size)]\r\n            # masked_lm_ids = data[\"masked_lm_ids\"][i:(i+batch_size)]\r\n            # masked_lm_weights = data[\"masked_lm_weights\"][i:(i+batch_size)]\r\n            # next_sentence_labels = data[\"next_sentence_labels\"][i:(i+batch_size)]\r\n\r\n            model([input_ids, segment_ids, input_mask, masked_lm_positions])\r\n            break\r\n\r\n\r\n    print(model.summary())\r\n\r\n    optimizer = keras.optimizers.Adam(lr=0.0002)\r\n    losses={'output_1':loss1,\r\n        'output_2':'binary_crossentropy'}\r\n\r\n    # lossWeights = {\"output_1\": 1.0, \"output_2\": 1.0}\r\n\r\n    model.compile(optimizer=optimizer, loss=losses)\r\n    # ,\r\n    #     sample_weight_mode={'output_1' : 'temporal', 'output_2':None})\r\n\r\nmodel.fit(x=gen(all_files,128), epochs=100, steps_per_epoch=100)#,\r\n    # validation_data=gen(val_files,128),\r\n    # \r\n    # validation_steps=100, validation_freq=1)\r\n```\r\n", "comments": ["@rakshanda22, Please provide the Tensorflow version that you are using. Thanks", "@gadagashwini I am using tensorflow 2.1.0 ", "@rakshanda22 Can you please share a standalone code to reproduce the error? I tried to make a [gist](https://colab.research.google.com/gist/jvishnuvardhan/d7a982e1fc59ca597b9c74ce8a0e5378/untitled88.ipynb), but it is throwing errors. Thanks!", "Could you try tf 2.2rc or tf nightly? There's a refactor model.fit() so it's somewhat hard to debug a 2.1 issue", "Keras and TPU Strategy will work best with eager execution enabled, as that will ensure the TF v2 datasets are used. Please enable eager execution, and file a new issue if the problem persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38506\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38506\">No</a>\n"]}, {"number": 38505, "title": "ImportError: DLL load failed: The specified module could not be found. Failed to load the native TensorFlow runtime.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version: 2\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N?A\r\n- CUDA/cuDNN version: CUDA 10.2,  cuDNN 7.6.5 \r\n- GPU model and memory: Nvidia Quadro M1200, 16 GB RAM\r\n\r\nI cannot import tensorflow\r\nThis is what I got:\r\nException has occurred: ImportError\r\nTraceback (most recent call last): File \"C:\\Users\\khiteliv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module> from tensorflow.python.pywrap_tensorflow_internal import * File \"C:\\Users\\khiteliv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module> _pywrap_tensorflow_internal = swig_import_helper() File \"C:\\Users\\khiteliv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"C:\\Users\\khiteliv\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 242, in load_module return load_dynamic(name, filename, file) File \"C:\\Users\\khiteliv\\AppData\\Local\\Continuum\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic return _load(spec) ImportError: DLL load failed: The specified module could not be found. Failed to load the native TensorFlow runtime. \r\n\r\n\r\nThank you for your help!", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "> From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\r\n> \r\n> * For TF-GPU - See point 1\r\n> * For TF-CPU - See point 2\r\n> \r\n> **1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\r\n> \r\n> _TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0._\r\n> \r\n> * If you have above configuration and using _**Windows**_ platform -\r\n>   \r\n>   * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\r\n>   * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\r\n> * If you have above configuration and using _**Ubuntu/Linux**_ platform -\r\n>   \r\n>   * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\r\n>   * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\r\n> * If error still persists then, apparently your CPU model does not support AVX instruction sets.\r\n>   \r\n>   * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\r\n> \r\n> **2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\r\n> \r\n> _TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets._\r\n> \r\n> Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\n> Apparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n> \r\n> * Try Google Colab to use TensorFlow.\r\n>   \r\n>   * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use`pip install` to install any other preferred TF version.\r\n>   * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\r\n>   * All you need is a good internet connection and you are all set.\r\n> * Try to build TF from sources by changing CPU optimization flags.\r\n> \r\n> _Please let us know if this helps._\r\n\r\nI updated my PATH variable - it didn't help.", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38505\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38505\">No</a>\n"]}, {"number": 38504, "title": "add fix file to examples", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38504) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 38503, "title": " AttributeError: 'tuple' object has no attribute 'shape' error while using Google colab", "body": "Based on research and understanding of the issue its looks to me as a bug as i tried different things suggested by other users for similar issues. but it doesn't resolve.\r\n\r\n------------------------\r\n\r\n### System information\r\nUsing google colab\r\naccess to the notebook: https://colab.research.google.com/drive/1sDg-5lDnUKYLQUbSkBiZsYK4ZBof3ZQZ#scrollTo=mopYecQ-eIJk&uniqifier=1\r\n\r\n\r\n\r\nYou can obtain the TensorFlow version with:\r\nUsing goolge colab\r\n\r\n### Describe the problem\r\nI am able to run the same code on a normal jupyter notebook on my uni server. but it throws me an error while trying to run it on google colab.\r\nI checked if there are any references to keras instaed of tensoflw.keras and corrected them. I also converted the datagenerator output to a tuple instead of list\r\n\r\n### Source code / logs\r\nWARNING:tensorflow:From <ipython-input-10-1ae13c8462de>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use Model.fit, which supports generators.\r\nEpoch 1/5\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-10-1ae13c8462de> in <module>()\r\n      7                   epochs=nb_epoch,\r\n      8                   steps_per_epoch=nb_train_samples,\r\n----> 9                   validation_data=gen_valid)\r\n\r\n12 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nAttributeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:505 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:477 train_step  **\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:386 update_state\r\n        self._build(y_pred, y_true)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:317 _build\r\n        self._metrics, y_true, y_pred)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1118 map_structure_up_to\r\n        **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1214 map_structure_with_tuple_paths_up_to\r\n        *flat_value_lists)]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1213 <listcomp>\r\n        results = [func(*args, **kwargs) for args in zip(flat_path_list,\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1116 <lambda>\r\n        lambda _, *values: func(*values),  # Discards the path arg.\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:416 _get_metric_objects\r\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:416 <listcomp>\r\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:437 _get_metric_object\r\n        y_t_rank = len(y_t.shape.as_list())\r\n\r\n    AttributeError: 'tuple' object has no attribute 'shape'\r\n", "comments": ["@mkpisk, can you please share full code to reproduce issue as i have not access to your colab file.", "# -*- coding: utf-8 -*-\r\n\"\"\"Lab2.ipynb\r\n\r\nAutomatically generated by Colaboratory.\r\n\r\nOriginal file is located at\r\n    https://colab.research.google.com/drive/1sDg-5lDnUKYLQUbSkBiZsYK4ZBof3ZQZ\r\n\"\"\"\r\n\r\nUSE_G_COLAB = False\r\nif USE_G_COLAB:\r\n    from google.colab import drive\r\n    drive.mount('/content/drive')\r\n\r\nif USE_G_COLAB:\r\n    root_dir = \"/content/drive/My Drive/CNN/\"\r\n    root_dir_to_test = \"/content/drive/My\\ Drive/CNN/\"\r\n    !ls $root_dir_to_test\r\n\r\nfrom google.colab import drive\r\ndrive.mount('/content/drive')\r\n\r\n# Import necessary packages for loading the dataset\r\n\r\nimport numpy as np  # Package for matrix operations, handling data\r\nnp.random.seed(2020)\r\nimport os\r\n# import cv2\r\nimport matplotlib.pyplot as plt  # Package for plotting\r\n#from PIL import Image\r\n# from tensorflow.keras.preprocessing.image import img_to_array\r\n# from tensorflow.keras.preprocessing.image import load_img\r\n# from tensorflow.keras.utils import to_categorical\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.utils.data_utils import Sequence\r\n\r\n\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n\r\n    def __init__(self,\r\n                 data_path,\r\n                 inputs,\r\n                 outputs,\r\n                 batch_size=32):\r\n\r\n        self.data_path = data_path\r\n        self.inputs = inputs\r\n        self.outputs = outputs\r\n        self.batch_size = batch_size\r\n\r\n        if data_path is None:\r\n            raise ValueError('The data path is not defined.')\r\n\r\n        if not os.path.isdir(data_path):\r\n            raise ValueError('The data path is incorrectly defined.')\r\n\r\n        self.file_idx = 0\r\n        self.file_list = [self.data_path + '/' + s\r\n                          for s in os.listdir(self.data_path)]\r\n\r\n        self.on_epoch_end()\r\n        with np.load(self.file_list[0]) as npzfile:\r\n            self.out_dims = []\r\n            self.in_dims = []\r\n            self.n_channels = 1\r\n\r\n            for i in range(len(self.inputs)):\r\n                im = npzfile[self.inputs[i]]\r\n                self.in_dims.append((self.batch_size,\r\n                                     *np.shape(im),\r\n                                     self.n_channels))\r\n\r\n            for i in range(len(self.outputs)):\r\n                im = npzfile[self.outputs[i]]\r\n                self.out_dims.append((self.batch_size,\r\n                                      *np.shape(im),\r\n                                      self.n_channels))\r\n\r\n    def __len__(self):\r\n        'Denotes the number of batches per epoch'\r\n        return int(np.floor((len(self.file_list)) / self.batch_size))\r\n\r\n    def __getitem__(self, index):\r\n        'Generate one batch of data'\r\n        # Generate indexes of the batch\r\n        indexes = self.indexes[index * self.batch_size:(index + 1) *\r\n                               self.batch_size]\r\n\r\n        # Find list of IDs\r\n        list_IDs_temp = [self.file_list[k] for k in indexes]\r\n\r\n        # Generate data\r\n        i, o = self.__data_generation(list_IDs_temp)\r\n\r\n        return i, o\r\n\r\n    def on_epoch_end(self):\r\n        'Updates indexes after each epoch'\r\n        self.indexes = np.arange(len(self.file_list))\r\n        np.random.shuffle(self.indexes)\r\n\r\n    #@threadsafe_generator\r\n    def __data_generation(self, temp_list):\r\n        'Generates data containing batch_size samples'\r\n        # X : (n_samples, *dim, n_channels)\r\n        # Initialization\r\n        inputs = []\r\n        outputs = []\r\n\r\n        for i in range(len(self.inputs)):\r\n            inputs.append(np.empty(self.in_dims[i]).astype(np.float32))\r\n\r\n        for i in range(self.outputs.__len__()):\r\n            outputs.append(np.empty(self.out_dims[i]).astype(np.float32))\r\n\r\n        for i, ID in enumerate(temp_list):\r\n            with np.load(ID) as npzfile:\r\n\r\n                for idx in range(len(self.inputs)):\r\n                    x = npzfile[self.inputs[idx]].astype(np.float32)\r\n                    x = x[..., np.newaxis]\r\n                    inputs[idx][i, ...] = x\r\n\r\n                for idx in range(len(self.outputs)):\r\n                    x = npzfile[self.outputs[idx]].astype(np.float32)\r\n                    x = x[..., np.newaxis]\r\n                    outputs[idx][i, ...] = x\r\n\r\n        return tuple(inputs), tuple(outputs)\r\n\r\n\r\ngen_dir = \"/content/drive/My Drive/CNN/data/\"\r\n# gen_dir = \"/import/software/3ra023vt20/brats/data/\"\r\n\r\n# Available arrays in data: 'flair', 't1', 't2', 't1ce', 'mask'\r\n# See the lab instructions for more info about the arrays\r\ninput_arrays = ['flair', 't1', 't1ce']\r\noutput_arrays = ['mask']\r\nbatch_size = 48\r\ngen_train = DataGenerator(data_path=gen_dir + 'training',\r\n                          inputs=input_arrays,\r\n                          outputs=output_arrays,\r\n                          batch_size=batch_size)\r\n\r\n# Look at some sample images\r\nimg_in, img_out = gen_train[np.random.randint(0, len(gen_train))]\r\nfor inp in range(np.shape(img_in)[0]):\r\n    plt.figure(figsize=(12, 5))\r\n    for i in range(4):\r\n        plt.subplot(1, 4, i + 1)\r\n        plt.imshow(img_in[inp][i, :, :, 0])\r\n        plt.title('Image size: ' + str(np.shape(img_in[inp][i, :, :, 0])))\r\n        plt.tight_layout()\r\n    plt.suptitle('Input for array: ' + gen_train.inputs[inp])\r\n    plt.show()\r\n\r\nplt.figure(figsize=(12, 4))\r\nfor outp in range(np.shape(img_out)[0]):\r\n    for i in range(4):\r\n        plt.subplot(1, 4, i + 1)\r\n        plt.imshow(img_out[outp][i, :, :, 0])\r\n        plt.title('Image size: ' + str(np.shape(img_out[outp][i, :, :, 0])))\r\n        plt.tight_layout()\r\n\r\n    plt.suptitle('Output for array: ' + gen_train.outputs[outp])\r\n# import cv2\r\nimport matplotlib.pyplot as plt  # Package for plotting\r\n#from PIL import Image\r\n# from tensorflow.keras.preprocessing.image import img_to_array\r\n# from tensorflow.keras.preprocessing.image import load_img\r\n# from tensorflow.keras.utils import to_categorical    plt.show()\r\n\r\ngen_dir = \"/content/drive/My Drive/CNN/data/\"\r\ninput_arrays = ['flair']\r\noutput_arrays = ['mask']\r\nbatch_size = 48\r\ngen_train = DataGenerator(data_path=gen_dir + 'training',\r\n                          inputs=input_arrays,\r\n                          outputs=output_arrays,\r\n                          batch_size=batch_size)\r\n\r\ngen_valid = DataGenerator(data_path=gen_dir + 'validating',\r\n                          inputs=input_arrays,\r\n                          outputs=output_arrays,\r\n                          batch_size=batch_size)\r\n\r\nprint(\"#ofFileSamples:\", len(gen_train))\r\nprint(\"#ofFileSamples:\", len(gen_valid))\r\n\r\nimport cv2\r\nimport matplotlib.pyplot as plt  # Package for plotting\r\nfrom PIL import Image\r\nfrom tensorflow.keras.preprocessing.image import img_to_array\r\nfrom tensorflow.keras.preprocessing.image import load_img\r\nfrom tensorflow.keras.utils import to_categorical\r\n\r\nimport tensorflow.keras as keras\r\n\r\nIMG_WIDTH = 128\r\nIMG_HEIGHT = 128\r\nIMG_CHANNELS = 1\r\n\r\n#Building the CNN model\r\n\r\ninputs = keras.layers.Input((IMG_WIDTH,IMG_HEIGHT,IMG_CHANNELS))\r\n\r\ns = keras.layers.Lambda(lambda x:x / 255)(inputs)\r\nc1 = keras.layers.Conv2D(16, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\r\nc1 = keras.layers.Dropout(0.1)(c1)\r\nc1 = keras.layers.Conv2D(16, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\r\np1 = keras.layers.MaxPool2D((2,2))(c1)\r\n\r\nc2 = keras.layers.Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\r\nc2 = keras.layers.Dropout(0.1)(c2)\r\nc2 = keras.layers.Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\r\np2 = keras.layers.MaxPool2D((2,2))(c2)\r\n\r\nc3 = keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\r\nc3 = keras.layers.Dropout(0.1)(c3)\r\nc3 = keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\r\np3 = keras.layers.MaxPool2D((2,2))(c3)\r\n\r\nc4 = keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\r\nc4 = keras.layers.Dropout(0.1)(c4)\r\nc4 = keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\r\np4 = keras.layers.MaxPool2D((2,2))(c4)\r\n\r\nc5 = keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\r\nc5 = keras.layers.Dropout(0.1)(c5)\r\nc5 = keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\r\n\r\nu6 = keras.layers.Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(c5)\r\nu6 = keras.layers.concatenate([u6, c4])\r\nc6 = keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\r\nc6 = keras.layers.Dropout(0.2)(c6)\r\nc6 = keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\r\n\r\nu7 = keras.layers.Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(c6)\r\nu7 = keras.layers.concatenate([u7, c3])\r\nc7 = keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\r\nc7 = keras.layers.Dropout(0.2)(c7)\r\nc7 = keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\r\n\r\nu8 = keras.layers.Conv2DTranspose(32, (2,2), strides=(2,2), padding='same')(c7)\r\nu8 = keras.layers.concatenate([u8, c2])\r\nc8 = keras.layers.Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\r\nc8 = keras.layers.Dropout(0.2)(c8)\r\nc8 = keras.layers.Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\r\n\r\nu9 = keras.layers.Conv2DTranspose(16, (2,2), strides=(2,2), padding='same')(c8)\r\nu9 = keras.layers.concatenate([u9, c1])\r\nc9 = keras.layers.Conv2D(16, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\r\nc9 = keras.layers.Dropout(0.2)(c9)\r\nc9 = keras.layers.Conv2D(16, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\r\n\r\noutputs = keras.layers.Conv2D(1, (1,1), activation='sigmoid')(c9)\r\n\r\nmodel = keras.Model(inputs=[inputs], outputs=[outputs])\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\n#checkpointer = tf.keras.callbacks.ModelCheckpoint('Model_Img_seg.h5', verbose=1, save_best_only=True)\r\n\r\ncallbacks = [keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\r\n            keras.callbacks.TensorBoard(log_dir='logs')]\r\n\r\nnb_train_samples = len(gen_train)\r\nnb_validation_samples = len(gen_train)\r\nnb_validation_samples\r\n\r\nnb_epoch = 5\r\nnb_train_samples = len(gen_train)\r\nnb_validation_samples = len(gen_train)\r\n\r\n\r\nresults=model.fit_generator(generator=gen_train,\r\n                  epochs=nb_epoch,\r\n                  steps_per_epoch=nb_train_samples,\r\n                  validation_data=gen_valid)\r\n\r\nidx = random.randint(0, len(X_train))\r\n\r\npreds_train = model.predict(X_train[:int(X_train.shape)])", "@mkpisk \r\n\r\nI am not able to access colab link you have provided. Request you to share the colab link or standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mkpisk \r\n\r\nAny update on this issue please. Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I am also facing the same issue. I am using autokeras in Google Colab.\r\n\r\nThis is the error:\r\nAttributeError: 'tuple' object has no attribute 'shape'\r\n\r\n@mkpisk was your issue resolved? If yes, how?", "no. its not resolved yet.", "I believe it is version issue on google colab. Same code worked for me on other notebook formats. Kaggle and Zure.", "Thanks for the fast reply.\r\n\r\nMy issue is still not resolved as :\r\n   1. In Azure Notebook, the version of Tensorflow being installed is 2.0.0-beta1 which is not needed.\r\n   \r\n   2. Kaggle Notebook installs the version 2.1.0 for Tensorflow and gives the following error when I \r\n   try to install autokeras:\r\n   ```\r\n   ERROR: Could not find a version that satisfies the requirement autokeras (from versions: none)\r\n   ERROR: No matching distribution found for autokeras\r\n   ```", "I dont know whether this would really solve the problem and/or if you are using something specific to `eager` execution, but the following things helped me solve the issue.\r\n\r\nBefore you build you model (preferably when you are import(ing) libraries)\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n```\r\n\r\nalso when you are compiling your model, use the following argument\r\n`experimental_run_tf_function=False`\r\n", "Same issue here. Google Colab. Tried default 2.x and 2.2.0rc2", "I faced the same issue, was fixed by switching to `tensorflow=2.1.0` and `keras=2.3.1`, also it did work fine with `tensorflow=2.0.0`\r\n\r\nIssue is in `tensorflow=2.2.0`", "> I faced the same issue, was fixed by switching to `tensorflow=2.1.0` and `keras=2.3.1`, also it did work fine with `tensorflow=2.0.0`\r\n> \r\n> Issue is in `tensorflow=2.2.0`\r\n\r\n`!pip install tensorflow=2.1.0` fixed it for me on colab! Thanks.", "Changing from metrics=['accuracy'] to metrics=[tf.keras.metrics.Accuracy()] fixed it for me.\r\n\r\nA bit more digging seems to point, in my case, at passing a list of tensors for y (the actual model is dynamically built by our application and often has multiple inputs and outputs). Here's a distilled replication (gloss over using accuracy with mse);\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.metrics import Accuracy\r\n\r\n# This make it work too.\r\n#tf.compat.v1.disable_eager_execution()\r\n\r\n# Simple model.\r\ni = Input((1,))\r\no = Dense(1,name='output_1')(i)\r\nmodel = Model([i],[o])\r\nmodel.summary()\r\n\r\n# Dummy training data.\r\nx = np.zeros((1))\r\ny = np.zeros((1))\r\n\r\n# Works: metric=string lookup, y=Tensor\r\nmodel.compile(loss={'output_1': 'mse'},optimizer='adam',metrics={'output_1': 'accuracy'})\r\nmodel.fit(x,y)\r\nprint(model.metrics)\r\n\r\n# Works: metric=metric instance, y=Tensor\r\nmodel.compile(loss={'output_1': 'mse'},optimizer='adam',metrics={'output_1': Accuracy()})\r\nmodel.fit([x],y)\r\nprint(model.metrics)\r\n\r\n# Works: metric=metric instance, y=List of Tensors\r\nmodel.compile(loss={'output_1': 'mse'},optimizer='adam',metrics={'output_1': Accuracy()})\r\nmodel.fit([x],[y])\r\nprint(model.metrics)\r\n\r\n# Errors: metric=string lookup, y=List of Tensors\r\nmodel.compile(loss={'output_1': 'mse'},optimizer='adam',metrics={'output_1': 'accuracy'})\r\nmodel.fit([x],[y])\r\nprint(model.metrics)", "> I dont know whether this would really solve the problem and/or if you are using something specific to `eager` execution, but the following things helped me solve the issue.\r\n> \r\n> Before you build you model (preferably when you are import(ing) libraries)\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> tf.compat.v1.disable_eager_execution()\r\n> ```\r\n> \r\n> also when you are compiling your model, use the following argument\r\n> `experimental_run_tf_function=False`\r\n\r\nThat solved the issue for me! Thank you.\r\nI did not use anything specific for \"eager\" execution or anything else. A rather simple CNN with an input pipeline. Maybe it is enabled on google collab by default?", "This bug exists in TensorFlow 2.2.0. Is there any fix going on if the user does not want to switch back to TensorFlow 2.1.0?", "I am running on 2.2.0 and the fix with disabling the eager execution worked without switching back to 2.1.0", "I found the same error on my Keras code when I switched to a generator `keras.util.Sequence`. Disabling eager execution solves the problem but the network trains too slow with the generator. Data is pre-loaded on ram and every `__getitem__` step only grabs a slice from the numpy dataset, so I assume the generator is not the bottleneck and could be the eager execution disabled.\r\n\r\n**Edit:** sorry about the report, upgrading to TensroFlow 2.3.0 solved the error.", "With version transformers==4.10.3 it works but transformers==4.12.5 does not work"]}, {"number": 38502, "title": "tf-trt converted deep models int8 inference crashes", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  NO\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  \r\nVersion: tf-gpu.1-15.m45 (dlvm)\r\nBased on: Debian GNU/Linux 9.12 (stretch) (GNU/Linux 4.9.0-12-amd64 x86_64\\n)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  v1.15.2-1-g61ff2cb 1.15.2\r\n- Python version: - Bazel\r\nversion (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from\r\nsource): n/a\r\n- CUDA/cuDNN version: - GPU model and memory:\r\nCuda compilation tools, release 10.0, V10.0.130\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 6\r\n#define CUDNN_PATCHLEVEL 5\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am using tf-trt to convert some of the models to different precision mode.\r\nit works fine for fp32 and fp16  (gets converted and inference runs fine)\r\nbut int8  only get converted but on inference gives the following error\r\n2020-04-12 10:20:31.123734: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:812] Starting calibration thread on device 0, Calibration Resource @ 0x7fd990004ea0\r\n2020-04-12 10:20:31.123883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.5\r\n2020-04-12 10:20:31.124498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.5\r\n2020-04-12 10:20:58.770669: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:812] Starting calibration thread on device 0, Calibration Resource @ 0x7fd96c004e80\r\n2020-04-12 10:21:20.543550: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-04-12 10:29:21.108202: F tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:349] Check failed: t.TotalBytes() == device_tensor->TotalBytes() (1832000 vs. 21467376)\r\nAborted\r\n\r\n**Describe the expected behavior**\r\nmodel should run faster with int8 ?\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nn/a\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nalready attached\r\n", "comments": ["@prateekpatelsc,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "@amahendrakar : unfortunately the code is confidential and private to snap . don't think would be able to share those.\r\n", "@prateekpatelsc,\r\nIs it possible to share a minimal code sample to replicate the error you are facing? Thanks!", "```\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nconverter = trt.TrtGraphConverter(\r\n\tinput_graph_def=frozen_graph,\r\n        precision_mode=\"INT8\",\r\n       max_batch_size=2,\r\n\tnodes_blacklist=['logits', 'classes'])\r\nfrozen_graph = converter.convert()\r\n```\r\n\r\nThis is 4 lines of code that i use to take my original graph def and convert it to tf-trt optimized graph for precision mode int8 .\r\nthe conversion works fine. During inference it takes huge time and then the above stacktrace,\r\nNOTE : if i use precision mode fp32 or fp16 , things work completely fine. \r\ni have been running this tesla t4 and v100 gpu", "@prateekpatelsc,\r\nOn running the above code, I'm facing an error stating `NameError: name 'frozen_graph' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/432e0c5cda019795e3fd8e2a1ca6eaf8/38502.ipynb). Thanks!", "@amahendrakar : can you please look at the code ,\r\ni sent you a snippet of the code , not the full code right , you definitely need to provide frozen graph for some model", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38502\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38502\">No</a>\n"]}, {"number": 38501, "title": "tf.estimator graph_saver.restore() error when calling train_and_evalute method", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): **Google Colab**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: **no**\r\n- TensorFlow installed from (source or\r\nbinary): **Provided by Colab**\r\n- TensorFlow version (use command below): **2.1.0**\r\n- Python version: **Python 3.6**\r\n- Bazel version (if compiling from source): **not relevant**\r\n- GCC/Compiler version (if compiling from\r\nsource): **not relevant**\r\n- CUDA/cuDNN version: **CUDA V10.1.243**\r\n- GPU model and memory: **GPU deactivated**\r\n\r\n**Describe the current behavior**\r\nAfter evaluation, the `train_and_evaluate` method of the tf.estimator API throws an error when the Saver restores the checkpoints after exporting the model using tf.estimator.Exporter. It seems that the estimator calls the restore() method with too many arguments. See logs for details.\r\n\r\n**Describe the expected behavior**\r\nUntil a few days ago, my (unchanged) code worked without any errors. So I guess some change in tensorflow/tf.estimator within the last days caused the issue to come up. Back then, the estimator was able to train, evaluate, export and then continue training without problems.\r\n\r\n**Standalone code to reproduce the issue** \r\nHere is a link to a [Colab Notebook]( https://colab.research.google.com/drive/1KHqAMZGqngMvQntyTqnj7boa4OZPPoi1) to reproduce the issue\r\n\r\n**Other info / logs** \r\n```\r\nINFO:tensorflow:Finished evaluation at 2020-04-13-14:15:37\r\nINFO:tensorflow:Saving dict for global step 2821: accuracy = 1.0, global_step = 2821, loss = 0.2742786\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 2821: /content/simple/model.ckpt-2821\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-128-30df9d6a3d21> in <module>()\r\n----> 1 tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n23 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    471         '(with task id 0).  Given task id {}'.format(config.task_id))\r\n    472 \r\n--> 473   return executor.run()\r\n    474 \r\n    475 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py in run(self)\r\n    611         config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    612       logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 613       return self.run_local()\r\n    614 \r\n    615     # Distributed case.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py in run_local(self)\r\n    712         max_steps=self._train_spec.max_steps,\r\n    713         hooks=train_hooks,\r\n--> 714         saving_listeners=saving_listeners)\r\n    715 \r\n    716     eval_result = listener_for_eval.eval_result or _EvalResult(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    372 \r\n    373       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 374       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    375       logging.info('Loss for final step: %s.', loss)\r\n    376       return self\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1162       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1163     else:\r\n-> 1164       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1165 \r\n   1166   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1196       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n   1197                                              hooks, global_step_tensor,\r\n-> 1198                                              saving_listeners)\r\n   1199 \r\n   1200   def _train_model_distributed(self, input_fn, hooks, saving_listeners):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\r\n   1495       any_step_done = False\r\n   1496       while not mon_sess.should_stop():\r\n-> 1497         _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n   1498         any_step_done = True\r\n   1499     if not any_step_done:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    776         feed_dict=feed_dict,\r\n    777         options=options,\r\n--> 778         run_metadata=run_metadata)\r\n    779 \r\n    780   def run_step_fn(self, step_fn):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1281             feed_dict=feed_dict,\r\n   1282             options=options,\r\n-> 1283             run_metadata=run_metadata)\r\n   1284       except _PREEMPTION_ERRORS as e:\r\n   1285         logging.info(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1382         raise six.reraise(*original_exc_info)\r\n   1383       else:\r\n-> 1384         raise six.reraise(*original_exc_info)\r\n   1385 \r\n   1386 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1367   def run(self, *args, **kwargs):\r\n   1368     try:\r\n-> 1369       return self._sess.run(*args, **kwargs)\r\n   1370     except _PREEMPTION_ERRORS:\r\n   1371       raise\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1448               results=outputs[hook] if hook in outputs else None,\r\n   1449               options=options,\r\n-> 1450               run_metadata=run_metadata))\r\n   1451     self._should_stop = self._should_stop or run_context.stop_requested\r\n   1452 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py in after_run(self, run_context, run_values)\r\n    599       if self._timer.should_trigger_for_step(global_step):\r\n    600         self._timer.update_last_triggered_step(global_step)\r\n--> 601         if self._save(run_context.session, global_step):\r\n    602           run_context.request_stop()\r\n    603 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py in _save(self, session, step)\r\n    625     should_stop = False\r\n    626     for l in self._listeners:\r\n--> 627       if l.after_save(session, step):\r\n    628         logging.info(\r\n    629             \"A CheckpointSaverListener requested that training be stopped. \"\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py in after_save(***failed resolving arguments***)\r\n    517       return True\r\n    518     if self._timer.should_trigger_for_step(global_step_value):\r\n--> 519       self._evaluate(global_step_value)  # updates self.eval_result\r\n    520       if not self._continuous_eval_listener.after_eval(self.eval_result):\r\n    521         logging.info('Exiting evaluation, as requested by '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py in _evaluate(self, global_step_value)\r\n    537     self._timer.update_last_triggered_step(global_step_value)\r\n    538     self.eval_result, self.export_results = (\r\n--> 539         self._evaluator.evaluate_and_export())\r\n    540     if self.eval_result.status != _EvalStatus.EVALUATED:\r\n    541       #  This is unexpected; should never happen.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py in evaluate_and_export(self)\r\n    930           self._max_training_steps if self._max_training_steps else False)\r\n    931       export_results = self._export_eval_result(eval_result,\r\n--> 932                                                 is_the_final_export)\r\n    933 \r\n    934       if is_the_final_export:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py in _export_eval_result(self, eval_result, is_the_final_export)\r\n    963                 checkpoint_path=eval_result.checkpoint_path,\r\n    964                 eval_result=eval_result.metrics,\r\n--> 965                 is_the_final_export=is_the_final_export))\r\n    966       return export_results\r\n    967 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/exporter.py in export(self, estimator, export_path, checkpoint_path, eval_result, is_the_final_export)\r\n    466     export_result = self._saved_model_exporter.export(\r\n    467         estimator, export_path, checkpoint_path, eval_result,\r\n--> 468         is_the_final_export)\r\n    469 \r\n    470     self._garbage_collect_exports(export_path)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/exporter.py in export(***failed resolving arguments***)\r\n    118         assets_extra=self._assets_extra,\r\n    119         as_text=self._as_text,\r\n--> 120         checkpoint_path=checkpoint_path)\r\n    121 \r\n    122     return export_result\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in export_saved_model(self, export_dir_base, serving_input_receiver_fn, assets_extra, as_text, checkpoint_path, experimental_mode)\r\n    737         as_text=as_text,\r\n    738         checkpoint_path=checkpoint_path,\r\n--> 739         strip_default_attrs=True)\r\n    740 \r\n    741   def experimental_export_all_saved_models(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _export_all_saved_models(self, export_dir_base, input_receiver_fn_map, assets_extra, as_text, checkpoint_path, strip_default_attrs)\r\n    860             builder, input_receiver_fn_map, checkpoint_path,\r\n    861             save_variables, mode=ModeKeys.PREDICT,\r\n--> 862             strip_default_attrs=strip_default_attrs)\r\n    863         save_variables = False\r\n    864 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py in _add_meta_graph_for_mode(self, builder, input_receiver_fn_map, checkpoint_path, save_variables, mode, export_tags, check_variables, strip_default_attrs)\r\n    967         if check_variables:\r\n    968           try:\r\n--> 969             graph_saver.restore(session, checkpoint_path)\r\n    970           except errors.NotFoundError as e:\r\n    971             msg = ('Could not load all requested variables from checkpoint. '\r\n\r\nTypeError: restore() takes 2 positional arguments but 3 were given\r\n```\r\n", "comments": ["@alxwdm, I tried to replicate the issue but getting different error, please take a look at the [gist](https://colab.sandbox.google.com/gist/gadagashwini/b00e3863ed74ffc604f2f3598632532b/untitled503.ipynb) and help us to reproduce the issue. Thanks", "@gadagashwini: I had to add a call to `get_model = get_model_fn()` after `def get_model_fn(): [...]` (sorry, this piece of code is a little confusing). Now the error as described above should appear.\r\n\r\nIn the meantime, I have looked around and I have something interesting. The file `estimator.py` in Colab (`/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py`) is different from the file `estimator.py` currently found in Github. I made some changes to the Colab version in the directory above and the error disappeared. \r\n\r\n* First of all, `estimator.py` in Colab has some missing imports compared to the current version on Github:\r\n`import tensorflow as tf`\r\n`from tensorflow.python.training.tracking import graph_view`\r\n`from tensorflow.python.training.tracking import util as trackable_util`\r\n* And then, before the line that causes the error (976), I changed the code to:\r\n`        graph_saver = tf.compat.v1.train.Saver(\r\n            var_list=graph_view.ObjectGraphView(\r\n              estimator_spec.scaffold.saver).frozen_saveable_objects(),\r\n            sharded=True)`\r\n* and uncommented\r\n` #graph_saver = (estimator_spec.scaffold.saver or  tf.compat.v1.train.Saver(sharded=True))`\r\nThen there is no error as described above. In the \"original\" version in Github, there is an additional `if isinstance(estimator_spec.scaffold.saver, ...) ...`, but I left this part out when I fixed the error.\r\n\r\nIn order to re-import tensorflow_estimator, I restarted the Colab runtime and added an extra import command `import tensorflow_estimator`.", "@alxwdm \r\ncan you please share a simple stand alone code to replicate the above issue or if possible share a colab gist with the error for us to analyse.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38501\">No</a>\n"]}, {"number": 38500, "title": "BatchNormalization with renorm=True doesn't work with TPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Colab\r\n- **TensorFlow version (use command below)**: Colab\r\n- **Python version**: Colab\r\n- **Bazel version (if compiling from source)**: Colab\r\n- **GCC/Compiler version (if compiling from source)**: Colab\r\n- **CUDA/cuDNN version**: Colab\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nBatchNormalization with the argument renorm=True using TPUs in Colab produces an error. It seems to be a bug since the code below works with CPUs and GPUs.\r\n\r\n### Source code / logs\r\n\r\n# ----------------------- Full code to reproduce the error: -----------------------\r\nimport tensorflow as tf\r\n\r\n# TPU stuff\r\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_cluster(tpu)\r\ntf.tpu.experimental.initialize_tpu_system(tpu)\r\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n\r\n# Build the network\r\ndef build_model():\r\n  from tensorflow.keras import Model\r\n  from tensorflow.keras.layers import Dense, Input, BatchNormalization\r\n  inputs = Input(1)\r\n  x = inputs\r\n  x = Dense(1)(x)\r\n  x = BatchNormalization(renorm=True)(x)\r\n  x = Dense(1, 'relu')(x)\r\n  model = Model(inputs, x)\r\n  return model\r\n\r\nwith strategy.scope():\r\n  model = build_model()\r\n  model.compile(loss='mse', optimizer='adam')\r\n\r\n\r\n# --------------------------- Error log:\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-abb50f72a18a> in <module>()\r\n     21 \r\n     22 with strategy.scope():\r\n---> 23   model = build_model()\r\n     24   model.compile(loss='mse', optimizer='adam')\r\n\r\n12 frames\r\n<ipython-input-3-abb50f72a18a> in build_model()\r\n     14   x = inputs\r\n     15   x = Dense(1)(x)\r\n---> 16   x = BatchNormalization(renorm=True)(x)\r\n     17   x = Dense(1, 'relu')(x)\r\n     18   model = Model(inputs, x)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    920                     not base_layer_utils.is_in_eager_or_tf_function()):\r\n    921                   with auto_control_deps.AutomaticControlDependencies() as acd:\r\n--> 922                     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    923                     # Wrap Tensors in `outputs` in `tf.identity` to avoid\r\n    924                     # circular dependencies.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in call(self, inputs, training)\r\n    817       if self.renorm:\r\n    818         r, d, new_mean, new_variance = self._renorm_correction_and_moments(\r\n--> 819             new_mean, new_variance, training, inputs_size)\r\n    820         # When training, the normalized values (say, x) will be transformed as\r\n    821         # x * gamma + beta without renorm, and (x * r + d) * gamma + beta\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in _renorm_correction_and_moments(self, mean, variance, training, inputs_size)\r\n    676     # TODO(yuefengz): colocate the operations\r\n    677     update_new_mean = _update_renorm_variable(self.renorm_mean, mean,\r\n--> 678                                               inputs_size)\r\n    679     update_new_stddev = _update_renorm_variable(self.renorm_stddev, stddev,\r\n    680                                                 inputs_size)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in _update_renorm_variable(var, value, inputs_size)\r\n    672       def _fake_update():\r\n    673         return array_ops.identity(var)\r\n--> 674       return tf_utils.smart_cond(training, _do_update, _fake_update)\r\n    675 \r\n    676     # TODO(yuefengz): colocate the operations\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)\r\n     63         pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     64   return smart_module.smart_cond(\r\n---> 65       pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n     66 \r\n     67 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\r\n     57   else:\r\n     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\r\n---> 59                                  name=name)\r\n     60 \r\n     61 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in cond(pred, true_fn, false_fn, strict, name, fn1, fn2)\r\n   1175   if (util.EnableControlFlowV2(ops.get_default_graph()) and\r\n   1176       not context.executing_eagerly()):\r\n-> 1177     return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n   1178 \r\n   1179   # We needed to make true_fn/false_fn keyword arguments for\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in cond_v2(pred, true_fn, false_fn, name)\r\n     99         false_graph.external_captures,\r\n    100         building_gradient=False,\r\n--> 101         name=scope)\r\n    102 \r\n    103 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in _build_cond(pred, true_graph, false_graph, true_inputs, false_inputs, building_gradient, name)\r\n    219   \"\"\"\r\n    220   _make_indexed_slices_indices_types_match(_COND, [true_graph, false_graph])\r\n--> 221   _check_same_outputs(_COND, [true_graph, false_graph])\r\n    222 \r\n    223   # Add inputs to true_graph and false_graph to make them match. Note that\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in _check_same_outputs(op_type, graphs)\r\n    799     for b0_out, bn_out in zip(graphs[0].outputs, graphs[b].outputs):\r\n    800       if b0_out.dtype != bn_out.dtype:\r\n--> 801         error(b, \"%s and %s have different types\" % (b0_out, bn_out))\r\n    802 \r\n    803 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/cond_v2.py in error(branch_idx, error_detail)\r\n    777             b0_out=graphs[0].structured_outputs,\r\n    778             bn_out=graphs[branch_idx].structured_outputs,\r\n--> 779             detail=error_detail))\r\n    780 \r\n    781   for b in range(1, len(graphs)):\r\n\r\nTypeError: true_fn and false_fn arguments to tf.cond must have the same number, type, and overall structure of return values.\r\n\r\ntrue_fn output: Tensor(\"Identity_1:0\", dtype=bool)\r\nfalse_fn output: Tensor(\"Identity_1:0\", shape=(1,), dtype=float32)\r\n\r\nError details:\r\nTensor(\"Identity_1:0\", dtype=bool) and Tensor(\"Identity_1:0\", shape=(1,), dtype=float32) have different types\r\n\r\n\r\n\r\n", "comments": ["@vduarte \r\nplease share the tensor-flow version on which this issue was faced and simple stand alone code for us to replicate the  issue", "with reference to the error faced, please refer to[ link1](https://stackoverflow.com/questions/47537064/tensorflow-incompatible-return-types-from-tf-cond) [link2](https://github.com/tensorflow/tensor2tensor/issues/311) [link3](https://www.programcreek.com/python/example/90430/tensorflow.cond) [link4](https://github.com/tensorflow/tensorflow/issues/31531) , and let us know if it helps", "Thanks for the references, but they didn't actually help much.\r\n\r\nHere is a link to a colab with sample code illustrating the problem:\r\n\r\nhttps://colab.research.google.com/drive/1TsytWJiFwXZWoJQgZrLKXsRaO_S_u5mo", "i am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/d6d16410402cc86f97b1c380dcbdd726/38500.ipynb)", "Yeah, this is a known issue and is likely will be fixed in TF 2.3.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38500\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38500\">No</a>\n"]}, {"number": 38499, "title": "Added code to pass appropriate logs to on_test|train_end() methods", "body": "fixes #38498 .  \r\n@mihaimaruseac , please review.", "comments": ["@mihaimaruseac , I am not able to see the logs of which part is failed. Can you paste the stack trace here? Thanks!", "Please ignore the macospy2 test. The windows failure is at https://source.cloud.google.com/results/invocations/c496c74d-d468-4265-bab5-f2a95242dcd0/targets/%2F%2Ftensorflow%2Ftools%2Fci_build%2Fbuilds:gen_win_out/log", "@mihaimaruseac , I don't think the error in windows Bazel CPU is because of the code added in this PR. Please check once and give your comments.", "@mihaimaruseac , when I am trying to rebase this PR with tf master branch, it is giving conflict errors in files which are not even changed. Please consider this fact too.", "Triggered a new build.\r\n\r\nRegarding the rebase, how do you do it? Do you have 2 upstreams (your fork and tensorflow)? That would be the recommended way as it works in most cases:\r\n\r\n```console\r\n$ git clone ${your_fork_url}\r\n$ cd tensorflow\r\n$ git remote add upstream ${main_tensorflow_repo_url}\r\n$ # do work, git add, git commit, everything on a branch for your repo\r\n$ git push # -u optionally\r\n$ # now syncing from upstream\r\n$ git checkout master\r\n$ git pull --rebase upstream\r\n$ git push  # now master on your fork is synced with master upstream\r\n$ git checkout -  # switch back to the branch\r\n$ git rebase master  # rebase on master without any merge commits\r\n$ git push  # now the changes on your repo and rebased on master upstream\r\n```", "@mihaimaruseac , Thanks for the code snippet. I actually had only one upstream. You cleared my doubts on that. Thanks so much!"]}, {"number": 38498, "title": "Currently logs param is None for on_train_end() and on_test_end()", "body": "You can see the current implementation of `fit()` ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L950)) and `evaluate()` ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L1180)) methods that the logs passed to the methods `on_train_end()` and `on_test_end()` are None which is as per the documentation can be changed in future.  \r\nIn tensorflow/addons, There is addition of `TQDMProgressBar()` callback. And recently I have raised PR [#1649](https://github.com/tensorflow/addons/pull/1649) to add code to make progress bar work in case of `evaluate()` too.  \r\nHere, we came across the problem that there are `logs` passed to `on_test_batch_end()` method to update the progress bar. But after the epoch is complete and when `on_test_end()` method is called, there are no `logs` passed to that. Because of this, there is no metrics results passed to the method. But in my opinion and also from @shun-lin's [#1649 (comment)](https://github.com/tensorflow/addons/pull/1649#issuecomment-612785521), it is good to pass logs which are output from the last call to `on_test_batch_end()` method. Currenly in tqdm callback, we are storing the `on_test_batch_end()` logs in class variable and using them in `on_test_end()`, which we think is temporary fix. \r\n\r\ncc @shun-lin, @gabrieldemarmiesse.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38498\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38498\">No</a>\n"]}, {"number": 38497, "title": "[tflite] fix missing dep for preprocess_coco_minival", "body": "add missing dep for\r\n```\r\n//tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival\r\n```\r\n\r\n`preprocess_coco_minival` uses `evaluation_stages`. `evaluation_stages` uses\r\n`preprocessing_steps` which is not generated.\r\n\r\nAdd the build rule and corresponding dependency so that\r\n```\r\nbazel run //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival -- \\\r\n  --images_folder=/path/to/val2014 \\\r\n  --instances_file=/path/to/instances_val2014.json \\\r\n  --whitelist_file=/path/to/minival_whitelist.txt \\\r\n  --output_folder=/path/to/output/folder\r\n```\r\nwill work again.", "comments": ["Hi can you sync to head and try again? I cannot reproduce the error at head", "I can reproduce the error with cf50b1f on Ubuntu 18.04 and macOS 10.15.x\r\nNote that this is a run time dependency problem.\r\n```\r\n> git checkout master\r\n> git pull\r\n> bazel clean --expunge\r\n> bazel run --config opt \\\r\n  //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival -- \\\r\n  --images_folder=/hack/freedom/mscoco/val2014 \\\r\n  --instances_file=/tmp/instances_val2014.json \\\r\n  --whitelist_file=/hack/freedom/mscoco/mscoco_minival_ids.txt \\\r\n  --output_folder=/hack/freedom/mscoco/processed\r\n\r\n...\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival up-to-date:\r\n  bazel-bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival\r\nINFO: Elapsed time: 91.072s, Critical Path: 22.86s\r\nINFO: 304 processes: 304 local.\r\nINFO: Build completed successfully, 313 total actions\r\nINFO: Running command line: bazel-bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival '--images_folder=/hack/freedom/mscoco/val2014' '--instances_file=/tmp/instances_val2014.json' '--whitelist_file=/hack/freedom/mscoco/mscoco_minival_ids.txt' '--output_folder=/hack/freedom/mscoco/prINFO: Build completed successfully, 313 total actions\r\nTraceback (most recent call last):\r\n  File \"/home/freedom/.cache/bazel/_bazel_freedom/6c0437fbbcac51299f8083154d82c985/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py\", line 39, in <module>\r\n    from tensorflow.lite.tools.evaluation.proto import evaluation_stages_pb2\r\n  File \"/home/freedom/.cache/bazel/_bazel_freedom/6c0437fbbcac51299f8083154d82c985/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/proto/evaluation_stages_pb2.py\", line 16, in <module>\r\n    from tensorflow.lite.tools.evaluation.proto import preprocessing_steps_pb2 as tensorflow_dot_lite_dot_tools_dot_evaluation_dot_proto_dot_preprocessing__steps__pb2\r\nImportError: cannot import name 'preprocessing_steps_pb2'\r\n```"]}, {"number": 38496, "title": "hard_swish(x) layer in TFLM app crash", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform: Windows 10\r\n- TensorFlow installed from pip\r\n- Tensorflow version 2.1.0\r\n- Target platform: Windows 10 (I debug this way)\r\n\r\n**Describe the problem**\r\nI converted mobilenet_v3to .tflite -> converted to .cc using xxd -i -> compiled app with cl.exe and got nullptr exception tflite::MicroInterpreter constructor.\r\nIf I replace hard_swish(x) layer with relu(x) - no error occurs\r\n```\r\ndef relu(x):\r\n    return layers.ReLU()(x)\r\ndef hard_sigmoid(x):\r\n    return layers.ReLU(6.)(x + 3.) * (1. / 6.)\r\ndef hard_swish(x):\r\n    return layers.Multiply()([layers.Activation(hard_sigmoid)(x), x])\r\n    # return relu(x)\r\n```\r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI use this mobilenet_v3.py:  \r\nhttps://drive.google.com/open?id=1FOj2p4mj-0VxWC-79uUlZExyrV9OHRJ2\r\n\r\nthis code to convert: \r\n```\r\ndef convertTFL(name, model):\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = quantizationDataGenerator\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n    quantModel = converter.convert()\r\n\r\n    with open(\"{}.tflite\".format(name), \"wb\") as f:\r\n        f.write(quantModel)\r\n\r\nimport mobilenet_v3\r\nmodel = mobilenet_v3.MobileNetV3Small(\r\n             input_shape=(IMAGE_SIDE, IMAGE_SIDE, 3),\r\n             alpha=1.0,\r\n             minimalistic=False,\r\n             include_top=None, weights='imagenet', pooling=None\r\n        )\r\nconvertTFL('mobilenetv3', model)\r\n```\r\n\r\n\r\nand this code for main.cc for TFLM app:\r\n```\r\n#include \"tensorflow/lite/micro/kernels/micro_ops.h\"\r\n#include \"tensorflow/lite/micro/micro_error_reporter.h\"\r\n#include \"tensorflow/lite/micro/micro_interpreter.h\"\r\n//#include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\r\n#include \"tensorflow/lite/micro/kernels/all_ops_resolver.h\"\r\n#include \"tensorflow/lite/schema/schema_generated.h\"\r\n#include \"tensorflow/lite/version.h\"\r\n\r\n#define IMAGE_SIDE 224\r\n#define IMAGE_SIZE ( 3 * IMAGE_SIDE * IMAGE_SIDE )\r\n#define NUM_TEST_IMAGES 1 // 10 // 100\r\n\r\nextern unsigned char modelBuffer[];\r\n\r\nnamespace {\r\n    constexpr int kTensorArenaSize = 5000 * 1024;\r\n    #pragma Bss(\".tensor_arena\")\r\n    static uint8_t tensor_arena[kTensorArenaSize];\r\n    #pragma Bss()\r\n}  // namespace\r\n\r\n\r\nint main(){\r\n     printf(\"entered main()\\n\");\r\n    static tflite::MicroErrorReporter micro_error_reporter;\r\n    tflite::ErrorReporter& error_reporter = micro_error_reporter;\r\n\r\n    const tflite::Model* model = tflite::GetModel(modelBuffer);\r\n    if (model->version() != TFLITE_SCHEMA_VERSION) {\r\n        printf(\"wrong version!!!\\n\");\r\n        return -1;\r\n    }\r\n    printf(\"got model\\n\");\r\n\r\n    static tflite::MicroMutableOpResolver micro_mutable_op_resolver;\r\n     printf(\"after  MicroMutableOpResolver()\\n\");\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(\r\n        tflite::BuiltinOperator_DEPTHWISE_CONV_2D,\r\n        tflite::ops::micro::Register_DEPTHWISE_CONV_2D(), 1, 3);\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_CONV_2D,\r\n                                        tflite::ops::micro::Register_CONV_2D(),\r\n                                        1, 3);\r\n                                        \r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_QUANTIZE,\r\n                                        tflite::ops::micro::Register_QUANTIZE());\r\n\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_DEQUANTIZE, tflite::ops::micro::Register_DEQUANTIZE(), 1, 2);\r\n\r\n                                    \r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_PAD, tflite::ops::micro::Register_PAD(), 1, 2 );\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_ADD, tflite::ops::micro::Register_ADD(), 1, 2 );\r\n\r\n\r\n printf(\"before MicroInterpreter\\n\");\r\n    // Build an interpreter to run the model with.\r\n    // NOLINTNEXTLINE(runtime-global-variables)\r\n    static tflite::MicroInterpreter interpreter = tflite::MicroInterpreter(\r\n        model, micro_mutable_op_resolver, tensor_arena, kTensorArenaSize,\r\n        &error_reporter);\r\n        printf(\"after MicroInterpreter\\n\");\r\n\r\n    // Allocate memory from the tensor_arena for the model's tensors.\r\n     printf(\"before AllocateTensors()\\n\");\r\n    TfLiteStatus allocate_status = interpreter.AllocateTensors();\r\n    if (allocate_status != kTfLiteOk) {\r\n        error_reporter.Report(\"AllocateTensors() failed\");\r\n        return -1;\r\n    }\r\n    printf(\"after AllocateTensors()\\n\");\r\n\r\n    TfLiteTensor* input = interpreter.input(0);\r\n    TfLiteTensor* output = interpreter.output(0);\r\n    return 0;\r\n}\r\n\r\n```\r\n", "comments": ["Hi @amahendrakar ! Have you had a look at this PR: https://github.com/tensorflow/tensorflow/pull/37641\r\nIt may help you.\r\n\r\nCheers!\r\nFredrik", "+1 for what Fredrik mentioned. This operator is not yet supported. Let's wait for The PR above to land.", "The associated PR is merged. Is this still an issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38496\">No</a>\n"]}, {"number": 38495, "title": "Outputs Nan of inf with auto mixed precision", "body": "I use a three-tier full connection network to train a DQN (reinforcement learning) agent. The code like this\r\n\r\n```\r\ndef observation_to_action(states):\r\n    # define policy neural network\r\n    W1 = tf.get_variable(\"W1\", [state_dim, 300], initializer=tf.random_normal_initializer())\r\n    b1 = tf.get_variable(\"b1\", [300], initializer=tf.constant_initializer(0))\r\n    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\r\n\r\n    W2 = tf.get_variable(\"W2\", [300, 300], initializer=tf.random_normal_initializer())\r\n    b2 = tf.get_variable(\"b2\", [300], initializer=tf.constant_initializer(0))\r\n    h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\r\n\r\n    W3 = tf.get_variable(\"W3\", [300, 300], initializer=tf.random_normal_initializer())\r\n    b3 = tf.get_variable(\"b3\", [300], initializer=tf.constant_initializer(0))\r\n    h3 = tf.nn.relu(tf.matmul(h2, W3) + b3)\r\n\r\n    Wn = tf.get_variable(\"Wn\", [300, num_actions], initializer=tf.random_normal_initializer())\r\n    bn = tf.get_variable(\"bn\", [num_actions], initializer=tf.constant_initializer(0))\r\n    q = tf.matmul(h3, Wn) + bn\r\n    return q\r\n```\r\n\r\nThen automatic mixed precision can be enabled by this \r\n\r\n```\r\nopt = tf.train.AdamOptimizer()\r\nopt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\r\ntrain_op = opt.minimize(loss)\r\n```\r\n\r\nThe log indicates that the mixed precision has been used\r\n\r\n```\r\n2020-04-13 09:56:57.471656: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1816] Running auto_mixed_precision graph optimizer\r\n2020-04-13 09:56:57.472372: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1772] Converted 16/44 nodes to float16 precision using 1 cast(s) to float16 (excluding Const and Variable casts)\r\n```\r\n\r\nBut the output of the network, q, is Nan or inf. Is this normal? I want to train a DQN agent, if the output Q-value is nan or inf, it can't choose a best action to act. As a result, it can't be trained. I think the output may overflow the range of FP16( $ 2^{-24} $ ~65504). How can I train a reinforcement learning agent with auto mixed precision. Besides this, I also try to use mix precision manually by following codes\r\n\r\n```\r\ndef observation_to_action(states):\r\n    # define policy neural network\r\n    W1 = tf.get_variable(\"W1\", [state_dim, 256], initializer=tf.random_normal_initializer())\r\n    b1 = tf.get_variable(\"b1\", [256], initializer=tf.constant_initializer(0))\r\n    h1 = tf.nn.relu(tf.cast(tf.matmul(tf.cast(states, tf.float16), tf.cast(W1, tf.float16)), tf.float32) + tf.cast(b1, tf.float32))\r\n\r\n    W2 = tf.get_variable(\"W2\", [256, 256], initializer=tf.random_normal_initializer())\r\n    b2 = tf.get_variable(\"b2\", [256], initializer=tf.constant_initializer(0))\r\n    h2 = tf.nn.relu(tf.cast(tf.matmul(tf.cast(h1, tf.float16), tf.cast(W2, tf.float16)), tf.float32) + tf.cast(b2, tf.float32))\r\n\r\n    W3 = tf.get_variable(\"W3\", [256, 256], initializer=tf.random_normal_initializer())\r\n    b3 = tf.get_variable(\"b3\", [256], initializer=tf.constant_initializer(0))\r\n    h3 = tf.nn.relu(tf.cast(tf.matmul(tf.cast(h2, tf.float16), tf.cast(W3, tf.float16)), tf.float32) + tf.cast(b3, tf.float32))\r\n\r\n\r\n    Wn = tf.get_variable(\"Wn\", [256, num_actions], initializer=tf.random_normal_initializer())\r\n    bn = tf.get_variable(\"bn\", [num_actions], initializer=tf.constant_initializer(0))\r\n    q = tf.cast(tf.matmul(tf.cast(h3, tf.float16), tf.cast(Wn, tf.float16)), tf.float32) + tf.cast(bn, tf.float32)\r\n\r\n    return q\r\n```\r\n\r\nBut it's outputs are Nan or inf too. Thank you very much.\r\n\r\n\r\n\r\n\r\n\r\nUbuntu 18.04.3 LTS x86_64\r\n\r\nTesla V100    Driver Version: 435.21       CUDA Version: 10.0", "comments": ["@SXKai, Can you share the complete standalone code to reproduce the issue and also provide the Tensorflow version. Thanks", "> @SXKai, Can you share the complete standalone code to reproduce the issue and also provide the Tensorflow version. Thanks\r\n\r\nThanks @gadagashwini  . The tensorflow version I used is v1.15. I used the code from [https://github.com/yukezhu/tensorflow-reinforce](url). And I replace run_dqn_cartpole.py like following\r\n```\r\nfrom __future__ import print_function\r\nfrom collections import deque\r\n\r\nfrom rl.neural_q_learner import NeuralQLearner\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport gym\r\n\r\nenv_name = 'CartPole-v0'\r\nenv = gym.make(env_name)\r\n\r\nsess = tf.Session()\r\noptimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.9)\r\noptimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(optimizer)\r\nwriter = tf.summary.FileWriter(\"/tmp/{}-experiment-1\".format(env_name))\r\n\r\nstate_dim   = env.observation_space.shape[0]\r\nnum_actions = env.action_space.n\r\n\r\ndef observation_to_action(states):\r\n  # define policy neural network\r\n    W1 = tf.get_variable(\"W1\", [state_dim, 300], initializer=tf.random_normal_initializer())\r\n    b1 = tf.get_variable(\"b1\", [300], initializer=tf.constant_initializer(0))\r\n    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\r\n\r\n    W2 = tf.get_variable(\"W2\", [300, 300], initializer=tf.random_normal_initializer())\r\n    b2 = tf.get_variable(\"b2\", [300], initializer=tf.constant_initializer(0))\r\n    h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\r\n\r\n    W3 = tf.get_variable(\"W3\", [300, 300], initializer=tf.random_normal_initializer())\r\n    b3 = tf.get_variable(\"b3\", [300], initializer=tf.constant_initializer(0))\r\n    h3 = tf.nn.relu(tf.matmul(h2, W3) + b3)\r\n\r\n    Wn = tf.get_variable(\"Wn\", [300, num_actions], initializer=tf.random_normal_initializer())\r\n    bn = tf.get_variable(\"bn\", [num_actions], initializer=tf.constant_initializer(0))\r\n    q = tf.matmul(h3, Wn) + bn\r\n  return q\r\n\r\nq_learner = NeuralQLearner(sess,\r\n                           optimizer,\r\n                           observation_to_action,\r\n                           state_dim,\r\n                           num_actions,\r\n                           summary_writer=writer)\r\n\r\nMAX_EPISODES = 10000\r\nMAX_STEPS    = 200\r\n\r\nepisode_history = deque(maxlen=100)\r\nfor i_episode in range(MAX_EPISODES):\r\n\r\n  # initialize\r\n  state = env.reset()\r\n  total_rewards = 0\r\n\r\n  for t in range(MAX_STEPS):\r\n    env.render()\r\n    action = q_learner.eGreedyAction(state[np.newaxis,:])\r\n    next_state, reward, done, _ = env.step(action)\r\n\r\n    total_rewards += reward\r\n    # reward = -10 if done else 0.1 # normalize reward\r\n    q_learner.storeExperience(state, action, reward, next_state, done)\r\n\r\n    q_learner.updateModel()\r\n    state = next_state\r\n\r\n    if done: break\r\n\r\n  episode_history.append(total_rewards)\r\n  mean_rewards = np.mean(episode_history)\r\n\r\n  print(\"Episode {}\".format(i_episode))\r\n  print(\"Finished after {} timesteps\".format(t+1))\r\n  print(\"Reward for this episode: {}\".format(total_rewards))\r\n  print(\"Average reward for last 100 episodes: {:.2f}\".format(mean_rewards))\r\n  if mean_rewards >= 195.0:\r\n    print(\"Environment {} solved after {} episodes\".format(env_name, i_episode+1))\r\n    break\r\n```\r\nIt can be ran by `python run_dqn_cartpole.py`.\r\n\r\nThis setting will result in Nan or inf. But I changed the initializer method of network weights ang bias as follows\r\n```\r\ndef observation_to_action(states):\r\n    # define policy neural network\r\n    stdv = 1. / math.sqrt(256.)\r\n    W1 = tf.get_variable(\"W1\", [state_dim, 256], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    b1 = tf.get_variable(\"b1\", [256], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    h1 = tf.nn.relu(tf.matmul(states, W1) + b1)\r\n    #h1 = tf.nn.relu(tf.cast(tf.matmul(tf.cast(states, tf.float16), tf.cast(W1, tf.float16)), tf.float32) + tf.cast(b1, tf.float32))\r\n\r\n    W2 = tf.get_variable(\"W2\", [256, 256], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    b2 = tf.get_variable(\"b2\", [256], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\r\n    #h2 = tf.nn.relu(tf.cast(tf.matmul(tf.cast(h1, tf.float16), tf.cast(W2, tf.float16)), tf.float32) + tf.cast(b2, tf.float32))\r\n\r\n    W3 = tf.get_variable(\"W3\", [256, 256], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    b3 = tf.get_variable(\"b3\", [256], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    h3 = tf.nn.relu(tf.matmul(h2, W3) + b3)\r\n    #h3 = tf.nn.relu(tf.cast(tf.matmul(tf.cast(h2, tf.float16), tf.cast(W3, tf.float16)), tf.float32) + tf.cast(b3, tf.float32))\r\n\r\n    W4 = tf.get_variable(\"W4\", [256, 256], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    b4 = tf.get_variable(\"b4\", [256], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    h4 = tf.nn.relu(tf.matmul(h3, W4) + b4)\r\n\r\n    W5 = tf.get_variable(\"W5\", [256, 256], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    b5 = tf.get_variable(\"b5\", [256], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    h5 = tf.nn.relu(tf.matmul(h4, W5) + b5)\r\n\r\n    Wn = tf.get_variable(\"Wn\", [256, num_actions], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    bn = tf.get_variable(\"bn\", [num_actions], initializer=tf.random_uniform_initializer(minval=-stdv, maxval=stdv))\r\n    q = tf.matmul(h5, Wn) + bn\r\n    #q2 = tf.cast(tf.matmul(tf.cast(h3, tf.float16), tf.cast(Wn, tf.float16)), tf.float32) + tf.cast(bn, tf.float32)\r\n    return q\r\n```\r\nThis can work and will not output Nan of inf. I think it maybe the weight is too big to overflow.", "@SXKai, I tried to reproduce the issue but received different error, Please take a look at [gist](https://colab.sandbox.google.com/gist/gadagashwini/ebd8b5922adbfa8092ae0a0d570f13e6/untitled506.ipynb) and provide more information to reproduce the issue. Thanks ", "Check out the loss scaler [TF](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer) and the [mixed precision guide](https://www.tensorflow.org/guide/keras/mixed_precision#loss_scaling) .\r\n", "@SXKai,\r\nplease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 38494, "title": "Coredump in tensorflow 1.12.0", "body": "```\r\nCorefile information as follow:\r\n#0  0x00007fb46bf905f7 in raise () from /lib64/libc.so.6\r\n#1  0x00007fb46bf91ce8 in abort () from /lib64/libc.so.6\r\n#2  0x00007fb46bfd0317 in __libc_message () from /lib64/libc.so.6\r\n#3  0x00007fb46bfd8023 in _int_free () from /lib64/libc.so.6\r\n#4  0x00007fb3dd81616b in std::_Function_base::_Base_manager<std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_manager(std::_Any_data&, std::_Any_data const&, std::_Manager_operation) () from /opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#5  0x00007fb3dd89254d in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from /opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#6  0x00007fb3dd891582 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /opt/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#7  0x00007fb3dcdaf220 in ?? () from /lib64/libstdc++.so.6\r\n#8  0x00007fb46ca2cdc5 in start_thread () from /lib64/libpthread.so.0\r\n#9  0x00007fb46c05129d in clone () from /lib64/libc.so.6\r\n```\r\n\r\n**System information** \r\n- Python 2.7\r\n- CentOS \r\n- Tensorflow 1.12.0\r\n- TensorFlow installed from: tensorflow-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl\r\n- GCC 4.8.5\r\n\r\nHas anyone ever been faced with same issue?\r\n", "comments": ["@zijiaozeng \r\n can you please share the code and error log faced", "> @zijiaozeng\r\n> can you please share the code and error log faced\r\n\r\nI can't find any error log in my background inference processes, it occurred by accident (very rare), there is a core_python_xxx file size 3 GB I can provide. ", "@zijiaozeng\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "Recommend switching to 1.15 or later. If you need to use 1.12, please make sure you use the latest patch available, since we have fixed several security vulnerabilities that could produce core dumps.", "> Recommend switching to 1.15 or later. If you need to use 1.12, please make sure you use the latest patch available, since we have fixed several security vulnerabilities that could produce core dumps.\r\n\r\nThank you very much.", "If switching has solved the issue, can we close this?\r\n\r\nIf it didn't, can't you post the new error log? Maybe even some code you're running so we can attempt to reproduce and fix?", "> If switching has solved the issue, can we close this?\r\n> \r\n> If it didn't, can't you post the new error log? Maybe even some code you're running so we can attempt to reproduce and fix?\r\n\r\nOkay, I will try it in the next few days,  if it doesn't work, I'll post detail information include code snippet when located the specific scene where the problem occurs.  However\uff0cno exceptions thrown when the program crash.", "@ziliangpeng \r\nplease confirm if we may move this to resolved status", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38494\">No</a>\n"]}, {"number": 38493, "title": "Update backend.py -> unique_object_name method's docstring section", "body": "\"unique_object_name\" methods' docstring example section typo is fixed, example section was using \"_unique_layer_name\" method name instead of \"unique_object_name\" method name", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38493) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38493) for more info**.\r\n\r\n@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38493) for more info**.\n\n<!-- ok -->", "> Thanks for the fix.\r\n\r\nHey, \r\nMy pleasure, thanks for the review."]}, {"number": 38492, "title": "depends on @local_config_cc//:cc-compiler-x64_windows in repository @local_config_cc which failed to fetch. no such package ", "body": "bazel build :hello_main\r\nINFO: Call stack for the definition of repository 'local_config_cc' which is a cc_autoconf (rule definition at C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl:143:15):\r\n - <builtin>\r\n - C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl:179:5\r\n - /DEFAULT.WORKSPACE.SUFFIX:317:1\r\nERROR: An error occurred during the fetch of repository 'local_config_cc':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n                _get_msvc_vars(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _get_msvc_vars\r\n                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nERROR: E:/tensorflow/02/BUILD:8:1: //:hello_main depends on @local_config_cc//:cc-compiler-x64_windows in repository @local_config_cc which failed to fetch. no such package '@local_config_cc//': Traceback (most recent call last):\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n                _get_msvc_vars(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _get_msvc_vars\r\n                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nERROR: Analysis of target '//:hello_main' failed; build aborted: no such package '@local_config_cc//': Traceback (most recent call last):  \r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nINFO: Elapsed time: 1.231s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\nPS E:\\TensorFlow\\02> bazel clean --expunge\r\nINFO: Starting clean.\r\nPS E:\\TensorFlow\\02> bazel build :hello_main\r\nStarting local Bazel server and connecting to it...\r\nINFO: Call stack for the definition of repository 'local_config_cc' which is a cc_autoconf (rule definition at C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl:143:15):\r\n - <builtin>\r\n - C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl:179:5\r\n - /DEFAULT.WORKSPACE.SUFFIX:317:1\r\nERROR: An error occurred during the fetch of repository 'local_config_cc':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n                _get_msvc_vars(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _get_msvc_vars\r\n                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nERROR: E:/tensorflow/02/BUILD:8:1: //:hello_main depends on @local_config_cc//:cc-compiler-x64_windows in repository @local_config_cc which failed to fetch. no such package '@local_config_cc//': Traceback (most recent call last):\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n                _get_msvc_vars(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _get_msvc_vars\r\n                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nERROR: Analysis of target '//:hello_main' failed; build aborted: no such package '@local_config_cc//': Traceback (most recent call last):  \r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120\r\n                configure_windows_toolchain(<1 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 690, in configure_windows_toolchain\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 503, in _get_msvc_vars\r\n                _find_missing_vc_tools(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, in _find_missing_vc_tools\r\n                find_msvc_tool(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 367, in find_msvc_tool\r\n                _get_vc_full_version(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 357, in _get_vc_full_version\r\n                _get_latest_subversion(<2 more arguments>)\r\n        File \"C:/users/administrator/_bazel_administrator/gtw4qbxk/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 336, in _get_latest_subversion\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2019/Professional/VC/Tools/MSVC (No such file or directory)\r\nINFO: Elapsed time: 7.744s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (12 packages loaded, 18 targets configured)\r\nPS E:\\TensorFlow\\02> bazel\r\n                                                           [bazel release 3.0.0]\r\nUsage: bazel <command> <options> ...\r\n\r\nAvailable commands:\r\n  analyze-profile     Analyzes build profile data.\r\n  aquery              Analyzes the given targets and queries the action graph.\r\n  build               Builds the specified targets.\r\n  canonicalize-flags  Canonicalizes a list of bazel options.\r\n  clean               Removes output files and optionally stops the server.\r\n  coverage            Generates code coverage report for specified test targets.\r\n  cquery              Loads, analyzes, and queries the specified targets w/ configurations.\r\n  dump                Dumps the internal state of the bazel server process.\r\n  fetch               Fetches external repositories that are prerequisites to the targets.\r\n  help                Prints help for commands, or the index.\r\n  license             Prints the license of this software.\r\n  mobile-install      Installs targets to mobile devices.\r\n  print_action        Prints the command line args for compiling a file.\r\n  query               Executes a dependency graph query.\r\n  run                 Runs the specified target.\r\n  shutdown            Stops the bazel server.\r\n  sync                Syncs all repositories specified in the workspace file\r\n  test                Builds and runs the specified test targets.\r\n  version             Prints version information for bazel.\r\n\r\nGetting more help:\r\n  bazel help <command>\r\n                   Prints help and options for <command>.\r\n  bazel help startup_options\r\n                   Options for the JVM hosting bazel.\r\n  bazel help target-syntax\r\n                   Explains the syntax for specifying targets.\r\n  bazel help info-keys\r\n                   Displays a list of keys used by the info command.\r\nPS E:\\TensorFlow\\02> bazel version\r\nBuild label: 3.0.0\r\nBuild target: bazel-out/x64_windows-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Apr 6 12:56:01 2020 (1586177761)\r\nBuild timestamp: 1586177761\r\nBuild timestamp as int: 1586177761\r\nPS E:\\TensorFlow\\02> ", "comments": ["@danedang \r\n\r\nWhich version of Tensorflow you are using. Request you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Provide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@danedang \r\n\r\nAny update on this issue please. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38492\">No</a>\n", "@ravikyram  2.0"]}, {"number": 38491, "title": "ERROR: Config value download_clang is not defined in any .rc file", "body": "tensorflow\uff1ar2.2\r\nbazel         \uff1a2.0.0\r\n\r\nERROR: Config value download_clang is not defined in any .rc file\r\n\r\nprint info\uff1a\r\n`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so  --crosstool_top=//external:android/crosstool  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  --cpu=armeabi-v7a\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/wushengqi/ai/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/wushengqi/ai/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /home/wushengqi/ai/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/wushengqi/anaconda3/bin/python --action_env PYTHON_LIB_PATH=/home/wushengqi/anaconda3/lib/python3.7/site-packages --python_path=/home/wushengqi/anaconda3/bin/python --config=xla --config=rocm --config=download_clang --action_env ANDROID_NDK_HOME=/home/wushengqi/android/android-ndk-r14b --action_env ANDROID_NDK_API_LEVEL=19 --action_env ANDROID_BUILD_TOOLS_VERSION=29.0.2 --action_env ANDROID_SDK_API_LEVEL=29 --action_env ANDROID_SDK_HOME=/home/wushengqi/android/Sdk --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /home/wushengqi/ai/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/wushengqi/ai/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:rocm in file /home/wushengqi/ai/tensorflow/.bazelrc: --crosstool_top=@local_config_rocm//crosstool:toolchain --define=using_rocm=true --define=using_rocm_hipcc=true --action_env TF_NEED_ROCM=1\r\nERROR: Config value download_clang is not defined in any .rc file\r\n`\r\n\r\nIf I'm using a lower version of bazel\uff081.2.1\uff09\uff1a\r\n`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so  --crosstool_top=//external:android/crosstool  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  --cpu=armeabi-v7a\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/wushengqi/ai/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nERROR: Unrecognized option: --experimental_repo_remote_exec\r\n`", "comments": ["I configured in./configure\uff1a\r\n`Do you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\nClang will not be downloaded.\r\n`\r\n\r\nThe above errors will not be reported again.\r\nI'm going to close this down", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38491\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38491\">No</a>\n", "@jvishnuvardhan  Please re-open this issue. I want to try the experimental feature and download clang as part of the build process since using my local LLVM installation I am getting a build error. However selecting `y` to the question:\r\n```\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: y\r\n```\r\nI am getting the error mentioned:\r\n```\r\n> bazel build --repo_env=CC=clang --config=monolithic -c opt //tensorflow/lite:libtensorflowlite.so --verbose_explanations --verbose_failures --explain=build.log\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=123\r\nINFO: Reading rc options for 'build' from /home/user/src/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/user/src/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /home/user/src/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/user/.pyenv/versions/core/bin/python3 --action_env PYTHON_LIB_PATH=/home/user/.pyenv/versions/core/lib/python3.8/site-packages --python_path=/home/user/.pyenv/versions/core/bin/python3 --config=xla --config=download_clang --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /home/user/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/user/src/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nERROR: Config value download_clang is not defined in any .rc file\r\n```\r\nI am trying to build TF 2.3.0 (`//tensorflow/lite:libtensorflowlite.so`) using Bazel 3.1.0 (I don't think this matters but I have also added flex:delegate to libtensorflowlite.so dependencies)", "@daravi, I've verified the following command works well with master branch. Could you try it?\r\n\r\n```\r\nbazel build --repo_env=CC=clang -c opt //tensorflow/lite:libtensorflowlite.so\r\n```", "@terryheo I cleared my bazel cache (`~/.cache/bazel`) then tried that and I am getting the same `Config value download_clang is not defined in any .rc file` error. More details:\r\n```\r\n$ ./configure \r\nYou have bazel 3.1.0 installed.\r\n\r\nPlease specify the location of python. [Default is /home/user/.pyenv/versions/core/bin/python3]: \r\n\r\nFound possible Python library paths:\r\n  /home/user/.pyenv/versions/core/lib/python3.8/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/user/.pyenv/versions/core/lib/python3.8/site-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: y\r\nClang will be downloaded and used to compile tensorflow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\nI am on CentOS 8 and have a local LLVM 10.0.1 installation. Let me know if there is something else you would want me to try.", "@daravi, the option is for TF build.\r\nIf you want to use clang for TFLite build, please use \"--repo_env=CC=clang\" option.\r\nIt'll use /usr/bin/clang for your build. (You can check it with \"-s\" option of Bazel)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38491\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38491\">No</a>\n", "in configuration, i select not to download clang. Then it worked for me\r\n./configure"]}, {"number": 38490, "title": "problem with keras.applications models if pooling != None", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform: Windows 10\r\n- TensorFlow installed from pip\r\n- Tensorflow version: 2.1.0\r\n- Target platform: windows 10 ( I debug this way)\r\n\r\n**Describe the problem**\r\nI want to convert following model:\r\n```\r\n        model = tf.keras.applications.mobilenet_v2.MobileNetV2(\r\n            input_shape=(IMAGE_SIDE, IMAGE_SIDE, 3), alpha=0.35,\r\n            include_top=False, weights='imagenet', pooling=None\r\n        )\r\n```\r\nIf I use If I use `pooling=None` - no problems \r\nIf I use `pooling='avg'` I got problems when compiling app with TFLM kernels:\r\nDidn't find op for builtin opcode 'MEAN' version '2'\r\nIf I use  `pooling='max'` I got problem with conversion: \r\nRuntimeError: Quantization not yet supported for op: REDUCE_MAX\r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI use this conversion code:\r\n```\r\ndef convertTFL(name, model):\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = quantizationDataGenerator\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n    quantModel = converter.convert()\r\n\r\n    with open(\"{}.tflite\".format(name), \"wb\") as f:\r\n        f.write(quantModel)\r\n```\r\nAfter that I got .cc file by using 'xxd -i' and compile following main.cc  with cl.exe \r\n```\r\n#include \"tensorflow/lite/micro/kernels/micro_ops.h\"\r\n#include \"tensorflow/lite/micro/micro_error_reporter.h\"\r\n#include \"tensorflow/lite/micro/micro_interpreter.h\"\r\n//#include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\r\n#include \"tensorflow/lite/micro/kernels/all_ops_resolver.h\"\r\n#include \"tensorflow/lite/schema/schema_generated.h\"\r\n#include \"tensorflow/lite/version.h\"\r\n\r\n#define IMAGE_SIDE 224\r\n#define IMAGE_SIZE ( 3 * IMAGE_SIDE * IMAGE_SIDE )\r\n#define NUM_TEST_IMAGES 1 // 10 // 100\r\n\r\nextern unsigned char modelBuffer[];\r\n\r\nnamespace {\r\n    constexpr int kTensorArenaSize = 5000 * 1024;\r\n    #pragma Bss(\".tensor_arena\")\r\n    static uint8_t tensor_arena[kTensorArenaSize];\r\n    #pragma Bss()\r\n}  // namespace\r\n\r\nint main(){\r\n     printf(\"entered main()\\n\");\r\n    static tflite::MicroErrorReporter micro_error_reporter;\r\n    tflite::ErrorReporter& error_reporter = micro_error_reporter;\r\n\r\n    const tflite::Model* model = tflite::GetModel(modelBuffer);\r\n    if (model->version() != TFLITE_SCHEMA_VERSION) {\r\n        printf(\"wrong version!!!\\n\");\r\n        return -1;\r\n    }\r\n    printf(\"got model\\n\");\r\n\r\n    static tflite::MicroMutableOpResolver micro_mutable_op_resolver;\r\n     printf(\"after  MicroMutableOpResolver()\\n\");\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(\r\n        tflite::BuiltinOperator_DEPTHWISE_CONV_2D,\r\n        tflite::ops::micro::Register_DEPTHWISE_CONV_2D(), 1, 3);\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_CONV_2D,\r\n                                        tflite::ops::micro::Register_CONV_2D(),\r\n                                        1, 3);\r\n                                        \r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_QUANTIZE,\r\n                                        tflite::ops::micro::Register_QUANTIZE());\r\n\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_DEQUANTIZE, tflite::ops::micro::Register_DEQUANTIZE(), 1, 2);\r\n\r\n                                    \r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_PAD, tflite::ops::micro::Register_PAD(), 1, 2 );\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_ADD, tflite::ops::micro::Register_ADD(), 1, 2 );\r\n\r\n\r\n printf(\"before MicroInterpreter\\n\");\r\n    // Build an interpreter to run the model with.\r\n    // NOLINTNEXTLINE(runtime-global-variables)\r\n    static tflite::MicroInterpreter interpreter = tflite::MicroInterpreter(\r\n        model, micro_mutable_op_resolver, tensor_arena, kTensorArenaSize,\r\n        &error_reporter);\r\n        printf(\"after MicroInterpreter\\n\");\r\n\r\n    // Allocate memory from the tensor_arena for the model's tensors.\r\n     printf(\"before AllocateTensors()\\n\");\r\n    TfLiteStatus allocate_status = interpreter.AllocateTensors();\r\n    if (allocate_status != kTfLiteOk) {\r\n        error_reporter.Report(\"AllocateTensors() failed\");\r\n        return -1;\r\n    }\r\n    printf(\"after AllocateTensors()\\n\");\r\n\r\n\r\n    TfLiteTensor* input = interpreter.input(0);\r\n    TfLiteTensor* output = interpreter.output(0);\r\n\r\n    return 0;\r\n}\r\n```\r\n", "comments": ["@yadonskov It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version  2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38490\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38490\">No</a>\n"]}, {"number": 38489, "title": "How to explain the result of tf.map_fn?", "body": "```\r\nitem_map = {223368: 2, 227300: 3}\r\ni =tf.reshape([223368,223368], [-1, ])\r\no1 = tf.map_fn(lambda x: item_map.get(x, 0),i)\r\no2 = tf.map_fn(lambda x: item_map.get(223368, 0), i)\r\no3 = tf.map_fn(lambda x:x, i)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(i.eval())  #\u8f93\u51fa[223368 223368]\r\n    print(o1.eval()) #\u8f93\u51fa[0 0]\r\n    print(o2.eval()) #\u8f93\u51fa[2 2]\r\n    print(o3.eval()) #\u8f93\u51fa[223368 223368]\r\n```\r\nwhy o1 is [0,0]?", "comments": ["@xueyuan1990 \r\nthe code shared is incomplete for us to replicate it, please share complete standalone code along with the tensorflow version.\r\n\r\nAs your question is related to understanding tf.map_fn, please refer to [this link](https://stackoverflow.com/questions/45905601/how-does-tf-map-fn-work/46154051) and let us know if it helps.\r\n[link2](https://stackoverflow.com/questions/46096767/how-to-explain-the-result-of-tf-map-fn) , \r\n[link3](https://blog.csdn.net/u012193416/article/details/86565633) ,\r\n[link4](https://www.tensorflow.org/api_docs/python/tf/map_fn)", "@xueyuan1990\r\nplease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 38488, "title": "Didn't find op for builtin opcode 'QUANTIZE' version '2'", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Windows 10\r\n- tensorflow install from pip\r\n- tensorflow:  2.1.0\r\n- traget platform : I run simply on windows\r\n\r\n**Describe the problem**\r\nI made app like one in examples and during call interpreter.AllocateTensors() I got this:\r\nDidn't   find op for builtin opcode 'QUANTIZE' version '2'\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI got this message for two models:\r\nNASNet-A\r\n```\r\nmodel = tf.keras.applications.nasnet.NASNetMobile(\r\n            input_shape=(224, 224, 3), \r\n            include_top=None, weights='imagenet', pooling=None\r\n        )\r\n```\r\nSquezeNetv1.1\r\nhttps://drive.google.com/open?id=1_A87NVmu_jRJ1ltSTR2iKffEgBGhargS\r\n( I shared heras h5 saved model)\r\n\r\nI convert with following code:\r\n```\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = quantizationDataGenerator\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n    quantModel = converter.convert()\r\n```\r\n\r\nget cc file with 'xxd -i ' and make application with following main.cpp.\r\nI use cl.exe compiler and run under windows 10.\r\n\r\n```\r\n#include \"tensorflow/lite/micro/kernels/micro_ops.h\"\r\n#include \"tensorflow/lite/micro/micro_error_reporter.h\"\r\n#include \"tensorflow/lite/micro/micro_interpreter.h\"\r\n//#include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\r\n#include \"tensorflow/lite/micro/kernels/all_ops_resolver.h\"\r\n#include \"tensorflow/lite/schema/schema_generated.h\"\r\n#include \"tensorflow/lite/version.h\"\r\n\r\n#define IMAGE_SIDE 224\r\n#define IMAGE_SIZE ( 3 * IMAGE_SIDE * IMAGE_SIDE )\r\n#define NUM_TEST_IMAGES 1 // 10 // 100\r\n\r\nextern unsigned char modelBuffer[];\r\n\r\nnamespace {\r\n    constexpr int kTensorArenaSize = 5000 * 1024;\r\n    #pragma Bss(\".tensor_arena\")\r\n    static uint8_t tensor_arena[kTensorArenaSize];\r\n    #pragma Bss()\r\n}  // namespace\r\n\r\nint main(){\r\n     printf(\"entered main()\\n\");\r\n    static tflite::MicroErrorReporter micro_error_reporter;\r\n    tflite::ErrorReporter& error_reporter = micro_error_reporter;\r\n\r\n    const tflite::Model* model = tflite::GetModel(modelBuffer);\r\n    if (model->version() != TFLITE_SCHEMA_VERSION) {\r\n        printf(\"wrong version!!!\\n\");\r\n        return -1;\r\n    }\r\n    printf(\"got model\\n\");\r\n\r\n    static tflite::MicroMutableOpResolver micro_mutable_op_resolver;\r\n     printf(\"after  MicroMutableOpResolver()\\n\");\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(\r\n        tflite::BuiltinOperator_DEPTHWISE_CONV_2D,\r\n        tflite::ops::micro::Register_DEPTHWISE_CONV_2D(), 1, 3);\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_CONV_2D,\r\n                                        tflite::ops::micro::Register_CONV_2D(),\r\n                                        1, 3);\r\n                                        \r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_QUANTIZE,\r\n                                        tflite::ops::micro::Register_QUANTIZE());\r\n\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_DEQUANTIZE, tflite::ops::micro::Register_DEQUANTIZE(), 1, 2);\r\n\r\n                                    \r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_PAD, tflite::ops::micro::Register_PAD(), 1, 2 );\r\n\r\n    micro_mutable_op_resolver.AddBuiltin(tflite::BuiltinOperator_ADD, tflite::ops::micro::Register_ADD(), 1, 2 );\r\n\r\n\r\n printf(\"before MicroInterpreter\\n\");\r\n    // Build an interpreter to run the model with.\r\n    // NOLINTNEXTLINE(runtime-global-variables)\r\n    static tflite::MicroInterpreter interpreter = tflite::MicroInterpreter(\r\n        model, micro_mutable_op_resolver, tensor_arena, kTensorArenaSize,\r\n        &error_reporter);\r\n        printf(\"after MicroInterpreter\\n\");\r\n\r\n    // Allocate memory from the tensor_arena for the model's tensors.\r\n     printf(\"before AllocateTensors()\\n\");\r\n    TfLiteStatus allocate_status = interpreter.AllocateTensors();\r\n    if (allocate_status != kTfLiteOk) {\r\n        error_reporter.Report(\"AllocateTensors() failed\");\r\n        return -1;\r\n    }\r\n    printf(\"after AllocateTensors()\\n\");\r\n\r\n    TfLiteTensor* input = interpreter.input(0);\r\n    TfLiteTensor* output = interpreter.output(0);\r\n\r\n    FILE * xfd = fopen( \"x-cifar-nasnet-100.cc\",\"rb\");\r\n    for (int i = 0; i < NUM_TEST_IMAGES; i++){\r\n        fread(input->data.f, sizeof(float), IMAGE_SIZE, xfd);\r\n        for (int i = 0; i < 100; i++) printf(\"%.3f \", input->data.f[i]);\r\n\r\n        if (kTfLiteOk != interpreter.Invoke()) {\r\n            error_reporter.Report(\"Invoke failed.\");\r\n            return -1;\r\n        } \r\n        printf(\"--------------\\n\");\r\n        for (int i = 0; i < 100; i++) printf(\"%.3f \", output->data.f[i]);\r\n        printf(\"==============\\n\");\r\n    }\r\n    fclose(xfd);\r\n\r\n    return 0;\r\n}\r\n\r\n\r\n", "comments": ["I got this issue recently on the latest nightly build(0.0.0-nightly) but when I switched to tfl lite version 2.2.0, it was gone for me.", "@yadonskov \r\nCould you please check on later versions and let us know, as per above comment this is resolved.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38488\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38488\">No</a>\n", "I got the same issue, have you already solved the problem?"]}, {"number": 38487, "title": "cannot install tensorflow 1.14.0 using pip", "body": "trying to install tensorflow==1.14.0 using pip but it is showing this error\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: 2.2.0rc1, 2.2.0rc2)                                                                                                 \r\nERROR: No matching distribution found for tensorflow==1.14", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38487\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38487\">No</a>\n"]}, {"number": 38486, "title": "tf/keras model.fit training accuracy doesn't match model.predict() accuracy on training data despite having only one training batch", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Mac OS Catalina\r\n- TensorFlow installed from (source or\r\nbinary): I tried both - The issue persists regardless\r\n- TensorFlow version (use command below):'2.1.0'\r\nkeras version '2.2.4-tf'\r\n- Python version: 3.7.7\r\n\r\n**Describe the current behavior**\r\nWhen I fine tune a pre-trained resnet using tensorflow.keras.applications.ResNet50, the accuracy shown in training does not match the accuracy of using predict on the same training data, despite the batch size being the size of the entire training set (i.e. only one batch per epoch).\r\n\r\nI did my due diligence researching this problem and apparently it has something to do with the batch normalization - but surely this is an error that needs to be fixed?\r\n\r\n**Describe the expected behavior**\r\nI expect the training accuracy during training after calling model.fit to be the same as the accuracy using model.predict on the same data, given that the batch size is the size of the entire training set.\r\n\r\n**Standalone code to reproduce the issue** \r\nHere's the data https://www.icloud.com/iclouddrive/0xyiKQ2D7CuucbbQ-M8NSPilQ#tf\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nfrom tensorflow.keras.layers import Conv2D, Input, MaxPool2D, add, Flatten, Dense\r\nimport tensorflow.keras as keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom seaborn import heatmap\r\ntf.keras.backend.clear_session()from tensorflow.keras.applications import ResNet50\r\nfrom tensorflow.keras.models import Sequential\r\nimport os\r\n\r\n\r\nos.chdir('/users/justusmulli/bayer')\r\nresnet_weights_path = 'resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\r\nos.chdir('/users/jm')\r\n\r\n\r\ndef classification_sampler(x, y, distribution = 'balanced'):\r\n    counts = np.unique(y, return_counts = True)\r\n    class_size = {}\r\n    s = np.argmin(counts[1])\r\n    if distribution == 'balanced':\r\n        for i in range(len(counts[0])):\r\n            class_size[counts[0][i]] = counts[1][s]\r\n    else:\r\n        for class_name in distribution:\r\n            if distribution[class_name]>1:\r\n                class_size[class_name] = distribution[class_name]\r\n            else:\r\n                class_size[class_name] = counts[1][s]*(distribution[class_name]/class_size[counts[0][s]])\r\n    x_sample = []\r\n    y_sample = []\r\n    for i in range(len(class_size)):\r\n        class_name = counts[0][i]\r\n        x_sample.extend(x[y==class_name][:class_size[class_name]])\r\n        y_sample.extend(y[y==class_name][:class_size[class_name]])\r\n    return np.array(x_sample), np.array(y_sample)\r\n\r\n\r\nx_train = np.load('x_train.npy')\r\ny_train = np.load('y_train.npy')\r\nx_val = np.load('x_test.npy')\r\ny_val = np.load('y_test.npy')\r\nx_train, y_train = classification_sampler(x_train, y_train)\r\nx_val, y_val = classification_sampler(x_val, y_val)\r\nencoder = OneHotEncoder()\r\nencoder.fit(y_train.reshape(-1,1))\r\ny_train = encoder.transform(y_train.reshape(-1,1)).toarray()\r\ny_val = encoder.transform(y_val.reshape(-1,1)).toarray()\r\n\r\nmodel = Sequential()\r\n\r\n\r\nmodel.add(ResNet50(include_top = False, pooling = 'avg'))\r\n\r\nmodel.add(Dense(512, activation = 'relu'))\r\nmodel.add(Dense(128, activation = 'relu'))\r\n\r\nmodel.add(Dense(3, activation = 'softmax'))\r\n\r\nmodel.layers[0].trainable = False\r\n\r\nfrom tensorflow.python.keras import optimizers\r\noptimizer = keras.optimizers.Adam(learning_rate = 0.00000005)\r\nmodel.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\r\nes = keras.callbacks.EarlyStopping(monitor='val_accuracy',\r\n                                  min_delta=0,\r\n                                  patience=100,\r\n                                  verbose=0, mode='max')\r\nhist = model.fit(x_train, y_train, validation_data = (x_val, y_val), batch_size = 267, epochs = 1000, callbacks = [es])\r\nb = hist.model.predict(x_train)\r\naccuracy_score(np.argmax(b, axis = 1), np.argmax(y_train, axis = 1))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nEpoch 101/1000\r\n267/267 [==============================] - 14s 52ms/sample - loss: 1.1680 - accuracy: 0.3708 - val_loss: 1.1356 - val_accuracy: 0.3333\r\n\r\nb = hist.model.predict(x_train)\r\naccuracy_score(np.argmax(b, axis = 1), np.argmax(y_train, axis = 1))\r\n0.3333333333333333", "comments": ["@justinmulli, One of reason is that regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time and also when you use `fit`, at each batch of the training data the weights are updated. The accuracy value returned by the `fit` method is not the mean of the accuracy of the final model, but the mean of the accuracy of all slightly different models used on each batch.\r\nOn the other hand, when you use `predict`, the same model is used on the whole dataset. And this model actually doesn't even appear in the accuracy of the fit method since even at the last batch of training, the loss computed is used to update the model's weights.\r\nReference -[https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss](https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss)", "> @justinmulli, One of reason is that regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time and also when you use `fit`, at each batch of the training data the weights are updated. The accuracy value returned by the `fit` method is not the mean of the accuracy of the final model, but the mean of the accuracy of all slightly different models used on each batch.\r\n> On the other hand, when you use `predict`, the same model is used on the whole dataset. And this model actually doesn't even appear in the accuracy of the fit method since even at the last batch of training, the loss computed is used to update the model's weights.\r\n> Reference -https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss\r\n\r\nOk so I shouldn't be getting the accuracy displayed in the last epoch, but I get 0.33333 accuracy using predict() no matter how many epochs I train it for, so something is wrong. the past 50 epochs did not have an accuracy of 0.3333333 - model.predict always predicts the same class and gets 0.333333 accuracy no matter when it is stopped or what the training accuracy is.", "@justinmulli There is not much difference between training, validation and prediction accuracy. Lets say if your training and validation accuracy are significantly different from prediction accuracy then there is a problem. \r\nLooks like there are many factors that can contribute to you facing this issue. Please go through this [issue](https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class). Also I would recommend you to post this question on stackoverflow where there is a wider community to respond. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @justinmulli There is not much difference between training, validation and prediction accuracy. Lets say if your training and validation accuracy are significantly different from prediction accuracy then there is a problem.\r\n> Looks like there are many factors that can contribute to you facing this issue. Please go through this [issue](https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class). Also I would recommend you to post this question on stackoverflow where there is a wider community to respond. Thanks!\r\n\r\nShouldn't the training accuracy printed during training (at least somewhat) reflect the accuracy on the training data using model.predict()?", "@justinmulli This is not a bug, its intended behaviour. I would recommend you to post this in stackoverflow as there is a wider community to answer. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it has been inactive for a long time. Please add additional comments or open a new issue. Thanks!"]}, {"number": 38485, "title": "How to obtain the activation value of neural network in Tensorflow 2.0?", "body": "I used a simple data set mnist to train the neural network, but I want to get the activation value (output) of the last layer of the neural network, how should I do it?\r\n\r\nThe structure of my neural network is as follows\r\n```\r\ninputs = tf.keras.Input(shape=(28, 28), name='mnist_input')\r\nh1 = tf.keras.layers.Flatten()(inputs)\r\nh2 = tf.keras.layers.Dense(128, activation='relu')(h1)\r\nh3 = tf.keras.layers.Dense(64, activation='relu')(h2)\r\noutputs = tf.keras.layers.Dense(10, activation='softmax', name='outputs')(h3)\r\nmodel = tf.keras.Model(inputs, outputs)\r\n```\r\n\r\nI try to use the callback function `on_epoch_end` to do this. Use `eval()` method on `model.outputs [0]` in the `on_epoch_end` method to get the activation value. But I got an error message: \r\n> {InvalidArgumentError} You must feed a value for placeholder tensor 'outputs/MatMul/ReadVariableOp/resource' with dtype resource\r\n\r\nIt seems that what is saved in model is only the framework of the neural network and does not fill in the data. How do I get the activation value after filling in the data?", "comments": ["@TeilyMa \r\nplease share the complete code for us to replicate the issue faced along with the tensorflow version.", "@Saduf2019 Thank you for your attention. I have solved the problem."]}, {"number": 38484, "title": "Fix reduce op ut failure: root rank=-1", "body": "I was running unit tests of nccl_ops.py with `bazel test tensorflow/python:nccl_ops_test` and got following errors. The failed test case was `SingleReduceTest.testSum`. \r\n\r\n![image](https://user-images.githubusercontent.com/4970790/79101916-383b4500-7d9c-11ea-864f-c5a9608ad58e.png)\r\n\r\nIt seems that `NcclManager::AddReduceRecv` at [nccl_manager.cc:L452](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/nccl/nccl_manager.cc#L452) does not set root rank as `NcclManager::AddBroadcastSend`.  I fixed it and `SingleReduceTest.testSum` passed.", "comments": []}]