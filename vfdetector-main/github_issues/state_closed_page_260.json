[{"number": 46637, "title": "tf.raw_ops.ImageProjectiveTransformV3 interpolates pixels that are outside image boundary (instead of using `fill_value`)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian testing\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.raw_ops.ImageProjectiveTransformV3` interpolates pixels that are outside image boundary (instead of using fill_value)\r\nIn the example below, the corner pixels are mapped from coordinates that lie outside the image. Hence, they must be set to fill_value (like, for example, scipy.ndimage.affine_transform and skimage.transform.AffineTransform). However, they are interpolated instead\r\n\r\n**Describe the expected behavior**\r\nPixels outside image boundaries should be set to `fill_value`. Namely, the corner pixels, in the example below must be zeros.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\n# Rotate 45 degrees\r\ntransform = tfa.image.angles_to_projective_transforms(np.pi/4, 3, 3)\r\n\r\nx = tf.ones((1, 3, 3, 1))\r\nres = tf.raw_ops.ImageProjectiveTransformV3(images=x,\r\n                                            transforms=transform,\r\n                                            output_shape=(3,3),\r\n                                            interpolation=\"BILINEAR\",\r\n                                            fill_value=0)\r\nnp.squeeze(res)\r\n\r\narray([[0.58578646, 1.        , 0.58578634],\r\n       [1.        , 1.        , 1.        ],\r\n       [0.58578646, 1.        , 0.58578634]], dtype=float32)\r\n```\r\n", "comments": ["Running the code with TF v2.3 throws an error stating `AttributeError: module 'tensorflow._api.v2.raw_ops' has no attribute 'ImageProjectiveTransformV3'`\r\n\r\nHowever, I was able to reproduce the issue with TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4be3af55b34eeecf51bfd8f426d4cfcb/46637.ipynb). Thanks!", "Tensorflow 2.3 and 2.4 have `ImageProjectiveTransformV2` which also must set pixels outside the image to zeros. The bug is present there as well", "Submitted a PR that fixes it: #46752 ", "@eli-osherovich  The PR you have submitted  was closed already. Please check PR for detailed information and seems the changes cannot be implemented due to several reasons as mentioned in PR. \r\n\r\nPlease go ahead and close the issue if you don't have any concern.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46637\">No</a>\n", "Warning says it all.\r\n```\r\n>>> import tensorflow_addons as tfa\r\nThe versions of TensorFlow you are currently using is 2.3.1 and is not supported. \r\nSome things might work, some things might not.\r\nIf you were to encounter a bug, do not file an issue.\r\nIf you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \r\n\r\n```"]}, {"number": 46634, "title": "tensorflow.greater returns a wrong bool", "body": "### System information\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5861094/tf_env.txt)\r\n\r\n\r\n-   I used the custom script shown below\r\n-   Windows 10 Family version 19041.746\r\n-   TensorFlow is installed from source\r\n-   v2.4.0-49-g85c8b2a817f 2.4.1\r\n-   Python 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32\r\n-   CUDA v11.1\r\n-   CPU Intel Core i7-3770 @ 3.40GHz\r\n-   GPU: GeForce GTX 1060 6GB\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nwhen I use the greater function from the package it gives me **True** when 1 > 1000** and **False when 1 > 10000**. I think the error is obvious enough\r\n\r\nEDIT: Issue Fixed, it was obviously the fact that I was using Int8. I guess Python made me unused to check for overflows. Sorry for that dumb issue ^-^\r\n\r\n### Source code / logs\r\n>>> import tensorflow as tf\r\ninitialization is successful\r\n>>>tf.greater(tf.convert_to_tensor(1, dtype=tf.int8), tf.convert_to_tensor(1000, dtype=tf.int8))\r\n<tf.Tensor: shape=(), dtype=bool, numpy=True>\r\n\r\n>>>tf.greater(tf.convert_to_tensor(1, dtype=tf.int8), tf.convert_to_tensor(10000, dtype=tf.int8))\r\n<tf.Tensor: shape=(), dtype=bool, numpy=False>\r\n", "comments": ["Because 1000 is overflow given dtype=int8, and it's converted to -24 in int8 representation. Therefore, 1 > -24.\r\nSimilarly, 10000 is overflow, and is 16 in int8 representation. Therefore, 1 < 16. ", "I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/30fcf688a66ad8dbb36c44f922e5d25f/untitled515.ipynb)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46634\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46634\">No</a>\n"]}, {"number": 46633, "title": "memory leak of tf2.3 with LSTM", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Enterprise Linux Server 7.9 (Maipo)\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\nmemory leak with training batch when using LSTM layer (no problem with dense `layer)`\r\n**Describe the expected behavior**\r\nno memory leak\r\n**Standalone code to reproduce the issue**\r\n      \r\nimport numpy as np\r\nimport tensorflow\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.callbacks import Callback\r\n\r\nimport resource\r\nclass MemoryCallback(Callback):\r\n    def on_epoch_end(self, epoch, log={}):\r\n        print('')\r\n        print('Memory usage: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\r\n        \r\nif __name__ == \"__main__\":\r\n        n_sample=1000\r\n        n_timestep =1000\r\n        num_channel = 1\r\n        batch_size=100\r\n        epochs=10\r\n        i_train_on_batch = False\r\n        \r\n        train = np.random.rand(n_sample, n_timestep, num_channel)\r\n        label = np.random.rand(n_sample, num_channel)\r\n        \r\n        x=layers.Input(shape=(n_timestep,num_channel))\r\n        z=  layers.LSTM(num_channel)(x)  \r\n        model = keras.Model(inputs=x, outputs=z)\r\n        \r\n        model.compile(optimizer=\"adam\", loss=\"mse\")\r\n        if i_train_on_batch:\r\n           for epoch in range(epochs):\r\n               for step in range(n_sample//batch_size):\r\n                   model.train_on_batch(train[step*batch_size:(step+1)*batch_size], label[step*batch_size:(step+1)*batch_size],)\r\n               print('Epoch ', epoch,'/', epochs, '  Memory usage: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\r\n        else:\r\n           model.fit(train, label, batch_size=batch_size, epochs=epochs, callbacks=[MemoryCallback()])\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n###################################################\r\ni_train_on_batch = False:\r\n###################################################\r\n\r\nbash-4.2$ python memory_leak_epoch.py \r\n2021-01-23 16:17:38.709232: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-23 16:17:43.397736: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2021-01-23 16:17:43.404166: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-01-23 16:17:43.404195: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (taurusi4181.taurus.hrsk.tu-dresden.de): /proc/driver/nvidia/version does not exist\r\n2021-01-23 16:17:43.406436: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nEpoch 1/10\r\n10/10 [==============================] - ETA: 0s - loss: 0.2690\r\nMemory usage:  424672\r\n10/10 [==============================] - 4s 364ms/step - loss: 0.2690\r\nEpoch 2/10\r\n10/10 [==============================] - ETA: 0s - loss: 0.2612\r\nMemory usage:  475888\r\n10/10 [==============================] - 4s 370ms/step - loss: 0.2612\r\nEpoch 3/10\r\n10/10 [==============================] - ETA: 0s - loss: 0.2533\r\nMemory usage:  527780\r\n10/10 [==============================] - 4s 374ms/step - loss: 0.2533\r\nEpoch 4/10\r\n10/10 [==============================] - ETA: 0s - loss: 0.2453\r\nMemory usage:  578732\r\n10/10 [==============================] - 4s 376ms/step - loss: 0.2453\r\nEpoch 5/10\r\n10/10 [==============================] - ETA: 0s - loss: 0.2373\r\nMemory usage:  627480\r\n10/10 [==============================] - 4s 381ms/step - loss: 0.2373\r\nEpoch 6/10\r\n10/10 [==============================] - ETA: 0s - loss: 0.2293\r\nMemory usage:  679224\r\n10/10 [==============================] - 4s 386ms/step - loss: 0.2293\r\nEpoch 7/10\r\n10/10 [==============================] - ETA: 0s - loss: 0.2212\r\nMemory usage:  728328\r\n10/10 [==============================] - 4s 385ms/step - loss: 0.2212\r\nEpoch 8/10\r\n10/10 [==============================] - ETA: 0s - loss: 0.2130\r\nMemory usage:  777960\r\n10/10 [==============================] - 4s 385ms/step - loss: 0.2130\r\nEpoch 9/10\r\n10/10 [==============================] - ETA: 0s - loss: 0.2050\r\nMemory usage:  827592\r\n10/10 [==============================] - 4s 390ms/step - loss: 0.2050\r\nEpoch 10/10\r\n10/10 [==============================] - ETA: 0s - loss: 0.1969\r\nMemory usage:  877752\r\n10/10 [==============================] - 4s 388ms/step - loss: 0.1969\r\nbash-4.2$ \r\n###################################################\r\ni_train_on_batch = True:\r\n###################################################\r\n\r\nbash-4.2$ python memory_leak_epoch.py \r\n2021-01-23 16:15:53.238289: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-23 16:15:58.148876: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2021-01-23 16:15:58.155607: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-01-23 16:15:58.155647: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (taurusi4181.taurus.hrsk.tu-dresden.de): /proc/driver/nvidia/version does not exist\r\n2021-01-23 16:15:58.158445: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nEpoch  0 / 10   Memory usage:  419684\r\nEpoch  1 / 10   Memory usage:  472220\r\nEpoch  2 / 10   Memory usage:  523804\r\nEpoch  3 / 10   Memory usage:  574228\r\nEpoch  4 / 10   Memory usage:  623068\r\nEpoch  5 / 10   Memory usage:  673756\r\nEpoch  6 / 10   Memory usage:  724180\r\nEpoch  7 / 10   Memory usage:  774076\r\nEpoch  8 / 10   Memory usage:  824500\r\nEpoch  9 / 10   Memory usage:  874660\r\n\r\n\r\n\r\n", "comments": ["@hlya23dd,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/2a8bbf5ed11817e2eae34066f37472e0/46633-2-3.ipynb) and [TF v2.4](https://colab.research.google.com/gist/amahendrakar/da77117dbfde3cbc3328a2da29a9ed5c/46633.ipynb).\r\n\r\nHowever on running the code with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/dbf2b2346cd5c41f3afa1c8ccde18f3c/46633-tf-nightly.ipynb), the memory increases for the first few epochs and then remains stable. Please check the linked gist for reference. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46632, "title": "object detection identifying even there is no image", "body": "hello guys i have been working in tensor flow object detection I have choose mobile ssd net model and trained the model but i am  getting a very false model it is detecting even when there is no image in the video stream.\r\n![image](https://user-images.githubusercontent.com/53184702/105580005-ca528e00-5daf-11eb-95ed-097ceeded753.png)\r\nEven it detects the same thing while a white blank wall is before it my accuracy is good.\r\n", "comments": ["@sidharth1805 \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46631, "title": "An error maybe about protobuf\uff1f", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.3.0-gpu\r\n- Python version: 3.6.0\r\n- Installed using virtualenv? pip? conda?:  pip\r\n- CUDA/cuDNN version: 10.1 / 7.6.4\r\n- GPU model and memory: 1050Ti \uff0c16G\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nMy tensorflow is installed in the new virtual environment of anaconda.And at first my protobuf version is 3.6.0\r\n\r\nToday, I tried to run a tensorflow program, but it reported this error.\r\n\r\n``` File \"E:\\Anaconda3.5\\envs\\ten2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1666, in _init_from_args\r\n    graph_mode=self._in_graph_mode)\r\n  File \"E:\\Anaconda3.5\\envs\\ten2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 243, in eager_safe_variable_handle\r\n    shape, dtype, shared_name, name, graph_mode, initial_value)\r\n  File \"E:\\Anaconda3.5\\envs\\ten2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 179, in _variable_handle_from_shape_and_dtype\r\n    handle_data.shape_and_type.append(\r\nAttributeError: 'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'append'\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nThen,i try to search some informations ,some ways suggest to upgrade the version of protobuf to the latest version, the 3.14.0.\r\nBut,after upgrading,there is new problems.It show import error.\r\n```\r\n  File \"E:\\Anaconda3.5\\envs\\ten2\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"E:\\Anaconda3.5\\envs\\ten2\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 48, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u7a0b\u5e8f\u3002\r\n```\r\n\r\nIt work well in tensorflow1.14.0 and cuda 10.0.What should i do if i still want to use tensorflow2.3.0\uff1f\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46631\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46631\">No</a>\n", "have you solve the problem?\r\nI have the same problem as you with the same configuration.\r\ncould you show your solution?", "> have you solve the problem?\r\n> I have the same problem as you with the same configuration.\r\n> could you show your solution?\r\n\r\nSorry\uff0cI can't solve that problem. I just changed other versions of python and tensorflow-gpu.\r\nNow\uff0cI use python-3.7 and tensorflow-gpu-2.1.0.\r\nIf it still doesn't work ,you can also try others.\r\nHope it helps you :)", "Please create a new issue as this issue is in closed status."]}, {"number": 46630, "title": "[MLIR] compile error", "body": "when I build the latest tensorflow mlir, commit id c30ad4ae0ca1a6240ec8619d5027a272394726a5, \r\nwith \r\n`bazel build --override_repository=llvm-project=$LLVM_SRC -c opt tensorflow/compiler/mlir:tf-opt` or even \r\n`bazel build -c opt tensorflow/compiler/mlir:tf-opt`\r\nI always get the message as below :\r\n`tensorflow/tensorflow/compiler/mlir/xla/BUILD:176:11: C++ compilation of rule '//tensorflow/compiler/mlir/xla:mhlo_to_lhlo_with_xla' failed (Exit 1): gcc failed: error executing command /path/to/gcc650/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' ... (remaining 320 argument(s) skipped)                                                                                                                                                                                                                                    tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc: In instantiation of 'mlir::LhloDialectEmitter::EmitGemm(const xla::HloCustomCallInstruction*)::<lambda(auto:10)> [with auto:10 = mlir::lmhlo_gpu::GEMMOp]':         tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:670:38:   required from here                                                                                                                                         tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:651:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.lhs_batch_dimensions()),                                                                                                                                                                             ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                     tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:652:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.rhs_batch_dimensions()),                                                                                                                                                                             ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                     tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:653:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.lhs_contracting_dimensions()),                                                                                                                                                                       ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                               tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:654:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.rhs_contracting_dimensions()),                                                                                                                                                                       ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                               tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc: In instantiation of 'mlir::LhloDialectEmitter::EmitGemm(const xla::HloCustomCallInstruction*)::<lambda(auto:10)> [with auto:10 = mlir::lmhlo_gpu::GEMM_BiasOp]':    tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:678:43:   required from here                                                                                                                                         tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:651:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.lhs_batch_dimensions()),                                                                                                                                                                             ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                     tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:652:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.rhs_batch_dimensions()),                                                                                                                                                                             ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                     tensorflow/compiler/mlir/xla/transforms/mhlo_to_lhlo_with_xla.cc:653:32: error: cannot call member function 'mlir::DenseIntElementsAttr mlir::LhloDialectEmitter::GetI64DenseElementsAttr(const T&) [with T = google::protobuf::RepeatedField<long int>]' without object                                                                                                                                                                                                             GetI64DenseElementsAttr(hlo_dims.lhs_contracting_dimensions()),`\r\n\r\nI have remove all the unrelated enviroment variables in the bash, but it still occurs. I have no idea, could anyone give some suggestions?\r\n\r\n**System information**\r\n- OS Platform and Distribution : CentOS Linux release 7.8.2003\r\n- GCC/Compiler version : 6.5.0\r\n- CUDA/cuDNN version: V10.0.130\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46630\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46630\">No</a>\n", "@zzpmiracle Hey have you solved this problem? I got the same error...", "I got the same error - have you resolved it? Related codes were introduced in bea9ecb9aad42dde0816a6e451320fdf5601f0a0, but I have no idea what's the problem.", "Finally, I turned off XLA to get a successful build...\r\n```sh\r\nexport TF_ENABLE_XLA=0\r\n```"]}, {"number": 46629, "title": "Linux C library for TensorFlow 2.4.1 contains macOS dylib files", "body": "**Describe the problem**\r\n\r\nI'm looking for the prebuilt TensorFlow C library for Linux (no GPU support).\r\n\r\nI see from the [documentation](https://www.tensorflow.org/install/lang_c) that the C library is available for version 2.4.0 although there is already TensorFlow 2.4.1 out.\r\n\r\nIf I download the 2.4.0 library - the contact is correct:\r\n\r\n```\r\nwget https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.4.0.tar.gz\r\ntar xvf libtensorflow-cpu-linux-x86_64-2.4.0.tar.gz\r\n[ ... ]\r\n./lib/\r\n./lib/libtensorflow.so.2.4.0\r\n./lib/libtensorflow_framework.so.2.4.0\r\n./lib/libtensorflow_framework.so\r\n./lib/libtensorflow_framework.so.2\r\n./lib/libtensorflow.so\r\n./lib/libtensorflow.so.2\r\n```\r\n\r\nInstead, if I download version 2.4.1 (I just change the URL), the content is for macOS (dylib instead of so):\r\n\r\n```\r\nwget https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.4.1.tar.gz\r\ntar xvf libtensorflow-cpu-linux-x86_64-2.4.1.tar.gz\r\n[ ... ]\r\n./lib/\r\n./lib/libtensorflow.2.4.1.dylib\r\n./lib/libtensorflow_framework.2.4.1.dylib\r\n./lib/libtensorflow_framework.dylib\r\n./lib/libtensorflow_framework.2.dylib\r\n./lib/libtensorflow.dylib\r\n./lib/libtensorflow.2.dylib\r\n```", "comments": ["I'm not sure how this happened but I'll take a look. I'm looking into overhauling the entire libtensorflow release so this should be resolved soon. Thanks for the issue and your patience. ", "This should be fixed, I think.\r\n\r\nPlease reopen if that is not the case", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46629\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46629\">No</a>\n"]}, {"number": 46628, "title": "Process finished with exit code 132 (interrupted by signal 4: SIGILL)", "body": "![image](https://user-images.githubusercontent.com/24789356/105575143-48a93300-5da4-11eb-9260-4846a809e2ea.png)\r\ni use tensorflow 2.4.1  in mac", "comments": ["Hello! \r\nI followed the PyCharm installation guide from the official Tensorflow Certification page. https://www.tensorflow.org/extras/cert/Setting_Up_TF_Developer_Certificate_Exam.pdf \r\n\r\nAfter installing all necessary packages, with the specified versions, I get the same problem as @tomuGo. \r\nMy version of tensorflow is 2.3.0 and I am running the script on my Apple Silicon.\r\n<img width=\"1187\" alt=\"Screen Shot 2021-01-23 at 7 48 26 PM\" src=\"https://user-images.githubusercontent.com/21693360/105620652-0a1b7480-5db4-11eb-85fb-59ffeb8a8b9e.png\">\r\n", "@gallogiulia @tomuGo I had same problem. After following the suggestion mentioned in a [stackoverflow](https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org) to install certificates,  everything worked as expected.\r\n\r\nBefore you take exam, install the certificate and run the your `mnist` code. Please let me know how it progresses.\r\nBest wishes for your certification. Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46628\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46628\">No</a>\n", "@gallogiulia https://github.com/apple/tensorflow_macos      look this,it worked", "@jvishnuvardhan \r\nVery thank you!, I decide to install certificates but I wonder what the certificates is. Is it AI TensorFlow Developer Professional Certificate?", "@kangdongju-peng This (`install certificates`) is not related to `TensorFlow Developer Professional Certificate`. You need `install certificates` to download some data from some websites (GitHub, webpages, etc). In the above case, the code need to download `mnist` data. Thanks!", "@jvishnuvardhan Really thank you! but I wonder about how to install certificates on ubuntu. If you give me answers about it, I will really thank you! ", "> stackoverflow\r\n\r\n I follow the \r\n\r\n> @gallogiulia @tomuGo I had same problem. After following the suggestion mentioned in a [stackoverflow](https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org) to install certificates, everything worked as expected.\r\n> \r\n> Before you take exam, install the certificate and run the your `mnist` code. Please let me know how it progresses.\r\n> Best wishes for your certification. Thanks!\r\n> \r\n> Please close the issue if this was resolved for you. Thanks!\r\n\r\nI follow the suggestion to install the certificate but still can't solve the problem. I wonder why. Thanks", "> > stackoverflow\r\n> \r\n> I follow the\r\n> \r\n> > @gallogiulia @tomuGo I had same problem. After following the suggestion mentioned in a [stackoverflow](https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org) to install certificates, everything worked as expected.\r\n> > Before you take exam, install the certificate and run the your `mnist` code. Please let me know how it progresses.\r\n> > Best wishes for your certification. Thanks!\r\n> > Please close the issue if this was resolved for you. Thanks!\r\n> \r\n> I follow the suggestion to install the certificate but still can't solve the problem. I wonder why. Thanks\r\n\r\nSame here I'm guessing it's because of the package we use."]}, {"number": 46627, "title": "Function contrastive_loss is not supported when converted", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):tensorflow2-GPU\r\n- Python version:3.6\r\n\r\n**Describe the current behavior**\r\nWhen I converted the trained hdf5 model to pb, the following function non-support occurred \uff1acontrastive_loss\r\n\r\n**Describe the expected behavior**\r\nThe hdf5 model should be successfully converted.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import print_function\r\nimport numpy as np\r\n\r\nimport random\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, Flatten, Dense, Dropout, Lambda\r\nfrom keras.optimizers import RMSprop\r\nfrom keras import backend as K\r\nimport time\r\nbegin_time = time.time()\r\nnum_classes = 10\r\nepochs = 20\r\n\r\n\r\ndef euclidean_distance(vects):\r\n    x, y = vects\r\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\r\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\r\n\r\n\r\ndef eucl_dist_output_shape(shapes):\r\n    shape1, shape2 = shapes\r\n    return (shape1[0], 1)\r\n\r\n\r\ndef contrastive_loss(y_true, y_pred):\r\n    '''Contrastive loss from Hadsell-et-al.'06\r\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\r\n    '''\r\n    margin = 1\r\n    square_pred = K.square(y_pred)\r\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\r\n    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\r\n\r\n\r\ndef create_pairs(x, digit_indices):\r\n    '''Positive and negative pair creation.\r\n    Alternates between positive and negative pairs.\r\n    '''\r\n    pairs = []\r\n    labels = []\r\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\r\n    for d in range(num_classes):\r\n        for i in range(n):\r\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\r\n            pairs += [[x[z1], x[z2]]]\r\n            inc = random.randrange(1, num_classes)\r\n            dn = (d + inc) % num_classes\r\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\r\n            pairs += [[x[z1], x[z2]]]\r\n            labels += [1, 0]\r\n    return np.array(pairs), np.array(labels)\r\n\r\n\r\ndef create_base_network(input_shape):\r\n    '''Base network to be shared (eq. to feature extraction).\r\n    '''\r\n    input = Input(shape=input_shape)\r\n    x = Flatten()(input)\r\n    x = Dense(128, activation='relu')(x)\r\n    x = Dropout(0.6)(x)\r\n    x = Dense(128, activation='relu')(x)\r\n    x = Dropout(0.1)(x)\r\n    x = Dense(128, activation='relu')(x)\r\n    return Model(input, x)\r\n\r\n\r\ndef compute_accuracy(y_true, y_pred):\r\n    '''Compute classification accuracy with a fixed threshold on distances.\r\n    '''\r\n    pred = y_pred.ravel() < 0.5\r\n    return np.mean(pred == y_true)\r\n\r\n\r\ndef accuracy(y_true, y_pred):\r\n    '''Compute classification accuracy with a fixed threshold on distances.\r\n    '''\r\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\r\n\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\ninput_shape = x_train.shape[1:]\r\n\r\n# create training+test positive and negative pairs\r\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\r\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\r\n\r\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\r\nte_pairs, te_y = create_pairs(x_test, digit_indices)\r\n\r\n# network definition\r\nbase_network = create_base_network(input_shape)\r\n\r\ninput_a = Input(shape=input_shape)\r\ninput_b = Input(shape=input_shape)\r\n\r\n# because we re-use the same instance `base_network`,\r\n# the weights of the network\r\n# will be shared across the two branches\r\nprocessed_a = base_network(input_a)\r\nprocessed_b = base_network(input_b)\r\n\r\ndistance = Lambda(euclidean_distance,\r\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\r\n\r\nmodel = Model([input_a, input_b], distance)\r\n\r\n# train\r\nrms = RMSprop()\r\nmodel.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\r\nmodel.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\r\n          batch_size=128,\r\n          epochs=epochs,\r\n          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\r\n# compute final accuracy on training and test sets\r\ny_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\r\ntr_acc = compute_accuracy(tr_y, y_pred)\r\ny_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\r\nte_acc = compute_accuracy(te_y, y_pred)\r\nstart_time = time.time()\r\nend_time = time.time()\r\ntime_data = end_time - start_time\r\nprint('Training Time costs', start_time - begin_time)\r\nprint('Tesing Time costs', time_data)\r\nprint('Test accuracy:' ,te_acc)\r\nmodel.save('model/151.h5')\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nUsing TensorFlow backend.\r\n2020-10-23 10:09:30.768594: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-10-23 10:09:32.233609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\r\npciBusID: 0000:05:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.77GiB\r\n2020-10-23 10:09:32.340284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325\r\npciBusID: 0000:09:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.77GiB\r\n2020-10-23 10:09:32.341621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1\r\n2020-10-23 10:09:33.016225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-23 10:09:33.016290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 \r\n2020-10-23 10:09:33.016303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y \r\n2020-10-23 10:09:33.016312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N \r\n2020-10-23 10:09:33.016476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10426 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2020-10-23 10:09:33.199564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10428 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\nE1023 10:09:33.461142 140655137916672 keras_to_tensorflow.py:94] Input file specified only holds the weights, and not the model definition. Save the model using model.save(filename.h5) which will contain the network architecture as well as its weights. If the model is saved using the model.save_weights(filename) function, either input_model_json or input_model_yaml flags should be set to to import the network architecture prior to loading the weights. \r\nCheck the keras documentation for more details (https://keras.io/getting-started/faq/)\r\nTraceback (most recent call last):\r\n  File \"keras_to_tensorflow.py\", line 181, in <module>\r\n    app.run(main)\r\n  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"keras_to_tensorflow.py\", line 127, in main\r\n    model = load_model(FLAGS.input_model, FLAGS.input_model_json, FLAGS.input_model_yaml)\r\n  File \"keras_to_tensorflow.py\", line 105, in load_model\r\n    raise wrong_file_err\r\n  File \"keras_to_tensorflow.py\", line 62, in load_model\r\n    model = keras.models.load_model(input_model_path)\r\n  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/engine/saving.py\", line 419, in load_model\r\n    model = _deserialize_model(f, custom_objects, compile)\r\n  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/engine/saving.py\", line 312, in _deserialize_model\r\n    sample_weight_mode=sample_weight_mode)\r\n  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/engine/training.py\", line 139, in compile\r\n    loss_function = losses.get(loss)\r\n  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/losses.py\", line 133, in get\r\n    return deserialize(identifier)\r\n  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/losses.py\", line 114, in deserialize\r\n    printable_module_name='loss function')\r\n  File \"/home/dl/anaconda3/envs/dong/lib/python3.6/site-packages/keras/utils/generic_utils.py\", line 165, in deserialize_keras_object\r\n    ':' + function_name)\r\nValueError: Unknown loss function:contrastive_loss\r\n\r\n```", "comments": ["@bugreporter450,\r\nOn running the given code snippet, I am facing an error stating `TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int64 of argument 'x'.`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1b85db5f9d99ec37711857bc8677f970/46627.ipynb).\r\n\r\nCould you please provide a minimal code snippet so that we can reproduce the issue on our end. \r\n\r\nAlso, instead of `from keras import xyz`, use `from tensorflow.keras import xyz` and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46627\">No</a>\n"]}, {"number": 46626, "title": "Operator Softsign and Softplus are not supported by the standard TensorFlow Lite runtime", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):tensorflow2-GPU\r\n- Python version:3.6\r\n\r\n**Describe the current behavior**\r\nWhen I converted the trained hdf5 model to tflite, the following operator non-support occurred \uff1aSoftsign\r\n\r\n**Describe the expected behavior**\r\nThe hdf5 model should be successfully converted to the format of tflite.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nbatch_size = 80\r\nepochs = 171\r\nnum_classes = 10\r\nimport os\r\nsave_dir = 'model'\r\nmodel_name = 'model.h5'\r\nimport keras as keras\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\r\nimg_rows, img_cols = x_train.shape[1], x_train.shape[2]\r\n\r\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\ninput_shape = (img_rows, img_cols, 1)\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\nimport keras as keras\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.ThresholdedReLU(theta=0.3514439122821289))\r\nmodel.add(keras.layers.LeakyReLU(alpha=0.4855740853866919))\r\nmodel.add(keras.layers.AveragePooling2D(pool_size = (1, 2), padding='same'))\r\n\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(num_classes, activation='softsign'))\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\n\r\nmodel_path = os.path.join(save_dir, model_name)\r\nmodel.save(model_path)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CAST, FULLY_CONNECTED, GREATER, MAXIMUM, MUL. Here is a list of operators for which you will need custom implementations: Softsign.\r\n```\r\n", "comments": ["Please consider enabling Softsign operator with TF select option. https://www.tensorflow.org/lite/guide/ops_select", "@bugreporter450 \r\nI ran the code on [colab](https://colab.research.google.com/gist/Saduf2019/df1b5da9008a5c9dbd3a3033bf869644/untitled507.ipynb) and do not face any errors, could you please refer to above comment [by abattery] and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46626\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46626\">No</a>\n", "Hi all, \r\n\r\nWe have the same problem converting our model to tensorflow lite during quantisation stage. Is Softplus being supported now? if not, what would be the solution to convert models with Softplus ops? \r\nThanks"]}, {"number": 46625, "title": "Operator Softplus is not supported by the standard TensorFlow Lite runtime", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): tensorflow2.1.0-GPU\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nWhen I converted the trained hdf5 model to tflite, the following operator non-support occurred \uff1aSoftplus\r\n\r\n**Describe the expected behavior**\r\nThe hdf5 model should be successfully converted to the format of tflite.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nbatch_size = 122\r\nepochs = 148\r\nnum_classes = 10\r\nimport os\r\nsave_dir = 'model'\r\nmodel_name = 'trained_model.h5'\r\nimport keras as keras\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\r\nimg_rows, img_cols = x_train.shape[1], x_train.shape[2]\r\n\r\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\ninput_shape = (img_rows, img_cols, 1)\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\nimport keras as keras\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.ThresholdedReLU(theta=0.3597445834106594))\r\nmodel.add(keras.layers.MaxPooling2D(pool_size = (1, 1), strides = (1, 1), padding = 'valid'))\r\n\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(num_classes, activation='softplus'))\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\n\r\nmodel_path = os.path.join(save_dir, model_name)\r\nmodel.save(model_path)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, FULLY_CONNECTED, GREATER, MAX_POOL_2D, MUL. Here is a list of operators for which you will need custom implementations: Softplus.\r\n```\r\n", "comments": ["duplicate issue of https://github.com/tensorflow/tensorflow/issues/46626", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46625\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46625\">No</a>\n"]}, {"number": 46624, "title": "NaN occurs when building with Dropout, Conv2DTranspose, LeakyReLU, ELU, PReLU, Flatten and Dense", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):tensorflow2-GPU\r\n- Python version:3.6\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI got the NaN loss when I tried to train my model\r\n\r\n**Describe the expected behavior**\r\nThe loss should be a number\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nbatch_size = 56\r\nepochs = 75\r\nnum_classes = 10\r\nimport os\r\nsave_dir = 'model'\r\nmodel_name = 'trained_model.h5'\r\nimport tensorflow.keras as keras\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\nimg_rows, img_cols = x_train.shape[1], x_train.shape[2]\r\n\r\nx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\nx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\ninput_shape = (img_rows, img_cols, 1)\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\nimport time\r\nbegin_time = time.time()\r\nimport tensorflow.keras as keras\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Dropout(rate = 0.21019177254509236,noise_shape=None, seed = 5))\r\nmodel.add(keras.layers.Conv2DTranspose(filters=7,kernel_size=(11, 11), strides=(11, 11), padding='same',activation='tanh',kernel_initializer='Constant'))\r\nmodel.add(keras.layers.LeakyReLU(alpha=0.28649457154482805))\r\nmodel.add(keras.layers.ELU(alpha=0.7631891438465761))\r\nmodel.add(keras.layers.PReLU(alpha_initializer='Constant'))\r\n\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(num_classes, activation='selu'))\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\nstart_time = time.time()\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nend_time = time.time()\r\ntime_data = end_time - start_time\r\nprint('Training Time costs', start_time - begin_time)\r\nprint('Tesing Time costs', time_data)\r\nprint('Test accuracy:', score[1])\r\nmodel_path = os.path.join(save_dir, model_name)\r\nmodel.save(model_path)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nEpoch 175\r\n10721072 [==============================] - 641s 598msstep - loss nan - accuracy 0.0987 - val_loss nan - val_accuracy 0.0980\r\nEpoch 275\r\n10721072 [==============================] - 651s 607msstep - loss nan - accuracy 0.0987 - val_loss nan - val_accuracy 0.0980\r\nEpoch 375\r\n10721072 [==============================] - 671s 626msstep - loss nan - accuracy 0.0987 - val_loss nan - val_accuracy 0.0980\r\nEpoch 475\r\n10721072 [==============================] - 684s 638msstep - loss nan - accuracy 0.0987 - val_loss nan - val_accuracy 0.0980\r\nEpoch 575\r\n 7601072 [====================.........] - ETA 307 - loss nan - accuracy 0.0989\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/e43abdc536cb6a95597f7cf99165be7f/46624.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/95743a9e7c13c31a4c68a66fd227cf98/46624-tf-nightly.ipynb).\r\n\r\nWhereas with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/e67f96992550fefa438da0eb01e76c24/46624-2-3.ipynb#scrollTo=kWLP9rDeOL9Q), the loss value is constant at 1.1921e-07. Please check the linked gist for reference. Thanks!\r\n", "Changing `kernel_initializer`(currently 'Constant') for `Conv2DTranspose` layer helps overcome the `NaN` values.\r\nI have tried with `glorot_uniform` and `LecunNormal` initializers.\r\n```python\r\nmodel.add(keras.layers.Conv2DTranspose(filters=7,kernel_size=(11, 11), strides=(11, 11), padding='same',activation='tanh',kernel_initializer='glorot_uniform'))\r\n```", "Consider changing the model's last layer to have a softmax activation (instead of selu) since this is a classification problem.\r\n`model.add(keras.layers.Dense(num_classes, activation='softmax'))`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46624\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46624\">No</a>\n"]}, {"number": 46623, "title": "UnimplementedError | InvalidArgumentError when running tf.nn.conv2d with data_format NCHW", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 2019\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAs declared in the specification, tf.nn.con2d should support both \"NHWC\" and \"NCHW\" format. However, per our experiment, running it with data in \"NCHW\" format will trigger some unexpected behaviors.\r\n\r\n*data_format \u2013 An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`. Specify the data format of the input and output data. With the default format \"NHWC\", the data is stored in the order of: [batch, height, width, channels]. Alternatively, the format could be \"NCHW\", the data storage order of: [batch, channels, height, width].*\r\n\r\n**Code to reproduce **\r\nx = np.random.randn(1, 3, 12, 12)\r\nx_tf = tf.convert_to_tensor(x, dtype=tf.float32)\r\n\r\nweights=np.ones([2,2,3,8])\r\nweights_tf = tf.convert_to_tensor(weights, dtype=tf.float32)\r\nstride=1\r\nsess = tf.compat.v1.Session()\r\ntf_conv2d = tf.nn.conv2d(x_tf,\r\n                         weights_tf,\r\n                         strides=[1, stride, stride, 1],\r\n                         padding=\"SAME\",data_format=\"NCHW\",use_cudnn_on_gpu=False)\r\nsess.run(tf.global_variables_initializer())\r\ntf_result = sess.run(tf_conv2d)\r\nprint(tf_result.shape)\r\n\r\nwhen set strides=[1,1,1,1]  trigger a bug:\r\n\"UnimplementedError (see above for traceback): Generic conv implementation only supports NHWC tensor format for now.\"\r\n\r\nwhen set strides=[1,2,2,1]  trigger a bug:\r\n\"InvalidArgumentError (see above for traceback): Current implementation does not yet support strides in the batch and depth dimensions.\"\r\n\r\n**Describe the expected behavior**\r\nNo exception during execution\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@bugpromax \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d941c83edf23fafc4a0e784904cbf23d/untitled507.ipynb). If possible please share a colab gist with the error reported.", "please find the [gist here](https://colab.research.google.com/drive/1iRdEG_6mp9WQb5PXaN1eErP9w-9vx6tT?usp=sharing)", "import tensorflow as tf\r\nimport numpy as np\r\n\r\nx = np.random.randn(1, 3, 12, 12)\r\nx_tf = tf.convert_to_tensor(x, dtype=tf.float32)\r\n\r\nweights=np.ones([2,2,3,8])\r\nweights_tf = tf.convert_to_tensor(weights, dtype=tf.float32)\r\nstride=1\r\nsess = tf.compat.v1.Session()\r\ntf_conv2d = tf.nn.conv2d(x_tf,\r\nweights_tf,\r\nstrides=[1, stride, stride, 1],\r\npadding=\"SAME\",data_format=\"NCHW\")\r\nsess.run(tf.global_variables_initializer())\r\ntf_result = sess.run(tf_conv2d)\r\nprint(tf_result.shape)", "I am able to replicate the error reported on tf 2.4 and nightly , please find the [gist here](https://colab.research.google.com/gist/Saduf2019/90593711239b0d59afbb6293385c4186/untitled513.ipynb)", "May I know how everything is going with this issue", "Issue still exists in TF 2.6 Nightly.", "@bugpromax, Sorry for late response. \r\n\r\nTensorFlow does not support `NCHW` on `CPU` . For more information please refer [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L912-L915). \r\n\r\nI tried to run your code on Colab using `TF 2.8` in eager mode and it is working. Please find the [gist](https://colab.research.google.com/gist/chunduriv/bf2d19e29ccbfc784e9426cbf9461f29/46623.ipynb) here for reference.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46622, "title": "NaN occurs when building with BatchNormalization, PReLU, Flatten and Dense", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):tensorflow2-GPU\r\n- Python version:3.6\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI got the NaN loss when I tried to train my model\r\n\r\n**Describe the expected behavior**\r\nThe loss should be a number\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nbatch_size = 112\r\nepochs = 119\r\nnum_classes = 10\r\nimport os\r\nsave_dir = 'model'\r\nmodel_name = 'test971_trained_model.h5'\r\nimport tensorflow.keras as keras\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\nimg_rows, img_cols = x_train.shape[1], x_train.shape[2]\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.BatchNormalization(momentum = 0.4639004933194679,epsilon=0.6515653837017596))\r\nmodel.add(keras.layers.PReLU(alpha_initializer='Zeros'))\r\n\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(num_classes))\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\r\nmodel_path = os.path.join(save_dir, model_name)\r\nmodel.save(model_path)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nEpoch 1/119\r\n\r\n  112/50000 [..............................] - ETA: 37:05 - loss: nan - accuracy: 0.0804\r\n\r\n  448/50000 [..............................] - ETA: 9:20 - loss: nan - accuracy: 0.0826 \r\n\r\n  784/50000 [..............................] - ETA: 5:22 - loss: nan - accuracy: 0.0778\r\n```\r\n", "comments": ["@bugreporter450 \r\nI am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/81e453ad89c757d31bf24c6d831d5699/untitled509.ipynb), but this has been fixed in [tf nightly](url)  please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/0de835ae99bb6d641c64ecb4237efba4/untitled509.ipynb).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46622\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46622\">No</a>\n"]}, {"number": 46621, "title": "tf.reduce_max returns wrong answers on large tensors e.g., (2048,2048,1024)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): docker run nvcr.io/nvidia/tensorflow:20.12-tf2-py3\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11\r\n- GPU model and memory: RTX 3090, 24GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nComputing the max of a large 3d matrix returns the incorrect answer\r\n\r\n**Describe the expected behavior**\r\nI would expect the value of the function to correct!\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nfor i in range(12):\r\n    n = 2**i\r\n    x = np.arange(n*n*1024, dtype=np.float32).reshape((n,n,1024))\r\n    print(x.shape, np.max(x), tf.reduce_max(x).numpy())\r\n```\r\n```\r\n\r\n(1, 1, 1024) 1023.0 1023.0\r\n(2, 2, 1024) 4095.0 4095.0\r\n(4, 4, 1024) 16383.0 16383.0\r\n(8, 8, 1024) 65535.0 65535.0\r\n(16, 16, 1024) 262143.0 262143.0\r\n(32, 32, 1024) 1048575.0 1048575.0\r\n(64, 64, 1024) 4194303.0 4194303.0\r\n(128, 128, 1024) 16777215.0 16777215.0\r\n(256, 256, 1024) 67108864.0 67108864.0\r\n(512, 512, 1024) 268435460.0 268435460.0\r\n(1024, 1024, 1024) 1073741800.0 1073741800.0\r\n\r\n(2048, 2048, 1024) 4294967300.0 -3.4028235e+38\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["It seems there is a maximum size tensor after which tensorflow will give silently give incorrect results?\r\nHow can I avoid exceeding this limit or detect when this occurs?", "This seems to happen when the size of the tensor input to reduce_max is greater than 2**30.\r\n\r\nIn the code below, sector_rct and tf_sector are the same data, except one is an np.array and the other is a tf tensor.\r\n```\r\nIn [21]: tf_sector = tf.identity(sector_rct)\r\n\r\nIn [29]: tf_sector.shape\r\nOut[29]: TensorShape([5016387584])\r\n\r\nIn [54]: tf_sector.device\r\nOut[54]: '/job:localhost/replica:0/task:0/device:GPU:0'\r\n\r\nIn [55]: tf.reduce_max(tf_sector[:2**31])\r\nOut[55]: <tf.Tensor: shape=(), dtype=float32, numpy=-3.4028235e+38>\r\n\r\nIn [56]: tf.reduce_max(tf_sector[:2**30])\r\nOut[56]: <tf.Tensor: shape=(), dtype=float32, numpy=448851.8>\r\n\r\nIn [57]: np.max(sector_rct[:2**31])\r\nOut[57]: 655147.94\r\n\r\nIn [58]: np.max(sector_rct[:2**30])\r\nOut[58]: 448851.8\r\n```\r\n\r\n", "I tried the same thing in pytorch and it works correctly.\r\nSo I do not think this is a problem with my drivers, or the basic formulation of my setup.\r\n\r\n```\r\nIn [14]: torch_sector = torch_sector.cuda()\r\nIn [15]: torch_sector.dtype\r\nOut[15]: torch.float32\r\nIn [16]: torch_sector.device\r\nOut[16]: device(type='cuda', index=0)\r\nIn [17]: torch_sector.max()                                                                                                                                                                 \r\nOut[17]: tensor(856689.3750, device='cuda:0')\r\nIn [18]: sector_rct.max()\r\nOut[18]: 856689.4\r\nIn [19]: sector_rct.shape\r\nOut[19]: (2048, 2048, 1196)\r\nIn [20]: torch_sector.shape\r\nOut[20]: torch.Size([2048, 2048, 1196])\r\n```", "@varung,\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!", "I tested on tf_nightly_version:\r\n2.5.0-dev20210122\r\ntf.reduce_max seems to return the smallest possible floating point number, if input tensor size is above some limit\r\n", "```\r\nimport numpy as np\r\nimport sys\r\n\r\nflat = np.zeros((2,2**30), dtype=np.float32)\r\nflat[1] = 1\r\nflat = flat.reshape(-1)\r\nprint(\"numpy_cpu\", flat.shape, flat.dtype, np.max(flat), np.max(flat[:2**30]))\r\n\r\nif sys.argv[1] == \"tf\":\r\n    import tensorflow as tf\r\n    print(tf.version.VERSION)\r\n    tf_flat = tf.identity(flat)\r\n    print(tf_flat.device, tf_flat.shape, tf_flat.dtype, tf.reduce_max(tf_flat), tf.reduce_max(tf_flat[:2**30]))\r\nelse:\r\n    import torch\r\n    torch_flat = torch.from_numpy(flat).cuda()\r\n    print(torch_flat.device, torch_flat.shape, torch_flat.dtype, torch.max(torch_flat), torch.max(torch_flat[:2**30]))\r\n```\r\n\r\n```\r\npython tf_error_case.py tf >> tf.out\r\n\r\ntf.out:\r\nnumpy_cpu (2147483648,) float32 1.0 0.0\r\n2.5.0-dev20210122\r\n/job:localhost/replica:0/task:0/device:GPU:0 (2147483648,) <dtype: 'float32'> tf.Tensor(-3.4028235e+38, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\r\n```\r\n\r\n```\r\npython tf_error_case.py torch > torch.out\r\n\r\ntorch.out:\r\nnumpy_cpu (2147483648,) 1.0 0.0\r\ncuda:0 torch.Size([2147483648]) tensor(1., device='cuda:0') tensor(0., device='cuda:0')\r\n```\r\n\r\n\r\n", "Ok! It turns out this minimal version was easy enough to test on colab.\r\nhttps://colab.research.google.com/drive/1-A0fWC9U2Dlt8FvzVVeFb8l1TY360EiU?usp=sharing\r\nYou can see that the bug is also present on 2.4", "Related to https://github.com/NVIDIA/cub/issues/212.  TensorFlow invokes NVIDIA's cub library for scalar reduction. However, it so far only supports 32 bits indexing. In your case, `2 ** 31 > std::numeric_limits<int>::max()` and overflow to -2147483648, which means `num_items` to be reduced becomes negative or 0. Therefore, there is no reduction happening, and returns minimum float32 (-3.4028235E+38).\r\nhttps://github.com/tensorflow/tensorflow/blob/79715317b315feb1687e83cccc76fe78f199ed0e/tensorflow/core/kernels/reduction_gpu_kernels.cu.h#L697-L698\r\nhttps://github.com/NVIDIA/cub/blob/8128ac8489e0f2d3dd82425bac24020367f72fe8/cub/device/device_reduce.cuh#L153", "ah. How does pytorch do it? Since it seems to work correctly there.\r\n", "Pytorch instead uses its own CUDA kernel.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Reduce.cuh", "renamed it back to \"wrong answers\", because it seems to depend on the precise roll over of the integer type. For instance, numbers > 2**32 would roll over again, may result in arbitrary undetectable answers.", "@amahendrakar this is not just v2.3, it is also v2.4 and v2.5", "Colab session crashes on running the code with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/0d30b9f7acafb0f4f37d6f849e7536c2/46621.ipynb#scrollTo=ZrVlDG5bsjSA). Thanks!", "Colab session crashes on running the code with TF v2.5,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/3c557f01da17d2c3e7e869c50c1ce857/untitled195.ipynb)..Thanks !", "@sushreebarsa That crash is probably not the issue that's reported here.  The colab is crashing because it is running out of memory on an older GPU.  The bug reported here manifests when reducing large tensors, which will only fit in newer GPUs that have more than 16G of memory.\r\n\r\nMy initial hunch is that address arithmetic is going wrong somewhere; TF uses 32bit indices in many places which will overflow with large tensors.\r\n\r\n@varung Are you running into this issue in realistic situations, or is this more of a theoretical concern at the moment?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46621\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46621\">No</a>\n", "Yes I am running into it on a real situation on a RTX 3090. I'm switched to pytorch instead which does not have the error."]}, {"number": 46620, "title": "TFLite outputs different inference results for the same input", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source): 2.3.1\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Notebook Link: https://github.com/BernardoGO/tflite_convert_bug/blob/main/BiLSTM%20Conversion%20Test.ipynb**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(new_model)\r\n#converter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n#converter.allow_custom_ops=True\r\ntflite_model = converter.convert()\r\n\r\nopen(\"tf_test.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nINFO:tensorflow:Assets written to: /tmp/tmp4xjrqwpb/assets\r\nOut[9]: 476052\r\n```\r\n\r\n**Graph Definition**\r\n\r\n```\r\nnew_model = Sequential()\r\nnew_model.add(Input(name='input', batch_size=1, shape=(925, 3)) )\r\nnew_model.add(Dense(8))\r\nnew_model.add(LSTM(64, return_sequences=True))\r\nnew_model.add(LSTM(64, return_sequences=True))\r\nnew_model.add(LSTM(64, return_sequences=True))\r\nnew_model.add(LSTM(64, return_sequences=True))\r\nnew_model.add(Activation('softmax', name='softmax'))\r\n\r\nnew_model.summary()\r\n```\r\n\r\n**Failure details**\r\n\r\n- Producing wrong results and/or decrease in accuracy\r\n\r\nThe model was converted successfully, but the output differs for each execution. The same does not happen for the non-converted TensorFlow model, which always yields the same results.\r\nI've attached a notebook that shows how to reproduce the problem and how to detect it. After a few runs, the output gets more consistent.\r\nIt only happens when using LSTMs and seems to get even worse when using return_sequences=True.\r\n\r\n**Any other info / logs**\r\n\r\n- I've used this function to check if the output is the same after each execution:\r\n```\r\ndef equal(tensor1, tensor2):\r\n    for i, j in zip(tensor1.reshape(-1), tensor2.reshape(-1)):\r\n        if abs(i - j) > 0.001:\r\n            return False\r\n    return True\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/1606650/105568097-9fb3f700-5d15-11eb-96c7-762667033f89.png)\r\n\r\n", "comments": ["Hi, This is because for tflite, the fused lstm kernel is \"stateful\", so you will need to reset the variable tensors, with python you can do \r\n\r\n```\r\ninterpreter.reset_all_variables()\r\n```", "Thanks for your answer.\r\nDo are you saying that in TFLite the LSTM is always stateful while in TensorFlow it should be explicitly declared?", "The fused unidirectional_sequence_lstm kernel is always stateful (if you visualize your graph).", "@BernardoGO ! Did you mean to write the **equal(tensor1,tensor2)**  function as below?\r\n```\r\ndef equal(tensor1, tensor2):\r\n    for i, j in zip(tensor1.reshape(-1), tensor2.reshape(-1)):\r\n        if abs(i - j) > 0.001:\r\n            return False\r\n        else: \r\n            return True\r\n```\r\n\r\nI was getting True value even I reset the environment multiple times. Attaching gist in [2.8](https://colab.sandbox.google.com/gist/mohantym/61d6d6eff57bc92aa5c78e9c92e99a2c/bilstm-conversion-test.ipynb#scrollTo=9_goVvjY3-pk)  and [2.7](https://colab.sandbox.google.com/gist/mohantym/bc45fc6124b3f54a21e22c6d34f41e51/bilstm-conversion-test.ipynb#scrollTo=9_goVvjY3-pk) for reference. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46619, "title": "Error MTCNN with tensorflow ROCM ", "body": "Traceback (most recent call last):\r\n  File \"webcam.py\", line 51, in <module>\r\n    main()\r\n  File \"webcam.py\", line 32, in main\r\n    boxes, _ = detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\r\n  File \"/home/taitang/tracking/mtcnn-face-detect/src/detect_face.py\", line 361, in detect_face\r\n    tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\r\nValueError: could not broadcast input array from shape (457,628,3) into shape (0,628,3)\r\n\r\ni tried to run pretrain model MTCNN with Tensorflow ROCM and I received error message as above. Any one help me? Thanks", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46619\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46619\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46619\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46619\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46619\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46619\">No</a>\n", "@taitang96,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using.\r\n\r\nAlso, please take a look at [this comment](https://stackoverflow.com/a/43977626) from a similar StackOverflow query and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46619\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46619\">No</a>\n"]}, {"number": 46617, "title": "TF 2.4.1 and tf_nightly-2.5.0.dev20210122 dump core on Intel Xeon ES462 under Ubuntu 20.04.1", "body": "**System information**\r\nSystem: Intel Xeon ES462 under Ubuntu 20.04.1\r\nPython 3.8.5\r\nTensorFlow 2.4.1 and tf_nightly-2.5.0.dev20210122-cp38-cp38-manylinux2010_x86_64.whl\r\n\r\n**Describe the current behavior**\r\nCore dumped with illegal instruction (vpxor)\r\n\r\n**Describe the expected behavior**\r\nCore not dumped\r\n\r\n**Standalone code to reproduce the issue**\r\nimport tensorflow\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nPartial backtrace:\r\n```\r\nProgram terminated with signal SIGILL, Illegal instruction.\r\n#0  0x00007fee27336930 in nsync::nsync_mu_init(nsync::nsync_mu_s_*) () from /home/REDACTED/virtualenv3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n[Current thread is 1 (Thread 0x7fee80a8d740 (LWP 68631))]\r\n(gdb) bt\r\n#0  0x00007fee27336930 in nsync::nsync_mu_init(nsync::nsync_mu_s_*) () from /home/REDACTED/virtualenv3/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007fee18748c5b in tensorflow::monitoring::Counter<2>* tensorflow::monitoring::Counter<2>::New<char const (&) [46], char const (&) [58], char const (&) [11], char const (&) [7]>(char const (&) [46], char const (&) [58], char const (&) [11], char const (&) [7]) ()\r\n   from /home/REDACTED/virtualenv3/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#2  0x00007fee1870d31e in _GLOBAL__sub_I_loader.cc () from /home/REDACTEDr/virtualenv3/lib/python3.8/site-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#3  0x00007fee80e6fb8a in ?? () from /lib64/ld-linux-x86-64.so.2\r\n\r\n(gdb) disassemble\r\nDump of assembler code for function _ZN5nsync13nsync_mu_initEPNS_11nsync_mu_s_E:\r\n=> 0x00007fee27336930 <+0>:\tvpxor  %xmm0,%xmm0,%xmm0\r\n   0x00007fee27336934 <+4>:\tpush   %rbp\r\n   0x00007fee27336935 <+5>:\tmov    %rsp,%rbp\r\n   0x00007fee27336938 <+8>:\tvmovups %xmm0,(%rdi)\r\n   0x00007fee2733693c <+12>:\tpop    %rbp\r\n   0x00007fee2733693d <+13>:\tretq   \r\n```\r\n", "comments": ["@PatonLewis \r\nCould you please create a new virtual environment and check if you are facing the same issue in that as well? Thanks!", "Error seems to come from `nsync`.\r\n\r\nCan you check with a previous version on nightly/release and see if that is working?", "> @PatonLewis\r\n> Could you please create a new virtual environment and check if you are facing the same issue in that as well? Thanks!\r\n\r\n@Saduf2019 \r\nI created a clean virtual environment and tried with the latest tf-nightly (tf_nightly-2.5.0.dev20210125-cp38-cp38-manylinux2010_x86_64.whl) and the problem persists.", "> Error seems to come from `nsync`.\r\n> \r\n> Can you check with a previous version on nightly/release and see if that is working?\r\n\r\n@mihaimaruseac \r\nI tried `pip install tf-nightly==2.5.0.dev20201230` in a fresh virtual environment and the problem occurs there as well.", "Can you use release versions and bisect? Curious if this reproduces on `2.3.0` release, for example.", "> Can you use release versions and bisect? Curious if this reproduces on `2.3.0` release, for example.\r\n\r\n@mihaimaruseac \r\nThe problem reproduces on 2.3.0 and 2.2.0", "What is the output of `pip list`? It looks like the error comes from a dependency.", "```\r\n$ pip list\r\nPackage                Version\r\n---------------------- ---------\r\nabsl-py                0.11.0\r\nastunparse             1.6.3\r\ncachetools             4.2.1\r\ncertifi                2020.12.5\r\nchardet                4.0.0\r\ngast                   0.3.3\r\ngoogle-auth            1.24.0\r\ngoogle-auth-oauthlib   0.4.2\r\ngoogle-pasta           0.2.0\r\ngrpcio                 1.35.0\r\nh5py                   2.10.0\r\nidna                   2.10\r\nKeras-Preprocessing    1.1.2\r\nMarkdown               3.3.3\r\nnumpy                  1.18.5\r\noauthlib               3.1.0\r\nopt-einsum             3.3.0\r\npip                    21.0\r\nprotobuf               3.14.0\r\npyasn1                 0.4.8\r\npyasn1-modules         0.2.8\r\nrequests               2.25.1\r\nrequests-oauthlib      1.3.0\r\nrsa                    4.7\r\nscipy                  1.4.1\r\nsetuptools             51.1.2\r\nsix                    1.15.0\r\ntensorboard            2.2.2\r\ntensorboard-plugin-wit 1.8.0\r\ntensorflow             2.2.0\r\ntensorflow-estimator   2.2.0\r\ntermcolor              1.1.0\r\nurllib3                1.26.2\r\nWerkzeug               1.0.1\r\nwheel                  0.36.2\r\nwrapt                  1.12.1\r\n```", "Does your CPU support AVX?", "> Does your CPU support AVX?\r\n\r\nWhen I run `grep -i avx /proc/cpuinfo` on the machine I get nothing, so I would guess not. If you want to double-check my answer, I listed the CPU model in the system information of the bug report.", "It does not support AVX.\r\n\r\nAll versions of TF since 1.6 require CPUs that support AVX. You can compile from source or use a cloud VM (for example via Google Collab).\r\n\r\nClosing since the issue is with the CPU, not with TF.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46617\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46617\">No</a>\n"]}, {"number": 46616, "title": "XLA Convolution Causes Segmentation Fault", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- TensorFlow installed from (source or binary): From source, commit: 78ba23b6bcd7fe416aad1da4fe47b2b6036e09ad\r\n- TensorFlow version (use command below): commit: 78ba23b6bcd7fe416aad1da4fe47b2b6036e09ad\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 11.0/8.0.4\r\n- GPU model and memory: GeForce RTX 2070, Driver: 460.32.03\r\n\r\n**Describe the current behavior**\r\n\r\nI'm currently working on writing XLA support for another language. Recently, I upgraded to CUDA 11.0 and cuDNN 8.0.4. Previously, we were using CUDA 10.2 and cuDNN 7. After the switch, all of our convolution tests started producing SegFaults. We also get some warnings during execution:\r\n\r\n```\r\nwarning: Linking two modules of different target triples: '/usr/local/cuda-11.0/nvvm/libdevice/libdevice.10.bc' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\r\n```\r\n\r\nOccasionally, it will warn about failing to find an optimal convolution algorithm and fall back to the default before SegFaulting.\r\n\r\nI suspect it has something to do with a version mismatch. I was originally running off an older commit (prior to 2.4 getting released), and upgraded to the most recent commit to see if it would fix it, but the issue persists.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nAn XLA computation similar to this should reproduce (I'm working in a separate language to build up the computations).\r\n\r\n```\r\n  xla::XlaBuilder* builder = new xla::XlaBuilder(\"conv\");\r\n  xla::Shape input_shape = xla::ShapeUtil::MakeShape(xla::PrimitiveType::F32, {32, 3, 120, 120});\r\n  xla::Shape kernel_shape = xla::ShapeUtil::MakeShape(xla::PrimitiveType::F32, {32, 3, 3, 3});\r\n\r\n  xla::XlaOp inp = xla::RngUniform(*builder, input_shape);\r\n  xla::XlaOp kernel = xla::RngUniform(*builder, kernel_shape);\r\n\r\n  xla::ConvolutionDimensionNumbers dimension_numbers;\r\n  dimension_numbers.set_input_batch_dimension(0);\r\n  dimension_numbers.set_input_feature_dimension(1);\r\n  dimension_numbers.set_kernel_output_feature_dimension(0);\r\n  dimension_numbers.set_kernel_input_feature_dimension(1);\r\n  dimension_numbers.set_output_batch_dimension(0);\r\n  dimension_numbers.set_output_feature_dimension(1);\r\n\r\n  xla::XlaOp result = xla::ConvGeneralDilated(inp,\r\n                                              kernel,\r\n                                              /*padding=*/{},\r\n                                              /*lhs_dilation=*/{},\r\n                                              /*rhs_dilation=*/{},\r\n                                              /*conv_dimnos=*/dimension_numbers);\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nCore Dump backtrace:\r\n\r\n```\r\n#0  0x00007fae997adc84 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#1  0x00007fae9974144b in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#2  0x00007fae997417b8 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#3  0x00007fae99998697 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#4  0x00007fae99998c02 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#5  0x00007fae99759d8a in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#6  0x00007fae9999e3e0 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#7  0x00007fae99713243 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#8  0x00007fae99714555 in ?? () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#9  0x00007fae997bab93 in cuLaunchKernel () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#10 0x00007faa1d62f8bb in ?? () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#11 0x00007faa1d671686 in ?? () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#12 0x00007faa1a926124 in cudnn::cnn::cudnnIm2Col4d(cudnnContext*, cudnnTensor4dStruct*, void const*, cudnnFilter4dStruct*, cudnnConvolutionStruct*, void*) ()\r\n   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#13 0x00007faa1a8e05a0 in cudnn::cnn::GemmConvolveEngine<cudnn::cnn::gemm_convolve_launch_pf<double, double, double, double>, false, 9, 2>::execute_internal_impl(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#14 0x00007faa1a537033 in cudnn::cnn::EngineInterface::execute(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#15 0x00007faa1a563960 in cudnn::cnn::EngineContainer<(cudnnBackendEngineName_t)2, 4096ul>::execute_internal_impl(cudnn::backend::VariantPack const&, CUstream_st*) ()\r\n   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#16 0x00007faa1a537033 in cudnn::cnn::EngineInterface::execute(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#17 0x00007faa1a5be18c in cudnn::cnn::AutoTransformationExecutor::execute_pipeline(cudnn::cnn::ConvolutionEngine&, cudnn::backend::VariantPack const&, CUstream_st*) const ()\r\n   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#18 0x00007faa1a5d9871 in cudnn::cnn::GeneralizedConvolutionEngine<cudnn::cnn::EngineContainer<(cudnnBackendEngineName_t)2, 4096ul> >::execute_internal_impl(cudnn::backend::VariantPack const&, CUs--Type <RET> for more, q to quit, c to continue without paging--\r\ntream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#19 0x00007faa1a537033 in cudnn::cnn::EngineInterface::execute(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#20 0x00007faa1a53e730 in cudnn::backend::execute(cudnnContext*, cudnn::backend::ExecutionPlan&, cudnn::backend::VariantPack&) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#21 0x00007faa1a63f40c in cudnn::backend::EnginesAlgoMap<cudnnConvolutionFwdAlgo_t, 8>::execute_wrapper(cudnnContext*, cudnnConvolutionFwdAlgo_t, cudnn::backend::ExecutionPlan&, cudnn::backend::VariantPack&) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#22 0x00007faa1a638ad9 in cudnn::backend::convolutionForward(cudnnContext*, void const*, cudnnTensorStruct const*, void const*, cudnnFilterStruct const*, void const*, cudnnConvolutionStruct const*, cudnnConvolutionFwdAlgo_t, void*, unsigned long, bool, void const*, void const*, void const*, cudnnActivationStruct const*, cudnnTensorStruct const*, void*) ()\r\n   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#23 0x00007faa1a73ae96 in cudnn::cnn::convolutionForward(cudnnContext*, void const*, cudnnTensorStruct*, void const*, cudnnFilterStruct*, void const*, cudnnConvolutionStruct*, cudnnConvolutionFwdAlgo_t, void*, unsigned long, void const*, cudnnTensorStruct*, void*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#24 0x00007faa1a73b94c in cudnnConvolutionForward () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#25 0x00007fadf91ab772 in cudnnConvolutionForward () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#26 0x00007fadf9191c6d in stream_executor::gpu::CudnnSupport::DoConvolve(stream_executor::dnn::ConvolutionKind, stream_executor::dnn::DataType, stream_executor::dnn::DataType, stream_executor::Stream*, stream_executor::dnn::BatchDescriptor const&, stream_executor::DeviceMemoryBase, stream_executor::dnn::FilterDescriptor const&, stream_executor::DeviceMemoryBase, stream_executor::dnn::BatchDescriptor const&, stream_executor::DeviceMemoryBase, stream_executor::dnn::ConvolutionDescriptor const&, stream_executor::dnn::AlgorithmDesc, stream_executor::DeviceMemory<unsigned char>, stream_executor::dnn::ProfileResult*) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#27 0x00007fadf72a1735 in tensorflow::Status xla::gpu::(anonymous namespace)::RunGpuConvImpl<double, double, double>(xla::gpu::GpuConvParams const&, stream_executor::ScratchAllocator*, stream_executor::Stream*, xla::gpu::RunConvOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#28 0x00007fadf72a4a2e in xla::gpu::RunGpuConv(xla::gpu::GpuConvConfig const&, absl::lts_2020_02_25::Span<stream_executor::DeviceMemoryBase>, stream_executor::DeviceMemoryBase, stream_executor::ScratchAllocator*, stream_executor::Stream*, xla::gpu::RunConvOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#29 0x00007fadf72a70e4 in xla::gpu::RunGpuConv(xla::gpu::GpuConvConfig const&, absl::lts_2020_02_25::Span<stream_executor::DeviceMemoryBase>, stream_executor::DeviceMemoryBase, stream_executor::DeviceMemoryBase, stream_executor::Stream*, xla::gpu::RunConvOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#30 0x00007fadf72473fb in xla::gpu::ConvolutionThunk::ExecuteOnStream(xla::gpu::Thunk::ExecuteParams const&) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n#31 0x00007fadf72573bb in xla::gpu::GpuExecutable::ExecuteThunks(xla::ServiceExecutableRunOptions const*, xla::gpu::BufferAllocations const&, bool, xla::HloExecutionProfile*) ()\r\n   from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#32 0x00007fadf725c2cc in xla::gpu::GpuExecutable::ExecuteAsyncOnStreamImpl(xla::ServiceExecutableRunOptions const*, absl::lts_2020_02_25::variant<absl::lts_2020_02_25::Span<xla::ShapedBuffer const* const>, absl::lts_2020_02_25::Span<xla::ExecutionInput> >, xla::HloExecutionProfile*) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#33 0x00007fadf725c8f8 in xla::gpu::GpuExecutable::ExecuteAsyncOnStream(xla::ServiceExecutableRunOptions const*, std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >, xla::HloExecutionProfile*) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#34 0x00007fadfd69b286 in xla::Executable::ExecuteAsyncOnStreamWrapper(xla::ServiceExecutableRunOptions const*, std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >) ()\r\n   from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#35 0x00007fadf90c0df7 in xla::LocalExecutable::RunAsync(absl::lts_2020_02_25::Span<xla::Shape const* const>, std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >, xla::ExecutableRunOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#36 0x00007fadf90c17c8 in xla::LocalExecutable::RunAsync(std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >, xla::ExecutableRunOptions) ()\r\n```\r\n\r\nForgive me if this is a CUDA version issue, or if this is a question better asked in the XLA Dev Group.", "comments": ["The segfault seems related to cuDNN. Could you try cuda-gdb for your stack trace?\r\n\r\ncc @nluehr @sanjoy ", "Sure! Looks like you're right. Here's the same core dump with `cuda-gdb`:\r\n\r\n```\r\n#0  0x00007fae997adc84 in cudbgGetAPI () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#1  0x00007fae9974144b in cuEGLApiInit () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#2  0x00007fae997417b8 in cuEGLApiInit () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#3  0x00007fae99998697 in cudbgApiInit () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#4  0x00007fae99998c02 in cudbgApiInit () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#5  0x00007fae99759d8a in cuEGLApiInit () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#6  0x00007fae9999e3e0 in cudbgApiInit () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#7  0x00007fae99713243 in cuMemGetAttribute () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#8  0x00007fae99714555 in cuMemGetAttribute () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#9  0x00007fae997bab93 in cuLaunchKernel () from /lib/x86_64-linux-gnu/libcuda.so.1\r\n#10 0x00007faa1d62f8bb in cask_cudnn::CutlassConvolutionDgradShader<cutlass::conv::device::ImplicitGemmConvolution<cutlass_tensorop_h16816dgrad_analytic_128x128_64x3_unity_stride> >::run(cask_cudnn::RunInfo&, void*, void const*, void const*, void const*, void const*, void const*, void const*, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#11 0x00007faa1d671686 in cask_cudnn::CutlassConvolutionDgradShader<cutlass::conv::device::ImplicitGemmConvolution<cutlass_tensorop_h16816dgrad_analytic_128x128_64x3_unity_stride> >::run(cask_cudnn::RunInfo&, void*, void const*, void const*, void const*, void const*, void const*, void const*, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#12 0x00007faa1a926124 in cudnn::cnn::cudnnIm2Col4d(cudnnContext*, cudnnTensor4dStruct*, void const*, cudnnFilter4dStruct*, cudnnConvolutionStruct*, void*) ()\r\n   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#13 0x00007faa1a8e05a0 in cudnn::cnn::GemmConvolveEngine<cudnn::cnn::gemm_convolve_launch_pf<double, double, double, double>, false, 9, 2>::execute_internal_impl(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#14 0x00007faa1a537033 in cudnn::cnn::EngineInterface::execute(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#15 0x00007faa1a563960 in cudnn::cnn::EngineContainer<(cudnnBackendEngineName_t)2, 4096ul>::execute_internal_impl(cudnn::backend::VariantPack const&, CUstream_st*) ()\r\n   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#16 0x00007faa1a537033 in cudnn::cnn::EngineInterface::execute(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#17 0x00007faa1a5be18c in cudnn::cnn::AutoTransformationExecutor::execute_pipeline(cudnn::cnn::ConvolutionEngine&, cudnn::backend::VariantPack const&, CUstream_st*) const ()\r\n---Type <return> to continue, or q <return> to quit---\r\n   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#18 0x00007faa1a5d9871 in cudnn::cnn::GeneralizedConvolutionEngine<cudnn::cnn::EngineContainer<(cudnnBackendEngineName_t)2, 4096ul> >::execute_internal_impl(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#19 0x00007faa1a537033 in cudnn::cnn::EngineInterface::execute(cudnn::backend::VariantPack const&, CUstream_st*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#20 0x00007faa1a53e730 in cudnn::backend::execute(cudnnContext*, cudnn::backend::ExecutionPlan&, cudnn::backend::VariantPack&) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#21 0x00007faa1a63f40c in cudnn::backend::EnginesAlgoMap<cudnnConvolutionFwdAlgo_t, 8>::execute_wrapper(cudnnContext*, cudnnConvolutionFwdAlgo_t, cudnn::backend::ExecutionPlan&, cudnn::backend::VariantPack&) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#22 0x00007faa1a638ad9 in cudnn::backend::convolutionForward(cudnnContext*, void const*, cudnnTensorStruct const*, void const*, cudnnFilterStruct const*, void const*, cudnnConvolutionStruct const*, cudnnConvolutionFwdAlgo_t, void*, unsigned long, bool, void const*, void const*, void const*, cudnnActivationStruct const*, cudnnTensorStruct const*, void*) ()\r\n   from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#23 0x00007faa1a73ae96 in cudnn::cnn::convolutionForward(cudnnContext*, void const*, cudnnTensorStruct*, void const*, cudnnFilterStruct*, void const*, cudnnConvolutionStruct*, cudnnConvolutionFwdAlgo_t, void*, unsigned long, void const*, cudnnTensorStruct*, void*) () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#24 0x00007faa1a73b94c in cudnnConvolutionForward () from /lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8\r\n#25 0x00007fadf91ab772 in cudnnConvolutionForward () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#26 0x00007fadf9191c6d in stream_executor::gpu::CudnnSupport::DoConvolve(stream_executor::dnn::ConvolutionKind, stream_executor::dnn::DataType, stream_executor::dnn::DataType, stream_executor::Stream*, stream_executor::dnn::BatchDescriptor const&, stream_executor::DeviceMemoryBase, stream_executor::dnn::FilterDescriptor const&, stream_executor::DeviceMemoryBase, stream_executor::dnn::BatchDescriptor const&, stream_executor::DeviceMemoryBase, stream_executor::dnn::ConvolutionDescriptor const&, stream_executor::dnn::AlgorithmDesc, stream_executor::DeviceMemory<unsigned char>, stream_executor::dnn::ProfileResult*) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#27 0x00007fadf72a1735 in tensorflow::Status xla::gpu::(anonymous namespace)::RunGpuConvImpl<double, double, double>(xla::gpu::GpuConvParams const&, stream_executor::ScratchAllocator*, stream_executor::Stream*, xla::gpu::RunConvOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#28 0x00007fadf72a4a2e in xla::gpu::RunGpuConv(xla::gpu::GpuConvConfig const&, absl::lts_2020_02_25::Span<stream_executor::DeviceMemoryBase>, stream_executor::DeviceMemoryBase, stream_executor::ScratchAllocator*, stream_executor::Stream*, xla::gpu::RunConvOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#29 0x00007fadf72a70e4 in xla::gpu::RunGpuConv(xla::gpu::GpuConvConfig const&, absl::lts_2020_02_25::Span<stream_executor::DeviceMemoryBase>, stream_executor::DeviceMemoryBase, stream_executor::De---Type <return> to continue, or q <return> to quit---\r\nviceMemoryBase, stream_executor::Stream*, xla::gpu::RunConvOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#30 0x00007fadf72473fb in xla::gpu::ConvolutionThunk::ExecuteOnStream(xla::gpu::Thunk::ExecuteParams const&) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#31 0x00007fadf72573bb in xla::gpu::GpuExecutable::ExecuteThunks(xla::ServiceExecutableRunOptions const*, xla::gpu::BufferAllocations const&, bool, xla::HloExecutionProfile*) ()\r\n   from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#32 0x00007fadf725c2cc in xla::gpu::GpuExecutable::ExecuteAsyncOnStreamImpl(xla::ServiceExecutableRunOptions const*, absl::lts_2020_02_25::variant<absl::lts_2020_02_25::Span<xla::ShapedBuffer const* const>, absl::lts_2020_02_25::Span<xla::ExecutionInput> >, xla::HloExecutionProfile*) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#33 0x00007fadf725c8f8 in xla::gpu::GpuExecutable::ExecuteAsyncOnStream(xla::ServiceExecutableRunOptions const*, std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >, xla::HloExecutionProfile*) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#34 0x00007fadfd69b286 in xla::Executable::ExecuteAsyncOnStreamWrapper(xla::ServiceExecutableRunOptions const*, std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >) ()\r\n   from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#35 0x00007fadf90c0df7 in xla::LocalExecutable::RunAsync(absl::lts_2020_02_25::Span<xla::Shape const* const>, std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >, xla::ExecutableRunOptions) () from /home/sean/projects/exla/_build/test/lib/exla/priv/libexla.so\r\n#36 0x00007fadf90c17c8 in xla::LocalExecutable::RunAsync(std::vector<xla::ExecutionInput, std::allocator<xla::ExecutionInput> >, xla::ExecutableRunOptions) ()\r\n```\r\n\r\nAny ideas on how I might be able to resolve? Or should I go looking for answers on the cuDNN side?", "You can try sharing the [API log](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#api-logging) with NVIDIA, they can usually handle it from there.", "And regarding the\r\n\r\n```\r\nwarning: Linking two modules of different target triples: '/usr/local/cuda-11.0/nvvm/libdevice/libdevice.10.bc' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\r\n```\r\n\r\nerror, that's a false alarm, I have a change in flight that will avoid printing this.", "I passed the issue on to NVIDIA along with the core dump and API Log. They're looking into it.", "> I passed the issue on to NVIDIA along with the core dump and API Log. They're looking into it.\r\n\r\n@seanmor5,\r\nThank you for the update. \r\n\r\nCan we close this issue for now, as it is being tracked by NVIDIA? Please feel free to re-open if necessary. Thanks!", "Yes, I'll re-open if I run into any more issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46616\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46616\">No</a>\n"]}, {"number": 46615, "title": "ExponentialMovingAverage Does Not Work Under tf.function (TF 2.4)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThe average variable becomes None and seems like the Tensor is not tracked.\r\n**Describe the expected behavior**\r\nTensor being properly tracked and valid average value being returned.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nColab: https://colab.research.google.com/drive/17W7CVfDqx-HTbcRMphhQYg6WCq-ELg_K?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nAutograph logs show that the function is traced twice. While the first pass shows everything alright, including the tensor being properly tracked, the second pass seems not to be the case although the generated graph has been reused.", "comments": ["@ChanZou,\r\nI do not have access to the Colab Notebook you have linked. Could you please provide the required permissions to view the files? Thanks!", "> @ChanZou,\r\n> I do not have access to the Colab Notebook you have linked. Could you please provide the required permissions to view the files? Thanks!\r\n\r\nTerribly sorry, this link should be open to everyone: \r\n[colab](https://colab.research.google.com/drive/17W7CVfDqx-HTbcRMphhQYg6WCq-ELg_K?usp=sharing)", "Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1becf068751c41e85591907fcf07fa99/46615.ipynb). Thanks!", "Is this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46615\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46615\">No</a>\n"]}, {"number": 46614, "title": "Make H-loop-fusion share operands with users.", "body": "", "comments": ["@cheshire please help to take a look.\r\n\r\n@akuegel @sanjoy FYI.", "@cheshire \r\n\r\nI thought more about the problem and, unfortunately, I think the current solution (as-it) is not correct. Mainly, the concatenate can create create extra dependencies between its operands. Below is an example.\r\n\r\n`    HloModule test\r\n\r\n    fused_computation {\r\n      // p0 has multiple transitive uses fed to concat and cannot share buffer\r\n      // with outputs, because the aliased output could be written before all\r\n      // the uses of p0 are finished.\r\n      p0 = f32[100] parameter(0)\r\n      p1 = f32[100] parameter(1)\r\n      add0 = f32[100] add(p0, p1)\r\n      concat0 = f32[200] concatenate(p0, add0), dimensions={0}\r\n      slice0 = f32[100] slice(concat0), slice={[0:100]}\r\n      slice1 = f32[100] slice(concat0), slice={[100:200]}\r\n      ROOT tuple = (f32[100], f32[100]) tuple(slice0, slice1)\r\n    }\r\n\r\n    ENTRY test {\r\n      p0 = f32[100] parameter(0)\r\n      p1 = f32[100] parameter(1)\r\n      ROOT fusion = (f32[100], f32[100]) fusion(p0, p1), kind=kInput, calls=fused_computation\r\n    }\r\n`\r\n\r\nWe can still follow the same way to prove that the transitive use closure is fed into only 1 concat and eventually into the user (that we want to set up alias.) Then, since the entire closure (inc. the user) shares the same iteration space (computed by the same thread ids), it is safe to share buffers.\r\n\r\nI don't have your cases at hands and I'm a bit worried that given the new constraints it may not address your cases in concern. What do you feel about this? If there is a good chance to solve your cases, I'd say let's still give it a try.\r\n\r\nAlternatively, we might have to step back to remove the concat from the HorizontalLoopFusion (i.e., fuse by sequentially running each fusion) and it might lose some performance in some cases.\r\n\r\nI will also discuss internally within NVIDIA to get some feedback tomorrow to see what I should do. (@nouiz and @bas-aarts )\r\n", "Unfortunately though the benchmark which was previously OOMing is still OOMing with this patch =(", "> Unfortunately though the benchmark which was previously OOMing is still OOMing with this patch =(\r\n\r\nAh~ That is indeed unfortunate. Does the OOM disappear with the original commit without the new constraints? I.e., https://github.com/tensorflow/tensorflow/pull/46614/commits/4edc3da19631fa3cb3525a2e4362262685d87e20\r\n\r\nIf it is the new constraints making it not work, we can simply exclude the cases with the constraints in the HorizontalLoopFusion. That is, we can simply exclude the cases where fusions share the same input parameters (because these input parameters will be used as different concat operands after the fusions are fused). Since this PR can already deal with the cases without sharing parameters across fusions, adding the exclusion should make the solution complete.\r\n", "> > Unfortunately though the benchmark which was previously OOMing is still OOMing with this patch =(\r\n> \r\n> Ah~ That is indeed unfortunate. Does the OOM disappear with the original commit without the new constraints? I.e., [4edc3da](https://github.com/tensorflow/tensorflow/commit/4edc3da19631fa3cb3525a2e4362262685d87e20)\r\n> \r\n> If it is the new constraints making it not work, we can simply exclude the cases with the constraints in the HorizontalLoopFusion. That is, we can simply exclude the cases where fusions share the same input parameters (because these input parameters will be used as different concat operands after the fusions are fused). Since this PR can already deal with the cases without sharing parameters across fusions, adding the exclusion should make the solution complete.\r\n\r\n@cheshire, I just pushed a commit to disable fusing fusions that share parameters. Could you test if this solves your case? It should according to our understanding. (Otherwise, we have missed something.)\r\n\r\n\r\n", "@trentlo I've shared the problematic HLOs with you, do you think you could take a look?", "> @trentlo I've shared the problematic HLOs with you, do you think you could take a look?\r\n\r\nSure. Will do. Thanks for preparing the HLOs.", "@trentlo  Any update on this PR? Please. Thanks!\r\n", "\r\n> @trentlo I've shared the problematic HLOs with you, do you think you could take a look?\r\n\r\n@cheshire, so, this PR works well for the HLOs you gave me although Adrian later traced the OOM issue under investigation to another major source.\r\n\r\nSome details on the analysis of the HLOs you gave me:\r\n1.\tHorizontalLoopFusion is only active on module 0000 and 0157 (out of 8 modules you gave me.)\r\n2.\tEach module has only 1 horizontal_loop_fusion produced. The case in module 0000 falls into the case of sharing parameters between (fused) fusions, so the i/o buffers of the horizontal fusion are not sharable. However, this case is excluded by this commit (https://github.com/tensorflow/tensorflow/pull/46614/commits/24c07a32c800c0ab4d2705491a2c48c8c9bdbff8), which skips the fusions that share input parameters. So, the case is solved by the commit.\r\n3.\tThe other case in Module 0157 can share i/o buffers. I don\u2019t have your i/o aliasing setups, so I tried several (random) setups of i/o aliasing and the PR succeeds proving the share-ability (so long as the i/o buffers indeed have in-place update relationship)!\r\n\r\nI'd suggest continuing the review process and merging the PR as we believe it addresses the io aliasing issue (that we've been discussing). Please let me know what you think. Thanks!\r\n\r\n", "@trentlo Thanks a lot! Does it mean that we can remove the guard at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/gpu/horizontal_loop_fusion.cc#L181-L183 and that the test case at https://gist.github.com/cheshire/0b9683c2c84d096b95bbbf02958173f4 now does not create copies?", "> @trentlo Thanks a lot! Does it mean that we can remove the guard at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/gpu/horizontal_loop_fusion.cc#L181-L183 and that the test case at https://gist.github.com/cheshire/0b9683c2c84d096b95bbbf02958173f4 now does not create copies?\r\n\r\n@cheshire thanks for the testcase!\r\n\r\nThis PR works for the test case after fixing a minor issue. I was not aware that DynamicUpdateSlice is not an elementwise op and we need to use IsElementwiseOnOperand() to test whether the compute is elementwise. The issue is fixed by this [commit](https://github.com/tensorflow/tensorflow/pull/46614/commits/3fb0103a881da1fdde23d2f3c2fde3dde2d3a988).\r\n\r\nOut of curiosity, I measure the kernel time for benchmark using nvprof. Horizontal (loop) fusion now speeds up the case by 30%! See below for the relevant stats from nvprof.\r\n\r\n```\r\n//Horizontal fusion (w/ PR), kernel time ~= 3.33ms (30% speedup vs. w/o horizontal fusion.)\r\n Time(%)    Time           Calls  Avg          Min          Max        Name\r\n    29.51%  1.8112ms      1000  1.8110us  1.7910us  2.3360us  fusion_5\r\n    24.68%  1.5148ms      1000  1.5140us  1.5030us  1.9840us  fusion_3\r\n      0.09%  5.5040us         3  1.8340us  1.5360us  2.1120us  [CUDA memcpy DtoD]\r\n\r\n//Horizontal fusion (w/o PR), kernel time ~= 5.74ms\r\n    28.91%  2.4695ms      2003  1.2320us  1.2150us  1.9200us  [CUDA memcpy DtoD]\r\n    21.13%  1.8052ms      1000  1.8050us  1.7910us  2.3360us  fusion_5\r\n    17.20%  1.4691ms      1000  1.4690us  1.4390us  1.6320us  fusion_3\r\n\r\n// No horizontal fusion, kernel time = ~4.36ms\r\n    21.21%  1.5209ms      1000  1.5200us  1.4720us  10.110us  fusion_3\r\n    19.81%  1.4204ms      1000  1.4200us  1.3750us  8.5750us  fusion_1\r\n    19.74%  1.4155ms      1000  1.4150us  1.3750us  1.7600us  fusion\r\n    0.07%  5.2800us         3  1.7600us  1.5040us  1.8880us  [CUDA memcpy DtoD]\r\n```\r\n", "> Out of curiosity, I measure the kernel time for benchmark using nvprof. Horizontal (loop) fusion now speeds up the case by 30%! \r\n\r\nAmazing ! BTW is there a short guide for using nvprof to get these numbers? I always kinda struggle when using the profiler.", "> > Out of curiosity, I measure the kernel time for benchmark using nvprof. Horizontal (loop) fusion now speeds up the case by 30%!\r\n> \r\n> Amazing ! BTW is there a short guide for using nvprof to get these numbers? I always kinda struggle when using the profiler.\r\n\r\nI usually just prepend `nvprof` to the command you want to run. For example, to replay the hlo, the following is what I did.\r\n\r\n`nvprof ./bazel-bin/tensorflow/compiler/xla/tools/replay_computation_gpu --use_fake_data=true --print_result=false --compile_only=false dynamic_slice.hlo`\r\n\r\nYou will see (reasonably nice) summary of stats after the run. There is also a mode to dump the entire trace. (nvprof -h to see the options.)\r\n\r\nFor short run time, e.g., within few minutes, my experience with nvprof is good. I avoid using nvprof for long runtime because its post-processing time is too long for me. For longer run time, I use nsight system.\r\n", "Thanks!", "@trentlo Unfortunately, this change got rolled back. It crashes the test in `https://cs.opensource.google/jax/jax/+/master:tests/jet_test.py;l=297?q=JetTest.test_cumsum&sq=`\r\n\r\nwith\r\n\r\n```\r\nF0212 16:11:17.079444    7826 shape_util.cc:536] Check failed: shape.IsArray() (f64[6], f64[6], f64[6], f64[6])\r\n*** Check failure stack trace: ***\r\n    @     0x55c202e100dc  absl::logging_internal::LogMessage::DieIfFatal()\r\n    @     0x55c202e0eb56  absl::logging_internal::LogMessage::SendToLog()\r\n    @     0x55c202e0e441  absl::logging_internal::LogMessage::Flush()\r\n    @     0x55c202e10c89  absl::logging_internal::LogMessageFatal::~LogMessageFatal()\r\n    @     0x55c1ff2eb768  xla::ShapeUtil::ElementsIn()\r\n    @     0x55c1fee0e81d  xla::HloDataflowAnalysis::CanShareOperandBufferWithUser()\r\n    @     0x55c1fedc4147  xla::HeapSimulator::RunComputation()\r\n    @     0x55c1fedc297f  xla::HeapSimulator::Run()\r\n    @     0x55c1fedc253c  xla::HeapSimulator::MinimumMemoryForComputation()\r\n    @     0x55c1fa3d72a7  xla::ListMemoryScheduler()\r\n    @     0x55c1fa3da29e  xla::DefaultMemoryScheduler()\r\n    @     0x55c1fa3dbeb9  xla::(anonymous namespace)::ScheduleComputationHelper()\r\n    @     0x55c1fa3dbc93  xla::ScheduleComputation()\r\n    @     0x55c1f5349ffb  xla::gpu::GpuHloSchedule::Build()\r\n    @     0x55c1f5318574  xla::gpu::CompileModuleToLlvmIrImpl()\r\n    @     0x55c1f5317348  xla::gpu::GpuCompiler::RunBackend()\r\n    @     0x55c1fc36ca27  xla::Service::BuildExecutable()\r\n    @     0x55c1fc3630f0  xla::LocalService::CompileExecutables()\r\n    @     0x55c1fc206c1e  xla::LocalClient::Compile()\r\n    @     0x55c1fc1ebd6b  xla::PjRtStreamExecutorClient::Compile()\r\n    @     0x55c1f8f25c83  xla::PyClient::Compile()\r\n```\r\n\r\nDo you think that's enough ? To repro you could install JAX (it's completely OSS), run that test, capture the HLO, and observe it.", "> ElementsIn\r\n\r\nThanks for the info. Let me try reproducing it by installing JAX. Will ask you for more if that is not enough.\r\n\r\nLooks like it might be some misuse of ElementsIn() according to the dump.\r\n", "Well it's a CHECK failure inside XLA, so it's definitely a bug.", "Yes, based on the stack trace, this is a call to the ElementsIn() function on a tuple shape. The PR adds three lines which call ElementsIn(). The call in ConcatIsEffectivelyElementwise should be fine, because the operand of a concat cannot have a tuple shape. This leaves the two calls in CanShareOperandBufferWithUser, which seems to also match what we see in the stack trace. I guess you need to add a check to the condition for whether it is a tuple shape or not.", "> Yes, based on the stack trace, this is a call to the ElementsIn() function on a tuple shape. The PR adds three lines which call ElementsIn(). The call in ConcatIsEffectivelyElementwise should be fine, because the operand of a concat cannot have a tuple shape. This leaves the two calls in CanShareOperandBufferWithUser, which seems to also match what we see in the stack trace. I guess you need to add a check to the condition for whether it is a tuple shape or not.\r\n\r\nRight. That is what I had in mind. Thanks for the analysis!\r\n\r\n", "> https://cs.opensource.google/jax/jax/+/master:tests/jet_test.py;l=297?q=JetTest.test_cumsum&sq=\r\n\r\n@cheshire I dumped the HLOs from cumsum in https://cs.opensource.google/jax/jax/+/master:tests/jet_test.py;l=297?q=JetTest.test_cumsum&sq= but I do not repro the crash with xla replay.\r\n\r\nIs it possible that the crashed test is another?\r\n", "> > https://cs.opensource.google/jax/jax/+/master:tests/jet_test.py;l=297?q=JetTest.test_cumsum&sq=\r\n> \r\n> @cheshire I dumped the HLOs from cumsum in https://cs.opensource.google/jax/jax/+/master:tests/jet_test.py;l=297?q=JetTest.test_cumsum&sq= but I do not repro the crash with xla replay.\r\n> \r\n> Is it possible that the crashed test is another?\r\n\r\n@cheshire I meant if you could double check whether the failing test is cumsum? Or I just should proceed to build JAX and see if I can repro the bug with running JAX?\r\n\r\n", "@trentlo sorry for the inconvenience! The test failure was in that file. You can also try to run all JAX tests with your patch:\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\n# check out branch\r\ngit clone https://github.com/google/jax.git\r\ncd jax\r\npython build/build.py --bazel_options=\"--override_repository=org_tensorflow=path/to/checked/out/tensorflow\"\r\npip install dist/*.whl\r\npip install -e .\r\npytest -n auto tests examples\r\n```", "> @trentlo sorry for the inconvenience! The test failure was in that file. You can also try to run all JAX tests with your patch:\r\n> \r\n> ```\r\n> git clone https://github.com/tensorflow/tensorflow.git\r\n> # check out branch\r\n> git clone https://github.com/google/jax.git\r\n> cd jax\r\n> python build/build.py --bazel_options=\"--override_repository=org_tensorflow=path/to/checked/out/tensorflow\"\r\n> pip install dist/*.whl\r\n> pip install -e .\r\n> pytest -n auto tests examples\r\n> ```\r\n\r\nI see. Thanks for the reply. Will try to repro with building JAX.\r\n\r\n\r\n", "> > @trentlo sorry for the inconvenience! The test failure was in that file. You can also try to run all JAX tests with your patch:\r\n> > ```\r\n> > git clone https://github.com/tensorflow/tensorflow.git\r\n> > # check out branch\r\n> > git clone https://github.com/google/jax.git\r\n> > cd jax\r\n> > python build/build.py --bazel_options=\"--override_repository=org_tensorflow=path/to/checked/out/tensorflow\"\r\n> > pip install dist/*.whl\r\n> > pip install -e .\r\n> > pytest -n auto tests examples\r\n> > ```\r\n> \r\n> I see. Thanks for the reply. Will try to repro with building JAX.\r\n\r\n@cheshire I built JAX with overriding the tensorflow path as you said and tested with `pytest -n 1 tests/jet_test.py`. However, I don't repro the issue as the stack dump you showed (i.e., the test runs through successfully). (BTW I'm quite sure the tensorflow source I provided is hooked.)\r\n\r\nI am trying running though all the JAX tests. But are you sure the failure is from jet_test.py?\r\n\r\nBTW, I saw some CUDA_ERROR_OUT_OF_MEMORY when I ran some tests other than jet_test.py. How could I limit the memory usage for the tests? I run on 16GB memory.", "> > > @trentlo sorry for the inconvenience! The test failure was in that file. You can also try to run all JAX tests with your patch:\r\n> > > ```\r\n> > > git clone https://github.com/tensorflow/tensorflow.git\r\n> > > # check out branch\r\n> > > git clone https://github.com/google/jax.git\r\n> > > cd jax\r\n> > > python build/build.py --bazel_options=\"--override_repository=org_tensorflow=path/to/checked/out/tensorflow\"\r\n> > > pip install dist/*.whl\r\n> > > pip install -e .\r\n> > > pytest -n auto tests examples\r\n> > > ```\r\n> > \r\n> > \r\n> > I see. Thanks for the reply. Will try to repro with building JAX.\r\n> \r\n> @cheshire I built JAX with overriding the tensorflow path as you said and tested with `pytest -n 1 tests/jet_test.py`. However, I don't repro the issue as the stack dump you showed (i.e., the test runs through successfully). (BTW I'm quite sure the tensorflow source I provided is hooked.)\r\n> \r\n> I am trying running though all the JAX tests. But are you sure the failure is from jet_test.py?\r\n> \r\n> BTW, I saw some CUDA_ERROR_OUT_OF_MEMORY when I ran some tests other than jet_test.py. How could I limit the memory usage for the tests? I run on 16GB memory.\r\n\r\nUpdate:\r\nRan through all the tests. See a bunch of CUDA_ERROR_OUT_OF_MEMORY. Do not see other failures.\r\n", "OK I'll run the tests internally myself and try to figure out where the\nfailures are coming from!\n\nOn Tue, Feb 23, 2021 at 6:29 PM Trent Lo <notifications@github.com> wrote:\n\n> @trentlo <https://github.com/trentlo> sorry for the inconvenience! The\n> test failure was in that file. You can also try to run all JAX tests with\n> your patch:\n>\n> git clone https://github.com/tensorflow/tensorflow.git\n> # check out branch\n> git clone https://github.com/google/jax.git\n> cd jax\n> python build/build.py --bazel_options=\"--override_repository=org_tensorflow=path/to/checked/out/tensorflow\"\n> pip install dist/*.whl\n> pip install -e .\n> pytest -n auto tests examples\n>\n> I see. Thanks for the reply. Will try to repro with building JAX.\n>\n> @cheshire <https://github.com/cheshire> I built JAX with overriding the\n> tensorflow path as you said and tested with pytest -n 1 tests/jet_test.py.\n> However, I don't repro the issue as the stack dump you showed (i.e., the\n> test runs through successfully). (BTW I'm quite sure the tensorflow source\n> I provided is hooked.)\n>\n> I am trying running though all the JAX tests. But are you sure the failure\n> is from jet_test.py?\n>\n> BTW, I saw some CUDA_ERROR_OUT_OF_MEMORY when I ran some tests other than\n> jet_test.py. How could I limit the memory usage for the tests? I run on\n> 16GB memory.\n>\n> Update:\n> Ran through all the tests. See a bunch of CUDA_ERROR_OUT_OF_MEMORY. Do not\n> see other failures.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/46614#issuecomment-784702227>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGHYDULD47RXMTKL6UCTTARP7DANCNFSM4WO7K7ZQ>\n> .\n>\n", "Are you running the debug build ? The specified test passes with `-c opt`, but fails with `-c dbg`.", "The problem reproduces on this HLO in debug build: https://gist.github.com/cheshire/765c08aae5a3d20ee00be65aeabc43db", "Ah. That makes sense. I ran without `-c dbg`. Thanks!\r\n\r\n> Are you running the debug build ? The specified test passes with `-c opt`, but fails with `-c dbg`.\r\n\r\n", "Strange. I built the replay_computation with the below command but still don't repro the bug.\r\n\r\n`bazel build -c dbg --config=cuda --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --output_filter MATCH_NOTHING --disk_cache=/mnt/trentl/bazel_cache.upstream-master/ //tensorflow/compiler/xla/tools:replay_computation_gpu`\r\n\r\nRun with the following command:\r\n`./bazel-bin/tensorflow/compiler/xla/tools/replay_computation_gpu --use_fake_data=true --print_result=false --compile_only=false george.hlo`\r\n", "\r\nYou meant you ran replay_computation or JAX? What is the command you used to build and run tests?\r\n\r\n> Are you running the debug build ? The specified test passes with `-c opt`, but fails with `-c dbg`.\r\n\r\n", "@trentlo could you try\r\n\r\n```\r\ntensorflow/compiler/xla/tools:run_hlo_module -- --platform=CUDA\r\n```\r\n\r\n?", "also turn these two lines into `CHECK` and `CHECK_EQ`: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/xla/shape_util.cc;l=536-537?q=file:shape_util.cc", "The actual error is: `Check failed: shape.IsArray() (f64[6], f64[6], f64[6], f64[6])`", "\r\nDid what you suggested but still no repro. It is good that you confirm that we do see a tuple shape in `Check failed: shape.IsArray()`. This makes sense and it is aligned to the stack dump at the very beginning of the thread. The only explanation to the difference we see might be some code diff between OSS and your internal TF/XLA codebase? Or there could be some configuration differences (e.g., some optimizations are by default off in OSS while on in your internal BUILD)?\r\n\r\nAnother way to debug may be to provide me all the hlos from the testcase jit__cumulative_reduction.115 with `--xla_dump_hlo_pass_re=.*`. What do you think?\r\n \r\nChange the DCHECK to CHECK as suggested and observe the below result. Also try with JAX. BTW, I'm sure my TF is the latest revision from the OSS repro.\r\n\r\n```\r\nxxx@xxx:/opt/tensorflow/tensorflow-source# ./bazel-bin/tensorflow/compiler/xla/tools/run_hlo_module --platform=cuda george.hlo\r\n2021-02-24 20:39:51.354027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\r\n2021-02-24 20:39:51.411459: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\nRunning HLO module on platform CUDA...\r\n... compiled and ran in 1.0083s.\r\nRunning HLO module on platform Interpreter...\r\n... compiled and ran in 0.033454s.\r\n\r\n** Results on cuda and Interpreter are close enough. **\r\n0/1 runs miscompared.\r\n\r\n```\r\n\r\n\r\n", "Ah~ @cheshire, it is my bad. I just realized that I merged the upstream TF so that I also merged the `revert` commit... (It canceled out the \"`Make H-loop-fusion share operands with users`\" changes in my local branch, and the upstream master and my local branch are merged perfectly) That is why I can't repro the bug. I repro it now.\r\n\r\nThanks for all the help. Sorry for the bothering.\r\n"]}, {"number": 46613, "title": "ERRORS ", "body": "\r\n\r\n- Windows 10 64-bit \r\n- python 3.8.3 and pip 20.3.3\r\n\r\nevery time I try to install I get the same error messages:\r\nERROR: Could not find a version that satisfies the requirement tensorflow\r\nERROR: No matching distribution found for tensorflow\r\n\r\nim kind of at a loss for what to do, any help would be appreciated \r\n", "comments": ["Can you please post the full error message? Does it list some TF versions that it tries?", "> Can you please post the full error message? Does it list some TF versions that it tries?\r\n\r\nthat is the whole error message. I key in pip3 install tensorflow like many tutorials suggest and all I get is that. ", "Can you fill in the issue template and also post a full output of `pip install` command?", "@CaptnJake \r\nCould you please update pip using the below commands and check if you are still facing the same issue?\r\n\r\n```pip install --upgrade pip```\r\n```pip install tensorflow```\r\nPlease ensure your tensorflow version is compatible with the python version used, TF 2.4 supports python from 3.6-3.8 . Please, refer the [document](https://www.tensorflow.org/install/source#cpu),Also, could you check if you are using the 64 bit version of Python. TensorFlow is tested and supported only on 64-bit systems. \r\n\r\nPlease refer to similar issues, #44615, #45147, #44775", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46613\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46613\">No</a>\n"]}, {"number": 46612, "title": "Missed \")\"", "body": "Small fix to doc.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46612) for more info**.\n\n<!-- need_sender_cla -->", "@igor-gudich-fpc please sign the CLA ", "No CLA signed, just one letter fix, closing"]}, {"number": 46611, "title": "[ROCm] Fix for a bug in ROCm batchnorm implementation.", "body": "Fixing the bug also makes the unit-test `//tensorflow/python/keras/layers:normalization_test_gpu` pass, so removing the `no_rocm` tag from it as well.\r\n\r\n------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work ", "comments": ["@gbaned gentle ping...seems like this PR got stuck in the merge pipeline :)"]}, {"number": 46610, "title": "Object Detection API to Tensorflow Lite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 2.4.0\r\n- CUDA 11.0\r\n\r\nI have successfully trained my own model using the Object Detection API. \r\nAs a pre-trained model I used ssd_resnet50v1_fpn_640x60.\r\nThe detection works fine.\r\n\r\nNow I want to convert the trained model to Tensorflow Lite.\r\nI used export_tflite_graph_tf2.py and after that the TensorFlow Lite-Konverter-API:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory from export_tflite_graph_tf2.py\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\nThe model.tflite was succesfully created but unfortunately when I run the detection, nothing is detected.\r\ntflite-runtime: 2.5.0\r\nI used this guide https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md and with the coco_ssd_mobilenet_v1_1.0_quant_2018_06_29 everything works.\r\n\r\nI got no errors, just no detection with my own model at all.\r\ndoes anyone have an idea what this can be due to?\r\n\r\n", "comments": ["Can you check error messages in `adb logcat` (if in Android)? I guess randomly picking one model in Object Detection API to convert to TFLite won't work directly. As different models are likely to have different model input and output shapes, you have to change the related inference code in general.", "You're right.\r\nI tried it again with the mobilnet_v2_FPNLite an the conversion worked just fine.\r\nThank you :)", "> I tried it again with the mobilnet_v2_FPNLite an the conversion worked just fine.\r\n\r\n@leo513,\r\nThank you for the update. Please feel free to close the issue if resolved. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46610\">No</a>\n"]}, {"number": 46608, "title": "Deleting a model and clearing the session if other models are still loaded and in use", "body": "In my Python application I load several TF 2.4 / Keras models in parallel and use their `predict()` functions in separate threads. The single GPU in the machine is locked by a Python `Lock`, so that these threads cannot run \"their\" TF models at the same time. Now at some point a thread can decide to delete its currently loaded TF model (via Python's `del` command) and load another one (via `tensorflow.keras.models.load_model()`). However, in that case the other threads with their loaded TF models must not be affected.\r\n\r\nNow I want to free all memory when I delete a model in a thread and load another model. So I thought about calling `tf.keras.backend.clear_session()` after deleting a model. But it seems that is not an option in my case, since other models are still loaded and in use, as described above.\r\n\r\nI might put all these models into their own processes (instead of using threads), but then I might run into the problem that I cannot lock and make use of that single GPU from multiple processes in parallel. So is there another, \"official\" method to properly free all memory after deleting a TF/Keras model in such a multi-model setup?", "comments": ["@haimat \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nPlease move this to closed status if ticket is created in Stackoverflow.", "@Saduf2019\r\nI have [asked this question on StackOverflow](https://stackoverflow.com/questions/65723891/how-to-free-tf-keras-memory-in-python-after-a-model-has-been-deleted-while-othe) as well, however, I have not received a solution or even valid answer so far. Even though I have added a bounty to my question. Is this really such an unusual request that nobody knows the answer to this?", "@haimat Is this related to [this issue](https://github.com/tensorflow/tensorflow/issues/45453).\r\nCan you try running against `tf-nightly` and let us know whether it is still an issue for you. Can you please share a simple standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan Hi, are you sure you linked the correct issue here?", "@haimat Sorry. I was working on the other issue and pasted by mistake. [Here](https://github.com/tensorflow/tensorflow/issues/45453) is the correct issue. Please let me know if you tried running your code with `tf-nightly`. \r\n\r\nCan you please share a simple standalone code to reproduce the issue? Thanks!\r\n\r\n", "Ok thanks, looks fine now."]}, {"number": 46607, "title": "INFO:absl:using experimental converter:if you encountered a problem please file a bug", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@aseelalmasholy \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!"]}, {"number": 46606, "title": "No GPU support for Windows10 + CUDA 11.0 + Tensorflow 2.4.1: Could not load dynamic library 'cudart64_110.dll'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n- TensorFlow installed from (source or binary): pip / packet manager of PyCharm\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip / from PyCharm settings menu but it is pip install tensorflow\r\n- CUDA/cuDNN version: CUDA: cuda_11.0.2_451.48_win10 and cuDNN: 8.0.4: cudnn-11.0-windows-x64-v8.0.4.30\r\n- GPU model and memory: GeForce MX330 with 2GB\r\n\r\n**Describe the problem**\r\nHi Guys, I've been trying for around 6hours to get Tensorflow 2.4.1 working with GPU support and following tutorial from pyimagesearch:\r\nhttps://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/\r\nUsing only CPU it worked fine.\r\nI have followed this guide here:\r\nhttps://www.tensorflow.org/install/gpu\r\nI installed the newest drivers for my GeforceMX330, as well as VisualStudio, CUDA11.0 and the cuDNN 8.0.4 version. Please see my log output\r\n\r\n\r\n**Any other info / logs**\r\n`train_vgg.py --dataset animals --model output/smallvggnet.model --label-bin output/smallvggnet_lb.pickle --plot output/smallvggnet_plot.png\r\n2021-01-22 15:21:23.627735: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-01-22 15:21:23.628081: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n[INFO] loading images...\r\n2021-01-22 15:21:42.850382: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-22 15:21:42.852694: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-01-22 15:21:43.635517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce MX330 computeCapability: 6.1\r\ncoreClock: 1.594GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 52.21GiB/s\r\n2021-01-22 15:21:43.636419: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-01-22 15:21:43.637390: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\r\n2021-01-22 15:21:43.638212: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\r\n2021-01-22 15:21:43.639402: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2021-01-22 15:21:43.640209: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2021-01-22 15:21:43.641187: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2021-01-22 15:21:43.642002: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\r\n2021-01-22 15:21:43.642798: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\r\n2021-01-22 15:21:43.643052: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2021-01-22 15:21:43.645778: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-22 15:21:43.647564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-22 15:21:43.647835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \r\n2021-01-22 15:21:43.648038: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n[INFO] training network...\r\n2021-01-22 15:21:44.544947: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)`", "comments": ["@mbobinger,\r\nPlease take a look at issues [#43193](https://github.com/tensorflow/tensorflow/issues/43193#issuecomment-749465330) and [#45055](https://github.com/tensorflow/tensorflow/issues/45055#issuecomment-732154795) with a similar error log and check if it works. Thanks!", "@amahendrakar thank you a lot, adding the environment variable for cuda fixed it for me, somehow creating the folder in c:/tools/cuda wasn't 100% clear to me.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46606\">No</a>\n", "> @mbobinger,\r\n> Please take a look at issues [#43193](https://github.com/tensorflow/tensorflow/issues/43193#issuecomment-749465330) and [#45055](https://github.com/tensorflow/tensorflow/issues/45055#issuecomment-732154795) with a similar error log and check if it works. Thanks!\r\n\r\nGot the same error - can not set the path because it is not available, but CUDA driver 11.2.135 is installed. Can not install other CUDA version, because I've already the newest... so how to fix it?! Uninstall everything?", "@rednag,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "You have either not installed CUDA correctly (and also cudnn) or you have installed the wrong version, for the TF binary you are trying to use.\r\nInstall : pip install tensorflow-gpu==2.1"]}, {"number": 46605, "title": "Subclassed tf.keras.Model can't access intermediate layer's output", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n*Yes*\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n*Google Colab*\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n*No*\r\n- TensorFlow installed from (source or binary):\r\n*Binary, pip install*\r\n- TensorFlow version (use command below):\r\n*2.4*\r\n- Python version:\r\n*Colab's version: 3.6.9\"\r\n- Bazel version (if compiling from source):\r\n*N/A - Colab*\r\n- GCC/Compiler version (if compiling from source):\r\n*N/A - Colab*\r\n- CUDA/cuDNN version:\r\n*N/A - Colab without GPU*\r\n- GPU model and memory:\r\n*N/A - Colab without GPU*\r\n\r\n**Describe the current behavior**\r\n\r\nAccessing internal layer's output of a sublcassed `Model` leads to `AttributeError: Layer XXX has no inbound nodes` exception. The code to reproduce the issue given below or avaiable [here](https://colab.research.google.com/drive/1w5p68LvzwPnYKzpDm6HzsRsRXlBp35hq?usp=sharing) \r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n  def __init__(self, name='MyModel', **kwargs):\r\n    super(MyModel, self).__init__(name=name, **kwargs)\r\n    self.dense1 = tf.keras.layers.Dense(4, name='layer1')\r\n    self.dense2 = tf.keras.layers.Dense(2, name='layer2')\r\n\r\n  def call(self, inputs, training=None, mask=None):\r\n    x = self.dense1(inputs)\r\n    return self.dense2(x)\r\n\r\nx_in = tf.keras.layers.Input(shape=(2,), dtype=tf.float32)\r\nmy_model = MyModel()\r\ny = my_model(x_in, training=False)\r\nmid_feat = my_model.get_layer('layer1').output\r\n```\r\n\r\nThis happens when using tensorflow `2.4`, however it does work with `2.3`. Something must have changed between the two versions. \r\n\r\n**Describe the expected behavior**\r\n\r\nThe old behaviour, where we can access internal layer's outputs from a model whether it has been build with `functional` API or sublclassed `Model`.\r\n", "comments": ["I ran the issue on tf 2.3,24 and nightly, am able to replicate the issue please find the [gist here](https://colab.research.google.com/gist/Saduf2019/1c88c7d7eba5a379de97c01a5b13154c/untitled507.ipynb).", "@Saduf2019, @rmothukuru, any updates on this ?", "This is intended behavior. (It only happened to work in certain situations in the past, but came at the cost of other bugs).\r\n\r\nWhen you use a subclassed model in a functional model it will not inline the layers of the subclassed model into the outer functional model.  So, it it doesn't make sense to access sub-layer inputs/outputs in the functional style.\r\n\r\nIn the past it would sort-of inline it in a way that could actually cause bugs by e.g. not always correctly capturing variables/layers/call arguments of the subclassed model. (This could become especially problematic if you tried to use the inner layer's outputs to construct a new functional model)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46605\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46605\">No</a>\n", "@tomerk thanks for the feedback.\r\n\r\n> When you use a subclassed model in a functional model it will not inline the layers of the subclassed model into the outer functional model. So, it it doesn't make sense to access sub-layer inputs/outputs in the functional style.\r\n\r\nDoes it means there is no way to access intermediate layes with subclassed model or reuse part of the model in the functional style ? ", "If by intermediate layer access you mean via the functional model layer intermediate inputs/outputs accessing for an already-made model, that is correct. We may come up with a solution at some point in the future for slicing into nested functional models, but it's less clear we'll be able to do so for subclass models in the general case. (Subclass models can support things that the functional api can't represent).\r\n\r\nAnother option to try though (if this would suit your needs), is:\r\n- You can make your nested functional/subclass model output *all* of its intermediate outputs that you will need, then only use some of them in your outer functional model. I'm not 100% sure off the top off my head, but I think you may then be able to get access to these unused intermediate outputs & use them for something else."]}, {"number": 46604, "title": "The Example Code in tf.feature_column.categorical_column_with_identity is not Executable/Complete/Self-Sufficient", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity#linear_model\r\n\r\n## Description of issue (what needs changing): \r\nThe example code cannot be Executed as it is. We have to add the namespaces by searching in the [tensorflow.org site](https://www.tensorflow.org/).\r\n\r\nComplete/Self-Sufficient/Stand-Alone example code will be very helpful, especially for the New Developers.\r\n\r\nSimilar issue was raised in #46203 (was waiting for it to be resolved) but it is closed now.", "comments": []}]