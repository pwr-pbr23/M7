[{"number": 43734, "title": "'@grpc//:gpr_base' failed (Exit 1)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14 and 1.13\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip \r\n- Bazel version (if compiling from source): 0.24.1 \r\n- GCC/Compiler version (if compiling from source): gcc-4.8.5 for bazel and gcc-8 for cuda libary\r\n- CUDA/cuDNN version: 10.1 / 7\r\n- GPU model and memory: 1070 ti 8gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nI can't compile from tensorflow 1.14-1.13 source code (haven't tried other 1 tensorflow branches) and 2.4 has been successfully compiled with and without GPU. Originally the error was with newer versions of basel and gcc-8, but after reading the https://www.tensorflow.org/install/source table, I installed gcc-4.8, but the error persisted.\r\n\r\n\r\nI have read the solutions\r\nhttps://github.com/tensorflow/serving/issues/928 but adding the --cxxopt = -std = c ++ 11 flag didn't work for me\r\nI also read that installing libc-ares-dev library might help, but that didn't help either\r\n\r\nI thought the problem might be related to the fact that I am using cuda-10.1 and not 10.0 as in the table. I tried to set up the file without cuda, but I got exactly the same error, so I think this is not the problem\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n**writte config file**\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.8/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.8/dist-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: \r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-8\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: -march=native -mno-avx\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n\r\n**build command**\r\nbazel build --cxxopt=-std=c++11 --config=cuda  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**output error**\r\nINFO: From ProtoCompile external/com_github_googleapis_googleapis/google/api/http.pb.h:\r\nbazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From ProtoCompile external/com_github_googleapis_googleapis/google/rpc/status.pb.h:\r\nbazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\nINFO: From Compiling external/mkl_dnn/src/cpu/simple_concat.cpp:\r\nexternal/mkl_dnn/src/cpu/simple_concat.cpp:98: warning: ignoring #pragma omp simd [-Wunknown-pragmas]\r\n             PRAGMA_OMP_SIMD()\r\n \r\nINFO: From Compiling external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:\r\nexternal/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc: In function 'grpc_error* try_http_parsing(grpc_chttp2_transport*)':\r\nexternal/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:2466:40: warning: 'void* memset(void*, int, size_t)' clearing an object of non-trivial type 'grpc_http_response' {aka 'struct grpc_http_response'}; use assignment or value-initialization instead [-Wclass-memaccess]\r\n   memset(&response, 0, sizeof(response));\r\n                                        ^\r\nIn file included from external/grpc/src/core/ext/transport/chttp2/transport/chttp2_transport.cc:44:\r\nexternal/grpc/src/core/lib/http/parser.h:71:16: note: 'grpc_http_response' {aka 'struct grpc_http_response'} declared here\r\n typedef struct grpc_http_response {\r\n                ^~~~~~~~~~~~~~~~~~\r\nERROR: /home/dmitry/.cache/bazel/_bazel_dmitry/1319b3f3ad6252a51422db144992f79d/external/grpc/BUILD:507:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: error: ambiguating new declaration of 'long int gettid()'\r\n static long gettid(void) { return syscall(__NR_gettid); }\r\n             ^~~~~~\r\nIn file included from /usr/include/unistd.h:1170,\r\n                 from external/grpc/src/core/lib/gpr/log_linux.cc:41:\r\n/usr/include/x86_64-linux-gnu/bits/unistd_ext.h:34:16: note: old declaration '__pid_t gettid()'\r\n extern __pid_t gettid (void) __THROW;\r\n                ^~~~~~\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: warning: 'long int gettid()' defined but not used [-Wunused-function]\r\n static long gettid(void) { return syscall(__NR_gettid); }\r\n             ^~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 281.090s, Critical Path: 57.54s\r\nINFO: 1539 processes: 1539 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n\r\n(there was even more INFO output, but I did not copy everything, there really is a lot)\r\n", "comments": ["@Int37,\r\nIs there any specific reason you are using TensorFlow 1.x? TensorFlow 1.x is not actively supported, could you please update TensorFlow to v2.3? \r\n\r\nAlso, please go through similar issues [#34291](https://github.com/tensorflow/tensorflow/issues/34291#issuecomment-624767789\r\n) and [#33758](https://github.com/tensorflow/tensorflow/issues/33758#issuecomment-547143723) and let us know if it helps. Thanks!\r\n\r\n", "@amahendrakar,  thanks for your answer,  I need tf.contrib elements to use the code: https://github.com/vlomme/Multi-Tacotron-Voice-Cloning, also most github projects still require version 1.14 or less. I could just download via pip, but my processor does not support avx instructions, without which tensorflow does not work even on gpu. i did what is written here  https://github.com/tensorflow/tensorflow/issues/33758#issuecomment-547143723 I  , it gave the same error, but in a different place, after a while\r\n**output bazel**\r\n   ^~~~~\r\nINFO: From Compiling tensorflow/cc/training/queue_runner.cc:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/cc/training/coordinator.h:26,\r\n                 from ./tensorflow/cc/training/queue_runner.h:24,\r\n                 from tensorflow/cc/training/queue_runner.cc:16:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:470:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:444:29: warning: comparison of integer expressions of different signedness: 'int' and 'absl::Span<const long long int>::size_type' {aka 'long unsigned int'} [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:444:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^~~~~\r\nINFO: From Compiling tensorflow/core/kernels/nth_element_op.cc:\r\nIn file included from ./tensorflow/core/framework/register_types.h:22,\r\n                 from tensorflow/core/kernels/nth_element_op.cc:23:\r\n./tensorflow/core/framework/variant.h: In member function 'void tensorflow::Variant::HeapOrInline::ResetMemory()':\r\n./tensorflow/core/framework/variant.h:653:45: warning: 'void* memset(void*, int, size_t)' clearing an object of type 'union tensorflow::Variant::HeapOrInline' with no trivial copy-assignment; use value-initialization instead [-Wclass-memaccess]\r\n       this, 0, sizeof(Variant::HeapOrInline));\r\n                                             ^\r\n./tensorflow/core/framework/variant.h:496:9: note: 'union tensorflow::Variant::HeapOrInline' declared here\r\n   union HeapOrInline {\r\n         ^~~~~~~~~~~~\r\nINFO: From Compiling tensorflow/core/grappler/clusters/virtual_cluster.cc:\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/framework/allocator.h:28,\r\n                 from ./tensorflow/core/common_runtime/device.h:35,\r\n                 from ./tensorflow/core/common_runtime/device_set.h:23,\r\n                 from ./tensorflow/core/grappler/clusters/virtual_cluster.h:21,\r\n                 from tensorflow/core/grappler/clusters/virtual_cluster.cc:16:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:470:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:444:29: warning: comparison of integer expressions of different signedness: 'int' and 'absl::Span<const long long int>::size_type' {aka 'long unsigned int'} [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:444:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^~~~~\r\nERROR: /home/dmitry/.cache/bazel/_bazel_dmitry/1319b3f3ad6252a51422db144992f79d/external/grpc/BUILD:507:1: C++ compilation of rule '@grpc//:gpr_base' failed (Exit 1)\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: error: ambiguating new declaration of 'long int gettid()'\r\n static long gettid(void) { return syscall(__NR_gettid); }\r\n             ^~~~~~\r\nIn file included from /usr/include/unistd.h:1170,\r\n                 from external/grpc/src/core/lib/gpr/log_linux.cc:41:\r\n/usr/include/x86_64-linux-gnu/bits/unistd_ext.h:34:16: note: old declaration '__pid_t gettid()'\r\n extern __pid_t gettid (void) __THROW;\r\n                ^~~~~~\r\nexternal/grpc/src/core/lib/gpr/log_linux.cc:43:13: warning: 'long int gettid()' defined but not used [-Wunused-function]\r\n static long gettid(void) { return syscall(__NR_gettid); }\r\n             ^~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2558.765s, Critical Path: 296.59s\r\nINFO: 2613 processes: 2613 local.\r\nFAILED: Build did NOT complete successfully\r\n", "> /usr/local/lib/python3.8/dist-packages\r\n\r\n@Int37,\r\nLooks like you are using Python 3.8. As per the [tested build configurations](https://www.tensorflow.org/install/source#gpu), TensorFlow 1.x is compatible with Python 3.3 - 3.7, please take a look at the below table for reference.  \r\n\r\n\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-1.15.0 | 2.7, 3.3-3.7 | GCC 7.3.1 | Bazel 0.26.1 | 7.4 | 10.0\r\ntensorflow_gpu-1.14.0 | 2.7, 3.3-3.7 | GCC 4.8 | Bazel 0.24.1 | 7.4 | 10.0\r\ntensorflow_gpu-1.13.1 | 2.7, 3.3-3.7 | GCC 4.8 | Bazel 0.19.2 | 7.4 | 10.0\r\n\r\nCould you please check if you are facing the same issue with Python 3.7 as well? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43734\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43734\">No</a>\n"]}, {"number": 43733, "title": "CUDA 10.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux CentOS 7.8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n.a.\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.3.1\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?: n.a.\r\n- Bazel version (if compiling from source): 3.1.0 (bazelisk)\r\n- GCC/Compiler version (if compiling from source):7.3.1\r\n- CUDA/cuDNN version:10.1/7.6\r\n- GPU model and memory:n.a.\r\n\r\n\r\n\r\n**Describe the problem**\r\nCompilation error when compiling tensorflow 2.3.1. with CUDA 10.1:\r\n\r\nERROR: /build/tensorflow/tensorflow/stream_executor/cuda/BUILD:457:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:cusparse_stub' failed (Exit 1)\r\nIn file included from tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0:\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nTried to follow the build instructions from source with the recommend/tested configuration:\r\nhttps://www.tensorflow.org/install/source#tested_build_configurations\r\n\r\nHere is my configure log:\r\n[configure.txt](https://github.com/tensorflow/tensorflow/files/5318180/configure.txt)\r\n\r\nHere is a minimal build command to reproduce the issue:\r\nBAZEL_LINKLIBS=-l%:libstdc++.a bazel build --config=nohdfs --config=noaws --config=nogcp --config=nonccl  --config=monolithic --config=cuda --config=v2 --config=opt  //tensorflow/stream_executor/cuda:cusparse_stub\r\n\r\nNote: I had to add BAZEL_LINKLIBS=-l%:libstdc++.a for CentOS, see \r\nhttps://github.com/tensorflow/tensorflow/issues/35867\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAs you can see in the build log, the error occurs in tensorflow/stream_executor/cuda/cusparse_stub.cc:59:0. I assume this is because of wrong version of tensorflow/stream_executor/cuda/cusparse_10_1.inc.\r\nThe file is identical to cusparse_10_2.inc and it works with CUDA 10.2\r\n\r\n[build.txt](https://github.com/tensorflow/tensorflow/files/5318181/build.txt)\r\n", "comments": ["@geeheim \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/43720#issuecomment-703869823) and let us know if it helps.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43733\">No</a>\n"]}, {"number": 43731, "title": "Fix bug: \"Unsupported activation type for quantize-activation: 0\"", "body": "When running unit tests from custom-op docker image, we got the following errors:\r\n```\r\n12:17:10  //tensorflow/lite/tools/optimize:quantize_model_test                     FAILED in 3 out of 3 in 0.3s\r\n12:17:10    Stats over 3 runs: max = 0.3s, min = 0.2s, avg = 0.3s, dev = 0.1s\r\n12:17:10      FAILED  QuantizeConstInputTestInst/QuantizeConstInputTest.VerifyConstOpInput/0 (0.0s)\r\n12:17:10      FAILED  QuantizeConstInputTestInst/QuantizeConstInputTest.VerifyConstOpInput/1 (0.0s)\r\n12:17:10  Test cases: finished with 118 passing and 2 failing out of 120 test cases\r\n```\r\n\r\nAfter investigating it, we have found that the fail can be fixed by properly accessing the parametrized test's parameter with the GetParam() method.", "comments": ["Incorporated into other PR, abandoning it."]}, {"number": 43730, "title": "NASNetLarge Keras Model - accuracy is very very low", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Issue happening on the Server while training\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0 and cuDNN 7.6.5\r\n- GPU model and memory: 2 GPU Tesla K80 and total memory 22 GB\r\n\r\n\r\n**Describe the current behavior**\r\nKeras version 2.3.0\r\nTensorflow-gpu version 2.0.0\r\n\r\nDataset: Imagenet cars dataset\r\nPreprocessing : Resizing images to 331 * 331 pixel using custom code and saving as new files, no other preprocessing\r\n\r\nI tested the NASNetLarge Keras model with parameters as below\r\n\r\n```\r\nfrom keras.applications.nasnet import NASNetLarge \r\nmodel=NASNetLarge(input_shape =  (331, 331, 3),include_top = False,  weights = 'imagenet')\r\nmodel.trainable = False\r\n\r\n```\r\nand created combined model using Sequential\r\nTraining Data and valid data processed through flow_from_directory and fed fit_generator \r\n\r\nLoss : approx 5.0\r\nval_acc: approx 0.00813\r\n\r\n", "comments": ["@suresh-s,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. \r\n\r\nAlso, please update TensorFlow to the latest version v2.3 and check if you are facing the same issue. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43729, "title": "AutoGraph could not transform", "body": "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000022F3CE53430> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: \r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert", "comments": ["@xuxinyang \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/37144#issuecomment-600350256) and let us know.\r\nSimilar issues: #38947 #41120 [link](https://stackoverflow.com/questions/62931610/warningtensorflowautograph-could-not-transform-function-format-example-at)\r\nWe see that you have not filled in the issue template, can you please try in the latest tf version(nightly) and let us know if the issue exist.", "@xuxinyang Can you please share a simple standalone code to reproduce the issue? Thanks!", "```\r\nOS: window10 Pro\r\nDevelopment platform:Anaconda\r\nPython: 3.8.3\r\nTensorflow: 2.2.0\r\nCode:\r\n----------------------------------------------------------------------------------------------------------------------\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# add dataset fashion\u2014\u2014mnist\r\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\r\nprint(train_images.shape)\r\n\r\n\r\n# image expand dimension\r\ntrain_images = np.expand_dims(train_images, -1)\r\ntest_images = np.expand_dims(test_images, -1)\r\n\r\n# build Sequential() model\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(32, (3,3), input_shape=(28,28,1), activation='relu', padding='same'))\r\nmodel.add(tf.keras.layers.MaxPooling2D())\r\nmodel.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))\r\nmodel.add(tf.keras.layers.GlobalAveragePooling2D())\r\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\r\nprint(model.output_shape)\r\n\r\nmodel.compile(\r\n    optimizer='adam',\r\n    loss='sparse_categorical_crossentropy',\r\n    metrics=['acc'])\r\n\r\n# save model training process\r\nhistory = model.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))\r\n```", "@xuxinyang \r\ni ran the code shared on tf 2.2 and tf nightly and do not face any errors please find the [gist here for nightly](https://colab.research.google.com/gist/Saduf2019/7409ab49eca8b5fe7cbac89e4153349b/untitled418.ipynb) and [tf 2.2](https://colab.research.google.com/gist/Saduf2019/d8c5bcf1a7b5fed844f6fce3fa9a628f/untitled424.ipynb).\r\nCould you let us know if you have gone through the link shared, and try your code on nightly and let us know if the issue exist.", "OK! I ran the code on nightly, the issue not exist! Thank you! Maybe some other factors\uff01", "@xuxinyang \r\nGlad the issue is resolved for you, can you please move this to closed status, thanks for your update!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43729\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43729\">No</a>\n"]}, {"number": 43728, "title": "multi-core support ", "body": "I have a question regarding the use of TensorFlow Microcontroller in the case of some multicore device (ARM CortexM cores). How to use TensorFlow Microcontroller within a multi-core device and switch between cores (heterogenous)?", "comments": ["Currently  TF Micro does not have a multi-core support.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43727, "title": "0.6.0", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43727) for more info**.\n\n<!-- need_sender_cla -->", "What are you trying to do? Development only happens on master branch\r\n\r\nIt seems this is a Hacktoberfest spam PR."]}, {"number": 43726, "title": "TFlu: Add Cortex-M generic target makefile", "body": "This is fixing issue: https://github.com/tensorflow/tensorflow/issues/43725\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@mansnils  Can you please resolve conflicts? Thanks!", "Hi @mansnils, I tried your PR. Yes, it goes into the right direction. I encountered two problems with it:\r\n\r\n1.  The gcc executable was not found. I had to add the following lines to cortex_m_generic_makefile.inc\r\n$(eval $(call add_third_party_download,$(GCC_EMBEDDED_URL),$(GCC_EMBEDDED_MD5),gcc_embedded,))\r\n`export PATH := $(MAKEFILE_DIR)/downloads/gcc_embedded/bin/:$(PATH)\r\n`\r\n\r\n2. The CMSIS include directories were not added the INCLUDE. This happens if CMSIS_PATH is not populated. Question here: why does cmsis_nn.inc add the include dirs of Core, DSP, NN only of CMSIS_PATH is undefined?", "@ml-0 Thanks for your comments.\r\n\r\n1) Will fix this.\r\n\r\n2) My understanding is that INCLUDES are not allowed in cmsis.inc because Arduino can't handle it. See the RFC document for more details: https://docs.google.com/document/d/14GRxeVEgSKgKBKAijO7oxnI49nLoTYBFQmPok-rG0cw\r\nAnd that's ok when using default CMSIS_PATH in tensorflow/lite/micro/tools/make/downloads/cmsis which gets patched. However when supplying a custom CMSIS_PATH when building, INCLUDES are needed. Assumption is that Arduino use the default path. (Actually jacking in a CMSIS_PATH have some other problems basically because it is unpatched but it is out of scope for this PR.)", "@mansnils, I found that the issue of missing CMSIS headers was due to an incomplete patching on my system (Win10/msys64). I filed a fix for it (#44001). After this is merged, I can use the new make target without adding the CMSIS include paths to INCLUDE.\r\n ", "@mansnils, I pushed a couple of commits to resolve merge conflicts and make some other cosmetic changes. I'm going to try and get this merged and we can follow-up with additional smaller PRs to tie up any loose ends that might remain, such as https://github.com/tensorflow/tensorflow/issues/43865"]}, {"number": 43725, "title": "There is no way to build Cortex-M55", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source): 1f9328f3a4662085b2cb948b9ed2b03d7259f78b\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nThere is a need to build for generic Cortex-M devices. There has been some PRs in the right directions lately, e.g. this one, which has good debug support: https://github.com/tensorflow/tensorflow/pull/41860\r\nBut it does not support, e.g. Cortex-M55.\r\n", "comments": ["Cortex-M55 support could easily be added to the generic Cortex-M Makefile. However, to my understanding the M55 is only supported by GCC version 10. TFLM currently downloads GCC version 7.\r\nI spent some effort in compiling for M55 using the GCC 10 preview without success. Best result I got with\r\n\r\n    -mcpu=cortex-m55 \\\r\n    -mfpu=auto \\\r\n    -mfloat-abi=hard\r\n\r\nwhere it compiles a couple of files and then terminates with\r\n`Error: instruction not allowed in IT block -- `vmullt.f32 s15,s15,s14'`\r\nwhen compiling\r\n./tensorflow/lite/micro/kernels/comparisons.cc\r\n\r\nAnyone who could help?\r\n", "GCC support for Cortex-M55 is perhaps a bit experimental still but there is a PR for this issue: https://github.com/tensorflow/tensorflow/pull/43726\r\n", "@ml-0 How about we merge your PR with the new PR, basically replace cortex_m_gcc_generic_makefile.inc with cortex_m_generic_makefile.inc? \r\nWe have a need to build other Cortex-M, other than Cortex-M4 and Cortex-M55 and with both Armclang and GCC as well.", "I am open to add other Cortex-M targets based on the gcc toolchain. Adding other toolchains to the same Makefile (and renaming it accordingly) is ok, too, as long as it integrates well and doesn't make its use too complicated. My concerns are:\r\n- armcc/armclang are proprietary and will not be downloadable by an entry in third_party_downloads.inc.\r\n- armcc/armclang require a license to operate\r\n- armcc/armclang require different CFLAGS/CXXFLAGS\r\n- All combinations of targets and toolchains build should be testable on the TFLM CI. Does it support the armcc/armclang toolchain?\r\n\r\nI am not sure how this translates into a makefile. @mansnils Can you make a suggestion?\r\n\r\n@advaitjain you will probably have the final speak on this.", "That sounds good. Idea is that it should support both GCC and Armclang so anyone without a license can always use GCC. I am working at Arm and we are working on getting a License for TFLM CI. I propose that I update the PR (https://github.com/tensorflow/tensorflow/pull/43726) to reflect this.", "Having a single cortex_m_generic_makefile.inc sounds good to me.\r\n\r\nI would propose that we continue to use the callback registration that @ml-0 has implemented:\r\nhttps://github.com/tensorflow/tensorflow/blob/d7bb5785ced563f113bdbbb3e7ecb1d5f06e187f/tensorflow/lite/micro/cortex_m_gcc_generic/debug_log_callback.h#L18-L33\r\n\r\n@mansnils if you could update PR #43726 then I can take a look.", "@mansnils can you comment why TFLM does not compile for M55 with GCC 10.1.1 toolchain? Is it a problem located in TFLM or on the GCC side? ", "Hi,\r\n\r\nThis is an issue with the GNU assembler that has been fixed with https://sourceware.org/pipermail/binutils/2020-April/110799.html\r\n\r\nThis fix should be picked up by the December major release of the GNU Arm Embedded toolchain, which will be made available at: https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm\r\n\r\nHope that helps.", "@ml-0 Till then, you can use [Arm Compiler](https://developer.arm.com/tools-and-software/embedded/arm-compiler/downloads/version-6) 6.14 and upwards. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43725\">No</a>\n"]}, {"number": 43724, "title": "How it is done convolution calculation order in conv2d?", "body": "I'm worried about cancellation of significant digits by convolution calculation order in conv2d.\r\nI tried coding on my own on Python(using tf.math.add etc.), could not be reproduced Tensorflow API conv2d output.\r\nDo you know detailed internal processing or original source code in API conv2d?\r\n\r\nDevelopment environment:\r\n    Python 3.6\r\n    TensorFlow 2.1.0\r\n    JupyterNotebook\r\n\r\n", "comments": ["@hrktngc \r\nWe see that you have not filled in the issue template, please share with simple indented stand alone code to replicate the issue faced or if possible share a colab gist with the issue reported.\r\nCan you try on tf nightly and see if the issue exist.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43723, "title": "XLA Compilation Bug", "body": "**System information**\r\nColab with TensorFlow 2.4.0-dev20201001\r\n\r\n**Describe the current behavior**\r\nWhen using `experimental_compile=True`, I get the following error:\r\n\r\n```\r\nInvalidArgumentError: Output shapes of then and else branches do not match: (f32[100,1]) vs. (f32[100])\r\n\t [[{{node gradient_tape/loop_body/logistic_loss/Select/pfor/cond_1}}]] [Op:__inference_train_step_2474]\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe code should run without errors just like with `experimental_compile=False`\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1TqFDfhxeM6JueuNYbiVBKj1gdUMw03GG?usp=sharing\r\n", "comments": ["I have tried in colab with TF nightly version(`2.4.0-dev20201004`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a8888d1e368e78b9660feb932a434133/untitled422.ipynb).Thanks!", "Was able to run your code successfully without any errors in Tf Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ff0541286012b1e1837ad364a67e9ecf/43723.ipynb).\r\nClosing this issue since it is fixed, feel free to reopen. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43723\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43723\">No</a>\n"]}, {"number": 43722, "title": "Update api_template_v1.__init__.py", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43722) for more info**.\n\n<!-- need_sender_cla -->", "@vraag00 Can you please sign CLA. Thanks!", "Please don't spam with Hacktoberfest PRs. You can get the tshirt by doing 4 real PRs, too much spam might remove you from the process."]}, {"number": 43721, "title": "TFBertMainLayer cannot be loaded from .h5", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: GeForce RTX 2080ti \r\n\r\n**The Issue**\r\nWhen I try to load the model according to the [documentation](https://www.tensorflow.org/tutorials/keras/save_and_load), I face with following issue:\r\n\r\nRunning code:\r\n```python\r\nnew_model.model = tf.keras.models.load_model(os.path.join(load_folder_path, trans_model_name))\r\n```\r\n\r\nError:\r\n```python\r\nValueError: Unknown layer: Custom>TFBertMainLayer\r\n```\r\n\r\nTraceback (most recent call last):\r\n```python\r\n  File \"bert_nlu_basic_api.py\", line 105, in <module>\r\n    initialize()\r\n  File \"bert_nlu_basic_api.py\", line 46, in initialize\r\n    model = JointTransBertModel.load(load_folder_path)\r\n  File \"/home/hakan/Documents/HB/dialog-nlu/models/joint_trans_bert.py\", line 42, in load\r\n    return BaseJointTransformerModel.load_model_by_class(BaseJointTransformerModel, load_folder_path, 'joint_bert_model.h5')\r\n  File \"/home/hakan/Documents/HB/dialog-nlu/models/base_joint_trans.py\", line 121, in load_model_by_class\r\n    new_model.model = tf.keras.models.load_model(os.path.join(load_folder_path, trans_model_name))\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 182, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 178, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 175, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 358, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py\", line 617, in from_config\r\n    config, custom_objects)\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py\", line 1204, in reconstruct_from_config\r\n    process_layer(layer_data)\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py\", line 1186, in process_layer\r\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 175, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 347, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"/home/hakan/.local/share/virtualenvs/dialog-nlu-TCo_F89F/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 296, in class_and_config_for_serialized_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\nValueError: Unknown layer: Custom>TFBertMainLayer\r\n```", "comments": ["@redrussianarmy Looks like you have a custom layer in your model. When you save a model (that has a custom layer) in `h5` format, then you need to use `custom_objects` when loading the model as described [here](https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects). \r\n\r\nFor example\r\n`new_model = keras.models.load_model('model.h5',custom_objects={'CustomMetric':CustomMetric()})`", "Thank you. I will try.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43721\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43721\">No</a>\n", "@redrussianarmy was it possible for you to load the .h5 model with a custom layer (the way @jvishnuvardhan suggested) even if you didn't use get_config() method while saving it?"]}, {"number": 43720, "title": "compatibility issue with cuda 11.1", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0 master\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): 3.5.1\r\n- GCC/Compiler version (if compiling from source):  visual studio 2019\r\n- CUDA/cuDNN version: 11.1/ 8.0.4\r\n- GPU model and memory: RTX2070 super \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\ncudart library not loaded\r\n**Describe the expected behavior**\r\ncudart dll should be loaded properly\r\n**Standalone code to reproduce the issue**\r\n```\r\npython\r\nimport tensorflow as tf\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\ncudart64 library name for cuda 11.1 is cudart64_110.dll,  not cudart64_111.dll", "comments": ["also emit error in ubuntu", "@alanpurple \r\nPlease share complete error log for us to analyse, also could you try with cuda 11.0 and not cuda 11.1 and let us know.Thank!", "@nluehr has a fix for this which should land soon.", "@sanjoy \r\nthank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43720\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43720\">No</a>\n", "Did the fix already land? Where can I find it?\r\nRegards Geert\r\n", "@geeheim It landed in the master branch on Oct 7th. https://github.com/tensorflow/tensorflow/commit/68a6fe0d984377b625b421d34f8c607d6ed73597"]}, {"number": 43719, "title": "Float16 quantized DistilBERT model performs poorly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.3\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI used the DistilBERT model for a text classification task with the SST-2 dataset. Now, when I use the float16 quantization recipe to convert the trained model to TensorFlow Lite the performance drops significantly (90% validation accuracy to ~50%). Upon investigating more I found out the TensorFlow Lite float16 quantized model always outputs `nan`. I am not sure why this is the case. When I convert the original model using dynamic-range quantization the performance is retained greatly. \r\n\r\n**Describe the expected behavior**\r\n\r\nThe float16 quantized model should not return `nan`.\r\n\r\n**Standalone code to reproduce the issue**\r\n- [Model training and conversion notebook](https://github.com/sayakpaul/BERT-for-Mobile/blob/master/DistilBERT_SST-2_TPU.ipynb)\r\n- [TensorFlow Lite model evaluationn notebook](https://colab.research.google.com/gist/sayakpaul/705d53e72f0f5722461f892150fde8b0/evaluation_sst-2_distilbert.ipynb)\r\n\r\nThis is a part of a project called [BERT for Mobile](https://github.com/sayakpaul/BERT-for-Mobile/) where I plan to compare two different BERT models designed specifically for mobile deployments - DistilBERT and MobileBERT. Currently, I have only taken text classification as the NLP task. I plan to also include question answering in the future. \r\n\r\nCc: @MeghnaNatraj @khanhlvg ", "comments": ["Hi. Any updates on this? ", "@sayakpaul we're looking into this now, will provide an update as soon as we have one.", "Hi,\r\n\r\nI tried to reproduce this but was not successful. I executed the two models in a Python script like this:\r\n\r\n```\r\n import numpy as np\r\n\r\n  # Load the TFLite model and allocate tensors.\r\n  fp32_model = \"./distilbert_fp32.tflite\"\r\n  interpreterf32 = Interpreter(model_path=fp32_model)\r\n  interpreterf32.allocate_tensors()\r\n  fp16_model = \"./distilbert_fp16.tflite\"\r\n  interpreterf16 = Interpreter(model_path=fp16_model)\r\n  interpreterf16.allocate_tensors()\r\n\r\n  # Get input and output tensors.\r\n  input_details = interpreterf32.get_input_details()\r\n  print(input_details)\r\n  output_details = interpreterf32.get_output_details()\r\n\r\n  # Test the model on random input data.\r\n  input_shape0 = input_details[0]['shape']\r\n  input_shape1 = input_details[1]['shape']\r\n  input_data0 = np.array(np.random.random_sample(input_shape0), dtype=np.int32)\r\n  input_data1 = np.array(np.random.random_sample(input_shape1), dtype=np.int32)\r\n\r\n  interpreterf32.set_tensor(input_details[0]['index'], input_data0)\r\n  interpreterf32.set_tensor(input_details[1]['index'], input_data1)\r\n  interpreterf32.invoke()\r\n  output_dataf32 = interpreterf32.get_tensor(output_details[0]['index'])\r\n  print(output_dataf32)\r\n\r\n  interpreterf16.set_tensor(input_details[0]['index'], input_data0)\r\n  interpreterf16.set_tensor(input_details[1]['index'], input_data1)\r\n  interpreterf16.invoke()\r\n  output_dataf16 = interpreterf16.get_tensor(output_details[0]['index'])\r\n  print(output_dataf16)\r\n\r\n```\r\n\r\nbut when I executed, I got an expected output (a float value in each case, fp16 quantized model producing a value that is similar but not exactly the same as the fp32 model):\r\n\r\n```\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\n[{'name': 'serving_default_bert_input_ids:0', 'index': 0, 'shape': array([  1, 128], dtype=int32), 'shape_signature': array([ -1, 128], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_bert_input_masks:0', 'index': 1, 'shape': array([  1, 128], dtype=int32), 'shape_signature': array([ -1, 128], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n[[0.4656006]]\r\n[[0.46526226]]\r\n```\r\n\r\nI'm not sure how to proceed. Is it easy for you to attempt converting the model again to TFL using the latest code?\r\n", "Also, please confirm that if the problem is reproducing, I should see `Nan` as the output. thanks!", "Thanks for looking into this, @talumbau. Did you use the same notebooks (linked in the issue above) to generate the fp32 and fp16 models? Also, which TensorFlow version did you use? ", "My colleague generated the graphs using your notebooks. I believe it was TF 2.3. Over to @khanhlvg to confirm.", "I see. Could it be because you used random data as the inputs?\r\n\r\nI think it's probably best to use [this notebook](https://colab.research.google.com/gist/sayakpaul/705d53e72f0f5722461f892150fde8b0/evaluation_sst-2_distilbert.ipynb) if you want reproduce the results. ", "Is it your hypothesis that an input to the fp16 model produces Nans or do you have an example of an input to the fp16 model that causes a Nan? If it is the latter, can you provide such an input?", "It's the latter. You can find the notebook from my previous comment to reproduce the same. ", "I am able to reproduce and am attempting to debug now.", "It appears that very large values are stored in one of the float buffers of the float32 model. To be precise, Tensor at index 74 in the float32 DistilBERT model is filled with all values `1000000015047466219876688855040.0` (1.0000000150474662e+30). This exceeds the range of the float16 type. The corresponding buffer in the fp16 model (Tensor at index 749, which is the output Tensor of the Dequantize Op taking Tensor 74) has all values `inf` which appears to be the default way to resolve the cast of float32 to float16 when the value exceeds the max float16 value. The `inf` values are multiplied by `0`, with the result of `nan`, which then begins to propagate through the model. The damage is already done by the time we dequantize the fp16 nan to float32 nan, so one option might be to clamp to the max value of the float16 value when we are quantizing the weights tensors to float16. I will try converting the float32 Distilbert model with this option and re-run to see the impact on accuracy.", "Thank you for this insight. Do you think there might be similar reasons for [this issue](https://github.com/tensorflow/tensorflow/issues/44306)?\r\n\r\n", "For that issue, it doesn't look like float16 quantization is involved, so I suspect it's a different underlying problem.", "Yeah right. It involves dynamic-range. The broader problem is the same - terrible drop in the performance although I understand it may be a different conversion problem. ", "@sayakpaul \r\nThis commit should have fixed the fp16 issue and will be included in tf-nightly build tomorrow.\r\nhttps://github.com/tensorflow/tensorflow/commit/612a5fb91ed6a6c229c5f4932307747699cabe90", "Thanks for letting me know @khanhlvg. I think we found out an important numerical issue here and I am really glad the TFLite team took the time to investigate it. THANK yOU!", "Just a note to say that I converted the f32 DistilBERT model with https://github.com/tensorflow/tensorflow/commit/612a5fb91ed6a6c229c5f4932307747699cabe90 applied and re-ran the inputs. No `nan`s were in the output and the agreement was good, so I think it's appropriate to close. Thanks for finding and reporting!", "@talumbau just to confirm, you used fp16 quantization during the conversion right?", "Yes correct. The output looked like this (ignore the debug printing):\r\n\r\n```\r\nindex= 0\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.999979]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\nInference result with fp16 model: [0.9999791]\r\nindex= 1\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.04669181]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.04674396]\r\nindex= 2\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.99949145]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.99949086]\r\nindex= 3\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.99908924]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.9990898]\r\nindex= 4\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.00043762]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.00043774]\r\nindex= 5\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.99986213]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.999862]\r\nindex= 6\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.00101683]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.0010168]\r\nindex= 7\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here [657/1955] Inference result with fp32 model: [0.0096097]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.00960544]\r\nindex= 8\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.8588852]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.85885197]\r\nindex= 9\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.00107691]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.00107661]\r\nindex= 10\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.99995357]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.99995345]\r\nindex= 11\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.00048542]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.00048554]\r\nindex= 12\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.00036857]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.0003686]\r\nindex= 13\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.40758246]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.40804586]\r\nindex= 14\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.00037163]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.00037152]\r\nindex= 15\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.999975]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.999975]\r\nindex= 16\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.99706167]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.99706614]\r\nindex= 17\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.9996885]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.9996884]\r\nindex= 18\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.0014008]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.00140226]\r\nindex= 19\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 600 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp32 model: [0.0035983]\r\nINFO: TfLiteFlexDelegate delegate: 6 nodes delegated out of 712 nodes with 6 partitions.\r\n\r\ncasting to float here\r\nInference result with fp16 model: [0.00360444]\r\n```"]}, {"number": 43718, "title": "RTX 3090 Cuda 10.1 Kernal image error", "body": "**System information**\r\nRTX3090, AMD 3950x\r\n\r\nNvidia Driver 455.23\r\n\r\nFri Oct  2 00:41:55 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 3090    On   | 00000000:0D:00.0  On |                  N/A |\r\n| 30%   33C    P8    31W / 350W |    590MiB / 24265MiB |     24%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1881      G   /usr/lib/xorg/Xorg                350MiB |\r\n|    0   N/A  N/A      2159      G   /usr/bin/gnome-shell               52MiB |\r\n|    0   N/A  N/A      2560      G   ...AAAAAAAAA= --shared-files       47MiB |\r\n|    0   N/A  N/A      2565      G   ...token=9020920654293169663       35MiB |\r\n|    0   N/A  N/A      3061      G   ...mviewer/tv_bin/TeamViewer       35MiB |\r\n|    0   N/A  N/A      5620      G   ...AAAAAAAAA= --shared-files       64MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nNVCC:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n\r\nTensorflow version:\r\nv2.3.0-54-gfcc4b966f1 2.3.1\r\n\r\n**Describe the current behavior**\r\n<pre>2020-10-02 00:44:35.590969: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nObserved TF Version:  2.3.1\r\nObserved Numpy Version:  1.18.5\r\nDirectory already exists!\r\nObservation Length:  28693\r\nLinear Length:  28693\r\nAngular Length:  28693\r\nLoad all complete\r\nCurrently using multiple GPUs\r\n2020-10-02 00:44:37.843072: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-02 00:44:37.876858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 00:44:37.877518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:0d:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2020-10-02 00:44:37.877531: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-02 00:44:37.878382: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-02 00:44:37.879285: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-02 00:44:37.879431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-02 00:44:37.880196: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-02 00:44:37.880602: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-02 00:44:37.882240: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-02 00:44:37.882327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 00:44:37.883011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 00:44:37.883625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\nSelected MultiGPU but only observe single GPU. Cancel Multi GPU training!\r\n2020-10-02 00:44:37.889428: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-02 00:44:37.894039: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3493600000 Hz\r\n2020-10-02 00:44:37.894764: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f88190 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-02 00:44:37.894780: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-02 00:44:37.954768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 00:44:37.955482: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f6fd90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-02 00:44:37.955503: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6\r\n2020-10-02 00:44:37.955670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 00:44:37.956825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:0d:00.0 name: GeForce RTX 3090 computeCapability: 8.6\r\ncoreClock: 1.695GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s\r\n2020-10-02 00:44:37.956850: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-02 00:44:37.956877: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-02 00:44:37.956891: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-02 00:44:37.956905: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-02 00:44:37.956918: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-02 00:44:37.956931: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-02 00:44:37.956945: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-02 00:44:37.957008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 00:44:37.958197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-02 00:44:37.959374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-02 00:44:37.959401: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n</pre>\r\n\r\nHalts for a minute then error:\r\n\r\n<pre>Traceback (most recent call last):\r\n  File &quot;train.py&quot;, line 94, in &lt;module&gt;\r\n    model = FrankNet.build(200, 150)\r\n  File &quot;/home/frank/Duckietown/AIDO5/challenge-aido_LF-baseline-behavior-cloning/duckieTrainer/frankModel.py&quot;, line 76, in build\r\n    linearVelocity = FrankNet.build_linear_branch(inputs)\r\n  File &quot;/home/frank/Duckietown/AIDO5/challenge-aido_LF-baseline-behavior-cloning/duckieTrainer/frankModel.py&quot;, line 12, in build_linear_branch\r\n    x = Conv2D(24, (5, 5), strides=(2, 2), padding=&quot;valid&quot;)(x)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 925, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 1098, in _functional_construction_call\r\n    self._maybe_build(inputs)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 2643, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py&quot;, line 197, in build\r\n    self.kernel = self.add_weight(\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 597, in add_weight\r\n    variable = self._add_variable_with_custom_getter(\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py&quot;, line 745, in _add_variable_with_custom_getter\r\n    new_variable = getter(\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py&quot;, line 133, in make_variable\r\n    return tf_variables.VariableV1(\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py&quot;, line 260, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py&quot;, line 206, in _variable_v1_call\r\n    return previous_getter(\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py&quot;, line 67, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py&quot;, line 2857, in creator\r\n    return next_creator(**kwargs)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py&quot;, line 199, in &lt;lambda&gt;\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py&quot;, line 2583, in default_variable_creator\r\n    return resource_variable_ops.ResourceVariable(\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py&quot;, line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py&quot;, line 1507, in __init__\r\n    self._init_from_args(\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py&quot;, line 1651, in _init_from_args\r\n    initial_value() if init_from_fn else initial_value,\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/keras/initializers/initializers_v2.py&quot;, line 397, in __call__\r\n    return super(VarianceScaling, self).__call__(shape, dtype=_get_dtype(dtype))\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/init_ops_v2.py&quot;, line 561, in __call__\r\n    return self._random_generator.random_uniform(shape, -limit, limit, dtype)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/init_ops_v2.py&quot;, line 1043, in random_uniform\r\n    return op(\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py&quot;, line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/ops/random_ops.py&quot;, line 288, in random_uniform\r\n    shape = tensor_util.shape_tensor(shape)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py&quot;, line 1029, in shape_tensor\r\n    return ops.convert_to_tensor(shape, dtype=dtype, name=&quot;shape&quot;)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py&quot;, line 1499, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 338, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 263, in constant\r\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 275, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 300, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 97, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File &quot;/home/frank/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py&quot;, line 539, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n</pre>\r\n\r\n**Describe the expected behavior**\r\nCode should execute fine.\r\n", "comments": ["Ubuntu version 20.04", "@mihaimaruseac I noticed that in another thread you mentioned there is a libtensorflow upload, will that potentially also fix this issue?", "Only if this issue stems from libtensorflow, not from the pip package", "@frank-qcd-qk,\r\nLooking at issue [#41990](https://github.com/tensorflow/tensorflow/issues/41990#issuecomment-686853339) with a similar error log, seems like the issue is fixed in the latest TF-nightly. \r\n\r\nCould you please check if you are facing the same issue with TF-nightly, CUDA 11 and cuDNN 8? Thanks!", "Is Cuda 10.1 just not going to be supported?", "TF 2.3 (2.2 and 2.1 too) builds against CUDA 10.1. TF 2.4 will build against CUDA 11.", "> TF 2.3 (2.2 and 2.1 too) builds against CUDA 10.1. TF 2.4 will build against CUDA 11.\r\n\r\nYes, I understand. so does that me whatever issue caused this problem with 30 series graphics card will not be addressed until TF2.4?", "I will test later today with the latest CUDA 11, and TF 2.4, but it would be nice if we can still use CUDA 10.1 and TF 2.3.", "> TF 2.3 (2.2 and 2.1 too) builds against CUDA 10.1. TF 2.4 will build against CUDA 11.\r\n\r\nAlso is it CUDA 11 or 11.1?", "Nightly already builds with CUDA 11. You can test with that, if possible, if you need CUDA 11.\r\n\r\n@sanjoy / @pkanwar23 do we build CUDA 11.0 or CUDA 11.1?", "Yeah, unfortunately TF 2.3 does not support GPUs with compute capability 8.x.  tf-nightly and the upcoming TF 2.4 release should both work, PTAL.  Both tf-nightly and TF 2.4 will use CUDA 11.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sanjoy  @mihaimaruseac  I have indeed used tf-nightly and cuda 11.0. I constantly get the issue of:\r\n\r\nInternal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\r\n\r\nAre we going to support cuda11.1 in nightly? Since it seems Cuda11.0 just don't support 3090?", "> Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal : Value 'sm_86' is not defined for option 'gpu-name'\r\n\r\nCan you share the full log?  Wherever this call is coming from, we should be able to ask ptxas to generate SASS for sm_80 since SASS for sm 80 should work for sm 86.", "@sanjoy From Nvidia Cuda 11.0 does not support SM86. For RTX 30 series card, should use SM86 capability. I have unfortunately nuked the setup and go with a source build. However, the error message is:\r\n\r\n2020-09-22 11:42:03.984823: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312]\r\nInternal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\r\n\r\nas pointed out by Pudget systems: https://www.pugetsystems.com/labs/hpc/RTX3090-TensorFlow-NAMD-and-HPCG-Performance-on-Linux-Preliminary-1902/\r\n\r\nAnd it seems for using tensorflow that uses tf1 behavior, it will break the code, for tf code 2.0 and above, it seems fine. ", "I can confirm that the RTX 3090 works with the following configuration:\r\ntf-nightly-gpu 2.4.0-dev20201016 \r\nLinux version 5.4.0-51-generic (buildd@lcy01-amd64-020) (gcc version 9.3.0 (Ubuntu 9.3.0-10ubuntu2))\r\nDriver Version: 455.23.05\r\nCuda Version 11.0.228 (failed to load with 10.1, 10.2, or 11.1)\r\ncuDNN 8.0.3.33 (failed with 7)", "Hi @frank-qcd-qk,\r\n\r\nAre you sure you're using a recent build?  The path in your log indicates that the TF build you're using is quite old.  Moreover, the error message you see should not be a fatal error, TF should fall back to the driver JIT.", "> Yeah, unfortunately TF 2.3 does not support GPUs with compute capability 8.x. tf-nightly and the upcoming TF 2.4 release should both work, PTAL. Both tf-nightly and TF 2.4 will use CUDA 11.\r\n\r\nI built TF 2.3.1 under CUDA 11.1 in a machine with RTX 3080, I haven't had any errors. Does this mean TF 2.3 doesn't utilize the GPU fully. @sanjoy Could you please elaborate on this?", "> I built TF 2.3.1 under CUDA 11.1 in a machine with RTX 3080, I haven't had any errors.\r\n\r\nI was talking about the TF binary pip packages we ship.  If you're building from source you can include support for any compute capability (>= 3.5) you like.", "@sanjoy understood.", "I'm facing the same issue. I recently purchased a RTX 3080 and followed the instructions on [Installing TensorFlow GPU in Ubuntu 20.04](https://towardsdatascience.com/installing-tensorflow-gpu-in-ubuntu-20-04-4ee3ca4cb75d).\r\n\r\nI have installed:\r\n- Cuda 10.1\r\n- CuDNN 7.6.5\r\n\r\nWhich are the recommended drivers on the TensorFlow docs for [GPU support](https://tensorflow.google.cn/install/gpu).\r\nWhen I run `tf.config.list_physical_devices(\"GPU\")` I get the following output:\r\n`[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]`\r\n\r\nBut when I was testing it in jupyter notebook\r\n```python\r\nif tf.test.gpu_device_name():\r\n    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\r\n```\r\nIt resulted in teh following error:\r\n_**RuntimeError:** CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid_\r\n\r\n**Useful system information**\r\nUbuntu 20.04\r\n\r\n`$ nvcc -V`\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n\r\n`$ nvidia-smi`\r\n![Screenshot from 2020-10-22 18-07-35](https://user-images.githubusercontent.com/47203161/96872673-9060ab80-1491-11eb-8ed8-2eaaaedaba43.png)\r\n\r\nI have also opened up #44229 to address the issue of the lack of instructions for ubuntu 20.04", "@Harsh188 , you have two versions of CUDA (10.1 and 11.1). That might have triggered the error, but I sense that this is a driver issue. Could you tell us if you're running TF on your machine or in a container and what's the actual Nvidia driver version that is loaded? In my case, drivers <455 failed to recognize the RTX 3080. And keep in mind that depending on the installation method you have chosen for CUDA 10.1, you might have installed an older bundled driver with the installation.\r\n", "@BorhenJlidi I think the default CUDA that it ships with is 11.1. I manually went thru and downloaded 10.1 since that's what TensorFlow recommends. My driver version is 455.28 and I'm running it on my machine. I'm still learning how to use docker. ", "@Harsh188 I recommend building TF with CUDA 11.1 and CUDNN v8.0.4.30, but you'd be obliged to turn TRT off for this build as it's not supported yet for CUDA 11.1. I tried different build configurations and this one worked for me. If you specifically need CUDA 10.1 and/or TRT enabled, you should figure out how to keep the 455 driver installed and loaded with breaking things up (you might have to install CUDA 10.1 manually from compressed archive). No matter what version of CUDA you use, a driver older than 455 will cause TF to not recognize you device (RTX 3080). Unless RTX 30 software support is full,  I do not recommend building a  Tensorflow/Tensorflow Serving docker image for RTX 3080 as builds currently fail and TensorRT for CUDA 11.1 isn't out yet.", "The confirmed workaround is to compile against the latest cuda, cudnn, tensorrt. Confirmed tf-nightly works but not with compute 8.6 capability and will get warning. ", "> @Harsh188 I recommend building TF with CUDA 11.1 and CUDNN v8.0.4.30, but you'd be obliged to turn TRT off for this build as it's not supported yet for CUDA 11.1. I tried different build configurations and this one worked for me. If you specifically need CUDA 10.1 and/or TRT enabled, you should figure out how to keep the 455 driver installed and loaded with breaking things up (you might have to install CUDA 10.1 manually from compressed archive). No matter what version of CUDA you use, a driver older than 455 will cause TF to not recognize you device (RTX 3080). Unless RTX 30 software support is full, I do not recommend building a Tensorflow/Tensorflow Serving docker image for RTX 3080 as builds currently fail and TensorRT for CUDA 11.1 isn't out yet.\r\n\r\n@Harsh188 @frank-qcd-qk TensorRT-7.2.1.6 is released, you can now build with TensorRT support.", "@BorhenJlidi I'm having trouble setting up my workspace. Can you check out #44307 and help me out? Thanks.", "> I can confirm that the RTX 3090 works with the following configuration:\r\n> tf-nightly-gpu 2.4.0-dev20201016\r\n> Linux version 5.4.0-51-generic (buildd@lcy01-amd64-020) (gcc version 9.3.0 (Ubuntu 9.3.0-10ubuntu2))\r\n> Driver Version: 455.23.05\r\n> Cuda Version 11.0.228 (failed to load with 10.1, 10.2, or 11.1)\r\n> cuDNN 8.0.3.33 (failed with 7)\r\n\r\n\r\n\r\n> configuration\r\n\r\nThanks, do your configuration compile successfully? Can your configuration run codes needs to be compiled on 3090? looking forward to your reply.", "> > I can confirm that the RTX 3090 works with the following configuration:\r\n> > tf-nightly-gpu 2.4.0-dev20201016\r\n> > Linux version 5.4.0-51-generic (buildd@lcy01-amd64-020) (gcc version 9.3.0 (Ubuntu 9.3.0-10ubuntu2))\r\n> > Driver Version: 455.23.05\r\n> > Cuda Version 11.0.228 (failed to load with 10.1, 10.2, or 11.1)\r\n> > cuDNN 8.0.3.33 (failed with 7)\r\n> \r\n> > configuration\r\n> \r\n> Thanks, do your configuration compile successfully? Can your configuration run codes needs to be compiled on 3090? looking forward to your reply.\r\n\r\nYes this configuration compiles computation graphs and executes them on the 3090", "> > > I can confirm that the RTX 3090 works with the following configuration:\r\n> > > tf-nightly-gpu 2.4.0-dev20201016\r\n> > > Linux version 5.4.0-51-generic (buildd@lcy01-amd64-020) (gcc version 9.3.0 (Ubuntu 9.3.0-10ubuntu2))\r\n> > > Driver Version: 455.23.05\r\n> > > Cuda Version 11.0.228 (failed to load with 10.1, 10.2, or 11.1)\r\n> > > cuDNN 8.0.3.33 (failed with 7)\r\n> > \r\n> > \r\n> > > configuration\r\n> > \r\n> > \r\n> > Thanks, do your configuration compile successfully? Can your configuration run codes needs to be compiled on 3090? looking forward to your reply.\r\n> \r\n> Yes this configuration compiles computation graphs and executes them on the 3090\r\n\r\ndo you mean compile you can compile c++ codes into .so files, and run it by python?", "> > > > I can confirm that the RTX 3090 works with the following configuration:\r\n> > > > tf-nightly-gpu 2.4.0-dev20201016\r\n> > > > Linux version 5.4.0-51-generic (buildd@lcy01-amd64-020) (gcc version 9.3.0 (Ubuntu 9.3.0-10ubuntu2))\r\n> > > > Driver Version: 455.23.05\r\n> > > > Cuda Version 11.0.228 (failed to load with 10.1, 10.2, or 11.1)\r\n> > > > cuDNN 8.0.3.33 (failed with 7)\r\n> > > \r\n> > > \r\n> > > > configuration\r\n> > > \r\n> > > \r\n> > > Thanks, do your configuration compile successfully? Can your configuration run codes needs to be compiled on 3090? looking forward to your reply.\r\n> > \r\n> > \r\n> > Yes this configuration compiles computation graphs and executes them on the 3090\r\n> \r\n> do you mean compile you can compile c++ codes into .so files, and run it by python?\r\n\r\n", "@soluwalana \r\nI am trying to match your configurations and I not getting any luck. I downloaded the TF-nightly but I cannot download the CUDA version with that driver - any help?", "> > Yeah, unfortunately TF 2.3 does not support GPUs with compute capability 8.x. tf-nightly and the upcoming TF 2.4 release should both work, PTAL. Both tf-nightly and TF 2.4 will use CUDA 11.\r\n> \r\n> I built TF 2.3.1 under CUDA 11.1 in a machine with RTX 3080, I haven't had any errors. Does this mean TF 2.3 doesn't utilize the GPU fully. @sanjoy Could you please elaborate on this?\r\n\r\nCan you please walk through your steps to reproduce your set-up/downloads for running with 3080? I believe I tried with a whole slew of different versions and I can't seem to get it to work", "@Amokstakov check my reply to another user having you problem https://github.com/tensorflow/tensorflow/issues/44307#issuecomment-716547268 or simply install these packages (please note that the inclusion of extensions deb,tgz.. is intended as I believe picking those yields an easier and more flexible install experience and don't forget to follow the relevant NVIDIA's installation guide by version and installer extension/format ) :\r\n\r\n- cuda-repo-ubuntu2004-11-1-local_11.1.0-455.23.05-1_amd64.deb\r\n- cudnn-11.1-linux-x64-v8.0.4.30.tgz\r\n- TensorRT-7.2.1.6.Ubuntu-18.04.x86_64-gnu.cuda-11.1.cudnn8.0.tar.gz\r\n\r\nand build Tensorflow 2.3.1 from source and you're good to go! ", "The same issue here.\r\n\r\n- RTX 3090\r\n- Cuda 10.1\r\n- CUDNN 7.6.5\r\n- Tensorflow-gpu 2.2.0\r\n\r\nisn't there any news with this issue?\r\n", "> isn't there any news with this issue?\r\n\r\n@fspider,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n\r\n", "Update:\r\nWith tensorflow 2.4 now officially released, I can confirm that cuda 11.0 will work on ubuntu 20.04, with tensorflow. Its just there will be nasty error about:\r\n\r\nYour CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'\r\n\r\nBut it do not affect training/inferencing"]}, {"number": 43717, "title": "Code improvement to fasten execution process", "body": "Added couple of shabang lines to configure.py and configure.md", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43717) for more info**.\n\n<!-- need_sender_cla -->", "@KrishnaRajule Can you please sign CLA. Thanks!", "@googlebot I signed it!\n\nOn Fri, Oct 2, 2020 at 9:35 AM gbaned <notifications@github.com> wrote:\n\n> @KrishnaRajule <https://github.com/KrishnaRajule> Can you please sign\n> CLA. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/43717#issuecomment-702514242>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANOVNC3DJER5C4JDNW3I4R3SIVGSDANCNFSM4SBGZ2NA>\n> .\n>\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43717) for more info**.\n\n<!-- ok -->", "@gbaned I have signed it!\n\nOn Fri, Oct 2, 2020 at 9:35 AM gbaned <notifications@github.com> wrote:\n\n> @KrishnaRajule <https://github.com/KrishnaRajule> Can you please sign\n> CLA. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/43717#issuecomment-702514242>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANOVNC3DJER5C4JDNW3I4R3SIVGSDANCNFSM4SBGZ2NA>\n> .\n>\n", "These files are intended to run without a shebang. Plus, the shebang you added is not general/invalid (you added a `bash` shebang on a file that is supposed to run on Windows and you added a shebang that requires users to have `python3` as a binary on the path, but there are systems where that is not the case -- they only come with Python 3.x by default).", "Okay, I understand. Thank You for your response! @mihaimaruseac "]}, {"number": 43716, "title": "Cuda 11.1 cudart SO name fix", "body": "In CUDA 11.1 the soname of libcudart remains versioned as 11.0 for better backward comparability. This PR updates the configure scripts accordingly.\r\n\r\nThis should also address issue #43689 \r\n\r\nAttn @sanjoy ", "comments": ["This is currently preventing TF and Jax builds on 11.1 (at least with Ampere cards). Making these changes to the bazel config fixes the issue. "]}, {"number": 43715, "title": "Eager loading of CUDNN sub-libraries.", "body": "Improves accuracy of TF autotuning by ensuring first calls to cudnn kernels run at full speed.\r\n\r\nAttention @sanjoy ", "comments": ["> cudnn_8_0.inc is auto-generated, is it still fine to make changes to it manually?\r\n\r\nYes, that's fine. There aren't that many additions that this would get out of hand.\r\n"]}, {"number": 43714, "title": "[tf.data server] prevent restart of the dispatcher server w/o fault-tolerance, in the test bed", "body": "This PR is a follow up of https://github.com/tensorflow/tensorflow/pull/43691 and prevents a restart of the dispatcher if it has been configured without fault-tolerance. Failing to do so will lead to a perpetual stream of `warn` messages on a dispatcher restart and unnecessarily extend the testing time without an immediate exit.\r\n\r\n@aaudiber until the enhancements are implemented to handle such scenarios (as per our discussion), this will help us in handling the upcoming test cases.", "comments": ["Made the change, @aaudiber.", "@aaudiber, I had a quick question regarding the updates to the workers based on the task list.\r\nReference: https://github.com/tensorflow/tensorflow/blob/377434ff41473a33f7bedd1c08199d5af1fd33e9/tensorflow/core/kernels/data/experimental/data_service_dataset_op.cc#L326\r\n\r\nHow would we identify whether the task list changed or not if we don't poll for changes?\r\nPlease let me know."]}, {"number": 43713, "title": "[Cherrypick:r2.3] Fix libtensorflow CUDA compute capabilities.", "body": "PiperOrigin-RevId: 330987201\nChange-Id: I1a10343229216f70da922c163cc9ebcbfe069947", "comments": []}, {"number": 43712, "title": "Docs: provide CUDA/CuDNN/TensorRT version information for PyPI packages", "body": "The title says it all, but I'll explain nonetheless.\r\n\r\nI have CUDA 10.1 Update 2, CuDNN 7.6.5 for CUDA 10.1, TensorRT 6.0.1.5 installed and run `tensorflow==2.3.1` from PyPI.\r\n\r\nToday I tried `tf-nightly`, which suddenly does not support my GPU. The error message implies that some CUDA 11 is missing, but now I am at a loss what to ask my server admin to install. Obviously, I have to install what the PyPI packages were compiled against.\r\n\r\n- CUDA 11.0? 11.1? 2 possibilities.\r\n- CuDNN 7.6? 8.0? For CUDA 10.1? 11.0? 11.1? 6 possibilities.\r\n- TensorRT 6? 7.0? 7.1? 7.2? 4 possibilities.\r\n\r\nI would like to be able to find this out without going through several (up to 50?) iterations with my server admin, and without having to ask to install *everything*.\r\n\r\nI understand that maybe this information is not available yet for 2.4.0, but \r\n\r\n- it should be available for current `tf-nightly`, and\r\n- it should *definitely* be available for `tensorflow==2.3.1` (it may be that this is what https://www.tensorflow.org/install/gpu has, but that wording is awkward: \"TensorFlow supports\" should probably be \"TensorFlow requires\", and I don't think TensorRT is optional: I remember having had to install it; also, the top says \"See the pip install guide for available packages, *systems requirements*, and instructions\", and then that page does not say anything about CUDA/CuDNN/etc., but instead refers back to the first page for CUDA),\r\n- it should be available for older versions, too.", "comments": ["@bersbersbers In the most recent versions (`TF2.4`), CUDA package is listed. Please check https://pypi.org/project/tensorflow-gpu/2.4.1/\r\n\r\n@mihaimaruseac Any plans of listing CuDNN, TensorRT versions? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Thanks! Also, release notes now say\r\n\r\n> TensorFlow pip packages are now built with CUDA11.2 and cuDNN 8.1.0\r\n\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v2.5.0-rc1", "This issue is not solved, unfortunately.\r\n\r\n- v2.6 release notes don't mention any CUDA/CuDNN information at all:\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v2.6.0\r\n\r\n- Of note, v2.5 release notes list \"CUDA11.2 and cuDNN 8.1.0\":\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v2.5.0\r\n\r\n- PyPI lists \"GPU :: NVIDIA CUDA :: 11.2\" for tf-nightly, but \"GPU :: NVIDIA CUDA :: 11.0\" for tensorflow:\r\nhttps://pypi.org/project/tf-nightly/2.7.0.dev20210815/\r\nhttps://pypi.org/project/tensorflow/2.6.0/", "@bersbersbers Thanks for finding this issue. Please create a new issue so that we can track the progress. Ping me in that issue. Thanks!"]}, {"number": 43711, "title": "[CherryPick:r2.3]Add cuda compute configs to the variable list for libtensorflow GPU builds.", "body": "PiperOrigin-RevId: 330570975\nChange-Id: Iafdd2711c505152cc719b2ce636a9aa18691e00f", "comments": []}, {"number": 43710, "title": "executing vectorized_map on batches triggers retracing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): dockerhub container 'latest' Digest: c57fb9628d80\r\n- TensorFlow version:tf.version.GIT_VERSION=v2.3.0-54-gfcc4b966f1 tf.version.VERSION=2.3.1\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nRepeatedly calling vectorized_map on the same function with parameters of same shape and dtype triggers retracing.\r\n\r\nIt seems that vectorized_map allocates memory to run all iterations simultaneously and this is causing OOM. Therefore, I separated the execution on batches to reduce the memory allocation of the function.  If there is a better way to do this, please let me know.\r\n\r\n**Describe the expected behavior**\r\nNo retracing\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\n\r\nprint('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))\r\nprint('tf.version.VERSION={}'.format(tf.version.VERSION))\r\n\r\n\r\ndef _jvp(f, primals, tangents):\r\n    with tf.autodiff.ForwardAccumulator(primals, tangents) as acc:\r\n        primals_out = f(*primals)\r\n    return primals_out, acc.jvp(\r\n        primals_out, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\ndef f(x, z, t, c, v1, v2, v3, v4):\r\n\r\n    p = tf.concat([x, z, t], axis=1)\r\n    pe = p[:, :, None]\r\n    ce = tf.transpose(a=c[:, :, None], perm=[2, 1, 0])\r\n    d = ce - pe\r\n    r = tf.reduce_sum(input_tensor=tf.square(d), axis=1)\r\n    G = tf.exp(-r / 2)\r\n\r\n    p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)\r\n    b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)\r\n    u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)\r\n    w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)\r\n\r\n    return p, b, u, w\r\n\r\ndef g(x, z, t, c, v1, v2, v3, v4):\r\n\r\n    tf.print('tf.print shapes', *[tf.shape(w) for w in (x, z, t, c, v1, v2, v3, v4)])\r\n    tf.print('tf.print dtypes', *[w.dtype for w in (x, z, t, c, v1, v2, v3, v4)])\r\n    #print('py.print x.shape', tf.shape(x))\r\n\r\n    fx = lambda xi: f(xi, z, t, c, v1, v2, v3, v4)\r\n    with tf.autodiff.ForwardAccumulator(primals=[x], tangents=[tf.ones_like(x)]) as fwd_outer:\r\n        [dpdx, dbdx, dudx, dwdx] = _jvp(fx, [x], [tf.ones_like(x)])[1]\r\n    [d2bd2x, d2ud2x, d2wd2x] = fwd_outer.jvp([dbdx, dudx, dwdx], tf.UnconnectedGradients.ZERO)\r\n\r\n    fz = lambda zi: f(x, zi, t, c, v1, v2, v3, v4)\r\n    with tf.autodiff.ForwardAccumulator(primals=[z], tangents=[tf.ones_like(z)]) as fwd_outer:\r\n        [dpdz, dbdz, dudz, dwdz] = _jvp(fz, [z], [tf.ones_like(z)])[1]\r\n    [d2bd2z, d2ud2z, d2wd2z] = fwd_outer.jvp([dbdz, dudz, dwdz], tf.UnconnectedGradients.ZERO)\r\n\r\n    ft = lambda ti: f(x, z, ti, c, v1, v2, v3, v4)\r\n    [p, b, u, w], [dpdt, dbdt, dudt, dwdt] = _jvp(ft, [t], [tf.ones_like(t)])\r\n\r\n    return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz, d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,\r\n\r\n\r\nn = 7500\r\nx = tf.random.uniform((n, 1), dtype=tf.float64)\r\nz = tf.random.uniform((n, 1), dtype=tf.float64)\r\nt = tf.random.uniform((n, 1), dtype=tf.float64)\r\n\r\nc = tf.random.uniform((n,3), dtype=tf.float64)\r\n\r\nv1 = tf.random.uniform((1, n), dtype=tf.float64)\r\nv2 = tf.random.uniform((1, n), dtype=tf.float64)\r\nv3 = tf.random.uniform((1, n), dtype=tf.float64)\r\nv4 = tf.random.uniform((1, n), dtype=tf.float64)\r\n\r\n\r\ndef batched_execution(fb, args, batch_size):\r\n    batch_size = tf.cast(batch_size, tf.int32)\r\n    n = tf.shape(args[0])[0]\r\n    i0 = tf.constant(0, dtype=tf.int32)\r\n    souts = []\r\n    while tf.less_equal(i0 + batch_size, n):\r\n        tf.print('tf print batch iteration batch_size={}'.format(batch_size))\r\n        il = i0 + batch_size\r\n        bsargs = [a[i0:il] for a in args]\r\n        bouts = fb(bsargs)\r\n        souts.append(bouts)\r\n        i0 = il\r\n    if tf.less(i0, n):\r\n        tf.print('tf print last batch iteration batch_size={}'.format(n-i0))\r\n        bsargs = [a[i0:n] for a in args]\r\n        bouts = fb(bsargs)\r\n        souts.append(bouts)\r\n\r\n    souts = [tf.concat([o[i] for o in souts], axis=0) for i in range(len(souts[0]))]\r\n    return souts\r\n\r\ndef shaped_vectorized_map(fv, args, axis=2):\r\n    sargs = [tf.expand_dims(a, axis=axis) for a in args]\r\n    outs = batched_execution(lambda args: tf.vectorized_map(fv, args, fallback_to_while_loop=False), sargs, batch_size=1000)\r\n    souts = [tf.squeeze(o, axis=[axis]) for o in outs]\r\n    return souts\r\n\r\nstart_time = time.clock()\r\ne2v = shaped_vectorized_map(lambda args: g(*args, c, v1, v2, v3, v4), (x, z, t))\r\ndelta_time = time.clock() - start_time\r\nprint('running with batched vectorized_map took {:f} seconds'.format(delta_time))\r\n```\r\n\r\n**Other info / logs** \r\n\r\n```\r\ntf.version.GIT_VERSION=v2.3.0-54-gfcc4b966f1\r\ntf.version.VERSION=2.3.1\r\ntf print batch iteration batch_size=1000\r\ntf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]\r\ntf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64\r\ntf print batch iteration batch_size=1000\r\ntf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]\r\ntf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64\r\ntf print batch iteration batch_size=1000\r\ntf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]\r\ntf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64\r\ntf print batch iteration batch_size=1000\r\ntf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]\r\ntf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64\r\ntf print batch iteration batch_size=1000\r\ntf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]\r\ntf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64\r\nWARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7f54481b3620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\ntf print batch iteration batch_size=1000\r\ntf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]\r\ntf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64\r\nWARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7f54402c4620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\ntf print batch iteration batch_size=1000\r\ntf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]\r\ntf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64\r\nWARNING:tensorflow:7 out of the last 7 calls to <function pfor.<locals>.f at 0x7f54407a37b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\ntf print last batch iteration batch_size=500\r\ntf.print shapes [1 1] [1 1] [1 1] [7500 3] [1 7500] [1 7500] [1 7500] [1 7500]\r\ntf.print dtypes tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64 tf.float64\r\nWARNING:tensorflow:8 out of the last 8 calls to <function pfor.<locals>.f at 0x7f54400c17b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\nrunning with batched vectorized_map took 119.087042 seconds\r\n```\r\n\r\nrelated issues: #42835 #43252", "comments": ["/cc @allenlavoie", "Unfortunately tf.vectorized_map is actually tracing the function in its argument each time (it doesn't yet support eager op-by-op vectorization), so the warning is informative in this case. I'd put `@tf.function` on `def shaped_vectorized_map` and all of the tracing will be cached.\r\n\r\ncc @agarwal-ashish ", "Yes, tf.vectorized_map API does not try to do any caching on its own. The call retraces and performs vectorization as well. Its usage should be wrapped in tf.function, as suggested by @allenlavoie  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43710\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43710\">No</a>\n", "Hi @allenlavoie @agarwal-ashish, thanks for the suggestion.  Simply wrapping `shaped_vectorized_map` with `tf.function` triggers other error:\r\n\r\n```\r\n    InaccessibleTensorError: The tensor 'Tensor(\"while/loop_body/PartitionedCall_29/pfor/PartitionedCall:0\", shape=(None, 1, 1), dtype=float64)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=while_body_87742, id=140105979881960); accessed from: FuncGraph(name=function_wrapped_shaped_vectorized_map, id=140105966842880).\r\n\r\n```\r\n\r\nyou may check the details on [this notebook in collab](https://colab.research.google.com/drive/1H6kuQ8E66NbRi8V42V58OkLmSodzkt1i?usp=sharing).  I'm trying to fix it without success.  Any further advise would be appreciated.  Should I open a new issue?", "Ah, I didn't realize there was tensor-dependent control flow in batched_execution. That might be an Autograph bug, or maybe just a case that isn't supported (yet? there was some question about the wisdom of re-writing Python lists as TensorLists, cc @mdanatg ). Filing sounds reasonable.\r\n\r\nTo get the vectorized graph cached you only need @tf.function around `lambda args: tf.vectorized_map(fv, args, fallback_to_while_loop=False)`, limiting it to that sounds like a reasonable workaround. May need a bit of refactoring so you're not re-creating the tf.function every time the outer function runs.\r\n\r\nPassing a new lambda to tf.function each time will cause that to retrace, so you may want to hang onto a single lambda if you end up benchmarking execution in a loop.", "The \"Inaccessible tensor\" error usually happens when you try to append to a Python list from within TF control flow. That's indeed not supported right now. I recommend using TensorArray or RaggedTensor instead of Python lists (for example the one used by `souts`).", "@mdanatg  Do you think that we can interecept this case with a better error message untill it will be supported?", "thank you all for your suggestions.  I did this:\r\n\r\n>To get the vectorized graph cached you only need @tf.function around lambda args: tf.vectorized_map(fv, args, fallback_to_while_loop=False), limiting it to that sounds like a reasonable workaround. May need a bit of refactoring so you're not re-creating the tf.function every time the outer function runs.\r\n\r\nand I could get 4x speed up in [this updated notebook](https://colab.research.google.com/drive/1H6kuQ8E66NbRi8V42V58OkLmSodzkt1i?usp=sharing#scrollTo=g107jX-qowJa).  I will try to scale up my solutions in this way and avoid the technicalities of TensorArray.  ", "@bhack Yep, there is a fix in the works to do that."]}, {"number": 43709, "title": "ValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build", "body": "model = tf.keras.Sequential([\r\nlayers.DenseFeatures(feature_columns),\r\nlayers.Dense(128,activation='relu'),\r\nlayers.Dropout(0.2),\r\nlayers.Dense(512,activation='relu'),\r\nlayers.Dropout(0.2),\r\nlayers.Dense(512, activation='relu'),\r\nlayers.Dense(31,activation='softmax')\r\n])\r\n\r\nThis is my model.How do i solve this issue or add input shape to the first layer? Please help\r\nWhen i try to save and load it with this piece of code \r\nmodel.save(\"my_model_tf\",save_format=\"tf\")\r\nmodel_temp = keras.models.load_model('my_model_tf')\r\nit works fine but when i write model.summary() it gives the above the title error.I have to deploy it on aws so it would be an issue there  so please help", "comments": [" @Dhananjay201 , I am new to open source and DL .I think I can work on this .\r\n", "@ritika-singh2000 Sure.Any help is appreciated.I think it is a bug with this whenever we use feature_columns to build a model.\r\n", "@Dhananjay201,\r\nOn running the given code snippet I am facing an error stating `NameError: name 'feature_columns' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/57260361ee70c003574113eba986021e/43709.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the TensorFlow version you are using.\r\n\r\nAlso, please go through [this guide](https://www.tensorflow.org/tutorials/quickstart/beginner) for building a basic neural network using Keras and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43707, "title": "TPU V3-1024 training crashes randomly", "body": "**System information**\r\n- Platform: Linux-5.4.0-1024-gcp-x86_64-with-debian-buster-sid\r\n- Python version: 3.7.7\r\n- Tennsorflow v1.15.2-30-g4386a66 1.15.3\r\n\r\n**Describe the current behavior**\r\nWhile training T5-11B model using mesh tensorflow and tensorflow on TPU V3-1024, the training crash randomly while training. After I remove the TPU node and create it again, then restart the training it resumes normally.  \r\n\r\n**Describe the expected behavior**\r\nThe crash should not occur as the dataset was tested with T5-3B model without any issue.\r\n\r\n**Standalone code to reproduce the issue**\r\nThis will be hard, but this is our running command:\r\n```\r\npython -m t5.models.mesh_transformer_main   \\\r\n--module_import=\"bfd100_task\"   \\\r\n--tpu=\"tpuv3-1024\"   \\\r\n--gcp_project=\"tum-covid19\"   \\\r\n--tpu_zone=\"europe-west4-a\"   \\\r\n--model_dir=\"gs://prot-transformers-eu/t5/models/bfd100_v2/11b_mask_rsqrt_4k/\"   \\\r\n--gin_file=\"objectives/mask_denoise.gin\"   \\\r\n--gin_file=\"models/t5.1.0.11B.gin\"   \\\r\n--gin_file=\"dataset.gin\"   \\\r\n--gin_file=\"learning_rate_schedules/rsqrt_no_ramp_down.gin\"   \\\r\n--gin_param=\"MIXTURE_NAME = 'unsupervised_bfd_protein_denoising_task'\"   \\\r\n--gin_param=\"utils.tpu_mesh_shape.tpu_topology = '16x32'\"   \\\r\n--gin_param=\"utils.tpu_mesh_shape.model_parallelism = 32\"   \\\r\n--gin_param=\"utils.run.save_checkpoints_steps=10000\"   \\\r\n--gin_param=\"utils.run.batch_size=('tokens_per_batch', 2097152)\"   \\\r\n--gin_param=\"utils.run.train_steps=1200000\"   \\\r\n--gin_param=\"utils.run.iterations_per_loop=2000\"   \\\r\n--gin_param=\"learning_rate_schedule_noam.warmup_steps=80000\"   \\\r\n--gin_param=\"SentencePieceVocabulary.extra_ids=100\"   \\\r\n--gin_param=\"run.perplexity_eval_steps=100\"\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n```\r\nI1001 16:08:36.562530 139829429921536 tpu_estimator.py:279] Outfeed finished for iteration (37, 555)                                                                                                      \r\nINFO:tensorflow:Outfeed finished for iteration (37, 564)                                                                                                                                                  \r\nI1001 16:09:42.150621 139829429921536 tpu_estimator.py:279] Outfeed finished for iteration (37, 564)                                                                                                      \r\n2020-10-01 16:10:42.574305: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"Unavailable: Socket closed\" and grpc_error_string = \"{\"created\":\"@1601568642.5$\r\n4100849\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC          \r\n2020-10-01 16:10:42.574367: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"Unavailable: Socket closed\" and grpc_error_string = \"{\"created\":\"@1601568642.5$\r\n4157686\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC          \r\n2020-10-01 16:10:42.574304: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"Unavailable: Socket closed\" and grpc_error_string = \"{\"created\":\"@1601568642.5$\r\n4136203\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC          \r\nINFO:tensorflow:An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a new session will be created. This error may also \r\noccur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing the number of parameter servers assigned to the jo$\r\n. Error: Session bb9ea551c191de28 is not found.                                                                                                                                                           \r\nI1001 16:13:55.492127 139833409849152 monitored_session.py:1269] An error was raised. This may be due to a preemption in a connected worker or parameter server. The current session will be closed and a \r\nnew session will be created. This error may also occur due to a gRPC failure caused by high memory or network bandwidth usage in the parameter servers. If this error occurs repeatedly, try increasing t$\r\ne number of parameter servers assigned to the job. Error: Session bb9ea551c191de28 is not found.                                                                                                          \r\nWARNING:tensorflow:An error occurred when attempting to close the session. This may be due to a preemption in a connected worker or parameter server. Error: Session bb9ea551c191de28 is not found. Possi$\r\nly, this master has restarted.                                                                                                                                                                            \r\nW1001 16:13:55.495698 139833409849152 monitored_session.py:1171] An error occurred when attempting to close the session. This may be due to a preemption in a connected worker or parameter server. Error$\r\n Session bb9ea551c191de28 is not found. Possibly, this master has restarted.                                                                                                                              \r\nINFO:tensorflow:Graph was finalized.                                                                                                                                                                      \r\nI1001 16:13:55.496506 139833409849152 monitored_session.py:240] Graph was finalized.                                                                                                                      \r\nINFO:tensorflow:Restoring parameters from gs://prot-transformers-eu/t5/models/bfd100_v2/11b_mask_rsqrt_4k/model.ckpt-72000                                                                                \r\nI1001 16:13:55.769371 139833409849152 saver.py:1284] Restoring parameters from gs://prot-transformers-eu/t5/models/bfd100_v2/11b_mask_rsqrt_4k/model.ckpt-72000                                           \r\nWARNING:tensorflow:From /home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_man$\r\ngement) is deprecated and will be removed in a future version.                                                                                                                                            \r\nInstructions for updating:                                                                                                                                                                                \r\nUse standard file utilities to get mtimes.                                                                                                                                                                \r\nW1001 16:18:45.793992 139833409849152 deprecation.py:323] From /home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from te$\r\nsorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.                                                                                                     \r\nInstructions for updating:                                                                                                                                                                                \r\nUse standard file utilities to get mtimes.                                                                                                                                                                \r\nINFO:tensorflow:Running local_init_op.                                                                                                                                                                    \r\nI1001 16:18:50.382259 139833409849152 session_manager.py:500] Running local_init_op.                                                                                                                      \r\nINFO:tensorflow:Done running local_init_op.                                                                                                                                                               \r\nI1001 16:18:54.161618 139833409849152 session_manager.py:502] Done running local_init_op.                                                                                                                 \r\nINFO:tensorflow:Initialized dataset iterators in 4 seconds                                                                                                                                                \r\nI1001 16:19:10.171010 139833409849152 util.py:98] Initialized dataset iterators in 4 seconds\r\nINFO:tensorflow:Installing graceful shutdown hook.\r\nI1001 16:19:10.171908 139833409849152 session_support.py:332] Installing graceful shutdown hook.\r\n2020-10-01 16:19:10.172536: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session\r\n has not yet been created.\r\nINFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:3/device:CPU:0', '/job:worker/replic\r\na:0/task:6/device:CPU:0', '/job:worker/replica:0/task:7/device:CPU:0', '/job:worker/replica:0/task:8/device:CPU:0', '/job:worker/replica:0/task:9/device:CPU:0', '/job:worker/replica:0/task:14/device:CPU\r\n:0', '/job:worker/replica:0/task:11/device:CPU:0', '/job:worker/replica:0/task:17/device:CPU:0', '/job:worker/replica:0/task:15/device:CPU:0', '/job:worker/replica:0/task:19/device:CPU:0', '/job:worker/\r\nreplica:0/task:16/device:CPU:0', '/job:worker/replica:0/task:23/device:CPU:0', '/job:worker/replica:0/task:22/device:CPU:0', '/job:worker/replica:0/task:24/device:CPU:0', '/job:worker/replica:0/task:25/\r\ndevice:CPU:0', '/job:worker/replica:0/task:27/device:CPU:0', '/job:worker/replica:0/task:30/device:CPU:0', '/job:worker/replica:0/task:33/device:CPU:0', '/job:worker/replica:0/task:31/device:CPU:0', '/j\r\nob:worker/replica:0/task:32/device:CPU:0', '/job:worker/replica:0/task:35/device:CPU:0', '/job:worker/replica:0/task:39/device:CPU:0', '/job:worker/replica:0/task:4/device:CPU:0', '/job:worker/replica:0\r\n/task:34/device:CPU:0', '/job:worker/replica:0/task:12/device:CPU:0', '/job:worker/replica:0/task:20/device:CPU:0', '/job:worker/replica:0/task:28/device:CPU:0', '/job:worker/replica:0/task:2/device:CPU\r\n:0', '/job:worker/replica:0/task:36/device:CPU:0', '/job:worker/replica:0/task:26/device:CPU:0', '/job:worker/replica:0/task:10/device:CPU:0', '/job:worker/replica:0/task:18/device:CPU:0', '/job:worker/\r\nreplica:0/task:5/device:CPU:0', '/job:worker/replica:0/task:40/device:CPU:0', '/job:worker/replica:0/task:21/device:CPU:0', '/job:worker/replica:0/task:13/device:CPU:0', '/job:worker/replica:0/task:38/d\r\nevice:CPU:0', '/job:worker/replica:0/task:37/device:CPU:0', '/job:worker/replica:0/task:29/device:CPU:0', '/job:worker/replica:0/task:41/device:CPU:0', '/job:worker/replica:0/task:43/device:CPU:0', '/jo\r\nb:worker/replica:0/task:45/device:CPU:0', '/job:worker/replica:0/task:42/device:CPU:0', '/job:worker/replica:0/task:44/device:CPU:0', '/job:worker/replica:0/task:46/device:CPU:0', '/job:worker/replica:0\r\n/task:47/device:CPU:0', '/job:worker/replica:0/task:48/device:CPU:0', '/job:worker/replica:0/task:49/device:CPU:0', '/job:worker/replica:0/task:50/device:CPU:0', '/job:worker/replica:0/task:51/device:CP\r\nU:0', '/job:worker/replica:0/task:53/device:CPU:0', '/job:worker/replica:0/task:52/device:CPU:0', '/job:worker/replica:0/task:55/device:CPU:0', '/job:worker/replica:0/task:54/device:CPU:0', '/job:worker\r\n/replica:0/task:56/device:CPU:0', '/job:worker/replica:0/task:59/device:CPU:0', '/job:worker/replica:0/task:57/device:CPU:0', '/job:worker/replica:0/task:58/device:CPU:0', '/job:worker/replica:0/task:60\r\n/device:CPU:0', '/job:worker/replica:0/task:62/device:CPU:0', '/job:worker/replica:0/task:61/device:CPU:0', '/job:worker/replica:0/task:63/device:CPU:0', '/job:worker/replica:0/task:64/device:CPU:0', '/\r\njob:worker/replica:0/task:65/device:CPU:0', '/job:worker/replica:0/task:67/device:CPU:0', '/job:worker/replica:0/task:66/device:CPU:0', '/job:worker/replica:0/task:68/device:CPU:0', '/job:worker/replica\r\n:0/task:70/device:CPU:0', '/job:worker/replica:0/task:69/device:CPU:0', '/job:worker/replica:0/task:71/device:CPU:0', '/job:worker/replica:0/task:72/device:CPU:0', '/job:worker/replica:0/task:73/device:\r\nCPU:0', '/job:worker/replica:0/task:74/device:CPU:0', '/job:worker/replica:0/task:75/device:CPU:0', '/job:worker/replica:0/task:78/device:CPU:0', '/job:worker/replica:0/task:79/device:CPU:0', '/job:work\r\ner/replica:0/task:76/device:CPU:0', '/job:worker/replica:0/task:77/device:CPU:0', '/job:worker/replica:0/task:80/device:CPU:0', '/job:worker/replica:0/task:82/device:CPU:0', '/job:worker/replica:0/task:\r\n81/device:CPU:0', '/job:worker/replica:0/task:85/device:CPU:0', '/job:worker/replica:0/task:84/device:CPU:0', '/job:worker/replica:0/task:83/device:CPU:0', '/job:worker/replica:0/task:86/device:CPU:0',\r\n'/job:worker/replica:0/task:87/device:CPU:0', '/job:worker/replica:0/task:88/device:CPU:0', '/job:worker/replica:0/task:89/device:CPU:0', '/job:worker/replica:0/task:92/device:CPU:0', '/job:worker/repli\r\nca:0/task:90/device:CPU:0', '/job:worker/replica:0/task:93/device:CPU:0', '/job:worker/replica:0/task:91/device:CPU:0', '/job:worker/replica:0/task:94/device:CPU:0', '/job:worker/replica:0/task:95/devic\r\ne:CPU:0', '/job:worker/replica:0/task:98/device:CPU:0', '/job:worker/replica:0/task:96/device:CPU:0', '/job:worker/replica:0/task:97/device:CPU:0', '/job:worker/replica:0/task:100/device:CPU:0', '/job:w\r\norker/replica:0/task:99/device:CPU:0', '/job:worker/replica:0/task:101/device:CPU:0', '/job:worker/replica:0/task:103/device:CPU:0', '/job:worker/replica:0/task:102/device:CPU:0', '/job:worker/replica:0\r\n/task:104/device:CPU:0', '/job:worker/replica:0/task:106/device:CPU:0', '/job:worker/replica:0/task:105/device:CPU:0', '/job:worker/replica:0/task:107/device:CPU:0', '/job:worker/replica:0/task:108/devi\r\nce:CPU:0', '/job:worker/replica:0/task:109/device:CPU:0', '/job:worker/replica:0/task:111/device:CPU:0', '/job:worker/replica:0/task:110/device:CPU:0', '/job:worker/replica:0/task:113/device:CPU:0', '/j\r\nob:worker/replica:0/task:112/device:CPU:0', '/job:worker/replica:0/task:114/device:CPU:0', '/job:worker/replica:0/task:115/device:CPU:0', '/job:worker/replica:0/task:117/device:CPU:0', '/job:worker/repl\r\nica:0/task:116/device:CPU:0', '/job:worker/replica:0/task:118/device:CPU:0', '/job:worker/replica:0/task:119/device:CPU:0', '/job:worker/replica:0/task:121/device:CPU:0', '/job:worker/replica:0/task:123\r\n/device:CPU:0', '/job:worker/replica:0/task:122/device:CPU:0', '/job:worker/replica:0/task:120/device:CPU:0', '/job:worker/replica:0/task:124/device:CPU:0', '/job:worker/replica:0/task:126/device:CPU:0'\r\n, '/job:worker/replica:0/task:125/device:CPU:0', '/job:worker/replica:0/task:127/device:CPU:0']\r\nI1001 16:19:10.224856 139833409849152 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:1/device:CPU:0', '/job:wor[334/1854]\r\na:0/task:3/device:CPU:0', '/job:worker/replica:0/task:6/device:CPU:0', '/job:worker/replica:0/task:7/device:CPU:0', '/job:worker/replica:0/task:8/device:CPU:0', '/job:worker/replica:0/task:9/device:CPU:\r\n0', '/job:worker/replica:0/task:14/device:CPU:0', '/job:worker/replica:0/task:11/device:CPU:0', '/job:worker/replica:0/task:17/device:CPU:0', '/job:worker/replica:0/task:15/device:CPU:0', '/job:worker/r\r\neplica:0/task:19/device:CPU:0', '/job:worker/replica:0/task:16/device:CPU:0', '/job:worker/replica:0/task:23/device:CPU:0', '/job:worker/replica:0/task:22/device:CPU:0', '/job:worker/replica:0/task:24/d\r\nevice:CPU:0', '/job:worker/replica:0/task:25/device:CPU:0', '/job:worker/replica:0/task:27/device:CPU:0', '/job:worker/replica:0/task:30/device:CPU:0', '/job:worker/replica:0/task:33/device:CPU:0', '/jo\r\nb:worker/replica:0/task:31/device:CPU:0', '/job:worker/replica:0/task:32/device:CPU:0', '/job:worker/replica:0/task:35/device:CPU:0', '/job:worker/replica:0/task:39/device:CPU:0', '/job:worker/replica:0\r\n/task:4/device:CPU:0', '/job:worker/replica:0/task:34/device:CPU:0', '/job:worker/replica:0/task:12/device:CPU:0', '/job:worker/replica:0/task:20/device:CPU:0', '/job:worker/replica:0/task:28/device:CPU\r\n:0', '/job:worker/replica:0/task:2/device:CPU:0', '/job:worker/replica:0/task:36/device:CPU:0', '/job:worker/replica:0/task:26/device:CPU:0', '/job:worker/replica:0/task:10/device:CPU:0', '/job:worker/r\r\neplica:0/task:18/device:CPU:0', '/job:worker/replica:0/task:5/device:CPU:0', '/job:worker/replica:0/task:40/device:CPU:0', '/job:worker/replica:0/task:21/device:CPU:0', '/job:worker/replica:0/task:13/de\r\nvice:CPU:0', '/job:worker/replica:0/task:38/device:CPU:0', '/job:worker/replica:0/task:37/device:CPU:0', '/job:worker/replica:0/task:29/device:CPU:0', '/job:worker/replica:0/task:41/device:CPU:0', '/job\r\n:worker/replica:0/task:43/device:CPU:0', '/job:worker/replica:0/task:45/device:CPU:0', '/job:worker/replica:0/task:42/device:CPU:0', '/job:worker/replica:0/task:44/device:CPU:0', '/job:worker/replica:0/\r\ntask:46/device:CPU:0', '/job:worker/replica:0/task:47/device:CPU:0', '/job:worker/replica:0/task:48/device:CPU:0', '/job:worker/replica:0/task:49/device:CPU:0', '/job:worker/replica:0/task:50/device:CPU\r\n:0', '/job:worker/replica:0/task:51/device:CPU:0', '/job:worker/replica:0/task:53/device:CPU:0', '/job:worker/replica:0/task:52/device:CPU:0', '/job:worker/replica:0/task:55/device:CPU:0', '/job:worker/\r\nreplica:0/task:54/device:CPU:0', '/job:worker/replica:0/task:56/device:CPU:0', '/job:worker/replica:0/task:59/device:CPU:0', '/job:worker/replica:0/task:57/device:CPU:0', '/job:worker/replica:0/task:58/\r\ndevice:CPU:0', '/job:worker/replica:0/task:60/device:CPU:0', '/job:worker/replica:0/task:62/device:CPU:0', '/job:worker/replica:0/task:61/device:CPU:0', '/job:worker/replica:0/task:63/device:CPU:0', '/j\r\nob:worker/replica:0/task:64/device:CPU:0', '/job:worker/replica:0/task:65/device:CPU:0', '/job:worker/replica:0/task:67/device:CPU:0', '/job:worker/replica:0/task:66/device:CPU:0', '/job:worker/replica:\r\n0/task:68/device:CPU:0', '/job:worker/replica:0/task:70/device:CPU:0', '/job:worker/replica:0/task:69/device:CPU:0', '/job:worker/replica:0/task:71/device:CPU:0', '/job:worker/replica:0/task:72/device:C\r\nPU:0', '/job:worker/replica:0/task:73/device:CPU:0', '/job:worker/replica:0/task:74/device:CPU:0', '/job:worker/replica:0/task:75/device:CPU:0', '/job:worker/replica:0/task:78/device:CPU:0', '/job:worke\r\nr/replica:0/task:79/device:CPU:0', '/job:worker/replica:0/task:76/device:CPU:0', '/job:worker/replica:0/task:77/device:CPU:0', '/job:worker/replica:0/task:80/device:CPU:0', '/job:worker/replica:0/task:8\r\n2/device:CPU:0', '/job:worker/replica:0/task:81/device:CPU:0', '/job:worker/replica:0/task:85/device:CPU:0', '/job:worker/replica:0/task:84/device:CPU:0', '/job:worker/replica:0/task:83/device:CPU:0', '\r\n/job:worker/replica:0/task:86/device:CPU:0', '/job:worker/replica:0/task:87/device:CPU:0', '/job:worker/replica:0/task:88/device:CPU:0', '/job:worker/replica:0/task:89/device:CPU:0', '/job:worker/replic\r\na:0/task:92/device:CPU:0', '/job:worker/replica:0/task:90/device:CPU:0', '/job:worker/replica:0/task:93/device:CPU:0', '/job:worker/replica:0/task:91/device:CPU:0', '/job:worker/replica:0/task:94/device\r\n:CPU:0', '/job:worker/replica:0/task:95/device:CPU:0', '/job:worker/replica:0/task:98/device:CPU:0', '/job:worker/replica:0/task:96/device:CPU:0', '/job:worker/replica:0/task:97/device:CPU:0', '/job:wor\r\nker/replica:0/task:100/device:CPU:0', '/job:worker/replica:0/task:99/device:CPU:0', '/job:worker/replica:0/task:101/device:CPU:0', '/job:worker/replica:0/task:103/device:CPU:0', '/job:worker/replica:0/t\r\nask:102/device:CPU:0', '/job:worker/replica:0/task:104/device:CPU:0', '/job:worker/replica:0/task:106/device:CPU:0', '/job:worker/replica:0/task:105/device:CPU:0', '/job:worker/replica:0/task:107/device\r\n:CPU:0', '/job:worker/replica:0/task:108/device:CPU:0', '/job:worker/replica:0/task:109/device:CPU:0', '/job:worker/replica:0/task:111/device:CPU:0', '/job:worker/replica:0/task:110/device:CPU:0', '/job\r\n:worker/replica:0/task:113/device:CPU:0', '/job:worker/replica:0/task:112/device:CPU:0', '/job:worker/replica:0/task:114/device:CPU:0', '/job:worker/replica:0/task:115/device:CPU:0', '/job:worker/replic\r\na:0/task:117/device:CPU:0', '/job:worker/replica:0/task:116/device:CPU:0', '/job:worker/replica:0/task:118/device:CPU:0', '/job:worker/replica:0/task:119/device:CPU:0', '/job:worker/replica:0/task:121/d\r\nevice:CPU:0', '/job:worker/replica:0/task:123/device:CPU:0', '/job:worker/replica:0/task:122/device:CPU:0', '/job:worker/replica:0/task:120/device:CPU:0', '/job:worker/replica:0/task:124/device:CPU:0',\r\n'/job:worker/replica:0/task:126/device:CPU:0', '/job:worker/replica:0/task:125/device:CPU:0', '/job:worker/replica:0/task:127/device:CPU:0']\r\nINFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\r\n\r\nI1001 16:19:10.292326 139833409849152 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\r\n\r\nINFO:tensorflow:Starting infeed thread controller.\r\nI1001 16:19:10.425031 139829429921536 tpu_estimator.py:521] Starting infeed thread controller.\r\nINFO:tensorflow:Starting outfeed thread controller.\r\nI1001 16:19:10.425418 139829438314240 tpu_estimator.py:540] Starting outfeed thread controller.\r\nINFO:tensorflow:Before copy master to slices.\r\nI1001 16:19:10.435552 139833409849152 ops.py:5767] Before copy master to slices.\r\nI1001 16:19:10.577743 139829421528832 transport.py:151] Attempting refresh to obtain initial access_token\r\nWARNING:tensorflow:TPUPollingThread found TPU b'tpuv3-1024' in state READY, and health HEALTHY.\r\nW1001 16:19:10.706137 139829421528832 preempted_hook.py:91] TPUPollingThread found TPU b'tpuv3-1024' in state READY, and health HEALTHY.\r\nERROR:tensorflow:Error recorded from training_loop: From /job:worker/replica:0/task:0:\r\n3 root error(s) found.\r\n  (0) Failed precondition: The TPU system has not been initialized.\r\n         [[node shared/embedding_slot_vr_1/Slice_24 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n  (1) Failed precondition: The TPU system has not been initialized.\r\n         [[node shared/embedding_slot_vr_1/Slice_25 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n  (2) Failed precondition: The TPU system has not been initialized.\r\n         [[node shared/embedding_slot_vr_1/Slice_26 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n0 successful operations.\r\n6 derived errors ignored.\r\nOriginal stack trace for 'shared/embedding_slot_vr_1/Slice_24':                                                                                                                                 [280/1854]\r\n  File \"/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/site-packages/t5/models/mesh_transformer_main.py\", line 244, in <module>\r\n    console_entry_point()\r\n  File \"/site-packages/t5/models/mesh_transformer_main.py\", line 241, in console_entry_point\r\n    app.run(main)\r\n  File \"/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/site-packages/t5/models/mesh_transformer_main.py\", line 235, in main\r\n    model_dir=FLAGS.model_dir)\r\n  File \"/site-packages/gin/config.py\", line 1055, in gin_wrapper\r\n    return fn(*new_args, **new_kwargs)\r\n  File \"/site-packages/mesh_tensorflow/transformer/utils.py\", line 2115, in run\r\n    train_dataset_fn, train_steps, ensemble_inputs)\r\n  File \"/site-packages/mesh_tensorflow/transformer/utils.py\", line 1498, in train_model\r\n    estimator.train(input_fn=input_fn, max_steps=train_steps)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3030, in train\r\n    saving_listeners=saving_listeners)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1191, in _train_model_default\r\n    features, labels, ModeKeys.TRAIN, self.config)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 2857, in _call_model_fn\r\n    config)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1149, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3159, in _model_fn\r\n    _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3604, in _train_on_tpu_system\r\n    device_assignment=ctx.device_assignment)\r\n  File \"/site-packages/tensorflow_core/python/tpu/tpu.py\", line 1277, in split_compile_and_shard\r\n    name=name)\r\n  File \"/site-packages/tensorflow_core/python/tpu/tpu.py\", line 992, in split_compile_and_replicate\r\n    outputs = computation(*computation_inputs)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3589, in multi_tpu_train_steps_on_single_shard\r\n    inputs=[0, _INITIAL_LOSS])\r\n  File \"/site-packages/tensorflow_core/python/tpu/training_loop.py\", line 178, in while_loop\r\n    condition_wrapper, body_wrapper, inputs, name=\"\", parallel_iterations=1)\r\n  File \"/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2753, in while_loop\r\n    return_same_structure)\r\n  File \"/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2245, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2170, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/site-packages/tensorflow_core/python/tpu/training_loop.py\", line 121, in body_wrapper\r\n    outputs = body(*(inputs + dequeue_ops))\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3588, in <lambda>\r\n    lambda i, loss: [i + 1, single_tpu_train_step(i)],\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 1715, in train_step\r\n    self._call_model_fn(features, labels))\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 1994, in _call_model_fn\r\n    estimator_spec = self._model_fn(features=features, **kwargs)\r\n  File \"/site-packages/mesh_tensorflow/transformer/utils.py\", line 672, in my_model_fn\r\n     log_file=model_info_file)                                                                                                                                                                             \r\n  File \"/site-packages/mesh_tensorflow/ops.py\", line 726, in __init__                                                                                                                                     \r\n    op.lower(self)\r\n  File \"/site-packages/mesh_tensorflow/ops.py\", line 4038, in lower\r\n    sv = mesh_impl.LaidOutVariable(self, mesh_impl)\r\n  File \"/site-packages/mesh_tensorflow/simd_mesh_impl.py\", line 212, in __init__\r\n    variable.get_master(), shape, slices, slice_shape)\r\n  File \"/site-packages/mesh_tensorflow/simd_mesh_impl.py\", line 250, in _gen_copy_master_to_slices_op\r\n    slice_begin, slice_shape)\r\n  File \"/site-packages/tensorflow_core/python/ops/array_ops.py\", line 855, in slice\r\n    return gen_array_ops._slice(input_, begin, size, name=name)\r\n  File \"/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 9222, in _slice\r\n    \"Slice\", input=input, begin=begin, size=size, name=name)\r\n  File \"/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nE1001 16:19:34.235139 139833409849152 error_handling.py:75] Error recorded from training_loop: From /job:worker/replica:0/task:0:\r\n3 root error(s) found.\r\n  (0) Failed precondition: The TPU system has not been initialized.\r\n         [[node shared/embedding_slot_vr_1/Slice_24 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n  (1) Failed precondition: The TPU system has not been initialized.\r\n         [[node shared/embedding_slot_vr_1/Slice_25 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n  (2) Failed precondition: The TPU system has not been initialized.\r\n         [[node shared/embedding_slot_vr_1/Slice_26 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n0 successful operations.\r\n6 derived errors ignored.\r\n\r\nOriginal stack trace for 'shared/embedding_slot_vr_1/Slice_24':\r\n  File \"/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/site-packages/t5/models/mesh_transformer_main.py\", line 244, in <module>\r\n    console_entry_point()\r\n  File \"/site-packages/t5/models/mesh_transformer_main.py\", line 241, in console_entry_point\r\n    app.run(main)\r\n  File \"/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/site-packages/t5/models/mesh_transformer_main.py\", line 235, in main\r\n    model_dir=FLAGS.model_dir)\r\n  File \"/site-packages/gin/config.py\", line 1055, in gin_wrapper\r\n    return fn(*new_args, **new_kwargs)\r\n  File \"/site-packages/mesh_tensorflow/transformer/utils.py\", line 2115, in run\r\n    train_dataset_fn, train_steps, ensemble_inputs)\r\n  File \"/site-packages/mesh_tensorflow/transformer/utils.py\", line 1498, in train_model\r\n    estimator.train(input_fn=input_fn, max_steps=train_steps)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3030, in train\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1191, in _train_model_default\r\n    features, labels, ModeKeys.TRAIN, self.config)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 2857, in _call_model_fn\r\n    config)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1149, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3159, in _model_fn\r\n    _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3604, in _train_on_tpu_system\r\n    device_assignment=ctx.device_assignment)\r\n  File \"/site-packages/tensorflow_core/python/tpu/tpu.py\", line 1277, in split_compile_and_shard\r\n    name=name)\r\n  File \"/site-packages/tensorflow_core/python/tpu/tpu.py\", line 992, in split_compile_and_replicate\r\n    outputs = computation(*computation_inputs)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3589, in multi_tpu_train_steps_on_single_shard\r\n    inputs=[0, _INITIAL_LOSS])\r\n  File \"/site-packages/tensorflow_core/python/tpu/training_loop.py\", line 178, in while_loop\r\n    condition_wrapper, body_wrapper, inputs, name=\"\", parallel_iterations=1)\r\n  File \"/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2753, in while_loop\r\n    return_same_structure)\r\n  File \"/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2245, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2170, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/site-packages/tensorflow_core/python/tpu/training_loop.py\", line 121, in body_wrapper\r\n    outputs = body(*(inputs + dequeue_ops))\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3588, in <lambda>\r\n    lambda i, loss: [i + 1, single_tpu_train_step(i)],\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 1715, in train_step\r\n    self._call_model_fn(features, labels))\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 1994, in _call_model_fn\r\n    estimator_spec = self._model_fn(features=features, **kwargs)\r\n  File \"/site-packages/mesh_tensorflow/transformer/utils.py\", line 672, in my_model_fn\r\n    log_file=model_info_file)\r\n  File \"/site-packages/mesh_tensorflow/ops.py\", line 726, in __init__\r\n    op.lower(self)\r\n  File \"/site-packages/mesh_tensorflow/ops.py\", line 4038, in lower\r\n    sv = mesh_impl.LaidOutVariable(self, mesh_impl)\r\n  File \"/site-packages/mesh_tensorflow/simd_mesh_impl.py\", line 212, in __init__\r\n    variable.get_master(), shape, slices, slice_shape)\r\n  File \"/site-packages/mesh_tensorflow/simd_mesh_impl.py\", line 250, in _gen_copy_master_to_slices_op\r\n    slice_begin, slice_shape)\r\n  File \"/site-packages/tensorflow_core/python/ops/array_ops.py\", line 855, in slice\r\n    return gen_array_ops._slice(input_, begin, size, name=name)\r\n  File \"/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 9222, in _slice\r\n    \"Slice\", input=input, begin=begin, size=size, name=name)\r\n  File \"/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nINFO:tensorflow:training_loop marked as finished\r\nI1001 16:19:34.237687 139833409849152 error_handling.py:101] training_loop marked as finished\r\nWARNING:tensorflow:Reraising captured error\r\nW1001 16:19:34.237852 139833409849152 error_handling.py:135] Reraising captured error\r\nTraceback (most recent call last):\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/t5/models/mesh_transformer_main.py\", line 244, in <module>\r\n    console_entry_point()\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/t5/models/mesh_transformer_main.py\", line 241, in console_entry_point\r\n    app.run(main)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/t5/models/mesh_transformer_main.py\", line 235, in main\r\n    model_dir=FLAGS.model_dir)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/gin/config.py\", line 1078, in gin_wrapper\r\n    utils.augment_exception_message_and_reraise(e, err_str)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/gin/utils.py\", line 49, in augment_exception_message_and_reraise\r\n    six.raise_from(proxy.with_traceback(exception.__traceback__), None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/gin/config.py\", line 1055, in gin_wrapper\r\n    return fn(*new_args, **new_kwargs)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/mesh_tensorflow/transformer/utils.py\", line 2115, in run\r\n    train_dataset_fn, train_steps, ensemble_inputs)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/mesh_tensorflow/transformer/utils.py\", line 1498, in train_model\r\n    estimator.train(input_fn=input_fn, max_steps=train_steps)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3035, in train\r\n    rendezvous.raise_errors()\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py\", line 136, in raise_errors\r\n    six.reraise(typ, value, traceback)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3030, in train\r\n    saving_listeners=saving_listeners)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\r\n    saving_listeners)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1494, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 754, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1254, in run\r\n    self._sess = self._create_session()\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1212, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\", line 885, in create_session\r\n    hook.after_create_session(self.tf_sess, self.coord)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/mesh_tensorflow/ops.py\", line 5768, in after_create_session\r\n    session.run(self._op)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 956, in run                                                                          \r\n    run_metadata_ptr)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"/home/ahmed/anaconda3/envs/t5_v1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: From /job:worker/replica:0/task:0:\r\n3 root error(s) found.\r\n  (0) Failed precondition: The TPU system has not been initialized.\r\n         [[node shared/embedding_slot_vr_1/Slice_24 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n  (1) Failed precondition: The TPU system has not been initialized.\r\n         [[node shared/embedding_slot_vr_1/Slice_25 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n  (2) Failed precondition: The TPU system has not been initialized.\r\n         [[node shared/embedding_slot_vr_1/Slice_26 (defined at /site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n0 successful operations.\r\n6 derived errors ignored.\r\n\r\nOriginal stack trace for 'shared/embedding_slot_vr_1/Slice_24':\r\n  File \"/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/site-packages/t5/models/mesh_transformer_main.py\", line 244, in <module>\r\n    console_entry_point()\r\n  File \"/site-packages/t5/models/mesh_transformer_main.py\", line 241, in console_entry_point\r\n    app.run(main)\r\n  File \"/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/site-packages/t5/models/mesh_transformer_main.py\", line 235, in main\r\n    model_dir=FLAGS.model_dir)\r\n  File \"/site-packages/gin/config.py\", line 1055, in gin_wrapper\r\n    return fn(*new_args, **new_kwargs)\r\n  File \"/site-packages/mesh_tensorflow/transformer/utils.py\", line 2115, in run\r\n    train_dataset_fn, train_steps, ensemble_inputs)\r\n  File \"/site-packages/mesh_tensorflow/transformer/utils.py\", line 1498, in train_model\r\n    estimator.train(input_fn=input_fn, max_steps=train_steps)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3030, in train\r\n    saving_listeners=saving_listeners)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1191, in _train_model_default\r\n    features, labels, ModeKeys.TRAIN, self.config)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 2857, in _call_model_fn\r\n    config)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1149, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3159, in _model_fn\r\n      _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3604, in _train_on_tpu_system\r\n    device_assignment=ctx.device_assignment)\r\n  File \"/site-packages/tensorflow_core/python/tpu/tpu.py\", line 1277, in split_compile_and_shard\r\n    name=name)\r\n  File \"/site-packages/tensorflow_core/python/tpu/tpu.py\", line 992, in split_compile_and_replicate\r\n    outputs = computation(*computation_inputs)\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3589, in multi_tpu_train_steps_on_single_shard\r\n    inputs=[0, _INITIAL_LOSS])\r\n  File \"/site-packages/tensorflow_core/python/tpu/training_loop.py\", line 178, in while_loop\r\n    condition_wrapper, body_wrapper, inputs, name=\"\", parallel_iterations=1)\r\n  File \"/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2753, in while_loop\r\n    return_same_structure)\r\n  File \"/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2245, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2170, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/site-packages/tensorflow_core/python/tpu/training_loop.py\", line 121, in body_wrapper\r\n    outputs = body(*(inputs + dequeue_ops))\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3588, in <lambda>\r\n    lambda i, loss: [i + 1, single_tpu_train_step(i)],\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 1715, in train_step\r\n    self._call_model_fn(features, labels))\r\n  File \"/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 1994, in _call_model_fn\r\n    estimator_spec = self._model_fn(features=features, **kwargs)\r\n  File \"/site-packages/mesh_tensorflow/transformer/utils.py\", line 672, in my_model_fn\r\n    log_file=model_info_file)\r\n  File \"/site-packages/mesh_tensorflow/ops.py\", line 726, in __init__\r\n    op.lower(self)\r\n  File \"/site-packages/mesh_tensorflow/ops.py\", line 4038, in lower\r\n    sv = mesh_impl.LaidOutVariable(self, mesh_impl)\r\n  File \"/site-packages/mesh_tensorflow/simd_mesh_impl.py\", line 212, in __init__\r\n    variable.get_master(), shape, slices, slice_shape)\r\n  File \"/site-packages/mesh_tensorflow/simd_mesh_impl.py\", line 250, in _gen_copy_master_to_slices_op\r\n    slice_begin, slice_shape)\r\n  File \"/site-packages/tensorflow_core/python/ops/array_ops.py\", line 855, in slice\r\n    return gen_array_ops._slice(input_, begin, size, name=name)\r\n  File \"/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 9222, in _slice\r\n    \"Slice\", input=input, begin=begin, size=size, name=name)\r\n  File \"/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n  In call to configurable 'run' (<function run at 0x7f2d3bc6df80>)\r\n```\r\n\r\nAny idea how I could fix this issue ?\r\n", "comments": ["@agemagician \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I have switched to tensorflow 2.3.1 and I don't have this issue anymore. Apparently it is an issue with tensorflow 1.15.3.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43707\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43707\">No</a>\n"]}, {"number": 43705, "title": "Project Gutenberg: make printing on TPUs easy!", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like to easily print the value of a tensor within a function decorated by `@tf.function`, on TPUs. It is currently only possible through various unpleasant, time-consuming, and error-prone hacks. \r\n**Will this change the current api? How?**\r\nI don't think so, `tf.print` should cover it.\r\n**Who will benefit with this feature?**\r\nAnyone who uses TensorFlow on TPUs.\r\n**Any Other info.** \r\n", "comments": ["tf.print will work on TPUs if the config option set_soft_device_placement is true (see https://www.tensorflow.org/api_docs/python/tf/config/set_soft_device_placement).  This option is by default false because it can lead to a decrease in performance and should be disabled once debugging with tf.print is complete.", "> tf.print will work on TPUs if the config option set_soft_device_placement is true (see https://www.tensorflow.org/api_docs/python/tf/config/set_soft_device_placement). This option is by default false because it can lead to a decrease in performance and should be disabled once debugging with tf.print is complete.\r\n\r\nthis didn't work for me. Here's a minimal example in colab: https://colab.research.google.com/drive/1kFdR9tpj7zJjwHIbWRVjVSiA9zwviErK?usp=sharing\r\n\r\nI wasn't sure where the config statement should be, so I tried placing it the three obvious scopes (the tf.function, normal python, and the TPU strategy) to no avail. "]}, {"number": 43704, "title": "0.6.0", "body": "Typo in Documentation", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43704) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 43703, "title": "For a dataset of nested elements `window` creates a dataset of nested datasets of flat elements, not a dataset of datasets of nested elements.", "body": "For a dataset of nested elements `window` creates a dataset of nested datasets of flat elements, not a dataset of datasets of nested elements.\r\n\r\nIn other words, the `x` in your `lambda` will be a dictionary mapping keys to datasets and if you wish to flatten it to a single dataset you will need to do:\r\n\r\n```\r\ndef map_fn(x):\r\n  result = {}\r\n  for key in x.keys():\r\n    result[key] = tf.data.experimental.get_single_element(x[key].batch(3))\r\n  return result\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1,2,3,4]}).window(3).map(map_fn)\r\n```\r\n\r\n_Originally posted by @jsimsa in https://github.com/tensorflow/tensorflow/issues/23581#issuecomment-529702702_\r\n\r\nWhen running the above, I am able to get a Dataset of dictionnaries of tensors of shape (window_size,).\r\nHowever, it seems that batching futher such object then triggers an 'out or range' error in model training, and even when going through the elements. Such error seem to occur only when number of windows are not amultiple of the final batch size..\r\n\r\n```\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\nwindow_size = 30\r\n\r\ndef map_fn(x):\r\n  result = {}\r\n  for key in x.keys():\r\n    result[key] = tf.data.experimental.get_single_element(x[key].batch(window_size))\r\n  return result\r\n\r\ndata = pd.DataFrame(\r\n    np.random.random_sample((200,5)),\r\n    columns=['feature1', 'feature2', 'feature3', 'feature4', 'feature5'],\r\n)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(dict(data))\r\ndataset = dataset.window(window_size, drop_remainder=True)\r\ndataset = dataset.map(map_fn)\r\n\r\nfor elem in dataset:\r\n    print(elem)\r\n\r\ndataset = dataset.batch(32)\r\n\r\nfor elem in dataset:\r\n    print(elem)\r\n```\r\n", "comments": ["This is working as intended so that you can for instance apply different batching logic on different elements of the nest.", "You can try the code I wrote: it is not working, it returns \"out or range\" error for some window_size, batch numbers (such as those I entered).", "Sorry, the title of the issue mislead me and I didn't read the details. I can confirm I can reproduce the issue and will investigate.", "There was indeed a bug and I have a pending fix out which when submitted will autoclose this issue.", "Hello, thanks for re-checking, however I installed tf-nightly today, and still have the issue when running the code above..", "@scd75 \r\n\r\nI am not seeing any issue in nightly version.Please,find the gist [here](https://colab.research.google.com/gist/ravikyram/d92fd919aaeac0f7137b15b849d46dab/untitled412.ipynb).Thanks!", "Hi, thanks I get it.. I'm working on a windows env, and for some reason the last tf-nightly version available for windows on Pypi is 20201002. Will re-check when available.", "so I got this method working..\r\nMy issue is that I want to create windowed datasets with named features. I actually found out that this method is super slow when running training, taking ages to fill shuffle buffer at the begining of each epoch.\r\n\r\nI ended up trying another approach. Here is the code of the 2 approaches I coded, first one being based on the method above, 2nd one being based on an \"ex-post\" naming of each feature.\r\nThe filling up of buffer at the begining of each epoch is 10 times faster with the 2nd method. The main issue with the 2nd method is that it requires all features to have the same dtype, as they are processed through a signle tensor..\r\n\r\nI think it would be very useful to have a better / cleaner way of generating window datasets with named features, while preserving speed at training time. Would appreciate if you could dig into that.. Thanks!\r\n\r\nFollowing is my code for the 2 methods:\r\n\r\n```\r\n\r\ndef make_window_dataset_tf(data, target_key, lags, horizon, shift=1, stride=1, shuffle=False, seed=None, batch_size=32):\r\n    \r\n    def map_fn(x):\r\n        result = {}\r\n        for key in x.keys():\r\n            result[key] = tf.data.experimental.get_single_element(x[key].batch(lags+horizon))\r\n        return result\r\n\r\n    ds = tf.data.Dataset.from_tensor_slices(dict(data))\r\n    ds = ds.window(size=lags+horizon, shift=shift, stride=stride,drop_remainder=True)\r\n    ds = ds.map(map_fn)\r\n    ds = ds.map(lambda x: (x, x[target_key][-horizon:]))\r\n    if shuffle:\r\n        ds = ds.shuffle(buffer_size=len(data),seed=seed)\r\n    ds = ds.batch(batch_size)\r\n\r\n    return ds\r\n\r\n\r\ndef make_window_dataset_tf2(data, target_key, lags, horizon, shift=1, stride=1, shuffle=False, seed=None, batch_size=32):\r\n\r\n    def tensor_to_dict(x):\r\n        dic = {}\r\n        for i, key in enumerate(data):\r\n            dic[key] = x[Ellipsis,i:i+1]\r\n        return dic\r\n\r\n    ds = tf.data.Dataset.from_tensor_slices(data)\r\n    ds = ds.window(size=lags+horizon, shift=shift, stride=stride,drop_remainder=True)\r\n    ds = ds.flat_map(lambda x: x.batch(lags+horizon))\r\n    ds = ds.map(tensor_to_dict)\r\n    ds = ds.map(lambda x: (x, x[target_key][-horizon:]))\r\n    if shuffle:\r\n        ds = ds.shuffle(buffer_size=len(data),seed=seed)\r\n    ds = ds.batch(batch_size)\r\n\r\n    return ds\r\n```"]}, {"number": 43702, "title": "Memory leaks in repeated model training despite garbage collection and session clearing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, see below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows and Linux\r\n- TensorFlow installed from (source or binary): `pip`\r\n- TensorFlow version (use command below): `v2.3.0-54-gfcc4b966f1 2.3.1`\r\n- Python version: `3.8.5`\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n**Describe the current behavior**\r\nThe code below leaks memory.\r\n\r\nFrom my local, CPU-only system:\r\n<details>\r\n\r\n```\r\niteration 0: rss 368 MB\r\niteration 1: rss 400 MB  # new maximum\r\niteration 2: rss 402 MB  # new maximum\r\niteration 3: rss 439 MB  # new maximum\r\niteration 4: rss 431 MB\r\niteration 5: rss 444 MB  # new maximum\r\niteration 6: rss 442 MB\r\niteration 7: rss 447 MB  # new maximum\r\niteration 8: rss 445 MB\r\niteration 9: rss 461 MB\r\niteration 10: rss 449 MB  # new maximum\r\niteration 11: rss 462 MB  # new maximum\r\niteration 12: rss 460 MB\r\niteration 13: rss 466 MB  # new maximum\r\niteration 14: rss 463 MB\r\niteration 15: rss 483 MB  # new maximum\r\niteration 16: rss 468 MB\r\niteration 17: rss 490 MB  # new maximum\r\niteration 18: rss 476 MB\r\niteration 19: rss 489 MB\r\niteration 20: rss 478 MB\r\niteration 21: rss 491 MB  # new maximum\r\niteration 22: rss 480 MB\r\niteration 23: rss 492 MB  # new maximum\r\niteration 24: rss 480 MB\r\niteration 25: rss 497 MB  # new maximum\r\niteration 26: rss 481 MB\r\niteration 27: rss 493 MB\r\niteration 28: rss 483 MB\r\niteration 29: rss 493 MB\r\niteration 30: rss 486 MB\r\niteration 31: rss 488 MB\r\niteration 32: rss 487 MB\r\niteration 33: rss 489 MB\r\niteration 34: rss 498 MB  # new maximum\r\niteration 35: rss 487 MB\r\niteration 36: rss 494 MB\r\niteration 37: rss 493 MB\r\niteration 38: rss 489 MB\r\niteration 39: rss 500 MB  # new maximum\r\niteration 40: rss 487 MB\r\niteration 41: rss 501 MB  # new maximum\r\niteration 42: rss 494 MB\r\niteration 43: rss 493 MB\r\niteration 44: rss 502 MB  # new maximum\r\niteration 45: rss 510 MB  # new maximum\r\niteration 46: rss 503 MB\r\niteration 47: rss 497 MB\r\niteration 48: rss 494 MB\r\niteration 49: rss 507 MB\r\niteration 50: rss 497 MB\r\niteration 51: rss 507 MB\r\niteration 52: rss 499 MB\r\niteration 53: rss 499 MB\r\niteration 54: rss 507 MB\r\niteration 55: rss 497 MB\r\niteration 56: rss 507 MB\r\niteration 57: rss 500 MB\r\niteration 58: rss 501 MB\r\niteration 59: rss 508 MB\r\niteration 60: rss 498 MB\r\niteration 61: rss 507 MB\r\niteration 62: rss 501 MB\r\niteration 63: rss 502 MB\r\niteration 64: rss 510 MB\r\niteration 65: rss 501 MB\r\niteration 66: rss 511 MB  # new maximum\r\niteration 67: rss 503 MB\r\niteration 68: rss 502 MB\r\niteration 69: rss 512 MB\r\niteration 70: rss 500 MB\r\niteration 71: rss 512 MB  # new maximum\r\niteration 72: rss 505 MB\r\niteration 73: rss 506 MB\r\niteration 74: rss 512 MB\r\niteration 75: rss 522 MB  # new maximum\r\niteration 76: rss 509 MB\r\niteration 77: rss 507 MB\r\niteration 78: rss 507 MB\r\niteration 79: rss 514 MB\r\niteration 80: rss 525 MB  # new maximum\r\niteration 81: rss 514 MB\r\niteration 82: rss 508 MB\r\niteration 83: rss 507 MB\r\niteration 84: rss 516 MB\r\niteration 85: rss 527 MB  # new maximum\r\niteration 86: rss 513 MB\r\niteration 87: rss 511 MB\r\niteration 88: rss 509 MB\r\niteration 89: rss 516 MB\r\niteration 90: rss 508 MB\r\niteration 91: rss 511 MB\r\niteration 92: rss 514 MB\r\niteration 93: rss 509 MB\r\niteration 94: rss 518 MB\r\niteration 95: rss 508 MB\r\niteration 96: rss 513 MB\r\niteration 97: rss 514 MB\r\niteration 98: rss 509 MB\r\niteration 99: rss 519 MB\r\n```\r\n\r\n</details>\r\n\r\n\r\nFrom Colab:\r\n<details>\r\n\r\n```\r\niteration 0: rss 547 MB\r\niteration 1: rss 652 MB  # new maximum\r\niteration 2: rss 671 MB  # new maximum\r\niteration 3: rss 674 MB  # new maximum\r\niteration 4: rss 674 MB\r\niteration 5: rss 674 MB\r\niteration 6: rss 674 MB\r\niteration 7: rss 675 MB  # new maximum\r\niteration 8: rss 680 MB  # new maximum\r\niteration 9: rss 680 MB\r\niteration 10: rss 689 MB  # new maximum\r\niteration 11: rss 689 MB\r\niteration 12: rss 689 MB\r\niteration 13: rss 689 MB\r\niteration 13: rss 689 MB\r\niteration 14: rss 689 MB\r\niteration 15: rss 689 MB\r\niteration 16: rss 689 MB\r\niteration 17: rss 689 MB\r\niteration 18: rss 689 MB\r\niteration 19: rss 689 MB\r\niteration 20: rss 689 MB\r\niteration 21: rss 689 MB\r\niteration 22: rss 689 MB\r\niteration 23: rss 689 MB\r\niteration 24: rss 689 MB\r\niteration 25: rss 689 MB\r\niteration 26: rss 689 MB\r\niteration 27: rss 689 MB\r\niteration 28: rss 689 MB\r\niteration 29: rss 689 MB\r\niteration 30: rss 689 MB\r\niteration 31: rss 689 MB\r\niteration 32: rss 689 MB\r\niteration 33: rss 689 MB\r\niteration 34: rss 689 MB\r\niteration 35: rss 691 MB  # new maximum\r\niteration 36: rss 691 MB\r\niteration 37: rss 697 MB  # new maximum\r\niteration 38: rss 702 MB  # new maximum\r\niteration 39: rss 704 MB  # new maximum\r\niteration 40: rss 704 MB\r\niteration 41: rss 704 MB\r\niteration 42: rss 704 MB\r\niteration 43: rss 704 MB\r\niteration 44: rss 704 MB\r\niteration 45: rss 704 MB\r\niteration 46: rss 704 MB\r\niteration 47: rss 704 MB\r\niteration 48: rss 704 MB\r\niteration 49: rss 704 MB\r\niteration 50: rss 704 MB\r\niteration 51: rss 704 MB\r\niteration 52: rss 704 MB\r\niteration 53: rss 704 MB\r\niteration 54: rss 704 MB\r\niteration 55: rss 704 MB\r\niteration 56: rss 704 MB\r\niteration 57: rss 704 MB\r\niteration 58: rss 704 MB\r\niteration 59: rss 704 MB\r\niteration 60: rss 704 MB\r\niteration 61: rss 704 MB\r\niteration 62: rss 704 MB\r\niteration 63: rss 704 MB\r\niteration 64: rss 704 MB\r\niteration 65: rss 704 MB\r\niteration 66: rss 704 MB\r\niteration 67: rss 704 MB\r\niteration 68: rss 704 MB\r\niteration 69: rss 704 MB\r\niteration 70: rss 704 MB\r\niteration 71: rss 704 MB\r\niteration 72: rss 704 MB\r\niteration 73: rss 704 MB\r\niteration 74: rss 704 MB\r\niteration 75: rss 704 MB\r\niteration 76: rss 705 MB  # new maximum\r\niteration 77: rss 705 MB\r\niteration 78: rss 705 MB\r\niteration 79: rss 705 MB\r\niteration 80: rss 705 MB\r\niteration 81: rss 705 MB\r\niteration 82: rss 706 MB  # new maximum\r\niteration 83: rss 706 MB\r\niteration 84: rss 713 MB  # new maximum\r\niteration 85: rss 713 MB\r\niteration 86: rss 713 MB\r\niteration 87: rss 713 MB\r\niteration 88: rss 719 MB  # new maximum\r\niteration 89: rss 719 MB\r\niteration 90: rss 719 MB\r\niteration 91: rss 719 MB\r\niteration 92: rss 719 MB\r\niteration 93: rss 719 MB\r\niteration 94: rss 719 MB\r\niteration 95: rss 719 MB\r\niteration 96: rss 719 MB\r\niteration 97: rss 720 MB  # new maximum\r\niteration 98: rss 720 MB\r\niteration 99: rss 720 MB\r\n```\r\n\r\n</details>\r\n\r\n**Describe the expected behavior**\r\nMemory consumptions stays constant\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport gc\r\nimport os\r\n\r\nimport numpy as np\r\nimport psutil\r\nimport tensorflow as tf\r\n\r\ntf.get_logger().setLevel(\"ERROR\")  # Suppress \"tf.function retracing\" warnings\r\nprocess = psutil.Process(os.getpid())\r\nfor i in range(100):\r\n    # do some work\r\n    model = tf.keras.applications.mobilenet.MobileNet()\r\n    model.compile(loss=\"mse\")\r\n    x = tf.zeros((1, *model.input.shape[1:]))\r\n    y = tf.zeros((1, *model.output.shape[1:]))\r\n    history = model.fit(x=x, y=y, verbose=0)\r\n    \r\n    # clean up\r\n    _ = gc.collect()\r\n    tf.keras.backend.clear_session()\r\n    \r\n    # show memory usage\r\n    print(f\"iteration {i}: rss {process.memory_info().rss >> 20} MB\")\r\n\r\n```\r\n\r\n**Other info / logs**\r\n- See also https://stackoverflow.com/q/63411142/\r\n", "comments": ["Note: in the example above,\r\n\r\n- when commenting out the `model.fit` line, or\r\n- when working with constant data (assigning `x` and `y` outside the loop),\r\n\r\nmemory consumption stays constant.\r\n\r\nThis example reproduces the issue with a much smaller network. Note that this example does use constant data:\r\n```python\r\nimport gc\r\nimport os\r\n\r\nimport psutil\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ntf.get_logger().setLevel(\"ERROR\")  # Suppress \"tf.function retracing\" warnings\r\nprocess = psutil.Process(os.getpid())\r\nprev_mem = 0\r\nfirst_mem = 0\r\nfor i in range(100):\r\n    # do some work\r\n    in_layer = keras.layers.Input(shape=(1,))\r\n    hidden_layer = in_layer\r\n    for _ in range(200):\r\n        hidden_layer = keras.layers.Dense(1)(hidden_layer)\r\n    out_layer = hidden_layer\r\n    model = keras.Model(inputs=in_layer, outputs=out_layer)\r\n    model.compile(loss=\"mse\")\r\n    history = model.fit(x=[0], y=[0], verbose=0)\r\n    \r\n    # clean up\r\n    _ = gc.collect()\r\n    keras.backend.clear_session()\r\n    \r\n    # show memory usage\r\n    mem = process.memory_info().rss\r\n    if i == 0:\r\n        first_mem = mem\r\n    print(\r\n        f\"iteration {i}: rss {mem >> 20} MB ({(mem - prev_mem) >> 10:+} KB; \"\r\n        + f\"{((mem - first_mem) // max(1, i)) >> 10:+} KB/it.)\"\r\n    )\r\n    prev_mem = mem\r\n\r\n\r\n```\r\n\r\nPlaying with the number of hidden layers and the number of nodes suggests this is related to the number of the layers, not the number of parameters:\r\n- 0 hidden layers leads to constant memory consumption\r\n- 1 hidden layer leads to several KB leak per iteration, even with thousands of nodes\r\n- 200 hidden layers with a single node (400 parameters) lead to several MB leaking per iteration. This is the case in the extreme example above, from Colab:\r\n\r\n```\r\niteration 0: rss 590 MB (+605152 KB; +0 KB/it.)\r\niteration 1: rss 786 MB (+200200 KB; +200200 KB/it.)\r\niteration 2: rss 790 MB (+4108 KB; +102154 KB/it.)\r\niteration 3: rss 803 MB (+13300 KB; +72536 KB/it.)\r\niteration 4: rss 807 MB (+4476 KB; +55521 KB/it.)\r\niteration 5: rss 813 MB (+5652 KB; +45547 KB/it.)\r\niteration 6: rss 814 MB (+784 KB; +38086 KB/it.)\r\niteration 7: rss 817 MB (+3140 KB; +33094 KB/it.)\r\niteration 8: rss 821 MB (+4376 KB; +29504 KB/it.)\r\niteration 9: rss 822 MB (+1152 KB; +26354 KB/it.)\r\niteration 10: rss 825 MB (+3384 KB; +24057 KB/it.)\r\niteration 11: rss 827 MB (+1840 KB; +22037 KB/it.)\r\niteration 12: rss 828 MB (+1140 KB; +20296 KB/it.)\r\niteration 13: rss 831 MB (+2636 KB; +18937 KB/it.)\r\niteration 14: rss 832 MB (+1320 KB; +17679 KB/it.)\r\n...\r\n```", "@bersbersbers it is the same issue as: https://github.com/tensorflow/tensorflow/issues/43627", "@bersbersbers I do not see a fix for this any time soon. In meanwhile use this as a workaround:\r\nhttps://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch", "@bersbersbers The link above will help. I have tested it.", "> @bersbersbers I do not see a fix for this any time soon. \r\n\r\nMay I ask what makes you think that? You reported the other issue just 3 days ago, and as far as I can see, did not provide a working reproduction code.\r\n\r\n> In meanwhile use this as a workaround:\r\n> https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\r\n\r\nThis is ten pages of documentation - which section is the \"workaround\" (other than implementing the whole fitting routine myself)?", "@bersbersbers You are right I have updated the link. This is correct:\r\nhttps://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch", "@bersbersbers to answer the second part of your question. You need to make your own fitting loop from scratch, and that is a way to do the training without applying the fit method.", "@markodjordjic: thanks, but I'm not quite willing to invest that effort.\r\n\r\nBy the way, each of your many individual messages triggers an individual email notification. Could you try to post your thoughts in a combined message? This is not an IRC chat here ;) You could also edit past message if you want to add anything.", "@bersbersbers,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/76dcef813f52ae31dd0bf018dd9d610d/43702.ipynb). However, the issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/97485bb1ae453d377e8bbea8a95c5c82/43702-tf-nightly.ipynb).\r\n\r\nAfter the first few epochs, I see that the increase in memory consumption is almost negligible. Please take a look at the linked gist for reference. Thanks!", "@amahendrakar interesting, thank you for testing this. I went ahead with `pip uninstall tensorflow` , `pip install tf-nightly`.\r\n\r\nI see that memory consumption stabilizes after ~10 iterations in your `tf-nightly` examples on Colab.\r\n\r\nRunning the code examples from the top post and from https://github.com/tensorflow/tensorflow/issues/43702#issuecomment-702069826 locally, however, I still get this:\r\n\r\nThe first example still takes much longer to stabilize:\r\n```\r\niteration 0: rss 357 MB\r\niteration 1: rss 384 MB\r\niteration 2: rss 390 MB\r\niteration 3: rss 409 MB\r\niteration 4: rss 420 MB\r\niteration 5: rss 427 MB\r\niteration 6: rss 427 MB\r\niteration 7: rss 439 MB\r\niteration 8: rss 440 MB\r\niteration 9: rss 442 MB\r\niteration 10: rss 441 MB\r\niteration 11: rss 443 MB\r\niteration 12: rss 442 MB\r\niteration 13: rss 445 MB\r\niteration 14: rss 456 MB\r\niteration 15: rss 446 MB\r\niteration 16: rss 444 MB\r\niteration 17: rss 460 MB\r\niteration 18: rss 447 MB\r\niteration 19: rss 461 MB\r\niteration 20: rss 462 MB\r\niteration 21: rss 464 MB\r\niteration 22: rss 451 MB\r\niteration 23: rss 463 MB\r\niteration 24: rss 466 MB\r\niteration 25: rss 463 MB\r\niteration 26: rss 452 MB\r\niteration 27: rss 464 MB\r\niteration 28: rss 451 MB\r\niteration 29: rss 464 MB\r\niteration 30: rss 466 MB\r\niteration 31: rss 468 MB\r\niteration 32: rss 467 MB\r\niteration 33: rss 468 MB\r\niteration 34: rss 470 MB\r\niteration 35: rss 469 MB\r\niteration 36: rss 469 MB\r\niteration 37: rss 469 MB\r\niteration 38: rss 471 MB\r\niteration 39: rss 471 MB\r\niteration 40: rss 468 MB\r\niteration 41: rss 469 MB\r\niteration 42: rss 470 MB\r\niteration 43: rss 469 MB\r\niteration 44: rss 470 MB\r\niteration 45: rss 469 MB\r\niteration 46: rss 470 MB\r\niteration 47: rss 469 MB\r\niteration 48: rss 470 MB\r\niteration 49: rss 469 MB\r\niteration 50: rss 470 MB\r\niteration 51: rss 470 MB\r\niteration 52: rss 470 MB\r\niteration 53: rss 470 MB\r\niteration 54: rss 470 MB\r\niteration 55: rss 470 MB\r\niteration 56: rss 469 MB\r\niteration 57: rss 471 MB\r\niteration 58: rss 471 MB\r\niteration 59: rss 472 MB\r\niteration 60: rss 472 MB\r\niteration 61: rss 469 MB\r\niteration 62: rss 471 MB\r\niteration 63: rss 472 MB\r\niteration 64: rss 473 MB\r\niteration 65: rss 472 MB\r\niteration 66: rss 472 MB\r\niteration 67: rss 472 MB\r\niteration 68: rss 471 MB\r\niteration 69: rss 471 MB\r\niteration 70: rss 456 MB\r\niteration 71: rss 471 MB\r\niteration 72: rss 470 MB\r\niteration 73: rss 472 MB\r\niteration 74: rss 469 MB\r\niteration 75: rss 472 MB\r\niteration 76: rss 473 MB\r\n```\r\n\r\nAlso the second example has not stabilized around iteration 30\r\n\r\n```\r\niteration 0: rss 553 MB (+566576 KB; +0 KB/it.)\r\niteration 1: rss 617 MB (+65944 KB; +65944 KB/it.)\r\niteration 2: rss 630 MB (+13036 KB; +39490 KB/it.)\r\niteration 3: rss 648 MB (+18408 KB; +32462 KB/it.)\r\niteration 4: rss 662 MB (+14712 KB; +28025 KB/it.)\r\niteration 5: rss 675 MB (+12788 KB; +24977 KB/it.)\r\niteration 6: rss 689 MB (+14444 KB; +23222 KB/it.)\r\niteration 7: rss 702 MB (+13408 KB; +21820 KB/it.)\r\niteration 8: rss 721 MB (+19108 KB; +21481 KB/it.)\r\niteration 9: rss 727 MB (+6692 KB; +19837 KB/it.)\r\niteration 10: rss 714 MB (-13352 KB; +16518 KB/it.)\r\niteration 11: rss 732 MB (+18696 KB; +16716 KB/it.)\r\niteration 12: rss 788 MB (+56772 KB; +20054 KB/it.)\r\niteration 13: rss 810 MB (+22948 KB; +20277 KB/it.)\r\niteration 14: rss 805 MB (-5196 KB; +18457 KB/it.)\r\niteration 15: rss 821 MB (+16216 KB; +18308 KB/it.)\r\niteration 16: rss 812 MB (-9680 KB; +16559 KB/it.)\r\niteration 17: rss 825 MB (+13888 KB; +16401 KB/it.)\r\niteration 18: rss 818 MB (-7188 KB; +15091 KB/it.)\r\niteration 19: rss 831 MB (+13516 KB; +15008 KB/it.)\r\niteration 20: rss 825 MB (-6792 KB; +13918 KB/it.)\r\niteration 21: rss 835 MB (+10788 KB; +13769 KB/it.)\r\niteration 22: rss 829 MB (-6468 KB; +12849 KB/it.)\r\niteration 23: rss 841 MB (+12472 KB; +12833 KB/it.)\r\niteration 24: rss 831 MB (-9784 KB; +11890 KB/it.)\r\niteration 25: rss 844 MB (+12768 KB; +11925 KB/it.)\r\niteration 26: rss 837 MB (-7016 KB; +11197 KB/it.)\r\n```\r\n\r\n**Edit**: The above are from a Windows, CPU-only system. Upon further testing I agree that the situation has improved on my Linux, GPU system, similarly to what you show in Colab (but I just noticed that tf-nightly requires a new version of CUDA which I don't have, so tf-nightly runs on the CPU, while 2.3.1 runs on the GPU). Oh my, that is confusing. I also checked with `tf.config.set_visible_devices([], 'GPU')` - that is still showing the issue.\r\n\r\nSo I see the issue \r\n- on Windows with a CPU and TF 2.3.1\r\n- on Windows with a CPU and TF nightly\r\n- on Linux with a CPU and TF 2.3.1\r\n- on Linux with a GPU and TF 2.3.1\r\n\r\nBut not on\r\n- on Linux with a CPU and TF nightly\r\n\r\n", "I don't see too much unstable memory consumption on Colab\r\n\r\n```\r\n!pip install --upgrade memory_profiler\r\n%load_ext memory_profiler\r\n```\r\n```\r\nimport gc\r\nimport tensorflow as tf\r\n\r\ndef memory_fit_test():\r\n    # do some work\r\n    model = tf.keras.applications.mobilenet.MobileNet()\r\n    model.compile(loss=\"mse\", run_eagerly=True)\r\n    x = tf.zeros((1, *model.input.shape[1:]))\r\n    y = tf.zeros((1, *model.output.shape[1:]))\r\n    history = model.fit(x=x, y=y, verbose=0)\r\n\r\nfor i in range(100):\r\n    print(\"Model fit:\")\r\n    %memit memory_fit_test()\r\n    tf.keras.backend.clear_session()\r\n    gc.collect()\r\n```", "Same example plotted with `mprun`\r\n\r\n![test](https://user-images.githubusercontent.com/1710528/94919824-60873f00-04b5-11eb-881d-47ab1fcbd143.png)\r\n", "@bhack I have, since reporting this issue, found an unrelated bug in my (GPU-based, Linux) environment that may have resulted in (and has since resolved) the OOM crashes that I have experienced. So first of all, I do not have a pressing issue that needs resolving. Also, `tf-nightly` is clearly better than `2.3.1` in at least one case, so let's hope `2.4` includes the relevant fixes and performance stays where it is in this case.\r\n\r\nHowever, there may still be an issue in general. I also ran `mprof` now for the code in https://github.com/tensorflow/tensorflow/issues/43702#issuecomment-702069826, and this is the result (Windows, CPU, `2.3.1`):\r\n\r\n![image](https://user-images.githubusercontent.com/12128514/95061138-800fa900-06fb-11eb-97b6-75e115274b32.png)\r\n\r\n<details>\r\n\r\n```\r\niteration 0: rss 555 MB (+568336 KB; +0 KB/it.)\r\niteration 1: rss 655 MB (+102644 KB; +102644 KB/it.)\r\niteration 2: rss 668 MB (+13936 KB; +58290 KB/it.)\r\niteration 3: rss 698 MB (+30728 KB; +49102 KB/it.)\r\niteration 4: rss 714 MB (+15920 KB; +40807 KB/it.)\r\niteration 5: rss 730 MB (+16896 KB; +36024 KB/it.)\r\niteration 6: rss 720 MB (-10324 KB; +28300 KB/it.)\r\niteration 7: rss 763 MB (+44004 KB; +30543 KB/it.)\r\niteration 8: rss 752 MB (-11972 KB; +25229 KB/it.)\r\niteration 9: rss 771 MB (+19708 KB; +24615 KB/it.)\r\niteration 10: rss 789 MB (+18512 KB; +24005 KB/it.)\r\niteration 11: rss 801 MB (+12616 KB; +22969 KB/it.)\r\niteration 12: rss 837 MB (+36504 KB; +24097 KB/it.)\r\niteration 13: rss 805 MB (-32320 KB; +19757 KB/it.)\r\niteration 14: rss 802 MB (-3504 KB; +18096 KB/it.)\r\niteration 15: rss 858 MB (+57864 KB; +20747 KB/it.)\r\niteration 16: rss 854 MB (-4848 KB; +19147 KB/it.)\r\niteration 17: rss 821 MB (-33520 KB; +16049 KB/it.)\r\niteration 18: rss 853 MB (+32664 KB; +16972 KB/it.)\r\niteration 19: rss 873 MB (+20176 KB; +17141 KB/it.)\r\niteration 20: rss 867 MB (-5840 KB; +15992 KB/it.)\r\niteration 21: rss 885 MB (+18284 KB; +16101 KB/it.)\r\niteration 22: rss 879 MB (-5608 KB; +15114 KB/it.)\r\niteration 23: rss 837 MB (-43208 KB; +12578 KB/it.)\r\niteration 24: rss 867 MB (+30256 KB; +13315 KB/it.)\r\niteration 25: rss 887 MB (+20796 KB; +13614 KB/it.)\r\niteration 26: rss 884 MB (-3176 KB; +12968 KB/it.)\r\niteration 27: rss 900 MB (+16940 KB; +13115 KB/it.)\r\niteration 28: rss 892 MB (-8400 KB; +12347 KB/it.)\r\niteration 29: rss 909 MB (+16752 KB; +12499 KB/it.)\r\niteration 30: rss 900 MB (-8200 KB; +11809 KB/it.)\r\niteration 31: rss 914 MB (+13468 KB; +11862 KB/it.)\r\niteration 32: rss 907 MB (-6352 KB; +11293 KB/it.)\r\niteration 33: rss 923 MB (+16040 KB; +11437 KB/it.)\r\niteration 34: rss 913 MB (-10332 KB; +10797 KB/it.)\r\niteration 35: rss 928 MB (+15464 KB; +10930 KB/it.)\r\niteration 36: rss 921 MB (-6960 KB; +10433 KB/it.)\r\niteration 37: rss 933 MB (+11584 KB; +10464 KB/it.)\r\niteration 38: rss 929 MB (-3824 KB; +10088 KB/it.)\r\niteration 39: rss 938 MB (+9608 KB; +10076 KB/it.)\r\niteration 40: rss 931 MB (-6980 KB; +9649 KB/it.)\r\niteration 41: rss 941 MB (+10136 KB; +9661 KB/it.)\r\niteration 42: rss 937 MB (-4548 KB; +9323 KB/it.)\r\niteration 43: rss 948 MB (+11060 KB; +9363 KB/it.)\r\niteration 44: rss 937 MB (-11484 KB; +8890 KB/it.)\r\niteration 45: rss 951 MB (+14796 KB; +9021 KB/it.)\r\niteration 46: rss 943 MB (-8172 KB; +8647 KB/it.)\r\niteration 47: rss 954 MB (+11132 KB; +8700 KB/it.)\r\niteration 48: rss 947 MB (-7348 KB; +8366 KB/it.)\r\niteration 49: rss 958 MB (+11264 KB; +8425 KB/it.)\r\niteration 50: rss 952 MB (-6196 KB; +8132 KB/it.)\r\niteration 51: rss 961 MB (+9632 KB; +8162 KB/it.)\r\niteration 52: rss 957 MB (-4104 KB; +7926 KB/it.)\r\niteration 53: rss 964 MB (+7192 KB; +7912 KB/it.)\r\niteration 54: rss 961 MB (-2864 KB; +7712 KB/it.)\r\niteration 55: rss 970 MB (+8984 KB; +7735 KB/it.)\r\niteration 56: rss 966 MB (-4168 KB; +7523 KB/it.)\r\niteration 57: rss 972 MB (+5988 KB; +7496 KB/it.)\r\niteration 58: rss 970 MB (-1888 KB; +7334 KB/it.)\r\niteration 59: rss 974 MB (+4396 KB; +7284 KB/it.)\r\niteration 60: rss 973 MB (-1084 KB; +7145 KB/it.)\r\niteration 61: rss 979 MB (+6168 KB; +7129 KB/it.)\r\niteration 62: rss 975 MB (-3812 KB; +6952 KB/it.)\r\niteration 63: rss 983 MB (+7544 KB; +6962 KB/it.)\r\niteration 64: rss 980 MB (-3372 KB; +6800 KB/it.)\r\niteration 65: rss 988 MB (+8604 KB; +6828 KB/it.)\r\niteration 66: rss 984 MB (-3968 KB; +6664 KB/it.)\r\niteration 67: rss 992 MB (+8552 KB; +6693 KB/it.)\r\niteration 68: rss 988 MB (-4888 KB; +6522 KB/it.)\r\niteration 69: rss 996 MB (+8040 KB; +6544 KB/it.)\r\niteration 70: rss 990 MB (-5264 KB; +6376 KB/it.)\r\niteration 71: rss 999 MB (+8432 KB; +6405 KB/it.)\r\niteration 72: rss 997 MB (-2108 KB; +6286 KB/it.)\r\niteration 73: rss 1005 MB (+9104 KB; +6325 KB/it.)\r\niteration 74: rss 1002 MB (-3692 KB; +6190 KB/it.)\r\niteration 75: rss 1009 MB (+7152 KB; +6202 KB/it.)\r\niteration 76: rss 1006 MB (-2660 KB; +6086 KB/it.)\r\niteration 77: rss 1014 MB (+7556 KB; +6105 KB/it.)\r\niteration 78: rss 1008 MB (-5256 KB; +5959 KB/it.)\r\niteration 79: rss 1016 MB (+7316 KB; +5976 KB/it.)\r\niteration 80: rss 1012 MB (-3388 KB; +5859 KB/it.)\r\niteration 81: rss 1022 MB (+9920 KB; +5909 KB/it.)\r\niteration 82: rss 1017 MB (-4788 KB; +5779 KB/it.)\r\niteration 83: rss 1026 MB (+8492 KB; +5812 KB/it.)\r\niteration 84: rss 1022 MB (-3464 KB; +5701 KB/it.)\r\niteration 85: rss 1030 MB (+7852 KB; +5726 KB/it.)\r\niteration 86: rss 1027 MB (-3152 KB; +5623 KB/it.)\r\niteration 87: rss 1033 MB (+5996 KB; +5628 KB/it.)\r\niteration 88: rss 1031 MB (-1732 KB; +5544 KB/it.)\r\niteration 89: rss 1039 MB (+7896 KB; +5570 KB/it.)\r\niteration 90: rss 1036 MB (-3064 KB; +5474 KB/it.)\r\niteration 91: rss 997 MB (-39496 KB; +4980 KB/it.)\r\niteration 92: rss 995 MB (-1824 KB; +4906 KB/it.)\r\niteration 93: rss 1002 MB (+6668 KB; +4925 KB/it.)\r\niteration 94: rss 998 MB (-3544 KB; +4835 KB/it.)\r\niteration 95: rss 1007 MB (+8732 KB; +4876 KB/it.)\r\niteration 96: rss 1003 MB (-4284 KB; +4781 KB/it.)\r\niteration 97: rss 1011 MB (+8868 KB; +4823 KB/it.)\r\niteration 98: rss 1008 MB (-3732 KB; +4735 KB/it.)\r\niteration 99: rss 1013 MB (+5104 KB; +4739 KB/it.)\r\n```\r\n</details>\r\n\r\nI will check with more iterations if the decrease around iteration 90/100 hints at some hard memory limit or whether memory is growing even further, but already what happens until iteration 90 tells me that memory is growing when it probably should not.", "Have you tried, like in my example, with `run_eagerly=True` in the compile phase?", "This (now using `2.4.0-dev20201002`, twice the number of iterations as before, but still non-eager) seems to confirm that there is no unlimited leaking, even though memory usage increases over a long period of 15 minute before becoming stable:\r\n\r\n![image](https://user-images.githubusercontent.com/12128514/95070531-88221580-0708-11eb-8438-cc7d5a4703e1.png)\r\n\r\n<details>\r\n\r\n```\r\n2.4.0-dev20201002\r\niteration 0: rss 576 MB (+589980 KB; +0 KB/it.)\r\niteration 1: rss 640 MB (+66248 KB; +66248 KB/it.)\r\niteration 2: rss 653 MB (+13272 KB; +39760 KB/it.)\r\niteration 3: rss 672 MB (+18668 KB; +32729 KB/it.)\r\niteration 4: rss 687 MB (+15744 KB; +28483 KB/it.)\r\niteration 5: rss 696 MB (+8804 KB; +24547 KB/it.)\r\niteration 6: rss 709 MB (+14108 KB; +22807 KB/it.)\r\niteration 7: rss 729 MB (+19676 KB; +22360 KB/it.)\r\niteration 8: rss 717 MB (-11960 KB; +18070 KB/it.)\r\niteration 9: rss 756 MB (+40036 KB; +20510 KB/it.)\r\niteration 10: rss 721 MB (-36272 KB; +14832 KB/it.)\r\niteration 11: rss 823 MB (+104888 KB; +23019 KB/it.)\r\niteration 12: rss 744 MB (-80428 KB; +14398 KB/it.)\r\niteration 13: rss 826 MB (+83092 KB; +19682 KB/it.)\r\niteration 14: rss 824 MB (-1768 KB; +18150 KB/it.)\r\niteration 15: rss 843 MB (+20136 KB; +18282 KB/it.)\r\niteration 16: rss 834 MB (-9228 KB; +16563 KB/it.)\r\niteration 17: rss 793 MB (-42044 KB; +13116 KB/it.)\r\niteration 18: rss 826 MB (+32968 KB; +14218 KB/it.)\r\niteration 19: rss 843 MB (+18204 KB; +14428 KB/it.)\r\niteration 20: rss 837 MB (-6944 KB; +13360 KB/it.)\r\niteration 21: rss 852 MB (+15700 KB; +13471 KB/it.)\r\niteration 22: rss 843 MB (-8920 KB; +12453 KB/it.)\r\niteration 23: rss 801 MB (-43616 KB; +10015 KB/it.)\r\niteration 24: rss 831 MB (+30976 KB; +10889 KB/it.)\r\niteration 25: rss 850 MB (+19196 KB; +11221 KB/it.)\r\niteration 26: rss 843 MB (-7000 KB; +10520 KB/it.)\r\niteration 27: rss 861 MB (+18324 KB; +10809 KB/it.)\r\niteration 28: rss 852 MB (-9300 KB; +10091 KB/it.)\r\niteration 29: rss 866 MB (+15068 KB; +10263 KB/it.)\r\niteration 30: rss 859 MB (-7688 KB; +9664 KB/it.)\r\niteration 31: rss 872 MB (+13084 KB; +9774 KB/it.)\r\niteration 32: rss 862 MB (-9812 KB; +9162 KB/it.)\r\niteration 33: rss 876 MB (+13840 KB; +9304 KB/it.)\r\niteration 34: rss 866 MB (-9348 KB; +8756 KB/it.)\r\niteration 35: rss 881 MB (+15032 KB; +8935 KB/it.)\r\niteration 36: rss 838 MB (-43928 KB; +7466 KB/it.)\r\niteration 37: rss 850 MB (+11628 KB; +7579 KB/it.)\r\niteration 38: rss 842 MB (-7940 KB; +7170 KB/it.)\r\niteration 39: rss 850 MB (+8424 KB; +7203 KB/it.)\r\niteration 40: rss 844 MB (-6588 KB; +6858 KB/it.)\r\niteration 41: rss 851 MB (+7832 KB; +6882 KB/it.)\r\niteration 42: rss 847 MB (-4784 KB; +6604 KB/it.)\r\niteration 43: rss 854 MB (+7516 KB; +6625 KB/it.)\r\niteration 44: rss 849 MB (-5292 KB; +6354 KB/it.)\r\niteration 45: rss 855 MB (+6528 KB; +6358 KB/it.)\r\niteration 46: rss 851 MB (-4416 KB; +6124 KB/it.)\r\niteration 47: rss 859 MB (+8172 KB; +6167 KB/it.)\r\niteration 48: rss 855 MB (-4216 KB; +5951 KB/it.)\r\niteration 49: rss 784 MB (-72688 KB; +4346 KB/it.)\r\niteration 50: rss 822 MB (+39308 KB; +5045 KB/it.)\r\niteration 51: rss 834 MB (+11772 KB; +5177 KB/it.)\r\niteration 52: rss 832 MB (-1468 KB; +5049 KB/it.)\r\niteration 53: rss 841 MB (+9408 KB; +5132 KB/it.)\r\niteration 54: rss 838 MB (-3788 KB; +4966 KB/it.)\r\niteration 55: rss 846 MB (+8300 KB; +5027 KB/it.)\r\niteration 56: rss 843 MB (-2616 KB; +4891 KB/it.)\r\niteration 57: rss 850 MB (+6932 KB; +4926 KB/it.)\r\niteration 58: rss 846 MB (-4244 KB; +4768 KB/it.)\r\niteration 59: rss 852 MB (+6292 KB; +4794 KB/it.)\r\niteration 60: rss 849 MB (-3120 KB; +4662 KB/it.)\r\niteration 61: rss 855 MB (+6020 KB; +4684 KB/it.)\r\niteration 62: rss 852 MB (-3212 KB; +4557 KB/it.)\r\niteration 63: rss 796 MB (-57268 KB; +3576 KB/it.)\r\niteration 64: rss 831 MB (+36168 KB; +4085 KB/it.)\r\niteration 65: rss 841 MB (+9976 KB; +4176 KB/it.)\r\niteration 66: rss 840 MB (-968 KB; +4098 KB/it.)\r\niteration 67: rss 845 MB (+5716 KB; +4122 KB/it.)\r\niteration 68: rss 845 MB (-636 KB; +4052 KB/it.)\r\niteration 69: rss 848 MB (+2896 KB; +4035 KB/it.)\r\niteration 70: rss 847 MB (-544 KB; +3970 KB/it.)\r\niteration 71: rss 851 MB (+3832 KB; +3968 KB/it.)\r\niteration 72: rss 850 MB (-1080 KB; +3898 KB/it.)\r\niteration 73: rss 854 MB (+4040 KB; +3900 KB/it.)\r\niteration 74: rss 852 MB (-1528 KB; +3826 KB/it.)\r\niteration 75: rss 856 MB (+3876 KB; +3827 KB/it.)\r\niteration 76: rss 854 MB (-1680 KB; +3754 KB/it.)\r\niteration 77: rss 857 MB (+2764 KB; +3741 KB/it.)\r\niteration 78: rss 857 MB (-304 KB; +3690 KB/it.)\r\niteration 79: rss 859 MB (+2584 KB; +3676 KB/it.)\r\niteration 80: rss 857 MB (-2572 KB; +3598 KB/it.)\r\niteration 81: rss 859 MB (+2604 KB; +3585 KB/it.)\r\niteration 82: rss 859 MB (-408 KB; +3537 KB/it.)\r\niteration 83: rss 861 MB (+2436 KB; +3523 KB/it.)\r\niteration 84: rss 861 MB (+104 KB; +3483 KB/it.)\r\niteration 85: rss 863 MB (+1328 KB; +3457 KB/it.)\r\niteration 86: rss 863 MB (-20 KB; +3417 KB/it.)\r\niteration 87: rss 864 MB (+1476 KB; +3394 KB/it.)\r\niteration 88: rss 864 MB (-184 KB; +3354 KB/it.)\r\niteration 89: rss 865 MB (+1332 KB; +3331 KB/it.)\r\niteration 90: rss 865 MB (-184 KB; +3292 KB/it.)\r\niteration 91: rss 867 MB (+1964 KB; +3277 KB/it.)\r\niteration 92: rss 867 MB (-416 KB; +3237 KB/it.)\r\niteration 93: rss 868 MB (+1920 KB; +3223 KB/it.)\r\niteration 94: rss 868 MB (-284 KB; +3186 KB/it.)\r\niteration 95: rss 870 MB (+1560 KB; +3169 KB/it.)\r\niteration 96: rss 869 MB (-896 KB; +3126 KB/it.)\r\niteration 97: rss 870 MB (+1492 KB; +3109 KB/it.)\r\niteration 98: rss 870 MB (-112 KB; +3077 KB/it.)\r\niteration 99: rss 871 MB (+1096 KB; +3057 KB/it.)\r\niteration 100: rss 871 MB (-108 KB; +3025 KB/it.)\r\niteration 101: rss 872 MB (+1220 KB; +3007 KB/it.)\r\niteration 102: rss 872 MB (-4 KB; +2978 KB/it.)\r\niteration 103: rss 803 MB (-71068 KB; +2259 KB/it.)\r\niteration 104: rss 846 MB (+44628 KB; +2666 KB/it.)\r\niteration 105: rss 851 MB (+4836 KB; +2687 KB/it.)\r\niteration 106: rss 853 MB (+1680 KB; +2677 KB/it.)\r\niteration 107: rss 856 MB (+2756 KB; +2678 KB/it.)\r\niteration 108: rss 856 MB (+624 KB; +2659 KB/it.)\r\niteration 109: rss 859 MB (+2728 KB; +2660 KB/it.)\r\niteration 110: rss 860 MB (+844 KB; +2643 KB/it.)\r\niteration 111: rss 862 MB (+2288 KB; +2640 KB/it.)\r\niteration 112: rss 862 MB (+32 KB; +2617 KB/it.)\r\niteration 113: rss 864 MB (+2612 KB; +2616 KB/it.)\r\niteration 114: rss 864 MB (-40 KB; +2593 KB/it.)\r\niteration 115: rss 866 MB (+2116 KB; +2589 KB/it.)\r\niteration 116: rss 867 MB (+44 KB; +2567 KB/it.)\r\niteration 117: rss 867 MB (+968 KB; +2553 KB/it.)\r\niteration 118: rss 868 MB (+912 KB; +2539 KB/it.)\r\niteration 119: rss 870 MB (+1592 KB; +2532 KB/it.)\r\niteration 120: rss 871 MB (+804 KB; +2517 KB/it.)\r\niteration 121: rss 872 MB (+1496 KB; +2509 KB/it.)\r\niteration 122: rss 871 MB (-720 KB; +2482 KB/it.)\r\niteration 123: rss 873 MB (+2104 KB; +2479 KB/it.)\r\niteration 124: rss 873 MB (-776 KB; +2453 KB/it.)\r\niteration 125: rss 875 MB (+2228 KB; +2451 KB/it.)\r\niteration 126: rss 874 MB (-1232 KB; +2422 KB/it.)\r\niteration 127: rss 876 MB (+2504 KB; +2422 KB/it.)\r\niteration 128: rss 875 MB (-1284 KB; +2394 KB/it.)\r\niteration 129: rss 878 MB (+2844 KB; +2397 KB/it.)\r\niteration 130: rss 877 MB (-1112 KB; +2370 KB/it.)\r\niteration 131: rss 879 MB (+2620 KB; +2372 KB/it.)\r\niteration 132: rss 878 MB (-896 KB; +2347 KB/it.)\r\niteration 133: rss 880 MB (+1708 KB; +2342 KB/it.)\r\niteration 134: rss 879 MB (-848 KB; +2319 KB/it.)\r\niteration 135: rss 880 MB (+1412 KB; +2312 KB/it.)\r\niteration 136: rss 880 MB (-300 KB; +2293 KB/it.)\r\niteration 137: rss 882 MB (+1960 KB; +2290 KB/it.)\r\niteration 138: rss 881 MB (-1408 KB; +2263 KB/it.)\r\niteration 139: rss 884 MB (+2948 KB; +2268 KB/it.)\r\niteration 140: rss 882 MB (-1580 KB; +2241 KB/it.)\r\niteration 141: rss 885 MB (+2588 KB; +2243 KB/it.)\r\niteration 142: rss 858 MB (-27564 KB; +2033 KB/it.)\r\niteration 143: rss 864 MB (+6172 KB; +2062 KB/it.)\r\niteration 144: rss 864 MB (+644 KB; +2052 KB/it.)\r\niteration 145: rss 867 MB (+3176 KB; +2060 KB/it.)\r\niteration 146: rss 868 MB (+616 KB; +2050 KB/it.)\r\niteration 147: rss 870 MB (+2244 KB; +2052 KB/it.)\r\niteration 148: rss 870 MB (-348 KB; +2035 KB/it.)\r\niteration 149: rss 873 MB (+2788 KB; +2040 KB/it.)\r\niteration 150: rss 872 MB (-780 KB; +2022 KB/it.)\r\niteration 151: rss 875 MB (+2772 KB; +2027 KB/it.)\r\niteration 152: rss 875 MB (-36 KB; +2013 KB/it.)\r\niteration 153: rss 877 MB (+2232 KB; +2014 KB/it.)\r\niteration 154: rss 877 MB (+192 KB; +2003 KB/it.)\r\niteration 155: rss 879 MB (+1888 KB; +2002 KB/it.)\r\niteration 156: rss 879 MB (-48 KB; +1989 KB/it.)\r\niteration 157: rss 823 MB (-56824 KB; +1614 KB/it.)\r\niteration 158: rss 864 MB (+42200 KB; +1871 KB/it.)\r\niteration 159: rss 869 MB (+4748 KB; +1889 KB/it.)\r\niteration 160: rss 870 MB (+948 KB; +1883 KB/it.)\r\niteration 161: rss 874 MB (+3932 KB; +1896 KB/it.)\r\niteration 162: rss 874 MB (+392 KB; +1887 KB/it.)\r\niteration 163: rss 876 MB (+2348 KB; +1889 KB/it.)\r\niteration 164: rss 876 MB (-200 KB; +1877 KB/it.)\r\niteration 165: rss 879 MB (+2496 KB; +1880 KB/it.)\r\niteration 166: rss 879 MB (-224 KB; +1868 KB/it.)\r\niteration 167: rss 880 MB (+1840 KB; +1868 KB/it.)\r\niteration 168: rss 881 MB (+676 KB; +1861 KB/it.)\r\niteration 169: rss 882 MB (+992 KB; +1855 KB/it.)\r\niteration 170: rss 883 MB (+644 KB; +1848 KB/it.)\r\niteration 171: rss 885 MB (+2044 KB; +1849 KB/it.)\r\niteration 172: rss 883 MB (-1608 KB; +1829 KB/it.)\r\niteration 173: rss 885 MB (+1876 KB; +1830 KB/it.)\r\niteration 174: rss 864 MB (-21160 KB; +1697 KB/it.)\r\niteration 175: rss 869 MB (+4488 KB; +1713 KB/it.)\r\niteration 176: rss 872 MB (+3212 KB; +1722 KB/it.)\r\niteration 177: rss 876 MB (+3948 KB; +1734 KB/it.)\r\niteration 178: rss 876 MB (+156 KB; +1726 KB/it.)\r\niteration 179: rss 879 MB (+3068 KB; +1733 KB/it.)\r\niteration 180: rss 878 MB (-716 KB; +1719 KB/it.)\r\niteration 181: rss 881 MB (+3380 KB; +1729 KB/it.)\r\niteration 182: rss 881 MB (-564 KB; +1716 KB/it.)\r\niteration 183: rss 881 MB (-128 KB; +1706 KB/it.)\r\niteration 184: rss 880 MB (-620 KB; +1693 KB/it.)\r\niteration 185: rss 883 MB (+3292 KB; +1702 KB/it.)\r\niteration 186: rss 830 MB (-54556 KB; +1399 KB/it.)\r\niteration 187: rss 871 MB (+41540 KB; +1614 KB/it.)\r\niteration 188: rss 873 MB (+2188 KB; +1617 KB/it.)\r\niteration 189: rss 876 MB (+3456 KB; +1627 KB/it.)\r\niteration 190: rss 878 MB (+1516 KB; +1626 KB/it.)\r\niteration 191: rss 879 MB (+1448 KB; +1625 KB/it.)\r\niteration 192: rss 880 MB (+732 KB; +1621 KB/it.)\r\niteration 193: rss 882 MB (+2028 KB; +1623 KB/it.)\r\niteration 194: rss 882 MB (+528 KB; +1617 KB/it.)\r\niteration 195: rss 884 MB (+1940 KB; +1619 KB/it.)\r\niteration 196: rss 885 MB (+1040 KB; +1616 KB/it.)\r\niteration 197: rss 885 MB (+352 KB; +1609 KB/it.)\r\niteration 198: rss 885 MB (-212 KB; +1600 KB/it.)\r\niteration 199: rss 886 MB (+1072 KB; +1598 KB/it.)\r\nmprof: Sampling memory every 0.1s\r\nrunning as a Python program...\r\n```\r\n\r\n</details>\r\n\r\nI have try `run_eagerly=True` for completeness, and I am puzzled. That version uses 2-4 times fewer memory and is roughly 10 times as fast.\r\n\r\n![image](https://user-images.githubusercontent.com/12128514/95072040-e223da80-070a-11eb-9380-175b8c750497.png)\r\n\r\nI also tested the run time of individual iterations to rule out differing numbers of iterations or other stupid mistakes:\r\n\r\n```\r\n>>> model.compile(loss=\"mse\"), timeit.timeit(lambda: model.fit(x=[0], y=[0], verbose=0), number=3)\r\n(None, 19.40625670000003)\r\n>>> model.compile(loss=\"mse\", run_eagerly=True), timeit.timeit(lambda: model.fit(x=[0], y=[0], verbose=0), number=3)\r\n(None, 1.3771592999999598)\r\n>>> model.compile(loss=\"mse\"), timeit.timeit(lambda: model.fit(x=[0], y=[0], verbose=0), number=3)\r\n(None, 16.871414200000004)\r\n>>> model.compile(loss=\"mse\", run_eagerly=True), timeit.timeit(lambda: model.fit(x=[0], y=[0], verbose=0), number=3)\r\n(None, 1.3355119000000286)\r\n>>> model.compile(loss=\"mse\"), timeit.timeit(lambda: model.fit(x=[0], y=[0], verbose=0), number=3)\r\n(None, 19.290036299999997)\r\n```\r\n\r\n(And this result reproduces on `2.3.1`/Linux/GPU.)", "I think that it is not the suggested behavior to compile the model multiple time in the loop:\r\nhttps://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/keras/engine/training.py#L516-L519", "> I think that it is not the suggested behavior to compile the model multiple time in the loop:\r\n\r\nWell, it's a new model every time, isn't it? (It certainly is in my context.)\r\n\r\nAnyway, running 1000 iterations with `run_eagerly=True` confirms again that there is no strict leakage, even if, again, it appears that way for the first 20 minutes. But memory is reclaimed, somewhat unpredictably, later:\r\n![image](https://user-images.githubusercontent.com/12128514/95076363-02a36300-0712-11eb-9171-80b2a2687334.png)\r\n", "Are you using gc after `clear_session`?: \r\n```\r\n tf.keras.backend.clear_session()\r\n gc.collect()\r\n```", "> Are you using gc after `clear_session`?:\r\n\r\nTried that too, same thing:\r\n\r\n```python\r\nimport gc\r\nimport os\r\n\r\nimport psutil\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n\r\ndef do_some_work():\r\n    in_layer = keras.layers.Input(shape=(1,))\r\n    hidden_layer = in_layer\r\n    for _ in range(200):\r\n        hidden_layer = keras.layers.Dense(1)(hidden_layer)\r\n    out_layer = hidden_layer\r\n    model = keras.Model(inputs=in_layer, outputs=out_layer)\r\n    model.compile(loss=\"mse\", run_eagerly=True)\r\n    history = model.fit(x=[0], y=[0], verbose=0)\r\n\r\n\r\nprint(tf.__version__)\r\ntf.get_logger().setLevel(\"ERROR\")  # Suppress \"tf.function retracing\" warnings\r\nprocess = psutil.Process(os.getpid())\r\nprev_mem = 0\r\nfirst_mem = 0\r\nfor i in range(1000):\r\n    do_some_work()\r\n\r\n    # clean up\r\n    keras.backend.clear_session()\r\n    _ = gc.collect()\r\n    \r\n    # show memory usage\r\n    mem = process.memory_info().rss\r\n    if i == 0:\r\n        first_mem = mem\r\n    print(\r\n        f\"iteration {i}: rss {mem >> 20} MB ({(mem - prev_mem) >> 10:+} KB; \"\r\n        + f\"{((mem - first_mem) // max(1, i)) >> 10:+} KB/it.)\"\r\n    )\r\n    prev_mem = mem\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/12128514/95100836-ce3f9f00-0731-11eb-9c77-38e482fdf1b4.png)\r\n\r\n", "I cannot reproduce this running your last code on a CPU machine with:\r\n```\r\ndocker run --rm -it  tensorflow/tensorflow:2.3.1-gpu-jupyter /bin/bash\r\n```\r\n![memory](https://user-images.githubusercontent.com/1710528/95102461-b5d08400-0733-11eb-8117-e356e3caf4fe.png)", "> I cannot reproduce this running your last code on a CPU machine with:\r\n> \r\n> ```\r\n> docker run --rm -it  tensorflow/tensorflow:2.3.1-gpu-jupyter /bin/bash\r\n> ```\r\n\r\nSorry to hear that. I'm happy to help by trying out more things, but as I said, my original issue has been solved, so I will not dig much deeper on my own. Just pointing out that I am not using docker, I am using vanilla Python on Windows 10 (and `pyenv` self-compiled Python on Linux).", "Ok as I cannot reproduce this if it is solved you can close this.", "As you wish :)"]}]