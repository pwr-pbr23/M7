[{"number": 26131, "title": "Modify Partitioning and Scheduling", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nHi,\r\nI'm trying to modify this combinatorial aspect in order to minimize the execution time of the global schedule. I'm looking for the files that contain the partitioning function that assigns tensors to devices and the scheduling function that assigns tensors to time slot in which the tensors are executed.\r\nDoes anyone have any ideas?\r\n\r\n**Will this change the current api? How?**\r\nNot sure\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to optimize this combinatorial problem.\r\n", "comments": []}, {"number": 26130, "title": "tf.contrib.rnn.LayerNormBasicLSTMCell does not work with keras RNN", "body": "**System information**\r\n\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nNormal RNN cells work with `keras.layers.RNN` which allows to use arbitrary rnn cell class (replacing `tf.nn.dynamic_rnn` in 2.0 ?):\r\n\r\n```python\r\nx = tf.zeros([1, 10, 7])   # batch_size, time_steps, channel\r\ncell = tf.nn.rnn_cell.BasicLSTMCell(7, dtype=tf.float32)\r\ntf.keras.layers.RNN(cell, return_sequences=True)(x)\r\n\r\n# Result: <tf.Tensor 'rnn_1/transpose_1:0' shape=(1, 10, 7) dtype=float32>\r\n```\r\n\r\nBut LayerNormBasicLSTMCell does not work:\r\n\r\n\r\n```python\r\nx = tf.zeros([1, 10, 7])   # batch_size, time_steps, channel\r\ncell = tf.contrib.rnn.LayerNormBasicLSTMCell(256)\r\ntf.keras.layers.RNN(cell, return_sequences=True)(x)\r\n```\r\n\r\nAn error happens:\r\n\r\n```python\r\n..../lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py in\r\n_get_single_variable(self, name, shape, dtype, initializer, regularizer,\r\n partition_info, reuse, trainable, collections, caching_device, \r\n validate_shape, use_resource, constraint, synchronization, aggregation)\r\n    846         tb = [x for x in tb if \"tensorflow/python\" not in x[0]][:3]\r\n    847         raise ValueError(\"%s Originally defined at:\\n\\n%s\" % (err_msg, \"\".join(\r\n--> 848             traceback.format_list(tb))))\r\n    849       found_var = self._vars[name]\r\n    850       if not shape.is_compatible_with(found_var.get_shape()):\r\n\r\nValueError: Variable kernel already exists, disallowed.\r\nDid you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\r\n\r\n\r\n  File \"..../lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\", line 1430, in _linear\r\n    weights = vs.get_variable(\"kernel\", [proj_size, out_size], dtype=dtype)\r\n  File \"..../lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\", line 1441, in call\r\n    concat = self._linear(args)\r\n  File \"<ipython-input-25-fd6ef19bdfdb>\", line 1, in <module>\r\n    tf.keras.layers.RNN(cell, return_sequences=True)(x)\r\n```\r\n", "comments": ["Thanks for reporting the issue. LayerNormBasicLSTMCell is still using the legacy way to create variable/weights via variable_scope, which is deprecated. We will update the cell and fix when we move them from contrib to tf/addons. ", "https://github.com/tensorflow/addons/pull/205", "https://github.com/tensorflow/addons/pull/210"]}, {"number": 26129, "title": "\"The name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.\"", "body": "**I have same issue.\r\n  I have doing real time face recognition using tensorflow. I'm follow this link \"https://github.com/btwardow/tf-face-recognition\" When I'm download clone repo it work in one system but try to run another one this show me error. Please help me out.**\r\n\r\n`Traceback (most recent call last):\r\nFile \"server.py\", line 49, in detect\r\nfaces = recognize(detection.get_faces(image, threshold))\r\nFile \"/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/recognition.py\", line 19, in recognize\r\nX[i, :] = embedding(img_to_np(img))\r\nFile \"/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/embedding.py\", line 43, in embedding\r\nimages_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3666, in get_tensor_by_name\r\nreturn self.as_graph_element(name, allow_tensor=True, allow_operation=False)\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3490, in as_graph_element\r\nreturn self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3532, in _as_graph_element_locked\r\n\"graph.\" % (repr(name), repr(op_name)))\r\n**KeyError: \"The name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.\"**\r\n[2019-02-26 11:28:06,532] ERROR in app: Exception on /detect [POST]\r\nTraceback (most recent call last):\r\nFile \"server.py\", line 49, in detect\r\nfaces = recognize(detection.get_faces(image, threshold))\r\nFile \"/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/recognition.py\", line 19, in recognize\r\nX[i, :] = embedding(img_to_np(img))\r\nFile \"/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/embedding.py\", line 43, in embedding\r\nimages_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3666, in get_tensor_by_name\r\nreturn self.as_graph_element(name, allow_tensor=True, allow_operation=False)\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3490, in as_graph_element\r\nreturn self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3532, in _as_graph_element_locked\r\n\"graph.\" % (repr(name), repr(op_name)))\r\nKeyError: \"The name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py\", line 2292, in wsgi_app\r\nresponse = self.full_dispatch_request()\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py\", line 1815, in full_dispatch_request\r\nrv = self.handle_user_exception(e)\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py\", line 1718, in handle_user_exception\r\nreraise(exc_type, exc_value, tb)\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/_compat.py\", line 35, in reraise\r\nraise value\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py\", line 1813, in full_dispatch_request\r\nrv = self.dispatch_request()\r\nFile \"/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py\", line 1799, in dispatch_request\r\nreturn self.view_functions[rule.endpoint](**req.view_args)\r\nFile \"server.py\", line 69, in detect\r\nprint('POST /detect error: %e' % e)\r\nTypeError: must be real number, not KeyError`", "comments": ["@sumit1212 I noticed this as a duplicate issue as you posted it on original \"tf-face-recognition\" Repo and that is correct place for this issue. This is not Build/Installation or Bug/Performance issue related to TF. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "@jvishnuvardhan - but this error occurs across multiple project in the wild..."]}, {"number": 26128, "title": "KeyError: \"The name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.\"", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@sumit1212 I noticed this as a duplicate issue as you posted it on original \"tf-face-recognition\" Repo and that is correct place for this issue. Closing this issue here. Thanks!"]}, {"number": 26127, "title": "Fix a bug for loss calculation", "body": "I think it is a bug for mini batch loss calculation. math_ops.reduce_sum(score_array) will make the 'batch' dim disappear. In the end, K.mean(score_map) doesn't work.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26127) for more info**.\n\n<!-- need_sender_cla -->", "> I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26127) for more info**.\n\n<!-- ok -->", "Thank you @Shanlans. Which version of TensorFlow are you using? This function has not been used in loss calculations for a while now, it was only used in a unit test and that has been removed as well.", "@Shanlans Did you get a chance to look on reviewer comments? Please let us know on the update. Also request you to have a look on conflicts. Thanks!"]}, {"number": 26126, "title": "typo issues in tensorflow/compiler", "body": "", "comments": []}, {"number": 26125, "title": "typo fixes in tensorflow/lite", "body": "", "comments": []}, {"number": 26124, "title": "typo fixes in tensorflow/core and contrib", "body": "", "comments": []}, {"number": 26123, "title": "For an SSD mobilenetv2 model, tflite quantized_uint8 inference is slower on some android devices than float inference", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, using the code in the tflite example apps.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.14.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S8 [Snapdragon 835] (API 26), HTC Bolt [Snapdragon 810] (API 24)\r\n- TensorFlow installed from (source or binary): Binary (using implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly' on android)\r\n- TensorFlow version (use command below): ('v1.12.0-rc0-17-g7b08198113', '1.12.0-rc1')\r\n- Python version: 2.7.15\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nOn the HTC Bolt, the performance of the SSD mobilenetv2 model is about the same for a quantized_uint8 model as well as a float model. However, on the Samsung Galaxy S8, the quantized_uint8 model is about twice as fast.\r\n\r\n**Describe the expected behavior**\r\nI would expect the quantized_uint8 model to be faster in every scenario (this is what I observed, for example on iOS).", "comments": ["This problem is caused by the specific differences in the CPU of the mobile device.Some Qualcomm Snapdragon processors support the use of DSP to speed up specific operations. If the NNAPI in the system is properly configured, there will be a high speed increase.Maybe the Qualcomm Snapdragon 835's DSP supports acceleration of the INT8, so it gets a higher speed.Specific information needs to visit the Qualcomm Developer website.\r\nThis page(http://ai-benchmark.com/ranking_processors.html) also gives some information that might be useful.", "I see, NNAPI is only supported on API 27+ right? So is there no way to get speedups on lower APIs (No one updates their android phones)?", "This is not just about the API version.NNAPI still has no effect if the device manufacturer does not implement hardware acceleration for NNAPI in the system.For situations where NNAPI is not available, if the device's SOC does support hardware acceleration, you can use the solution provided by the SOC vendor(E.g Qualcomm Neural Processing SDK for AI).But currently Qualcomm Snapdragon\r\nHisilicon Kirin, MediaTek Helio, and Samsung Exynos all have their own neural network SDKs.These SDKs need to be converted to the model when they are used, sometimes there will be problems.\r\n", "Ok, thanks for the response!"]}, {"number": 26122, "title": "Updated verifier_test.cc", "body": "Fixed warning in the file", "comments": []}, {"number": 26121, "title": "Updated model_cmdline_flags.cc", "body": "Fixed warning in the file", "comments": []}, {"number": 26120, "title": "Updated resolve_svdf.cc", "body": "Fixed warning in the file", "comments": []}, {"number": 26119, "title": "Updated export.cc", "body": "Fixed warning issue in the file", "comments": []}, {"number": 26118, "title": "Updated quantize_weights.cc", "body": "Fixed warnings in the file", "comments": ["@srjoglekar246 , thanks for your comments, i have updated the code as per the comments, kindly check and approve.", "@srjoglekar246 , i have updated the code as per your comments, kindly check and approve."]}, {"number": 26117, "title": "Updated resolve_constant_strided_slice.cc", "body": "Fixed the warning in the file", "comments": []}, {"number": 26116, "title": "Updated group_bidirectional_sequence_ops.cc", "body": "Fixed warning issues in the file", "comments": []}, {"number": 26115, "title": "Fix MemoryWriterIterator LOG(WARNING) spacing", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26115) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26115) for more info**.\n\n<!-- ok -->"]}, {"number": 26114, "title": "ImportError: DLL load failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 1.12.0\r\n- Python version: Python 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 9.2.148.1\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nwork well with tensorflow, but get error when change to tensorflow-gpu\r\n\r\nI ran ```pip install tensorflow``` and all things goes OK, but then I ran ```pip install tensorflow-gpu```, get errors below\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n", "comments": ["cuDNN version is v7.5.0.56, and all three things added to path", "TF 1.12 supports cuda 9.0 . Please lower your cuda version to 9.0.", "I am getting this error while compiling TF GPU  v1,13,1 with windows 10.  I am using CUDA 10 as it's default for 1.13.1. \r\n`\r\nERROR: C:/sw/temp/tensorflow/tensorflow/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command                                                                  cd C:/users/rishi/_bazel_rishi/t6earjat/execroot/org_tensorflow                                                                                                                                                                              SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0                                                                                                                                                                 SET CUDNN_INSTALL_PATH=c:/tools/cuda                                                                                                                                                                                                         SET PATH=C:\\tools\\msys64\\usr\\bin;C:\\tools\\msys64\\bin                                                                                                                                                                                         SET PYTHON_BIN_PATH=C:/Users/Rishi/AppData/Local/Programs/Python/Python36/python.exe                                                                                                                                                         SET PYTHON_LIB_PATH=C:/Users/Rishi/AppData/Local/Programs/Python/Python36/lib/site-packages                                                                                                                                                  SET TF_CUDA_CLANG=0                                                                                                                                                                                                                          SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0                                                                                                                                                                                                     SET TF_CUDA_VERSION=10.0                                                                                                                                                                                                                     SET TF_CUDNN_VERSION=7                                                                                                                                                                                                                       SET TF_NEED_CUDA=1                                                                                                                                                                                                                           SET TF_NEED_OPENCL_SYCL=0                                                                                                                                                                                                                    SET TF_NEED_ROCM=0                                                                                                                                                                                                                         C:/tools/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh                                                                                                                      Execution platform: @bazel_tools//platforms:host_platform                                                                                                                                                                                    Traceback (most recent call last):                                                                                                                                                                                                             File \"\\\\?\\C:\\Users\\Rishi\\AppData\\Local\\Temp\\Bazel.runfiles_p60i3529\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>                                                                                      from tensorflow.python.pywrap_tensorflow_internal import *                                                                                                                                                                                 File \"\\\\?\\C:\\Users\\Rishi\\AppData\\Local\\Temp\\Bazel.runfiles_p60i3529\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>                                                                             _pywrap_tensorflow_internal = swig_import_helper()                                                                                                                                                                                         File \"\\\\?\\C:\\Users\\Rishi\\AppData\\Local\\Temp\\Bazel.runfiles_p60i3529\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper                                                                   _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)                                                                                                                                                           File \"C:\\Users\\Rishi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module                                                                                                                                              return load_dynamic(name, filename, file)                                                                                                                                                                                                  File \"C:\\Users\\Rishi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic                                                                                                                                             return _load(spec)                                                                                                                                                                                                                       ImportError: DLL load failed: The specified module could not be found.                                                                                                                                                                                                                                                                                      \r\n`\r\n", "> TF 1.12 supports cuda 9.0 . Please lower your cuda version to 9.0.\r\n\r\nproblem solved, but when i run examples in https://www.tensorflow.org/tutorials/keras/basic_text_classification, its just nothing better than CPU only. I wonder if I get something wrong or it should be that?", "This issue is still there with v1.13.1, I am trying master branch to check if it compiles. ", "master barnch keeps giving following error:\r\n\r\nnvcc error   : 'cudafe++' died with status 0xC0000005 (ACCESS_VIOLATION)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 4770.792s, Critical Path: 472.50s\r\nINFO: 1547 processes: 1547 local.\r\nFAILED: Build did NOT complete successfully ", "@flybikeGx The training time for your model can be reduced drastically when computed on gpu compared to cpu computation. If you compare the training time elapsed for both you will find that gpu is much superior.\r\n@rishi-rranjan Can you please post a [new issue](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md) by providing the information asked by the template?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!", "I have win 10 and Nvidia Geforce 920M on my laptop. I am using Python 3.6.4 and Pycharm as IDE.\r\n\r\nI have also faced the same problem. After having tried several versions i could manage to solve the problem by installing the following versions:\r\n\r\nKeras 2.2.4\r\nTensorflow 1.12.0\r\nCuda 9.0\r\nCuDNN 7.4 ( Dnn for Cuda 9.0 )\r\n\r\nYou can find Cuda 9.0 and CuDNN in Nvidia Archive Site\r\n\r\nWorks Perfect !", "Tensorflow is Tensorflow-gpu by the way"]}, {"number": 26113, "title": "Remove old Docker directory", "body": "As of a couple weeks ago, the Dockerfile infrastructure in\ntools/dockerfiles has supplanted the old set of Dockerfiles in\ntools/docker. I don't know of any uses of the files remaining here.\n\nIf you think any of the files in this directory are important, please\nadd a comment. Otherwise, they'll be recoverable via Git's history.", "comments": ["@jayfurmanek @wdirons @claynerobison FYI, please make sure all your dockerfiles have moved to the new directories.", "My team supporting ppc64le has no concerns with removing the files listed in this PR. We publish docker images using the new process and we use the docker images in tensorflow/tools/ci_build for regression testing. ", "@gunan WIP", "This would be great. Many tutorials for example still assume pandas to be part of official tensorflow dockers but it seems to be gone during the move from `docker` to `dockerfiles` as the official source. Please correct me if I'm wrong.\r\n\r\n```docker run -it --rm tensorflow/tensorflow:latest-jupyter python -c \"import pandas\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named pandas```", "@claynerobison I'd like to get this in -- are you planning on making any changes?\r\n\r\n@fabboe That's correct; we removed Pandas from the default images for size reasons. Pandas should be available in all of the `-jupyter`-tagged images, however.", "@angersson can you please resolve conflicts", "@angersson We have PRs in progress to move the horovod and data science Dockerfiles to the partial infrastructure. Once  https://github.com/tensorflow/tensorflow/pull/27169 is merged, you can merge this. ", "> @fabboe That's correct; we removed Pandas from the default images for size reasons. Pandas should be available in all of the `-jupyter`-tagged images, however.\r\n\r\nThanks for the response @angersson. I can't find pandas in those images. Find below the steps to reproduce indication that pandas isn't part of most recent `-jupyter`  images (e.g. those from dockerhub).\r\n\r\n```\r\n$ docker pull tensorflow/tensorflow:latest-py3-jupyter\r\n[..]\r\n\r\n$ docker images -a\r\nREPOSITORY              TAG                   IMAGE ID            CREATED             SIZE\r\ntensorflow/tensorflow   latest-jupyter        81fed7ff566d        5 weeks ago         1.19GB\r\ntensorflow/tensorflow   latest-py3-jupyter    631103b2f156        6 weeks ago         1.26GB\r\n\r\n$ docker run -it --rm tensorflow/tensorflow:latest-py3-jupyter python -c \"import pandas\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named 'pandas'\r\n\r\n$ docker run -it --rm tensorflow/tensorflow:latest-jupyter python -c \"import pandas\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named pandas\r\n\r\n```\r\n\r\nI'm new to these dockerfiles scripts but indeed I **can't** find pandas package mentioned in [dockerfiles/dockerfiles/cpu-jupyter.Dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/cpu-jupyter.Dockerfile) nor [dockerfiles/partials/jupyter.partial.Dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/partials/jupyter.partial.Dockerfile)\r\n\r\nI **can** find pandas in [dockerfiles/devel-cpu-jupyter.Dockerfile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-cpu-jupyter.Dockerfile#L92) **but** either that image isn't published or I'm unable to guess the right tag name.\r\n\r\n```\r\n% docker pull tensorflow/tensorflow:devel-jupyter\r\nError response from daemon: manifest for tensorflow/tensorflow:devel-jupyter not found\r\n% docker pull tensorflow/tensorflow:devel-cpu-jupyter\r\nError response from daemon: manifest for tensorflow/tensorflow:devel-cpu-jupyter not found\r\n% docker pull tensorflow/tensorflow:devel-py3-jupyter\r\nError response from daemon: manifest for tensorflow\r\n/tensorflow:devel-py3-jupyter not found\r\n```\r\n\r\nWhat am I missing? Or is pandas just only part of the unpublished devel images?", "@fabboe Sorry about that, I forgot about how minimized the `jupyter` images are. `pandas` is in `tensorflow:devel`, as it's a build dependency, but isn't in any others.\r\n\r\nIf the tutorials are official TF tutorials, we should probably keep `pandas`. You're welcome to make a PR to add it.", "I've rebased on master, fixed a typo, and updated the version_updater script.\r\n\r\n@av8ramit @gunan @yifeif Can you take another look?"]}, {"number": 26112, "title": "Updated unidirectional_sequence_lstm.cc", "body": "", "comments": []}, {"number": 26111, "title": "Updated reshape_test.cc", "body": "", "comments": []}, {"number": 26110, "title": "Typo fix while.cc", "body": "", "comments": []}, {"number": 26109, "title": "Updated maximum_minimum.cc", "body": "", "comments": []}, {"number": 26108, "title": "2.0 Compile is leaking memory.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (2 different machines)\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): gpu 2.0.0-dev20190214, gpu 2.0.0-dev20190224\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: 7gb  K5200, 4gb GTX 970\r\n\r\n**Describe the current behavior**\r\nCompile leaks memory.\r\n**Describe the expected behavior**\r\nCompile clears the memory when overwritten\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\n\r\nfor i in range(100000):\r\n    model.compile(optimizer='adam',\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n```\r\n\r\nMemory slowly increases about 20mb at a time.\r\nI discovered this whilst working on a genetic algorithm which generates models which are then compiled and tested, but it fills my memory and I have narrowed it down to this simple code.\r\n", "comments": ["Changing the parameters of `compile` seems to makes no difference. And the size of the network also doesn't seem to make a difference.", "Me too... Hope this can get solved soon, for now I have to use an old version", "Same issue...  And K.clear_session() doesn't work.", "@Worthy7 Can you try this [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/6cf3a9ee853a805963e461df92e224bf/oom_tf26108.ipynb) and let me know if the issue still persists. I don't see any issue if I add `tf.keras.backend.clear_session()` to the end of your code.\r\n\r\n```\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\n\r\nfor i in range(100000):\r\n    model.compile(optimizer='adam',\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n    tf.keras.backend.clear_session()\r\n```", "That seems to work in that notebook on the nightly build. \r\nIn my original 2.0a the clear_session() doesn't fix it. \r\nSo I assume you fixed a bug somewhere in clear_session() which seems to work.\r\n\r\nI think that clear_session shouldn't be necessary really... don't you?\r\nI feed there should be no residue from a previous compile when running another compile on the same model.", "I'm not able to replicate your 20 mb figure; I see an increase of just under 0.5 mb / compile:\r\n\r\nhttps://colab.sandbox.google.com/gist/robieta/9820655313efc01f15bd864110e98a45/sequential_leak_test.ipynb\r\n\r\nGiven that compile does add ops to the graph (gradient ops, metric variables, etc.), I think this probably reasonable since compile is an infrequent call. (And yes, there was a bug with clear_session in the 2.0 alpha. It should be fixed in the beta.) See https://github.com/tensorflow/tensorflow/issues/28844 for an explanation of why there is some unnecessarily persisted state for the time being.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26108\">No</a>\n", "@robieta I said \"at a time\", I didn't mean \"per compile\" :) I was just watching the task manager window, probably per second.\r\nThanks for sorting it!", "> \r\n> \r\n> That seems to work in that notebook on the nightly build.\r\n> In my original 2.0a the clear_session() doesn't fix it.\r\n> So I assume you fixed a bug somewhere in clear_session() which seems to work.\r\n> \r\n> I think that clear_session shouldn't be necessary really... don't you?\r\n> I feed there should be no residue from a previous compile when running another compile on the same model.\r\n\r\nI agree with you @Worthy7 . A similar situation, if I train the model in the main thread and load the model in another thread **AT THE SAME TIME**. All things is in a loop. **However, if I use  clear_session() method in one thread, the code in another thread won't work!!!**  I test **pytorch** and **mxnet** , and **there is no any memory leak in a loop.** why???  amazing tensorflow!!!  I think that clear_session shouldn't be necessary."]}, {"number": 26107, "title": "Big memory consumption conv2d vs conv3d", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\n- **TensorFlow installed from**: \r\nBinary\r\n- **TensorFlow version**:\r\n1.12.0\r\n- **Python version**:\r\nPython 3.5.2\r\n- **CUDA/cuDNN version**:\r\nCUDA: 9.0, V9.0.176\r\ncuDNN: 7.4.2\r\n- **GPU model and memory**:\r\nNVIDIA Tesla V100 SXM2, 32GB\r\nDriver Version: 384.145\r\n\r\n### Describe the problem\r\nConv2D layer consumes a lot of memory comparing to the same operation performed by Conv3D.\r\nI have 2 independent graphs:\r\n1)\r\nInput (shape=[16, 224, 224, 4])\r\nconv2d (padding = SAME, filter=[3, 3, 4, 16])\r\nbias_add (shape=[16])\r\n2)\r\nInput (shape=[1,16, 224, 224, 4])\r\nconv3d (padding = SAME, filter=[1, 3, 3, 4, 16])\r\nbias_add (shape=[16])\r\n\r\nTF profiler reports that conv2d operation in first graph consumes 2900MB whereas conv3d that I presume should perform the same operation (cause leading dimensions of filter is 1) consumes 269.2MB which is an order of magnitude less.\r\n\r\nAlso for the graph with conv2d I see 2 additional layers are injected:\r\nConv2D-0-TransposeNHWCToNCHW-LayoutOptimizer consumes 16.78MB\r\nConv2D-0-0-TransposeNCHWToNHWC-LayoutOptimizer consumes 67.11MB\r\nthese layers are disappeared if I remove bias_add operation but memory consumption still stays the same.\r\n\r\nAm i missing something obvious or my expectations regarding conv2d vs conv3d doing the same in my case are wrong?\r\n\r\n### Source code / logs\r\n```\r\nimport os, time\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"1\")\r\n\r\nimport tensorflow as tf \r\nimport numpy as np\r\n\r\ndef test_conv2d():\r\n    input_ = tf.placeholder(tf.float32, shape=[16, 224, 224, 4], name=\"input_2d\")\r\n    filter_ = tf.get_variable(dtype=tf.float32, shape=[3, 3, 4, 16], name=\"filter_2d\")\r\n    conv = tf.nn.conv2d(input_, filter_, strides=(1,1,1,1), padding='SAME', dilations=(1,1,1,1))\r\n\r\n    vBias1 = tf.get_variable(name='bias_2d', shape=[16], dtype=tf.float32)\r\n    lBias1 = tf.nn.bias_add(conv, vBias1)\r\n    return lBias1\r\n\r\ndef test_conv3d():\r\n    input_ = tf.placeholder(tf.float32, shape=[1, 16, 224, 224, 4], name=\"input_3d\")\r\n    filter_ = tf.get_variable(dtype=tf.float32, shape=[1, 3, 3, 4, 16], name=\"filter_3d\")\r\n    conv = tf.nn.conv3d(input_, filter_, strides=(1,1,1,1,1), padding='SAME', dilations=(1,1,1,1,1))\r\n\r\n    vBias1 = tf.get_variable(name='bias_3d', shape=[16], dtype=tf.float32)\r\n    lBias1 = tf.nn.bias_add(conv, vBias1)\r\n    return lBias1\r\n\r\ngpu_options = tf.GPUOptions(allow_growth=True, visible_device_list=str(0))\r\nconfig = tf.ConfigProto(gpu_options=gpu_options)\r\n\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.RunMetadata()\r\n\r\ninputs = [\r\n    {\r\n        \"l\":test_conv2d(), \r\n        \"i\":{\"input_2d:0\": np.random.random([16, 224, 224, 4])}\r\n    },\r\n    {\r\n        \"l\":test_conv3d(), \r\n        \"i\":{\"input_3d:0\": np.random.random([1, 16, 224, 224, 4])}\r\n    }\r\n]\r\noutputs = []\r\nfor input in inputs:\r\n    with tf.Session(config=config) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(\r\n            input[\"l\"], \r\n            input[\"i\"],\r\n            options = run_options,\r\n            run_metadata = run_metadata)\r\n        \r\n        tf.profiler.profile(tf.get_default_graph(),\r\n                                run_meta=run_metadata,\r\n                                cmd='op',\r\n                                options=tf.profiler.ProfileOptionBuilder.time_and_memory())\r\n```\r\n### Output\r\n1)\r\nProfile:\r\nnode name | requested bytes | total execution time | accelerator execution time | cpu execution time\r\nConv2D                      2900.02MB (100.00%, 97.19%),      2.70sec (100.00%, 99.83%),      14.11ms (100.00%, 97.40%),      2.68sec (100.00%, 99.84%)\r\nConv2D-0-TransposeNHWCToNCHW-LayoutOptimizer        16.78MB (2.81%, 0.56%),          4.23ms (0.17%, 0.16%),            48us (2.60%, 0.33%),          4.18ms (0.16%, 0.16%)\r\nBiasAdd                               0B (0.00%, 0.00%),           207us (0.02%, 0.01%),           168us (2.27%, 1.16%),            39us (0.00%, 0.00%)\r\nConv2D-0-0-TransposeNCHWToNHWC-LayoutOptimizer        67.11MB (2.25%, 2.25%),           202us (0.01%, 0.01%),           160us (1.10%, 1.10%),            42us (0.00%, 0.00%)\r\nVariableV2                        2.56KB (0.00%, 0.00%),            20us (0.00%, 0.00%),             0us (0.00%, 0.00%),            20us (0.00%, 0.00%)\r\n\r\n2)\r\nProfile:\r\nnode name | requested bytes | total execution time | accelerator execution time | cpu execution time\r\nConv3D                      269.20MB (100.00%, 100.00%),     137.06ms (100.00%, 99.85%),      64.03ms (100.00%, 99.74%),      73.03ms (100.00%, 99.94%)\r\nBiasAdd                               0B (0.00%, 0.00%),           197us (0.15%, 0.14%),           168us (0.26%, 0.26%),            29us (0.06%, 0.04%)\r\nVariableV2                        2.56KB (0.00%, 0.00%),            12us (0.01%, 0.01%),             0us (0.00%, 0.00%),            12us (0.02%, 0.02%)\r\n", "comments": ["Update.\r\nOn Ubuntu 18.04, CUDA: 10.0, V10.0.130, cuDNN: 7.5, Driver Version: 410.79, Python 3.6.7, tensorflow-gpu 1.13.1 I got the same result.\r\n\r\nHowever on Windows 10 it's not reproduced on CPU: \r\n\u2022\tCUDA: 10.0, V10.0.130, cuDNN: 7.3.1, GeForce RTX 2080 Ti, Driver Version: 417.01, Python 3.7.2, tensorflow 1.13.1 \r\n\r\nCONV2D graph\r\nProfile:\r\nnode name | requested bytes | total execution time | accelerator execution time | cpu execution time\r\n*BiasAdd                      51.38MB (100.00%, 100.00%),      56.93ms (100.00%, 99.98%),             0us (0.00%, 0.00%),      56.93ms (100.00%, 99.98%)\r\nVariableV2                        2.37KB (0.00%, 0.00%),            13us (0.02%, 0.02%),             0us (0.00%, 0.00%),            13us (0.02%, 0.02%)\r\n\r\n*Here BiasAdd is fused with Conv2D. Also no **Transpose** layers.\r\n\r\nCONV3D graph\r\nProfile:\r\nnode name | requested bytes | total execution time | accelerator execution time | cpu execution time\r\nConv3D                       51.38MB (100.00%, 100.00%),      62.21ms (100.00%, 96.43%),             0us (0.00%, 0.00%),      62.21ms (100.00%, 96.43%)\r\nBiasAdd                               0B (0.00%, 0.00%),          2.29ms (3.57%, 3.55%),             0us (0.00%, 0.00%),          2.29ms (3.57%, 3.55%)\r\nVariableV2                        2.37KB (0.00%, 0.00%),            11us (0.02%, 0.02%),             0us (0.00%, 0.00%),            11us (0.02%, 0.02%)\r\n", "This does not reproduce on 1.14.\r\n\r\nProfile with reproducer run as-is: https://gist.github.com/sanjoy/8eee43352c6498b8e62f70f425cb097a\r\nProfile without the bias-add node: https://gist.github.com/sanjoy/f4fed522dd9bebf5d7b55e048e740f18\r\n\r\nLooks like we always use 51.38MB now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26107\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26107\">No</a>\n", "I'm sorry, my previous [comment](https://github.com/tensorflow/tensorflow/issues/26107#issuecomment-570003319) is incorrect.  I'm able to reproduce this in 1.14 (the logs I pasted were from a CPU run).", "This seems to have been fixed in TF 2.0.\r\n\r\nWith https://gist.github.com/sanjoy/b4fe8791a2f71625758ffbce5ad89e83 I see:\r\n\r\n```\r\nTF 2.0, P100\r\nConv3D                      228.18MB (100.00%, 100.00%),    941.59ms (100.00%, 100.00%),             0us (0.00%, 0.00%),    941.59ms (100.00%, 100.00%)\r\nConv2D                      201.93MB (100.00%, 100.00%),     20.96ms (100.00%, 100.00%),             0us (0.00%, 0.00%),     20.96ms (100.00%, 100.00%)\r\n\r\nTF 1.14, P100\r\nConv3D                      228.18MB (100.00%, 100.00%),    927.60ms (100.00%, 100.00%),             0us (0.00%, 0.00%),    927.60ms (100.00%, 100.00%)\r\nConv2D                     1550.98MB (100.00%, 100.00%),      15.96ms (100.00%, 99.89%),             0us (0.00%, 0.00%),      15.96ms (100.00%, 99.89%)\r\n\r\nTF 2.0, Titan-V\r\nConv3D                      235.65MB (100.00%, 100.00%),     1.43sec (100.00%, 100.00%),             0us (0.00%, 0.00%),     1.43sec (100.00%, 100.00%)\r\nConv2D                      201.93MB (100.00%, 100.00%),     33.63ms (100.00%, 100.00%),             0us (0.00%, 0.00%),     33.63ms (100.00%, 100.00%)\r\n\r\nTF 1.14, Titan-V\r\nConv3D                      235.65MB (100.00%, 100.00%),     1.46sec (100.00%, 100.00%),             0us (0.00%, 0.00%),     1.46sec (100.00%, 100.00%)\r\nConv2D                     2900.02MB (100.00%, 100.00%),      20.61ms (100.00%, 99.85%),             0us (0.00%, 0.00%),      20.61ms (100.00%, 99.85%)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26107\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26107\">No</a>\n"]}, {"number": 26106, "title": "Fix reference in HParams docstring.", "body": "In the tf.contrib.training.HParams.HParams class docstring, `tf.HParams` should be `tf.contrib.training.HParams`. (I don't think there currently exists a `tf.HParams` alias.)", "comments": []}, {"number": 26105, "title": "vocab_size = len(tokenizer.word_index) ?", "body": "Update it to \"vocab_size = top_k\"? ", "comments": []}, {"number": 26104, "title": "[TF2.0] Variable with dynamic shape", "body": "Hello everyone,\r\n\r\nWhy variable's `assign` doesn't work for different shapes? It doesn't look like correct behaviour.\r\n\r\n```python\r\nIn [5]: v = tf.Variable([1.0])  # tf.Variable([1.0], validate_shape=False) doesn't work as well\r\n\r\nIn [6]: v.assign(tf.random.normal((1, 1)))\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-27ac642ab52e> in <module>\r\n----> 1 v.assign(tf.random.normal((1, 1)))\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in assign(self, value, use_locking, name, read_value)\r\n   1052     with _handle_graph(self.handle):\r\n   1053       value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)\r\n-> 1054       self._shape.assert_is_compatible_with(value_tensor.shape)\r\n   1055       assign_op = gen_resource_variable_ops.assign_variable_op(\r\n   1056           self.handle, value_tensor, name=name)\r\n\r\n~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)\r\n   1070     \"\"\"\r\n   1071     if not self.is_compatible_with(other):\r\n-> 1072       raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\n   1073\r\n   1074   def most_specific_compatible_shape(self, other):\r\n\r\nValueError: Shapes (1,) and (1, 1) are incompatible\r\n```\r\n\r\n\r\n**System information**\r\n- macOS Mojave\r\n- TensorFlow installed from `pip install -U tf-nightly-2.0-preview` - \"2.0.0.dev20190225\"\r\n- Python version: 3.6\r\n\r\nKind regards,\r\nArtem Artemev", "comments": ["`tf.random.normal` expects you to have the 'shape' argument to be a 1-D integer Tensor or a Python array. [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random/normal](url). Hence this error was raised. I tested and it seems it is working properly.\r\n\r\n``` python\r\nPython 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>import tensorflow as tf\r\n>>> tf.__version__\r\n'2.0.0-dev20190225'\r\n>>> v = tf.Variable([2], validate_shape=False, dtype=\"float32\")\r\n2019-02-25 15:55:13.913333: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-25 15:55:13.928275: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz\r\n2019-02-25 15:55:13.929439: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x558008caef90 executing computations on platform Host. Devices:\r\n2019-02-25 15:55:13.929489: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n>>> v.assign(tf.random.normal(v.shape))\r\n<tf.Variable 'UnreadVariable' shape=(1,) dtype=float32, numpy=array([1.3112373], dtype=float32)>\r\n```\r\n\r\n**System Information**\r\n- Linux Opensuse Leap 15.0\r\n- TensorFlow installed from `pip install tf-nightly-2.0-preview` - \"2.0.0.dev20190225\"\r\n- Python Version: 3.6.8", "@Dexter2389, I wanted to show that variable's dynamic shape behaviour doesn't work at the moment. I put a different shape in `tf.random.normal` deliberately, there is no issue there.", "@alextp is the authority on variables in the TF 2.0 API, and will know if this is expected to work.", "@wangpengmit I think you're already looking at it?", "Yes, I'm the assignee of a similar issue internally.", "@wangpengmit, @alextp Hello guys! Is this feature in progress? I have a project which would benefit a lot from this.", "Artem, we haven't started work on it yet.\n\nDo you want to send a pull request? It amounts to adding a new constructor\nargument for the desired shape for ResourceVariable and Variable, and a\ntest.\n\nOn Thu, Apr 4, 2019 at 3:07 AM Artem Artemev <notifications@github.com>\nwrote:\n\n> @wangpengmit <https://github.com/wangpengmit>, @alextp\n> <https://github.com/alextp> Hello guys! Is this feature in progress? I\n> have a project which would benefit a lot from this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/26104#issuecomment-479834678>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxeOturRpxcTH4ybV1oKCfl_tjoQhks5vdc72gaJpZM4bQwmZ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp , yep, I will take a look.", "@awav, a fix is in flight, FYI.", "Fixed by https://github.com/tensorflow/tensorflow/commit/f9f2547b085969c67dc140aaa230551b6471a956", "@wangpengmit that's awesome! thanks a lot, it is almost what I have done in my tensorflow fork :)", "As I had some difficulties to understand out the above or from the documentation how this fix can be used, here is a working example:\r\n```\r\nv = tf.Variable([1], shape=tf.TensorShape(None), dtype=tf.int32) \r\ntf.print(v)\r\nv.assign([1, 1, 1])\r\ntf.print(v)\r\n```\r\nAnd this outputs:\r\n```\r\n[1]\r\n[1 1 1]\r\n```"]}, {"number": 26103, "title": "1.13.0 cherry-pick request: Update tensorboard dependency to 1.13.x", "body": "Cherrypick of b6ed9186089de852c933244a7d772f836cc3eb27 to bump dependency on TensorBoard to 1.13.x.\r\n\r\nTensorBoard release: https://pypi.org/project/tensorboard/1.13.0/\r\n\r\nPiperOrigin-RevId: 235563447", "comments": []}, {"number": 26102, "title": "ResizeNearestNeighbor op not supported in TFlite conversion", "body": "<em>\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.13.0-dev20190219\r\n- Python version: 3\r\n\r\n**Describe the current behavior**\r\nToCo converter is throwing an error when trying to convert ResizeNearestNeighbour op.\r\n\r\n**Describe the expected behavior**\r\nShould convert fine\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/FCOS/MastersWork/tree/master/error\r\nDownload files and edit model location in run.py\r\n\r\n**Other info / logs**\r\nConverterError: TOCO failed. See console for info.\r\n2019-02-25 19:02:10.960423: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: ResizeNearestNeighbor\r\n2019-02-25 19:02:10.960608: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: ResizeNearestNeighbor\r\n2019-02-25 19:02:10.961120: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 88 operators, 126 arrays (0 quantized)\r\n2019-02-25 19:02:10.961641: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 88 operators, 126 arrays (0 quantized)\r\n2019-02-25 19:02:10.962586: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 20 operators, 53 arrays (0 quantized)\r\n2019-02-25 19:02:10.962719: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 20 operators, 53 arrays (0 quantized)\r\n2019-02-25 19:02:10.962839: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 24305664 bytes, theoretical optimal value: 12247040 bytes.\r\n2019-02-25 19:02:10.962964: F tensorflow/contrib/lite/toco/tflite/export.cc:374] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.TocoConverter(). Here is a list of operators for which  you will need custom implementations: ResizeNearestNeighbor.\r\nAborted (core dumped)\r\n", "comments": ["I have added the request to our tracking issue [#21526](https://github.com/tensorflow/tensorflow/issues/21526). Closing this one...", "tensorflow version = 2.0.0\r\nStill error not resolved.\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-1-26f0299d545c> in <module>\r\n     20 tflite_converter.inference_output_type = tf.uint8\r\n     21 \r\n---> 22 tflite_model = tflite_converter.convert()\r\n     23 open(output_model_name, \"wb\").write(tflite_model)\r\n     24 \r\n\r\n~/Environments/liveness/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    448     if self._is_calibration_quantize():\r\n    449       result = self._calibrate_quantize_model(result, constants.FLOAT,\r\n--> 450                                               constants.FLOAT)\r\n    451 \r\n    452     return result\r\n\r\n~/Environments/liveness/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type)\r\n    237     return calibrate_quantize.calibrate_and_quantize(\r\n    238         self.representative_dataset.input_gen, inference_input_type,\r\n--> 239         inference_output_type, allow_float)\r\n    240 \r\n    241   def _get_base_converter_args(self):\r\n\r\n~/Environments/liveness/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float)\r\n     76     return self._calibrator.QuantizeModel(\r\n     77         np.dtype(input_type.as_numpy_dtype()).num,\r\n---> 78         np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n\r\n~/Environments/liveness/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\n    113 \r\n    114     def QuantizeModel(self, input_py_type, output_py_type, allow_float):\r\n--> 115         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\n    116 CalibrationWrapper_swigregister = _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_swigregister\r\n    117 CalibrationWrapper_swigregister(CalibrationWrapper)\r\n\r\nRuntimeError: Quantization not yet supported for op: RESIZE_NEAREST_NEIGHBOR"]}]