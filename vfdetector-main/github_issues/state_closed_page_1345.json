[{"number": 12734, "title": "cmake external projects: pass --libdir to configure", "body": "If `--libdir` is not set, configure will install into a subdirectory `lib64` on some platforms, while the build-system has `lib` hard-coded to find the static libraries later.\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "CLA is signed! (as SUSE LINUX GmbH)", "Jenkins test this please.", "@tensorflow-jenkins test this please", "@dmacvicar could you check the company CLA again?", "I can see the SUSE CLA signed in my list of corporate CLA's https://cla.developers.google.com/clas\r\nI am a member of the google group.\r\n\r\nI suspect it has to do with the corporate signer (@cfarrell1980) not being admin/owner of the google group.", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please.", "Jenkins, test this please.", "https://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/6922/ seems all green.", "Jenkins, test this please."]}, {"number": 12733, "title": "Incompatible Shapes during Validation for TensorFlow's seq2seq module", "body": "I'm using TensorFlow's seq2seq module. During validation, my decoder will occasionally produce output sequences with different lengths than the target sequences; this causes the following error when calculating the loss using `tf.nn.sigmoid_cross_entropy_with_logits` (batch major, not time major):\r\n\r\n`InvalidArgumentError (see above for traceback): Incompatible shapes: [128,22,4] vs. [128,26,4]`\r\n\r\nWhat is the best practice for dealing with this problem?\r\n\r\nI checked how the NMT tutorial solved the problem. As far as I can tell, they use a TrainingHelper during validation, which forces the decoder to unroll the same number of steps as the target sequence, but this seems like cheating - they're estimating how the model will perform during inference, but the decoder is receiving additional information (the target sequence length) that it won't have at inference time. I opened an [issue](https://github.com/tensorflow/nmt/issues/73) to clarify, but I haven't heard back.\r\n\r\nI also [posted](https://stackoverflow.com/questions/45944131/incompatible-shapes-during-validation-for-tensorflows-seq2seq-module) on StackOverflow, but from earlier experience, I doubt I'll receive a response.\r\n\r\nMy problem isn't specific to platform or TensorFlow version, but here's that information regardless:\r\n\r\nOS: macOS Sierra version 10.12.6\r\nTensorFlow installed from source\r\nTensorFlow version: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n", "comments": ["Thank you for reaching out. You were correct to post on Stack Overflow earlier. But as you know this issue tracker is for bugs and feature requests, rather than an escalation path for Stack Overflow. We like to focus on bugs and feature requests because those types of issues, once resolved, will be beneficial to everyone in the community.", "@jart , I can see why you think I'm escalating my problem, but there is an issue with TensorFlow's `Seq2Seq` module - during validation, no `Helper` subclass resolves the `Incompatible shapes` error.\r\n\r\nI don't know if the existing helpers should solve this problem or if something new is needed, so this issue can be thought of as either a feature request or a bug. One solution is that `GreedyEmbeddingHelper`, `SampleEmbeddingHelper`, etc. should optionally take in the length of the target sequence if known (i.e. during validation) and dynamically ensure that the decoder's output matches the length of the target sequence.\r\n\r\nThe simplest solution would be to use `ScheduledOutputTrainingHelper` with `sampling_probability = 1`, but this seems to overloads the purpose of the `ScheduledOutputTrainingHelper` class.\r\n\r\nMy point regarding the NMT tutorial is that I believe the tutorial could have the same error if it didn't use a `TrainingHelper` during validation.\r\n\r\nPlease reopen the issue.", "@ebrevdo @adarob Am I missing something incredibly obvious?", "For validation, I either use a TrainingHelper or ScheduledOutputTrainingHelper with sampling_probabilty = 1 as you suggested. Otherwise you'll need to come up with another way of computing the loss since you will have incompatible dimensions between the inferred outputs and ground truth, as you are seeing.", "I guess it doesn't really matter, but it feels wrong to use a TrainingHelper during validation.", "Same problem. I want to do validation when traning, but meet the ```Incompatible shapes``` err. I don't know what shape the ```GreedyEmbeddingHelper``` is.", "A stupid approach is set ```batch_size=1``` when validation."]}, {"number": 12732, "title": "Has anyone tried to visualize this program with tensorboard? ", "body": "I was trying to visualize the vectors with thensorboard, but it did not work, it was loading for over 30 minutes, what is wrong? ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12731, "title": "Reproducing results in tensorflow", "body": "Hi All,\r\n\r\nI set the graph-level seed and op-level seed to be 0, and run the same script for 10 times. However, for a different run, 5 out of 10 times, the results are exactly the same (weights in the model and the training loss). For the other 5 times, the results are completely different for each run apart from the initialization steps.\r\n\r\nAnyone has any idea of what is going on here?\r\n\r\nThanks!", "comments": ["In this link: https://stackoverflow.com/questions/45865665/getting-reproducible-results-using-tensorflow-gpu?newreg=4a6ec43834884576a175961e7f2188db\r\n\r\nThey mentioned that the tf.reduce_sum function has been fixed in a certain commit to solve the tensorfow-gpu non-determistic problem. \r\n\r\nHas anyone tried the nightly build version in GPU mode? can it resolve the non-deterministic problem?", "Stack Overflow says this was resolved by https://github.com/tensorflow/tensorflow/commit/d93a55b8. Try installing the [nightly whl file](https://github.com/tensorflow/tensorflow#installation) and let us know if it doesn't work. If that is the case, I'll reopen this issue.", "Thank you! Are you sure that version you mentioned has a whl file under\nnightly-matrix-linux-gpu\n<https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/>\n ?\n\nI have not found it.\n\nIs it OK to just install the latest whl file? Will the change of\ntf.reduce_sum function be reverted/missed somehow in this latest version?\n\nOn 4 September 2017 at 03:19, Justine Tunney <notifications@github.com>\nwrote:\n\n> Stack Overflow says this was resolved by d93a55b\n> <https://github.com/tensorflow/tensorflow/commit/d93a55b8>. Try\n> installing the nightly whl file\n> <https://github.com/tensorflow/tensorflow#installation> and let us know\n> if it doesn't work. If that is the case, I'll reopen this issue.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12731#issuecomment-326851244>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ASsNxtzkD7JdWYy0V-AqtIoJlr1-QO0oks5se14lgaJpZM4PI67y>\n> .\n>\n\n\n\n-- \nBest Regards,\n\nYours Sincerely\nRuomei\n"]}, {"number": 12730, "title": "Op type not registered 'CudnnRNNParamsSize' in binary", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: example\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 64bit\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: Cude - 8.0, cuDNN - 6.0\r\n- **GPU model and memory**: M1000M\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nThe tutorial RNN model fails due to: \"Op type not registered 'CudnnRNNParamsSize' in binary\"\r\n\r\n### Source code / logs\r\nhttps://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\r\n\r\n```\r\nb'unknown' 1.3.0\r\n2017-08-31 16:25:32.021526: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-31 16:25:32.021822: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-31 16:25:32.959216: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties: \r\nname: Quadro M1000M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0715\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.65GiB\r\n2017-08-31 16:25:32.959478: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0 \r\n2017-08-31 16:25:32.959642: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y \r\n2017-08-31 16:25:32.959848: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0)\r\nTraceback (most recent call last):\r\n  File \"C:/Work/Projects/tensorflow-models/tutorials/rnn/ptb/ptb_word_lm.py\", line 526, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"C:/Work/Projects/tensorflow-models/tutorials/rnn/ptb/ptb_word_lm.py\", line 470, in main\r\n    m = PTBModel(is_training=True, config=config, input_=train_input)\r\n  File \"C:/Work/Projects/tensorflow-models/tutorials/rnn/ptb/ptb_word_lm.py\", line 135, in __init__\r\n    output, state = self._build_rnn_graph(inputs, config, is_training)\r\n  File \"C:/Work/Projects/tensorflow-models/tutorials/rnn/ptb/ptb_word_lm.py\", line 174, in _build_rnn_graph\r\n    return self._build_rnn_graph_cudnn(inputs, config, is_training)\r\n  File \"C:/Work/Projects/tensorflow-models/tutorials/rnn/ptb/ptb_word_lm.py\", line 186, in _build_rnn_graph_cudnn\r\n    params_size_t = self._cell.params_size()\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\", line 597, in params_size\r\n    direction=self._direction)[0]\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\ops\\gen_cudnn_rnn_ops.py\", line 283, in cudnn_rnn_params_size\r\n    name=name)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2632, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1911, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 595, in call_cpp_shape_fn\r\n    require_shape_fn)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 654, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'CudnnRNNParamsSize' in binary running on ULTRALISK. Make sure the Op and Kernel are registered in the binary running in this process.\r\n```\r\n", "comments": ["I can confirm that the Op exists in the `build_pip_package.runfiles/org_tensorflow/tensorflow/contrib/cudnn_rnn/python/ops/_cudnn_rnn_ops.so` built from the head of master (yesterday), so perhaps this is an issue that has been fixed and will be in the 1.4 release.\r\n", "It looks as if that tutorial was not present in 1.3 so it is possible it won't work unless you use a more recent version. Can you try that?", "It seems I have the answer, this seems a bug when using cuDnn. Don't worry, this obstacle can be bypassed by using the following method. The parameter --model=... in your command line must be _small_. In another word, you must run this tutorial using the command like: \"python ptb_word_lm.py --data_path=... --model=_small_\". Through reading the code, I found that if I do not use cuDnn which means I type the following command: \"python ptb_word_lm.py --data_path=... --model=**_medium_**\" or \"python ptb_word_lm.py --data_path=... --model=**_large_**\", such errors will be eliminated. Hope this will help.", "I also encounter this issue.I also use the Win 10,but I installed Tensorflow-GPU from pip,and the version of tensorflow is also 1.3.0 . As @yangyixiaof  sayed , this issue is eliminated when I try to set \"the model\" as medium\".", "After install tf_nightly_gpu-1.5.0.dev, this problem is solved.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of activity, and because the most recent comment indicates the problem was solved.", "---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-5-0e97fbc26e6d> in <module>()\r\n      5     serialized_graph = fid.read()\r\n      6     od_graph_def.ParseFromString(serialized_graph)\r\n----> 7     tf.import_graph_def(od_graph_def, name='')\r\n\r\nC:\\Users\\310211146\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py in new_func(*args, **kwargs)\r\n    430                 'in a future version' if date is None else ('after %s' % date),\r\n    431                 instructions)\r\n--> 432       return func(*args, **kwargs)\r\n    433     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n    434                                        _add_deprecated_arg_notice_to_docstring(\r\n\r\nC:\\Users\\310211146\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    487         try:\r\n    488           results = c_api.TF_GraphImportGraphDefWithResults(\r\n--> 489               graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n    490           results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n    491         except errors.InvalidArgumentError as e:\r\n\r\nNotFoundError: Op type not registered 'NonMaxSuppressionV3' in binary running on YY041775. Make sure the Op and Kernel are registered in the binary running in this process."]}, {"number": 12729, "title": "NO documentation for Linux SUSE SLES 12", "body": "Hello Team,\r\n\r\nI struggled to get the clear documentation for Linux SUSE SLES12. Now a days one single rpm usually contains the entire install able binaries.\r\n\r\nCould you please add this as new feature?\r\n\r\nRegards,\r\nNaveen", "comments": ["Our main OS support matrix is: Ubuntu Linux 14 LTE to 16. Mac OS X El Capitan. CentOS 7 and above. We have very little familiarity with SUSE. If we wrote a guide, it wouldn't be nearly as good as what experts in the community are able to produce. Consider reaching out on Stack Overflow for community support."]}, {"number": 12728, "title": "Run Retrained inception model", "body": "I have trained tensorflow inception v3 model with new data sets. (on ubuntu)\r\nI got output_graph.pb and output_labels.txt files in tmp folder.\r\nWhen I tried to run as below \r\n\r\npython label_image.py --image /home/ubuntu/140924_HDR3DSC_0092_ISO00050_SS1250.JPG --graph /tmp/output_graph.pb --labels /tmp/output_labels.txt\r\n\r\nI got below errors\r\n\r\n  File \"label_image.py\", line 121, in <module>\r\n    input_operation = graph.get_operation_by_name(input_name);\r\n  File \"/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2836, in get_operation_by_name\r\n    return self.as_graph_element(name, allow_tensor=False, allow_operation=True)\r\n  File \"/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2708, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/home/ubuntu/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2768, in _as_graph_element_locked\r\n    \"graph.\" % repr(name))\r\nKeyError: \"The name 'import/input' refers to an Operation not in the graph.\"\r\n\r\nPlease suggest me what to do to resolve this.\r\nPlease suggest me if anything wrong from my end.", "comments": ["I'm getting the same thing", "I am having this issue as well. ", "This looks like the same as #12736 which has a workaround in the comments.", "Thank you all for your quick responce\r\nI could able to resolve the problem by changing the script path and run.\r\n\r\n/tensorflow/bin/examples/image_retraining$ python label_image.py --image /home/ubuntu/index.jpeg --graph /tmp/output_graph.pb --labels /tmp/output_labels.txt --output_layer=final_result:0\r\n\r\nEarlier script path was \"/home/ubuntu/tensorflow/bin/examples/label_image\" where I was getting mentioned error\r\n\r\n", "@ashishagni5 : Is this issue got resolved, if yes, why is this still open?", "@nitish11 forgot to close. done now", "@ashishagni5 \r\n Have you ever run the code label_image.py in tensorflow/tensorflow/examples/label_image/label_image.py\r\nI have modify it to run on a dataset and read and calssify image one by one,and as the number of images goes,the speed is slower and slower,at first,that's about ten images per second,and when the number of image goes to 1000,the time is about 7s,Incredibly!and I find the problem is in the function read_tensor_from_image_file in label_image.py and this part is read and preprocess images, so what's the matter?and I want to know how to speed up?and how to modify the code so as to making it run for batches ?"]}, {"number": 12727, "title": "Is tf.one_hot() w/ GPU not working under windows10?", "body": "I'm trying to run tensorflow in Windows10 environment.\r\nWhen I use `tf.one_hot` function with GPU then it occur error.\r\nBelow is test code, and it's working find in Ubuntu.\r\n\r\n### Test code\r\n```python\r\nfrom sklearn.datasets.samples_generator import make_regression\r\nimport tensorflow as tf\r\n\r\nX, y, coef = make_regression(n_samples=100, n_features=2, noise=0.1, coef=True)\r\n# To make all X values positive integer\r\nX = list(map(lambda xx: list(map(lambda x: abs(int(x*10)), xx)), X))\r\n\r\ntf.reset_default_graph()\r\ndef build_model(x):\r\n    xs = tf.split(x, [1,1], 1)\r\n    h1s = []\r\n    for i in range(2):\r\n        with tf.variable_scope('h1_%d'%i):\r\n            # Seems below line is the problem..\r\n            xs_temp = tf.reshape(tf.one_hot(xs[i], depth=30, on_value=1.0, off_value=0.0), [-1, 30])\r\n            weights = tf.get_variable('weight', [30, 2], initializer=tf.contrib.layers.xavier_initializer())\r\n            biases = tf.get_variable('biases', [2], initializer=tf.constant_initializer(0.0))\r\n            xs_temp = tf.nn.relu(tf.matmul(xs_temp, weights) + biases)\r\n            h1s.append(xs_temp)\r\n    h1 = tf.concat(h1s, 1)\r\n    with tf.variable_scope('h2'):\r\n        weights = tf.get_variable('weight', [4, 1], initializer=tf.contrib.layers.xavier_initializer())\r\n        biases = tf.get_variable('biases', [1], initializer=tf.constant_initializer(0.0))\r\n        y = tf.matmul(h1, weights) + biases\r\n    return y\r\n\r\nwith tf.device('/gpu:0'):\r\n    X_ph =  tf.placeholder(shape=[None,2],dtype=tf.int32)\r\n    y_ph =  tf.placeholder(shape=None,dtype=tf.float32)\r\n    output = build_model(X_ph)\r\nloss = tf.losses.mean_squared_error(y_ph, output)\r\nupdateModel = tf.train.AdamOptimizer(1e-2).minimize(loss)\r\n\r\ninit = tf.global_variables_initializer()\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    sess.run(init)\r\n    for _ in range(100):\r\n        sess.run(updateModel, feed_dict={X_ph: X, y_ph: y})\r\n```\r\n\r\n### Error \r\n```\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'h1_1/one_hot': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: h1_1/one_hot = OneHot[T=DT_FLOAT, TI=DT_INT32, axis=-1, _device=\"/device:GPU:0\"](split:1, h1_1/one_hot/depth, h1_1/one_hot/on_value, h1_1/one_hot/off_value)]]\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: Titan XP (12GB)\r\n- **Exact command to reproduce**:\r\n", "comments": ["(1) Refer [using_gpu](https://www.tensorflow.org/tutorials/using_gpu) link for NVIDIA devices.There is a option of `allow_soft_placement=True` which might work for you.\r\n(2) For windows 10, there are a few gpu settings which works well along with GPU. You can check [link](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso) here.", "In case I set `allow_soft_placement=True`, or I didn't specifically set `with device='/gpu:0'` in graph, `tf.one_hot` is automatically allocated to `/cpu:0`. It's working but it's getting really slow because all the data are need to be transferred between cpu and gpu memory.", "I have following questions,\r\nDo you get any warning while putting `allow_soft_placement=True` ? Does is work without any errors?\r\nAlso, What is the computation speed difference between Ubuntu and Windows OS for a same program?", "It's not possible to use `with tf.device('/gpu:0'):` to wrap an entire model to have it execute purely on the GPU. TensorFlow actually schedules to the GPU automatically, when it makes sense to do so.\r\n\r\nThere are actually a significant number of ops that only have CPU kernels. The ops that have both CPU and GPU kernels are mostly the ones that would stand to gain from the number crunching GPUs provide.\r\n\r\nIf the cost of copying memory between CPU and GPU is an issue, you might be able to find help redesigning your model to work around that on Stack Overflow.", "What I've issued is `tf.one_hot` is perfectly allocate to `/GPU:0` on Linux, but it can not be allocated on WIndows environment. I think the GPU kernel for Windows is not developed or have some error. And it makes 4~5 times slower on Windows than I have run on Linux when I allocated CPU for `tf.one_hot` module."]}, {"number": 12726, "title": "AssertionError: Cannot find .runfiles directory for bazel-bin/tensorflow/python/tools/freeze_graph", "body": "After I install the tensorflow from source, i'm trying to use freeze_graph tool, and it appear this error. \r\nMy OS is MacOS 10.12.5", "comments": ["hi,how can you deel with the problems, I meet it now", "Hi, what Bazel version are you using? There's a whole list of tensorflow native builds and their accompanying requirements. https://www.tensorflow.org/install/install_sources"]}, {"number": 12725, "title": "error bazel building quantize_graph with cuda", "body": "this is for tf 1.3 and bazel 0.53 with cuda 8 and cudnn 6 on windows 10 64 and python 3.6\r\n\r\n$ bazel build tensorflow/tools/quantization:quantize_graph  --verbose_failures\r\n____Loading complete.  Analyzing...\r\n____Found 1 target...\r\n____Building...\r\n____[0 / 10] Linking tensorflow/python/gen_state_ops_py_wrappers_cc.exe [for host]\r\n____From Linking tensorflow/python/gen_state_ops_py_wrappers_cc.exe [for host]:\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/pthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/ldl'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/ldl'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\n   Creating library bazel-out/host/bin/tensorflow/python/gen_state_ops_py_wrappers_cc.lib and object bazel-out/host/bin/tensorflow/python/gen_state_ops_py_wrappers_cc.exp\r\n____From Compiling tensorflow/core/kernels/batch_norm_op_gpu.cu.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-x'\r\ncl : Command line warning D9002 : ignoring unknown option '-nvcc_options=relaxed-constexpr'\r\ncl : Command line warning D9002 : ignoring unknown option '-nvcc_options=ftz=true'\r\ncl : Command line warning D9002 : ignoring unknown option '-msse3'\r\ncl : Command line warning D9024 : unrecognized source file type 'cuda', object file assumed\r\ncl : Command line warning D9027 : source file 'cuda' ignored\r\nERROR: C:/users/user/downloads/tensorflow/tensorflow/core/kernels/BUILD:2703:1: C++ compilation of rule '//tensorflow/core/kernels:depthwise_conv_op_gpu' failed (Exit 2): cl.exe failed: error executing command\r\n    SET CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0\r\n    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.14393.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.14393.0\\um\\x64;\r\n    SET NO_WHOLE_ARCHIVE_OPTION=1\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Anaconda3\\;C:\\users\\user\\downloads\\;C:\\tools\\msys64\\usr\\local\\bin;C:\\tools\\msys64\\usr\\bin;C:\\tools\\msys64\\usr\\bin;C:\\tools\\msys64\\opt\\bin;C:\\Windows\\System32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\tools\\msys64\\usr\\bin\\site_perl;C:\\tools\\msys64\\usr\\bin\\vendor_perl;C:\\tools\\msys64\\usr\\bin\\core_perl;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\user\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=8.0\r\n    SET TF_CUDNN_VERSION=6\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL=0\r\n    SET TMP=C:\\Users\\user\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /nologo /I. /Ibazel-out/msvc_x64-py3-opt/genfiles /Iexternal/bazel_tools /Ibazel-out/msvc_x64-py3-opt/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_sycl /Iexternal/protobuf /Ibazel-out/msvc_x64-py3-opt/genfiles/external/protobuf /Iexternal/gif_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive /Iexternal/jpeg /Ibazel-out/msvc_x64-py3-opt/genfiles/external/jpeg /Iexternal/com_googlesource_code_re2 /Ibazel-out/msvc_x64-py3-opt/genfiles/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/msvc_x64-py3-opt/genfiles/external/fft2d /Iexternal/highwayhash /Ibazel-out/msvc_x64-py3-opt/genfiles/external/highwayhash /Iexternal/png_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/zlib_archive /Iexternal/snappy /Ibazel-out/msvc_x64-py3-opt/genfiles/external/snappy /Iexternal/local_config_cuda /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda /Iexternal/bazel_tools/tools/cpp/gcc3 /Iexternal/eigen_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/eigen_archive /Iexternal/protobuf/src /Ibazel-out/msvc_x64-py3-opt/genfiles/external/protobuf/src /Iexternal/gif_archive/lib /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive/windows /Iexternal/farmhash_archive/src /Ibazel-out/msvc_x64-py3-opt/genfiles/external/farmhash_archive/src /Iexternal/png_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/zlib_archive /Iexternal/local_config_cuda/cuda /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include /showIncludes /MT /O2 /c tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc /Fobazel-out/msvc_x64-py3-opt/bin/tensorflow/core/kernels/_objs/depthwise_conv_op_gpu/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.o -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true -DGOOGLE_CUDA=1 -msse3 /DLANG_CXX11 /D__VERSION__=\"MSVC\" /DPLATFORM_WINDOWS /DTF_COMPILE_LIBRARY /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /U_HAS_EXCEPTIONS /D_HAS_EXCEPTIONS=1 /EHsc.\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(359): error C3861: 'atomicAdd': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(360): error C3861: 'atomicAdd': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(361): error C3861: 'atomicAdd': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(362): error C3861: 'atomicAdd': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(365): error C3861: 'atomicMax': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(366): error C3861: 'atomicMax': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(378): error C3861: 'max': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(378): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(394): error C3861: '__longlong_as_double': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(394): error C3861: '__double_as_longlong': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(393): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(399): error C3861: '__longlong_as_double': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(445): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(462): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(498): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(507): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(516): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(527): error C3861: '__int_as_float': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(527): error C3861: '__float_as_int': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(526): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(529): error C3861: '__int_as_float': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(538): error C3861: '__longlong_as_double': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(538): error C3861: '__double_as_longlong': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(537): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(540): error C3861: '__longlong_as_double': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(548): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(557): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(566): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(577): error C3861: '__int_as_float': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(577): error C3861: '__float_as_int': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(576): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(579): error C3861: '__int_as_float': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(588): error C3861: '__longlong_as_double': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(588): error C3861: '__double_as_longlong': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(587): error C3861: 'atomicCAS': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(590): error C3861: '__longlong_as_double': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(617): error C2065: 'warpSize': undeclared identifier\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(619): error C2059: syntax error: 'volatile'\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(620): error C3861: '__shfl': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(621): error C3861: '__shfl': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(622): error C2059: syntax error: 'volatile'\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(637): error C2065: 'warpSize': undeclared identifier\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(639): error C2059: syntax error: 'volatile'\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(640): error C3861: '__shfl_up': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(641): error C3861: '__shfl_up': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(642): error C2059: syntax error: 'volatile'\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(657): error C2065: 'warpSize': undeclared identifier\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(659): error C2059: syntax error: 'volatile'\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(660): error C3861: '__shfl_down': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(661): error C3861: '__shfl_down': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(662): error C2059: syntax error: 'volatile'\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(677): error C2065: 'warpSize': undeclared identifier\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(679): error C2059: syntax error: 'volatile'\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(680): error C3861: '__shfl_xor': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(681): error C3861: '__shfl_xor': identifier not found\r\n.\\tensorflow/core/util/cuda_kernel_helper.h(682): error C2059: syntax error: 'volatile'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(70): error C2182: '__launch_bounds__': illegal use of type 'void'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(71): error C2061: syntax error: identifier 'DepthwiseConv2dGPUKernelNHWC'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(161): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(161): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(295): error C2182: '__launch_bounds__': illegal use of type 'void'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(295): error C2374: 'tensorflow::__launch_bounds__': redefinition; multiple initialization\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(70): note: see declaration of 'tensorflow::__launch_bounds__'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(296): error C2061: syntax error: identifier 'DepthwiseConv2dGPUKernelNCHW'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(431): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(431): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(759): warning C4068: unknown pragma\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(711): error C2182: '__launch_bounds__': illegal use of type 'void'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(711): error C2374: 'tensorflow::__launch_bounds__': redefinition; multiple initialization\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(70): note: see declaration of 'tensorflow::__launch_bounds__'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(712): error C2061: syntax error: identifier 'DepthwiseConv2dBackpropInputGPUKernelNHWC'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(779): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(779): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(916): error C2182: '__launch_bounds__': illegal use of type 'void'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(916): error C2374: 'tensorflow::__launch_bounds__': redefinition; multiple initialization\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(70): note: see declaration of 'tensorflow::__launch_bounds__'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(917): error C2061: syntax error: identifier 'DepthwiseConv2dBackpropFilterGPUKernelNHWC'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1024): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1024): error C2447: '{': missing function header (old-style formal list?)\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1170): error C2182: '__launch_bounds__': illegal use of type 'void'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1170): error C2374: 'tensorflow::__launch_bounds__': redefinition; multiple initialization\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(70): note: see declaration of 'tensorflow::__launch_bounds__'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1171): error C2061: syntax error: identifier 'DepthwiseConv2dBackpropFilterGPUKernelNCHW'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1284): error C2143: syntax error: missing ';' before '{'\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc(1284): error C2447: '{': missing function header (old-style formal list?)\r\ncl : Command line warning D9002 : ignoring unknown option '-x'\r\ncl : Command line warning D9002 : ignoring unknown option '-nvcc_options=relaxed-constexpr'\r\ncl : Command line warning D9002 : ignoring unknown option '-nvcc_options=ftz=true'\r\ncl : Command line warning D9002 : ignoring unknown option '-msse3'\r\ncl : Command line warning D9024 : unrecognized source file type 'cuda', object file assumed\r\ncl : Command line warning D9027 : source file 'cuda' ignored\r\nTarget //tensorflow/tools/quantization:quantize_graph failed to build\r\n____Elapsed time: 8.033s, Critical Path: 4.14s\r\n", "comments": ["At the moment, we use cmake to build all of our TF code on windows GPU.\r\nBazel is not very well tested, but we are working hard to make bazel fully work on windows.\r\n@meteorcloudy may have some ideas about the messages you see. It may be as simple as needing  a different `--config` value in your bazel command.\r\n\r\nYour logs have these messages, which look suspicious to me:\r\n```\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored\r\nLINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored\r\n.....\r\ncl : Command line warning D9024 : unrecognized source file type 'cuda', object file assumed\r\n```\r\n\r\n@meteorcloudy could you comment?", "@eyaler Looks like the python wrapper script is not used in Bazel, from Bazel 0.5.3 we are not using the python script to invoke cl.exe, but for the cuda compiler we still need it.\r\nPlease add `--action_env=USE_MSVC_WRAPPER=1` to enable it.\r\n\r\nAs @gunan pointed out, the Cuda build with Bazel is not well tested, so it's not guaranteed to work. But we are working on making it better!", "@meteorcloudy thanks. I now get:\r\n\r\n$ bazel build tensorflow/tools/quantization:quantize_graph --verbose_failures --action_env=USE_MSVC_WRAPPER=1\r\n____Loading package: tensorflow/tools/quantization\r\n____Found 1 target...\r\n____Building...\r\n____[0 / 6] Creating source manifest for //tensorflow/tools/quantization:quantize_graph\r\n____[5 / 29] Compiling tensorflow/core/common_runtime/gpu/gpu_tracer.cc\r\nERROR: C:/users/user/downloads/tensorflow/tensorflow/core/BUILD:1770:1: C++ compilation of rule '//tensorflow/core:gpu_tracer' failed (Exit 2): msvc_cl.bat failed: error executing command\r\n    SET CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET CUDA_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0\r\n    SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.14393.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.14393.0\\um\\x64;\r\n    SET NO_WHOLE_ARCHIVE_OPTION=1\r\n    SET PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v8.0/bin;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Anaconda3\\;C:\\users\\user\\downloads\\;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin;C:\\tools\\msys64\\mingw64\\bin;C:\\tools\\msys64\\usr\\local\\bin;C:\\tools\\msys64\\usr\\bin;C:\\tools\\msys64\\usr\\bin;C:\\Windows\\System32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\tools\\msys64\\usr\\bin\\site_perl;C:\\tools\\msys64\\usr\\bin\\vendor_perl;C:\\tools\\msys64\\usr\\bin\\core_perl;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Anaconda3\r\n    SET TEMP=C:\\Users\\user\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=8.0\r\n    SET TF_CUDNN_VERSION=6\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL=0\r\n    SET TMP=C:\\Users\\user\\AppData\\Local\\Temp\r\n    SET USE_MSVC_WRAPPER=1\r\n  external/local_config_cc/wrapper/bin/msvc_cl.bat /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 -Xcompilation-mode=opt /nologo /I. /Ibazel-out/msvc_x64-py3-opt/genfiles /Iexternal/bazel_tools /Ibazel-out/msvc_x64-py3-opt/genfiles/external/bazel_tools /Iexternal/protobuf /Ibazel-out/msvc_x64-py3-opt/genfiles/external/protobuf /Iexternal/eigen_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive /Iexternal/jpeg /Ibazel-out/msvc_x64-py3-opt/genfiles/external/jpeg /Iexternal/com_googlesource_code_re2 /Ibazel-out/msvc_x64-py3-opt/genfiles/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/msvc_x64-py3-opt/genfiles/external/fft2d /Iexternal/highwayhash /Ibazel-out/msvc_x64-py3-opt/genfiles/external/highwayhash /Iexternal/png_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/zlib_archive /Iexternal/snappy /Ibazel-out/msvc_x64-py3-opt/genfiles/external/snappy /Iexternal/local_config_cuda /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda /Iexternal/bazel_tools/tools/cpp/gcc3 /Iexternal/protobuf/src /Ibazel-out/msvc_x64-py3-opt/genfiles/external/protobuf/src /Iexternal/eigen_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/msvc_x64-py3-opt/genfiles/external/gif_archive/windows /Iexternal/farmhash_archive/src /Ibazel-out/msvc_x64-py3-opt/genfiles/external/farmhash_archive/src /Iexternal/png_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibazel-out/msvc_x64-py3-opt/genfiles/external/zlib_archive /Iexternal/local_config_cuda/cuda /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/extras/CUPTI/include /Ibazel-out/msvc_x64-py3-opt/genfiles/external/local_config_cuda/cuda/cuda/extras/CUPTI/include /showIncludes /MT /O2 /c tensorflow/core/common_runtime/gpu/gpu_tracer.cc /Fobazel-out/msvc_x64-py3-opt/bin/tensorflow/core/_objs/gpu_tracer/tensorflow/core/common_runtime/gpu/gpu_tracer.o -DGOOGLE_CUDA=1 -msse3 /DLANG_CXX11 /D__VERSION__=\"MSVC\" /DPLATFORM_WINDOWS /DTF_COMPILE_LIBRARY /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /U_HAS_EXCEPTIONS /D_HAS_EXCEPTIONS=1 /EHsc -DGOOGLE_CUDA=1.\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(131): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(131): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(182): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(182): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(183): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(183): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(184): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(184): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(185): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(185): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(186): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(186): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(187): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(187): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(194): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(194): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(195): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(195): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(196): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(196): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(197): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(197): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(198): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(198): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(199): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(199): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(200): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(200): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(201): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(201): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(202): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(202): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(203): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(203): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(204): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(204): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(205): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(205): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(244): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(244): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(416): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(416): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(419): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(419): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(422): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(422): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(426): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(426): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(429): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(429): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(432): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(432): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(435): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(435): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(438): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(438): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(441): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(441): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(445): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(445): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(457): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(457): error C2059: syntax error: '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(461): error C2589: 'constant': illegal token on right side of '::'\r\nC:\\tools\\msys64\\tmp\\_bazel_user\\8rsmy-kr\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_tracer.cc(461): error C2059: syntax error: '::'\r\nWarning: Unmatched arguments: -msse3\r\nTarget //tensorflow/tools/quantization:quantize_graph failed to build\r\n____Elapsed time: 4.785s, Critical Path: 2.32s\r\n", "git clean -xdf\r\nhelped me", "Hi @eyaler \r\nIs your problem solved?"]}, {"number": 12724, "title": "Flatten all gradients in an MLP to a tensor of rank 1 (i.e. 1D array)", "body": "Suppose the following Keras model:\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(Dense(512, activation='sigmoid', input_shape=(784,)))\r\nmodel.add(Dense(10, activation='softmax'))\r\n```\r\nObviously we can calculate the gradients by:\r\n\r\n`grads = K.gradients(loss, params)\r\n` \r\n\r\nwhich just calls:\r\n\r\n`tf.gradients(loss, variables, colocate_gradients_with_ops=True)\r\n`\r\n\r\nThis returns a list of tensors containing:\r\n\r\n1) a tensor with 512x784 elements (input to hidden connections)\r\n2) a tensor with the biases of the 512 units in the hidden layer\r\n3) a tensor with 10x512 elements (hidden to output connections)\r\n4) a tensor with the biases of the 10 output units\r\n\r\nI would like to ask if there's a simple way to \"flatten\" `grads` to a single tensor of rank 1 (i.e. 1D array) with `(512x784)+512+(10x512)+10 `elements, without looping over the layers and corresponding biases.\r\n\r\nThanks", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Yes you are right - sorry about that. I was just hoping to reach more tensorflow experts here "]}, {"number": 12723, "title": "uploads tensorflow as follow error:", "body": "uploads tensorflow as follow error:\r\n\r\nImportError: numpy.core.multiarray failed to import   Failed to load the native TensorFlow runtime.  See https://www.tensorflow.org/install/install_sources#common_installation_problems \r\n\r\n\r\nThank you for your help!\r\n", "comments": ["Not sure I understand what you mean.\r\nAre you asking us to update common installation problems with a new issue?\r\nCould you elaborate?"]}, {"number": 12722, "title": "from tensorflow.python.ops.gen_audio_ops import *", "body": "I am trying to follow the steps as per the readme file.\r\nI get an import error.\r\nAfter i inspected the repo i found that \"gen_audio_ops\" is missing under tensorflow/python/ops package.\r\n\r\nFollowing is an image which shows the import \r\n![image](https://user-images.githubusercontent.com/17523473/29909246-9f1e1b0e-8e42-11e7-8f89-d601281d38c0.png)\r\n\r\n\r\nCan you please help to get me the said package.", "comments": ["I ran into similar issue. But if you built from source you will see that. Look like it will be generated during building. ", "Ya - all files prefaced by `gen_` are files that are **gen**erated during the build process (for example ops in Python because they are built by a processor that examines the actual C code containing the operations.)\r\n", "I faced the same issue. I built tensorflow from source,but still getting the same issue. What else do I have to build from source to get this file? very confused :/", "@sarah-zaheer It would be most straight forward if you built the pip builder:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nAfter that completes, you could either build a wheel archive to install with pip by doing\r\n```\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /some/directory/to/put/the/whl/in\r\n```\r\nor if you're interested in this specific generated file, it should be at\r\n```\r\nbazel-genfiles/tensorflow/python/ops/gen_audio_ops.py\r\n```", "@quaeler  Thank you very much :) Do I move this file to 'tensorflow/python/ops/' now?\r\nOR would it be better if I changed the path and pointed it to 'bazel-genfiles/tensorflow/python/ops/'?", "@sarah-zaheer Were it me, i'd build the wheel and pip install into a virtualenv or a conda env. If you don't want to go that route, you could change the path, but try pointing it at\r\n```\r\nbazel-out/darwin_x86_64-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow\r\n```\r\nto pick up everything (adjusting that path for the name of your build architecture as necessary.)\r\n", "***My command:**\r\nsudo python tensorflow/examples/speech_commands/train.py > --data_url= > --data_dir= /home/sarah/Downloads/final/ > --wanted_words= shout,glass > --how_many_training_steps= 1000,200 > --learning_rate= 0.001,0.0001 \r\n\r\n***The output:**\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/speech_commands/train.py\", line 79, in <module>\r\n    import input_data\r\n  File \"/home/sarah/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 35, in <module>\r\n    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\r\nImportError: cannot import name audio_ops\r\n\r\n***In /tensorflow/contrib/framework/python/ops/audio_ops.py :** \r\n   _from tensorflow.python.ops.gen_audio_ops import *_ \r\n(same as krishnadn reported)\r\n\r\n***Then I found 'gen_audio_ops' at /bazel-genfiles/tensorflow/python/ops (just like you suggested) and moved this file to /tensorflow/python/ops**\r\n\r\n***Now in gen_audio_ops,this line seems to be the problem :** \r\n    _from tensorflow.core.framework import op_def_pb2 as_op_def_pb2_ \r\n\r\nbecause 'op_def_pb2' is not at /tensorflow/core/framework\r\n\r\n***Now as you suggested changing the path, I have this,**\r\n_/bazel-out/local-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow_\r\nwhere do I use this path?\r\n\r\n(Im sorry if my question is not in proper github format, Im very new here)", "Not gonna use pip, eh? :- )  Ok.. \r\n\r\nFirst, running this via sudo makes me feel unwell - usual caveats about not running things as sudo go here, and i'm done being a nanny.\r\n\r\nSecond, if you moved the `gen_audio_ops.py` file to `/tensorflow/python/ops`, move it back (or at least delete it from `/tensorflow/python/ops`) - i only meant to point out where it gets generated to, not to suggest you should modify the contents of the tensorflow local code directories.\r\n\r\nWRT the path, if you append your PYTHONPATH environment variable to include the full path to your\r\n```\r\nbazel-out/local-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow\r\n```\r\nthen it should also find `op_def_pb2` because it should be sitting in\r\n```\r\nbazel-out/local-opt/bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/core/framework/op_def_pb2.py\r\n```\r\nDon't forget that if you redefine the environment variable, but still continue to use sudo, you'll need to use `sudo -E ...` to pass along the environment variables.\r\n", "Alright, no sudo from now on. :-)\r\nI'm very sorry if I forgot to mention that I did the pip part already. (followed all the instructions at [https://www.tensorflow.org/install/install_sources](url) including pip installing the .whl file.)\r\n\r\n", "Ah ok - if you pip installed the whl created by the methods described on that page, then switch to the virtualenv or conda environment into which you installed it, unless you installed it into the system using `sudo pip ...`.\r\n\r\nThen, make sure you change directories out of the tensorflow source root, as this has been reported by some people to end up with Python trying to import from the source tree; at that point, the Python script should be able to find all imports without problem.\r\n\r\nThat still isn't finding the imports, lemme know what commit of master or branch you pulled your source from.", "**After appending my PYTHONPATH environment variable like you suggested, (and no sudo)**\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/speech_commands/train.py\", line 77, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/sarah/tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/sarah/tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/sarah/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/sarah/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named pywrap_tensorflow_internal\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n**Found this on Stack Overflow:** \r\n[https://stackoverflow.com/questions/44768751/import-tensorflow-error-for-ubuntu-16-04-importerror-no-module-named-pywrap-te](url)\r\n\r\nthe suggestion there is to uninstall tensorflow, which I dont think so! because it took 6+ hours to build tensorflow from source.\r\n", "The PYTHONPATH suggestion was a \"if you don't install via pip, then you can have Python reference the built contents which would otherwise go in to the pip install by setting your PYTHONPATH to look at the bazel output directories.\"\r\n\r\nSince you installed from pip, don't alter the PYTHONPATH, and make sure your current working directory is not your local tensorflow repository directory when you run python. (i.e If we're seeing any files in the repository directory in your stack trace, we know Python is reading from the wrong place.)", "Yes I did install using sudo pip..\r\n(which commit? I dont understand)\r\n\r\nI cloned tensorflow into my home directory so now running \r\n_python tensorflow/tensorflow/examples/speech_commands/train.py_\r\nfrom my home directory and still getting this error message\r\n(Previously I was running from within the tensorflow root directory)\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow/tensorflow/examples/speech_commands/train.py\", line 79, in <module>\r\n    import input_data\r\n  File \"/home/sarah/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 35, in <module>\r\n    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\r\nImportError: cannot import name audio_ops\r\n\r\n", "You don't want to invoke Python on tensorflow-dependent code from **any** directory that is at the head of the tensorflow repository code - whether that be where you git-cloned to, or whether you made a duplicate elsewhere.\r\n\r\nMaybe the best way is for you to repeat my attempt at this; i've done the following after last pulling my local repository from master around 100 minutes ago:\r\n```\r\nautogenic:tensorflow loki$ bazel clean\r\nautogenic:tensorflow loki$ ./configure \r\nautogenic:tensorflow loki$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nautogenic:tensorflow loki$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nautogenic:tensorflow loki$ cd ~/arbeit/virtualenvs\r\nautogenic:virtualenvs loki$ virtualenv --system-site-packages tensorflow_20170902\r\nautogenic:virtualenvs loki$ source tensorflow_20170902/bin/activate\r\n(tensorflow_20170902) autogenic:virtualenvs loki$ pip install /tmp/tensorflow_pkg/tensorflow-1.3.0-cp27-cp27m-macosx_10_12_intel.whl\r\n(tensorflow_20170902) autogenic:virtualenvs loki$ cd tensorflow_20170902/\r\n(tensorflow_20170902) autogenic:tensorflow_20170902 loki$ mkdir sarah-zaheer\r\n(tensorflow_20170902) autogenic:tensorflow_20170902 loki$ cd sarah-zaheer\r\n(tensorflow_20170902) autogenic:sarah-zaheer loki$ python ~/arbeit/worg/projects/tensorflow/tensorflow/examples/speech_commands/train.py \r\n>> Downloading speech_commands_v0.01.tar.gz 100.0%\r\nINFO:tensorflow:Successfully downloaded speech_commands_v0.01.tar.gz (1488293908 bytes)\r\nWARNING:tensorflow:From /Users/loki/arbeit/worg/projects/tensorflow/tensorflow/examples/speech_commands/train.py:161: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.get_or_create_global_step\r\nINFO:tensorflow:Training from step: 1 \r\nINFO:tensorflow:Step #1: rate 0.001000, accuracy 6.0%, cross entropy 2.673610\r\nINFO:tensorflow:Step #2: rate 0.001000, accuracy 7.0%, cross entropy 2.632296\r\nINFO:tensorflow:Step #3: rate 0.001000, accuracy 11.0%, cross entropy 2.587655\r\nINFO:tensorflow:Step #4: rate 0.001000, accuracy 10.0%, cross entropy 2.540978\r\nINFO:tensorflow:Step #5: rate 0.001000, accuracy 14.0%, cross entropy 2.560040\r\nINFO:tensorflow:Step #6: rate 0.001000, accuracy 11.0%, cross entropy 2.539041\r\nINFO:tensorflow:Step #7: rate 0.001000, accuracy 4.0%, cross entropy 2.655353\r\nINFO:tensorflow:Step #8: rate 0.001000, accuracy 9.0%, cross entropy 2.609798\r\n...\r\n```\r\n\r\nThe take-away point i'd make in a super-preachy sort of way would be:\r\n* never, ever, (like Chris Rock \"never-ever\") pip install this stuff into your system Python install (unless you're running throw away docker-and-its-ilk-like images); once done, you've got some transient version of tensorflow stuck in your system and you pray that pip uninstall manages to wipe it all away correctly\r\n\r\nand its corollary:\r\n* use a virtualenv for this sort of stuff; if i end up installing the worst buggiest snapshot of tensorflow, i just delete that environment directory (`tensorflow_20170902` in the above example) from my filesystem and i'm back to pure-as-the-driven-snow.\r\n\r\nLemme know how the above works.\r\n\r\nPS. Do try to do a `sudo pip uninstall ...` of tensorflow before doing anything else. If you don't, the above virtualenv creation command will copy the system's tensorflow install into your virtualenv and many continued problems and hilarities will ensure.\r\n", "Thank you very much for helping me out so much. I'm working on it like you have suggested. Let's see what happens :)", "I have had similar issue as seen below:\r\n\r\n\"(tensorflow)*usrname*@*usrname*:~/tensorflow/tensorflow-0$ python tensorflow/examples/speech_commands/train.py\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/speech_commands/train.py\", line 79, in <module>\r\n    import input_data\r\n  File \"/home/cogknit/tensorflow/tensorflow-0/tensorflow/examples/speech_commands/input_data.py\", line 35, in <module>\r\n    from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\r\nImportError: cannot import name audio_ops_\"\r\n\r\nBut I am sorry I am not able to follow your suggestions due to my own lack of understanding of the software based lingo.To begin with, how to go into the autogenic:tensorflow mode as you suggested above in your most recent reply.\r\nIs that the solution that I should implement when I have installed tensorflow through virtualenv and it uses python 2.7 ?\r\nThanks ahead.\r\n", "Yes, create a virutalenv, then switch to that virtualenv (\"activate\" that virtualenv) and [pip-install tensorflow into that virtualenv.](https://www.tensorflow.org/install/install_linux#installing_with_virtualenv)\r\n\r\nFrom a directory **that is not in the tensorflow source tree** and while still in the virtualenv, run the example Python script.\r\n", "As suggested by you I pip installed the tensorflow within a virtualenv (on Ubuntu 14.04 ,followed the steps in the following link :https://github.com/ravivalluri/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import-error)\r\nI am attaching the trailing end of the installation : \r\n\".\r\n .\r\n    x86_64-linux-gnu-gcc: numpy/random/mtrand/distributions.c\r\n    x86_64-linux-gnu-gcc: numpy/random/mtrand/mtrand.c\r\n    x86_64-linux-gnu-gcc: numpy/random/mtrand/initarray.c\r\n    x86_64-linux-gnu-gcc: numpy/random/mtrand/randomkit.c\r\n    x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/numpy/random/mtrand/mtrand.o build/temp.linux-x86_64-2.7/numpy/random/mtrand/randomkit.o build/temp.linux-x86_64-2.7/numpy/random/mtrand/initarray.o build/temp.linux-x86_64-2.7/numpy/random/mtrand/distributions.o -Lbuild/temp.linux-x86_64-2.7 -o build/lib.linux-x86_64-2.7/numpy/random/mtrand.so\r\n    Creating build/scripts.linux-x86_64-2.7/f2py\r\n      adding 'build/scripts.linux-x86_64-2.7/f2py' to scripts\r\n    changing mode of build/scripts.linux-x86_64-2.7/f2py from 664 to 775\r\n    \r\n    warning: no previously-included files matching '*.pyo' found anywhere in distribution\r\n    warning: no previously-included files matching '*.pyd' found anywhere in distribution\r\n    changing mode of /home/cogknit/tensorflow/bin/f2py to 775\r\n  Found existing installation: setuptools 2.2\r\n    Uninstalling setuptools:\r\n      Successfully uninstalled setuptools\r\nSuccessfully installed tensorflow protobuf wheel mock numpy setuptools funcsigs pbr\r\nCleaning up...\r\n\"\r\n\r\nI then run the python script as follows : \r\n(tensorflow)cogknit@cogknit:~$ python /home/cogknit/tensorflow/tensorflow/examples/speech_commands/train.py \r\n\r\nI am sorry but I am still getting the same error.Can you please tell me what am I doing wrong here?Also can you please elaborate on what you mean by \"a directory that is not in the tensorflow source tree\" if that's where the error is?\r\n\r\nThank You", "I'm not familiar with the instructions in the link you provided, and see nothing in the section of installation that you chose to include that is alarming. So from those things, i can't tell why it's not working for you.\r\n\r\nSince this issue is closed, no other users are likely to chime in their help; you might have more luck using Stack Overflow for this sort of issue since it's not really a bug in TF.\r\n", "@quaeler, can you please share your insight on the comments related to the error in the following link that I came across in stack overflow ?  \r\n  https://stackoverflow.com/questions/45952387/anaconda-install-of-tensorflow-missing-audio-ops-from-contrib-framework\r\n\r\nWhile they say it's a missing piece from the developers' side itself, your method seems to have worked.\r\n\r\nNonetheless,your suggestions have given me a lot of clarity. Thanks a lot for your help thus far. "]}, {"number": 12721, "title": "dataset with tf.pyfunc error", "body": "The new IO API is awesome, but I found this error in my project.\r\n![image](https://user-images.githubusercontent.com/25046619/29908076-feb68952-8e51-11e7-8bd2-feed44d143cb.png)\r\n I tried to get some index from dataset, then make a batch index, \r\ngiven the batch index, I got the features from memory which used tf.py_func API, but I found this error.\r\nInvalid argument: Shapes of all inputs must match: values[0].shape = [4096] != values[3].shape = [4096,41]", "comments": ["I solved the Problem: convert the results' type into tuple instead use of list.The official Guide document should be updated.", "Thanks for the suggestion!\r\nWould you like to send a pull request with your proposed fix?", "The official  Guide(https://www.tensorflow.org/programmers_guide/datasets) is different from that on master branch (tensorflow/tensorflow/docs_src/programmers_guide/datasets.md).The usage example from master branch is right.Can you update the official Guide? Additionally, why can't the API convert the list to tuple automatically?", "Oh, someone has made an issue before on this problem, Please update the official guide\r\n#11786 "]}, {"number": 12720, "title": "PREP: migrate lgamma gradient to c++ side", "body": "It's my first attempt to contribute to C++ side. Hence the PR is opened early to get feedback.\r\n\r\n### What changes were proposed in this pull request?\r\n\r\nMigrate the implementation of _Lagmma from python side to c++ side, see  #12686.\r\n\r\n### How was this patch tested?\r\n\r\n+ [x] add unit tests.\r\n+ [ ] pass all tests.", "comments": ["Can one of the admins verify this patch?", "@facaiy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @caisq to be potential reviewers.", "Hi, @kbsriram @skye . Could you take a look? Thanks.", "Looks ok to me, may be an appropriate time to get an opinion/review from one of the maintainers like @suharshs ?", "@tensorflow-jenkins test this please", "Thanks, @suharshs . As select values to avoid instability is a little tricky,  I am curious where are the unit tests of python implementation.", "I don't believe testing was done very thoroughly for the python gradients :( if there are these for math_grad they would be here: https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/ops/math_grad_test.py\r\n\r\nBut it doesn't look like there is one for this op.", "Thanks, I got it."]}, {"number": 12719, "title": "Add uint16 support for tf.decode_raw", "body": "This fix tries to address the request raised in #10124 where uint16 support for tf.decode_raw is needed. tf.decode_raw already support half, float32, float64, int8, int16, int32, int64, uint8.\r\n\r\nAnd uint16 was not supported yet.\r\n\r\nThis fix adds uint16 support for tf.decode_raw.\r\n\r\nThis fix fixes #10124.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.", "Can one of the admins verify this patch?", "Jenkins, test this please.", "The previously Jenkins test failure `tfexample_decoder_test` was caused by the fact that `decode_raw` support `uint16` now.\r\n\r\nThe PR has been updated. Now Jenkins tests should pass.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@rmlarsen Thanks for the review. The PR has been updated with unit tests added. Please take a look.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 12718, "title": "how to find distance from camera to object?", "body": "hello.\r\ni am a beginner.\r\nhow to find distance(Meter) from camera to object with tensorflow?\r\nThanks.", "comments": ["please cove me.\r\nnobody to answer my question?\r\nNobody knows the answer?\r\npls.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12717, "title": "The installation instructions not updated for latest Mac OS X. ", "body": "Please update instructions; ML engineers dont have time to waste spending on configurations.", "comments": ["Could you be clearer on what issues you are seeing in our installation instructions?\r\nNothing should have changed on MacOS since we last updated our instructions, but we are happy to revise them.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 12716, "title": "Change `dim` to `axis` for tf.nn.softmax and tf.nn.log_softmax", "body": "This fix tries to address the issue raised in #7391 where `dim` was not changed to `axis` for tf.nn.softmax and tf.nn.log_softmax.\r\n\r\nThis is inconsistent with other ops in tensorflow.\r\n\r\nThis fix adds axis while at the same time keeps dim so that backward compatibility is maintained.\r\n\r\nThis fix fixes #7391.\r\n\r\nThis fix fixes #14191.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @jhseu to be potential reviewers.", "Can one of the admins verify this patch?", "@martinwicke Thanks for the review. The PR has been updated using `@deprecated_args`.", "Jenkins, test this please.\r\n\r\n@martinwicke any opinion on the new version?", "The Jenkins failure is `//tensorflow/tools/api/tests:api_compatibility_test `. Will updated the golden after api changes approved.", "(Okay for API change, deferring to @aselle / @martinwicke for the review)", "Thanks all for the review. The PR has been rebased and updated with API goldens:\r\n```\r\n    bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n              --update_goldens True\r\n```\r\n\r\nPlease take a look.", "Jenkins, test this please.", "The rest of the tests seems to be fine. For `Linux CPU Tests Makefile` I think it might be unrelated?", "Jenkins, test this please.", "The PR has been updated to change `dim` to `axis` for `tf.nn.l2_normalize` as well, to address the issue raised in #14191. /cc @girving ", "Jenkins, test this please.", "Not sure why the previous Internal build failed as I run it on my dev machine and all passes. I rebased the PR anyway to bring it to the most recent HEAD. I think a rerun of Jenkins will pass.", "Jenkins. test this please.\r\n\r\nThere are currently some issues with our test infrastructure. Sorry about that :./", "Thanks @aselle for the review. The PR has been updated with `deprecated_argument_lookup` used. Please take a look and let me know if there are any issues.", "(Removing API Review label as the API change is fine)", "Jenkins, test this please.", "Actually, didn't need testing, changes still to be done.\r\n", "@aselle @martinwicke think the review comment about `deprecated_argument_lookup` has been covered in the update. Please take a look and let me know if there are additional issues.", "Oops, sorry, my bad, I didn't see that. LGTM.", "I have a suspicion that the failure is due to a broken GPU on gpu6.\r\n\r\nJenkins, test this please.", "None of the test failures are related.", "Thanks @yongtang!"]}, {"number": 12715, "title": "C++ gradients: reduce_min, reduce_max", "body": "Anyone already working on adding these two operators to the C++ gradients? Otherwise, I'll sign up for it.\r\n\r\n/cc @bpiel @suharshs ", "comments": ["@kbsriram Not me. Thanks for asking."]}, {"number": 12714, "title": "Fix dtype for streaming confusion matrix", "body": "Currently, dependents of `_streaming_confusion_matrix` don't actually work correctly with floating-point weights.", "comments": ["@taion, thanks for your PR! By analyzing the history of the files in this pull request, we identified @drpngx, @tensorflower-gardener and @jart to be potential reviewers.", "Can one of the admins verify this patch?", "@poxvoculi your name was all over blame, I was confused too. Probably a merge you did.\r\n\r\n@alextp can you take a look?", "Updated.", "Jenkins, test this please.", "Please address the test failure as it is relevant.", "Oops, my bad. I updated the wrong weight for the test. Fixed now.", "Anyway, be aware that if you have any internal consumers of `_streaming_confusion_matrix` without weights in Python 2 that divide by integers (e.g. for building a streaming normalized confusion matrix metric), this would constitute a bit of a breaking change in practice, as in the no-weights case, the confusion matrix tensors will now be of type `int64` rather than `float64` as before.", "Jenkins, test this please.", "Jenkins, test this please.", "One change request from API review.", "Jenkins, test this please. "]}, {"number": 12713, "title": "Branch 167045325", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @lukaszkaiser and @vrv to be potential reviewers.", "I signed it!", "Thank you!"]}, {"number": 12712, "title": "sharing variables but matrices are transposed even though src & dst tensors appear to have same shape", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (pip install tensorflow-gpu==1.2.1)\r\n\r\n- **TensorFlow version (use command below)**:\r\nv1.2.0-5-g435cdfc, 1.2.1\r\n\r\n- **Python version**: \r\nPython 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15)\r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\nIPython 5.3.0 -- An enhanced Interactive Python.\r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCuda V8.0.61, CuDNN 5.1\r\n\r\n- **GPU model and memory**:\r\nGeForce GTX 1080, 8GB\r\n\r\n### Describe the problem\r\nAt first I thought I was doing something wrong (initially posted on [stackoverflow](https://stackoverflow.com/questions/45969089/trying-to-share-tensorflow-variables-between-autoencoder-and-decoder-but-weight)), but now I think this is a bug. Full text from SO pasted below.\r\n\r\nI want to share variables between an autoencoder and a decoder. But the weights matrix from z to the first fully connected flat layer is transposed, even though at every step of the graph construction I dump the previous tensor scopename and shape to the console, and in both cases ('autoencoder' and 'decoder') the scopenames and shapes are identical.\r\n\r\nTo be specific the error is \r\n\r\n> Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).\r\n\r\nBut in both cases I'm creating a dense layer from (?, 128) to units=65536 with the same code:\r\n\r\n    print('dense from {} to {}'.format(t, post_z_flat_dim))\r\n    t = tf.layers.dense(inputs=t, units=post_z_flat_dim, ...)\r\n\r\nwhich gives the output\r\n> autoencoder: dense from Tensor(\"z/Merge:0\", shape=(?, 128), dtype=float32) to 65536\r\n\r\n> decoder: dense from Tensor(\"z_1:0\", shape=(?, 128), dtype=float32) to 65536\r\n\r\nThe only difference is, for the decoder t is a placeholder whereas for autoencoder it's the result of a sequence of operations (but still of shape [None, 128])\r\n\r\nRelevant code is below followed by the output.\r\n\r\n```python\r\nclass Model:\r\n    def __init__(self,\r\n                 usage, # Usage enum\r\n                 reuse=None, # instance of another model to reuse sess, graph and variables from\r\n                 ):\r\n\r\n        if reuse == None:\r\n            config = tf.ConfigProto()\r\n            config.gpu_options.allow_growth=True    \r\n            self.graph = tf.Graph()\r\n            self.sess = tf.Session(graph=self.graph, config=config)\r\n            root_scope = self.name\r\n        else:\r\n            self.graph = reuse.graph\r\n            self.sess = reuse.sess\r\n            root_scope = reuse.name\r\n\r\n        with self.graph.as_default():\r\n            with tf.variable_scope(root_scope, reuse=(reuse != None)):\r\n                \r\n                if usage >= Usage.autoencoder:\r\n                    self.x = tf.placeholder(tf.float32, (None,) + tuple(img_shape), name='x')\r\n                    t = self.x        \r\n                    print('> x', t.shape, t.name)\r\n\r\n                    # ENCODER\r\n                    for i, filter_depth in enumerate(filters):\r\n                        name = 'encoder{}'.format(i)\r\n                        with tf.variable_scope(name):         \r\n                            t = tf.layers.conv2d(...)                \r\n                            if usage >= Usage.trainer: t = tf.layers.dropout(inputs=t, rate=self.dropout_conv_amt_T, training=True)\r\n                            print('> conv', t.shape, t.name)\r\n                            \r\n                 \r\n                # Z    \r\n                if usage >= Usage.autoencoder:\r\n                    t = tf.contrib.layers.flatten(t)\r\n                    print('> pre z flat', t.shape, t.name)\r\n\r\n                    if usage >= Usage.trainer:\r\n                        t = tf.layers.dropout(inputs=t, rate=self.dropout_fc_amt_T, training=True)\r\n                    \r\n                    def vae_z(t):\r\n                        self.mu = tf.layers.dense(inputs=t, units=z_dim, activation=None)\r\n                        self.log_sigma = tf.layers.dense(inputs=t, units=z_dim, activation=None)                        \r\n                        epsilon = tf.random_normal(tf.shape(self.log_sigma), name='epsilon')\r\n                        t = self.mu + epsilon * tf.exp(self.log_sigma)\r\n                        return t\r\n                    \r\n                    def nonvae_z(t):\r\n                        return tf.layers.dense(inputs=t, units=z_dim, activation=None)\r\n                    \r\n                    t = tf.cond(self.use_vae_T, lambda: vae_z(t), lambda: nonvae_z(t), name='z')\r\n                    self.z = t\r\n                else:\r\n                    t = tf.placeholder(tf.float32, [None, z_dim], name='z')\r\n                    self.z = t\r\n\r\n                print('> z', t.shape, t.name)\r\n                \r\n                # TODO is there a better way to calculate desired flat shape post z?\r\n                post_z_img_dim = self.img_shape[0] // (2**len(filters))\r\n                post_z_img_shape = [-1, post_z_img_dim, post_z_img_dim, filters[-1]]\r\n                post_z_flat_dim = filters[-1] * post_z_img_dim * post_z_img_dim\r\n                print('dense from {} to {}'.format(t, post_z_flat_dim))\r\n                t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)\r\n                print('> post z flat', t.shape, t.name)\r\n                \r\n                t = tf.reshape(t, shape=tf.constant(post_z_img_shape))\r\n                print('> post z img', t.shape, t.name)\r\n\r\n\r\n                # DECODER   \r\n\r\n```\r\n\r\nOutput when I use the above. Note the scopenames and shapes in decoder1 (which is created without variable sharing), and it's identical to autoencoder. But decoder2 (which tries to share variables with autoencoder) fails.\r\n\r\n    autoencoder = Model(usage=Usage.autoencoder, reuse=None)\r\n    --------------------------------------------------------------------------------\r\n    __main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\r\n    > x (?, 256, 256, 3) x:0\r\n    > conv (?, 128, 128, 64) encoder0/conv/Relu:0\r\n    > conv (?, 64, 64, 128) encoder1/conv/Relu:0\r\n    > conv (?, 32, 32, 128) encoder2/conv/Relu:0\r\n    > conv (?, 16, 16, 256) encoder3/conv/Relu:0\r\n    > pre z flat (?, 65536) Flatten/Reshape:0\r\n    > z (?, 128) z/Merge:0\r\n    dense from Tensor(\"z/Merge:0\", shape=(?, 128), dtype=float32) to 65536\r\n    > post z flat (?, 65536) dense/Relu:0\r\n    > post z img (?, 16, 16, 256) Reshape:0\r\n    > deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0\r\n    > deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0\r\n    > deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0\r\n    > deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0\r\n    > y (?, 256, 256, 3) final/conv/Sigmoid:0\r\n\r\n\r\n    \r\n    decoder1 = Model(usage=Usage.decoder, reuse=None)\r\n    --------------------------------------------------------------------------------\r\n    __main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\r\n    > z (?, 128) z:0\r\n    dense from Tensor(\"z:0\", shape=(?, 128), dtype=float32) to 65536\r\n    > post z flat (?, 65536) dense/Relu:0\r\n    > post z img (?, 16, 16, 256) Reshape:0\r\n    > deconv (?, 32, 32, 256) decoder0/up_sampling2d_1/ResizeNearestNeighbor:0\r\n    > deconv (?, 64, 64, 128) decoder1/up_sampling2d_2/ResizeNearestNeighbor:0\r\n    > deconv (?, 128, 128, 128) decoder2/up_sampling2d_3/ResizeNearestNeighbor:0\r\n    > deconv (?, 256, 256, 64) decoder3/up_sampling2d_4/ResizeNearestNeighbor:0\r\n    > y (?, 256, 256, 3) final/conv/Sigmoid:0\r\n    \r\n\r\n\r\n    decoder2 =Model(usage=Usage.decoder, reuse=autoencoder)\r\n    --------------------------------------------------------------------------------\r\n    __main__.Model.init  (256, 256, 3) [64, 128, 128, 256] 3\r\n    > z (?, 128) z_1:0\r\n    dense from Tensor(\"z_1:0\", shape=(?, 128), dtype=float32) to 65536\r\n    Traceback (most recent call last):\r\n    \r\n      File \"<ipython-input-4-6f2915008bb4>\", line 1, in <module>\r\n        decoder2 =Model(usage=Usage.decoder, reuse=autoencoder)\r\n    \r\n      File \"/home/memo/Dropbox/research/py/apps/webcam-pix2pix-tensorflow/models/cnnvae.py\", line 152, in __init__\r\n        t = tf.layers.dense(inputs=t, units=post_z_flat_dim, activation=activation_fc)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 218, in dense\r\n        return layer.apply(inputs)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 320, in apply\r\n        return self.__call__(inputs, **kwargs)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 286, in __call__\r\n        self.build(input_shapes[0])\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 123, in build\r\n        trainable=True)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1049, in get_variable\r\n        use_resource=use_resource, custom_getter=custom_getter)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 948, in get_variable\r\n        use_resource=use_resource, custom_getter=custom_getter)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 349, in get_variable\r\n        validate_shape=validate_shape, use_resource=use_resource)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 275, in variable_getter\r\n        variable_getter=functools.partial(getter, **kwargs))\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 228, in _add_variable\r\n        trainable=trainable and self.trainable)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 341, in _true_getter\r\n        use_resource=use_resource)\r\n    \r\n      File \"/home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 658, in _get_single_variable\r\n        found_var.get_shape()))\r\n    \r\n    ValueError: Trying to share variable dense/kernel, but specified shape (128, 65536) and found shape (65536, 128).\r\n\r\n\r\n\r\n\r\n", "comments": ["Sorry, issue was a scope naming issue (skipping the encoder meant that the auto numbering of dense layers changed. Giving them all explicit names addressed the issue. So not a bug, user error). "]}, {"number": 12711, "title": "Update CUB version in the cmake build.", "body": "", "comments": ["yeah, I think a push will be better, there are more fixes we want externally."]}, {"number": 12710, "title": "MklReshapeOp fix for TensorShape int64 support", "body": "MklReshapeOp fix for TensorShape int64 data type.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 12709, "title": "Fix for the IOU metric", "body": "Previously the IOU metric under estimated the true value as described [here](https://github.com/tensorflow/tensorflow/issues/12294)\r\n\r\nThis pull request proposes a fix following the suggestion of @aquariusjay. The score is now only taking into account classes that actually appear in the sample. \r\n\r\nThe new code works like this:\r\na) We compute how many classes appear in the sample.\r\nb) We compute the score for every class. If the denominator is 0 then the nominator will be 0 as well! To avoid a zero division we set the denominator to 1 so the result will be 0/1=0. \r\nc) Instead of taking the mean over all scores we sum up all scores and divide by the previously computed number of classes in the sample. \r\n", "comments": ["Can one of the admins verify this patch?", "@Sylvus, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @drpngx and @taehoonlee to be potential reviewers.", "I wanted to add a test but couldn't find the correct unit test file. Maybe it is missing? I posted on SO but got no reply, maybe somebody want to give me some advice. The SO post is here:\r\n\r\nhttps://stackoverflow.com/questions/45937936/writing-tests-for-tensorflow\r\n", "The correct test file is tensorflow/python/kernel_tests/metrics_test.py\r\n", "@Sylvus any luck on adding the test?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Sorry for the delay. Had to finish an other project.\r\n\r\nYes, I found the test files and added 3 tests on my own. One is ensuring that the behaviour in the normal case stays the same, one is a minimal example of the problem and the third one is a proper test case showing the new behaviour. Let me know if you would like to see any changes. \r\n\r\nAlso I accidentally pushed from my AWS machine (without a user email) so I am not sure if the cla/google check will pass. I tried to rebase it but I am not sure that helps.\r\n\r\nThank you!", "You can change the email on git with an option to commit\n\nOn Sep 18, 2017 9:21 AM, \"Sylvus\" <notifications@github.com> wrote:\n\n> Sorry for the delay. Had to finish an other project.\n>\n> Yes, I found the test files and added 3 tests on my own. One is just\n> ensuring that the behaviour in the normal case stays the same, one is a\n> minimal example of the problem and the third one is a proper test case\n> showing the new behaviour. Let me know if you would like to see any changes.\n>\n> Also I accidentally pushed from my AWS machine (without a user email) so I\n> am not sure if the cla/google check will pass. I tried to rebase it but I\n> am not sure that helps.\n>\n> Thank you!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12709#issuecomment-330274473>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbSNVWlIZei_DHn8Do8jukRezl0Y4ks5sjphvgaJpZM4PH2cW>\n> .\n>\n", "Jenkins, test this please.\n\nOn Sep 18, 2017 9:23 AM, \"Patrick Nguyen\" <drpng@google.com> wrote:\n\n> You can change the email on git with an option to commit\n>\n> On Sep 18, 2017 9:21 AM, \"Sylvus\" <notifications@github.com> wrote:\n>\n>> Sorry for the delay. Had to finish an other project.\n>>\n>> Yes, I found the test files and added 3 tests on my own. One is just\n>> ensuring that the behaviour in the normal case stays the same, one is a\n>> minimal example of the problem and the third one is a proper test case\n>> showing the new behaviour. Let me know if you would like to see any changes.\n>>\n>> Also I accidentally pushed from my AWS machine (without a user email) so\n>> I am not sure if the cla/google check will pass. I tried to rebase it but I\n>> am not sure that helps.\n>>\n>> Thank you!\n>>\n>> \u2014\n>> You are receiving this because you commented.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/12709#issuecomment-330274473>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AT_SbSNVWlIZei_DHn8Do8jukRezl0Y4ks5sjphvgaJpZM4PH2cW>\n>> .\n>>\n>\n", "Here is the error;\r\n\r\n```\r\nFAIL: //tensorflow/contrib/metrics:metric_ops_test (shard 2 of 3) (see /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/contrib/metrics/metric_ops_test/shard_2_of_3/test.log).\r\nINFO: From Testing //tensorflow/contrib/metrics:metric_ops_test (shard 2 of 3):\r\n==================== Test output for //tensorflow/contrib/metrics:metric_ops_test (shard 2 of 3):\r\n.2017-09-18 16:50:49.363948: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n...................................F.............../var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/metrics/metric_ops_test.runfiles/org_tensorflow/tensorflow/contrib/metrics/python/ops/metric_ops_test.py:4576: RuntimeWarning: Degrees of freedom <= 0 for slice\r\n  fweights=weights[:stride * (i + 1)])\r\n/usr/local/lib/python2.7/dist-packages/numpy/lib/function_base.py:2929: RuntimeWarning: divide by zero encountered in double_scalars\r\n  c *= 1. / np.float64(fact)\r\n/usr/local/lib/python2.7/dist-packages/numpy/lib/function_base.py:2929: RuntimeWarning: invalid value encountered in multiply\r\n  c *= 1. / np.float64(fact)\r\n............WARNING:tensorflow:From /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/metrics/metric_ops_test.runfiles/org_tensorflow/tensorflow/contrib/metrics/python/ops/metric_ops_test.py:2304: streaming_recall_at_k (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed after 2016-11-08.\r\nInstructions for updating:\r\nPlease use `streaming_sparse_recall_at_k`, and reshape labels from [batch_size] to [batch_size, 1].\r\n................................\r\n======================================================================\r\nFAIL: testMultipleUpdatesWithMissingClass (__main__.StreamingMeanIOUTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/metrics/metric_ops_test.runfiles/org_tensorflow/tensorflow/contrib/metrics/python/ops/metric_ops_test.py\", line 4982, in testMultipleUpdatesWithMissingClass\r\n    self.assertAlmostEqual(desired_output, miou.eval())\r\nAssertionError: 0.27777777777777773 != 0.41666669 within 7 places\r\n```", "Thank you! Sent a fix. I wasn't aware of the fact that I also needed to change the test file for the streaming metrics iou which was located somewhere else. I added my test cases there as well. This seems like a lot of code duplication!\r\n\r\nLet me try: Jenkins, test this please.", "Jenkins, test this please.", "All good. Can somebody review and let me know how to change the already pushed commits to get approved by cla/google (or just override the tag)?", "You need to change commits: `9207da9e9f9653c84481eba6853f20785c2b58a4`, `9207da9e9f9653c84481eba6853f20785c2b58a4`, and `8921c7b58f17045b58fb8e5a3700c9deff88bcfe` which are committed from the wrong author. Assuming that they are yours, you can simply squash them. You can use `git rebase --interactive`, then mark these commits as `fixup` or `squash` to merge them with previous commits.", "CLAs look good, thanks!\n\n<!-- ok -->", "OK, now it looks like there are conflicts. Could you pull rebase and push again?", "Can one of the admins verify this patch?", "Yep, my squash did not work because of the merge. I rebased and pushed again.", "Great, CLA issue solved. Did you add the test as @alextp requested?", "Yes, added 3 tests in total. Happy to delete one or two again if they are not needed. \r\n1. Test verifies that my change does not break the normal behaviour (when the class is present)\r\n2. Test is a minimal example that fails without my changes\r\n3. Test is a normal use case that would also fail without my changes.\r\n", "Thanks! @alextp PTAL?", "@alextp ping", "Jenkins, test this please."]}, {"number": 12708, "title": "Remove initialize_all_tables", "body": "Finishing TODO(yleon): Remove this function.", "comments": ["Can one of the admins verify this patch?", "@alanyee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ysuematsu, @tensorflower-gardener and @keveman to be potential reviewers.", "Thank you for doing some cleanup work. We're very careful with removing functions from the API though. In this case, even though we're technically allowed to according to our deprecation and versioning policy, we'll err on the side of caution to avoid breaking users.\r\n\r\nIf you'd like, you can remove the TODO, and set the date in the deprecation notice to None instead. Best to open a new PR for that."]}, {"number": 12707, "title": "Input function tutorial doesn't deliver reported performance.", "body": "Running the example explained [here](https://www.tensorflow.org/get_started/input_fn), results in a reported loss 100 times bigger than what's reported in the tutotrial. It also doesn't converge, and it seems to be going up and down.\r\n\r\nThe expected performance is:\r\n\r\n```\r\nINFO:tensorflow:Step 1: loss = 483.179\r\nINFO:tensorflow:Step 101: loss = 81.2072\r\nINFO:tensorflow:Step 201: loss = 72.4354\r\n...\r\nINFO:tensorflow:Step 1801: loss = 33.4454\r\nINFO:tensorflow:Step 1901: loss = 32.3397\r\nINFO:tensorflow:Step 2001: loss = 32.0053\r\nINFO:tensorflow:Step 4801: loss = 27.2791\r\nINFO:tensorflow:Step 4901: loss = 27.2251\r\nINFO:tensorflow:Saving checkpoints for 5000 into /tmp/boston_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 27.1674.\r\n```\r\n\r\nWhereas, here is what I get when I run it:\r\n\r\n```\r\n$ python2 boston.py \r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_model_dir': '/tmp/boston_model', '_save_summary_steps': 100}\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n2017-08-30 16:38:36.378259: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-30 16:38:36.378318: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-30 16:38:36.378321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-30 16:38:36.378325: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-30 16:38:36.378328: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nINFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-5000\r\nINFO:tensorflow:Saving checkpoints for 5001 into /tmp/boston_model/model.ckpt.\r\nINFO:tensorflow:loss = 3897.4, step = 5001\r\nINFO:tensorflow:global_step/sec: 476.014\r\nINFO:tensorflow:loss = 2826.95, step = 5101 (0.210 sec)\r\nINFO:tensorflow:global_step/sec: 485.788\r\nINFO:tensorflow:loss = 4138.43, step = 5201 (0.206 sec)\r\nINFO:tensorflow:global_step/sec: 475.238\r\nINFO:tensorflow:loss = 3182.46, step = 5301 (0.210 sec)\r\nINFO:tensorflow:global_step/sec: 477.947\r\nINFO:tensorflow:loss = 4233.69, step = 5401 (0.209 sec)\r\nINFO:tensorflow:global_step/sec: 484.752\r\nINFO:tensorflow:loss = 3593.6, step = 5501 (0.206 sec)\r\nINFO:tensorflow:global_step/sec: 487.786\r\nINFO:tensorflow:loss = 3316.72, step = 5601 (0.205 sec)\r\nINFO:tensorflow:global_step/sec: 493.849\r\nINFO:tensorflow:loss = 3041.69, step = 5701 (0.203 sec)\r\nINFO:tensorflow:global_step/sec: 462.593\r\nINFO:tensorflow:loss = 2753.05, step = 5801 (0.216 sec)\r\nINFO:tensorflow:global_step/sec: 468.178\r\nINFO:tensorflow:loss = 4609.25, step = 5901 (0.214 sec)\r\nINFO:tensorflow:global_step/sec: 505.747\r\nINFO:tensorflow:loss = 5840.77, step = 6001 (0.198 sec)\r\nINFO:tensorflow:global_step/sec: 484.518\r\nINFO:tensorflow:loss = 4149.44, step = 6101 (0.206 sec)\r\nINFO:tensorflow:global_step/sec: 516.622\r\nINFO:tensorflow:loss = 2485.26, step = 6201 (0.193 sec)\r\nINFO:tensorflow:global_step/sec: 480.998\r\nINFO:tensorflow:loss = 3849.6, step = 6301 (0.208 sec)\r\nINFO:tensorflow:global_step/sec: 480.866\r\nINFO:tensorflow:loss = 3684.53, step = 6401 (0.208 sec)\r\nINFO:tensorflow:global_step/sec: 468.132\r\nINFO:tensorflow:loss = 2943.32, step = 6501 (0.214 sec)\r\nINFO:tensorflow:global_step/sec: 463.973\r\nINFO:tensorflow:loss = 3135.92, step = 6601 (0.216 sec)\r\nINFO:tensorflow:global_step/sec: 464.991\r\nINFO:tensorflow:loss = 4463.47, step = 6701 (0.215 sec)\r\nINFO:tensorflow:global_step/sec: 386.855\r\nINFO:tensorflow:loss = 3488.36, step = 6801 (0.259 sec)\r\nINFO:tensorflow:global_step/sec: 402.492\r\nINFO:tensorflow:loss = 3472.3, step = 6901 (0.248 sec)\r\nINFO:tensorflow:global_step/sec: 468.325\r\nINFO:tensorflow:loss = 6648.4, step = 7001 (0.213 sec)\r\nINFO:tensorflow:global_step/sec: 482.791\r\nINFO:tensorflow:loss = 5161.27, step = 7101 (0.207 sec)\r\nINFO:tensorflow:global_step/sec: 468.145\r\nINFO:tensorflow:loss = 6147.99, step = 7201 (0.214 sec)\r\nINFO:tensorflow:global_step/sec: 465.968\r\nINFO:tensorflow:loss = 5066.78, step = 7301 (0.215 sec)\r\nINFO:tensorflow:global_step/sec: 464.296\r\nINFO:tensorflow:loss = 3810.97, step = 7401 (0.215 sec)\r\nINFO:tensorflow:global_step/sec: 464.02\r\nINFO:tensorflow:loss = 3235.41, step = 7501 (0.215 sec)\r\nINFO:tensorflow:global_step/sec: 465.755\r\nINFO:tensorflow:loss = 4455.11, step = 7601 (0.215 sec)\r\nINFO:tensorflow:global_step/sec: 445.682\r\nINFO:tensorflow:loss = 2335.67, step = 7701 (0.224 sec)\r\nINFO:tensorflow:global_step/sec: 446.977\r\nINFO:tensorflow:loss = 4264.51, step = 7801 (0.224 sec)\r\nINFO:tensorflow:global_step/sec: 473.768\r\nINFO:tensorflow:loss = 2708.95, step = 7901 (0.211 sec)\r\nINFO:tensorflow:global_step/sec: 470.603\r\nINFO:tensorflow:loss = 4107.13, step = 8001 (0.212 sec)\r\nINFO:tensorflow:global_step/sec: 494.138\r\nINFO:tensorflow:loss = 2911.78, step = 8101 (0.202 sec)\r\nINFO:tensorflow:global_step/sec: 504.535\r\nINFO:tensorflow:loss = 7470.54, step = 8201 (0.198 sec)\r\nINFO:tensorflow:global_step/sec: 516.137\r\nINFO:tensorflow:loss = 2581.56, step = 8301 (0.195 sec)\r\nINFO:tensorflow:global_step/sec: 503.267\r\nINFO:tensorflow:loss = 4270.98, step = 8401 (0.198 sec)\r\nINFO:tensorflow:global_step/sec: 528.762\r\nINFO:tensorflow:loss = 4186.45, step = 8501 (0.189 sec)\r\nINFO:tensorflow:global_step/sec: 509.902\r\nINFO:tensorflow:loss = 5767.94, step = 8601 (0.196 sec)\r\nINFO:tensorflow:global_step/sec: 510.832\r\nINFO:tensorflow:loss = 3960.38, step = 8701 (0.196 sec)\r\nINFO:tensorflow:global_step/sec: 525.036\r\nINFO:tensorflow:loss = 4677.09, step = 8801 (0.190 sec)\r\nINFO:tensorflow:global_step/sec: 522.431\r\nINFO:tensorflow:loss = 1912.38, step = 8901 (0.191 sec)\r\nINFO:tensorflow:global_step/sec: 521.464\r\nINFO:tensorflow:loss = 2646.11, step = 9001 (0.192 sec)\r\nINFO:tensorflow:global_step/sec: 523.779\r\nINFO:tensorflow:loss = 2803.72, step = 9101 (0.191 sec)\r\nINFO:tensorflow:global_step/sec: 517.767\r\nINFO:tensorflow:loss = 2995.35, step = 9201 (0.193 sec)\r\nINFO:tensorflow:global_step/sec: 467.628\r\nINFO:tensorflow:loss = 3767.14, step = 9301 (0.214 sec)\r\nINFO:tensorflow:global_step/sec: 463.383\r\nINFO:tensorflow:loss = 3825.25, step = 9401 (0.215 sec)\r\nINFO:tensorflow:global_step/sec: 554.173\r\nINFO:tensorflow:loss = 3352.59, step = 9501 (0.180 sec)\r\nINFO:tensorflow:global_step/sec: 528.989\r\nINFO:tensorflow:loss = 4678.92, step = 9601 (0.189 sec)\r\nINFO:tensorflow:global_step/sec: 490.677\r\nINFO:tensorflow:loss = 2191.4, step = 9701 (0.204 sec)\r\nINFO:tensorflow:global_step/sec: 463.207\r\nINFO:tensorflow:loss = 5998.51, step = 9801 (0.216 sec)\r\nINFO:tensorflow:global_step/sec: 461.608\r\nINFO:tensorflow:loss = 3440.55, step = 9901 (0.217 sec)\r\nINFO:tensorflow:Saving checkpoints for 10000 into /tmp/boston_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 4595.88.\r\nINFO:tensorflow:Starting evaluation at 2017-08-30-14:38:47\r\nINFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-10000\r\nINFO:tensorflow:Finished evaluation at 2017-08-30-14:38:47\r\nINFO:tensorflow:Saving dict for global step 10000: average_loss = 16.7064, global_step = 10000, loss = 1670.64\r\nLoss: 1670.643555\r\nINFO:tensorflow:Restoring parameters from /tmp/boston_model/model.ckpt-10000\r\nPredictions: [array([ 34.28430557], dtype=float32), array([ 18.48677063], dtype=float32), array([ 24.49580383], dtype=float32), array([ 34.83223343], dtype=float32), array([ 15.90157318], dtype=float32), array([ 20.3986187], dtype=float32)]\r\n```\r\n\r\nEnvironment:\r\n\r\n== cat /etc/issue ===============================================\r\nLinux adrin-leni.ancud.de 4.12.7-1-ARCH #1 SMP PREEMPT Sun Aug 13 08:17:09 CEST 2017 x86_64 GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 7.2.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux adrin-leni.ancud.de 4.12.7-1-ARCH #1 SMP PREEMPT Sun Aug 13 08:17:09 CEST 2017 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.5)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Asked on [stackoverflow](https://stackoverflow.com/questions/45975463/tensorflow-example-building-input-functions-with-tf-estimator-doesnt-deliver). No answer there."]}, {"number": 12706, "title": "ERROR:tensorflow:Only one valid folder of images found at tf_files2/images - multiple classes are needed for classification.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Can anyone tell what all are the conditions for validity of images in the subfolders? is there any size , format etc , we need to check?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12705, "title": "Verbs fix: Removed Dependency on Duplicate Recv Flag", "body": "This PR is a response for #11825 \r\n\r\nI've added a new Queue and Table for the **RdmaTensorBuffer**, where the relevant information from the send-recv waiter table is stored in case the actual rdma-write does not occur (If the allocated buffer is too small, or busy). When we get a RDMA_MESSAGE_BUFFER_RESPONSE, or RDMA_MESSAGE_BUFFER_IDLE, we check the new queue/table instead of checking the waiter table. (This happens in the new ReSendNextItem() function).\r\n\r\nAlso added a new function that generates the callback lambda function, to avoid code redundancy.\r\nThe result is that the code is less clean- I am open to suggestions on making it more tidy.\r\n\r\n@junshi15 @shamoya ", "comments": ["Can one of the admins verify this patch?", "+ @poxvoculi ", "@yanivbl6, thanks for the quick response. It looks mostly good. I have a comment regarding the reference count.", "Thanks @junshi15,\r\nI've added a constructor / destructor to the ReItem struct, to take care of the referencing and the de-referencing.  Also removed the unused field (Status).", "It looks good to me. @shamoya any comment?", "@shamoya participated in the code writing, and reviewed it prior to the PR.", "Jenkins, test this please.", "@tensorflow-jenkins test this please"]}]