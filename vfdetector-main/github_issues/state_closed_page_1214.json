[{"number": 16746, "title": "Grammatical error fixed", "body": "", "comments": []}, {"number": 16745, "title": "Consider supporting Microsoft Quantum", "body": "Please consider supporting Microsoft Quantum as a Runtime just like GPUs and TPUs.\r\n\r\nHere is the same issue on Microsoft Quantum's repo\r\nhttps://github.com/Microsoft/Quantum/issues/30\r\n\r\nIt would be great to have an XLA device/target for Microsoft Quantum", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler, yes this is still an issue", "We don't have plans to support Quantum yet. Thanks for your feedback."]}, {"number": 16744, "title": "Fix the Windows GPU build #2", "body": "Tested that this fixes the build:\r\nhttps://ci.tensorflow.org/view/Experimental/job/exp-win-gpu/5/", "comments": []}, {"number": 16743, "title": "MonitoredSession after_run hook returning empty SessionRunValues results", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: Arch Linux 4.14.15-1\r\n- **TensorFlow installed from**: source (master)\r\n- **TensorFlow version**: v1.5.0-2123-g66105a6144\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: 9.1.85/7.0.5\r\n- **GPU model and memory**: Nvidia GTX 1080 8GB\r\n- **Exact command to reproduce**: `python test.py`\r\n\r\n### Describe the problem\r\nWhen running a `MonitoredSession` with `after_run` hooks, the result passed to `run_values` is None, when there should be output.\r\n\r\n### Source code / logs\r\n`test.py`:\r\n```python\r\nimport tensorflow as tf\r\n\r\none = tf.Variable(1)\r\n\r\nclass TestHook(tf.train.SessionRunHook):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.result = None\r\n\r\n    def after_run(self, run_context, run_values):\r\n        # run_values.results should be 1 here\r\n        self.result = run_values.results\r\n\r\nhook = TestHook()\r\nwith tf.train.MonitoredSession(hooks=[hook]) as sess:\r\n    print('Eval result: {}'.format(\r\n        one.eval(session=sess)))\r\n    print('Hook result: {}'.format(\r\n        hook.result))\r\n```\r\n\r\nExpected output:\r\n```\r\nEval result: 1\r\nHook result: 1\r\n```\r\n\r\nActual output:\r\n```\r\nEval result: 1\r\nHook result: None\r\n```\r\n\r\nI've changed this in my fork by replacing https://github.com/tensorflow/tensorflow/blob/3fb47614c4c3f29d59085c2eb6ad9a4f9adfa98e/tensorflow/python/training/monitored_session.py#L1176 with `results=outputs['caller'],`. However, this breaks training with an `Estimator` wrapping a `MonitoredSession`. If I'm misinterpreting the usage of the `after_run` hook please let me know!", "comments": ["You used hooks it in a wrong way. Please read the docs: https://www.tensorflow.org/versions/master/api_docs/python/tf/train/SessionRunHook.", "Thanks for the clarification. After looking at the examples in `basic_session_run_hooks` I see what the proper usage is."]}, {"number": 16742, "title": "Allow passing Saver write_version to 'evaluation_once' and 'evaluatio\u2026", "body": "\u2026n_loop'\r\n\r\nThanks @tedhtchang for the commit https://github.com/tensorflow/tensorflow/commit/2a16133061ba3f8fa60c0338cd629f2211f9b17d to add checkpoint file prefix check with default to **SaverDef.V2**.\r\n\r\nHowever there are some [pre-trained slim models](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models) which stored in **SaverDef.V1** format. This will broken [eval_image_classifier.py](https://github.com/tensorflow/models/blob/master/research/slim/eval_image_classifier.py) for these models since there is no way to choose in [slim/evaluation.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/evaluation.py).\r\n\r\nSo a parameter **write_version** is added for **evaluation_once** and **evaluation_loop** which allows programs like [eval_image_classifier.py](https://github.com/tensorflow/models/blob/master/research/slim/eval_image_classifier.py) could tell which format the checkpoint uses.\r\n \r\n\r\n\r\n\r\n", "comments": ["Nagging Assignee @sguada: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for the suggestion, I'll update the branch with evalutation_test.py.", "In current master branch, there is no prefix check as described in saver.py, so it's OK now to eval pre-trained slim models.\r\n"]}, {"number": 16741, "title": "Making the for_canonicalization_test only for py2 and 3", "body": "http://ci.tensorflow.org/view/Release/job/release-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/239/consoleFull", "comments": []}, {"number": 16740, "title": "error in code ", "body": "    W_0 = utils.weight_variable([FLAGS.z_dim, 64 * GEN_DIMENSION / 2 * IMAGE_SIZE / 16 * IMAGE_SIZE / 16],\r\nNameError: name 'utils' is not defined\r\ncan you help me please ", "comments": ["I believe the source of that function is here: https://github.com/shekkizh/TensorflowProjects/blob/7bc5267da1bf4cf4a05cb1bac8bfd8f284728067/TensorflowUtils.py#L63\r\n\r\nSince this does not appear to be a bug in or feature request for TensorFlow, I'm going to close this issue. If you continue to have difficulties, I'd recommend asking on Stack Overflow or opening an issue on the repository that contains that problematic line of code."]}, {"number": 16739, "title": "Fixing the cuda and cudnn versions in 1.5 docs. (#16702)", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16738, "title": "Fix logging format error in retrain.py", "body": "This fix fixes the logging format error in `tensorflow/examples/image_retraining/retrain.py`.\r\n\r\nThis fix fixes #16735.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@caisq Thanks for the review. The PR has been updated."]}, {"number": 16737, "title": "Discrepancies between GPU and CPU in floating-point operations", "body": "```\r\nbs = 32\r\ndim = 1024\r\n\r\ntf.reset_default_graph()\r\nwith tf.device(\"/cpu:0\"):\r\n  probs = tf.where(tf.greater(tf.random_normal((bs, dim)), 0.), tf.ones((bs, dim)), tf.zeros((bs, dim)))\r\n  print(probs)\r\n  logits = tf.log(probs / (1e-10 + 1 - probs))\r\n  s = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n  print(s.run([logits]))\r\n\r\ntf.reset_default_graph()\r\nwith tf.device(\"/gpu:0\"):\r\n  probs = tf.where(tf.greater(tf.random_normal((bs, dim)), 0.), tf.ones((bs, dim)), tf.zeros((bs, dim)))\r\n  print(probs)\r\n  logits = tf.log(probs / (1e-10 + 1 - probs))\r\n  s = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n  print(s.run([logits]))\r\n```\r\n\r\nRunning this graph on GPU results in positive infinities, whereas on CPU these tensor entries evaluate to ~88.72284. I could not quite figure out which operation is responsible for the difference. In both cases TensorFlow reports `probs` as `float32`. The difference does not occur when replacing `probs` with a `tf.ones` tensor in `float32` format.", "comments": ["Eigen's SSE implementation of log: https://bitbucket.org/eigen/eigen/src/2355b229ea4c2876f490e726526cdcd8a63c7f54/Eigen/src/Core/arch/SSE/MathFunctions.h?at=default&fileviewer=file-view-default#MathFunctions.h-11:13 \r\nIt comes from http://gruntthepeon.free.fr/ssemath/ which says log(inf)=88.72284 (i.e., log(inf) = log(max_float)."]}, {"number": 16736, "title": "Add NLSTM RNN cell and the unit tests", "body": "The PR implement the RNN cell in the following paper. https://arxiv.org/abs/1801.10308", "comments": ["@drpngx @lukaszkaiser PTAL; is this something that we can maintain?", "It looks like a very self-contained code, so it could be fine for contrib. I'm not sure it'll be too useful, but maybe that's another question.", "@lukaszkaiser i'd need someone to take ownership internally and ensure it is maintained, review code to it, etc.  let me know if there's someone on the team who can do that.", "I'm not sure, I'm a bit overwhelmed with other stuff right now so I'd not subscribe to it. Maybe Erich Elsen?", "@ekelsen is that something you would sign up for?", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I haven't been able to find an internal maintainer for this code. Without\none, we can't accept it.\n\nOn Fri, Mar 2, 2018, 11:57 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Assignee @ebrevdo <https://github.com/ebrevdo>: It has been 14\n> days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16736#issuecomment-370128256>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8CtrrlPoW20QqpRsMCwfy2Vbt7lks5takx0gaJpZM4R4XVx>\n> .\n>\n", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 16735, "title": "incorrect logging formatting used in tensorflow / examples / image_retraining / retrain.py, causes error", "body": "In tensorflow -> examples -> image_retraining -> retrain.py, currently lines 347 / 348 look like this:\r\n\r\n```\r\ntf.logging.info('Successfully downloaded', filename, statinfo.st_size,\r\n                    'bytes.')\r\n```\r\n\r\nThis understandably causes an error since this function accepts strings and it is being fed an instance of statinfo.st_size which does not seem to be a string.  On my machine at least (TensorFlow 1.5, Windows 10) this causes the following error in function maybe_download_and_extract:\r\n\r\n`TypeError: not all arguments converted during string formatting`\r\n\r\nHere is a screenshot if that helps:\r\n\r\n![error](https://user-images.githubusercontent.com/5672876/35772391-74f4433c-08f2-11e8-83d2-084605c14844.png)\r\n\r\nThe line numbers are slightly different in my screenshot because I moved a few lines around, but I can assure you the line above is causing the logging error.\r\n\r\nI would suggest changing this line to the following, or similar:\r\n\r\n`tf.logging.info('Successfully downloaded ' + str(filename) + ', statinfo.st_size = ' + str(statinfo.st_size) + ' bytes')`\r\n", "comments": ["Added #16738 for the fix."]}, {"number": 16734, "title": "Fix incorrect reference DOI number/link for GDR", "body": "This fix fixes the incorrect reference DOI number/link for GDR:\r\n`https://doi.org/10.1145/3123878.3123907` -> `https://doi.org/10.1145/3123878.3131975`.\r\n\r\nThe previous link (https://doi.org/10.1145/3123878.3123907) in the README.md does not work and returns 404. The new link (https://doi.org/10.1145/3123878.3131975) should be the correct one.\r\n\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks :)"]}, {"number": 16733, "title": "dupe (#14385)", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 16732, "title": "Delete device_functions.h include.", "body": "", "comments": []}, {"number": 16731, "title": "Fixed a couple of typos", "body": "Fixed a couple of typos", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I've signed", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 16730, "title": "fix tf.GIT_VERSION always 'unknown' on windows cmake build", "body": "fix tf.GIT_VERSION always 'unknown' on windows cmake build", "comments": ["@tensorflow-jenkins  please test this.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 16729, "title": "Fix the Windows GPU build", "body": "device_functions.h moved in CUDA 9.1, which breaks the Windows GPU build. It's not needed here.", "comments": ["Note that this is to the master branch, and we'll have to cherrypick."]}, {"number": 16728, "title": "Disable win io utils test for windows", "body": "", "comments": []}, {"number": 16727, "title": "Fix Python3 crazy SessionTest.testReentryWithCApi failure.", "body": "credit: skyewm", "comments": []}, {"number": 16726, "title": "Branch 184376425", "body": "", "comments": []}, {"number": 16725, "title": "python 2.7 unit test error repair on windows", "body": "python 2.7 unit test error repair on windows", "comments": []}, {"number": 16724, "title": "R1.6", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16723, "title": "Bug: Compile Tensorflow 1.5.0 Java from source failed on NVIDIA Jetson TX2 with error \"'@bazel_tools//tools/jdk:singlejar' must produce a single file\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04 aarch64\r\n- **TensorFlow installed from (source or binary)**:\r\nsource on branch r1.5\r\n- **TensorFlow version (use command below)**:\r\nv1.5.0-1934-g9e7ce91 1.5.0\r\n- **Python version**:\r\nPython 3.5\r\n- **Bazel version (if compiling from source)**:\r\n0.9.0 and 0.10.0 (both tried with clean installation)\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0, cuDNN 7.0\r\n- **GPU model and memory**:\r\nNVIDIA Tegra X2 major (Pascal\u2122 architecture) 8G\r\n- **JDK Version**:\r\n\r\n```\r\nroot@tegra-ubuntu:/usr/src# java -version\r\nopenjdk version \"1.8.0_151\"\r\nOpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12)\r\nOpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)\r\n\r\nroot@tegra-ubuntu:/usr/src# javac -version\r\njavac 1.8.0_151\r\n```\r\n- **Exact command to reproduce**:\r\n\r\n```shell\r\nroot@tegra-ubuntu:/usr/src/tensorflow# ./configure\r\nExtracting Bazel installation...\r\nYou have bazel 0.9.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: \r\nHadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: \r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: \r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: \r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: \r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]6.2\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=tensorrt    \t# Build with TensorRT support.\r\nConfiguration finished\r\n```\r\nThe output from compile procedure is\r\n\r\n```shell\r\nroot@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\n.........................\r\nERROR: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/bazel_tools/tools/jdk/BUILD:193:17: in singlejar attribute of java_toolchain rule @bazel_tools//tools/jdk:toolchain: '@bazel_tools//tools/jdk:singlejar' must produce a single file\r\nERROR: Analysis of target '//tensorflow/java:tensorflow' failed; build aborted: Analysis of target '@bazel_tools//tools/jdk:toolchain' failed; build aborted\r\nINFO: Elapsed time: 57.138s\r\nFAILED: Build did NOT complete successfully (7 packages loaded)\r\n    currently loading: tensorflow\r\n```\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n```\r\nroot@tegra-ubuntu:/usr/src# tensorflow/tools/tf_env_collect.sh\r\nCollecting system information...\r\n2018-02-03 09:51:47.561112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:865] ARM64 does not support NUMA - returning NUMA node zero\r\n2018-02-03 09:51:47.561338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 0 with properties: \r\nname: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005\r\npciBusID: 0000:00:00.0\r\ntotalMemory: 7.66GiB freeMemory: 465.56MiB\r\n2018-02-03 09:51:47.561450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0\r\n2018-02-03 09:51:48.341988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 52 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)\r\nWrote environment to tf_env.txt. You can review the contents of that file.\r\nand use it to populate the fields in the github issue template.\r\n\r\ncat tf_env.txt\r\n```\r\n\r\n```\r\nroot@tegra-ubuntu:/usr/src# cat tf_env.txt\r\n\r\n== cat /etc/issue ===============================================\r\nLinux tegra-ubuntu 4.4.38-tegra #1 SMP PREEMPT Fri Dec 1 06:08:28 PST 2017 aarch64 aarch64 aarch64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux tegra-ubuntu 4.4.38-tegra #1 SMP PREEMPT Fri Dec 1 06:08:28 PST 2017 aarch64 aarch64 aarch64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.0)\r\nprotobuf (3.5.1)\r\ntensorflow (1.5.0)\r\ntensorflow-tensorboard (1.5.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.5.0\r\ntf.GIT_VERSION = v1.5.0-1934-g9e7ce91\r\ntf.COMPILER_VERSION = v1.5.0-1934-g9e7ce91\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntensorflow/tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart.so.9.0.252\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n```\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n```\r\nroot@tegra-ubuntu:/usr/src# python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.5.0-1934-g9e7ce91 1.5.0\r\n```\r\n\r\n### Describe the problem\r\n\r\nI have successfully compiled tensorflow python from source using same configure procedure as above with the following command:\r\n\r\n```\r\nroot@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n``` \r\nThere is no error in output and I can install the .whl file with pip.\r\n\r\nAfter that, I tried to compile the Java native library without the configure step (because I already configured it when compiling python version) using the command:\r\n\r\n```\r\nroot@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\n```\r\n\r\nIt failed with errors:\r\n\r\n```shell\r\nroot@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni\r\n.........................\r\nERROR: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/bazel_tools/tools/jdk/BUILD:193:17: in singlejar attribute of java_toolchain rule @bazel_tools//tools/jdk:toolchain: '@bazel_tools//tools/jdk:singlejar' must produce a single file\r\nERROR: Analysis of target '//tensorflow/java:tensorflow' failed; build aborted: Analysis of target '@bazel_tools//tools/jdk:toolchain' failed; build aborted\r\nINFO: Elapsed time: 57.138s\r\nFAILED: Build did NOT complete successfully (7 packages loaded)\r\n    currently loading: tensorflow\r\n```\r\n\r\n### Here are some ways I tried but faild with same error:\r\n\r\n1. Configure again (With same configure settings) and compile\r\n2. Remove the directory ~/.cache and do the step 1\r\n3. Remove the directory ~/.cache and tensorflow source directory, git clone tensorflow from r1.5 branch then do step 1\r\n4. Remove ~/.cache and the bazel binary, compile and install bazel from latest source release (0.10.0) without error. Then I use bazel 0.10.0 to compile tensorflow java. This produced same error.\r\n", "comments": ["@asimshankar can you comment or redirect? Thanks.", "This sounds like either an error with the JDK installation on the machine or an issue with bazel. The rule that is complaining is:\r\n\r\n```\r\nexternal/bazel_tools/tools/jdk/BUILD:193:17: in singlejar attribute of java_toolchain rule @bazel_tools//tools/jdk:toolchain: '@bazel_tools//tools/jdk:singlejar' must produce a single file\r\n```\r\n\r\nwhich isn't specific to TensorFlow, it's where bazel finds the JDK. This seems similar to https://github.com/bazelbuild/bazel/issues/3988 .\r\n\r\nCould you try https://docs.bazel.build/versions/master/tutorial/java.html to validate that the bazel and JDK configuration on your machine is fine? It should be something as simple as this:\r\n\r\n```sh\r\ngit clone https://github.com/bazelbuild/examples/\r\ncd examples/java-tutorial\r\nbazel build //:ProjectRunner\r\n```", "@asimshankar Same error\r\n\r\n```\r\nroot@tegra-ubuntu:~/examples/java-tutorial# bazel build //:ProjectRunner\r\n.................................\r\nERROR: /root/.cache/bazel/_bazel_root/9318dfd54d71a6b918f8033146aeac2a/external/bazel_tools/tools/jdk/BUILD:193:17: in singlejar attribute of java_toolchain rule @bazel_tools//tools/jdk:toolchain: '@bazel_tools//tools/jdk:singlejar' must produce a single file\r\nERROR: Analysis of target '//:ProjectRunner' failed; build aborted: Analysis of target '@bazel_tools//tools/jdk:toolchain' failed; build aborted\r\nINFO: Elapsed time: 5.553s\r\nFAILED: Build did NOT complete successfully (10 packages loaded)\r\n```", "@YanzheL, since this seems to be an issue with your bazel setup and not TensorFlow, I'm going to close this issue here.  Thanks!", "sudo apt-get install openjdk-8-jdk"]}, {"number": 16722, "title": "fix typo", "body": "fix typo", "comments": []}, {"number": 16721, "title": "Customized loss in keras", "body": "Dear all,\r\n\r\nI can run properly with the following code:\r\n### System Information ####\r\nHave I written custom code : As following\r\nOS Platform and Distribution: Linux Ubuntu16.04\r\nTensorFlow installed from : conda script\r\nTensorFlow version : '1.4.0'\r\nBazel version : N/A\r\nCUDA/cuDNN version : CUDA8.0, cudnn6.0\r\nGPU model and memory : 1070/8G\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python.keras.backend import categorical_crossentropy\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import  Input\r\n\r\nmnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\r\nimg_size_flat = 28*28\r\nbatch_size = 64\r\n\r\ndef gen(batch_size=32):\r\n    while True:\r\n        batch_data, batch_label = mnist_data.train.next_batch(batch_size)\r\n        yield batch_data, batch_label   \r\n\r\n\r\ninputs = Input(shape=(img_size_flat,))\r\nx = Dense(128, activation='relu')(inputs)  # fully-connected layer with 128 units and ReLU activation\r\nx = Dense(128, activation='relu')(x)\r\npreds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation\r\nmodel = Model(inputs=inputs, outputs=preds)\r\n\r\nmodel.compile(optimizer='rmsprop',\r\n               loss='categorical_crossentropy',\r\n               metrics=['accuracy'])\r\n\r\n\r\nmodel.fit_generator(gen(batch_size), steps_per_epoch=len(mnist_data.train.labels)//batch_size, epochs=2)\r\n```\r\n\r\nBut if I want to write loss function with my own code like:\r\n```\r\npreds_softmax = tf.nn.softmax(preds)\r\nstep1 = tf.cast(y_true, tf.float32) * tf.log(preds_softmax)\r\nstep2 = -tf.reduce_sum(step1, reduction_indices=[1])\r\nloss = tf.reduce_mean(step2)       # loss\r\n```\r\n\r\nIs something like the following code on tensorflow?\r\n```\r\ninputs = tf.placeholder(tf.float32, shape=(None, 784))\r\nx = Dense(128, activation='relu')(inputs) # fully-connected layer with 128 units and ReLU activation\r\nx = Dense(128, activation='relu')(x)\r\npreds = Dense(10, activation='softmax')(x) # output layer with 10 units and a softmax activation\r\n\r\ny_true = tf.placeholder(tf.float32, shape=(None, 10))\r\n```\r\n\r\nHow can I do based on above code(part I)? Thanks for any help!!\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler\r\nOk, I've updated it!", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I already post it on\r\nhttps://stackoverflow.com/questions/48654851/customized-loss-in-tensorflow-with-keras\r\nThanks for reply  :)"]}, {"number": 16720, "title": "ImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _Py_FalseStruct", "body": "I update my tensorflow to 1.5, the last version is 1.3.0. But I got a issue about numpy, after lot of attempts, it was solved, numpy can be used. \r\n\r\n**But now, I have a new issue when I import tensorflow:**\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _Py_FalseStruct\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nI can't get any solutions on the Internet. Anyone can help me?", "comments": ["This error (`undefined symbol: _Py_FalseStruct`) is a symptom of a mismatch between the python version and the version of a module. Have you somehow installed the py3 version of tensorflow into your py2 environment?\r\n\r\nHow did you install TensorFlow? \r\n\r\nCan you try to install tensorflow into a clean virtualenv? It does look like your python installation is broken.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'll close this, assuming this is a problem with your installation. Please reopen if more information is available.", "Hello, I met a problem like yours. I have a model in docker and I try to train it. It works and everything is ok. But later on , I try t train it, a problem occurs:\r\nImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZTIN10tensorflow17CostGraphDef_NodeE\r\nFailed to load the native TensorFlow runtime.\r\nCould you please give some suggestion? Thank you in advance.\r\n ", "ImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _Py_FalseStruct\r\n\r\n\r\nI am using flask and wsgi "]}, {"number": 16719, "title": "TypeError: 'NoneType' object is not callable", "body": "My environment\r\n\r\npython 3.5.2 anaconda (use pyenv)\r\nkeras 2.1.3\r\ntensorflow 1.4.1\r\ntheano 1.0.1\r\ncntk 2.3\r\n\r\nWhen backend is tensorflow, TypeError occurs just before learning is finished.\r\n\r\nTypeError: 'NoneType' object is not callable\r\n\r\nIs there a solution?", "comments": ["These information may be not enough to solve this problem. Could you please post the full error log and a minimum reproducible sample code.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "yes", "I restarted my pc its working "]}, {"number": 16718, "title": "add an interface to check if a variable is initialized", "body": "Currently, if we create an Adam optimizer and minimize some loss, Adam will create some new variables that need to be initialized. Howerver, this is only a part of variables and we donot want to use `tf.global_variables_initializer()`.\r\n\r\nIf there is an interface to check if a variable is initialized, then we can filter global variables and initialize only what needs to be initialized!\r\n\r\nthe interface should look like `Variable.is_initialized() -> bool`", "comments": ["Oh, I donot know that there is a function named` tf.is_variable_initialized(var)->bool`\r\n\r\nsorry to bother!\r\n\r\nclosed.", "Currently, I am using following code:\r\n\r\n```\r\nfor x in tf.global_variables():\r\n    if not sess.run(tf.is_variable_initialized(x)):\r\n        sess.run(x.initializer)\r\n```\r\n\r\nbut I found it extremely slow!\r\n\r\nCan you consider maintain a flag to indicate whether a variable is initialized?", "@alextp what do you think is the best way to do this?", "tf.is_variable_initialized returns a tensor. If you want to initialize all uninitialized variables in a single session.run call you can build a graph which loops over the variables and assigns them to initialized_value (which is their current value or the value of their initializer). So something like `sess.run([x.assign(x.initialized_value()) for x in tf.global_variables()])` should do the trick.", "@alextp  but is it efficient? I mean, this has the same amout of work as `sess.run(tf.global_variables_initializer())`\r\n\r\ne.g, if I have a large matrix which is already initialized, this is going to be time-consuming.", "You can look at the implementation of initialized_value to see how to write\ntf.cond predicated on whether a variable is initialized.\n\nI am suspicious of your initialization logic though as it's not safe on\ndistributed. Much better to keep track of the variables you want to\nreinitialize.\n\nOn Feb 6, 2018 18:08, \"youkaichao\" <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> but is it efficient? I mean, this has\n> the same amout of work as sess.run(tf.global_variables_initializer())\n>\n> e.g, if I have a large matrix which is already initialized, this is going\n> to be time-consuming.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16718#issuecomment-363631556>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQHyb8O7s6BXXAn_spICew4rmGlNks5tSQW1gaJpZM4R4AIl>\n> .\n>\n"]}, {"number": 16717, "title": "Cherrypicks", "body": "", "comments": ["Looks like the failure was just a flake. It passes now."]}]