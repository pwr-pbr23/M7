[{"number": 38928, "title": "Get grpc error when I use tensorflow java API to load model.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.7\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI run tensorflow java api to load model in flink cluster, it works fine the first time. But When I run the job the second time in cluster,  it turns out to be an error like below:\r\n\r\n**[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:58] File already exists in database: tensorflow/core/protobuf/eager_service.proto\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1358] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\nlibc++abi.dylib: terminating with uncaught exception of type google::protobuf::FatalException: CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size):**\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["There is a similar bug issue link in grpc issues\r\nhttps://github.com/grpc/grpc/issues/13249", "The root cause may be similar to https://github.com/tensorflow/tensorflow/issues/20561. Both issues have error message \"File already exists in database..\". It is possible that Flink and Tensorflow linked different versions of the same library (e.g. protobuf) twice.\r\n\r\nSo it is likely a compatibility issue between the given versions of TensorFlow and Flink ,rather than being a bug in TensorFlow.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38928\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38928\">No</a>\n", "Here is the github where I show how to reproduce the issue.   @lindong28  @sjamesr Thanks!\r\nhttps://github.com/tywtyw/flink-tensorflow", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38928\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38928\">No</a>\n"]}, {"number": 38927, "title": "ICU dependency needs to be updated to at least 66.1 for C++20", "body": "TF depends on ICU 64.2. Version 66.1 contains a bug fix needed to compile in C++20 mode. Might as well update to 67.1, though, since it just came out. See https://unicode-org.atlassian.net/browse/ICU-20972", "comments": ["Nevermind. The latest version of ICU still does not compile in C++20 mode due to https://unicode-org.atlassian.net/browse/ICU-20973", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38927\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38927\">No</a>\n"]}, {"number": 38926, "title": "TFLite failed to run Transpose Conv2D on mobile.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Virtual\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nInternal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/transpose_conv.cc:247 NumInputs(node) != 3 (4 != 3)\r\n```\r\n\r\n**Describe the expected behavior**\r\nPython API run fine but when i run on mobile i get above error. Tflite 2.1.0 on mobile can avoid this bug but i will get \"Didn't find op for builtin opcode 'SPACE_TO_BATCH_ND' version '3'\" bug instead.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @dathudeptrai,\r\nCan you extend your input to SPACE_TO_BATCH_ND op to be 4D so you can use SPACE_TO_BATCH_ND version 1 or 2.\r\nThe 'SPACE_TO_BATCH_ND' version '3'\" is not yet included in the TFlite 2.1.0.", "hi @thaink ofc i can extend all input from 1D to fake 2D to avoid SPACE_TO_BATCH_ND bug, but tflite-nightly have more supported op than tf 2.1, let say bert-related ops. The real bug i want to report here is \"tensorflow/lite/kernels/transpose_conv.cc:247 NumInputs(node) != 3 (4 != 3)\" on tflite-nightly, tf 2.1.0 don't have this error. Python API tf-nightly still fine, so there is a missmatch betweent tf-nightly and tflite-nightly ? ", "@dathudeptrai Thanks for your explanation. \r\nhttps://bintray.com/google/tensorflow/tensorflow-lite/0.0.0-nightly is updated everyday so it should have the latest code. However, i don't think your error is occurred https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/transpose_conv.cc#L247\r\n", "hi @thaink that is why i ask if there is a missmatch betweent tf-nightly and tflite-nightly. The line 247 of above code expaind why tf-nightly don't occur this error.", "The problem i don't understand here is why tf 2.1.0 don't have this error. since it is much older and above code and tf-nightly is just recent. tf-nightly and tflite-nightly are triggered independently but they are both built overnight so should have roughly the same code.\r\nCould you try to build the aar from source and check if the error occur?", "ok let me check. Ofc i can trick the model to run correctly by fake 2D input. But the missmatch between Tf-nightly and tflite-nightly makes me quite curious :)). ", "\"But the missmatch between Tf-nightly and tflite-nightly makes me quite curious\"\r\n@multiverse-tf do you have any clue about this?", "Not sure. My wild guess is that there might be some bug introduced between tf-nightly and tflite-nightly as they were built separately and independently (i.e. not built against the same github head).", "@multiverse-tf pls check, tflite nightly still have problem on Conv2DTranspose. ```java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/transpose_conv.cc:247 NumInputs(node) != 3 (4 != 3)\r\n    Node number 12 (TRANSPOSE_CONV) failed to prepare. ```. Tf-nightly still run fine, and based on the L247 in tensorflow/lite/kernels/transpose_conv.cc, this problem shouldn't happend. On mobile i use ```implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'```", "Hmm, are you using an old tflite-nightly build? The particular check as indicated by \"transpose_conv.cc:247 NumInputs(node) != 3 (4 != 3)\" was removed in https://github.com/tensorflow/tensorflow/commit/43b8f6e71001be7bacee4559e3021965ae67cf33#diff-84dcd410cd1816346c679396513fbcd2L247 on April 23, 2020.", "@multiverse-tf that is why i ask if there is a missmatch between tf-nightly and tflite. What do u mean \"an old tflite-nightly build\" ?. i use ```implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'``` and sync project so android will re-download tflite-nightly ? am i right?", "@dathudeptrai I am not familiar with Gradle but could you give \"./gradlew build --refresh-dependencies\" a try? Or delete the gradle cache $HOME/.gradle/caches/ to make sure it fetches the latest tflite-nightly?", "@thaink @multiverse-tf ok the problem is cache :))). Seem if we want use tflite-nightly we need delete caches file :). I will close the issue, thank for ur help :D.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38926\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38926\">No</a>\n"]}, {"number": 38925, "title": "DLL load failed while importing _pywrap_tensorflow_internal", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@kisoojeon Provide the exact sequence of commands / steps that you executed before running into the problem and the error logs. Also, Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?labels=type%3Abuild%2Finstall&template=10-build-installation-issue.md) as it helps us to analyze the issue faster.Thanks!", "@kisoojeon Any Updates regarding on this issue?.Thanks!", "No template filled in, closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38925\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38925\">No</a>\n"]}, {"number": 38924, "title": "mirroredstrategy not working? parallel GPUs working like serial", "body": "I'm working with TF2.0\r\nI find a situation confusing when using mirroredstrategy just the same as the tutorial:\r\n\r\n    strategy = tf.distribute.MirroredStrategy()\r\n\r\n    # Create a dataset\r\n    dataset = dataset_ops.Dataset.TFRecordDataset([\r\n      \"/a/1.tfr\", \"/a/2.tfr\", \"/a/3.tfr\", \"/a/4.tfr\"])\r\n\r\n    # Distribute that dataset\r\n    dist_dataset = strategy.experimental_distribute_dataset(dataset)\r\n    # Iterate over the distributed dataset\r\n    for x in dist_dataset:\r\n      # process dataset elements\r\n      strategy.experimental_run_v2(train_step, args=(x,))\r\n\r\nhere's step_fn() in function train_step(), I made 2 time stamp in the code:\r\n\r\n    # @tf.function(experimental_relax_shapes=True)\r\n    def train_step(dist_inputs):\r\n        def step_fn(inputs):\r\n            (mel_specs, pred_inp, \r\n             spec_lengths, label_lengths, labels) = inputs\r\n            with tf.GradientTape() as tape:\r\n                outputs = model([mel_specs, pred_inp], \r\n                    training=True)\r\n                # adding line below\r\n               begin_time = time.tme()\r\n                loss = loss_fn(labels, outputs,\r\n                    spec_lengths, label_lengths)\r\n                loss *= (1. / batch_size)\r\n\r\n            if train_metrics is not None:\r\n                metric_results = run_metrics(mel_specs, labels,\r\n                    metrics=train_metrics)\r\n                metric_results = {name: result * (1. / max(len(gpus), 1)) for name, result in metric_results.items()}\r\n\r\n            gradients = tape.gradient(loss, model.trainable_variables)\r\n\r\n            # adding line below before update gradients\r\n            end_time = time.time()\r\n            print(begin_time, end_time, end_time-begin_time)\r\n           optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n            return loss, metric_results\r\n        losses, metrics_results = strategy.experimental_run_v2(step_fn, args=(dist_inputs,))\r\n        mean_loss = strategy.reduce(\r\n            tf.distribute.ReduceOp.SUM, losses, axis=0)\r\n        mean_metrics = {name: strategy.reduce(\r\n            tf.distribute.ReduceOp.SUM, result, axis=0) for name, result in metrics_results.items()}\r\n        return mean_loss, mean_metrics\r\n\r\nI found that if using 2 GPUs(say gpu0 and gpu1),\r\nthe two gpu printing time_stamp: bg1, ed1 and bg2, ed2\r\nthe four params relationship is like this:\r\nbg1<ed1<bg2<ed2\r\nthat is only when gpu0 finished inference, gpu1 start inferencing\r\nWas the MirroredStrategy not working?\r\nI commented the tf.function() ,was there any relationship?\r\nwhen I not comment the tf.function(), the code ran into the ERROR: python segment fault (core dumped)...\r\n\r\nsomeone can help me? thanks", "comments": ["When not using tf.function, this is expected. We do not recommend using MirroredStrategy with eager op by op mode for now unless for debugging purposes.\r\nWhen using with tf.function, the time you are printing is for the function tracing / graph construction, and is not representative of the actual execution time. ", "> When not using tf.function, this is expected. We do not recommend using MirroredStrategy with eager op by op mode for now unless for debugging purposes.\r\n> When using with tf.function, the time you are printing is for the function tracing / graph construction, and is not representative of the actual execution time.\r\n\r\nthanks for reply!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38924\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38924\">No</a>\n"]}, {"number": 38923, "title": "tensorflow 2.1  loading model when saved with `tf` format does not work", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: 16 GB RAM\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nCheck the code below to create and save model\r\n\r\n[Test the code on Google-Colab](https://github.com/SpikingNeuron/tfpy_warrior/blob/master/tf2_save_and_load_custom_model.ipynb)\r\n\r\n\r\n```python\r\n# import necessary modules\r\nimport tensorflow as tf\r\nimport tensorflow.keras as tk\r\nprint(tf.__version__)\r\n\r\n# define a custom model\r\nclass MyModel(tk.Model):\r\n    ...\r\n\r\n# Define a simple sequential model\r\ndef create_model():\r\n    a = tk.Input(shape=(32,))\r\n    b = tk.layers.Dense(32)(a)\r\n    model = MyModel(inputs=a, outputs=b)\r\n\r\n    model.compile(optimizer='adam',\r\n                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                metrics=['accuracy'])\r\n    return model\r\n\r\n\r\n# create model\r\nmy_model = create_model()\r\n\r\n# Display the model's architecture\r\nmy_model.summary()\r\n\r\n# save model\r\nmy_model.save(filepath=\"./saved_model\", save_format=\"tf\")\r\n```\r\n\r\nNow I want to load back the weights from disk. When I use the code below I get the error.\r\n\r\n```python\r\n# load back model\r\nmy_model.load_weights(filepath=\"./saved_model\")\r\n```\r\n\r\nThis gives the error:\r\n\r\n```txt\r\n/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)\r\n    171         if swmr and swmr_support:\r\n    172             flags |= h5f.ACC_SWMR_READ\r\n--> 173         fid = h5f.open(name, flags, fapl=fapl)\r\n    174     elif mode == 'r+':\r\n    175         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5f.pyx in h5py.h5f.open()\r\n\r\nOSError: Unable to open file (file read failed: time = Tue Apr 28 15:30:40 2020\r\n, filename = './saved_model', file descriptor = 57, errno = 21, error message = 'Is a directory', buf = 0x7fff093afd50, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)\r\n```\r\n\r\nBut then I debugged and found the TensorFlow library confuses itself in thinking that it is an `h5` file. So I modified the code as below and now it works. I also get to use my custom `MyModel`\r\n\r\nBasically I added extra path i.e. `\\\\variables\\\\variables` so that it detects the folder as `tf` checkpoint. Can anyone suggest a better approach?\r\n\r\n```python\r\nmy_model.load_weights(filepath=\"./saved_model/variables/variables\")\r\nprint(my_model.__class__)\r\n```\r\n\r\nThe other option is to use `tk.models.load(...)` as in the code below. But, the\r\nproblem is I lose my sub-classed model `MyModel`\r\n\r\n```python\r\n_loaded_my_model = tk.models.load_model(\"./saved_model\")\r\nprint(_loaded_my_model.__class__)\r\n```\r\n\r\n**Describe the expected behavior**\r\nWas expecting the below code to work:\r\n\r\n```python\r\n# load back model\r\n# .... this does not work\r\nmy_model.load_weights(filepath=\"saved_model\")\r\n```\r\n\r\nOr else provide extra method `my_model.load(filepath=..., save_format='tf')` which is in line with `my_model.save(filepath=..., load_format='tf')` with extra kwarg `load_format` if needed.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n[Test the code on Google-Colab](https://github.com/SpikingNeuron/tfpy_warrior/blob/master/tf2_save_and_load_custom_model.ipynb)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I could able to replicate the issue. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/34cb21704963c03f8704a468deef6f79/untitled33.ipynb#scrollTo=8v7hSas77msv).Thanks!", "@SpikingNeuron \r\nActually you need to use `load_model` to load the model that was saved earlier using `model.save`. As `model.save` saves everything including architecture, weights, optimizer status etc. In your code you tried to access `variables` folder within the `saved_model` folder as weights are saved there. Best way is to use 'model.save` and `load_model`.\r\n\r\nIf you want to `save_weights` and then load the weights back then you can use `model.load_weights`.\r\n\r\nSo you need to comment out the last line and use load_model as shown below.\r\n```\r\n# model.load_weights(filepath=\"saved_model\") #\r\nloaded_model = tf.keras.models.load_model('./saved_model')\r\n```\r\n\r\n```\r\nHere is the code for saving weights and loading them back.\r\n# saving and loading weights\r\nmodel.save_weights('./Mymodel_weights',save_format='tf')\r\n# create a new model architecture\r\nnew_model = create_model()\r\nnew_model.load_weights('./Mymodel_weights')\r\nnew_model.summary()\r\n```\r\n\r\nCan you Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/ef893f1f17072f30251ffa530cbc2bc6/saving_loading_model.ipynb)\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Actually I override Model class. And with `tf.keras.models.load_model(...)` I cannot get that class. So I prefer something like `my_model_instance.load(...)` to do the job for me. I will modify the question above to reflect this point.", "@SpikingNeuron If you have any `custom_objects` in your model, then please follow this [guide](https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects). Thanks!", "> @SpikingNeuron If you have any `custom_objects` in your model, then please follow this [guide](https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects). Thanks!\r\n\r\nPlease read the updated question now. I assume that providing extra method `my_model.load(filepath=..., load_format='tf')` which is in line with `my_model.save(filepath=..., save_format='tf')` with extra kwarg `load_format` if needed can be a solution that is consistent with `save()` api..\r\n", "@SpikingNeuron As I mentioned in my [last response](https://github.com/tensorflow/tensorflow/issues/38923#issuecomment-620104527), when you save a keras model using `model.save` then we need to need `tf.keras.models.load_model` to load the model. I have updated your colab [here](https://colab.research.google.com/gist/jvishnuvardhan/6689b38218dc53c634e7b5324cdafa72/tf2_save_and_load_custom_model.ipynb). Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "I will close the issue ... but I still think that having a class method in `tf.keras.models.Model` for loading model can be a better idea as the custom class model `MyCustomModel` that is inheriting `tf.keras.models.Model` can then load model with class type `MyCustomModel` instead of `tf.keras.models.Model` ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38923\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38923\">No</a>\n"]}, {"number": 38922, "title": "LSTMCell Dropout mask is reset at every call", "body": "**System information**\r\n- TensorFlow version (use command below):2.2.0-rc3\r\n\r\n**Describe the current behavior**\r\nThe dropout mask is being reset at every call\r\n\r\n**Describe the expected behavior**\r\nThe dropout mask should be fixed unless reset is called\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.preprocessing import sequence\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Dense, Embedding\r\nfrom tensorflow.keras.layers import LSTM\r\nfrom tensorflow.keras.datasets import imdb\r\n\r\nmax_features = 20000\r\nmaxlen = 80\r\n\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\r\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\r\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(max_features, 128))\r\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\nx_sample = x_train[:1]\r\na = model(x_sample, training=True)\r\ndp_mask1 = model.layers[1].cell.get_dropout_mask_for_cell(x_sample, training=True, count=4)\r\nrec_dp_mask1 = model.layers[1].cell.get_dropout_mask_for_cell(x_sample, training=True, count=4)\r\n\r\nb = model(x_sample, training=True)\r\ndp_mask2 = model.layers[1].cell.get_dropout_mask_for_cell(x_sample, training=True, count=4)\r\nrec_dp_mask2 = model.layers[1].cell.get_dropout_mask_for_cell(x_sample, training=True, count=4)\r\n\r\n# check if masks are the same after call\r\nprint(np.all([np.all(dp_mask1[i] == dp_mask2[i]) for i in range(len(dp_mask1))]))\r\nprint(np.all([np.all(rec_dp_mask1[i] == rec_dp_mask2[i]) for i in range(len(rec_dp_mask1))]))\r\n```\r\n\r\nJupyter Notebook example [here](https://colab.research.google.com/gist/zacwellmer/f56a9e1959e687d03ee89069d78af683/untitled4.ipynb)", "comments": ["Was able to reproduce the issue with TF v2.1, [TF v2.2.0rc3](https://colab.research.google.com/gist/amahendrakar/0b74be016ac78170739b6795632135b4/38922-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/404923b89b3d15d7fe3d531ed5f78838/38922-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Thanks for reporting the issue. I think resetting the dropout mask for every call is the intended behavior. This is also aligned with the dropout layer in keras. Could u point me to any of the API doc that makes you think the dropout mask is caches across calls?\r\n\r\n```\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\n\r\ndropout = keras.layers.Dropout(0.2)\r\n\r\na = np.arange(10, dtype=np.float32)\r\nd1 = dropout(a, training=True)\r\nd2 = dropout(a, training=True)\r\nprint(d1)\r\nprint(d2)\r\n```", "Hi @qlzh727 \r\nThe LSTMCell's docs on [reset_dropout_mask](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell#reset_dropout_mask) and [reset_recurrent_dropout_mask](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell#reset_recurrent_dropout_mask) led me to believe that the mask was only reset when explicitly called (example: the docs exaggeration on it being used in the call function).\r\n\r\nAt the very least if it's decided by default to reset the dropout mask at every step there should be a way to specify the mask. Similar to how we can specify the [RNN states via initial_states](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)", "I see.\r\n\r\nThe reset_*_mask is there in case user want to use LSTMCell directly. They are called with in LSTM  layer at the start of each batch, but the same mask is used for all the timesteps within the batch. This will ensure same feature get dropped within the sequence (eg same word is dropped in the sentence).\r\n\r\nThe mask is by default generated by keras framework, if you want to control the mask by yourself, you can call the LSTM or LSTMCell with \r\n\r\n```\r\nlstm = tf.keras.layers.LSTM(10)\r\nlstm(inputs, mask=self_managed_mask)\r\n```\r\n\r\nSee more details about mask in https://www.tensorflow.org/guide/keras/masking_and_padding#passing_mask_tensors_directly_to_layers", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38922\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38922\">No</a>\n", "That mask argument does not fully support the use case of rnn dropout with a specific mask (ex: the recurrent dropout mask)", "yes, it doesn't support the recurrent dropout mask, and it is aligned with we do in Dropout. The mask suppose to be a randomly generated filter in this case, and we didn't consider user injection as a common API use case."]}, {"number": 38921, "title": "unable to  install tensorflow!", "body": "  File \"d:/anaconda/AI,CSS50/src5/banknotes/banknotes.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\",\r\nline 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 114\r\n    def TFE_ContextOptionsSetAsync(arg1, async):\r\n                                             ^\r\nSyntaxError: invalid `syntax**  File \"d:/anaconda/AI,CSS50/src5/banknotes/banknotes.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\",\r\nline 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 114\r\n    def TFE_ContextOptionsSetAsync(arg1, async):\r\n                                             ^\r\nSyntaxError: invalid synta  File \"d:/anaconda/AI,CSS50/src5/banknotes/banknotes.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\",\r\nline 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 114\r\n    def TFE_ContextOptionsSetAsync(arg1, async):\r\n                                             ^\r\nSyntaxError: invalid `syntax**``", "comments": ["@tahirCS50 \r\nThis issue is more suitable on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues) repo since its related to TF installation with Anaconda.\r\nPlease post it on [Continuum Anaconda]( Continuum Anaconda.).\r\nThanks!"]}, {"number": 38920, "title": " AutoGraph failed  to convert if statement into the equivalent tf.cond  in a RNN custom cell", "body": "\r\n**System information**\r\n- I have written custom code.\r\n- OS Windows 10:\r\n- TensorFlow version : 2 . 1 .0\r\n- Python version: 3.6\r\n\r\n- CUDA/cuDNN version: 10.1   /  7.6.5\r\n- GPU model and memory: rtx 2070\r\n\r\n\r\n**Describe the current behavior**\r\nI write a custom cell to be used in  a RNN model, the cell has some 'if' statements where a condition is  tf.Variable.  When I use @function to build a graph, the 'if 'condition work fine in  the model class but throw an error in the custom cell class :\r\n\r\n\r\n    TypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.\r\n    Encountered error:\r\n    \r\n    using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n\r\n  \r\n**Describe the expected behavior**\r\n\r\nAutoGraph should convert if statements into the equivalent tf.cond \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n# custom cell\r\n```\r\nclass tst_cell(tf.keras.layers.Layer):\r\n\r\n    def __init__(self ):\r\n         super(tst_cell , self).__init__()\r\n         self.corr      = tf.keras.layers.Dense(4)\r\n         self.state_size = 1\r\n      \r\n    def call (self ,inpt ,  states  ):\r\n   \r\n        if ee == 0 :            #  throw the error  !!!!!!\r\n            print('cell  ee == 0')\r\n            \r\n        self.corr_s    = self.corr(inpt)\r\n    \r\n        return self.corr_s , states\r\n \r\n\r\n \r\n    def get_initial_state(self,inputs, batch_size , dtype ):\r\n   \r\n       \r\n      init_stat= tf.zeros((2,1) , dtype = dtype)\r\n        \r\n      return init_stat    \r\n\r\n\r\n```\r\n ` ee = tf.Variable(0) # global variable used in 'if' condition \r\n`\r\n# the model \r\n```\r\nclass tst_rnn_model(tf.keras.Model):\r\n     def __init__ (self):\r\n        super(tst_rnn_model , self).__init__()\r\n       \r\n        self.msk = tf.keras.layers.Masking(mask_value=0. , input_shape = (6,3))\r\n        \r\n        self.tst_gmm_cell        = tst_cell()\r\n        \r\n        self.tst_rnn_gmm          = tf.keras.layers.RNN(self.tst_gmm_cell ,  return_sequences=True )\r\n\r\n     @tf.function  \r\n     def call (self , inpt ):\r\n        \r\n        if ee == 0 :        # work fine  !!!!!\r\n            print('model ee == 0')\r\n            \r\n        x = self.msk(inpt)\r\n        mask = self.msk.compute_mask(inpt)\r\n        outputs = self.tst_rnn_gmm(x , mask = mask)\r\n\r\n        return outputs\r\n\r\n\r\ndata=np.arange(36.).reshape(2,6,3)\r\ntst_model = tst_rnn_model()\r\nout = tst_model(data)   \r\nout\r\n```\r\n**Other info / logs** Include any logs \r\n\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-222-ee4d06aa06c9> in <module>\r\n     29 #run the model\r\n     30 tst_model = tst_rnn_model()\r\n---> 31 out = tst_model(data)\r\n     32 out\r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    820           with base_layer_utils.autocast_context_manager(\r\n    821               self._compute_dtype):\r\n--> 822             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    823           self._handle_activity_regularization(inputs, outputs)\r\n    824           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    613       # This is the first call of __call__, so we have to initialize.\r\n    614       initializers = []\r\n--> 615       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    616     finally:\r\n    617       # At this point we know that the initialization is complete (or less\r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    495     self._concrete_stateful_fn = (\r\n    496         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 497             *args, **kwds))\r\n    498 \r\n    499     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2387       args, kwargs = None, None\r\n   2388     with self._lock:\r\n-> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2390     return graph_function\r\n   2391 \r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   2701 \r\n   2702       self._function_cache.missed.add(call_context_key)\r\n-> 2703       graph_function = self._create_graph_function(args, kwargs)\r\n   2704       self._function_cache.primary[cache_key] = graph_function\r\n   2705       return graph_function, args, kwargs\r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2591             arg_names=arg_names,\r\n   2592             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2593             capture_by_value=self._capture_by_value),\r\n   2594         self._function_attributes,\r\n   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    976                                           converted_func)\r\n    977 \r\n--> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n    979 \r\n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    438         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    440     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    441 \r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in bound_method_wrapper(*args, **kwargs)\r\n   3209     # However, the replacer is still responsible for attaching self properly.\r\n   3210     # TODO(mdan): Is it possible to do it here instead?\r\n-> 3211     return wrapped_fn(*args, **kwargs)\r\n   3212   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)\r\n   3213 \r\n\r\nc:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nTypeError: in converted code:\r\n\r\n    <ipython-input-219-ee4d06aa06c9>:22 call  *\r\n        outputs = self.tst_rnn_gmm(x , mask = mask)\r\n    c:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py:644 __call__\r\n        return super(RNN, self).__call__(inputs, **kwargs)\r\n    c:\\users\\ultrapc\\anaconda3\\envs\\env3.6\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:785 __call__\r\n        str(e) + '\\n\"\"\"')\r\n\r\n    TypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.\r\n    Encountered error:\r\n    \"\"\"\r\n    using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n    \"\"\"\r\n\r\n\r\n      ", "comments": ["@said120276 \r\n\r\nI am not seeing any error message in TF version 2.2.rc3(`!pip install tensorflow==2.2-rc3 `).Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/6d3deecc5c6b08101557a12401e57959/untitled830.ipynb).Please verify once and close the issue. Thanks!", "Thank you for your reply . So it was a bug in the previous version ? I will upgrade to tensorflow 2.2 and try this night.", "@said120276 \r\n\r\nPlease close this thread if it solves your question in latest TF version(2.2-rc3). Thanks!", "Work fine in TF version 2.2.rc3.Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38920\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38920\">No</a>\n"]}, {"number": 38919, "title": "Discrepancy of soname for libtensorflow_framework.[dylib|so] on macOS and Linux", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 + macOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary (tf-nightly)\r\n- TensorFlow version: tf-nightly-2.2.0.dev20200426\r\n- Python version: 3.6 on Ubuntu 18.04 and 3.7 on macOS\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n**Describe the problem**\r\n\r\nWhile trying to link against `libtensorflow_framework.so` for custom ops, we noticed that the soname on macOS and Linux are different:\r\n**[macOS]:**\r\n```\r\nls -la /Library/Python/3.7/site-packages/tensorflow\r\ntotal 169680\r\n...\r\n-rwxr-xr-x   1 root  wheel  28946620 Apr 26 08:42 libtensorflow_framework.2.2.0.dylib\r\n-rwxr-xr-x   1 root  wheel  28946620 Apr 26 08:42 libtensorflow_framework.2.dylib\r\n-rwxr-xr-x   1 root  wheel  28946620 Apr 26 08:42 libtensorflow_framework.dylib\r\n...\r\n```\r\n\r\n**[Linux]:**\r\n```\r\nls -la /usr/local/lib/python3.6/dist-packages/tensorflow\r\ntotal 37000\r\n...\r\n-rwxr-xr-x  1 root staff 37806248 Apr 26 15:40 libtensorflow_framework.so.2\r\n...\r\n```\r\n\r\nFrom the above, it looks like macOS ties the soname to 2.2.0 while Linux still stays with 2. Given the fact that libtensorflow_framework.so largely relies on C++ API which does not guarantee compatibility across different version, it would be desirable to apply the same of macOS [`2.2.0`] to Linux [`2`] as well. This will help downstream projects to better hand version compatibility issue I think. /cc @seanpmorgan @perfinion ", "comments": ["Agree would be very helpful. Similar topic was discussed in this thread and agreed upon naming the framework so with minor version... not sure where that stands:\r\nhttps://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/developers/gdnFNTJbJFM/8oL2PSMOBQAJ", "oh huh i didnt realize mac was the full version. but yeah lets change it, i'll find some time soon and prepare a patch. The lib version should be 2.2 only, it shouldnt have the patch in it i dont htink. normally tf-x.y.1 releases are only for security or similar issues so the ABI will be the same and would be a huge hassle to rebuild everything for patch changes", "@yongtang,\r\n\r\nCan you try updating to latest stable version i.e `2.6.0` and let us know if the discrepancy exists even in newer version? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38919\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38919\">No</a>\n"]}, {"number": 38918, "title": "Propagate loss throw models", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0-rc3\r\n- Python version: 3\r\n\r\n**Describe the current behavior**\r\nI have two models i want to run sequentially : a generator and a classifier.\r\nThe loss from the generator is depending from the classifier outputs.\r\nIn the code i provide, i can't manage to get the backpropagation working : \r\nValueError: No gradients provided for any variable: ['dense_generator/kernel:0', 'dense_generator/bias:0']\r\n\r\nNo grad seems computed for the generator.\r\n\r\nThe code i wrote is strongly inspired from [dcgan tensorflow tutorial](https://www.tensorflow.org/tutorials/generative/dcgan)\r\n\r\n**Describe the expected behavior**\r\nI expect to be able to generate gradients for my generator based on a loss computed at my classifier level.\r\n\r\n**Standalone code to reproduce the issue**\r\nCopy paste the following code into a colab notebook and execute it : \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint(tf.__version__)\r\n\r\ndef build_generator():\r\n    in_data = tf.keras.layers.Input(shape=(1,), dtype=tf.int32, name=\"in\")\r\n    \r\n    out = tf.keras.layers.Dense(10, name='dense_generator')(in_data)\r\n    return tf.keras.Model(inputs=in_data, outputs=out)\r\n    \r\ndef build_classifier():\r\n    features = tf.keras.layers.Input(shape=(10,), dtype=tf.int32, name=\"features\")\r\n    out = tf.keras.layers.Dense(1, activation='sigmoid', dtype = tf.float32, name='dense_class')(features)\r\n\r\n    return tf.keras.Model(inputs=features, outputs=out)\r\n\r\ngenerator = build_generator()\r\nclassifier_toxic = build_classifier()\r\n\r\ngenerator_optimizer = tf.keras.optimizers.Adam()\r\nclass_toxic_optimizer = tf.keras.optimizers.Adam() \r\n\r\nclassifier_loss = lambda a,b: tf.keras.losses.binary_crossentropy(a, b, from_logits=True)\r\n\r\n@tf.function\r\ndef train_step(inputs):\r\n    with tf.GradientTape() as generator_tape:\r\n        features = generator(inputs, training=True)\r\n        preds = classifier_toxic(features)\r\n        loss = classifier_loss(tf.zeros_like(preds), preds)\r\n        # with this folowing loss that takes output from generator, it works but this is not what i want\r\n        #loss = classifier_loss(tf.zeros_like(features), features)      \r\n    # apply gradient on generator\r\n    grads_generator = generator_tape.gradient(loss, generator.trainable_variables)\r\n    generator_optimizer.apply_gradients(zip(grads_generator, generator.trainable_variables))\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(np.arange(10)).batch(1)\r\n\r\nfor count, inputs in enumerate(train_dataset):\r\n    # run training step\r\n    train_step(inputs)\r\n    print('done :', count)\r\n```\r\n\r\n", "comments": ["I could able to replicate the issue. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/57593a78bdeeff9d167db69cdfff5b37/untitled32.ipynb).Thanks!.", "Hello,\r\n\r\nI found the issue : the dtype for the input of my classifier was incorrect : \r\nfeatures = tf.keras.layers.Input(shape=(10,), dtype=**tf.int32**, name=\"features\")\r\n\r\nto be replaced by : \r\nfeatures = tf.keras.layers.Input(shape=(10,), dtype=**tf.float32**, name=\"features\")", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38918\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38918\">No</a>\n"]}, {"number": 38917, "title": "Add missing mlir_passthrough_op in pip wheel install", "body": "While trying to following the example of MlirPassthroughOp in the docstring\r\n(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_MlirPassthroughOp.pbtxt):\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.compiler.mlir.tensorflow.gen_mlir_passthrough_op import mlir_passthrough_op\r\n\r\nmlir_module = '''python\r\nfunc @main(%arg0 : tensor<10xf32>, %arg1 : tensor<10xf32>) -> tensor<10x10xf32> {\r\n   %add = \"magic.op\"(%arg0, %arg1) : (tensor<10xf32>, tensor<10xf32>) -> tensor<10x10xf32>\r\n   return %ret : tensor<10x10xf32>\r\n}\r\n'''\r\n\r\n@tf.function\r\ndef foo(x, y):\r\n  return mlir_passthrough_op([x, y], mlir_module, Toutputs=[tf.float32])\r\n\r\ngraph_def = foo.get_concrete_function(tf.TensorSpec([10], tf.float32), tf.TensorSpec([10], tf.float32)).graph.as_graph_def()\r\n```\r\n\r\nThe following error occurs in tf-nightly:\r\n```python\r\n>>> from tensorflow.compiler.mlir.tensorflow.gen_mlir_passthrough_op import mlir_passthrough_op\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.compiler.mlir'\r\n>>>\r\n>>>\r\n```\r\n\r\nThe reason was that dependency `\"//tensorflow/compiler/mlir/tensorflow:gen_mlir_passthrough_op_py\"`\r\nwas not included in pip.\r\n\r\nThis PR fixes the issue.\r\n\r\nThis PR is more or less related to #38894, which fixes a typo in docstring itself.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["LGTM, but I don't know much about our pip package and the nightly, so I'd rather have @mihaimaruseac signing on this as well."]}, {"number": 38916, "title": "Failed to load the native TensorFlow runtime. ImportError DLL load failed.", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary (I think)\r\n- TensorFlow version: 2.2.0rc3 CPU version\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\ntrying to run ascript that is importing tensorflow as follows:\r\n```\r\nimport tensorflow as tf\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n```\r\n*the full trace back is below*\r\n\r\nThis `ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found` is evidently the issue but I am unsure how to fix it after having gone through your other responses to similar github issues.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n```\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", " CPU model: Intel Core i7-8700 CPU @ 3.2GHz\r\n\r\nWhich after googling does seem to support AVX", "Hey @Lamm-a please have a look at Python 3.8 support #33374 I guess it is not supported yet :cry: so you may have to wait. It will come soon I guess for Python 3.8. It worked for Mac but may not work for windows as of now. So downgrading python maybe an option or use virtual environment for tensorflow with other versions of Python. ", "I gave that a go right after I posted this and it semi worked, I still got a similar error of DLL not found. Reverting back to Tensorflow 2.0 fixed that. It seems to be an issue with Microsoft visuals c++. I have it installed but it is not being regonised I think?\r\n\r\nI am now getting hit with this error though:\r\n\r\n```\r\nimport tensorflow as tf\r\nTypeError: expected bytes, Descriptor found\r\n```\r\n\r\n**Full traceback:**\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 949, in _find_and_load_unlocked\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\core\\framework\\graph_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\core\\framework\\node_def_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\core\\framework\\attr_value_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\core\\framework\\tensor_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\core\\framework\\resource_handle_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\r\n  File \"C:\\Users\\wmn262\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\core\\framework\\tensor_shape_pb2.py\", line 112, in <module>\r\n    '__module__' : 'tensorflow.core.framework.tensor_shape_pb2'\r\nTypeError: expected bytes, Descriptor found\r\n```\r\n", "Yes there can be a problem with MS Visual C++ as well. Maybe it is not in PATH or not recognized. Please check installation guide [here](https://www.tensorflow.org/install/pip#windows)", "@Lamm-a \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest microsoft visual c++ redistributable from [here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\n).\r\nAlso, please follow the instructions from to install from Tensorflow [website](https://www.tensorflow.org/install/source_windows).\r\n\r\nAlso, refer similar issues.\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 #38615 and see if it helps you.Thanks!", "@Lamm-a \r\n\r\nI suspect problem with latest microsoft visual c++ redistributable .Please, follow instructions [here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).Thanks!", "@saikumarchalla I give the CPU model above :)\r\n\r\n> \r\n> \r\n> CPU model: Intel Core i7-8700 CPU @ 3.2GHz\r\n> \r\n> Which after googling does seem to support AVX\r\n\r\nThank you @ravikyram I will give that a look.", "I have to mention that this problem only exist in 2.1.0 but not in 2.0.0.\r\nSo, my homework forces to use tensorflow, and I noticed that I suffered from the same problem described in this issue. If tensorflow-gpu==2.0.0 is also ok, maybe it is an alternative choice.", "> I have to mention that this problem only exist in 2.1.0 but not in 2.0.0.\r\n> So, my homework forces to use tensorflow, and I noticed that I suffered from the same problem described in this issue. If tensorflow-gpu==2.0.0 is also ok, maybe it is an alternative choice.\r\n\r\nThen this means you need the latest MSVC redistributable. If you tried installing it and it didn't work, please check that that is the one that gets accessed.", "> \r\n> \r\n> > I have to mention that this problem only exist in 2.1.0 but not in 2.0.0.\r\n> > So, my homework forces to use tensorflow, and I noticed that I suffered from the same problem described in this issue. If tensorflow-gpu==2.0.0 is also ok, maybe it is an alternative choice.\r\n> \r\n> Then this means you need the latest MSVC redistributable. If you tried installing it and it didn't work, please check that that is the one that gets accessed.\r\n\r\nThanks for your help, and I will try it.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38916\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38916\">No</a>\n"]}, {"number": 38915, "title": "Loding dataset with TFRecord throws incompatible with the layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (also colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0-rc3\r\n- Python version: 3.8.2 64 bit\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: RTX 2080 8gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nModel.fit throws\r\n    \r\n> ValueError: Input 0 of layer srcnn is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.\r\n\r\n**Describe the expected behavior**\r\ntrains network with loaded dataset from TFRecord\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1GcIhMHv8GJff03WADzTo29oUFgjdMd_N\r\n\r\n**Other info / logs** \r\n```\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-96-ee31b3c773a3> in <module>()\r\n     20     train_dataset_count/BATCH_SIZE).numpy()\r\n     21 model.fit(parsed_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch,\r\n---> 22           callbacks=[cp_callback], use_multiprocessing=True)\r\n\r\n10 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    849                 batch_size=batch_size):\r\n    850               callbacks.on_train_batch_begin(step)\r\n--> 851               tmp_logs = train_function(iterator)\r\n    852               # Catch OutOfRangeError for Datasets of unknown size.\r\n    853               # This blocks until the batch has finished executing.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    616       # In this case we have not created variables on the first call. So we can\r\n    617       # run the first trace but we should fail if variables are created.\r\n--> 618       results = self._stateful_fn(*args, **kwds)\r\n    619       if self._created_variables:\r\n    620         raise ValueError(\"Creating variables on a non-first call to a function\"\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2417     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   2418     with self._lock:\r\n-> 2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2421 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2772           and self.input_signature is None\r\n   2773           and call_context_key in self._function_cache.missed):\r\n-> 2774         return self._define_function_with_shape_relaxation(args, kwargs)\r\n   2775 \r\n   2776       self._function_cache.missed.add(call_context_key)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _define_function_with_shape_relaxation(self, args, kwargs)\r\n   2704         relaxed_arg_shapes)\r\n   2705     graph_function = self._create_graph_function(\r\n-> 2706         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\r\n   2707     self._function_cache.arg_relaxed[rank_only_cache_key] = graph_function\r\n   2708 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2665             arg_names=arg_names,\r\n   2666             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2667             capture_by_value=self._capture_by_value),\r\n   2668         self._function_attributes,\r\n   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    979         _, original_func = tf_decorator.unwrap(python_func)\r\n    980 \r\n--> 981       func_outputs = python_func(*func_args, **func_kwargs)\r\n    982 \r\n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    440         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    442     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    443 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\r\n        y_pred = self(x, training=True)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:886 __call__\r\n        self.name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:168 assert_input_compatibility\r\n        layer_name + ' is incompatible with the layer: '\r\n\r\n    ValueError: Input 0 of layer srcnn is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.\r\n```\r\n\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/3f4df88d93e65ad47fbf3b41eaeb7df8/38915-2-2.ipynb#scrollTo=Z7Ph0RJHNxcp) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/c013ed797a1a56d6efd3d15322299120/38915-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@ll01,\r\nPlease take a look at these comments from similar issues and let us know if it helps? \r\n\r\nLink 1 - https://github.com/tensorflow/tensorflow/issues/18079#issuecomment-377986751\r\n\r\nLink 2 - https://github.com/tensorflow/tensorflow/issues/36321#issuecomment-581660542\r\n\r\nThanks!", "Hi but these links all relate from creating the dataset from a generator. This code is creating the dataset from a tfrecord. ", "@tomerk can you please take a look? thanks", "Hi @ll01 , sorry for the delay!\r\n\r\nWhat's happening here is your tfrecord loading function does not specify any shapes for the read values, so the end dataset has an unknown shape and rank. This is insufficient information for the keras model to construct & run, which is why it raises that error about the input being incompatible.\r\n\r\nYou can fix this by specifying the shape of the parsed values in your dataset construction using `tf.ensure_shape`\r\n\r\ne.g. the colab gists above work if you set it as such:\r\n```\r\ndef _parse_image_function(example_proto):\r\n    # Parse the input tf.Example proto using the dictionary above.\r\n    example = tf.io.parse_single_example(\r\n        example_proto, image_feature_description)\r\n    label_normalized = normalize_image(example['label'], False)\r\n    label_normalized = tf.ensure_shape(label_normalized, (33, 33, 3))\r\n    feature_normalized = normalize_image(example['feature'], False)\r\n    feature_normalized = tf.ensure_shape(feature_normalized, (33, 33, 3))\r\n    return feature_normalized, label_normalized\r\n```\r\n\r\n@aaudiber I'm reassigning this to you to triage if `tf.io.decode_image` should include an optional `shape` argument for the output that it tries to ensure. Perhaps it might be nice if we can somehow be more aggressive about not returning unknown ranks from `decode_image`?\r\n", "`decode_image` is a wrapper around `decode_jpeg`, `decode_bmp`, `decode_png`, and `decode_gif`. `decode_image` automatically detects the file format based on a file's contents, then uses one of those functions. The rank of the result will depend on the file contents, since GIFs will have rank 4 while other images have rank 3. Shape inference happens before reading file data, so there is no way to know the rank ahead of time.\r\n\r\nIn addition to using `ensure_shape`, there are a few other workarounds:\r\n\r\n- If the file type is known, use `tf.io.decode_bmp`, `tf.io.decode_jpeg`, `tf.io.decode_png`, or `tf.io.decode_gif` instead. These produce tensors of known rank (3 for non-GIF, 4 for GIF).\r\n- If you are ok with truncating GIFs to a single frame, pass `expand_animations=False` to `tf.io.decode_image`, so that the rank is guaranteed to be 3.", "Thanks such an embarising mistake ..."]}, {"number": 38914, "title": "Can't seem to use mirrored strategy in graph mode using tf.function", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA: 10.1 cudNN:7.6\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nMy main objective is achieve parallelism during model training. I have been following the tutorial from the below: \r\nhttps://www.tensorflow.org/tutorials/distribute/custom_training\r\n\r\nAt first, I tried to use eager mode and did not use tf.function to decorate any of my functions. There is no runtime error but the GPUs were still being used sequentially and i as getting the warning that Mirrored Strategy has significant overhead in eager mode and to decorate my APIs with `@tf.function`.\r\nSo, I proceeded to do everything exactly like in the above link.\r\n\r\nBut, now I am getting the error that I cannot iterate over a `tf.tensor` in Graph Mode while iterating over the distributed batched dataset. I am unable to get past this issue. \r\n\r\nThis is how i create my dataset. `list_ds` is list of lists , each sublist has 5 strings in it.\r\n`dataset = tf.data.Dataset.from_tensor_slices(list_ds)`\r\n`train_dataset` = dataset.batch(GLOBAL_BATCH_SIZE, drop_remainder=True)`\r\n`train_data_dataset = strategy.experimental_distribute_dataset(train_dataset)`\r\n\r\nOnly the API where I call `strategy.experimental_run_v2` is decorated with tf.function.\r\n`def train_step(input): `\r\n    `.....`\r\n`@tf.function`\r\n`def distributed_train_step(dataset_inputs):`\r\n    `strategy.experimental_run_v2(train_step, args=dataset_inputs)`\r\n`for epoch in range(EPOCHS):`\r\n    `for batch in train_dist_dataset:`\r\n    `distributed_train_step(batch)`\r\n\r\nUnfortunately, I will be unable to upload the exact code that I have written. That's why the indentations are wrong\r\n**Describe the expected behavior**\r\n I should be able to iterate over my dataset in Graph mode as according to the tutorial it is possible.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@tusharrain28 \r\ncan you please provide simple standalone code to replicate the issue, along with the error log", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38913, "title": "How to use cudnn LSTMP", "body": "Firstly, I've posted this on [stackoverflow\r\n](https://stackoverflow.com/questions/61438013/how-to-use-cudnn-lstmp) before. And I know this is not a bug or something so if there is an answer I'll delete this issue. Much appreciated!\r\n\r\nUsually I use `tf.contrib.cudnn_rnn.CudnnLSTM` (tensorflow 1.15) and want to use LSTMP(LSTM projection) now. I know that `tf.contrib.rnn.LSTMCell` has LSTMP but that additional training time is too long for me. And I found that [nvidia support LSTMP](https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#features-of-rnn-functions). How to use it on tensorflow?\r\n\r\nThanks!", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you serious? I mean, could you just READ the question before answering. Nevermind I found that tensorflow.contrib has something like `num_proj`. Still appreciated for your reply tho.", "Closing this issue since you have found workaround. Feel free to comment/reopen the issue if necessary. Thanks!"]}, {"number": 38912, "title": "How to pass flags to nvcc when building tensorflow with bazel?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version:\r\nr1.14\r\n- Python version:\r\n3.6.9\r\n- Installed using virtualenv? pip? conda?:\r\nbuild on raw linux environment, with pip installed\r\n- Bazel version (if compiling from source):\r\n0.24.1\r\n- GCC/Compiler version (if compiling from source):\r\n4.8.5\r\n- CUDA/cuDNN version:\r\nCUDA 10.0 CUDNN 7.6.5\r\n- GPU model and memory:\r\nNvidia GTX 1660Ti 6GB\r\n\r\n\r\n**Describe the problem**\r\nI want to pass some custom flags to nvcc during the build, like telling nvcc dynamically link the cuda runtime library with the flag --cudart=shared. On windows 10 with visual studio 2015, I could just use the following command line:\r\nbazel build --config=opt --config=cuda --copt=-nvcc_options=cudart=shared //tensorflow/tools/pip_package:build_pip_package, and msvc will just pass the flag specified by the -nvcc_options to nvcc,  but on linux it doesn't work, with the following errors pop out:\r\n\r\nERROR: /root/.cache/bazel/_bazel_root/5ac94cf1f70fd575be3807b8a4a32ede/external/com_google_absl/absl/debugging/BUILD.bazel:190:1: C++ compilation of rule '@com_google_absl//absl/debugging:leak_check' failed (Exit 1)\r\ngcc: error: unrecognized command line option '-nvcc_options=cudart=shared'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 152.342s, Critical Path: 48.24s\r\nINFO: 1236 processes: 1236 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\njust as the official build document said, build with command line:\r\nbazel build --config=opt --config=cuda --copt=-nvcc_options=cudart=shared //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR: /root/.cache/bazel/_bazel_root/5ac94cf1f70fd575be3807b8a4a32ede/external/com_google_absl/absl/debugging/BUILD.bazel:190:1: C++ compilation of rule '@com_google_absl//absl/debugging:leak_check' failed (Exit 1)\r\ngcc: error: unrecognized command line option '-nvcc_options=cudart=shared'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 152.342s, Critical Path: 48.24s\r\nINFO: 1236 processes: 1236 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["passing nvcc flags through bazel still remain unsolved, but my first intenstion of building tensorflow that dynamic links to cuda library is solved, see https://github.com/tensorflow/tensorflow/issues/38941 for detail ", "Is this still an issue? ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Is this still an issue?\r\n\r\nYes, still found no way to pass nvcc flags to bazel other than modifying the build file", "> > Is this still an issue?\r\n> \r\n> Yes, still found no way to pass nvcc flags to bazel other than modifying the build file\r\n\r\n@CptJack2  Could you give an example about how to modify build file to pass nvcc flags?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38912\">No</a>\n"]}, {"number": 38911, "title": "[XLA] Remove cross device nodes from clusters", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nThis pr removes the the cross device nodes (nodes that have input or output from\r\nother device) from the compilation candidates in `MarkForCompilation`. The reason for this change is that if the cross device nodes are introduced into clusters, the following 2 situation may happen and largely harm the performance.\r\n```\r\n  device0:  op1 ---    op2        cluster(op1, op2) ---\r\n                  |          ===>                     |\r\n  device1:        ---> op3                            ---> op3\r\n\r\n  or\r\n\r\n  device0:  op1   ---> op2              ---> cluster(op1, op2)\r\n                  |          ===>       |\r\n  device1:  op3 ---               op3 ---\r\n```\r\nXLA will delay the execution of an op when one of its predecessors is merged in the middle of a cluster. This delay will be ok when the op and the cluster are on the same device, because their kernels are executed sequentially. But when using multiple devices, this delay will make one device wait. For example, this is the timeline of a transformer model:\r\n- Before removing cross device nodes:\r\n![image](https://user-images.githubusercontent.com/10428324/80304808-105fce80-87eb-11ea-845d-473d9397c421.png)\r\n- After removing cross device nodes:\r\n![image](https://user-images.githubusercontent.com/10428324/80304824-29687f80-87eb-11ea-8712-bca355333d52.png)\r\n\r\nThe alternative way to solve this problem is to add more condition in `TryToContract`. However,  that will result in larger modification because we need to save the device information of all nodes in a cluster. If you prefer this way, we are also glad to help.\r\n\r\nThank you for your time on reviewing this pr.", "comments": ["> Is it possible to write a test for this?\r\n\r\nI'd love to do that. What I need is a test like the `XlaCompilationTest` in `mark_for_compilation_pass_test.cc`, right?", "@cheshire I found tests like `CreateCombinedCpuGpuClusters` and `ClusterResourceOpsWhenSafe` where cluster across device is allowed. Should XLA be able to cluster in this way? If that's the case, I need to change accordingly. The main purpose of this pr is to solve the TODO in `NOT_DontClusterSpreadingNodes`.", "> For example, this is the timeline of a transformer model\r\n\r\nBTW for BERT specifically we have found we were able to get better performance with `tf.function(experimental_compile=True)`: https://github.com/tensorflow/models/blob/master/official/nlp/modeling/layers/transformer.py#L228\r\n\r\n> Should XLA be able to cluster in this way? \r\n\r\nI'm actually not sure;\r\n @sanjoy any comments?\r\nBut in general if you are changing behavior it needs to be tested", "Generally we only allow one kind of cross device cluster: if one device is a GPU and another device is the CPU.  We don't allow a single cluster to have ops that are placed on different GPUs.\r\n\r\nBtw, your case should have been handled by [this logic](https://github.com/tensorflow/tensorflow/blob/37ab9c3bfcbb278ae003cf32f08b3d41a78401a7/tensorflow/compiler/jit/mark_for_compilation_pass.cc#L1375), do you know why it didn't?", "@sanjoy The `ClusteringWillIntroduceInterDeviceDependency` only deal with the cross device edge between XLA clusters and for my case, the \"op3\" is not in one of the clusters.\r\nI do think the code in this pr may remove too much ops and we need to come up with a better solution. Any thoughts on how to deal with the case when \"op3\" is not clustered? An alternative way I can think of is saving the output devices and input devices of all clusters to help with contraction.", "One nit to Sanjoy's comment. We don't have clusters using multiple devices as far as I know. But in the case of GPU/CPU, we will move the CPU ops to the GPU if possible to make a larger cluster.\r\n\r\nI also don't think the problem is that `ClusteringWillIntroduceInterDeviceDependency` only works with clusters at a basic level. It is generally assumed that any individual Op is much less expensive than a cluster of many ops. If OP2 and OP1 become CLUSTER2 and CLUSTER1, and OP2 depends on OP3 as in example 2, clustering of CLUSTER2 and CLUSTER1 shouldn't cause much problems because it is assumed that OP3 is relatively cheap (which is not assumed if OP3 is instead CLUSTER3). This isn't always reality because maybe OP3 has CLUSTER4 as an input, in which case there suddenly becomes a transitive dependency from CLUSTER4 -> (CLUSTER1, CLUSTER2) when previously it was only CLUSTER4 -> CLUSTER2. This new solution does catch that case and helps, but at the same time, if OP3 is always ReadVariable and doesn't depend on CLUSTER4, then this change is hurting performance.\r\n\r\nRegarding this approach, `ClusteringWillIntroduceInterDeviceDependency` only handles the second example because the straighforward way of handling the first case caused large regressions in multi-gpu resnet through breaking clusters into many pieces. Since this approach is the same but potentially breaking clusters even more, I would expect the same result.\r\n\r\nI'll try to think about this more and respond, but generally I expect this won't be acceptable as is because breaking apart clusters in this way will make some layouts improve and some layouts regress (and in this case I expect resnet to regress which is important). That is the general result with all of this autoclustering though because it is impossible to have a generally optimal clustering function without a cost function. And I'm not even sure if it's possible to have an optimal solution with reasonable compile time even if we assume all nodes cost the same amount of time.  This is all my long winded way of saying that I don't know of an ideal solution, and because I have constructed pros and cons for every clustering conditions I've thought of so far, I would generally require/settle for proof that the most common models are generally improving with the autoclustering changes.\r\n\r\nAlso, if you haven't tried `TF_XLA_FLAGS=--tf_xla_auto_jit=fusible` I would recommend trying it and seeing if this alternative clustering strategy helps in this case.", "@sanjoy  Any update on this PR? Please. Thanks!", "The benchmark where a similar change caused problems did not show problems with this change, so I will run a larger suite of benchmarks tomorrow to confirm nothing else has problems, but presumably they won't show any problems as they're for 1-gpu models, and we can review this and look into submitting it.\r\n\r\nJust be warned that changes like these around autoclustering have a larger chance of triggering complaints after at which point we might have to rollback if other important models are found to have large regressions with this change.", "@tpopp \r\nThank you for your benchmarking and review. I've changed the code to `absl::c_any_of`. Could you have another look?\r\n> Just be warned that changes like these around autoclustering have a larger chance of triggering complaints after at which point we might have to rollback if other important models are found to have large regressions with this change.\r\n\r\nI totally understand and agree with this.\r\n\r\n", "The code looks good.\r\n\r\nThe one issue now is that 11 test cases fail because less clustering happens now in `tensorflow/compiler/jit:compilation_passes_test` I don't have a good solution for a quick fix to these tests.\r\n\r\nNOT_DontClusterSpreadingNodes Also should become DontClusterSpreadingNodes and the last check would be inverted.", "@tpopp \r\nI've checked the test failures. \r\n3 of them is due the nodes connected to `_SOURCE` or `_SINK` which may have different device assigned and can be fix by ignoring the edges with `_SOURCE` or `_SINK` involve. The remaining are:\r\n```\r\n[==========] 135 tests from 12 test suites ran. (16741 ms total)\r\n[  PASSED  ] 127 tests.\r\n[  FAILED  ] 8 tests, listed below:\r\n[  FAILED  ] XlaCompilationTest.CallXlaDeviceFuncWithResourceOp\r\n[  FAILED  ] XlaCompilationTest.TensorArrayShapeOnXlaDevice\r\n[  FAILED  ] XlaCompilationTest.DontClusterMergingNodes\r\n[  FAILED  ] XlaCompilationTest.DontClusterMergingNodesOnCPU\r\n[  FAILED  ] XlaCompilationTest.NOT_DontClusterSpreadingNodes\r\n[  FAILED  ] XlaCompilationTest.CreateCombinedCpuGpuClusters\r\n[  FAILED  ] XlaCompilationTest.ClusterResourceOpsWhenSafe\r\n[  FAILED  ] XlaCompilationTest.ClusterShapeConsumerWithProducer\r\n```\r\nThe reasons of error in them are:\r\n- `CallXlaDeviceFuncWithResourceOp`: `/job:worker/replica:0/task:0/device:XLA_CPU:0` is not compatible with `/job:localhost/replica:0/task:0/cpu:0`.\r\n- `TensorArrayShapeOnXlaDevice`: `placeholder` is on cpu while `test/reshape` is on gpu, this pr will remove `test/reshape` from the cluster.\r\n- `DontClusterMergingNodes` and `NOT_DontClusterSpreadingNodes` are meant to be changed for this pr.\r\n- `CreateCombinedCpuGpuClusters`: `test/x` has connection with cpu while it is on gpu. This also has the problem whether we will allow clusters that cross cpu and gpu.\r\n- `ClusterResourceOpsWhenSafe`: `test/b` is a cross-cpu-and-gpu op.\r\n- `ClusterShapeConsumerWithProducer`: `test/y` is a cross-cpu-and-gpu op.\r\n\r\nI think we may need to change 7 of them and figure out a way to make \"cpu:0\" and \"XLA_CPU\"0\" compatible. I wonder if those two device will appear at the same time in production?\r\n\r\nAs for the `_SOURCE` and `_SINK` related condition, I've tried with\r\n```c++\r\n    // skip nodes that have cross device edges.\r\n    auto is_cross_device_edge = [](const Edge* e) {\r\n      auto src = e->src();\r\n      if (src->name() == \"_SOURCE\")\r\n        return false;\r\n      auto dst = e->dst();\r\n      if (dst->name() == \"_SINK\")\r\n        return false;\r\n      return !src->assigned_device_name().empty() &&\r\n             !dst->assigned_device_name().empty() &&\r\n             src->assigned_device_name() != dst->assigned_device_name();\r\n    };\r\n```", ">This also has the problem whether we will allow clusters that cross cpu and gpu.\r\n\r\nI'm actually surprised that I didn't see benchmark regressions now that I think about this. I personally would vote for still making CPUs special and allowing them through because there was good reason at the time, but I won't block you doing otherwise if I don't see a benchmark regression (like was currently the case).\r\n\r\n> and figure out a way to make \"cpu:0\" and \"XLA_CPU\"0\" compatible. I wonder if those two device will appear at the same time in production?\r\n\r\nThose two devices can occur together, but I don't think they can to be clustered. In `CallXlaDeviceFuncWithResourceOp` It looks like just an unfortunate case of not having enough ops to be clustered now that the ops along the edge of a device boundary aren't clustered. In theory, it would be nice to realize they're both on CPU, but that's more a shortcoming of TF->XLA's symbolic devices, and I don't think you need to feel responsible to handle that because I suspect it would involve either a lot of work or a hardcoded check between the names. (I might have misinterpreted the problem though)\r\n\r\nThere should be:\r\n`src->IsSource()` and `src->IsSink()` and an `src->IsOp()` (which returns Not (IsSink() || IsSource()) ) by the way.", "@tpopp \r\nI've changed the test according to this pr. Right now there should be no building failure related to the new cluster rule.\r\n> I personally would vote for still making CPUs special and allowing them through because there was good reason at the time, but I won't block you doing otherwise if I don't see a benchmark regression (like was currently the case).\r\n\r\nFor simplicity, I remain removing all cpu ops because it may require the same \"hardcoded check between the names\".", "Dedicated hardware? I love it.\n\nOn Mon, Jun 15, 2020, 5:55 PM Tres <notifications@github.com> wrote:\n\n> *@tpopp* requested changes on this pull request.\n>\n> Sorry for the delay. I had some holidays last week.\n>\n> I don't know how this github approval works. A few of these test cases can\n> be deleted now and then it looks good to go.\n> ------------------------------\n>\n> In tensorflow/compiler/jit/mark_for_compilation_pass_test.cc\n> <https://github.com/tensorflow/tensorflow/pull/38911#discussion_r440257135>\n> :\n>\n> > @@ -322,7 +322,7 @@ TEST(XlaCompilationTest, CallXlaDeviceFuncWithResourceOp) {\n>        MarkForCompilationPassTestHelper::MarkForCompilation(&graph, &flib_def));\n>    auto clusters = GetClusters(*graph);\n>\n> -  EXPECT_NE(clusters[\"A\"], \"\");\n> +  EXPECT_EQ(clusters[\"A\"], \"\");\n>\n> You can delete this test since we will now not compile functions with\n> resource variables as input.\n> ------------------------------\n>\n> In tensorflow/compiler/jit/mark_for_compilation_pass_test.cc\n> <https://github.com/tensorflow/tensorflow/pull/38911#discussion_r440260473>\n> :\n>\n> >    std::unordered_map<string, string> clusters = GetClusters(*graph);\n> -  EXPECT_NE(clusters[\"MatMul0_dev0\"], clusters[\"MatMul1_dev1\"]);\n> -  EXPECT_NE(clusters[\"MatMulCombined_cpu\"], clusters[\"MatMul0_dev0\"]);\n> -  EXPECT_NE(clusters[\"MatMulCombined_cpu\"], clusters[\"MatMul1_dev1\"]);\n> -  EXPECT_EQ(clusters[\"A_dev0\"], clusters[\"MatMul0_dev0\"]);\n> -  EXPECT_EQ(clusters[\"B_dev1\"], clusters[\"MatMul1_dev1\"]);\n> +  EXPECT_EQ(clusters[\"MatMulCombined_cpu\"], \"\");\n> +  EXPECT_EQ(clusters[\"MatMul0_dev0\"], \"\");\n> +  EXPECT_EQ(clusters[\"MatMul1_dev1\"], \"\");\n>  }\n>\n>  // TODO(b/117085735): This form of clustering should be prevented.\n>\n> You can delete this TODO and remove the \"NOT_\"\n> ------------------------------\n>\n> In tensorflow/compiler/jit/mark_for_compilation_pass_test.cc\n> <https://github.com/tensorflow/tensorflow/pull/38911#discussion_r440262279>\n> :\n>\n> > @@ -1379,7 +1371,7 @@ TEST(XlaCompilationTest, CreateCombinedCpuGpuClusters) {\n>\n>    std::unordered_map<string, string> clusters = GetClusters(*graph);\n>\n> -  EXPECT_NE(clusters[\"test/x\"], \"\");\n> +  EXPECT_EQ(clusters[\"test/x\"], \"\");\n>\n> You can delete this test case, now that it's not supported (this is just\n> checking that nothing was clustered).\n> ------------------------------\n>\n> In tensorflow/compiler/jit/mark_for_compilation_pass_test.cc\n> <https://github.com/tensorflow/tensorflow/pull/38911#discussion_r440267896>\n> :\n>\n> > @@ -1451,8 +1443,7 @@ TEST(XlaCompilationTest, ClusterResourceOpsWhenSafe) {\n>\n>    std::unordered_map<string, string> clusters = GetClusters(*graph);\n>\n> -  EXPECT_NE(clusters[\"test/b\"], \"\");\n> -  EXPECT_EQ(clusters[\"test/b\"], clusters[resource_read_name]);\n> +  EXPECT_EQ(clusters[\"test/b\"], \"\");\n>\n> Can you add a TODO(tpopp): See if this test case should be deleted or\n> changed\n> ------------------------------\n>\n> In tensorflow/compiler/jit/mark_for_compilation_pass_test.cc\n> <https://github.com/tensorflow/tensorflow/pull/38911#discussion_r440270333>\n> :\n>\n> > @@ -1562,9 +1553,9 @@ TEST(XlaCompilationTest, ClusterShapeConsumerWithProducer) {\n>\n>    std::unordered_map<string, string> clusters = GetClusters(*graph);\n>\n> -  EXPECT_NE(clusters[\"test/y\"], \"\");\n> -  EXPECT_EQ(clusters[\"test/x\"], clusters[\"test/y\"]);\n> -  EXPECT_NE(clusters[\"test/z\"], clusters[\"test/y\"]);\n> +  EXPECT_EQ(clusters[\"test/y\"], \"\");\n>\n> This is also checking that nothing is clustered, so it can be deleted.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/38911#pullrequestreview-430751763>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIT525RGG3YN75B6FBNOJV3RWY76FANCNFSM4MRFLIWQ>\n> .\n>\n", "@tpopp \r\nI've removed those tests. Could you have another look :).", "@zhuzilin Can you please fix build failures ? Thanks!", "@gbaned I'd love to.\r\n@tpopp I've just checked the building errors. All of them seem to have nothing to do with the change made in this pr. Instead, they have triggered the following error:\r\n```\r\n2020-06-16 12:07:21.049677: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at xla_compile_on_demand_op.cc:209 : Invalid argument: Unsupported type in DataTypeToPrimitiveType: 'variant'\r\nFatal Python error: Segmentation fault\r\n```\r\nDo you have any idea the cause of this problem? Thank you :).", "Compile on Android tablet?\n\nOn Tue, 16 Jun 2020, 15:57 Zilin Zhu, <notifications@github.com> wrote:\n\n> @gbaned <https://github.com/gbaned> I'd love to.\n> @tpopp <https://github.com/tpopp> I've just checked the building errors.\n> All of them seem to have nothing to do with the change made in this pr.\n> Instead, they have triggered the following error:\n>\n> 2020-06-16 12:07:21.049677: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at xla_compile_on_demand_op.cc:209 : Invalid argument: Unsupported type in DataTypeToPrimitiveType: 'variant'\n> Fatal Python error: Segmentation fault\n>\n> Do you have any idea the cause of this problem? Thank you :).\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/38911#issuecomment-644782869>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIT525T6TQ5U3TOKLWZMJQDRW523PANCNFSM4MRFLIWQ>\n> .\n>\n", "I didn't get time today to look into this very deeply but did confirm it's from this PR.\r\n\r\n I think the current location of the check is preventing ops from being compiled that have AlwaysCompile semantics, and this check needs to be moved to inside the CrossDeviceDependency check or similar to still create 1 op clusters when needed. I could be wrong, but that's my guess. ", "@tpopp \r\nSorry that I'm not familiar with the \"AlwaysCompile\" semantic... However, I found out that there are test errors for both gpu and cpu tests (like add_n_test_cpu) and for cpu ones, I think there shouldn't be any cross device edges.\r\nBTW, could you tell me how to run the `add_n_test`? I tried to use bazel build and run the binary but it exited with:\r\n```\r\n  File \"/root/ttensorflow/bazel-bin/tensorflow/compiler/tests/add_n_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py\", line 89, in __init__\r\n    for name in FLAGS.types.split(',')\r\nAttributeError: 'NoneType' object has no attribute 'split'\r\n```\r\nThe error is caused by the flag in `xla_test.py`, but I couldn't pass the flag to the binary...", "If `bazel run` or `bazel test` work for you, I would recommend that. These tests need additional arguments that are encoded in the BUILD and bzl files and are automatically added. In this case, it looks like you would need something like `--test_device=XLA_GPU --types=DT_HALF,DT_FLOAT,DT_DOUBLE,DT_UINT8,DT_QUINT8,DT_INT8,DT_QINT8,DT_INT32,DT_QINT32,DT_INT64,DT_BOOL,DT_COMPLEX64,DT_COMPLEX128`\r\n\r\nRegarding AlwaysCompile, devices in Tensorflow don't have to correspond to just concepts like \"CPU\" and \"GPU\".  If you look here (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/jit/mark_for_compilation_pass.cc#L1629), you'll see `registration->autoclustering_policy` and `XlaOpRegistry::AutoclusteringPolicy::kAlways` which is a concept that you might have a device \"XLA_GPU\" device which requires that all ops on it be compiled, or you might take a specific node in the tensorflow graph and set it to always be compiled as checked by `cluster.is_xla_compile_attr_true()`.\r\n\r\nSo I believe in this case, the operations are being forcefully put on \"XLA_CPU\" and \"XLA_GPU\", but then this new code prevents them from ever being put in a cluster, so they are never compiled like we want. What should be acceptable instead is putting all these ops in clusters, but then possibly never combining them with other clusters. Then the cluster is only the single op, so no dependencies are created like you desire with this PR, but they are still in a \"cluster\" which is compiled like the devices require.", "One thing I forgot to clarify. In this example, the CPU test still fails because \"CPU\" and \"XLA_CPU\" are different \"devices\" even though they will end up running on the same hardware.", "@zhuzilin Can you please check @tpopp's comments and keep us posted. Thanks!", "@gbaned Sorry for the delay...\r\n@tpopp Thank you for these detailed explanation. I also agree that allowing those ops with cross-device edges inside a cluster but stopping them from merging may be a better way to walk around these tests. In that case, we will need to add a new attribute to `Cluster` that shows the input devices and output devices of it. Any if we separate the input devices and output devices, we can allow cluster A and B be merged when only B has a cross device output and A is an input of B. Any suggestion on the design? or maybe we need a new pr thread for that?", "Hi,\r\n\r\nsorry for the delay in response on my side. I haven't had time to think this through, so I can't say how I would solve this if I were implementing this. After checking with cheshire though, I think the issue is that we can't have a VARIANT type as a value living out of a cluster, so the issue might not be specifically the lack of ops being clustered, but the test expecting certain ops to not be on the edge of a cluster.\r\n\r\nI think your idea for the clusters work. I also think the performance will not be an issue if you walk through all the input/output nodes to a cluster each time and look at their devices, though I could be wrong.\r\n\r\nRegarding a new PR, I don't know what Tensorflow's policy is on that. If no one complains, I'm not going to complain though.", "Ok. I'll close this pr for now and will propose another design."]}, {"number": 38910, "title": "Simple loop is slow", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 1080Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTrivial loop is 300 times slower than in pure python or pytorch, note that the ratio doesn't change with the size of the loop.\r\nNote that I do get a speed up by implementing the loop using while_loop syntax, but it's far (still >250 times slower) from catching up.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n# TO BE RUN IN A NOTEBOOK\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.config.set_visible_devices([], 'GPU')\r\n\r\n@tf.function\r\ndef tf_loop(n_iter):\r\n    val = 0.\r\n    for i in tf.range(n_iter):\r\n        val += 0.\r\n    return val\r\n\r\ndef loop(n_iter):\r\n    val = 0.\r\n    for i in range(n_iter):\r\n        val += 0.\r\n    return val\r\n\r\nn_iter = tf.constant(10000)\r\n%timeit tf_loop(n_iter) # 92.3 ms \u00b1 566 \u00b5s\r\n%timeit loop(10000) # 288 \u00b5s \u00b1 4.31 \u00b5s\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["i am able to replicate the issue faced, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/f3b781e4b903c2cd935b46b5d748ffc9/untitled158.ipynb).", "@AdrienCorenflos When you say trivial loop, do you mean a regular `for i in range` inside `tf.function`?", "See #34500 for a  very similar discussion. The gist of the story is that the default TensorFlow runtime is not efficient for scalar computations. Enabling XLA compilation with `@tf.function(experimental_compile=True)` can greatly improve performance in cases like this. Note that the overhead of calling tf.function alone has a fixed cost of around 200 us."]}, {"number": 38909, "title": "Sampling uniforms is slow", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\nSampling uniforms is 100 times slower in tensorflow on GPU than in pytorch, it's 30 times slower on CPU. \r\n\r\n**Standalone code to reproduce the issue**\r\n# TO BE RUN IN A NOTEBOOK\r\n```python\r\nimport tensorflow as tf\r\nimport torch\r\n# tf.config.set_visible_devices([], 'GPU') # if no GPU tensorflow is now 30 times slower\r\n%timeit tf.random.uniform([], 0., 1.) # 310 \u00b5s \u00b1 3.53 \u00b5s per loop\r\n%timeit torch.rand([]) # 2.04 \u00b5s \u00b1 38.7 ns per loop\r\n``` ", "comments": ["I have tried on colab with TF version 2.1.0., 2.2-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2bcf67f312eb7e055462658a3129ac14/untitled829.ipynb).Thanks!\r\n", "Hi,\r\n\r\nAs an FYI, this behaviour persists when asking for larger samples. This is a table of the log-time (log seconds) it took to sample a uniform of a certain log-size. For most practical purposes (<1M samples) tensorflow is severely underperforming. Note that this is not improved by decorating the call to random nor is by using stateless sampling. I tried using THREEFRY to see if the otherhead was coming from PHILOX, but it raises.\r\n\r\n|   log-size |    torch |   tensorflow |\r\n|-----------:|---------:|-------------:|\r\n|    0       | -5.62487 |     -3.92051 |\r\n|    1       | -5.59297 |     -3.92344 |\r\n|    1.30103 | -5.57734 |     -3.92785 |\r\n|    1.62325 | -5.52077 |     -3.9329  |\r\n|    1.94448 | -5.51241 |     -3.91702 |\r\n|    2.26245 | -5.43938 |     -3.92665 |\r\n|    2.57864 | -5.29286 |     -3.92046 |\r\n|    2.89432 | -5.12171 |     -3.85816 |\r\n|    3.21032 | -4.90156 |     -3.8801  |\r\n|    3.52621 | -4.64741 |     -3.80525 |\r\n|    3.84205 | -4.35693 |     -3.75417 |\r\n|    4.15788 | -4.04589 |     -3.64094 |\r\n|    4.47368 | -3.7245  |     -3.37539 |\r\n|    4.78947 | -3.41472 |     -3.21007 |\r\n|    5.10526 | -3.08295 |     -2.8371  |\r\n|    5.42105 | -2.70882 |     -2.63069 |\r\n|    5.73684 | -2.43453 |     -2.39153 |\r\n|    6.05263 | -2.13015 |     -2.09891 |\r\n|    6.36842 | -1.8209  |     -1.799   |\r\n|    6.68421 | -1.44797 |     -1.45077 |\r\n|    7       | -1.14071 |     -1.10838 |", "@AdrienCorenflos \r\nplease refer to [this gist](https://colab.research.google.com/gist/Saduf2019/2870c0a2de935e5d7a775dd9a155fbe9/untitled602.ipynb) and let us know if the latest tf version helps.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38908, "title": "    TypeError: len is not well defined for symbolic Tensors. (transpose:0) Please call `x.shape` rather than `len(x)` for shape information.", "body": "I'm try to make my `padd_spectrograms`  function execute faster. So I add `@tf.function` but when I add `@tf.function` decorator I'm getting ` TypeError: len is not well defined for symbolic Tensors. (transpose:0) Please call `x.shape` rather than `len(x)` for shape information.`\r\n\r\nI'm using `colab tensorflow 2.x`\r\n\r\nIf I do not use `@tf.function` it works without any error\r\n\r\n```python\r\n@tf.function\r\ndef padd_spectrograms(spectogram, padd_len):\r\n    t = tf.transpose(spectogram) # feature x timestep -> timestep x feature\r\n    p = tf.keras.preprocessing.sequence.pad_sequences(\r\n        t, maxlen=padd_len, dtype='float', padding='post', truncating='post')\r\n    return tf.transpose(p) # back to feature x timestep\r\n```\r\n```python\r\nTypeError: in user code:\r\n\r\n    /content/utils/helper.py:83 padd_spectrograms  *\r\n        p = tf.keras.preprocessing.sequence.pad_sequences(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/sequence.py:158 pad_sequences  **\r\n        padding=padding, truncating=truncating, value=value)\r\n    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/sequence.py:56 pad_sequences\r\n        num_samples = len(sequences)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:754 __len__\r\n        \"shape information.\".format(self.name))\r\n\r\n    TypeError: len is not well defined for symbolic Tensors. (transpose:0) Please call `x.shape` rather than `len(x)` for shape information.\r\n```\r\n\r\nThanks", "comments": ["@menon92,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here.\r\n\r\nYou can also share your Colab gist with us. Select 'File' -> 'Save a copy as Github Gist', and share the link of the new window. Thanks!", "@amahendrakar \r\nIt's very hard to share a Colab gist because, I'm using an external script helper.py which contain padd_spectrograms method as a part of data generation", "@menon92,\r\nIs it possible for you to share a minimal standalone code to reproduce the error? Thanks!", "@amahendrakar ,\r\nIs it possible to apply `@tf.function` to `tf.keras.preprocessing.sequence.pad_sequences()` ? Please correct me if I'm wrong.\r\nThanks", "@amahendrakar ,\r\nI think there is an issue with `tf.keras.preprocessing.sequence.pad_sequences()` . I replace `.pad_sequences` with `tf.pad()` and it works without any error.\r\n\r\n```python\r\n@tf.function\r\ndef padd_spectrograms(spectogram, padd_len):\r\n    t = tf.transpose(spectogram) # feature x timestep -> timestep x feature\r\n    paddings = tf.constant([[0, 0,], [0, padd_len-t.shape[1]]])\r\n    p = tf.pad(t, paddings, \"CONSTANT\")\r\n    return tf.transpose(p) # back to feature x timestep\r\n```\r\nCould you please investigate graph mode with `tf.keras.preprocessing.sequence.pad_sequences()`\r\n\r\nThanks", "> Could you please investigate graph mode with `tf.keras.preprocessing.sequence.pad_sequences()`\r\n\r\n@menon92,\r\nI was able to apply `@tf.function` to `tf.keras.preprocessing.sequence.pad_sequences()` without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/75b69759473fd025668ac9ed048ed909/38908.ipynb). Thanks!", "@amahendrakar,\r\nI'm finally able to reproduce the issue please check the gist of it [here](https://gist.github.com/menon92/8458fbde8d10d9705c51e97c7ae9d228) . Thanks", "Was able to reproduce the error with [TF v2.2.0-rc4](https://colab.research.google.com/gist/amahendrakar/3551b5ad96a2c3f5d99e8e485cb57c0b/38908-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/cf2a8cd19b814b13d309165ffc9a405a/38908-tf-nightly.ipynb).\r\n\r\nWhereas running the code with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/ea72f33f5d78cef814915d4001d76160/38908-2-1.ipynb) throws an error stating `    NotImplementedError: Cannot convert a symbolic Tensor (TensorArrayV2Read/TensorListGetItem:0) to a numpy array`. Please find the attached gist. Thanks!", "`keras.preprocessing.pad_sequences` expects a list of lists, and doesn't work with tensor inputs, which is what gets produced by `tf.transpose`. Using `tf.pad` is the right workaround in this case.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38908\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38908\">No</a>\n", "Why is the issue closed? Is `tf.keras.preprocessing.sequence.pad_sequences()` not intended to be used in graph execution mode? Having a padding function to use in `.map()` for time series models is quite nice.", "> \r\n\r\nit seems pad_sequences cannot do broadcasting in autograph. the following works in eager:\r\n\r\n```\r\ndef pad_function(x):\r\n    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(x,padding='post')\r\n    return padded_inputs\r\n\r\npad_function([tf.constant([[1, 2, 3], [1, 2, 3]]), tf.constant([[1, 2, 3]])])\r\n\r\n>> output:\r\narray([[[1, 2, 3],\r\n        [1, 2, 3]],\r\n\r\n       [[1, 2, 3],\r\n        [0, 0, 0]]], dtype=int32)\r\n```\r\n\r\nHowever, when having the `@tf.function` decorator, it does not work:\r\n\r\n```\r\n@tf.function\r\ndef pad_function(x):\r\n    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(x,padding='post')\r\n    return padded_inputs\r\n```\r\n\r\nOutput:\r\n```\r\nValueError: `sequences` must be a list of iterables. Found non-iterable: Tensor(\"x:0\", shape=(2, 3), dtype=int32)\r\n```"]}, {"number": 38907, "title": "%%bash cd models/research/ protoc object_detection/protos/*.proto --python_out= ./object_detection/protos/anchor_generator_pb2.py: Permission denied", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["show the error when i run it \r\n./object_detection/protos/anchor_generator_pb2.py: Permission denied\r\n\r\nCalledProcessError                        Traceback (most recent call last)\r\n<ipython-input-7-99b2dbacc58a> in <module>\r\n----> 1 get_ipython().run_cell_magic('bash', '', 'cd models/research/\\nprotoc object_detection/protos/*.proto --python_out=.\\n')\r\n\r\n/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell)\r\n   2360             with self.builtin_trap:\r\n   2361                 args = (magic_arg_s, cell)\r\n-> 2362                 result = fn(*args, **kwargs)\r\n   2363             return result\r\n   2364 \r\n\r\n/usr/lib/python3/dist-packages/IPython/core/magics/script.py in named_script_magic(line, cell)\r\n    140             else:\r\n    141                 line = script\r\n--> 142             return self.shebang(line, cell)\r\n    143 \r\n    144         # write a basic docstring:\r\n\r\n<decorator-gen-110> in shebang(self, line, cell)\r\n\r\n/usr/lib/python3/dist-packages/IPython/core/magic.py in <lambda>(f, *a, **k)\r\n    185     # but it's overkill for just that one bit of state.\r\n    186     def magic_deco(arg):\r\n--> 187         call = lambda f, *a, **k: f(*a, **k)\r\n    188 \r\n    189         if callable(arg):\r\n\r\n/usr/lib/python3/dist-packages/IPython/core/magics/script.py in shebang(self, line, cell)\r\n    243             sys.stderr.flush()\r\n    244         if args.raise_error and p.returncode!=0:\r\n--> 245             raise CalledProcessError(p.returncode, cell, output=out, stderr=err)\r\n    246 \r\n    247     def _run_script(self, p, cell, to_close):\r\n\r\nCalledProcessError: Command 'b'cd models/research/\\nprotoc object_detection/protos/*.proto --python_out=.\\n'' returned non-zero exit status 1.\r\n\r\n\r\n\r\n", "Could not install packages due to an EnvironmentError: [('/root/Desktop/models/research/include', '/tmp/pip-req-build-jdgyFf/include', \"[Errno 13] Permission denied: '/root/Desktop/models/research/include'\"), ('/root/Desktop/models/research/readme.txt', '/tmp/pip-req-build-jdgyFf/readme.txt', \"[Errno 13] Permission denied: '/root/Desktop/models/research/readme.txt'\"), ('/root/Desktop/models/research/bin', '/tmp/pip-req-build-jdgyFf/bin', \"[Errno 13] Permission denied: '/root/Desktop/models/research/bin'\")]", "@Tborth \r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nyou may refer to [this link](https://github.com/googlesamples/assistant-sdk-python/issues/236) for assistance\r\n\r\n", "@Tborth\r\nMoving this issue to closed status due to lack of activity."]}, {"number": 38906, "title": "MemoryOptimizer produces broken graph with AlreadyExistsError exception while running GRU layer on Tensorflow 2.2.0rc_3", "body": "**System information**\r\n- Custom model built using keras\r\n- MacBook Pro, 8-Core Intel Core i9, macOS Catalina 10.15.4 \r\n- TensorFlow installed from `pip` in virtual environment\r\n- TensorFlow `v2.2.0-rc2-77-gaad398b5e9 2.2.0-rc3`\r\n- Python 3.7.5\r\n- Running on CPU\r\n\r\n**Describe the current behavior**\r\nThe code snippet listed below outputs multiple `tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource` warnings and finally exists with `tensorflow.python.framework.errors_impl.AlreadyExistsError` exception.\r\n\r\nNote: the code works correctly if the GRU layer size is decreased from `320` to `80`. It also works if TensorFlow is downgraded to version `2.0.1`.\r\n\r\nThe issue is related to https://github.com/tensorflow/tensorflow/issues/23780 issue reported in 2018. This issue offers code to reproduce it and occurs on the latest version of TensorFlow.\r\n\r\n**Describe the expected behavior**\r\nThe code should work without exception.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Flatten, Bidirectional, GRU\r\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D\r\n\r\nx = np.random.rand(1000, 401, 17)\r\ny = np.random.choice([0, 1], size=(1000, 301))\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv1D(filters=320, kernel_size=26, activation='relu', input_shape=(401, x.shape[2])))\r\nmodel.add(MaxPooling1D(pool_size=13, strides=13))\r\nmodel.add(Bidirectional(GRU(320, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(2000, activation=\"relu\"))\r\nmodel.add(Dense(301, activation=\"sigmoid\"))\r\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\r\n\r\nmodel.summary()\r\n\r\nmodel.fit(x=x, y=y, epochs=1, verbose=1)\r\n```\r\n\r\nThe Google Colab notebook is available [here](https://colab.research.google.com/drive/1YohTA6Hxi3H6aOQgFj34C0tKErW2V_rL). The error is reproducible.\r\n\r\n**Other info / logs** \r\nThe code above generates following output:\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv1d (Conv1D)              (None, 376, 320)          141760    \r\n_________________________________________________________________\r\nmax_pooling1d (MaxPooling1D) (None, 28, 320)           0         \r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 28, 640)           1232640   \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 17920)             0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 2000)              35842000  \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 301)               602301    \r\n=================================================================\r\nTotal params: 37,818,701\r\nTrainable params: 37,818,701\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2020-04-26 10:19:57.349570: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2020-04-26 10:19:57.363399: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_7/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2020-04-26 10:19:57.377361: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n... (repeated multiple times) ...\r\n2020-04-26 10:19:57.677304: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/forward_gru/while/sequential/bidirectional/forward_gru/while_grad/body/_577/gradients/AddN_7/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\nTraceback (most recent call last):\r\n  File \"alreadyexists_err.py\", line 21, in <module>\r\n    model.fit(x=x, y=y, epochs=1, verbose=1)\r\n  File \"venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 851, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 644, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"venv/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError:  Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n\t [[{{node gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var}}]] [Op:__inference_train_function_7551]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.2.0-rc3 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/8652e1dae44721b792b47b6e9e45052c/untitled828.ipynb).Thanks!", "Adding @saxenasaurabh who works on core ops to provide more information about while loop.\r\n\r\n", "@rmlarsen mentioned [here](https://github.com/tensorflow/tensorflow/issues/23780#issuecomment-498334346) that this may be due to a bug in grappler generating non-unique names. @rmlarsen @ezhulenev do you know if this was fixed?", "I dug a bit further into this issue. This is what I found:\r\n\r\nIt's indeed an optimizer issue, namely the `ScopedAllocatorOptimizer` optimizer.\r\n\r\nInterestingly, you cannot turn this optimizer off using `tf.config.optimizer.set_experimental_options()`. The `scoped_allocator_optimization` key is treated as `Bool` at:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/109fd384bdb58fbc8e61848723157a43621fec26/tensorflow/python/eager/context.py#L958\r\nwhile it's interpreted as `int` at: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8e0eecc8e396f8c1859b1b3954a89a41da8b5b45/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L292\r\n\r\nShould I file another issue for that?", "Let me rename this bug to pointing to ScopedAllocatorOptimizer.", "Verified this is fixed with latest tf-nightly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38906\">No</a>\n"]}, {"number": 38905, "title": "Run a GitHub project on google Colabratory", "body": "I have a question, there is a GitHub project about text to speech with deep-learning but i can't neither run it on local machine nor google colaboratory,\r\n\r\ncan anyone help on how to run it? as i downloaded the files but it has many files i dont know how to run it.\r\n\r\nit's git hub link==>\r\n\r\nhttps://github.com/AlisterTA/Persian-text-to-speech\r\nThanks", "comments": ["@dantpy700 This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!"]}, {"number": 38904, "title": "fix tflite python whl package pybind11 build fail", "body": "Get the correct pybind11.h include path when build the tflite run time whl package.\r\nAs reported in this issue: https://github.com/tensorflow/tensorflow/issues/38903", "comments": ["Tei, could you take a look at this PR?", "Sorry for the delay - verified it works with multiple platforms :)"]}, {"number": 38903, "title": "[TFLite] TFlite failed to build the whl package", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: latest master branch [64c32cf5100fc8807fe6598bc5517c3456b68b97]\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 3.0.0\r\n- GCC/Compiler version (if compiling from source): 8.3\r\n\r\n**Describe the problem**\r\nFollowing the BKM[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package] to build tflite, failed with the error:\r\n\r\n_interpreter_wrapper/interpreter_wrapper_pybind11.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory\r\n #include \"pybind11/pybind11.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nerror: command 'gcc' failed with exit status 1_\r\n\r\nI have install pybind11 and numpy through pip. and I have checked pybind11.h do exsit in the PATH. suspect the bug relates with the build file.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nsudo apt install swig libjpeg-dev zlib1g-dev python3-dev python3-numpy\r\npip install numpy pybind11\r\nsh tensorflow/lite/tools/make/download_dependencies.sh\r\nsh tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n\r\n```\r\n\r\n", "comments": ["As in this issue https://github.com/tensorflow/tensorflow/issues/37639 mentions, swig has been removed and pybind11 would be used.", "@teijeong Hi, any comments on this issue", "Hi Leslie-Fang, sorry for the error :( we're working on getting pybind dependencies clean, meanwhile it's bit tricky to get proper include dir for pybind11. Current setup is configured to work on docker, but seems it's not stable in other environments.\r\n\r\nIf you can find location of headers for pybind11, try adding it on `setup.py` here:\r\nhttps://github.com/tensorflow/tensorflow/blob/6a701314351845ecbc6641ecd2f68e21b582c3f8/tensorflow/lite/tools/pip_package/setup.py#L134", "@teijeong Thanks for the comments. I have made a PR(https://github.com/tensorflow/tensorflow/pull/38904) to fix this issue. Could you help to have a review.\r\n", "@Leslie-Fang \r\nPlease confirm if we may move this to closed status.", "@Saduf2019  Thanks, Close and wait the PR got merged.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38903\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38903\">No</a>\n"]}, {"number": 38902, "title": "There are no control inputs between 'Assign' and 'read' nodes", "body": "I thought there should be some dependencies between 'Assign' and 'read' nodes, so that 'read' only executes after 'Assign' is done. But through the following toy example, this seems to be not the case:\r\n\r\n    import tensorflow as tf\r\n    a = tf.get_variable('a', shape = (2,3))\r\n\r\n    print ('op_name', 'control_inputs', 'input_ops', 'output[0]_shape')\r\n    for op in tf.get_default_graph().get_operations():\r\n      print (op.name, op.control_inputs, [inp.op.name for inp in op.inputs], op.outputs[0].shape)\r\n\r\noutput:\r\n\r\n    op_name control_inputs input_ops output[0]_shape\r\n    a/Initializer/random_uniform/shape [] [] (2,)\r\n    a/Initializer/random_uniform/min [] [] ()\r\n    a/Initializer/random_uniform/max [] [] ()\r\n    a/Initializer/random_uniform/RandomUniform [] ['a/Initializer/random_uniform/shape'] (2, 3)\r\n    a/Initializer/random_uniform/sub [] ['a/Initializer/random_uniform/max', 'a/Initializer/random_uniform/min'] ()\r\n    a/Initializer/random_uniform/mul [] ['a/Initializer/random_uniform/RandomUniform', 'a/Initializer/random_uniform/sub'] (2, 3)\r\n    a/Initializer/random_uniform [] ['a/Initializer/random_uniform/mul', 'a/Initializer/random_uniform/min'] (2, 3)\r\n    a [] [] (2, 3)\r\n    a/Assign [] ['a', 'a/Initializer/random_uniform'] (2, 3)\r\n    a/read [] ['a'] (2, 3)\r\n\r\nSo `a/Assign` and `a/read` nodes have no dependencies, and may execute in any order. Is there supposed to be any?", "comments": ["@zhao1157 \r\nI have tried in colab with 2.2.0-rc3.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/7b3b2cec0583130542c0c2c82aea4be9/untitled827.ipynb).I am not able to reproduce complete output to check the order.Thanks!", "@ravikyram Can you try it in TF-1.x, not TF-2?", "I have tried in colab with TF 1.15 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/f84bf3655d794b765d84fb29c0ca08c4/untitled836.ipynb).Thanks!", "TF1 doesn't add any dependencies you didn't add yourself. It's up to you to decide what order you want stuff to run in.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38902\">No</a>\n", "@alextp Can you clarify it a little more? When building a model, we usually don't specify the execution order of `Assign` and `read` since they are added by `TF` internally, and `TF` does not add control dependencies between them either. So how does `TF` decide which one should execute first? Hope you can comment on that a little. Thanks.", "The default assign and the default read are unordered. If you want ordered\nreads and writes you have to add them yourself with the control\ndependencies you need.\n\nOn Tue, Apr 28, 2020 at 6:21 PM Lianshui Zhao <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> Can you clarify it a little more?\n> When building a model, we usually don't specify the execution order of\n> Assign and read since they are added by TF internally, and TF does not\n> add control dependencies between them either. So how does TF decide which\n> one should execute first? Hope you can comment on that a little. Thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38902#issuecomment-620937829>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPWXKISGAIIUNRVEULRO56I3ANCNFSM4MRAMLOA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Thanks. But aren't they supposed to be ordered? `Variables` should be initialized by `Assign` before being `read`. Right?", "No, not really. Control dependencies only apply to the scope of a single\nsession.run, and if all reads depended on the initialization you'd\nreinitialize the variable on every call to session.run, not just the first\none, so you'd never be able to update any variable.\n\nOn Wed, Apr 29, 2020 at 10:50 PM Lianshui Zhao <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> Thanks. But aren't they supposed to\n> be ordered? Variables should be initialized by Assign before being read.\n> Right?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38902#issuecomment-621626737>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJC4FPBE5XYAFSO4R3RPEGQ7ANCNFSM4MRAMLOA>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 38901, "title": "Expected image (JPEG, PNG, or GIF), got unknown format starting with 'BM\\\"\\360\\003\\000\\000\\000\\000\\0006\\000\\000\\000(\\000' \t [[{{node DecodeJpeg}}]]", "body": "\r\n## System information\r\n**I have followed examples from tensorflow official website with some changes but essentially the same. I want to get an image generator that is efficient at its task of generating only images without caring about labels (its only cat images), -- AND I am using jupyter notebook.**\r\n\r\n- **OS Platform and Distribution - Linux Ubuntu 19.10**\r\n- **TensorFlow installed from conda using -- ```conda install tensorflow-gpu```**\r\n- **TensorFlow version == 2.1.0**\r\n- **Python version == 3.7.7**\r\n- **CUDA Toolkit version == 10.1.243**\r\n- **cuDNN version == 7.6.5**\r\n- **GPU model and memory == NVIDIA GTX 1050 (4GB)**\r\n\r\n\r\n\r\n## **Source code sniplet**:\r\n```python\r\ndata_dir = pathlib.Path('cat/')\r\nlist_ds = tf.data.Dataset.list_files(str(data_dir/'CAT/*.jpg'))\r\n\r\ndef parse_image(filename):\r\n    image = tf.io.read_file(filename)\r\n    image = tf.image.decode_jpeg(image)\r\n    image = tf.image.convert_image_dtype(image, tf.float32)\r\n    image = tf.image.resize(image, [64,64])\r\n    return image\r\n\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\n# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\r\nimages_ds = list_ds.map(parse_image, num_parallel_calls=AUTOTUNE)\r\n\r\nimages_ds = images_ds.prefetch(buffer_size=AUTOTUNE)\r\nimages_ds = images_ds.repeat()\r\nimages_ds = images_ds.cache().batch(BATCH_SIZE)\r\nimages_ds = images_ds.shuffle(buffer_size=1000)\r\n\r\nimage_batch = next(iter(images_ds)) ## This line causes error\r\n```\r\n\r\n\r\n\r\n### **I have seen this error pop up for many users on the internet but did not find a viable solution to it. Some suggested to check all the images if they were not corrupted. I used the below script but everything turned out to be fine.**\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\n\r\nfor i, file_name in enumerate(os.walk('cat/CAT/')):\r\n#     print(file_name[2])\r\n    try:\r\n        img = tf.io.read_file('cat/CAT/'+file_name[2][i])\r\n        img = tf.image.decode_jpeg(img)\r\n    except Exception as e:\r\n        print(e)\r\n        print('******************************************************')\r\n        print('{} - broken'.format(file_name[2][i]))\r\n```\r\n\r\n### **Please help me in this issue.**\r\n\r\n\r\n## Detailed Error\r\n```python\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/anaconda3/envs/sci/lib/python3.7/site-packages/tensorflow_core/python/eager/context.py in execution_mode(mode)\r\n   1896     ctx.executor = executor_new\r\n-> 1897     yield\r\n   1898   finally:\r\n\r\n~/anaconda3/envs/sci/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    658             output_types=self._flat_output_types,\r\n--> 659             output_shapes=self._flat_output_shapes)\r\n    660 \r\n\r\n~/anaconda3/envs/sci/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)\r\n   2478     except _core._NotOkStatusException as e:\r\n-> 2479       _ops.raise_from_not_ok_status(e, name)\r\n   2480   # Add nodes to the TensorFlow graph.\r\n\r\n~/anaconda3/envs/sci/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6605   # pylint: disable=protected-access\r\n-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6607   # pylint: enable=protected-access\r\n\r\n~/anaconda3/envs/sci/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Expected image (JPEG, PNG, or GIF), got unknown format starting with 'BM\\\"\\360\\003\\000\\000\\000\\000\\0006\\000\\000\\000(\\000'\r\n\t [[{{node DecodeJpeg}}]] [Op:IteratorGetNextSync]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-119-e1154546d6ba> in <module>\r\n----> 1 image_batch = next(iter(images_ds))\r\n\r\n~/anaconda3/envs/sci/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in __next__(self)\r\n    628 \r\n    629   def __next__(self):  # For Python 3 compatibility\r\n--> 630     return self.next()\r\n    631 \r\n    632   def _next_internal(self):\r\n\r\n~/anaconda3/envs/sci/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in next(self)\r\n    672     \"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\r\n    673     try:\r\n--> 674       return self._next_internal()\r\n    675     except errors.OutOfRangeError:\r\n    676       raise StopIteration\r\n\r\n~/anaconda3/envs/sci/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    663         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    664       except AttributeError:\r\n--> 665         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    666 \r\n    667   @property\r\n\r\n~/anaconda3/envs/sci/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)\r\n    128                 value = type()\r\n    129             try:\r\n--> 130                 self.gen.throw(type, value, traceback)\r\n    131             except StopIteration as exc:\r\n    132                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n~/anaconda3/envs/sci/lib/python3.7/site-packages/tensorflow_core/python/eager/context.py in execution_mode(mode)\r\n   1898   finally:\r\n   1899     ctx.executor = executor_old\r\n-> 1900     executor_new.wait()\r\n   1901 \r\n   1902 \r\n\r\n~/anaconda3/envs/sci/lib/python3.7/site-packages/tensorflow_core/python/eager/executor.py in wait(self)\r\n     65   def wait(self):\r\n     66     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 67     pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     68 \r\n     69   def clear_error(self):\r\n\r\nInvalidArgumentError: Expected image (JPEG, PNG, or GIF), got unknown format starting with 'BM\\\"\\360\\003\\000\\000\\000\\000\\0006\\000\\000\\000(\\000'\r\n\t [[{{node DecodeJpeg}}]]\r\n```", "comments": ["@kyteinsky,\r\nI was able to run the code without any issues using a sample dataset, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/158c4cc1eecf8e19183fbf5f9a7e2572/38901.ipynb#scrollTo=qJW2Xk6IgJFq). Could you please verify if the code works with another dataset? Thanks!", "@amahendrakar \nThank you sir for your time. If the code works for another dataset, can you suggest methods for checking out broken images in my dataset and removing them so that this error doesn't come up again.\n\n", "And what does **'BM ...'** message mean?", "> Thank you sir for your time. If the code works for another dataset, can you suggest methods for checking out broken images in my dataset and removing them so that this error doesn't come up again.\r\n\r\n@kyteinsky,\r\nAs the issue is with that particular dataset, unfortunately I will not be able to help with it. Alternatively, you can post this query on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) as there is a larger community that reads questions there. Thanks!", "Yeah sure, I'll lookup there for a solution.\n\nThank you for your help."]}, {"number": 38900, "title": "Include .h.inc file into the header files of the wheel package", "body": "While working on inlcuding some header files in compiler directory\r\nfrom a installed tf-nightly wheel package, we noticed that the\r\ngenerated header files from llvm (*.h.inc) in `tensorflow/compiler`\r\ndirectories are not packaged into the wheel package.\r\n\r\nAs a result, it is not possible to include the header files in `tensorflow/compiler`\r\nbecause they include `*.h.inc` as well.\r\n\r\nFor example, `mlir/tensorflow/ir/tf_ops.h` needs both `mlir/tensorflow/ir/tf_interfaces.h.inc`\r\nand `mlir/tensorflow/ir/tf_ops.h.inc\"\r\n\r\nThis PR add `.h.inc` into the packaged wheel file so that it is possible to use\r\nheader files in a locall install.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 38899, "title": "Fix exception handling of tf.histogram_fixed_width_bins", "body": "\r\nThis PR tries to address the issue in #29661 where\r\ntf.histogram_fixed_width_bins does not throw out an exception\r\nwhen (value_range[0] < value_range[1]) is not satified.\r\nThis is dfferent from the documentation specified in the docstring.\r\n\r\nThis is different from a similiar API `tf.histogram_fixed_width`\r\nwhere exception is thrown out correctly. The reason is that\r\n`tf.histogram_fixed_width_bins` is handled in python while\r\n`tf.histogram_fixed_width` has a C++ kernel.\r\n\r\nThis PR uses tf.Assert to make sure `(value_range[0] < value_range[1])`\r\nsatisty.\r\n\r\nThis PR fixes #29661.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Can you please check reviewer comments and keep us posted. Thanks!", "Thanks @alextp for the review. I have updated the PR. Now `tensor_util.constant_value` is used to resolve value if possible, and throws out a ValueError incase there is overlap of `value_range`. This is in graph compilation time and avoids runtime check. Let me know if this makes sense.", "Ah see the approve, that was quick. Thanks! \ud83d\udc4d ", "@yongtang Can you please address Ubuntu Sanity errors? Thanks!", "Thanks @alextp @gbaned The pylint has been fixed, sorry to update the PR again. "]}]