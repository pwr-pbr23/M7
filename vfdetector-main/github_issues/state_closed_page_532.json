[{"number": 37769, "title": "Distributed Tensorflow : Issue while starting/connecting to parameter server", "body": "I am trying to run a distributed tensorflow program with one parameter server and one worker over 2 vm each having gpu card but  it gives issues while connecting to parameter server with following 2 issues in the log:\r\n1.tensorflow/core/distributed_runtime/master.cc:268] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2.2020-03-20 17:34:11.395256: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nerror: error running IJS server: \"could not get remote execution state\"\r\n\r\nSystem Info : \r\nTensorflow-gpu = 1.14.0\r\ngpu - Tesla V100-SXM2\r\nUbuntu 18.04.4\r\n\r\nWith grpc debug enabled got the following logs:\r\nUsing TensorFlow backend.\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nWARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\nWARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\r\n\r\nWARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\r\n\r\nWARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nWARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\r\n\r\nWARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\r\n\r\nWARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n2020-03-20 17:34:00.582262: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-20 17:34:00.595624: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2020-03-20 17:34:01.159488: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x43f71d0 executing computations on platform CUDA. Devices:\r\n2020-03-20 17:34:01.159525: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2020-03-20 17:34:01.190018: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194895000 Hz\r\n2020-03-20 17:34:01.195769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3bd9420 executing computations on platform Host. Devices:\r\n2020-03-20 17:34:01.195833: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-03-20 17:34:01.198050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0000:0b:00.0\r\n2020-03-20 17:34:01.199864: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2020-03-20 17:34:01.202665: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2020-03-20 17:34:01.204955: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2020-03-20 17:34:01.206197: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2020-03-20 17:34:01.208933: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-03-20 17:34:01.211107: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-03-20 17:34:01.216280: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-03-20 17:34:01.220159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2020-03-20 17:34:01.220215: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2020-03-20 17:34:01.222970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-03-20 17:34:01.222996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2020-03-20 17:34:01.223010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2020-03-20 17:34:01.227719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13690 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0b:00.0, compute capability: 7.0)\r\nWARNING:tensorflow:From /dev/local/store_keras_graph.py:32: The name tf.train.write_graph is deprecated. Please use tf.io.write_graph instead.\r\n\r\nWARNING:tensorflow:From /dev/local/store_keras_graph.py:33: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\r\n\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:17: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\r\n\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:31: The name tf.train.Server is deprecated. Please use tf.distribute.Server instead.\r\n\r\n2020-03-20 17:34:11.395256: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nerror: error running IJS server: \"could not get remote execution state\"\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/user//vtensor/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:17: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\r\n\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:31: The name tf.train.Server is deprecated. Please use tf.distribute.Server instead.\r\n\r\n2020-03-20 17:34:12.093571: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-20 17:34:12.130334: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2020-03-20 17:34:12.362334: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3b74510 executing computations on platform CUDA. Devices:\r\n2020-03-20 17:34:12.362373: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2020-03-20 17:34:12.390397: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194855000 Hz\r\n2020-03-20 17:34:12.395027: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x40bcb60 executing computations on platform Host. Devices:\r\n2020-03-20 17:34:12.395049: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-03-20 17:34:12.396859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0000:0b:00.0\r\n2020-03-20 17:34:12.398523: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2020-03-20 17:34:12.400933: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2020-03-20 17:34:12.403149: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2020-03-20 17:34:12.404455: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2020-03-20 17:34:12.406953: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-03-20 17:34:12.409163: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-03-20 17:34:12.413758: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-03-20 17:34:12.416923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2020-03-20 17:34:12.416983: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2020-03-20 17:34:12.419143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-03-20 17:34:12.419161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2020-03-20 17:34:12.419185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2020-03-20 17:34:12.422384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 15023 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0b:00.0, compute capability: 7.0)\r\nD0320 17:34:12.423871920   55784 ev_posix.cc:170]            Using polling engine: poll\r\nD0320 17:34:12.423907616   55784 dns_resolver.cc:325]        Using native dns resolver\r\nI0320 17:34:12.425125694   55784 socket_utils_common_posix.cc:346] Disabling AF_INET6 sockets because ::1 is not available.\r\nI0320 17:34:12.425176554   55784 socket_utils_common_posix.cc:292] Enabling TCP_USER_TIMEOUT with a timeout of 20000 ms\r\nI0320 17:34:12.425247870   55784 tcp_server_posix.cc:308]    Failed to add :: listener, the environment may not support IPv6: {\"created\":\"@1584750852.425163790\",\"description\":\"Address family not supported by protocol\",\"errno\":97,\"file\":\"external/grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":379,\"os_error\":\"Address family not supported by protocol\",\"syscall\":\"socket\",\"target_address\":\"[::]:2225\"}\r\n2020-03-20 17:34:12.425340: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job ps -> {0 -> lgpbddgx03.gso.aexp.com:2224}\r\n2020-03-20 17:34:12.425355: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2225}\r\n2020-03-20 17:34:12.432092: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2225\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:49: The name tf.train.replica_device_setter is deprecated. Please use tf.compat.v1.train.replica_device_setter instead.\r\n\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:52: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\r\n\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:53: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:188: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\r\n\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:70: The name tf.train.create_global_step is deprecated. Please use tf.compat.v1.train.create_global_step instead.\r\n\r\nWARNING:tensorflow:From /user//vtensor/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1308: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:86: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:89: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\nWARNING:tensorflow:From /dev/local/tensorflow_backend.py:94: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\nD0320 17:34:15.655343415   56157 dns_resolver.cc:275]        Start resolving.\r\nI0320 17:34:15.658418088   56042 tcp_client_posix.cc:332]    CLIENT_CONNECT: ipv4:10.16.53.197:2224: asynchronously connecting fd 0x154c04009a30\r\nI0320 17:34:15.658539193   56045 tcp_client_posix.cc:150]    CLIENT_CONNECT: ipv4:10.16.53.197:2224: on_writable: error=\"No Error\"\r\nI0320 17:34:15.658636266   56045 tcp_client_posix.cc:113]    CLIENT_CONNECT: ipv4:10.16.53.197:2224: on_alarm: error=\"Cancelled\"\r\nI0320 17:34:15.658682435   56045 subchannel.cc:973]          Connect failed: {\"created\":\"@1584750855.658611519\",\"description\":\"Failed to connect to remote host: Connection refused\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":207,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.16.53.197:2224\"}\r\nI0320 17:34:15.658714952   56045 subchannel.cc:897]          Subchannel 0x154c04008290: Retry in 1000 milliseconds\r\nD0320 17:34:15.658725238   56045 dns_resolver.cc:255]        In cooldown from last resolution (from 3 ms ago). Will resolve again in 997 ms\r\nD0320 17:34:15.658734069   56045 dns_resolver.cc:275]        Start resolving.\r\n ", "comments": ["@alokmp83, Can you provide the simple standalone code to replicate the issue. Thanks!", "@alokmp83,Please update for the above comment.", "Hi - Sorry for the late due to unforeseen reasons.\r\nI have been running this program through Univa grid engine and it turns out that this error:\r\nerror: error running IJS server: \"could not get remote execution state\"\r\nis being thrown by the grid engine and its not an issue with tensorflow ps and worker grpc connection.\r\nHence closing this issue.Thanks for the followup.\r\n\r\n"]}, {"number": 37767, "title": "[INTEL MKL] Upgrade DNNL to V1.2.2", "body": "This PR fixes (i) memory leak in scratchpad destructor and (ii) reduces sgemm performance regression with mkldnn_sgemm API.", "comments": ["@penpornk Thanks a lot! I would like to put more info on your sgemm benchmarks (https://github.com/penpornk/tensorflow/tree/sgemm/tensorflow/sgemm). Wondering how to put those. ", "@mdfaijul Please feel free to open a pull request or email me a patch and I can patch it. :)"]}, {"number": 37766, "title": "[INTEL MKL] Upgrade DNNL to V1.2.2", "body": "This PR fixes (i) memory leak in scratchpad destructor and (ii) reduces sgemm performance regression with mkldnn_sgemm API.", "comments": ["Merge conflict."]}, {"number": 37765, "title": "[Bug] TF 2.2.0rc0 fails with AMP and Horovod 0.19.1 in Keras compile & fit", "body": "With the recent changes in the Tensorflow Keras Optimizer API and Horovod. We did some testing and found that the following configuration was now broken:\r\n- Tensorflow 2.2.0rc0\r\n- Horovod 0.19.1\r\n- AMP + Keras Model Compile & Fit \r\n\r\n@sanjoy @pkanwar23 could we make sure to fix this one before TF 2.2.0 gets officially published ? It's still an RC release for now :)\r\n\r\nIf needed you can use this docker container which contains the right set of dependency and based on the public TF2.2.0rc0 container:\r\n```bash\r\ndocker pull born2data/tensorflow:hvd-0.19.1_tf_2.2.0rc0\r\n```\r\n\r\n**Code to reproduce:**\r\n```bash\r\nmpirun \\\r\n    -np 2 \\\r\n    -H localhost:2 \\\r\n    -bind-to none \\\r\n    -map-by slot \\\r\n    -x NCCL_DEBUG=VERSION \\\r\n    -x LD_LIBRARY_PATH \\\r\n    -x PATH \\\r\n    -mca pml ob1 -mca btl ^openib \\\r\n    --allow-run-as-root \\\r\n    python main.py\r\n```\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport horovod.tensorflow.keras as hvd\r\n\r\n# Horovod: initialize Horovod.\r\nhvd.init()\r\n\r\n# Horovod: pin GPU to be used to process local rank (one GPU per process)\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\nif gpus:\r\n    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\r\n\r\n(mnist_images, mnist_labels), _ = \\\r\n    tf.keras.datasets.mnist.load_data(path='mnist-%d.npz' % hvd.rank())\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(\r\n    (tf.cast(mnist_images[..., tf.newaxis] / 255.0, tf.float32),\r\n             tf.cast(mnist_labels, tf.int64))\r\n)\r\ndataset = dataset.repeat().shuffle(10000).batch(128)\r\n\r\npolicy = tf.keras.mixed_precision.experimental.Policy('mixed_float16', 128)\r\ntf.keras.mixed_precision.experimental.set_policy(policy)\r\n\r\nmnist_model = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),\r\n    tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),\r\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n    tf.keras.layers.Dropout(0.25),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dropout(0.5),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\n# Horovod: adjust learning rate based on number of GPUs.\r\nopt = tf.optimizers.Adam(0.001)\r\n\r\n# Horovod: add Horovod DistributedOptimizer.\r\nopt = hvd.DistributedOptimizer(opt)\r\n\r\n# Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow\r\n# uses hvd.DistributedOptimizer() to compute gradients.\r\nmnist_model.compile(loss=tf.losses.SparseCategoricalCrossentropy(),\r\n                    optimizer=opt,\r\n                    metrics=['accuracy'],\r\n                    experimental_run_tf_function=False)\r\n\r\ncallbacks = [\r\n    # Horovod: broadcast initial variable states from rank 0 to all other processes.\r\n    # This is necessary to ensure consistent initialization of all workers when\r\n    # training is started with random weights or restored from a checkpoint.\r\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\r\n]\r\n\r\n# Train the model.\r\n# Horovod: adjust number of steps based on number of GPUs.\r\nmnist_model.fit(\r\n    dataset,\r\n    steps_per_epoch=500 // hvd.size(),\r\n    callbacks=callbacks,\r\n    epochs=24,\r\n    verbose=1 if hvd.rank() == 0 else 0\r\n)\r\n```\r\n\r\n**Error:**\r\n```python\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:503 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:473 train_step  **\r\n        _minimize(tape, self.optimizer, loss, self.trainable_variables)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1739 _minimize\r\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:232 apply_gradients\r\n        args=(grads_and_vars, name, all_reduce_sum_gradients))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2420 merge_call\r\n        return self._merge_call(merge_fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2427 _merge_call\r\n        return merge_fn(self._strategy, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:256 _apply_gradients_cross_replica  **\r\n        control_flow_ops.no_op)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/smart_cond.py:54 smart_cond\r\n        return true_fn()\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:248 apply_fn\r\n        all_reduce_sum_gradients))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py:262 _apply_gradients\r\n        name, all_reduce_sum_gradients)\r\n    /usr/local/lib/python3.6/dist-packages/horovod/_keras/__init__.py:73 apply_gradients\r\n        raise Exception('`apply_gradients()` was called without a call to '\r\n\r\n    Exception: `apply_gradients()` was called without a call to `get_gradients()` or `_aggregate_gradients`. If you're using TensorFlow 2.0, please specify `experimental_run_tf_function=False` in `compile()`.\r\n```\r\n\r\nPlease let me know how I can help\r\n\r\nCC: @nluehr @reedwm @tgaddair @cliffwoolley @omalleyt12 @houtoms", "comments": ["Hmmm this is probably because LossScaleOptimizer doesn't define _HAS_ALL_REDUCE_SUM_GRAD. We should set _HAS_ALL_REDUCE_SUM_GRAD to True if the inner optimizer has set it to true.", "JFYI. @reedwm By changing the loss_scale_optimizer.py as the following can solve the problem. But we are not sure if this is just a WAR.\r\n```python\r\nclass LossScaleOptimizer(optimizer_v2.OptimizerV2):\r\n ...\r\n _HAS_ALL_REDUCE_SUM_GRAD = True\r\n def _aggregate_gradients(self, grads_and_vars):\r\n   return self._optimizer._aggregate_gradients(grads_and_vars)\r\n ...\r\n```", "@reedwm any chance we can get this small fix inside 2.2.0 ?", "Ah we commented at the same time. Yeah that would fix it but it should be set to True only if the inner optimizer defined it.\r\n\r\nI will fix.", "I will try very hard to cherrypick but I cannot promise anything", "Awesome, thanks a lot. Let us know if we can do anything to help ya", "@reedwm Any good news ?", "In trying to fix this by adding `_HAS_ALL_REDUCE_SUM_GRAD = True`, the mixed precision CentralStorageStrategy tests broke. We realized the `all_reduce_sum_gradients` parameter has issues with CentralStorageStrategy. As a result, the the parameter was renamed to `experimental_aggregate_gradients` in 75ae7742abc027b001a5f3d7c020bb4504cc0f78. The attribute has also been renamed to `_HAS_AGGREGATE_GRAD`. You will have to make these changes in the Horovod optimizer.\r\n\r\nI'll disable the central storage tests, then add the `_HAS_AGGREGATE_GRAD` attribute, which should fix this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37765\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37765\">No</a>\n", "This is not closed until bcfc1ba6798ced6889f579644c2c79515832d098 is cherrypicked into 2.2", "Thanks @reedwm for your quick help. **Much** appreciated ;) ", "@reedwm It seems we still get ```Exception: `apply_gradients()` was called without a call to `get_gradients()` or `_aggregate_gradients`. If you're using TensorFlow 2.0, please specify `experimental_run_tf_function=False` in `compile()`.``` error.\r\n\r\nDon't we need  \r\n```python\r\ndef _aggregate_gradients(self, grads_and_vars):\r\n   return self._optimizer._aggregate_gradients(grads_and_vars)\r\n```\r\nin class LossScaleOptimizer to pass the gradients aggregation to the wrapped optimizer? After I add these two lines, the problem is gone.", "Good point, we need to define `_aggregate_gradients` if the LossScaleOptimizer wraps the DistributedOptimizer, as is done in the example. If the LossScaleOptimizer wraps the DistributedOptimizer, everything should work fine.\r\n\r\nI'll try to cherry-pick the fix in, but 2.2rc2 is already released so there's a good chance I won't be able to. Also try testing with that fix in and let me know if there are any other fixes that are needed.", "[2.2.0-rc3](https://pypi.org/project/tensorflow/2.2.0rc3/#files) is now released, @houtoms can you please check ?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37765\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37765\">No</a>\n"]}, {"number": 37764, "title": "Tensorflow can build and even run a model with `tf.nn.conv2d('filter_width=0' and 'filter_height=0')`", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1.0\r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTensorflow crashed when I run code with filter width and filter height is 0 \r\n**Describe the expected behavior**\r\nCheck the bad situation and throw an error.\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n[colab example](https://colab.research.google.com/drive/1_dD7KBsr0H08tqNpDTNh5ztGoHQpVKAz)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe bug seems to happen at tensorflow backend.\r\nRelated #37334", "comments": ["Hey, I am working on this. The same issue exists for all nn.conv1d, conv2d, conv3d, and their transpose version. I am almost done with this but have several questions:\r\n\r\n- the nn.conv version accepts a tensor directly. If we disable kernel to be 0, should we also disable in_channel and out_channel to be zero as well? To me, I don't think a 0 in-out channel is useful.\r\n- should I add the zero-dim checking in python end or in C++ end? Currently I made my changes in python end. I also targeted the Conv2D op's dimension checking in C++ end, e.g., mostly I can add some checking in (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L415)\r\n- as a follow-up, I didn't find similar checkings for Conv3D and Transpose version in C++ end. I suppose it should be added in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_3d.cc. If it's better to add these checkings in C++ end, can someone tell me the related files that I can modify for Conv3D and other ops?\r\n", "> Hey, I am working on this. The same issue exists for all nn.conv1d, conv2d, conv3d, and their transpose version. I am almost done with this but have several questions:\r\n> \r\n> * the nn.conv version accepts a tensor directly. If we disable kernel to be 0, should we also disable in_channel and out_channel to be zero as well? To me, I don't think a 0 in-out channel is useful.\r\n> * should I add the zero-dim checking in python end or in C++ end? Currently I made my changes in python end. I also targeted the Conv2D op's dimension checking in C++ end, e.g., mostly I can add some checking in (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L415)\r\n> * as a follow-up, I didn't find similar checkings for Conv3D and Transpose version in C++ end. I suppose it should be added in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_3d.cc. If it's better to add these checkings in C++ end, can someone tell me the related files that I can modify for Conv3D and other ops?\r\n\r\nIn my option, add a check in conv kernel could fix the bug.", "Why is this a bug? AFAICT the result of applying a convolution with a filter with one of the shapes 0 is perfectly well-defined, so we should support it.", "> Why is this a bug? AFAICT the result of applying a convolution with a filter with one of the shapes 0 is perfectly well-defined, so we should support it.\r\n\r\n``` python\r\nimport numpy as np\r\ndef conv(image, conv_filter):\r\n    feature_maps=np.zeros([image.shape[0]-conv_filter.shape[1],image.shape[1]-conv_filter.shape[2],conv_filter.shape[0]])\r\n    for filter_num in range(conv_filter.shape[0]):\r\n        curr_filter = conv_filter[filter_num,:]\r\n        filter_size=curr_filter.shape[0]\r\n        result=np.zeros(image.shape)\r\n        for r in np.uint16(np.arange(filter_size/2, image.shape[0]-filter_size/2-2)):\r\n            for c in np.uint16(np.arange(filter_size/2, image.shape[1]-filter_size/2-2)):\r\n                curr_region=image[r:r+filter_size,c:c+filter_size]\r\n                curr_result=curr_region*curr_filter\r\n                conv_sum=np.sum(curr_result)\r\n                result[r,c]=conv_sum\r\n        \r\n        conv_map=result[np.uint16(filter_size/2):result.shape[0]-np.uint16(filter_size/2),\r\n                        np.uint16(filter_size/2):result.shape[1]-np.uint16(filter_size/2)]\r\n        feature_maps[:,:,filter_num]=conv_map\r\n    return feature_maps\r\n\r\n# features=conv(np.ones([416,416]), np.ones([1,3,3]))\r\n# print(features)\r\n# print(features.shape)\r\n\r\nfeatures2=conv(np.ones([416,416])*2, np.ones([1,0,0]))\r\nprint(features2)\r\nprint(features2.shape)\r\n```\r\nI use above code simulate the conv with shapes 0 and I get a zero matrix with same shape as input.\r\nIs it correct?\r\nThe value will equal bias.", "Depending on whether a height/width dimension is zero-sized vs a channel\ndimension you will either get a zero image or an image with a zero-sized\nchannel dimension.\n\nOn Mon, Mar 23, 2020 at 11:10 PM who who who <notifications@github.com>\nwrote:\n\n> Why is this a bug? AFAICT the result of applying a convolution with a\n> filter with one of the shapes 0 is perfectly well-defined, so we should\n> support it.\n>\n> import numpy as npdef conv(image, conv_filter):\n>     feature_maps=np.zeros([image.shape[0]-conv_filter.shape[1],image.shape[1]-conv_filter.shape[2],conv_filter.shape[0]])\n>     for filter_num in range(conv_filter.shape[0]):\n>         curr_filter = conv_filter[filter_num,:]\n>         filter_size=curr_filter.shape[0]\n>         result=np.zeros(image.shape)\n>         for r in np.uint16(np.arange(filter_size/2, image.shape[0]-filter_size/2-2)):\n>             for c in np.uint16(np.arange(filter_size/2, image.shape[1]-filter_size/2-2)):\n>                 curr_region=image[r:r+filter_size,c:c+filter_size]\n>                 curr_result=curr_region*curr_filter\n>                 conv_sum=np.sum(curr_result)\n>                 result[r,c]=conv_sum\n>\n>         conv_map=result[np.uint16(filter_size/2):result.shape[0]-np.uint16(filter_size/2),\n>                         np.uint16(filter_size/2):result.shape[1]-np.uint16(filter_size/2)]\n>         feature_maps[:,:,filter_num]=conv_map\n>     return feature_maps\n> # features=conv(np.ones([416,416]), np.ones([1,3,3]))# print(features)# print(features.shape)\n>\n> features2=conv(np.ones([416,416])*2, np.ones([1,0,0]))print(features2)print(features2.shape)\n>\n> I use above code simulate the conv with shapes 0 and I get a zero matrix\n> with same shape as input.\n> Is it correct?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/37764#issuecomment-603037138>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRMLBURDMSCX33KHJU3RJBFEXANCNFSM4LQWV6JA>\n> .\n>\n\n\n-- \n - Alex\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@fsx950223  Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 37763, "title": "There is a bug that strings are not displayed in iOS darkmode.", "body": "**System information** \r\n\r\n- Mobile device : iPhone 8 , iOS 13\r\n\r\n**Describe the current behavior**\r\nurl : [image_classification/ios](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/ios)\r\nThere is a bug that strings are not displayed in iOS darkmode.\r\n<img src=\"https://user-images.githubusercontent.com/55723667/77207772-9da16a80-6b3d-11ea-8e27-b9e0593600e4.PNG\" width=\"300\"> \r\n\r\nThere is not a bug in light mode.\r\n<img src=\"https://user-images.githubusercontent.com/55723667/77211304-ead60a00-6b46-11ea-8a1b-0d049318db84.PNG\" width=\"300\"> \r\n\r\n\r\n\r\n**Other info / logs** \r\nThe reason for this is that the background and text are the same color.\r\n", "comments": ["I'm working on this bug.\r\nhttps://github.com/tensorflow/examples/pull/177", "This is fixed by merging the PR. Thanks for the fix!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37763\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37763\">No</a>\n"]}, {"number": 37762, "title": "release 2.2-<rc2> cherry-pick request:Do not constant-fold nodes that will use ScopedAllocator.", "body": "The constant folding optimizer creates a subgraph with constant foldable nodes,\r\nand runs them on the CPU using `GraphRunner`.  If any of the nodes in the\r\nsubgraph contain `_scoped_allocator` attribute, they are intended to be\r\nallocated with a ScopedAllocator.  However, the `_ScopedAllocator` node is\r\nabsent from this graph, which causes the execution of this subgraph to error\r\nout.\r\n\r\nThis change skips nodes that are marked with `_scoped_allocator` attribute in\r\nthe constant folding optimizer.\r\n\r\nPiperOrigin-RevId: 299421744\r\nChange-Id: Ic8e5896dbd5612d7782b9aa2e078e0128ad02f90\r\n\r\nReasons for requesting a cherrypick: This bug fix is needed to run an ObjectDetection model with MultiWorkerMirroredStrategy on CAIP Deep learning VMs. We need an official release to be able to create VM images that can be pushed onto these Deep learning VMs. These are set to be used as part of an upcoming launch. ", "comments": ["LGTM but please wait for release owners, i.e. @goldiegadde to approve as well."]}, {"number": 42791, "title": "[ja] load_data/csv.ipynb using non-existant \"Dataset.output_shapes\".", "body": "This tutorials is currently broken (also needs a `%tensorflow_version 2.x`):\r\n\r\nhttps://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/load_data/csv.ipynb", "comments": ["Please take a look REVIEWERS: @ohtaman @sfujiwara @masa-ita @AseiSugiyama @yukinagae @nuka137 @chie8842 @kiszk\r\n\r\nYou can try using the [nb_code_sync.py](https://github.com/tensorflow/docs/blob/master/tools/nb_code_sync.py) script to help keep notebooks up-to-date.", "I think this issue has been fixed by https://github.com/tensorflow/docs-l10n/pull/176 so we can close this issue safely.\r\n\r\n@lamberta @MarkDaoust How do you think?", "Thanks, @AseiSugiyama !"]}, {"number": 42790, "title": "Some notebooks using: `tf-nightly-2.0-preview`", "body": "This package does not exist anymore. Please upgrade all notebooks to use `%tensorflow_version 2x`.\r\n\r\nhttps://github.com/tensorflow/docs-l10n/search?q=tf-nightly-2.0-preview&unscoped_q=tf-nightly-2.0-preview", "comments": ["Broke this issue up on language to make it easier for reviewers:\r\n\r\n* https://github.com/tensorflow/docs-l10n/issues/145\r\n* https://github.com/tensorflow/docs-l10n/issues/146\r\n* https://github.com/tensorflow/docs-l10n/issues/147\r\n"]}, {"number": 42789, "title": "Several notebooks failing on `keras.experimental.export_saved_model`", "body": "https://github.com/tensorflow/docs-l10n/search?q=%22keras.experimental.export_saved_model%22&unscoped_q=%22keras.experimental.export_saved_model%22\r\n\r\n```\r\n# Exportar el modelo a 'SavedModel'\r\nkeras.experimental.export_saved_model(model, 'path_to_saved_model')\r\n\r\n# Recrea exactamente el mismo modelo\r\nnew_model = keras.experimental.load_from_saved_model('path_to_saved_model')\r\n```\r\n\r\nThese should just be `model.save('path_to_saved_model')`, and `keras.models.load_model('path_to_saved_model')`.", "comments": ["If we create issues by language, then we can assign/cc folks from the `site/<lang>/REVIEWERS` file(s)"]}, {"number": 37761, "title": "Failed to convert object detection model to tensorflow lite", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: from binary \r\n- **TensorFlow version (use command below)**: 2.0\r\n- **Python version**: 3.69\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:7.4.0\r\n- **CUDA/cuDNN version**: 10.2\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nfrozen_model_path = '/home/tflite/tflite_graph.pb'\r\ninput_arrays=[\"image_tensor\"]\r\noutput_arrays=[\"detection_boxes\",\"detection_scores\",\"num_detections\",\"detection_classes\"]\r\ninput_tensor={\"image_tensor\":[1,300,300,3]}\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_model_path, input_arrays, output_arrays, input_tensor)\r\ntflite_model = converter.convert()\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nI am using object detection API. I have a pretrained ssd_mobilenet_v2_coco model and I want to convert it to tensorflow lite(and do float16 quantization). In the folder of the model, I have checkpoint, frozen_inference_graph.pb, model.ckpt.data-00000-of-00001, model.ckpt.index, model.ckpt.meta, pipeline.config, and saved_model folder.\r\n\r\nI used the following command to get the tflite.pb\r\npython3 export_tflite_ssd_graph.py --output_directory=/home/tflite --pipeline_config_path=/home/exportedLP1/pipeline.config --trained_checkpoint_prefix=/home/exportedLP1/model.ckpt\r\n\r\nThen I used the method posted online to convert tflite.pb. The last step  tflite = converter.convert() caused \"abort(core dumped)'. According to posts I read online, I should be able to convertt to tflite without any issue.\r\n\r\n### Source code / logs\r\nCurrent thread 0x00007efbff6f1740 (most recent call first):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56 in execute\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299 in run\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93 in main\r\n  File \"/usr/local/bin/toco_from_protos\", line 8 in <module>\r\nAborted (core dumped)\r\n\r\n", "comments": ["@valorusa \r\nplease share simple stand alone code for us to replicate the issue faced by you, with complete error logs.", "Hi, all the codes are here:\r\n\r\nfrozen_model_path = '/home/tflite/tflite_graph.pb'\r\ninput_arrays=[\"image_tensor\"]\r\noutput_arrays=[\"detection_boxes\",\"detection_scores\",\"num_detections\",\"detection_classes\"]\r\ninput_tensor={\"image_tensor\":[1,300,300,3]}\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_model_path, input_arrays, output_arrays, input_tensor)\r\ntflite_model = converter.convert()\r\n", "@valorusa \r\ni have replicated the code shared by you and [face this error](https://colab.sandbox.google.com/gist/Saduf2019/d7086f73cd08b759c26f24c42189d82f/untitled106.ipynb), as requested please share simple stand alone code such that we can replicate the error faced by you to analyse it.", "Hi Thanks for the response. I tried to attached the file /home/tflite/tflite_graph.pb here. But it is too large to attach it here. So I add the description here (replace your_usrname by the actual name you are using on your side):\r\n\r\n1.go to tensorflow object detection model zoo (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) to download **ssd_mobilenet_v2_coco**. extract it under your user name directory to get home/your_usrname/ssd_mobilenet_v2_coco_2018_03_29/, and open the pipeline.config file in it, find and comment out this line **_batch_norm_trainable: true_** (otherwise you will get error like this: google.protobuf.text_format.ParseError: 35:7 : Message type \"object_detection.protos.SsdFeatureExtractor\" has no field named \"batch_norm_trainable\".)\r\n\r\n2.make a new directory under your user name: /home/your_usrname/tflite, \r\n\r\n3.open the object detection api directory, go to /models/research/object_detection \r\n\r\n4.then in the terminal, run  python3 export_tflite_ssd_graph.py  --output_directory=/home/your_usrname/tflite --pipeline_config_path=/home/your_usrname/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config --trained_checkpoint_prefix=/home/your_username/ssd_mobilenet_v2_coco_2018_03_29/model.ckpt\r\n\r\n5.now you should be able to find tflite_graph.pb and tflite_graph.pbtxt in home/your_usrname/tflite/\r\n\r\n6.in python, run the following codes:\r\n  frozen_model_path = '/home/your_usrname/tflite/tflite_graph.pb'\r\n  input_arrays=[\"image_tensor\"]\r\n  output_arrays=[\"detection_boxes\",\"detection_scores\",\"num_detections\",\"detection_classes\"]\r\n  input_tensor={\"image_tensor\":[1,300,300,3]}\r\n  converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_model_path, input_arrays, output_arrays, input_tensor)\r\n  tflite_model = converter.convert()", "Hello @valorusa I found your description very useful. Just one question: \r\nWhere did you find information about input_arrays and output_arrays? \r\nI successfully converted ssd_mobilenet_v2_coco_2018_03_29 but I am not able to convert a custom model because I don't know input and output arrays.\r\nThanks", "Hi @Chris1hub I use the tensorflow object detection api to do the object detection and recognition. The input arrays are the names of the tensors used for the inference. They are used in the below  inference routine:\r\n\r\n```\r\nwith session.graph.as_default():\r\n        image_tensor = detect_graph.get_tensor_by_name('image_tensor:0')\r\n        \r\n        # Each box represents a part of the image where a particular object was detected.\r\n        boxes = detect_graph.get_tensor_by_name('detection_boxes:0')\r\n        \r\n        # Each score represent how level of confidence for each of the objects.\r\n        # Score is shown on the result image, together with the class label.\r\n        scores = detect_graph.get_tensor_by_name('detection_scores:0')\r\n        classes = detect_graph.get_tensor_by_name('detection_classes:0')\r\n        num_detections = detect_graph.get_tensor_by_name('num_detections:0')\r\n\r\n        # Actual detection.\r\n        (boxes, scores, classes, num_detections) = session.run([boxes, scores, classes, num_detections],feed_dict={image_tensor: image_np_expanded})\r\n        return (boxes, scores, classes, num_detections)\r\n\r\n```", "\r\n@valorusa It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version 2.5 or 2.4.1 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37761\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37761\">No</a>\n"]}, {"number": 37760, "title": "Modifying an existing Tensorflow.js tutorial. Trying to modify 'part_3' of tutorial with code from 'part_4 handsign example'. Images not being read correctly.", "body": "[Part 3: MNIST Classification]: https://medium.com/ailab-telu/learn-and-play-with-tensorflow-js-part-3-dd31fcab4c4b\r\n\r\n[Part 4: Application Example]: https://medium.com/ailab-telu/learn-and-play-with-tensorflow-js-part-4-f011eb7bf4dc\r\n\r\nGithub link for tutorial: https://github.com/adf-telkomuniv/TensorFlowJS_Tutorial\r\n\r\nAfter completing the the tutorial for '[Part 3: MNIST Classification]', I tried to apply code to the script to allow the option to import an image of a handwritten digit from the PC and have the trained model predict the value of the digit in the image.\r\n\r\nI tried to use the code in the 'handsign.js' and 'handsign.html' files given in the 'part_4' github folder, but the images I'm using all give the same result despite all being different digits.\r\n\r\nI think it might be reading the data from the canvas incorrectly?\r\n\r\nThis is the code I added to the 'index.js' file of Part 3 from the 'handsign.js' file of Part 4:\r\n`initCanvas('predict-canvas1')\r\n$('#upload-btn').click(async() => {\r\n$('#img-upload').click()\r\n})\r\n\r\nvar imgTensor\r\n$('#img-upload').change(async(evt) => {\r\nvar canvas = $('#predict-canvas1')[0]\r\nvar context = canvas.getContext(\"2d\")\r\n\r\nvar img = new Image() \r\n\r\nvar file = evt.target.files[0]\r\nif(file.type.match('image.*')) {\r\n    var reader = new FileReader()\r\n    reader.readAsDataURL(file)\r\n    reader.onload = function(evt){\r\n        if( evt.target.readyState == FileReader.DONE) {\r\n            img.src = evt.target.result\r\n            img.onload = () => {                    \r\n                context.clearRect(0, 0, canvas.width, canvas.height)\r\n                context.drawImage(img, 0, 0)\r\n                \r\n\r\n                \r\n            }\r\n        }\r\n    }        \r\n    $('#predict-btn1').prop('disabled', false)\r\n    // $('#prediction1').text( 'Predicted: ')    \r\n} else {\r\n    alert(\"not an image\")\r\n}\r\n})\r\n\r\n$('#predict-btn1').click(async() => {\r\n// try{\r\n// $('#predict-btn1').prop('disabled', true)\r\n// const img = imgTensor.div(tf.scalar(255))\r\n\r\n//     var x_data = tf.cast(resized.reshape([1, 28, 28, 3]), 'float32')\r\n//     var y_pred = await model.predict(x_data)\r\n//     var predictions = Array.from(y_pred.argMax(1).dataSync())\r\n\r\n//     $('#prediction1').text( 'Predicted: '+ predictions)    \r\n// } catch (e) {\r\n//     console.log(e)\r\n//     alert('failed to predict image')\r\n// }\r\nvar canvas = $('#predict-canvas1')[0]\r\nvar preview = $('#preview-canvas1')[0]\r\n\r\nvar img1 = tf.browser.fromPixels(imgTensor, 4)\r\nvar resized = util.cropImage(img1, canvas.width)        \r\ntf.browser.toPixels(resized, preview)    \r\nvar x_data = tf.cast(resized.reshape([1, 28, 28, 1]), 'float32')\r\nvar y_pred = model.predict(x_data)\r\n\r\nvar prediction = Array.from(y_pred.argMax(1).dataSync())    \r\n$('#prediction1').text( 'Predicted: '+ prediction)    \r\n\r\nconst barchartData = Array.from(y_pred.dataSync()).map((d, i) => {\r\n    return { index: i, value: d }\r\n})\r\ntfvis.render.barchart($('#predict-graph1')[0], barchartData,  { width: 400, height: 140 })    \r\n})`\r\n\r\nIs there a way for this part of 'handsign.js' to work with the 'index.js' file?\r\n\r\nIf not, is there an alternative way for the trained model to predict the digit contained in an image?\r\n\r\nAll the code used in this project, including the images I'm trying to use, can be found at this repository: https://github.com/Miller144/verbose-octo-carnival", "comments": ["@Miller144 \r\n\r\nThis issue is more suitable for TensorFlow Js repo. Please post it on tfjs repo from [here.](https://github.com/tensorflow/tfjs/issues) .Thanks!", "@Miller144 \r\n\r\nI am closing this issue here and we can track the issue in TFjs repo. Thanks!"]}, {"number": 37759, "title": "ImportError: DLL load failed: Uma rotina de inicializa\u00e7\u00e3o da biblioteca de v\u00ednculo din\u00e2mico (DLL) falhou.", "body": "ImportError: DLL load failed: Uma rotina de inicializa\u00e7\u00e3o da biblioteca de v\u00ednculo din\u00e2mico (DLL) falhou.\r\nTraceback (most recent call last):\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python\\Python36\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Uma rotina de inicializa\u00e7\u00e3o da biblioteca de v\u00ednculo din\u00e2mico (DLL) falhou.\r\n\r\nDuring handling of the above exception, another exception occurred:", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "In the future, please fill in issue template, surround your code with ` ``` `, and check for similar issues.\r\n\r\nClosing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37759\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37759\">No</a>\n"]}, {"number": 37758, "title": "tf.io.gfile.glob missing some patterns. Using tf-nightly", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): tf-nightly\r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\ncc @Conchylicultor,\r\nPlease have a look on issue from `TFDS` tensorflow/datasets#1670, tests are failing for `PlantVillage `and `The300wLp` datasets because in `_generate_example ` function of both [`plant_village.py`](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image/plant_village.py#L142) and [the300w_lp.py](https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/image/the300w_lp.py#L116) `tf.io.gfile.glob()` does not correctly matches all examples patterns. However python [`glob`](https://docs.python.org/3/library/glob.html) solves issue see PR tensorflow/datasets#1684\r\n**Describe the expected behavior**\r\ntf.io.gfile.glob() must matches all patterns provided so that all required examples are generated.\r\n\r\n**Standalone code to reproduce the issue** \r\nPlease have a look on this [`colab`](https://colab.research.google.com/drive/1tfLOQubRWd6Dc9mPTtAwca-eAzctGLId) notebook, it contains all tracebacks as well as problem with ` tf.io.gfile.glob()` and how python `glob `solves this issue.\r\n\r\n**As glob fix this issue but we have to use `tf.io.gfile` because we need to support GCS and other distributed files systems.**\r\n", "comments": ["Yes, TFDS tests have started failing for patterns like: `tf.io.gfile.glob('/path/to/file/[!Code]*[!_Flip]/[!_]*.jpg')` or `tf.io.gfile.glob('/path/to/*.[jJ][pP][gG]')`.\r\n\r\n@Eshan-Agarwal Could you provide a small self-contained code snippet to reproduce the issue ?\r\n\r\nSomething like:\r\n```\r\nimport glob\r\nimport tensorflow as tf\r\n\r\nwith tf.io.gfile.GFile('/tmp/file') as f:\r\n  pass\r\n\r\nprint(list(tf.io.gfile('/tmp/some_pattern')))\r\nprint(list(glob.glob('/tmp/some_pattern')))   # Should show different result\r\n```", "+1 on providing simple patterns.\r\n\r\nI can look into this in about a month or two, once modular filesystems (tensorflow/community#101) are implemented", "@Conchylicultor  @mihaimaruseac  please look on this [colab](https://colab.research.google.com/drive/1rXrpPVhiDfUH4DBT3Og65yx61R5MleHr) notebook", "@Eshan-Agarwal I get `NotFoundError: /content/temp/plant_village/Grape___Leaf_blight_(Isariopsis_Leaf_Spot); No such file or directory` when executing your colab.", "@Eshan-Agarwal the difference between the colab and the example template suggested is that we need to have the exact same setup for the colab, whereas the suggested template creates the files (with zero bytes) so it can be easily converted into a test case that now fails and after fixing will succeed.\r\n\r\nBut it's ok, I'll take care of this issue.", "@Eshan-Agarwal For the future, here is what a minimum reproductible example looks like:\r\n\r\n```python\r\nimport os\r\nimport glob\r\nimport tensorflow.compat.v2 as tf\r\n\r\n# Write a dummy file\r\nroot_dir = '/tmp/dir_with_(brace)/'\r\ntf.io.gfile.makedirs(root_dir)\r\nwith tf.io.gfile.GFile(os.path.join(root_dir, 'some_file.txt'), 'w') as f:\r\n  f.write('')\r\n\r\n# Search the file\r\nglob_path = os.path.join(root_dir, \"*\")\r\nprint(list(glob.iglob(glob_path)))        # ['/tmp/dir_with_(brace)/some_file.txt']\r\nprint(list(tf.io.gfile.glob(glob_path)))  # []  << Bug: File not found\r\n```\r\n\r\nThis allow the team to easily understand what the issue is. They can just copy past the code and experiment with it. This save many hours, as all people working on the issue can get started immediately without having to go through the 10000+ lines of codes of TFDS.\r\n\r\n@mihaimaruseac The bug is that `tf.io.gfile.glob` fails when `(` are present in the path. This is a regression as it only appear in TF nightly. Not TF 2.1.\r\nThis make some TFDS tests fails as some datasets rely on this global pattern to generate the dataset.\r\n", "I think I have an idea where that might come from.", "@Conchylicultor  @mihaimaruseac  thanks for your quick responses, Actually I upload temp folder containing some example you can download folder from [here](https://drive.google.com/drive/folders/1DyMHIXSUue10k1a9pDbwgHK0Lng9VxuM?usp=sharing).\r\nbut it is good to use code provided by @Conchylicultor  without any external uploading.\r\n", "@mihaimaruseac, do you know if this will this be fixed before the 2.2 release ? Otherwise, it will break some of our TFDS users using those datasets.\r\nWorse, as no error is raised with `tf.io.gfile.glob`, it will silently generate bad datasets where some examples are missing.\r\n\r\n@gadagashwini I'm not sure why you added the `TF 2.1` label. `TF 2.1` works fine, but `tf-nightly` is broken.", "We haven't prioritized this for 2.2 release, unfortunately. Can you verify that installing 2.2.-rc1 breaks you? Let me know if that is the case and I'll see if we can prioritize it.", "@Eshan-Agarwal, could you check if the issue occurs with `2.2.-rc1` ?", "@Conchylicultor  sure, I will", "@Conchylicultor  @mihaimaruseac Yes same problem occurs with `2.2.-rc1`, `tf.io.gfile.glob()` not matches patterns. \r\nPlease see this [colab ](https://colab.research.google.com/drive/1SAXEnHx0fXdnRjKYRdk3g0BzCoCgitKL)notebook, it shows both original tests as well as test with minimal reproductible code. ", "@Eshan-Agarwal thank you for confirming.\r\n\r\n@mihaimaruseac I believe this should be prioritised. This not only impact TFDS but potentially every users using `tf.io.gfile.glob`. As the issue is silent, users may not even notice there is a bug.\r\nIn our case we got lucky to have good unit-tests.\r\nNote: The issue only happened externally. Internally, our tests works fine.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37758\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37758\">No</a>\n", "Reopening to close once the fix lands on `r2.2` branch (and then will be picked up by the next RC).\r\n\r\nSee #37915", "@mihaimaruseac Thank you for the fix.\r\nI'll update here when [our tests](https://storage.googleapis.com/tfds-kokoro-public/kokoro-build.html) are back to green.", "the cherrypick has been merged into the r2.2 branch", "TF 2.2.0-rc2 has been released and this issue should be fixed now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37758\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37758\">No</a>\n", "I confirm this fixed our tests. Thank you very much!"]}, {"number": 37757, "title": "TFLite - Experimental New Converter, incorrect model for Bidirectional GRU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\nYes Provided Below.\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\nMac OS 10.12.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: Samsung Note9\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n2.1.0\r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTFLite produces a graph with 1 output of size 1(*4 bytes)\r\n**Describe the expected behavior**\r\nTFLite produces a graph with 1 output of size 64(*4 bytes)\r\n**Standalone code to reproduce the issue** \r\nimport tensorflow as tf\r\nword = tf.keras.Input(shape=(1,), name='word', dtype=tf.int32)\r\nembedding_output = tf.keras.layers.Embedding(8000,100,input_length=1)(word)\r\nmodified_embedding = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32))(embedding_output)\r\nmodel = tf.keras.Model(inputs=[word], outputs=[modified_embedding])\r\nmodel._make_predict_function()\r\nmodel.summary()\r\nmodel.save(\"testmodel\")\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"testmodel\")\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nopen(\"testmodel.tflite\", \"wb\").write(tflite_model)\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Have you actually tried executing the graph in the TFLite interpreter? Shape inference during conversion isn't guaranteed to propagate to output nodes, especially when using SELECT_TF_OPS. It should be resolved at runtime when you run inference.", "Good to know. It works on device with the correct size, closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37757\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37757\">No</a>\n"]}, {"number": 37756, "title": "AttributeError when attempting to import Tensorflow on Windows 10", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None just CPU\r\n- GPU model and memory: Just CPU\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen importing tensorflow, there seems to be a problem with compat not existing for datasetinitializerhook...\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nPython 3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\n```\r\n>>> import tensorflow\r\n2020-03-20 16:51:15.881191: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-03-20 16:51:15.884479: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 46, in <module>\r\n    from . _api.v2 import compat\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\", line 39, in <module>\r\n    from . import v1\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\", line 32, in <module>\r\n    from . import compat\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\__init__.py\", line 39, in <module>\r\n    from . import v1\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py\", line 29, in <module>\r\n    from tensorflow._api.v2.compat.v1 import app\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\", line 39, in <module>\r\n    from . import v1\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\", line 32, in <module>\r\n    from . import compat\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\__init__.py\", line 39, in <module>\r\n    from . import v1\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py\", line 667, in <module>\r\n    from tensorflow_estimator.python.estimator.api._v1 import estimator\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_estimator\\__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator._api.v1 import estimator\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator._api.v1.estimator import experimental\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\experimental\\__init__.py\", line 10, in <module>\r\n    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\", line 33, in <module>\r\n    from tensorflow_estimator.python.estimator import estimator\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 53, in <module>\r\n    from tensorflow_estimator.python.estimator import util as estimator_util\r\n  File \"C:\\Users\\gragundier\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py\", line 75, in <module>\r\n    class _DatasetInitializerHook(tf.compat.v1.train.SessionRunHook):\r\nAttributeError: module 'tensorflow' has no attribute 'compat'\r\n>>>\r\n```", "comments": ["Just to add and prove that I've at least looked at previous posts similar to this:\r\nCPU: i7-4790k\r\nI do indeed have VS 2015-2019 x64 installed.  \r\nFrom what I can tell this CPU supports AVX2", "Can you please post the output of `pip list`?", "`Package                Version\r\n---------------------- -------------------\r\nabsl-py                0.9.0\r\nastor                  0.8.1\r\nastunparse             1.6.3\r\ncachetools             4.0.0\r\ncertifi                2019.11.28\r\nchardet                3.0.4\r\ngast                   0.2.2\r\ngoogle-auth            1.11.3\r\ngoogle-auth-oauthlib   0.4.1\r\ngoogle-pasta           0.2.0\r\ngrpcio                 1.27.2\r\nh5py                   2.10.0\r\nidna                   2.9\r\nKeras-Applications     1.0.8\r\nKeras-Preprocessing    1.1.0\r\nkeyboard               0.13.4\r\nMarkdown               3.2.1\r\nmss                    5.0.0\r\nnumpy                  1.18.2\r\noauthlib               3.1.0\r\nopt-einsum             3.2.0\r\npip                    20.0.2\r\nprotobuf               3.11.3\r\npyasn1                 0.4.8\r\npyasn1-modules         0.2.8\r\nrequests               2.23.0\r\nrequests-oauthlib      1.3.0\r\nrsa                    4.0\r\nscipy                  1.4.1\r\nsetuptools             41.2.0\r\nsix                    1.14.0\r\ntb-nightly             2.2.0a20200320\r\ntensorboard            2.1.1\r\ntensorboard-plugin-wit 1.6.0.post2\r\ntensorflow             2.1.0\r\ntensorflow-estimator   2.1.0\r\ntermcolor              1.1.0\r\ntf-estimator-nightly   2.3.0.dev2020032001\r\nurllib3                1.25.8\r\nWerkzeug               1.0.0\r\nwheel                  0.34.2\r\nwrapt                  1.12.1`", "So you have both nightly and released versions. Can you try the following two commands please?\r\n\r\n```\r\npip uninstall -y tb-nightly tensorboard tensorboard-plugin-wit tensorflow tensorflow-estimator tf-estimator-nightly\r\npip install tensorflow\r\n```", "Thank you.  Closing issue.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37756\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37756\">No</a>\n", "> So you have both nightly and released versions. Can you try the following two commands please?\r\n> \r\n> ```\r\n> pip uninstall -y tb-nightly tensorboard tensorboard-plugin-wit tensorflow tensorflow-estimator tf-estimator-nightly\r\n> pip install tensorflow\r\n> ```\r\n\r\nthank you", "> So you have both nightly and released versions. Can you try the following two commands please?\r\n> \r\n> ```\r\n> pip uninstall -y tb-nightly tensorboard tensorboard-plugin-wit tensorflow tensorflow-estimator tf-estimator-nightly\r\n> pip install tensorflow\r\n> ```\r\n\r\nthanks a lot"]}, {"number": 37755, "title": "sparse_placeholder missing tensor name in SavedModel", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): mac-osx\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 1.14\r\n- Python version: - Bazel\r\nversion (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from\r\nsource): n/a\r\n- CUDA/cuDNN version: - GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIf a sparse_placeholder is used in ServingInputReceiver, the SavedModel does not contain the name of the sparse_placeholder. This causes issues when the SavedModel is loaded by SavedModelEstimator, as shown below.\r\n**Describe the expected behavior**\r\nThe tensor represented by sparse_placeholder should be assigned a name in the SavedModel.\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nSave the code below as a file, test.py. \r\nthen run `python test.py`\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom pathlib import Path\r\n\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\r\n\r\nDIM = 4\r\nFEATURE_NAME = \"myfeature\"\r\nLABEL_NAME = \"mylabel\"\r\n\r\n\r\ndef train_input_fn():\r\n    dataset = tf.data.Dataset.from_tensors((\r\n        {FEATURE_NAME: tf.SparseTensor(indices=[[0], [2]],\r\n                        values=[1.0, 2.0],\r\n                        dense_shape=[DIM])},[1.0, 2.3]))\r\n    dataset = dataset.repeat(9).batch(3)\r\n    return dataset\r\n\r\n\r\ndef serving_input_receiver_fn():\r\n    \"\"\"Serving input_fn that builds features from placeholders\r\n\r\n    Returns\r\n    -------\r\n    tf.estimator.export.ServingInputReceiver\r\n    \"\"\"\r\n    feature = tf.compat.v1.sparse_placeholder(dtype=tf.float32, shape=[None, DIM], name=FEATURE_NAME)\r\n    receiver_tensors = {FEATURE_NAME: feature}\r\n    return tf.estimator.export.ServingInputReceiver(receiver_tensors, receiver_tensors)\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    w = tf.compat.v1.get_variable(\r\n        name='w',\r\n        initializer=tf.compat.v1.random.truncated_normal((DIM,1), stddev=1))\r\n    predictions = tf.compat.v1.sparse.sparse_dense_matmul(features[FEATURE_NAME], w)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n    else:\r\n        loss = tf.nn.l2_loss(predictions - labels)\r\n        if mode == tf.estimator.ModeKeys.EVAL:\r\n            return tf.estimator.EstimatorSpec(\r\n                mode, loss=loss)\r\n\r\n        elif mode == tf.estimator.ModeKeys.TRAIN:\r\n            train_op = tf.compat.v1.train.AdamOptimizer(learning_rate=0.5).minimize(\r\n                loss, global_step=tf.compat.v1.train.get_global_step())\r\n            return tf.estimator.EstimatorSpec(\r\n                mode, loss=loss, train_op=train_op)\r\n        else:\r\n            raise NotImplementedError()\r\n\r\n\r\ndef find_latest(export_dir):\r\n    subdirs = [x for x in Path(export_dir).iterdir()\r\n               if x.is_dir() and 'temp' not in str(x)]\r\n    latest = str(sorted(subdirs)[-1])\r\n    return latest\r\n\r\n\r\nif __name__ == '__main__':\r\n    config = tf.estimator.RunConfig(save_summary_steps=1,\r\n                                    save_checkpoints_steps=5,\r\n                                    log_step_count_steps=1)\r\n    export_dir = '/tmp/saved_model'\r\n    checkpoint_dir = '/tmp/checkpoint'\r\n\r\n    # train\r\n    estimator = tf.estimator.Estimator(model_fn, checkpoint_dir, config=config)\r\n    estimator.train(train_input_fn)\r\n\r\n    # export\r\n    estimator.export_saved_model(export_dir, serving_input_receiver_fn)\r\n\r\n    # load\r\n    latest_model = find_latest(export_dir)\r\n    print(latest_model)\r\n\r\n    # predict, tf 1.x\r\n    estimator = tf.contrib.estimator.SavedModelEstimator(latest_model)\r\n    predictions = estimator.predict(train_input_fn)\r\n    for x in predictions:\r\n        print(x)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nTraceback\r\n```\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/checkpoint', '_tf_random_seed': None, '_save_summary_steps': 1, '_save_checkpoints_steps': 5, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12caa1eb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nWARNING:tensorflow:From /envs/py3.6/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nWARNING:tensorflow:From /envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nINFO:tensorflow:Graph was finalized.\r\n2020-03-20 08:09:40.379515: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/checkpoint/model.ckpt.\r\nINFO:tensorflow:loss = 5.3372154, step = 1\r\nINFO:tensorflow:global_step/sec: 732.118\r\nINFO:tensorflow:loss = 1.6047388, step = 2 (0.002 sec)\r\nINFO:tensorflow:global_step/sec: 713.317\r\nINFO:tensorflow:loss = 4.220809, step = 3 (0.001 sec)\r\nINFO:tensorflow:Saving checkpoints for 3 into /tmp/checkpoint/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 4.220809.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nWARNING:tensorflow:From /envs/py3.6/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\r\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\r\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\r\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\r\nWARNING:tensorflow:From /envs/py3.6/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nINFO:tensorflow:Restoring parameters from /tmp/checkpoint/model.ckpt-3\r\nINFO:tensorflow:Assets added to graph.\r\nINFO:tensorflow:No assets to write.\r\nINFO:tensorflow:SavedModel written to: /tmp/saved_model/temp-b'1584716980'/saved_model.pb\r\n/tmp/saved_model/1584716980\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/vr/_8gkdt4s3wb88ncn_jmhk8hc000wcr/T/tmp1criy16p\r\nINFO:tensorflow:Using config: {'_model_dir': '/var/folders/vr/_8gkdt4s3wb88ncn_jmhk8hc000wcr/T/tmp1criy16p', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x130e7eeb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Checking available modes for SavedModelEstimator.\r\nWARNING:tensorflow:train mode not found in SavedModel.\r\nWARNING:tensorflow:eval mode not found in SavedModel.\r\nINFO:tensorflow:Available modes for Estimator: ['infer']\r\nINFO:tensorflow:Could not find trained model in model_dir: /var/folders/vr/_8gkdt4s3wb88ncn_jmhk8hc000wcr/T/tmp1criy16p, running initialization to predict.\r\nWARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\r\nINFO:tensorflow:Calling model_fn.\r\nTraceback (most recent call last):\r\n  File \"test_savedmodel.py\", line 83, in <module>\r\n    for x in predictions:\r\n  File \"/envs/py3.6/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 619, in predict\r\n    features, None, ModeKeys.PREDICT, self.config)\r\n  File \"/envs/py3.6/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1146, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/envs/py3.6/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/saved_model_estimator.py\", line 243, in _model_fn_from_saved_model\r\n    g, tags, input_map=input_map, return_elements=output_tensor_names)\r\n  File \"/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\", line 352, in load_graph\r\n    meta_graph_def, import_scope=import_scope, **saver_kwargs)\r\n  File \"/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1473, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 857, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 412, in import_graph_def\r\n    input_map = _ConvertInputMapValues(name, input_map)\r\n  File \"/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 193, in _ConvertInputMapValues\r\n    'tf.import_graph_def() requires a non-empty `name` if `input_map` '\r\nValueError: tf.import_graph_def() requires a non-empty `name` if `input_map` contains non-Tensor values. Try calling tf.convert_to_tensor() on `input_map` values before calling tf.import_graph_def().\r\n```\r\n\r\nFurther examine the savedmodel with cli `saved_model_cli show --dir /tmp/saved_model/1584716980 --all`\r\n\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['myfeature'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, -1)\r\n        name:\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['output'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: SparseTensorDenseMatMul/SparseTensorDenseMatMul:0\r\n  Method name is: tensorflow/serving/predict\r\n```\r\nNote: `name` is missing from inputs['myfeature'] tensor_info", "comments": ["Was able to reproduce the issue with [TF-1.14](https://colab.research.google.com/gist/amahendrakar/acf0094fe5f26740e7338f827441c8c9/37755-1-14.ipynb) and [TF-1.15.2](https://colab.research.google.com/gist/amahendrakar/5659e56ae2ab72c7faa1c3df71c67490/37755-1-15.ipynb). Please find the attached gist. Thanks!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37755\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37755\">No</a>\n"]}, {"number": 37754, "title": "Add -lrt to linkflags", "body": "Second attempt in tackling compilation on e.g. older CentOS, see for details #15129 after #37719 failed on OSX\r\n\r\nI added code from which I believe it will only add the flag on Linux. Please verify that I got that right, the code is mostly copied and adjusted as I don't really know Bazel.\r\n\r\nccing @gunan @mihaimaruseac", "comments": []}, {"number": 37753, "title": "Micro: Enable M-Profile Vector Extension if Supported.", "body": "", "comments": []}, {"number": 37752, "title": "./build_all_linux.sh", "body": "```\r\n/home/light/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/util/test_log.pb.o:(.data.rel.ro._ZTVN6google8protobuf8internal12MapEntryImplIN10tensorflow35BenchmarkEntry_ExtrasEntry_DoNotUseENS0_7MessageENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_10EntryValueELNS1_14WireFormatLite9FieldTypeE9ELSE_11ELi0EEE[_ZTVN6google8protobuf8internal12MapEntryImplIN10tensorflow35BenchmarkEntry_ExtrasEntry_DoNotUseENS0_7MessageENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_10EntryValueELNS1_14WireFormatLite9FieldTypeE9ELSE_11ELi0EEE]+0x58)\uff1a\u5bf9\u2018google::protobuf::Message::InitializationErrorString[abi:cxx11]() const\u2019\u672a\u5b9a\u4e49\u7684\u5f15\u7528\r\n/usr/bin/ld: \u6700\u540e\u7684\u94fe\u7ed3\u5931\u8d25: \u7b26\u53f7\u9700\u8981\u4e0d\u5b58\u5728\u7684\u8c03\u8bd5\u8282\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/contrib/makefile/Makefile:899: recipe for target '/home/light/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text' failed\r\nmake: *** [/home/light/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\r\n```", "comments": ["Please fill in the template and use ` ``` ` around the code locks. Please do some effort to make the issue readable so solutions can be provided faster.", "`contrib` no longer exists, please use `bazel` to build", "@tianhongbao, Please update for the above comment. Thanks!", "I want to build tensorflow_c++_1.13.\r\nI use bazel-0.19.2\uff0cProtobuf 3.5.0\r\n```\r\ngit clone --recursive https://github.com/tensorflow/tensorflow\r\ncd ./tensorflow\r\ngit checkout r1.13\r\n./configure\r\nbazel build --config=monolithic //tensorflow:libtensorflow_cc.so\r\n./tensorflow/contrib/makefile/download_dependencies.sh\r\n cd  ./tensorflow/contrib/makefile\r\n./build_all_linux.sh\r\n```\r\nI encountered this problem when I ran  \"./build_all_linux.sh\"\r\n", "If this problem is not solved, is there a docker for tensorflow c++ 1.13", "@gadagashwini @mihaimaruseac Can you help me solve this problem?", "Can you try building from master/2.2/2.1/1.15 branch instead?\r\n\r\n1.13 is too old and no longer supported", "@tianhongbao, Did you try @mihaimaruseac's workaround.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37752\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37752\">No</a>\n"]}, {"number": 37750, "title": "run label_image at linux-armv7l wrong", "body": "\r\ni follow the [guide](https://www.tensorflow.org/lite/guide/build_rpi) to build `minimal` and `label_image` for linux-armv7l, but when i run `label_image`, i got this error:\r\n`Illegal instruction (core dumped) `\r\ni build it at ubuntu18.04 with arm-linux-gnueabihf-gcc-4.8, and the ARM is cortexe-A9 with linux12.04.\r\nwhat should i do?", "comments": ["Could be either software (e.g., your environment doesn't not use HF) or hardware (e.g., Cortex-A9 has some hardware options, see ARM's [doc](https://developer.arm.com/docs/ddi0388/g/introduction/configurable-options)).", "> \u53ef\u4ee5\u662f\u8f6f\u4ef6\uff08\u4f8b\u5982\uff0c\u60a8\u7684\u73af\u5883\u4e0d\u4f7f\u7528HF\uff09\u6216\u786c\u4ef6\uff08\u4f8b\u5982\uff0cCortex-A9\u6709\u4e00\u4e9b\u786c\u4ef6\u9009\u9879\uff0c\u8bf7\u53c2\u9605ARM\u7684[\u6587\u6863](https://developer.arm.com/docs/ddi0388/g/introduction/configurable-options)\uff09\u3002\r\n\r\nHi, @freedomtan \r\ni run the `label_image` at cortex-A7 with linux version 4.1.15,and the gcc version is 4.9.3,it works well. but when i run it at coetex-A9 with linux version 3.0.15,and the gcc version is 4,4,1, it  give me the error back. so, maybe its because of the linux kernel's version or gcc's version ?", "Well, mostly TFLite works in user space. It should has little to do with Linux kernel. Surely, your compiler (e.g., gcc version) could be a problem. But I think you should figure what kind of Cortex-A9 you have first.", "@Yooong-W \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37749, "title": "Add error checks", "body": "Add error checks for close, closedir, fclose callings.", "comments": []}, {"number": 37748, "title": "TF 2.2.0 Windows Bazel 2.0.0 '@local_config_cuda//cuda'  (Python Path)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Winndows 10\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):  2.0.0\r\n- GCC/Compiler version (if compiling from source):  Visual Studio 2019 14.25.28610\r\n- CUDA/cuDNN version:  CUDA 10.2 cuDNN 7.5\r\n- GPU model and memory: RTX 2060 6Go\r\n\r\nthe problem is a bazel error when, I start building with bazel\r\nThe problem comes from \\tensorflow\\third_party\\gpus\\cuda_configure.bzl in the line 608:\r\n`return execute(repository_ctx, [python_bin, \"-c\", decompress_and_execute_cmd])`\r\n\r\nthe real problem comes from the python_bin Path when it executes that line:\r\n`system(\"C:/Program Files/Python37/python.exe script.py cuda cudnn\")`\r\nthe space between Program and Files make an error in execution.\r\n\r\nThe current solution that i have found is to move the python folder too : \r\n`C:/Python37/python.exe`\r\n\r\nThat move resolves that problem for now.\r\nI can't help more.\r\nthanks\r\n\r\n\r\n\r\n", "comments": ["@khaled-besrour,\r\nCould you please provide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "python ./configure.py\r\n\r\nCUDA enable\r\ncompute capability 7.35\r\n\r\n bazel build --config=opt --config=cuda --config=v2 --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n", "@khaled-besrour,\r\nI see that you have mentioned the cuDNN version as 7.5. As per the official TensorFlow [software requirements](https://www.tensorflow.org/install/gpu#software_requirements) you need cuDNN SDK (>= 7.6). \r\n\r\nCould you please try upgrading the cuDNN version to 7.6 or above and let us know if it works? Thanks!\r\n", "it's just a mistake i have a cuDNN 7.6 and a compute compatibility of 7.5.\r\n\r\nbut as said i resolve the problem by moving python it's not a DLL error, but python a script problem when the python path have space.\r\n\r\n`system(\"C:/Program Files/Python37/python.exe script.py cuda cudnn\")`\r\n\r\nit said that `C:/Program` is not executable, i think it come from bazel ", "@khaled-besrour thank you for the analysis.\r\n@meteorcloudy do you know how we can make sure paths with spaces are correctly handled here?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L609", "As @khaled-besrour pointed out, the problem comes from:\r\nhttps://github.com/tensorflow/tensorflow/blob/242129341e19afab300558df319a9e5e523b8092/third_party/gpus/cuda_configure.bzl#L606\r\n\r\nWe should quote the python_bin str in the command passed to `system` function.\r\n`system('\\\"%s\\\" script.py %s');`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37748\">No</a>\n", "Can you help me with this issue? I am new in this territory and need more explanation.\r\nI run:\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\nand I get these errors:\r\n\r\nINFO: Call stack for the definition of repository 'local_config_cuda' which is a cuda_configure (rule definition at C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl:1243:18):\r\n - <builtin>\r\n - C:/users/asus/tensorflow/tensorflow/workspace.bzl:90:5\r\n - C:/users/asus/tensorflow/WORKSPACE:19:1\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1213\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1016, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 217, in find_cc\r\n                _get_msvc_compiler(<1 more arguments>)\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 135, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace\r\n'NoneType' value has no field or method 'replace'\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1213\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1016, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 217, in find_cc\r\n                _get_msvc_compiler(<1 more arguments>)\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 135, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace\r\n'NoneType' value has no field or method 'replace'\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1213\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1016, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 217, in find_cc\r\n                _get_msvc_compiler(<1 more arguments>)\r\n        File \"C:/users/asus/tensorflow/third_party/gpus/cuda_configure.bzl\", line 135, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\").replace\r\n'NoneType' value has no field or method 'replace'\r\nINFO: Elapsed time: 2.967s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n\r\nC:\\Users\\asus\\tensorflow>", "Please open I new issue.\r\n\r\nAnyway, I think that the problem come from the global environment, have you add the MVSC_tool to your path?\r\n\r\nand if so check if you have the cl.exe file inside it \r\n ", "I solved the issue through setting the BAZEL_SH environment variable, restarting the pc and running the cmd with Administrator rights."]}, {"number": 37747, "title": "[TF 2.2] keras fit class weights are being cast to int", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- TensorFlow version : introduced since 2.2.0-rc0\r\n- Python version: - Bazel\r\nversion (if compiling from source): Python 3 (Colab)\r\n\r\nGoogle Colaboratory\r\nTensorflow 2.2.0-rc0 with Python 3\r\n\r\n\r\n**Describe the current behavior**\r\nThe class weights passed to keras model.fit are converted to int64, which leads to all class weights that are below 1 being set to 0.\r\n\r\n**Describe the expected behavior**\r\nThat the class weights are kept as float values, so that results can be more precise based on the class weights\r\n\r\n**Other info / logs**\r\nI tracked the piece of code that is responsible for this behavior: \r\n[https://github.com/tensorflow/tensorflow/blob/c0306ef626b02ab5ab10aac2cec6d08f56136a5c/tensorflow/python/keras/engine/data_adapter.py#L1258](url)\r\n`class_weight_tensor = ops.convert_to_tensor_v2(\r\n      [class_weight[c] for c in class_ids])\t      [int(class_weight[c]) for c in class_ids], dtype=\"int64\")`\r\n\r\nand it seems that this commit for TF 2.2 introduced it, but I cannot figure out why it was changed like this:\r\n[https://github.com/tensorflow/tensorflow/commit/10666c59dd4858645d1b03ce01f4450da80710ec#diff-f8dd40712ac721c1b363e1a1ec44c1a3](url)\r\n\r\nAs a temporary workaround, I tried to make my lowest weight normalized to 1, but then I still get a floating poant value for the other weight, so even if I round it, it will still not be as precise as I need it.\r\nSince I use tf.dataset loaded from GCS, I cannot simply pass the sample_weights directly myself either as a workaround.\r\n\r\n\r\nSo I am not sure whether is was a bug, or an intended change. In case it was an intended change, I would like to know what the alternative would be.\r\n", "comments": ["@mgmverburg, Could you provide the simple standalone code to replicate the reported issue. Thanks! ", "@gadagashwini I can try and make a snippet with a simple model etc. but I don't think it will show much, because it isn't an error that is thrown, just incorrect behavior which can be directly explained by the casting to int. So somehow we will need to find out what the rationale was behind that change, because quite likely there must have been a motivation for that change, and changing it back might hence have negative side-effects.\r\n\r\nBut if you think it will help, I will try and make a standalone snippet later today.", "@mgmverburg this [commit](https://github.com/tensorflow/tensorflow/commit/bfc51b2d2384a6945ce7023aa08f6c629766ebdf) should fix this issue. \r\nthe fix has also been cherrypicked into r2.2 branch. ", "> @mgmverburg this [commit](https://github.com/tensorflow/tensorflow/commit/bfc51b2d2384a6945ce7023aa08f6c629766ebdf) should fix this issue.\r\n> the fix has also been cherrypicked into r2.2 branch.\r\n\r\nAs far as I can tell, it is fixed! Thank you!", "Closing this as this was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37747\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37747\">No</a>\n"]}, {"number": 37746, "title": "Build TensorFlow library iOS from source", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iOS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source): 0.26.1\r\n\r\n\r\n**Describe the problem**\r\nI need to build Tensorflow 2.0.0 from source for iOS devices, to get a library libtensorflow-core.a, I found this link https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios where I found another link that seems to have the answer, but it redirect me to a missing page.\r\n\r\nHas anyone an idea or a simple guide on how I can build 2.0.0 library for iOS?\r\n\r\nThanks!\r\n\r\n", "comments": ["@SestoAle \r\nplease refer to [this link](https://www.tensorflow.org/lite/guide/build_ios) and let us know if it helps.", "@SestoAle\r\nplease update on above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37746\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37746\">No</a>\n"]}, {"number": 37745, "title": "[EfficientDet] ValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04 x86_64, CUDA10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): tf_nightly-2.2.0.dev20200319\r\n- Python version: 3.6\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nI am trying to perform Weight Quantization using converter from saved_model, but I am suffering from the problem that the INPUT OP of saved_model is not recognized correctly by converter. You can confirm that the INPUT OP is included in the saved_model in the following steps, but an error will occur when the conversion is performed.\r\n1. Download the **`EfficientDet`** **[google/automl](https://github.com/google/automl)** trained checkpoint from the link **[here](https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d0.tar.gz)**. This is the Google official release EfficientDet model. Then unzip the downloaded tar.gz file to any location.\r\n2. Clone the google / automl repository.\r\n```console:\r\n$ sudo pip3 install tf-nightly\r\n$ git clone https://github.com/google/automl.git\r\n$ cd automl/efficientdet\r\n```\r\n3. The following script freezes the downloaded checkpoint.\r\n```console\r\n$ python3 model_inspect.py \\\r\n  --model_name=efficientdet-d0 \\\r\n  --delete_logdir=False \\\r\n  --freeze=True \\\r\n  --runmode=freeze \\\r\n  --input_image_size=512 \\\r\n  --ckpt_path=${HOME}/Downloads/efficientdet-d0 \\\r\n  --logdir=${HOME}/Downloads/efficientdet-d0/log \\\r\n  --export_ckpt=${HOME}/Downloads/efficientdet-d0/export \\\r\n  --threads=4\r\n```\r\n4. Use the following Python script to extract the layer name of the INPUT layer. This indicates that the INPUT layer name of the model is \"input\".\r\n```python\r\n### tf-nightly 2.2.0-dev20200319\r\n\r\nimport tensorflow as tf\r\nimport os\r\nfrom tensorflow.python import ops\r\n\r\ndef get_graph_def_from_file(graph_filepath):\r\n  tf.compat.v1.reset_default_graph()\r\n  with ops.Graph().as_default():\r\n    with tf.compat.v1.gfile.GFile(graph_filepath, 'rb') as f:\r\n      graph_def = tf.compat.v1.GraphDef()\r\n      graph_def.ParseFromString(f.read())\r\n      return graph_def\r\n\r\n# Look up the name of the placeholder for the input node\r\ngraph_def=get_graph_def_from_file('./efficientdet-d0_train.pb')\r\ninput_name=\"\"\r\nfor node in graph_def.node:\r\n    if node.op=='Placeholder':\r\n        print(\"##### efficientdet-d0_train - Input Node Name #####\", node.name) # this will be the input node\r\n        input_name=node.name\r\n```\r\n5. The following command converts **`efficientdet-d0_train.pb`** and save_model is created in **` saved_model`** folder.\r\n```python\r\n### tf-nightly 2.2.0-dev20200319\r\n\r\nimport tensorflow as tf\r\nimport os\r\nimport shutil\r\nfrom tensorflow.python.saved_model import tag_constants\r\nfrom tensorflow.python import ops\r\n\r\ndef get_graph_def_from_file(graph_filepath):\r\n  tf.compat.v1.reset_default_graph()\r\n  with ops.Graph().as_default():\r\n    with tf.compat.v1.gfile.GFile(graph_filepath, 'rb') as f:\r\n      graph_def = tf.compat.v1.GraphDef()\r\n      graph_def.ParseFromString(f.read())\r\n      return graph_def\r\n\r\ndef convert_graph_def_to_saved_model(export_dir, graph_filepath, input_name, outputs):\r\n  graph_def = get_graph_def_from_file(graph_filepath)\r\n  with tf.compat.v1.Session(graph=tf.Graph()) as session:\r\n    tf.import_graph_def(graph_def, name='')\r\n    tf.compat.v1.saved_model.simple_save(\r\n        session,\r\n        export_dir,# change input_image to node.name if you know the name\r\n        inputs={input_name: session.graph.get_tensor_by_name('{}:0'.format(node.name))\r\n            for node in graph_def.node if node.op=='Placeholder'},\r\n        outputs={t.rstrip(\":0\"):session.graph.get_tensor_by_name(t) for t in outputs}\r\n    )\r\n    print('Optimized graph converted to SavedModel!')\r\n\r\n# Look up the name of the placeholder for the input node\r\ngraph_def=get_graph_def_from_file('./efficientdet-d0_train.pb')\r\ninput_name=\"input\"\r\noutputs = ['class_net/class-predict/BiasAdd:0',\r\n           'class_net/class-predict_1/BiasAdd:0',\r\n           'class_net/class-predict_2/BiasAdd:0',\r\n           'class_net/class-predict_3/BiasAdd:0',\r\n           'class_net/class-predict_4/BiasAdd:0',\r\n           'box_net/box-predict/BiasAdd:0',\r\n           'box_net/box-predict_1/BiasAdd:0',\r\n           'box_net/box-predict_2/BiasAdd:0',\r\n           'box_net/box-predict_3/BiasAdd:0',\r\n           'box_net/box-predict_4/BiasAdd:0']\r\n# convert this to a TF Serving compatible mode\r\nshutil.rmtree('./saved_model', ignore_errors=True)\r\nconvert_graph_def_to_saved_model('./saved_model', './efficientdet-d0_train.pb', input_name, outputs)\r\n```\r\n6. Confirm that saved_model is generated correctly by the following command.\r\n```console:\r\n$ saved_model_cli show --dir ./saved_model --all\r\n\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['input'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 512, 512, 3)\r\n        name: input:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['box_net/box-predict/BiasAdd'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 64, 64, 36)\r\n        name: box_net/box-predict/BiasAdd:0\r\n    outputs['box_net/box-predict_1/BiasAdd'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 32, 32, 36)\r\n        name: box_net/box-predict_1/BiasAdd:0\r\n    outputs['box_net/box-predict_2/BiasAdd'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 16, 16, 36)\r\n        name: box_net/box-predict_2/BiasAdd:0\r\n    outputs['box_net/box-predict_3/BiasAdd'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 8, 8, 36)\r\n        name: box_net/box-predict_3/BiasAdd:0\r\n    outputs['box_net/box-predict_4/BiasAdd'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 4, 4, 36)\r\n        name: box_net/box-predict_4/BiasAdd:0\r\n    outputs['class_net/class-predict/BiasAdd'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 64, 64, 810)\r\n        name: class_net/class-predict/BiasAdd:0\r\n    outputs['class_net/class-predict_1/BiasAdd'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 32, 32, 810)\r\n        name: class_net/class-predict_1/BiasAdd:0\r\n    outputs['class_net/class-predict_2/BiasAdd'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 16, 16, 810)\r\n        name: class_net/class-predict_2/BiasAdd:0\r\n    outputs['class_net/class-predict_3/BiasAdd'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 8, 8, 810)\r\n        name: class_net/class-predict_3/BiasAdd:0\r\n    outputs['class_net/class-predict_4/BiasAdd'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (1, 4, 4, 810)\r\n        name: class_net/class-predict_4/BiasAdd:0\r\n  Method name is: tensorflow/serving/predict\r\n```\r\n7. Finally, an error occurs when performing Weight Quantization with the following Python script.\r\n```python\r\n### tf_nightly-2.2.0.dev20200319\r\nimport tensorflow as tf\r\n\r\n# Weight Quantization - Input/Output=float32\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./saved_model')\r\n#converter.experimental_new_converter = True\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops = True\r\ntflite_quant_model = converter.convert()\r\nwith open('./efficientdet-d0_train.tflite', 'wb') as w:\r\n    w.write(tflite_quant_model)\r\nprint(\"Weight Quantization complete! - efficientdet-d0_train.tflite\")\r\n```\r\nThe following error occurs even though the \"input\" Placeholder does exist in the last check procedure.\r\n```\r\nValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```console\r\n2020-03-20 17:36:18.835931: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:18.836031: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:18.836056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (MetaGraph import) with ops with custom gradients. Will likely fail if a gradient is requested.\r\n2020-03-20 17:36:20.375954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-03-20 17:36:20.388477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-03-20 17:36:20.388790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 with Max-Q Design computeCapability: 6.1\r\ncoreClock: 1.2655GHz coreCount: 16 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-03-20 17:36:20.388916: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:20.389000: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:20.389063: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:20.389126: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:20.389185: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:20.389246: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:20.391780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-03-20 17:36:20.391813: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-03-20 17:36:20.392010: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-20 17:36:20.414641: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\r\n2020-03-20 17:36:20.415705: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xc1805f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-20 17:36:20.415744: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-03-20 17:36:20.460041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-03-20 17:36:20.460435: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xc216420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-03-20 17:36:20.460451: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070 with Max-Q Design, Compute Capability 6.1\r\n2020-03-20 17:36:20.460530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-03-20 17:36:20.460538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      \r\n2020-03-20 17:36:21.550114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-03-20 17:36:21.550409: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-03-20 17:36:21.550490: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-03-20 17:36:21.551093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-03-20 17:36:21.551353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 with Max-Q Design computeCapability: 6.1\r\ncoreClock: 1.2655GHz coreCount: 16 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-03-20 17:36:21.551456: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:21.551525: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:21.551587: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:21.551648: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:21.551710: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:21.551772: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\r\n2020-03-20 17:36:21.551790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-03-20 17:36:21.551796: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-03-20 17:36:21.551808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-03-20 17:36:21.551812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-03-20 17:36:21.551818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-03-20 17:36:21.582079: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-03-20 17:36:21.582108: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2020-03-20 17:36:21.582113: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nTraceback (most recent call last):\r\n  File \"03_weight_quantization.py\", line 12, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 423, in convert\r\n    self._funcs[0], lower_control_flow=False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/convert_to_constants.py\", line 506, in convert_variables_to_constants_v2\r\n    raise ValueError(\"Cannot find the Placeholder op that is an input \"\r\nValueError: Cannot find the Placeholder op that is an input to the ReadVariableOp.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d0.tar.gz\r\n```\r\n\r\n**Failure details**\r\nThe error occurs even though the \"input\" Placeholder does exist in the last check procedure.\r\n\r\n\r\n**Any other info / logs**\r\n\r\nThe .pb, checkpoint, and script I used to check are committed below.\r\n**https://github.com/PINTO0309/PINTO_model_zoo/tree/master/18_EfficientDet/01_float32**\r\n", "comments": ["Hi  @PINTO0309 , I follow the steps to create the frozen graph .pb, also I have the same problem for create the .tflite. \r\n\r\nOn the other hand, do you have any way to make inference with this frozen .pb ? , i was following the steps from the classic https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb but I have this problem in the end trying to make some inference. \r\n\r\n```console\r\nFailedPreconditionError:  Error while reading resource variable fpn_cells/cell_2/fnode3/WSM_1_load_168070 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/fpn_cells/cell_2/fnode3/WSM_1_load_168070/N10tensorflow3VarE does not exist.\r\n\t [[node fpn_cells/cell_2/fnode3/Relu_1/ReadVariableOp (defined at :3) ]] [Op:__inference_pruned_179631]\r\n\r\nFunction call stack:\r\npruned\r\n```\r\n\r\nThanks @PINTO0309 ", "@stanlee321 \r\nAlthough it is not directly related to this issue, it has succeeded until I created a .pb of EfficientDet and confirmed the operation below. However, it is not an official Google model. If you stick with the official model, ignore it. Unfortunately, I haven't tested with the official model .pb so far. My goal is to infer fast on low-spec devices such as RaspberryPi. I hope that my information is useful.\r\n## Sample repository\r\n**https://github.com/xuannianz/EfficientDet.git**\r\n## Environment\r\n- Ubuntu18.04 x86_64\r\n- Pascal-VOC 2012 Dataset Retraining\r\n- EfficientDet D0\r\n- Threshould 0.70\r\n## Demo result\r\n**https://twitter.com/PINTO03091/status/1240794115945549824**\r\n![FireShot Capture 004 - Super PINTO on Twitter_ _EfficientDet \u306e b0 \u30e2\u30c7\u30eb\u3092 50epoch \u56de\u3057\u3066freezegrap_ - twitter com](https://user-images.githubusercontent.com/33194443/77219075-7f576100-6b75-11ea-910a-60599dec4efd.png)", "\r\nThank you @PINTO0309 !!", "The problem that Placeholder is not detected has been solved by tf-nightly==2.2.0-dev20200411. So this issue will be closed.\r\n\r\nThe following is an error log that is displayed when issue is resolved and weight quantification is performed, but it has nothing to do with this issue.\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"05_weight_quantization.py\", line 13, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 621, in convert\r\n    self._funcs[0], lower_control_flow=False))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 706, in convert_variables_to_constants_v2_as_graph\r\n    func, lower_control_flow, aggressive_inlining)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py\", line 512, in _convert_variables_to_constants_v2_impl\r\n    func.prune([], [identity_node.name])()[0])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1605, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1645, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError:  Attempting to use uninitialized value fpn_cells/cell_0/fnode3/resample_0_0_8/conv2d/kernel\r\n\t [[node Identity (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py:621) ]] [Op:__inference_pruned_10971]\r\n\r\nFunction call stack:\r\npruned\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37745\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37745\">No</a>\n"]}, {"number": 37744, "title": "tflite | -O3 warning during tflite dll build in windows", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.2.0rc1\r\n- Bazel version (if compiling from source): 2.0.0\r\n\r\n**Describe the problem**\r\nI'm getting a long list of -O3 warnings (below is only a small selection) during the tflite dll build in windows. The option -O3 should be changed to /O3 (or removed) for all windows builds.\r\n```\r\nINFO: From Compiling tensorflow/lite/core/subgraph.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/model.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/shape.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/skip_gram.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/segment_sum.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/if.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/floor_div.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/round.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/resize_nearest_neighbor.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/basic_rnn.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/slice.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/arg_min_max.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/local_response_norm.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/squared_difference.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\nINFO: From Compiling tensorflow/lite/kernels/squeeze.cc:\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`bazel build -c opt //tensorflow/lite:tensorflowlite`", "comments": ["It's added by https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/build_def.bzl#L30\r\n\r\nI wanted use something like following,\r\n```\r\n    }) + select({\r\n        clean_dep(\"//third_party/tensorflow:windows\"): [\"/O3\"],\r\n        clean_dep(\"//third_party/tensorflow:optimized\"): [\"-O3\"],\r\n        \"//conditions:default\": [],\r\n```\r\nBut it didn't work since tensorflow:optimized and tensorflow:windows can be selected together.\r\nThere might be a solution. Please feel free to suggest a PR on this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37744\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37744\">No</a>\n"]}, {"number": 37743, "title": "Predictions are not accurate after loading the saved model - Resume classification", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: NA\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1.0\r\n- Python version: - Bazel\r\nversion (if compiling from source):3.7\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWe have created a sample resume classification model and saved the model with morethan 95% accuracy on train and test data set.After we have loaded the model and trying to make the predictions by defining prediction_classes using tf.argmax and we have observed some ambuiguity in predictions(Not predicting the correct label)\r\n\r\n**Describe the expected behavior**\r\n\r\nThe predictions should be accurate after loading the model\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n[Resume_classification_V_1_5.zip](https://github.com/tensorflow/tensorflow/files/4358924/Resume_classification_V_1_5.zip)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n", "comments": ["You can find the data from the below link(New_Data.zip)\r\n\r\nhttps://drive.google.com/corp/drive/folders/1IJjiaHUBt_Gq6XXV8NPazd0cWZ2LzkW_", "@ravikairam, Drive link is leading to 404 error. Please provide the proper link. Thanks! ", "@gadagashwini \r\n\r\nCan you please copy and paste the link in new  web browser and see if you are facing any issue. Thanks!", "@ravikairam Are you still facing the issue?", "@gowthamkpr \r\n\r\nYes the issue still persists. Predictions are accurate on train and validation data before saving the model.After saving the model we loaded the model and tested on new unseen data where predictions are going wrong. I tried with saving different file formats(.h5,.pb formats) but still problem exists, Can you please help us to fix this issue . Thanks!", "@ravikairam I am closing this issue as it has been resolved. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37743\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37743\">No</a>\n"]}, {"number": 37742, "title": "Add more ops in tflite", "body": "**System information**\r\n- Windows 10\r\n- TensorFlow 2.0\r\n\r\n\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MUL, SUB. Here is a list of operators for which you will need custom implementations: IdentityN.\r\n\r\n\r\n", "comments": ["@kaiwalya4850\r\nplease share simple stand alone code for us to replicate the issue faced.", "Here's the code for the same:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# Load the MobileNet tf.keras model.\r\nmodel = \"C:/Users/Kaiwalya/Desktop/sign_language/new/saved_model.pb\"\r\n\r\n# Convert the model.\r\nconverter = lite.TFLiteConverter.from_saved_model(model)\r\ntflite_model = converter.convert()\r\n```", "We are already at the size limit for our pips, so this will need to wait a while until lite can be separated from TF", "Any alternative to this? Anything else that i can do? \r\nThank you.", "Assigning @jdduke from TF Lite. I guess if the ops don't take much space we can take them, but letting TF Lite provide guidance", "Can you try with the latest TF nightly build? Or TF 2.2? IdentityN should be supported now.", "Yeah, thats working fine now!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37742\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37742\">No</a>\n"]}, {"number": 37741, "title": "load_model error when model has multiple inputs", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\nThis is the last bit of the error\r\n\r\n> ValueError: Layer lstm expects 1 inputs, but it received 3 input tensors. Inputs received: [<tf.Tensor 'input_1_2:0' shape=(None, 368, 32) dtype=float32>, <tf.Tensor 'input_2_2:0' shape=(None, 72) dtype=float32>, <tf.Tensor 'input_3_2:0' shape=(None, 72) dtype=float32>]\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  no\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): W10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if \r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel 3.7 for Python and latest 2.1 for TF-gpu\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory: 10.1 -- 7.6 -- TitanRTX\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nCreated a SIMPLE model with multiple inputs as shown below, saved it with model.save method but failed to load it.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ndef create_model():\r\n    ip1 = tf.keras.layers.Input(shape=(seq_len, ip_size))\r\n    ip2 = tf.keras.layers.Input(shape=(hidden_size,))\r\n    ip3 = tf.keras.layers.Input(shape=(hidden_size,))\r\n    y0 = tf.keras.layers.LSTM(units=hidden_size)(inputs=ip1, initial_state=[ip2, ip3])\r\n    y1 = tf.keras.layers.Dense(units=num_points, activation=tf.keras.activations.tanh)(y0)\r\n    y = tf.keras.layers.Dense(units=num_points)(y1)\r\n    model = tf.keras.models.Model(inputs=[ip1, ip2, ip3], outputs=y, name='cheater_model')\r\n    return model\r\n```\r\nmodel = create_model()\r\nmodel.save(path)\r\nmodel = tf.keras.models.load(path)\r\n\r\nPS: fill up the variables with any numbers, it doesn't matter\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@kernelizd, please share full code or colab link to reproduce the issue.", "@khimraj Hi, after hard work, I reproduced the simplist **shortest** example of the problem on colab as you requested. Please have a look here:\r\n\r\nhttps://colab.research.google.com/gist/kernelizd/c79941c4f919fbb441e8bf9998dc42f5/untitled6.ipynb\r\n", "@kernelizd \r\n\r\nCan you please try with latest TF version(`!pip install tensorflow==2.2.0rc1`) . I am not seeing any issue with 2.2.0rc1 version. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/7272689f2e7769ff5da86ec3967de9d8/untitled744.ipynb). Thanks!", "@ravikyram Hi. I installed 2.2 releae candidate, its working smoothly now.\r\nThank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37741\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37741\">No</a>\n"]}]