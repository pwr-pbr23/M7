[{"number": 47389, "title": "Fix describe error in keil readme template", "body": "Fix describe error in keil readme template\r\n\r\n# TensorFlow Lite Micro Mbed Project \r\n-> \r\n# TensorFlow Lite Micro Keil Project ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47389) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 47388, "title": "Libcudnn Major Version Variable attached", "body": "The libcudnn Major Version was not used in for getting the Packages with apt-get", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47388) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Thanks for this. Can you explain more about the situation where this is causing issues? I don't really understand why this will be better.\r\n\r\nAlso, please follow the instructions in the README for the docker directory to regenerate the dockerfiles.", "Hey Angerson,\r\nI played around with these Dockerfiles to get my Dockerimage with Python 3.8 running when I found this. It was not really necessary for me to change but I thought it will be useful in the Future. When a new Libcudnn Major Version will be availble - it won't be necessary to change the Major Version on all the Positions in the dockerfile - just change the Major Version in top of the File.  Also the Major Version is set at the moment but not used anywhere.", "@angerson Can you please take a look on the above comment from @kellergoech. Thanks!\r\n", "@angerson  Any update on this PR? Please. Thanks!", "We're working on new dockerfiles over at https://github.com/tensorflow/build/pull/21, and since this PR isn't a significant change, I'll close it.\r\n\r\nThanks!"]}, {"number": 47387, "title": "Unable to migrate TF1 code to TF2, tape.gradient returns None", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos big sur\r\n- TensorFlow installed from (source or binary): binary, pip\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.7\r\n\r\n**Describe the current behavior**\r\nI'm migrating code from TF1 that uses `tf.gradients()` for doing a custom gradient calculation. I'm trying to get the same results I get with TF1 in TF2 using `tf.GradientTape()` however, no matter what I do including:\r\n- I tried to use `tape.watch()` and the issue persists.\r\n- I tried manually creating a `tf.Variable()` with `trainable=True`, watch the variable and the issue persists.\r\n- I tried using `tf.gradients()` within a `tf.function` and `tf.compat.v1.gradients()` if there is any difference at all, and the issue persists\r\n\r\nHere's a jupyter [notebook](https://colab.research.google.com/drive/1eeEc9eIlvMVAJbueIfhpgh569pi-lGx4?usp=sharing) with the full code to be able to reproduce the issue.\r\n\r\nHere's the [code](https://github.com/openai/baselines/blob/master/baselines/acer/acer.py) I'm migrating. Check lines [156 - 176]. Below is the part of interest:\r\n\r\n```\r\n    g = tf.gradients(-loss, f)  # loss being a float and f being a (m, n) tensor\r\n    k = -f_pol / (f + eps)  # f_pol another (m, n) tensor and eps a float\r\n    k_dot_g = tf.reduce_sum(k * g, axis=-1)\r\n    adj = tf.maximum(\r\n        0.0,\r\n        (tf.reduce_sum(k * g, axis=-1) - delta)\r\n        / (tf.reduce_sum(tf.square(k), axis=-1) + eps),\r\n    )\r\n    g = g - tf.reshape(adj, [nenvs * nsteps, 1]) * k\r\n    grads_f = -g / (nenvs * nsteps)\r\n    grads_policy = tf.gradients(f, params, grads_f)  # params being the model parameters\r\n```\r\n\r\nand here's a simplified version of what I'm trying to do:\r\n\r\n    with tf.GradientTape() as tape:\r\n        f = calculate_f()\r\n        f_pol = calculate_f_pol()\r\n        others = do_further_calculations()\r\n        loss = calculate_loss()\r\n    g = tape.gradient(-loss, f)\r\n    print(g)\r\n\r\nresults in:\r\n\r\n    None\r\n\r\n**Describe the expected behavior**\r\n\r\nAs far as I understand `tf.GradientTape()` is the TF2 alternative to `tf.gradients()`. I'm trying to replicate the exact same results in TF2 and it doesn't work, so this implies either there is something wrong with my code or it is a bug. This is not the first time it happens with someone, I found numerous other complains including closed issues and neither includes a solution I have not tried.\r\n \r\n**Standalone code to reproduce the issue**\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\njupyter [notebook](https://colab.research.google.com/drive/1eeEc9eIlvMVAJbueIfhpgh569pi-lGx4?usp=sharing) \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@emadboctorx \r\n\r\nI have tried in colab with TF-nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d5a2fef218318c7c8535965be44c4f4e/untitled679.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47387\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47387\">No</a>\n"]}, {"number": 47386, "title": "[XLA:GPU] Build failure (bazel cache end up being corrupt)", "body": "This commit:\r\n\r\n```\r\ncommit 9f8d5d0ef4e990ab2398d573a14aaa8ed02a10e2\r\nAuthor: Rahul Joshi <jurahul@google.com>\r\nDate:   Mon Feb 22 08:41:59 2021 -0800\r\n\r\n    [MLIR:LHLO] Add optional call target arg mapping to LMHLO CustomCall operations.\r\n\r\n    - XLA:HLO -> LMHLO conversion drops all token arguments and return values, however\r\n      custom calls that users write still expect to get buffer pointers for these token types.\r\n    - To be able to support this, add an optional call target argument mapping attribute to\r\n      LMHLO custom calls. When this attribute is present, it indicates the number of\r\n      arguments and returns that the custom call expects and also indicates which LMHLO\r\n      arg() or output() maps to which arg or result number of the custom call.\r\n\r\n    PiperOrigin-RevId: 358826664\r\n    Change-Id: I36e839e9ff5b73890715b71717a4c13631955fba\r\n```\r\n\r\nIntroduced a compilation regression. Before that commit, this command line works:\r\n\r\n```\r\nbazel test -c opt --config=cuda //tensorflow/compiler/xla/tests:conv_depthwise_test_gpu //tensorflow/compiler/xla/tests:grouped_convolution_test_gpu\r\n```\r\n\r\nStarting at this commit, it gives this error:\r\n```\r\nERROR: /home/fbastien/github/tensorflow-tf2-upstream2/tensorflow/compiler/mlir/hlo/BUILD:485:11: undeclared inclusion(s) in rule '//tensorflow/compiler/mlir/hlo:lhlo':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/lhlo_ops_structs.cc':\r\n  'bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'\r\n  'bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc'\r\ncc1plus: warning: command line option \u2018-Wno-pointer-sign\u2019 is valid for C/ObjC but not for C++\r\n```\r\n\r\n@jurahul that authored it.", "comments": ["Thanks for the bug. It was reported internally as well, and I have a fix in flight.", "Should be fixed now with https://github.com/tensorflow/tensorflow/commit/d439d8c6d4137e0bc468a2de06e41b375029ee92. Would be great if you can confirm.", "I still have this error:\r\n```\r\nERROR: /home/fbastien/github/tensorflow-tf2-upstream2/tensorflow/compiler/xla/service/gpu/BUILD:1635:11: undeclared inclusion(s) in rule '//tensorflow/compiler/xla/service/gpu:buffer_comparator':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/xla/service/gpu/buffer_comparator.cc':\r\n  'bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'\r\n```\r\n\r\nwith your commit and with e5bf7893fefb12b796c075d6d4cd3a608d84c2f2.", "Thanks for checking. Looking again, I guess I need to attempt to build TF from OSS and try to reproduce the issue first.", "I was able to resolve this by wiping out my bazel cache.\r\n\r\n\r\n```\r\nrm -rf ~/.cache/bazel \r\n```", "Thanks Michael. I also was able to build OSS TF after this change. Back to @nouiz to check if deleting the bazel cache fixes the issue.", "I hit this error too. Doing a `bazel clean --expunge` fixed it. Does this indicate a bug in Bazel?", "I have the same behavior as @benbarsdell. the clean shouldn't be needed.", "I guess there is nothing to fix now on the tensorflow side. @mihaimaruseac should this issue be reported to bazel?", "Yes please, that's a better approach.", "I have filed a new issue with bazel: https://github.com/bazelbuild/bazel/issues/13135. @nouiz, would it be possible for you to provide build configuration information on the bazel issue? After that, I think we can close this issue.", "Closing this as this is a bazel issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47386\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47386\">No</a>\n", "We didn't got any update on the bazel bug in ~2.5 mounts. I poked them yesterday, but still no news.\r\n\r\nThis is very troublesome for us. Mostly, we can't use git bisect or reuse the same directory when moving between branch. This normally help save TF compilation time that is very slow without a build cluster as you have.\r\n\r\nAny idea how to make sure this get fixed?\r\n\r\nAlso, are you sure it isn't a bug in TF bazel rules and not inside bazel itself?"]}, {"number": 47385, "title": "Port micro op EXPAND_DIMS for it to pass the tests", "body": "PR4 for Issue #46258. Please review this PR carefully. Several notes about the implementation:\r\n1. Only constant tensors are supported; dynamic ones are not. This is different from the TFLite counterpart, which supports both.\r\n2. For constant tensors, the TFLite op calls GetAxisValueFromTensor() and ExpandTensorDim() in the Prepare function. The TFLM op does not call either of them in Prepare, but in the Eval function.\r\n3. The code to check the output tensor's shape starts at line 57, lite/micro/kernels/expand_dims_test.cc.\r\n4. The output_dims used to configure the output tensor must have the right dimension number.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@rsun-bdti  Can you please resolve conflicts? Thanks!\r\n", "@rsun-bdti Can you please resolve conflicts? Thanks!", "@rsun-bdti Thank you for the PR, very useful addition (Keras seems to be using EXPAND_DIMS op a lot for Convolutions and Pooling). I built an Arduino library with the repository content from your PR branch, it did work, however I had to change \r\nhttps://github.com/rsun-bdti/tensorflow/blob/8e5d6da9ab9b5de6be37d19099ce156f5102d59e/tensorflow/lite/micro/micro_mutable_op_resolver.h#L218\r\nto \r\n`return AddBuiltin(BuiltinOperator_EXPAND_DIMS, Register_EXPAND_DIMS(),`\r\nNot sure if that was intentional or a bug, EXPAND_DIMS and EXP are two different operators...\r\nAnother thing I did, just for testing is adding \r\nAddExpandDims();\r\nto all_ops_resolver.cpp. I switch to macro ops resolver once the code and model are tested and polished, but all ops resolver is useful for testing, when you're trying different architectures."]}, {"number": 47384, "title": "H fusion sharing opnd with user upstream again", "body": "\r\nRe-submitting the PR from https://github.com/tensorflow/tensorflow/pull/46614.", "comments": ["@cheshire, the fix to the issue is quite straightforward as in this [commit](https://github.com/tensorflow/tensorflow/pull/47384/commits/941e049f0bf9a26876336eaa0b4b6a14d075f2ad). Other codes are the same as in the previous PR.\r\n\r\nPlease help to take a look again!\r\n\r\n", "@penpornk Could you take a look at the oneDNN failure? Looks spurious to me.", "@cheshire It's not related to this PR. I think I know what PR causes it. Thank you for letting me know!\r\nThis check is non-blocking (doesn't block pulling the PR in) so please just ignore it. :)"]}, {"number": 47383, "title": "ModifyGraphWithDelegate(delegate) running very slow (ubuntu with USB accelerator)", "body": "Hi, \r\nI wrote a minimal C++ code loading the mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite\r\nand doing some inferencing.\r\nWhen running on a ubuntu-16.04 notebook (i7, SSD, 12G) with USB accelerator,\r\ninterpreter_->ModifyGraphWithDelegate\r\nruns substantially (50x) slower than the same(see below) code on the dev-board.\r\n\r\nDev-board:\r\nCode line: interpreter_->ModifyGraphWithDelegate({delegate, edgetpu_free_delegate});\r\nlibedgetpu version: libedgetpu1-std:arm64                14.1\r\nrun time: 0.07sec (incl. loading network etc)\r\n\r\nUbuntu 16.04, \r\nCode line: interpreter_->ModifyGraphWithDelegate(delegate);\r\n[ the version above with the 2 delegates can't be compiled]\r\nlibedgetpu: libedgetpu1-max/coral-edgetpu-stable,now 15.0 amd64 [installed]\r\nTENSORFLOW: tensorflow_src cloned today and built tflite library\r\nruntime:  3.2sec (!)\r\n\r\nAfter that, the inference is running about only half as fast as on the devboard. \r\n(slower USB speed??? or problem with the delegate?)\r\n\r\nCode snippet:\r\n std::unique_ptr<tflite::FlatBufferModel> model;\r\n  model = tflite::FlatBufferModel::BuildFromFile(model_path.c_str());\r\n\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  tflite::InterpreterBuilder(*model, resolver)(&interpreter_);\r\n\r\n  size_t num_devices;\r\n  std::unique_ptr<edgetpu_device, decltype(&edgetpu_free_devices)> devices(\r\n      edgetpu_list_devices(&num_devices), &edgetpu_free_devices);\r\n  TFLITE_MINIMAL_CHECK(num_devices);\r\n  const auto& device = devices.get()[0];\r\n  auto* delegate = edgetpu_create_delegate(device.type, device.path, nullptr, 0);\r\n  ON DEVBOARD:  interpreter_->ModifyGraphWithDelegate({delegate, edgetpu_free_delegate});\r\n  ON UBUNTU/USB: interpreter_->ModifyGraphWithDelegate(delegate);\r\n\r\nThank you!\r\n-johann\r\n\r\n", "comments": ["@JohannSchumann \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "Hi,\nhere the details - sorry forgot to add them.\n\n1) small C++ file minimal.cc which opens the mobilenet.. and does the\nModifyGraphWithDelegate(): below\n2) compilation via make (commands for each platform see below)\n3) execution:\n       time ./min_example\n4) results: Wall-clock:\n    Coral DevBoard:   0.053s\n    ubuntu notebook: 3.25 s (!)    with USB Accelerator\n\n5) Versions:\ncoralboard:\nuname -a:    Linux tuned-apple 4.14.98-imx #1 SMP PREEMPT Fri Nov 8\n23:28:21 UTC 2019 aarch64 GNU/Linux\ndpkg -l:         iil  libedgetpu1-std:arm64  14.1    arm64        Support\nlibrary for Edge TPU\ntf:  source tree from github.  Release 2.1.0   (see RELEASE.md in top dir)\n\nNotebook:  ubuntu, 16.04, i7, 12GB\nuname -a: Linux johann-Aspire-R5-571TG 4.15.0-133-generic\n#137~16.04.1-Ubuntu SMP Fri Jan 15 02:55:18 UTC 2021 x86_64 x86_64 x86_64\nGNU/Linux\ndpkg -l: ii  libedgetpu1-max:amd64   15.0    amd64        Support library\nfor Edge TPU\ntf:  source tree from github.  Release 2.5.0   (see RELEASE.md in top dir)\nusb-devices:  (for that USB accelerator):\nT:  Bus=01 Lev=01 Prnt=01 Port=03 Cnt=02 Dev#= 15 Spd=480 MxCh= 0\nD:  Ver= 2.10 Cls=00(>ifc ) Sub=00 Prot=00 MxPS=64 #Cfgs=  1\nP:  Vendor=18d1 ProdID=9302 Rev=01.00\nC:  #Ifs= 1 Cfg#= 1 Atr=80 MxPwr=498mA\nI:  If#= 0 Alt= 0 #EPs= 6 Cls=ff(vend.) Sub=ff Prot=ff Driver=(none)\n\nSame run times with the -std libedgetpu on the ubuntu notebook\n\nCompilation on coralboard:\ng++  -o \"min_example\" minimal.cc\n/home/mendel/TENSORFLOW/tensorflow/tensorflow/lite/tools/make/gen/aarch64_armv8-a/lib/libtensorflow-lite.a\n-std=c++11 -O3 -Wall -I/usr/lib/aarch64-linux-gnu/glib-2.0/include\n-I/home/mendel/TENSORFLOW/tensorflow\n-I/home/mendel/TENSORFLOW/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include\n -lrt -ldl -lpthread -ledgetpu\n\nCompilation on notebook:\ng++  -o \"min_example\" minimal.cc\n/home/johann/TOOLS/TENSORFLOW/tensorflow_src/tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a\n-std=c++11 -O3 -Wall -I/home/johann/TOOLS/TENSORFLOW/tensorflow_src\n-I/home/johann/TOOLS/TENSORFLOW/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include\n -lrt -ldl -lpthread -ledgetpu\n\n//--------------------------- small C++ example code -----------------------\n#include <sys/stat.h>\n#include <iostream>\n#include <memory>\n#include <vector>\n#include <fstream>\n#include <string>\n\n#include <errno.h>\n#include <fcntl.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n#include <unistd.h>\n\n#include \"inferencewrapper.h\"\n#include \"edgetpu_c.h\"\n#include \"tensorflow/lite/builtin_op_data.h\"\n#include \"tensorflow/lite/kernels/register.h\"\n#include \"tensorflow/lite/model.h\"\n\n#define TFLITE_MINIMAL_CHECK(x)                              \\\n  if (!(x)) {                                                \\\n    fprintf(stderr, \"Error at %s:%d\\n\", __FILE__, __LINE__); \\\n    exit(EXIT_FAILURE);                                      \\\n  }\n\nint main(int argc, char* argv[]) {\n\n  std::unique_ptr<tflite::FlatBufferModel> model;\n  std::unique_ptr<tflite::Interpreter> interpreter_;\n\n  model =\ntflite::FlatBufferModel::BuildFromFile(\"mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite\");\n  TFLITE_MINIMAL_CHECK(model != nullptr);\n\n  tflite::ops::builtin::BuiltinOpResolver resolver;\n  tflite::InterpreterBuilder(*model, resolver)(&interpreter_);\n\n  size_t num_devices;\n  std::unique_ptr<edgetpu_device, decltype(&edgetpu_free_devices)> devices(\n      edgetpu_list_devices(&num_devices), &edgetpu_free_devices);\n  TFLITE_MINIMAL_CHECK(num_devices);\n  printf(\"Wrapper: found %d devices\\n\",(int)num_devices);\n\n  // edgetpu_verbosity(10);\n\n  const auto& device = devices.get()[0];\n  auto* delegate =\n      edgetpu_create_delegate(device.type, device.path, nullptr, 0);\n//  interpreter_->ModifyGraphWithDelegate({delegate,\nedgetpu_free_delegate});\n  interpreter_->ModifyGraphWithDelegate(delegate);\n  printf(\"ModifyGraph done\\n\");\n  return 0;\n  }\n\n\nOn Thu, Feb 25, 2021 at 5:54 AM Saduf2019 <notifications@github.com> wrote:\n\n> @JohannSchumann <https://github.com/JohannSchumann>\n> We see that the issue template has not been filled, could you please do so\n> as it helps us analyse the issue [tf version, steps followed before you ran\n> into this error or stand alone code to reproduce the issue faced]\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47383#issuecomment-785611071>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAHRRD2PHI3LLU7RWGGLZH3TAXJYRANCNFSM4YFMJ6EA>\n> .\n>\n", "@JohannSchumann did you use USB 3.0 port to connect the accelerator? ", "Hi,\n\nyes, it's connecting to USB 3.0 - see below.\n\nlsusb -t\n/:  Bus 02.Port 1: Dev 1, Class=root_hub, Driver=xhci_hcd/6p, 5000M\n    |__ Port 1: Dev 2, If 0, Class=Application Specific Interface, Driver=,\n5000M\n/:  Bus 01.Port 1: Dev 1, Class=root_hub, Driver=xhci_hcd/12p, 480M\n    |__ Port 2: Dev 17, If 0, Class=Human Interface Device, Driver=usbhid,\n1.5M\n\ndmesg | grep usb\n[343085.437163] usb 2-1: new SuperSpeed USB device number 2 using xhci_hcd\n[343085.458086] usb 2-1: New USB device found, idVendor=1a6e, idProduct=089a\n[343085.458091] usb 2-1: New USB device strings: Mfr=0, Product=0,\nSerialNumber=0\n...\n\nOn Wed, Mar 3, 2021 at 5:11 AM Terry Heo <notifications@github.com> wrote:\n\n> @JohannSchumann <https://github.com/JohannSchumann> did you use USB 3.0\n> port to connect the accelerator?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47383#issuecomment-789413802>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAHRRDYWFM3NR6D42MQCYMLTBWZI3ANCNFSM4YFMJ6EA>\n> .\n>\n", "Is the source at https://github.com/tensorflow/tensorflow/issues/47383#issuecomment-785951123 what you're using?\r\nCould you share where you referred?\r\n\r\nWhat I found was the following page which uses external context. It might worth to try.\r\nhttps://coral.ai/docs/edgetpu/tflite-cpp/#set-up-the-tf-lite-interpreter-with-libedgetpu\r\n\r\nAlso I wonder if you see the same symptom with Python example.", "Hi,\n\nIs the source at #47383 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/47383#issuecomment-785951123>\n> what you're using?\n>\nThe exact code that I am running is listed in this comment.\n\n> What I found was the following page which uses external context. It might\n> worth to try.\n>\n> https://coral.ai/docs/edgetpu/tflite-cpp/#set-up-the-tf-lite-interpreter-with-libedgetpu\n>\n> I followed that, but it didn't change\n\n> Also I wonder if you see the same symptom with Python example.\n>\n Yes. The long run-time shows up in the \"make interpreter\". After that,\nrun-times seem to be OK.\nSo the problem seems to be in the START-UP of the accelerator\nFor the parrot example (\nhttps://coral.ai/docs/accelerator/get-started/#3-run-a-model-on-the-edge-tpu\n):\npython startup: 0.27s  (wall time)\nStartup + make_interpreter: 2.9s <--- *this is the very long time*\nfull example:  2.966s     (inference times:   11ms, 2.4ms, 2.4ms...)  <----\nthese times seem to be OK\n\n[this is not the first run after plug-in, where the firmware seems to be\ndownloaded]\n\nFor the C++ example, I ran with verbosity = 10. The log-file (see below)\nstops noticeably at:\nafter: [I :1386] Open device and check if DFU is needed\nbefore [I :1013] OpenDevice: [/sys/bus/usb/devices/2-2]\nand\nafter: I :287] Close: performing graceful reset\nbefore: I :320] Close: final clean up completed\n\nWaiting with any timeouts there?\n\nThank you!\n\n=====================LOG file (abbreviated) [C++ example]\n===========================\n$ time min_example\nWrapper: found 1 devices\nI :453] No matching device is already opened for shared ownership.\nI :31] Failed to open /sys/class/apex: No such file or directory\nI :944] EnumerateDevices: vendor:0x1a6e, product:0x89a\nI :979] EnumerateDevices: checking bus[2] port[2]\n...\nI :998] EnumerateDevices: found [/sys/bus/usb/devices/2-2]\n...\nI :225] Enumerate: adding path [/sys/bus/usb/devices/2-2]\nI :104] USB always DFU: False (default)\nI :145] USB bulk-in queue capacity: 8\nI :65] Performance expectation: Max (default)\nI :1386] Open device and check if DFU is needed\n<<<<<<< pause in execution ~1/2 second or longer\nI :1013] OpenDevice: [/sys/bus/usb/devices/2-2]\nI :1050] OpenDevice: checking bus[2] port[2]\nI :1081] OpenDevice: device opened 0x1cc2070\nI :182] LocalUsbDevice\nI :36] UsbStandardCommands\nI :37] UsbDfuCommands\nI :43] GetDeviceDescriptor\nI :397] GetDescriptor\nI :78] Vender ID: 0x18d1\nI :79] Product ID: 0x9302\nI :1413] Device is already in application mode, skipping DFU\nI :1425] Resetting device\nI :241] Close: closing device 0x1cc2070\nI :214] DoCancelAllTransfers: cancelling 0 async transfers\nI :222] DoCancelAllTransfers: waiting for all async transfers to complete\nI :232] DoCancelAllTransfers: all async transfers have completed\nI :274] Close: releasing 0 transfer buffers\nI :287] Close: performing graceful reset\n<<<<<<< pause in execution ~1/2 second or longer\nI :320] Close: final clean up completed\nI :1366] Opening device expecting application mode\nI :1013] OpenDevice: [/sys/bus/usb/devices/2-2]\nI :1050] OpenDevice: checking bus[2] port[2]\nI :1081] OpenDevice: device opened 0x1cc1ec0\nI :182] LocalUsbDevice\nI :36] UsbStandardCommands\nI :47] UsbMlCommands\nI :40] ~UsbDfuCommands\nI :39] ~UsbStandardCommands\nI :194] ~LocalUsbDevice\nI :241] Close: closing device (nil)\nI :350] ClaimInterface\nI :81] ReadRegister32 offset 0x1a30c\nI :512] SendControlCommandWithDataIn\nI :519] SYNC CTRL WITH DATA IN begin\nI :536] SYNC CTRL WITH DATA IN end\nI :111] ReadRegister32 [0x1A30C] == 0xF0059\nI :154] WriteRegister32 [0x1A30C] := 0xF0059\nI :473] SendControlCommandWithDataOut\nI :783] AsyncInterruptInTransfer\n...\nI :796] ASYNC IN 3 begin\nI :1262] WorkerThreadFunc Installing bulk-in reader. buffer index [0]\nI :748] AsyncBulkInTransfer\nI :761] ASYNC IN 1 begin\nI :1262] WorkerThreadFunc Installing bulk-in reader. buffer index [1]\nI :748] AsyncBulkInTransfer\nI :761] ASYNC IN 1 begin\n\nreal 0m2.917s\nuser 0m0.030s\nsys 0m0.045s\n\n\u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47383#issuecomment-792396985>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAHRRDYIECDVS7PWJXJ4VA3TCQPTFANCNFSM4YFMJ6EA>\n> .\n>\n", "You'd better file a bug under Coral page. This is not something TF team can answer.\r\n\r\nhttps://github.com/google-coral/edgetpu/issues"]}, {"number": 47382, "title": "Update STM32 Bare Lib for zero initialization of the bss section", "body": "With google/stm32_bare_lib@aaabdeb STM32 Bare Lib zero-initializes the bss section.\r\n\r\nThis change is also pulling out the download into a standalone bash script.\r\n\r\nSee #46937 for more discussion on this.\r\n\r\nFixes #46937", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47381, "title": "Generating the descriptors of constant shape", "body": "The local features generated by tensorflowv1 is different from tensorflowv2.5\r\n\r\n```\r\ndef generate_vectors(paths):\r\n    \r\n    ops.reset_default_graph()\r\n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.FATAL)\r\n\r\n    model = hub.Module('https://tfhub.dev/google/delf/1')\r\n\r\n    image_placeholder = tf.compat.v1.placeholder(tf.float32, shape=(None, None, 3), name='input_image')\r\n\r\n    module_inputs = {\r\n        'image': image_placeholder,\r\n        'score_threshold': 100.0,\r\n        'image_scales': [0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0],\r\n        'max_feature_num': 1000,\r\n    }\r\n\r\n    module_outputs = model(module_inputs, as_dict=True)\r\n    image_tf = paths_to_image_loader(list(paths))\r\n    img_paths = []\r\n    with tf.compat.v1.train.MonitoredSession() as sess:\r\n        features = []\r\n\r\n        for i in tqdm(range(len(paths))):\r\n            image = sess.run(image_tf)\r\n            local_feature = sess.run([module_outputs['locations'], module_outputs['descriptors']],\r\n                                     feed_dict={image_placeholder: image})\r\n\r\n```\r\n\r\nThe local_feature[1] and local_feature[0] has got a constant shape of 1000 with the above code that has the legacy cide,.\r\n\r\nI used the latest code to generate vectors as follow,\r\n\r\n```\r\ndef run_delf(image):\r\n    np_image = np.array(image)\r\n    float_image = tf.image.convert_image_dtype(np_image, tf.float32)\r\n\r\n    return delf(\r\n        image=float_image,\r\n        score_threshold=tf.constant(100.0),\r\n        image_scales=tf.constant([0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0]),\r\n        max_feature_num=tf.constant(1000))\r\n```\r\n\r\nUnfortunately `result[\"locations\"].numpy()` and `result[\"descriptors\"].numpy()` always has got a different shapes for every image inputs while it is intended to have 1000 features. How can I fix it?", "comments": ["@Zumbalamambo \r\n\r\nThis issue is more suitable for TensorFlow Hub repo. Please post it on hub repo from [here](https://github.com/tensorflow/hub/issues/new). Thanks!", "@ravikyram The error is due to the mismatch in the tensorflow version and not because of tf-hub as I have clearly stated that the same code generates a different embedding while I use previous version of tensorflow!!", "@Zumbalamambo \r\n\r\nCan you please help us with the colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@Zumbalamambo\r\nPlease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47381\">No</a>\n"]}, {"number": 47380, "title": "Use xa_nnlib for depthwise_conv for Fusion F1", "body": "The code in this change is the subset of functionality needed for int8 svdf for Hifi4 copied from pnikam-cad/tensorflow@a737c1e/tensorflow/lite/micro/kernels/xtensa_hifi/depthwise_conv.cc\r\n\r\nNote that the current change has not pulled in the floating point, uint8 implementation or the Hifi5 implementation.\r\n\r\nProfiled the person_detection_benchmark with the following command:\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_person_detection_benchmark -j8\r\ngives a latency of 9.661M ticks with this change vs 73.761M ticks without this change.\r\n\r\nPer OP latency with this change:\r\n```\r\nWithPersonDataIterations(1) took 9661310 ticks (9661 ms)\r\nDEPTHWISE_CONV_2D took 1156157 ticks (1156 ms).\r\nDEPTHWISE_CONV_2D took 628525 ticks (628 ms).\r\nCONV_2D took 987374 ticks (987 ms).\r\nDEPTHWISE_CONV_2D took 395420 ticks (395 ms).\r\nCONV_2D took 554630 ticks (554 ms).\r\nDEPTHWISE_CONV_2D took 545252 ticks (545 ms).\r\nCONV_2D took 665222 ticks (665 ms).\r\nDEPTHWISE_CONV_2D took 172412 ticks (172 ms).\r\nCONV_2D took 334262 ticks (334 ms).\r\nDEPTHWISE_CONV_2D took 283280 ticks (283 ms).\r\nCONV_2D took 444854 ticks (444 ms).\r\nDEPTHWISE_CONV_2D took 88394 ticks (88 ms).\r\nCONV_2D took 225302 ticks (225 ms).\r\nDEPTHWISE_CONV_2D took 158090 ticks (158 ms).\r\nCONV_2D took 335894 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 158090 ticks (158 ms).\r\nCONV_2D took 335894 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 158090 ticks (158 ms).\r\nCONV_2D took 335894 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 158090 ticks (158 ms).\r\nCONV_2D took 335894 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 158090 ticks (158 ms).\r\nCONV_2D took 335894 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 59525 ticks (59 ms).\r\nCONV_2D took 173270 ticks (173 ms).\r\nDEPTHWISE_CONV_2D took 112424 ticks (112 ms).\r\nCONV_2D took 283862 ticks (283 ms).\r\nAVERAGE_POOL_2D took 75604 ticks (75 ms).\r\nCONV_2D took 3398 ticks (3 ms).\r\nRESHAPE took 290 ticks (0 ms).\r\nSOFTMAX took 1933 ticks (1 ms).\r\n```\r\n\r\nWithout this change:\r\n```\r\nKeywordRunNIerations(1) took 38516 ticks (38 ms)\r\nDEPTHWISE_CONV_2D took 11961939 ticks (11961 ms).\r\nDEPTHWISE_CONV_2D took 12296923 ticks (12296 ms).\r\nCONV_2D took 987358 ticks (987 ms).\r\nDEPTHWISE_CONV_2D took 6138259 ticks (6138 ms).\r\nCONV_2D took 554614 ticks (554 ms).\r\nDEPTHWISE_CONV_2D took 12063331 ticks (12063 ms).\r\nCONV_2D took 665206 ticks (665 ms).\r\nDEPTHWISE_CONV_2D took 3018615 ticks (3018 ms).\r\nCONV_2D took 334246 ticks (334 ms).\r\nDEPTHWISE_CONV_2D took 5837463 ticks (5837 ms).\r\nCONV_2D took 444838 ticks (444 ms).\r\nDEPTHWISE_CONV_2D took 1462009 ticks (1462 ms).\r\nCONV_2D took 225286 ticks (225 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 335878 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 335878 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 335878 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 335878 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 335878 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 685980 ticks (685 ms).\r\nCONV_2D took 173254 ticks (173 ms).\r\nDEPTHWISE_CONV_2D took 1197084 ticks (1197 ms).\r\nCONV_2D took 283846 ticks (283 ms).\r\nAVERAGE_POOL_2D took 75604 ticks (75 ms).\r\nCONV_2D took 3382 ticks (3 ms).\r\nRESHAPE took 290 ticks (0 ms).\r\nSOFTMAX took 1933 ticks (1 ms).\r\n```\r\n\r\nConfirmed that the kernel_conv_test passes with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_depthwise_conv_test -j8\r\n```\r\nProgress towards http://b/177457688", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47379, "title": "Update callbacks.py", "body": "Mirrored cl/358287618\r\nFixes GitHub #40604", "comments": []}, {"number": 47378, "title": "Use xa_nnlib for conv for Fusion F1", "body": "The code in this change is the subset of functionality needed for int8 svdf for Hifi4 copied from pnikam-cad/tensorflow@a737c1e/tensorflow/lite/micro/kernels/xtensa_hifi/conv.cc\r\n\r\nNote that the current change has not pulled in the floating point, uint8 implementation or the Hifi5 implementation.\r\n\r\nProfiled the person_detection_benchmark with the following command:\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_person_detection_benchmark -j8\r\ngives a latency of 73.761M ticks with this change vs 212.980M ticks without this change.\r\n\r\nPer OP latency with this change:\r\n```\r\nKeywordRunNIerations(1) took 38516 ticks (38 ms)\r\nDEPTHWISE_CONV_2D took 11961939 ticks (11961 ms).\r\nDEPTHWISE_CONV_2D took 12296923 ticks (12296 ms).\r\nCONV_2D took 987358 ticks (987 ms).\r\nDEPTHWISE_CONV_2D took 6138259 ticks (6138 ms).\r\nCONV_2D took 554614 ticks (554 ms).\r\nDEPTHWISE_CONV_2D took 12063331 ticks (12063 ms).\r\nCONV_2D took 665206 ticks (665 ms).\r\nDEPTHWISE_CONV_2D took 3018615 ticks (3018 ms).\r\nCONV_2D took 334246 ticks (334 ms).\r\nDEPTHWISE_CONV_2D took 5837463 ticks (5837 ms).\r\nCONV_2D took 444838 ticks (444 ms).\r\nDEPTHWISE_CONV_2D took 1462009 ticks (1462 ms).\r\nCONV_2D took 225286 ticks (225 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 335878 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 335878 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 335878 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 335878 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 335878 ticks (335 ms).\r\nDEPTHWISE_CONV_2D took 685980 ticks (685 ms).\r\nCONV_2D took 173254 ticks (173 ms).\r\nDEPTHWISE_CONV_2D took 1197084 ticks (1197 ms).\r\nCONV_2D took 283846 ticks (283 ms).\r\nAVERAGE_POOL_2D took 75604 ticks (75 ms).\r\nCONV_2D took 3382 ticks (3 ms).\r\nRESHAPE took 290 ticks (0 ms).\r\nSOFTMAX took 1933 ticks (1 ms).\r\n```\r\nWithout this change:\r\n```\r\nWithPersonDataIterations(1) took 212980371 ticks (212980 ms)\r\nDEPTHWISE_CONV_2D took 11961939 ticks (11961 ms).\r\nDEPTHWISE_CONV_2D took 12296923 ticks (12296 ms).\r\nCONV_2D took 13604549 ticks (13604 ms).\r\nDEPTHWISE_CONV_2D took 6138259 ticks (6138 ms).\r\nCONV_2D took 9585893 ticks (9585 ms).\r\nDEPTHWISE_CONV_2D took 12063331 ticks (12063 ms).\r\nCONV_2D took 15189221 ticks (15189 ms).\r\nDEPTHWISE_CONV_2D took 3018615 ticks (3018 ms).\r\nCONV_2D took 7590389 ticks (7590 ms).\r\nDEPTHWISE_CONV_2D took 5837463 ticks (5837 ms).\r\nCONV_2D took 13193717 ticks (13193 ms).\r\nDEPTHWISE_CONV_2D took 1462009 ticks (1462 ms).\r\nCONV_2D took 6596093 ticks (6596 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 12199421 ticks (12199 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 12199421 ticks (12199 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 12199421 ticks (12199 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 12199421 ticks (12199 ms).\r\nDEPTHWISE_CONV_2D took 2734009 ticks (2734 ms).\r\nCONV_2D took 12199421 ticks (12199 ms).\r\nDEPTHWISE_CONV_2D took 685980 ticks (685 ms).\r\nCONV_2D took 6099809 ticks (6099 ms).\r\nDEPTHWISE_CONV_2D took 1197084 ticks (1197 ms).\r\nCONV_2D took 11703137 ticks (11703 ms).\r\nAVERAGE_POOL_2D took 75604 ticks (75 ms).\r\nCONV_2D took 10983 ticks (10 ms).\r\nRESHAPE took 290 ticks (0 ms).\r\nSOFTMAX took 1933 ticks (1 ms).\r\n```\r\nConfirmed that the kernel_conv_test passes with:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_conv_test -j8\r\n```\r\nProgress towards http://b/177457688", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47377, "title": "Fix TFLM github CI bazel build", "body": "To keep the bazel build short, we maintain a copy of the subset of packages that are needed for the TFLM (+ shared TfLite) bazel targets.\r\n\r\nEigen was updated for TF with 0effd3dc59621d79c2535e2d928d0cf41db94d3c and we make the corresponding change in TFLM's pared down version of workspace.bzl with this change.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Will we to have to continue updating these packages manually?", "> Will we to have to continue updating these packages manually?\r\n\r\nyes, for now. It just so happened that my enabling bazel for TFLM github CI coincided with changes and updates to eigen. Things had been quite stable for a while."]}, {"number": 47376, "title": "No version of tensorflow will install on python 3.7.9 x64 / pip 20.1", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `OS X 11.1 (PC)`\r\n- TensorFlow installed from (source or binary): `pip`\r\n- TensorFlow version: `1.15 or any other`\r\n- Python version: `3.7.9`\r\n- Installed using virtualenv? pip? conda?: `pip`\r\n\r\n**Describe the problem**\r\n\r\ninstalled python 3.7.9 as compatible platform with TF 1.15 via `pyenv`\r\nno version of tf will install at all:\r\n\r\n```\r\n:~$ python3 -VV\r\nPython 3.7.9 (default, Feb 24 2021, 13:04:10)\r\n[Clang 12.0.0 (clang-1200.0.32.29)]\r\n:~$ python3 -c 'import sys;print(\"%x\" % sys.maxsize, sys.maxsize > 2**32)'\r\n7fffffffffffffff True\r\n:~$ python3 -m pip install tensorflow==1.15\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==1.15\r\n```\r\nsame output with just `pip3 install tensorflow`\r\n\r\nIs there something else I am overlooking as far the python/pip version/build type?\r\n", "comments": ["@abolotnov,\r\nTensorFlow 1.x is not actively supported. Please try installing the latest stable version of TensorFlow v2.4.1 using the below command.\r\n\r\n`pip install --upgrade tensorflow`\r\n\r\nThanks!", "Not sure how exactly I fixed it, but I got it fixed somehow. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47376\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47376\">No</a>\n"]}, {"number": 47375, "title": "[INTEL MKL] Remove unnecessary MKL  build macros and bug fixes", "body": "Remove the following MKL build options (and usage of related MACRO's):\r\n\r\n1. ENABLE_INTEL_MKL_BFLOAT16  (treat as \"TRUE\" value)\r\n2. INTEL_MKL_DNN_ONLY              (treat same as \"INTEL_MKL\")\r\n3. ENABLE_MKLDNN_V1                 (DNN 0.x code cleanup; cleanup was overwritten by later PRs).\r\n\r\n#3 fixes some bugs caused by the merge of final DNN 0.x code cleanup PR (https://github.com/tensorflow/tensorflow/pull/46370/), most MKL unit test failure.\r\n", "comments": ["The fix was in cbaadc39c48f282f09c3ab92146ccbe62e599a11, but the CI still have some other failures (about benchmarks). \r\nCould you please resync to master to resolve the merge conflict? Thank you!", "> This looks so much cleaner. Thank you very much!\r\n> \r\n> This PR will take a while to merge. I might try submitting a separate commit to fix `mkl_dequantize_op.cc` first so the CI can stop failing.\r\n\r\nThank you Penporn! I have just addressed the merge conflicts", "For CI  failures (about benchmarks, there is another PR (submitted by my teammate)."]}, {"number": 47374, "title": "Fixed docstring formatting for api_docs", "body": "The api_docs website formatting of [leaky_relu](https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu) was wrong due to a missing newline", "comments": []}, {"number": 47373, "title": "AttributeError: Can't set the attribute \"name\" when building bidirectional layer with stackedrnncell", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nOn Jupyter Notebook, Python 3.6, Tensorflow Version 2.4\r\n\r\n**Describe the current behavior**\r\nI am currently trying to build a bidirectional layer using stackedrnncell. However, I am running to an attribute error but the error message is not helping me in debugging this error. \r\n\r\n**Describe the expected behavior**\r\nWould expect the model to be build. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nlstm_1 = tf.keras.layers.LSTMCell(128)\r\nlstm_2 = tf.keras.layers.LSTMCell(128)\r\nlstm_fw_cell = tf.keras.layers.StackedRNNCells([lstm_1, lstm_2])\r\nlstm_fw = tf.keras.layers.RNN(lstm_fw_cell,unroll=True)\r\n\r\nlstm_3 = tf.keras.layers.LSTMCell(128)\r\nlstm_4 = tf.keras.layers.LSTMCell(128)\r\nlstm_bw_cell= tf.keras.layers.StackedRNNCells([lstm_3, lstm_4])\r\nlstm_bw = tf.keras.layers.LSTM(lstm_bw_cell,go_backwards=True,unroll=True)\r\n\r\nmodel = Sequential()\r\nmodel.add(Bidirectional(lstm_fw, backward_layer=lstm_bw))\r\nmodel.add(Dense(4, activation=\"softmax\"))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\nmodel.summary()\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n![image](https://user-images.githubusercontent.com/41911024/109015802-7a3d5300-76f0-11eb-9ed7-b6a7b3f34f04.png)\r\n", "comments": ["`lstm_cell_1 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(rnn_size, forget_bias=1.0, state_is_tuple=True)\r\nlstm_cell_2 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(rnn_size, forget_bias=1.0, state_is_tuple=True)\r\nlstm_fw_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\r\nfw = RNN(lstm_fw_cell, unroll=True)\r\n\r\nlstm_cell_3 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(rnn_size, forget_bias=1.0, state_is_tuple=True)\r\nlstm_cell_4 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(rnn_size, forget_bias=1.0, state_is_tuple=True)\r\nlstm_bw_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell_3, lstm_cell_4], state_is_tuple=True)\r\n\r\nmodel = Sequential()\r\nmodel.add(Bidirectional(fw))\r\nmodel.add(Dense(4, activation=\"softmax\"))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\nmodel.summary()`\r\n\r\nI have also tried another method using tf.compat rather than tf.keras but I got the following error instead. And is there a difference between StackedRNNCell and MultiRNN?\r\n\r\n![image](https://user-images.githubusercontent.com/41911024/109018999-8d9ded80-76f3-11eb-827d-ff86782a3151.png)\r\n", "@kongyansan \r\nCode provided is not sufficient to replicate issue reported, can you please share a colab gist with the error.", "Hi, i think there was some issues with my environment because I managed to run model.summary() after initialising the new one. Thank you and I will be closing this issue\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47373\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47373\">No</a>\n"]}, {"number": 47372, "title": "TFLM: Enable pooling tests for FVP build", "body": "Enabling pooling and bumping CMSIS version as the problem has been fixed.\r\n\r\nThis progress towards: https://github.com/tensorflow/tensorflow/issues/47070", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47371, "title": "tf_upgrade_v2: UnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 385: character maps to <undefined>", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.6.1\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: RTX3070 8GB\r\n\r\nI tried to update my project with script tf_upgrade_v2 and command:\r\n```\r\ntf_upgrade_v2  --intree D:\\Work\\in_dir --outtree D:\\Work\\out_dir --reportfile D:\\Work\\report.txt --print_all\r\n```\r\nbut I get error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"c:\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\python36\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\python36\\Scripts\\tf_upgrade_v2.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\tools\\compatibility\\tf_upgrade_v2_main.py\", line 178, in main\r\n    args.input_tree, output_tree, args.copy_other_files)\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\tools\\compatibility\\ast_edits.py\", line 1076, in process_tree\r\n    _, l_report, l_errors = self.process_file(input_path, output_path)\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\tools\\compatibility\\ast_edits.py\", line 921, in process_file\r\n    temp_file)\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\tools\\compatibility\\ast_edits.py\", line 982, in process_opened_file\r\n    lines = in_file.readlines()\r\n  File \"c:\\python36\\lib\\encodings\\cp1250.py\", line 23, in decode\r\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x98 in position 385: character maps to <undefined>\r\n```\r\n\r\nHow to check which file in my project is troublesome?\r\n", "comments": ["@kiflowb777 \r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue. It helps us in debugging further. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47371\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47371\">No</a>\n"]}, {"number": 47370, "title": "True loss values", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.15.3 and 2.4.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe log loss values of the training dataset recorded during model fitting are not the actual loss values obtained when evaluating the resulting model. I explain this througly in https://github.com/sparklingdew/keras_loss_discrepancy.\r\n\r\n**Will this change the current api? How?**\r\nThe only change that I suggest is to give the actual loss value for the training dataset, rather than this other internal value which is difficult to understand. It should be calculated similarly to the validation loss, which is obtained from evaluating the resulting model. I understand that this may go in detriment to running time. In that case, this new feature could be optional.\r\n\r\n**Who will benefit with this feature?**\r\nNN users will be benefitted, since model tunning is performed based on loss and accuaracy results. If these values are misleading, the models are difficult to tune.\r\n\r\n**Any Other info.**\r\nMany thanks for your time!", "comments": ["@sparklingdew Sorry for the late response. Are you still interested in contributing? If yes, please feel free to open a PR in  [keras-team/keras repo.](https://github.com/keras-team/keras/issues) repository.\r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus on only keras. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47369, "title": "sorry for mistake reference issue, please delete it", "body": "", "comments": ["sorry for mistake reference issue, please delete it\r\n"]}, {"number": 47368, "title": "Tensorflow is ignoring `tf.config.set_soft_device_placement(True)`", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (GPU)\r\n- TensorFlow installed from (source or binary): Google Colab (GPU)\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.10\r\n- CUDA/cuDNN version: Google Colab (GPU)\r\n- GPU model and memory: Google Colab (GPU)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n\r\n**Describe the current behaviour**\r\n\r\nI'm training a neural network with embedding layer using GPU-distributed strategy (TF's MirroredStrategy), and I'm getting the following error:\r\n\r\n```\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nGatherV2: GPU CPU \r\nCast: GPU CPU \r\nConst: GPU CPU \r\nResourceSparseApplyAdagradV2: CPU \r\n_Arg: GPU CPU \r\nReadVariableOp: GPU CPU \r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  model_6_user_embedding_embedding_lookup_readvariableop_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  adagrad_adagrad_update_1_update_0_resourcesparseapplyadagradv2_accum (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  model_6/User-Embedding/embedding_lookup/ReadVariableOp (ReadVariableOp) \r\n  model_6/User-Embedding/embedding_lookup/axis (Const) \r\n  model_6/User-Embedding/embedding_lookup (GatherV2) \r\n  gradient_tape/model_6/User-Embedding/embedding_lookup/Shape (Const) \r\n  gradient_tape/model_6/User-Embedding/embedding_lookup/Cast (Cast) \r\n  Adagrad/Adagrad/update_1/update_0/ResourceSparseApplyAdagradV2 (ResourceSparseApplyAdagradV2) /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n     [[{{node model_6/User-Embedding/embedding_lookup/ReadVariableOp}}]] [Op:__inference_train_function_2997]\r\n```\r\n\r\n\r\nFrom the error log, it looks like the embedding_lookup/ReadVariableOp is not GPU compatible - which I can understand why. \r\n\r\nBut I've set `tf.config.set_soft_device_placement(True)` so I'm expecting TF to use CPU for CPU only operations, and that doesn't seemed to have worked. Is TF ignoring that config setting? Or does that setting doesn't yet support some other conditions? \r\n\r\n\r\n**Describe the expected behaviour**\r\n\r\nI'd expect MirroredStrategy to default to CPU for CPU only computations by setting `tf.config.set_soft_device_placement(True)` but that doesn't seem to be happening.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\nLink to Google Colab notebook:\r\nhttps://colab.research.google.com/drive/1ZN1HzSTTfvA_zstuI-EsKjw7Max1f73v?usp=sharing\r\n\r\nDataset can be found on Kaggle:\r\nhttps://www.kaggle.com/zygmunt/goodbooks-10k\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nFull log can be seen on Google Colab by downloading the dataset from kaggle, uploading to google colab, and running the code on GPU. The neural network is really small -takes less than 5 seconds to execute the whole notebook.\r\n\r\n", "comments": ["I found this similar issue for TF1.14:\r\nhttps://github.com/tensorflow/tensorflow/issues/31318\r\n\r\nLooks like we can't do MirroredStrategy cannot support training Embedding layers using momentum-based optimisers. There seems to be a fix for TF1.14, but this became a bug again in TF2.0?\r\n\r\nWill try to see if using non-momentum-based optimisers work for now to unblock. Still something that should be fixed IMO! ", "Also, the error message certainly hasn't helped!! ", "https://colab.research.google.com/drive/13MXa8Q96M6uzlkK3K_M7vmQfclL59eRj?usp=sharing\r\n\r\nConfirmed working when training with RMSProp with no momentum, similar to issue #31318 for TF1.14.", "Hi @shengy90, I ran the colab notebook you provided[ (link)](https://colab.research.google.com/drive/1ZN1HzSTTfvA_zstuI-EsKjw7Max1f73v?usp=sharing) on the GPU runtime, but I'm not seeing any errors. Please take a look and let me know if I'm missing something.", "<img width=\"1818\" alt=\"Screenshot 2021-03-15 at 11 53 10\" src=\"https://user-images.githubusercontent.com/12820124/111149654-0bcc1200-8585-11eb-991e-3e58215b02fb.png\">\r\n\r\nI just ran it (and got this error - see screenshot). Are you connected to GPU run time? This error only happens when I'm trying to do GPU distributed training using AdaGrad, or RMSprop (with momentum parameter > 0).\r\n\r\n@nikitamaia \r\n\r\n\r\n\r\n", "I was able to reproduce the error in colab. I'm not quite sure how `set_soft_device_placement` works with MirroredStrategy, so doing some more investigation.\r\n", "hello, do we have some expectation of when this will be fixed? ", "Was able to resolve the issue with TF v2.5 ,please find the gist [here.](https://colab.research.google.com/gist/mohantym/4f8b14d77515e0f5a1a89217d954bddd/mirroredstrategywithrmsprop.ipynb#scrollTo=0neHYIlVAaBl)Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47368\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47368\">No</a>\n"]}, {"number": 47367, "title": "Add a status badge for Arduino examples", "body": "This PR adds a status badge to TFLite Micro Readme.\r\n\r\nThe graphic style is slightly different to the current badges, but it's auto-generated by GitHub, so it requires no additional handling on the CI side.\r\n\r\ncc @advaitjain ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Fixed", "copybara failed :-( reapplying the tags to trigger it again."]}, {"number": 47366, "title": "metrics values don't match between custom metric function and model.fit callbacks", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Google Colab default installed\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**  \r\nreturned metrics value from custom metrics functions and callback printed metrics from model.fit doesn't match.\r\n\r\n**Describe the expected behavior**  \r\nreturned metrics value from custom metrics functions and callback printed metrics from model.fit match.\r\n\r\n**Standalone code to reproduce the issue**  \r\n[Google Colab gist](https://gist.github.com/asterisk37n/9e8c492fc7f01a9f37f9d5fb33ee3406)\r\n\r\n**Other info / logs**  \r\nI opened a quesiton on stackoverflow.  \r\n[https://stackoverflow.com/questions/66311611/when-and-how-keras-calculate-metrics-for-each-batch-of-samples](https://stackoverflow.com/questions/66311611/when-and-how-keras-calculate-metrics-for-each-batch-of-samples)", "comments": ["Hey @asterisk37n \r\nI tried giving your script a run, modifying a little bit\r\n```python\r\nhistory = model.fit(x=x, y=y, batch_size=2, epochs=1, verbose=2)\r\n```\r\nHere I want to output only one line for each epoch and not the progress bar, hence `verbose=2`\r\nUpon doing this, I do in fact get results that are aligned with the `history.history`\r\n\r\nMy guesses are to dive deep into the output of `model.fit` and check whether the progress bar messes up with the results that we see. Hope this helps \ud83d\ude04 ", "Hi all. Here are the issues describing the same problem: #46713 #47099. Hope this helps.", "@asterisk37n,\r\nOn running the Colab notebook you have linked, I see that the `loss` and `my_metric_fn` values are identical. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/caa1b8fc1c251ef54a4fbc8395ec0003/47366.ipynb#scrollTo=EZK_OjvBY4Pj). Thanks!", "@amahendrakar \r\nThank you for running. I'm referring to the different thing. In my log, you can see 0.1429. Where has this come from?\r\n```\r\n2/2 [==============================] - 0s 7ms/step - loss: 0.1429 - my_metric_fn: 0.1429\r\n```\r\n```my_metric_fn```'s value for the second batch? No, it is 0.0450076237.\r\nAverage of ```my_metric_fn``` among the first and second batch? No, it is (0.191785976 +  0.0450076237)/2 = 0.11839679985.", "@ariG23498 Thanks for verbose param info.\r\n@evgeniya-egupova  Thank you. yeah, very similar to https://github.com/tensorflow/tensorflow/issues/47099", "Could that be a running average with some exp decay parameter? i.e. not a simple average.\r\nMight be linked to https://github.com/tensorflow/tensorflow/issues/39448\r\n", "related issue https://github.com/tensorflow/tensorflow/issues/47099 has been closed, could you check if the present issue is solved too on the new commit 979c5093da6238eae2f137e2f5538cf6373c4c02?\r\n", "@ricvo very likely so. If you find that the original issue is still not fixed after using a nightly build (that has the fix), please comment. Thanks!", "@asterisk37n As mentioned by @rchao and @ricvo, this was resolved with recent `tf-nightly`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/5a03e019a55ba1e65ed2995a1e7a5e35/47366.ipynb).\r\n\r\nOld output (with `TF2.4.1`)\r\n\r\n```\r\n2.4.1\r\nTensorShape([2, 1]) TensorShape([2, 1]) [0.216433451 0.167138502] 0.191785976\r\n1/2 [==============>...............] - ETA: 0s - loss: 0.1918 - my_metric_fn: 0.1918\r\nTensorShape([2, 1]) TensorShape([2, 1]) [0.0477369316 0.0422783121] 0.0450076237\r\n2/2 [==============================] - 0s 12ms/step - loss: 0.1429 - my_metric_fn: 0.1429\r\n{'loss': [0.1183968037366867], 'my_metric_fn': [0.1183968037366867]}\r\n```\r\n\r\nNew output (with `tf-nightly`)\r\n\r\n\r\n```\r\nEpoch TensorShape([2, 1]) TensorShape([2, 1]) [0.216433451 0.167138502] 0.191785976\r\n1/2 [==============>...............] - ETA: 0s - loss: 0.1918 - my_metric_fn: 0.1918\r\nEpoch TensorShape([2, 1]) TensorShape([2, 1]) [0.0477369316 0.0422783121] 0.0450076237\r\n2/2 [==============================] - 0s 15ms/step - loss: 0.1184 - my_metric_fn: 0.1184\r\n{'loss': [0.1183968037366867], 'my_metric_fn': [0.1183968037366867]}\r\n```\r\n\r\nPlease verify once and close the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@jvishnuvardhan Thank you. I checked your gist and it works perfectly as expected! Thank you for contribution.", "Thanks for confirming. Closing the issue as this was resolved.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47366\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47366\">No</a>\n"]}, {"number": 47365, "title": "Getting ERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- MacBook pro\r\n- processor: 2GHz Quad-Core Intel Core i5,  \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI'm successfully able to install 1.8.0, \r\ngetting below error with all available wheel from mac os, supported wheel for my mac is cp39, which is not avilable. \r\ntensorflow-2.4.1-cp36-cp36m-macosx_10_11_x86_64.whl is not a supported wheel on this platform.\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@arpans2112 \r\n\r\nCould you please update pip using the below commands and check if you are still facing the same issue?\r\n\r\npip install --upgrade pip\r\npip install tensorflow\r\nPlease ensure your tensorflow version is compatible with the python version used, TF 2.4 supports python from 3.6-3.8 . Please, refer the [document](https://www.tensorflow.org/install/source#cpu),Also, could you check if you are using the 64 bit version of Python. TensorFlow is tested and supported only on 64-bit systems.\r\n\r\nPlease refer to similar issues, #44615, #45147, #44775, #39130, [link](https://stackoverflow.com/questions/48720833/could-not-find-a-version-that-satisfies-the-requirement-tensorflow), ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47365\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47365\">No</a>\n"]}, {"number": 47364, "title": "ssd mobilenet model size is not correct using tflite converter python api", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1\r\n\r\n### 2. Code\r\n\r\nMy script [SSD mobilenet convert to TF lite colab script](https://colab.research.google.com/drive/1qaGW7ViN0fW2z4bqRKL4sd8X-HIq5Vm2?usp=sharing)\r\n\r\n\r\n### 3. Failure after conversion\r\n\r\nconvert offical SSD MobileNet v2 320x320 model using python api from [tf2_detection_zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md), conversion is successful, but tflite file size is too small (only 6.4 MB) and run on mobile extremely slow (900 ms).\r\n\r\nAnd I found following result:\r\n\r\n* if **convert to saved model and tflite both using tf 2.4**\r\n   * using tflite converter python api is 6.4M\r\n   * using command line tool (tflite_converter) is 324 bytes\r\n   * **both file is NOT correct**\r\n* if **convert to saved model using TF 2.3 and convert to tflite using tf 2.4**\r\n   * using tflite converter python api is 6.4M\r\n   * using command line tool (tflite_converter) is 24M\r\n   * **the command line tool transform result is correct** which run on mobile very fast (100 ms)!\r\n\r\nsaved model convert script\r\n\r\n```bash=\r\npython object_detection/export_tflite_graph_tf2.py --pipeline_config_path ./ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config --trained_checkpoint_dir ./ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint/ --output_directory ./output/\r\n```\r\n\r\ntflite converter python api convert code\r\n\r\n```python=\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('output/saved_model/',signature_keys=[\"serving_default\"])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('output/detect_using_api.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\ncommand line tool convert script\r\n\r\n```bash=\r\ntflite_convert --output_file output/detect_using_command.tflite --saved_model_dir output/saved_model/ --experimental_new_converter True\r\n```\r\n\r\nMy question is\r\n\r\nWhy output result is different between using tflite converter python api and command line tool?\r\n\r\n### 5. (optional) Any other info / logs\r\nN/A\r\n", "comments": ["We recommend using the python API conversion over the command line. Could you verify this behavior with tf-nightly?", "I have tried, but it keep same size (6.4 MB) by using python API conversion.\r\n\r\nIt doesn't make sense because the weight file of SSD MobileNet v2 320x320 is about 24 MB, tflite file size is too small.", "Did you verify the converted models their correctness other than file sizes? The file size can be reduced due to pruning unused parts in the model when the some weight data aren't related with the inference run.", "install converted tflite file on mobile which convert by tf-nightly, it can detect object (but I don't test whether the accuracy is same as saved model file) but extremely slow (900 ms).\r\n\r\nbut if I install converted tflite file which convert by command line tool, it run fast (100 ms).", "Could you share the above models to us?", "Sure.\r\n[detect_using_api.zip](https://github.com/tensorflow/tensorflow/files/6040054/detect_using_api.zip)\r\n[detect_using_command.zip](https://github.com/tensorflow/tensorflow/files/6040055/detect_using_command.zip)\r\n", "Interestingly, they have the identical graph structure when I looked at the netron. Will look at the details.", "@talumbau Could you take a look?\r\n\r\nThe graph structures are identical but the user reported that there is a 9x performance difference on mobile.\r\n\r\nI tried on the x86 machine only. Their latency data look similar.\r\n\r\n- detect_using_api.tflite\r\n```\r\nInference timings in us: Init: 1078, First inference: 122996, Warmup (avg): 88264.2, Inference (avg): 69998.7\r\n```\r\n\r\n- detect_using_command.tflite\r\n```\r\nInference timings in us: Init: 3028, First inference: 108867, Warmup (avg): 85563.7, Inference (avg): 79799.9\r\n```", "Hi, I test tflite file on Colab today.\r\n\r\nIt show same performance issue, detect_using_command.tflite **is much better than** detect_using_api.tflite.\r\n\r\ntest code:\r\n\r\n```python=\r\nimport os, time\r\nimport numpy as np\r\n\r\ndef prepare(model_name):\r\n    interpreter = tf.lite.Interpreter(model_path=model_name)\r\n    input_index = interpreter.get_input_details()[0][\"index\"]\r\n    output_index = interpreter.get_output_details()[0][\"index\"]\r\n    interpreter.allocate_tensors()\r\n    return interpreter, input_index, output_index\r\n\r\ndef infer(image, interpreter, input_index, output_index):\r\n    interpreter.set_tensor(input_index, image)\r\n    interpreter.invoke()\r\n    outputs = interpreter.get_tensor(output_index)\r\n    return outputs\r\n\r\napi_interpreter = prepare(os.path.join(\"output\", \"detect_using_api.tflite\"))\r\ncommand_interpreter = prepare(os.path.join(\"output\", \"detect_using_command.tflite\"))\r\n\r\napi_time = []\r\ncommand_time = []\r\n\r\nfor _ in range(100):\r\n  img = (np.random.random(size=(1, 300, 300, 3))).astype(np.float32)\r\n\r\n  start = time.time()\r\n  out = infer(img, *api_interpreter)\r\n  api_time.append(time.time() - start)\r\n\r\n  start = time.time()\r\n  out = infer(img, *command_interpreter)\r\n  command_time.append(time.time() - start)\r\n\r\nprint(\"api version avg time\", sum(api_time)/len(api_time))\r\nprint(\"cmd version avg time\", sum(command_time)/len(command_time))\r\n```\r\n\r\nand result\r\n\r\n```\r\napi version avg time 2.333226749897003\r\ncmd version avg time 0.08322675704956055\r\n```", "@uranusx86 @abattery The model is very different. The API converted model uses `tf.lite.Optimize.DEFAULT`, which enables quantization as per https://www.tensorflow.org/api_docs/python/tf/lite/Optimize?version=nightly (I would say this argument is bogus because it's not default at all... ). For example, you can see that `ssd_mobile_net_v2keras_feature_extractor/model/block_1_project/Conv2D` has int8 weights while command line converted model has float32 weights. Therefore, the API converted model has smaller size. \r\n\r\nFor the performance, on my Pixel3a, \r\n\r\nAPI converted model\r\n```\r\nTimings (microseconds): count=50 first=138922 curr=138345 min=136733 max=29662368 avg=2.51898e+06 std=6203433\r\n```\r\nCommandline converted model\r\n```\r\nTimings (microseconds): count=38 first=187153 curr=39225742 min=172200 max=39250433 avg=4.93929e+06 std=10202402\r\n```\r\n\r\nOn x86 arch, it's less optimized on int kernel, so the performance is expected to be slower when trying to use integer kernel.", "Thanks @WindQAQ a lot for the through investigations! :-) Your explanations make sense.", "@uranusx86 You can remove the `converter.optimizations = [tf.lite.Optimize.DEFAULT]` line. Then, you will have the almost same model with the one generated by the command line.", "WOW, thank you guys.\r\n\r\nAfter remove `converter.optimizations = [tf.lite.Optimize.DEFAULT]`, the files are same.\r\n\r\nI never knew tf.lite.Optimize.DEFAULT option enables quantization ...\r\n\r\nAnd it make sense why API converted model is slow than Commandline converted model because I use very old mobile (snapdragon 801) to test, the CPU performance may degrade when compute quantization operation.\r\n"]}, {"number": 47363, "title": "Any suggestion to use tensorflow feasibly?", "body": "I am currently doing a research while it is based on a project written with Tensorflow. While I could not implement the dynamic time wrapping easily since it is a dynamic method. With pytorch I could implement it very quickly (since pytorch supports dynamic graph). \r\n\r\nCould anyone help me choose the correct version and documents of Tensorflow that supports dynamic graph? I checked on Tensorflow official website while the document is kind of messy. ", "comments": ["Hey @DabiaoMa \r\nI think this [thread](https://github.com/tensorflow/tensorflow/issues/12321) might help you.", "@DabiaoMa,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47361, "title": "Support tf.IsFinite lowering", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): nightly\r\n\r\n### 2. Code\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef func(x):\r\n  return tf.is_finite(x)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([func.get_concrete_function(tf.TensorSpec(shape=(1, 2, 3)))])\r\nconverter.convert()\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nIt will fail if flex is not enabled. However, `tf.is_inf()` is supported without flex because it is lowered to other ops in `lower_tf.cc`. It should be easy to support lowering pattern of `tf.is_finite`. Note that `tf.is_finite` is also being used in `tf.keras.layers.Softmax` with multi-axes reduction. I understand that I can enable flex to use it, but it should be a minimum work to lower it directly.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/advanced_activations.py#L328-L345", "comments": ["Thanks for filing the feature request! Is there any plan for this in your side?", "@WindQAQ I'll make a quick PR to support op lowering.", "> Thanks for filing the feature request! Is there any plan for this in your side?\r\n\r\n@abattery It's probably like `tf.logical_not(tf.logical_or(tf.is_nan(x), tf.is_infinite(x)))`\r\n\r\n> @WindQAQ I'll make a quick PR to support op lowering.\r\n\r\nThank you for the help!", "Finally my PR is merged at e89316dfca814d2d2ff2bef47d33bc33140cc112\r\nThe tf.IsFinite is lowered to tf.Equal(tf.Subtract(x, x), 0)."]}, {"number": 47360, "title": "Having issue with NotImplemented Tensor to NumPy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nArch Linux\r\nTensorflow version 2.4.1\r\nPython version 3.9.1\r\n\r\nI am doing the [TensorFlow tutorial](https://www.tensorflow.org/tutorials/images/classification#data_augmentation) and am getting NotImplemented errors at the Data Augmentation stage:\r\n\r\n```\r\ndata_augmentation = keras.Sequential(\r\n    [\r\n        layers.experimental.preprocessing.RandomFlip(\"horizontal\",\r\n                                                     input_shape=(img_height,\r\n                                                                  img_width,\r\n                                                                  3)),\r\n        layers.experimental.preprocessing.RandomRotation(0.1),\r\n        layers.experimental.preprocessing.RandomZoom(0.1),\r\n    ]\r\n)\r\n```\r\n\r\n\r\nError Trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/yathavan/Documents/tensorflow/flowers/flowers.py\", line 136, in <module>\r\n    data_augmentation = keras.Sequential(\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\", line 517, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\", line 144, in __init__\r\n    self.add(layer)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py\", line 517, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py\", line 223, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 951, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1090, in _functional_construction_call\r\n    outputs = self._keras_tensor_symbolic_call(\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 863, in _infer_output_signature\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\", line 866, in call\r\n    output = control_flow_util.smart_cond(training, random_rotated_inputs,\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/utils/control_flow_util.py\", line 114, in smart_cond\r\n    return smart_module.smart_cond(\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/framework/smart_cond.py\", line 54, in smart_cond\r\n    return true_fn()\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\", line 861, in random_rotated_inputs\r\n    get_rotation_matrix(angles, img_hd, img_wd),\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\", line 757, in get_rotation_matrix\r\n    array_ops.zeros((num_angles, 2), dtypes.float32),\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2819, in wrapped\r\n    tensor = fun(*args, **kwargs)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2868, in zeros\r\n    output = _constant_if_small(zero, shape, dtype, name)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 2804, in _constant_if_small\r\n    if np.prod(shape) < 1000:\r\n  File \"<__array_function__ internals>\", line 5, in prod\r\n  File \"/usr/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 3030, in prod\r\n    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n  File \"/usr/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 87, in _wrapreduction\r\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 852, in __array__\r\n    raise NotImplementedError(\r\nNotImplementedError: Cannot convert a symbolic Tensor (random_rotation/rotation_matrix/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n```", "comments": ["seems to be a compatibility issue between numpy 1.20+ and tf, #47263 ", "@yparam98,\r\nI was able to run the code without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/6b61ab9b4c2f1e038b9392d155827582/classification.ipynb).\r\n\r\nCurrently TensorFlow is not compatible with Python 3.9. Could you please try running the code with Python 3.8 and check if you are facing the same error. Thanks!", "By god it works... \r\n\r\nThanks @amahendrakar! \ud83d\udc4d\ud83c\udffd ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47360\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47360\">No</a>\n"]}, {"number": 47359, "title": "Im trying to list my gpus, but only show gpu0  and do not show gpu1 that is my nvidia", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@jpmrs1313 \r\nCan you please provide with more details, how many gpus do you have, what is the purpose[ what function are you calling],  are you planning to use  a stratergy?", "@Saduf2019 \r\n\r\nI have 2  gpus, Intel HD Graphics 630 (in CPU) e GeForce GTX 1050TI and when i list gpus with this command line:\r\n\r\nde tensorflow.python.client import device_lib\r\ndevice_lib.list_local_devices ()\r\n\r\nOutput:\r\n\r\nname: \"/device:CPU:0\"\r\n device_type: \"CPU\"\r\n memory_limit: 268435456\r\n locality {\r\n }\r\n incarnation: 16820838733360620967\r\n\r\nI have CUDA v11.2 installed and  cudnn too.\r\nMy tensorflow  version is 2.4.1 and python 3.8.7", "@jpmrs1313 \r\nCan you please try with CUDA 11.0 and let us know if you still face issue.", "> @jpmrs1313\r\n> Can you please try with CUDA 11.0 and let us know if you still face issue.\r\n\r\nThanks a lot. It works", "@jpmrs1313\r\nThank you for your update, glad your issue is resolved, please move this to closed status.", "Thanks"]}]