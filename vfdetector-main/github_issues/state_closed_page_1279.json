[{"number": 14760, "title": "fix: tf.contrib.slim.nets.vgg will not work", "body": "__init__.py of slim module don't import nets module. so \r\n\r\n`tf.contrib.slim.nets.vgg`\r\n\r\nwill not work. change to this:\r\n```\r\nimport tensorflow.contrib.slim.nets as nets\r\nvgg = nets.vgg\r\n```\r\nanother way of correcting this is change __init__.py. but change documentation is the easy way.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14759, "title": "Version 1.4.0 Can't enable peer access between some devices", "body": "\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7-3\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**:  3.4.5\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.3.0\r\n- **CUDA/cuDNN version**: CUDA 9.0.175 / cuDNN 7.0\r\n- **GPU model and memory**:  10 x GeForce GTX 1080 Ti  12 GB\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nSystem has 10 GPUs on one pci root hub but Tensorflow can not enable peer access to all devices. Nvidia CUDA-Example 1_Utilities/p2pBandwidthLatencyTest is able to enable these\r\n\r\n### Source code / logs\r\nTensorflow output:\r\n> 2017-11-21 13:58:12.914211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.72GiB\r\n2017-11-21 13:58:13.249428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:05:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.74GiB\r\n2017-11-21 13:58:13.574464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.74GiB\r\n2017-11-21 13:58:13.899631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 3 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:07:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.74GiB\r\n2017-11-21 13:58:14.219023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 4 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:08:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.74GiB\r\n2017-11-21 13:58:14.553864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 5 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:0b:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.74GiB\r\n2017-11-21 13:58:14.888727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 6 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:0c:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.74GiB\r\n2017-11-21 13:58:15.208341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 7 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:0d:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.74GiB\r\n2017-11-21 13:58:15.524748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 8 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:0e:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.74GiB\r\n2017-11-21 13:58:15.831437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 9 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:0f:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.74GiB\r\n2017-11-21 13:58:15.837982: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 0 and 9\r\n2017-11-21 13:58:15.843596: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 1 and 9\r\n2017-11-21 13:58:15.848661: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 2 and 9\r\n2017-11-21 13:58:15.852889: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 3 and 9\r\n2017-11-21 13:58:15.856213: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 4 and 9\r\n2017-11-21 13:58:15.858748: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 5 and 9\r\n2017-11-21 13:58:15.860537: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 6 and 9\r\n2017-11-21 13:58:15.861548: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 7 and 9\r\n2017-11-21 13:58:15.861791: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 8 and 9\r\n2017-11-21 13:58:15.861915: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 0\r\n2017-11-21 13:58:15.862038: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 1\r\n2017-11-21 13:58:15.862161: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 2\r\n2017-11-21 13:58:15.862283: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 3\r\n2017-11-21 13:58:15.862405: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 4\r\n2017-11-21 13:58:15.862523: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 5\r\n2017-11-21 13:58:15.862642: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 6\r\n2017-11-21 13:58:15.862759: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 7\r\n2017-11-21 13:58:15.862878: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 8\r\n2017-11-21 13:54:16.736201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Device peer to peer matrix\r\n2017-11-21 13:54:16.736590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1051] DMA: 0 1 2 3 4 5 6 7 8 9 \r\n2017-11-21 13:54:16.736598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 0:   Y Y Y Y Y Y Y Y Y Y \r\n2017-11-21 13:54:16.736602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 1:   Y Y Y Y Y Y Y Y Y Y \r\n2017-11-21 13:54:16.736606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 2:   Y Y Y Y Y Y Y Y Y Y \r\n2017-11-21 13:54:16.736610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 3:   Y Y Y Y Y Y Y Y Y Y \r\n2017-11-21 13:54:16.736613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 4:   Y Y Y Y Y Y Y Y Y Y \r\n2017-11-21 13:54:16.736617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 5:   Y Y Y Y Y Y Y Y Y Y \r\n2017-11-21 13:54:16.736621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 6:   Y Y Y Y Y Y Y Y Y Y \r\n2017-11-21 13:54:16.736625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 7:   Y Y Y Y Y Y Y Y Y Y \r\n2017-11-21 13:54:16.736629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 8:   Y Y Y Y Y Y Y Y Y Y \r\n2017-11-21 13:54:16.736633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 9:   Y Y Y Y Y Y Y Y Y Y \r\n\r\n\r\nNvidia CUDA P2P Output:\r\n\r\n> [P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\r\nDevice: 0, GeForce GTX 1080 Ti, pciBusID: 4, pciDeviceID: 0, pciDomainID:0\r\nDevice: 1, GeForce GTX 1080 Ti, pciBusID: 5, pciDeviceID: 0, pciDomainID:0\r\nDevice: 2, GeForce GTX 1080 Ti, pciBusID: 6, pciDeviceID: 0, pciDomainID:0\r\nDevice: 3, GeForce GTX 1080 Ti, pciBusID: 7, pciDeviceID: 0, pciDomainID:0\r\nDevice: 4, GeForce GTX 1080 Ti, pciBusID: 8, pciDeviceID: 0, pciDomainID:0\r\nDevice: 5, GeForce GTX 1080 Ti, pciBusID: b, pciDeviceID: 0, pciDomainID:0\r\nDevice: 6, GeForce GTX 1080 Ti, pciBusID: c, pciDeviceID: 0, pciDomainID:0\r\nDevice: 7, GeForce GTX 1080 Ti, pciBusID: d, pciDeviceID: 0, pciDomainID:0\r\nDevice: 8, GeForce GTX 1080 Ti, pciBusID: e, pciDeviceID: 0, pciDomainID:0\r\nDevice: 9, GeForce GTX 1080 Ti, pciBusID: f, pciDeviceID: 0, pciDomainID:0\r\nDevice=0 CAN Access Peer Device=1\r\nDevice=0 CAN Access Peer Device=2\r\nDevice=0 CAN Access Peer Device=3\r\nDevice=0 CAN Access Peer Device=4\r\nDevice=0 CAN Access Peer Device=5\r\nDevice=0 CAN Access Peer Device=6\r\nDevice=0 CAN Access Peer Device=7\r\nDevice=0 CAN Access Peer Device=8\r\nDevice=0 CAN Access Peer Device=9\r\nDevice=1 CAN Access Peer Device=0\r\nDevice=1 CAN Access Peer Device=2\r\nDevice=1 CAN Access Peer Device=3\r\nDevice=1 CAN Access Peer Device=4\r\nDevice=1 CAN Access Peer Device=5\r\nDevice=1 CAN Access Peer Device=6\r\nDevice=1 CAN Access Peer Device=7\r\nDevice=1 CAN Access Peer Device=8\r\nDevice=1 CAN Access Peer Device=9\r\nDevice=2 CAN Access Peer Device=0\r\nDevice=2 CAN Access Peer Device=1\r\nDevice=2 CAN Access Peer Device=3\r\nDevice=2 CAN Access Peer Device=4\r\nDevice=2 CAN Access Peer Device=5\r\nDevice=2 CAN Access Peer Device=6\r\nDevice=2 CAN Access Peer Device=7\r\nDevice=2 CAN Access Peer Device=8\r\nDevice=2 CAN Access Peer Device=9\r\nDevice=3 CAN Access Peer Device=0\r\nDevice=3 CAN Access Peer Device=1\r\nDevice=3 CAN Access Peer Device=2\r\nDevice=3 CAN Access Peer Device=4\r\nDevice=3 CAN Access Peer Device=5\r\nDevice=3 CAN Access Peer Device=6\r\nDevice=3 CAN Access Peer Device=7\r\nDevice=3 CAN Access Peer Device=8\r\nDevice=3 CAN Access Peer Device=9\r\nDevice=4 CAN Access Peer Device=0\r\nDevice=4 CAN Access Peer Device=1\r\nDevice=4 CAN Access Peer Device=2\r\nDevice=4 CAN Access Peer Device=3\r\nDevice=4 CAN Access Peer Device=5\r\nDevice=4 CAN Access Peer Device=6\r\nDevice=4 CAN Access Peer Device=7\r\nDevice=4 CAN Access Peer Device=8\r\nDevice=4 CAN Access Peer Device=9\r\nDevice=5 CAN Access Peer Device=0\r\nDevice=5 CAN Access Peer Device=1\r\nDevice=5 CAN Access Peer Device=2\r\nDevice=5 CAN Access Peer Device=3\r\nDevice=5 CAN Access Peer Device=4\r\nDevice=5 CAN Access Peer Device=6\r\nDevice=5 CAN Access Peer Device=7\r\nDevice=5 CAN Access Peer Device=8\r\nDevice=5 CAN Access Peer Device=9\r\nDevice=6 CAN Access Peer Device=0\r\nDevice=6 CAN Access Peer Device=1\r\nDevice=6 CAN Access Peer Device=2\r\nDevice=6 CAN Access Peer Device=3\r\nDevice=6 CAN Access Peer Device=4\r\nDevice=6 CAN Access Peer Device=5\r\nDevice=6 CAN Access Peer Device=7\r\nDevice=6 CAN Access Peer Device=8\r\nDevice=6 CAN Access Peer Device=9\r\nDevice=7 CAN Access Peer Device=0\r\nDevice=7 CAN Access Peer Device=1\r\nDevice=7 CAN Access Peer Device=2\r\nDevice=7 CAN Access Peer Device=3\r\nDevice=7 CAN Access Peer Device=4\r\nDevice=7 CAN Access Peer Device=5\r\nDevice=7 CAN Access Peer Device=6\r\nDevice=7 CAN Access Peer Device=8\r\nDevice=7 CAN Access Peer Device=9\r\nDevice=8 CAN Access Peer Device=0\r\nDevice=8 CAN Access Peer Device=1\r\nDevice=8 CAN Access Peer Device=2\r\nDevice=8 CAN Access Peer Device=3\r\nDevice=8 CAN Access Peer Device=4\r\nDevice=8 CAN Access Peer Device=5\r\nDevice=8 CAN Access Peer Device=6\r\nDevice=8 CAN Access Peer Device=7\r\nDevice=8 CAN Access Peer Device=9\r\nDevice=9 CAN Access Peer Device=0\r\nDevice=9 CAN Access Peer Device=1\r\nDevice=9 CAN Access Peer Device=2\r\nDevice=9 CAN Access Peer Device=3\r\nDevice=9 CAN Access Peer Device=4\r\nDevice=9 CAN Access Peer Device=5\r\nDevice=9 CAN Access Peer Device=6\r\nDevice=9 CAN Access Peer Device=7\r\nDevice=9 CAN Access Peer Device=8\r\n\r\nAny idea how i can fix it ?\r\n", "comments": ["@Phhere Thanks for filing the issue!\r\n\r\n@zheng-xq might be interested in this, and might have some ideas too.\r\n\r\nSadly the LOG message is dropping the status, which would have told us *why* peer access isn't enabled.\r\n\r\nThe relevant code that's dropping the status is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/5439c1e2de01a8684b62aba224d44c392176ac32/tensorflow/core/common_runtime/gpu/gpu_device.cc#L950\r\n\r\nCan you change that line from this:\r\n```\r\nLOG(WARNING)\r\n    << \"Unable to enable peer access between device ordinals \"\r\n    << i_gpu_id << \" and \" << j_gpu_id;\r\n```\r\nto this:\r\n```\r\nLOG(WARNING)\r\n    << \"Unable to enable peer access between device ordinals \"\r\n    << i_gpu_id << \" and \" << j_gpu_id << \", status: \" << status;\r\n```\r\n\r\nAnd then re-compile and re-run?  That should give us more ideas on what's going wrong.\r\n\r\nFYI the actual CUDA call to enable peer access is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/5439c1e2de01a8684b62aba224d44c392176ac32/tensorflow/stream_executor/cuda/cuda_driver.cc#L1591", "Hello, \r\ni compiled tensorflow and now i get an interesting message:\r\n`2017-12-01 09:17:42.115092: W tensorflow/core/common_runtime/gpu/gpu_device.cc:918] Unable to enable peer access between device ordinals 9 and 8, status: Internal: failed to enable peer access from 0x8270570 to 0x7ba7860: CUDA_ERROR_TOO_MANY_PEERS\r\n`", "In Cuda, only 8 peer device can be enabled per device. Since there 10 GPUs, therefore the warnings. \r\n\r\nDoes anything actually fail with those warnings? You can still use all 10 GPU devices as normal. \r\n\r\nI think in the long term, TensorFlow should do a better job looking at which GPUs should have peer access, when the system have more than 8 GPUs.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing this out, since the PR with extra logging has been merged, and it's not clear that there's anything extra to do."]}, {"number": 14758, "title": "Build Tensorflow Lite C++ API into a dynamic library for Android", "body": "Is there any way of building the Tensorflow Lite C++ API into a dynamic library for Android?\r\nI have tried to build with bazel for armv7a but this only gives the corresponding static libraries:\r\n`bazel build -c opt //tensorflow/contrib/lite:* --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\" --verbose_failures\r\n`\r\n\r\n", "comments": ["You could try adding linkshared=1 to the framework rule in tensorflow/contrib/lite/BUILD\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/BUILD#L64\r\n(see bazel docs here https://docs.bazel.build/versions/master/be/c-cpp.html)", "Approaching the problem without reading the bazel docs was really not a good idea.\r\nWorked like a charm after, and also managed to use a LINKER_SCRIPT to expose only needed functions, thus reducing the size of the library. ", "Closing as this seems to be resolved."]}, {"number": 14757, "title": "change bazel-mirror to mirror.bazel", "body": "grep -v bazel-mirror:\r\ndo you mean to not use mirror to download ?\r\n\r\nmirror links in workspace.bzl is marked as mirror.bazel , not bazel-mirror.\r\nafter this change , the download speed enhanced.\r\n\r\ni already signed as contributor", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Thanks for this fix! Jenkins: test this please. All yours @yifeif.", "@jakiechris could you fix the cla as well? Thank you!", "sorry it's my first trying pull req.\r\ni did some other changes, and git config user.email \"liuzhenjluccst@163.com\"  on my pc,    \r\nand pushed it to my fork of tensor.\r\ndon't know this time cla's judge result  ", "`liuzhenjluccst@163.com` is not on record as having signed the CLA. Please visit https://cla.developers.google.com/ to sign.", "Contributor License Agreements\r\nGoogle Individual Contributor License Agreement\r\n\r\nBelow is the contact information we have for your Google Individual CLA. The information you provide below will be maintained in accordance with Google's privacy policy.\r\n\r\n* required fields\r\nDate Signed\r\nNov 21, 2017 00:37 PST\r\nEmail(s)\r\nLearn more about managing email addresses for your account.\r\nwobushiliu2@gmail.com\r\nName *\r\n\r\nZhenLiu\r\nAddress *\r\n\r\nShenZhen city , nanshan district. langshan road TongFang information harbor C 8\r\nCountry *\r\n\r\nChina\r\nPhone Number *\r\n\r\n86 13728881140\r\nGitHub Account Name\r\n\r\nliuzhenjluccst@163.com\r\n", "@jart thnks for this support.\r\n          and should i do git config user.email \"wobushiliu2@gmail.com\" on my pc, and push some modifications ? ", "Running these commands might work:\r\n\r\n```sh\r\ngit reset HEAD~1\r\ngit add .\r\ngit commit --amend --no-edit --author 'Your Name <wobushiliu2@gmail.com>'\r\n```\r\n\r\nThen `git push` with the `--force` flag.", "\u279c  tensorflow git:(master) git reset HEAD~1\r\nUnstaged changes after reset:\r\nM\ttensorflow/contrib/makefile/Makefile\r\n\u279c  tensorflow git:(master) \u2717 git add *\r\n\u279c  tensorflow git:(master) \u2717 git config user.email \"wobushiliu2@gmail.com\"\r\n\u279c  tensorflow git:(master) \u2717 git config user.email                        \r\nwobushiliu2@gmail.com\r\n\u279c  tensorflow git:(master) \u2717 git commit --amend --no-edit --author 'jakiechris <wobushiliu2@gmail.com>'\r\n[master c1b6c75] change bazel-mirror to mirror.bazel\r\n Author: jakiechris <wobushiliu2@gmail.com>\r\n 2 files changed, 3 insertions(+), 2 deletions(-)\r\n\u279c  tensorflow git:(master) git push --force origin master\r\nUsername for 'https://github.com': jakiechris\r\nPassword for 'https://jakiechris@github.com': \r\nCounting objects: 13, done.\r\nDelta compression using up to 4 threads.\r\nCompressing objects: 100% (7/7), done.\r\nWriting objects: 100% (7/7), 625 bytes | 0 bytes/s, done.\r\nTotal 7 (delta 6), reused 0 (delta 0)\r\nremote: Resolving deltas: 100% (6/6), completed with 6 local objects.\r\nTo https://github.com/jakiechris/tensorflow\r\n + b3d4708...c1b6c75 master -> master (forced update)\r\n", "@jart  i followed,  could cla rerun ?", "> Once you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.", "I signed it!", "Thank you for taking the time to cut through the red tape!\r\n\r\nI've confirmed `wobushiliu2@gmail.com` is on record with signcla/ and that it is the email used when I `git log` on the CLI.\r\n\r\nThe problem now is likely that this email isn't associated with your GitHub account, so the bot has no way of knowing you are you.\r\n\r\n![image](https://user-images.githubusercontent.com/49262/33112238-12655c26-cf07-11e7-88be-4fb1ccd13d82.png)\r\n\r\nI'd recommend going into your settings and make it look something like this:\r\n\r\n![image](https://user-images.githubusercontent.com/49262/33112419-f7b8fbe8-cf07-11e7-8627-010cbf7533db.png)\r\n", "thank you for ur quit detailed guiding. now i added the gmail addr to github.\r\n\r\n\r\n\r\n\r\nEmails\r\nliuzhenjluccst@163.com Primary  Public Notifications \r\nwobushiliu2@gmail.com \r\nAdd email address\r\n\r\n  Add\r\nPrimary email address\r\nliuzhenjluccst@163.com will be used for account-related notifications and for web-based GitHub operations (e.g. edits and merges).\r\n\r\n  Save\r\nBackup email address\r\nYour backup GitHub email address can be used to reset your password if you no longer have access to your primary email address.\r\n\r\n  Save\r\n Keep my email address private\r\nWe'll remove your public profile email and use 12059735+jakiechris@users.noreply.github.com when performing web-based Git operations and sending email on your behalf. If you want command line Git operations to use your private email you must set your email in Git.\r\n", "CLAs look good, thanks!\n\n<!-- ok -->", "need i set \"wobushiliu2@gmail.com\" to be my primary email address ?", "@jart \r\nthanks to jart again, it seems worked~\r\nis the next step reviewing my pull req by tensor administrators ?", "> thank you for ur quit detailed guiding. now i added the gmail addr to github.\r\n\r\nDon't mention it :) I believe in taking care of the folks who put aside the time to send us bug fixes.\r\n\r\n> need i set \"wobushiliu2@gmail.com\" to be my primary email address ?\r\n\r\nI'm not sure if it matters. Whatever you did looks great though.\r\n\r\n@yifeif This one should be ready to go. Jenkins: test this please.", "Thanks for getting the cla working @jakiechris  and @jart!\r\nIs the change in tensorflow/contrib/makefile/Makefile intentional?", "Yes if that Makefile change is worth keeping, it might be better to send that in a separate pull request.", "sorry for not testing all ndk versions, yes there are 2 commits : \r\n1. Makefile file added a NDK path include for NDK r16 (which may cause failure in ci.tensorflow.org, i will test it soon )\r\n2. download_dependencies.sh file changed the mirror config.\r\n\r\ni should split these 2 commits ", "Can you send an entirely separate pull request for that ndk_root thing?", "ok, i 'll send a new pull request only for ndk r16 ,  which modified the only file of Mafile \r\ni am now testing the compilation myself and make sure it's can pass, sorry for pulling without a test ", "i found it's not the easy thing to solve ndk r16 issue ,  so i will close this pull , and new a pull req to change only the download part . \r\n\r\nandroid NDK r16 killed GNU , so tensor of android platform need be prepared for llvm-clang.\r\nit'll be a huge change in Makefile , so i think leave it to tensor developers."]}, {"number": 14756, "title": "Downgrade to Bazel 0.4.2 for Tensorflow r1.0", "body": "Hello, \r\n\r\nI need to checkout to Tensorflow r1.0 and as suggested the Bazel version should be 0.4.2\r\n\r\nI have already installed Bazel and after upgrade bazel version is 0.7.0\r\n\r\nDo you know the steps so I can downgrade to Bazel 0.4.2? \r\n\r\nI have tried with apt-get install bazel=0.4.2 but this does not work, \r\n\r\nand I have also tried to uninstall by executing the command rm -fr ~/.bazel ~/.bazelrc and deleting relevant data in ~/.cache/bazel/ folder, but this did not also work.\r\n\r\nAny suggestions ?\r\n\r\nThank you in advance", "comments": ["This issue is relevant for Bazel community, closing it here."]}, {"number": 14755, "title": "Problem with assigning values to matrix indices in tensorflow", "body": "Hi,\r\n\r\nI am constructing a NN with tensorflow that uses a custom stddev function. I have for a batch and indices i and j a function `AcrossBatchSD(batch, i, j)`. Of course, `import tensorflow as tf`.\r\n\r\n```\r\ndef AcrossBatchSD(batch, i, j):\r\n\r\n    _, varR = tf.nn.moments(batch[:, i, j, 0], axes=[0])\r\n    _, varG = tf.nn.moments(batch[:, i, j, 1], axes=[0])\r\n    _, varB = tf.nn.moments(batch[:, i, j, 2], axes=[0]) \r\n \r\n    return tf.sqrt(varR), tf.sqrt(varG), tf.sqrt(varB)\r\n```\r\n\r\nThis function seems to work, but then I would like to do the following:\r\n\r\n```\r\ndef MinibatchStdDev(batch, window, mb_size):\r\n\r\n    n = batch[0].shape[0].value \r\n    f1 = tf.Variable(tf.zeros([n, n]))\r\n    f2 = tf.Variable(tf.zeros([n, n]))\r\n    f3 = tf.Variable(tf.zeros([n, n]))\r\n\r\n    for i in range(n):\r\n        for j in range(n):\r\n\r\n            sqrtR, sqrtG, sqrtB = AcrossBatchSD(batch, i, j)          \r\n            f1[i, j].assign(sqrtR)\r\n            f2[i, j].assign(sqrtG)\r\n            f3[i, j].assign(sqrtB) \r\n                \r\n    f = tf.divide(tf.add(tf.add(f1, f2), f3), 3)\r\n    F = tf.reduce_mean(f)\r\n\r\n    return tf.multiply(F, tf.ones(([mb_size, window, window]))))\r\n```\r\n\r\nMy problem is that the function `assign` in tensorflow is not differentiable, so `tf.gradients` will outout a `NoneType`. Therefore, my network cannot be trained.\r\n\r\nA test can be done with\r\n\r\n```\r\nmb_size = 3\r\nwindow = 4\r\nbatch = tf.Variable(tf.random_normal([mb_size, 1024, 1024, 3]), tf.float32)\r\nout = MinibatchStdDev(batch=batch, window=window, mb_size=mb_size)\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\nsess.run(out)\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14754, "title": "Hi,", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 14753, "title": "compile tensorflow lite static library using QCC on QNX Platform", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:master latest\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:0.7\r\n\r\n### Describe the problem\r\n\r\nmy goal is compile tensorflow lite static library using QNX-QCC compiler\u3002 This is possible?\r\nAnd do I need to modify the code in kernels/*.cc or other souce code , and how do\u3002\r\n\r\nPlease give me some advice, thx\u3002\r\n\r\n**the make script is just  like below\uff1a**\r\n\r\n**make script file:**\r\n\r\nfunction make_qnx() {\r\n\tif [ ! -d $1 ];then\r\n\t\tmkdir $1 || exit_popd 1\r\n\tfi\r\n\tcd $1\r\n\texport QNX_ABI=$1\r\n\tsource /opt/qnx660/qnx660-env.sh\r\n\t~/DevTools/cmake-3.9.0-rc5-Linux-x86_64/bin/cmake -G \"Unix Makefiles\" -DQNX_PLATFORM_ABI=\"$1\" -DPLATFORM_ABI=qnx -DCMAKE_TOOLCHAIN_FILE=${CURRENT_SCRIPT_DIR}/toolchains/**qnx.toolchain.cmake**\r\n\tmake\r\n}\r\n\r\n**qnx.toolchain.cmake file :**\r\n\r\ncmake_minimum_required(VERSION 2.8)\r\nset(CMAKE_SYSTEM_NAME QNX)\r\nset(QNX_PLATFORM_ABI \"$ENV{QNX_ABI}\")\r\n\r\nif(QNX_PLATFORM_ABI STREQUAL \"x86\")\r\n\tset(ARCH_NAME gcc_ntox86)\r\n\r\n\tset(CMAKE_C_COMPILER /opt/qnx660/host/linux/x86/usr/bin/qcc)\r\n\tset(CMAKE_C_COMPILER_TARGET ${ARCH_NAME})\r\n\tset(CMAKE_CXX_COMPILER /opt/qnx660/host/linux/x86/usr/bin/QCC)\r\n\tset(CMAKE_CXX_COMPILER_TARGET ${ARCH_NAME})\r\nelseif(QNX_PLATFORM_ABI STREQUAL \"armv7\")\r\n\tset(ARCH_NAME gcc_ntoarmv7le)\r\n\t\r\n\tset(CMAKE_C_COMPILER /opt/qnx660/host/linux/x86/usr/bin/qcc)\r\n\tset(CMAKE_C_COMPILER_TARGET ${ARCH_NAME})\r\n\tset(CMAKE_CXX_COMPILER /opt/qnx660/host/linux/x86/usr/bin/QCC)\r\n\tset(CMAKE_CXX_COMPILER_TARGET ${ARCH_NAME})\r\nelse()\r\n message( SEND_ERROR \"Unknown QNX_PLATFORM_ABI=\\\"${QNX_PLATFORM_ABI}\\\" is specified.\" )\r\nendif()\r\n\r\n@andrehentz @aselle \r\n\r\n\r\n", "comments": ["It should be possible. You likely will have to modify some things. Could you give it a try and, we can help you when you run into trouble you can't solve. You can look at the Makefile for a simple build environment. You will likely not be able to build using bazel, since it doesn't support QNX toolchains.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Has this been resolved? It would be very helpful if you could please publish steps or advice", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I would also like to have tensorflow lite on QNX. Hence, I am willing to try with some support.\r\nSo, do I need to port Bazel first to QNX ?", "I compile and run tensorflowlite 2.4 on snapdragon sa8155p using make file successfully with qnx700. Need to change build scripts and lib such as abseil.\r\nBut new cmake file still not working. With enough time I will give it a try."]}, {"number": 14752, "title": "How to solve the error: tensorflow.python.framework.errors_impl.NotFoundError: Key conv_layer3/bias not found in checkpoint", "body": "\r\n#coding=utf-8\r\n#tensorflow 1.4\r\n#python 3.6\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\n\r\n#\u83b7\u53d6dataset\r\ndef load_data(dataset_path):\r\n    img = Image.open(dataset_path)\r\n    # \u5b9a\u4e49\u4e00\u4e2a20 \u00d7 20\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u4e00\u5171\u670940\u4e2a\u4eba\uff0c\u6bcf\u4e2a\u4eba\u90fd10\u5f20\u6837\u672c\u7167\u7247\r\n    img_ndarray = np.asarray(img, dtype='float64') / 256        #\u56fe\u7247\u7070\u5ea6\u503c\u8f93\u51fa\r\n    #img_ndarray = np.asarray(img, dtype='float32') / 32\r\n    # \u8bb0\u5f55\u8138\u6570\u636e\u77e9\u9635\uff0c57 * 47\u4e3a\u6bcf\u5f20\u8138\u7684\u50cf\u7d20\u77e9\u9635\r\n    faces = np.empty((400, 57 * 47))        #\u8138\u6570\u636e\u77e9\u9635\r\n    # \u8138\u6570\u636e\u77e9\u9635\u5316\u4e3a\u4e00\u7ef4\u5411\u91cf\r\n    for row in range(20):\r\n        for column in range(20):\r\n            faces[20 * row + column] = np.ndarray.flatten(\r\n                img_ndarray[row * 57: (row + 1) * 57, column * 47 : (column + 1) * 47]\r\n                )\r\n\r\n    label = np.zeros((400, 40))     #\u7a7a\u767d\u7684label\u77e9\u9635\r\n    for i in range(40):\r\n        label[i * 10: (i + 1) * 10, i] = 1      #\u521d\u59cb\u5206\u7c7b\u77e9\u9635\r\n\r\n    # \u5c06\u6570\u636e\u5206\u6210\u8bad\u7ec3\u96c6\uff0c\u9a8c\u8bc1\u96c6\uff0c\u6d4b\u8bd5\u96c6\r\n    train_data = np.empty((320, 57 * 47))\r\n    train_label = np.zeros((320, 40))\r\n\r\n    vaild_data = np.empty((40, 57 * 47))\r\n    vaild_label = np.zeros((40, 40))\r\n\r\n    test_data = np.empty((40, 57 * 47))\r\n    test_label = np.zeros((40, 40))\r\n\r\n    # \u5404\u6570\u636e\u96c6\u521d\u59cb\u5316\r\n    for i in range(40):\r\n        train_data[i * 8: i * 8 + 8] = faces[i * 10: i * 10 + 8]\r\n        train_label[i * 8: i * 8 + 8] = label[i * 10: i * 10 + 8]\r\n\r\n        vaild_data[i] = faces[i * 10 + 8]\r\n        vaild_label[i] = label[i * 10 + 8]\r\n\r\n        test_data[i] = faces[i * 10 + 9]\r\n        test_label[i] = label[i * 10 + 9]\r\n\r\n    train_data = train_data.astype('float32')\r\n    vaild_data = vaild_data.astype('float32')\r\n    test_data = test_data.astype('float32')\r\n\r\n    return [\r\n        (train_data, train_label),\r\n        (vaild_data, vaild_label),\r\n        (test_data, test_label)\r\n    ]\r\n\r\ndef convolutional_layer(data, kernel_size, bias_size, pooling_size):        #\u6570\u636e\uff0c\u5377\u79ef\u6838\uff0c\u504f\u5dee\uff0c\u6c60\r\n    kernel = tf.get_variable(\"conv\", kernel_size, initializer=tf.random_normal_initializer())\r\n    bias = tf.get_variable('bias', bias_size, initializer=tf.random_normal_initializer())\r\n    conv = tf.nn.conv2d(data, kernel, strides=[1, 1, 1, 1], padding='SAME')     #strides\u6b65\u957f\uff0cpadding\u5377\u79ef\u65b9\u5f0f\uff0c\u8868\u793a\u5377\u79ef\u6838\u53ef\u4ee5\u505c\u7559\u5728\u56fe\u50cf\u8fb9\u7f18\r\n    linear_output = tf.nn.relu(tf.add(conv, bias))      #\u6fc0\u6d3b\u51fd\u6570\r\n    # pooling: Tensor(\"conv_layer2/MaxPool:0\", shape=(40, 15, 12, 64), dtype=float32)\r\n    pooling = tf.nn.max_pool(linear_output, ksize=pooling_size, strides=pooling_size, padding=\"SAME\")#\u6c60\u5316\u51fd\u6570\r\n    return pooling\r\n\r\ndef linear_layer(data, weights_size, biases_size):\r\n    weights = tf.get_variable(\"weigths\", weights_size, initializer=tf.random_normal_initializer())      #\u5377\u79ef\u6743\u91cd\u77e9\u9635\r\n    biases = tf.get_variable(\"biases\", biases_size, initializer=tf.random_normal_initializer())\r\n    return tf.add(tf.matmul(data, weights), biases)     #f(x) = Wx + b\r\n\r\ndef convolutional_neural_network(data):\r\n    # \u6839\u636e\u7c7b\u522b\u4e2a\u6570\u5b9a\u4e49\u6700\u540e\u8f93\u51fa\u5c42\u7684\u795e\u7ecf\u5143\r\n    n_ouput_layer = 40\r\n\r\n    kernel_shape1 = [5, 5, 1, 32]       #\u5377\u79ef\u6838\u7684\u5927\u5c0f\r\n    kernel_shape2 = [5, 5, 32, 64]\r\n    kernel_shape3 = [5, 5, 64, 128]\r\n\r\n    bias_shape1 = [32]      #\u7b2c\u4e00\u5c42\u504f\u5dee\u77e9\u9635\u7684\u5927\u5c0f\r\n    bias_shape2 = [64]      #\u7b2c\u4e8c\u5c42\u504f\u5dee\u77e9\u9635\u7684\u5927\u5c0f\r\n    bias_shape3 = [128]\r\n\r\n    full_conn_w_shape = [8 * 6 * 128, 1024]        #Softmax Regression\u6a21\u578b\u53c2\u6570\r\n    full_conn_b_shape = [1024]\r\n\r\n    out_b_shape = [n_ouput_layer]\r\n    out_w_shape = [1024, n_ouput_layer]  # \u8f93\u51fa\u5c42\u6743\u91cd\u77e9\u9635\r\n\r\n    data = tf.reshape(data, [-1, 57, 47, 1])\r\n\r\n    # \u7ecf\u8fc7\u7b2c\u4e00\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u540e\uff0c\u5f97\u5230\u7684\u5f20\u91cfshape\u4e3a\uff1a[batch, 29, 24, 32]\r\n    with tf.variable_scope(\"conv_layer1\") as layer1:\r\n        layer1_output = convolutional_layer(\r\n            data = data,\r\n            kernel_size = kernel_shape1,\r\n            bias_size = bias_shape1,\r\n            pooling_size = [1, 2, 2, 1]\r\n        )\r\n    # \u7ecf\u8fc7\u7b2c\u4e8c\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u540e\uff0c\u5f97\u5230\u7684\u5f20\u91cfshape\u4e3a\uff1a[batch, 15, 12, 64]\r\n    with tf.variable_scope(\"conv_layer2\") as layer2:\r\n        layer2_output = convolutional_layer(\r\n            data=layer1_output,\r\n            kernel_size=kernel_shape2,\r\n            bias_size=bias_shape2,\r\n            pooling_size=[1, 2, 2, 1]\r\n        )\r\n    with tf.variable_scope(\"conv_layer3\") as layer3:\r\n        layer3_output = convolutional_layer(\r\n            data=layer2_output,\r\n            kernel_size=kernel_shape3,\r\n            bias_size=bias_shape3,\r\n            pooling_size=[1, 2, 2, 1]\r\n        )\r\n    with tf.variable_scope(\"full_connection\") as full_layer4:\r\n        # \u8bb2\u5377\u79ef\u5c42\u5f20\u91cf\u6570\u636e\u62c9\u62102-D\u5f20\u91cf\u53ea\u6709\u6709\u4e00\u5217\u7684\u5217\u5411\u91cf\r\n        layer3_output_flatten = tf.contrib.layers.flatten(layer3_output)\r\n        layer4_output = tf.nn.relu(\r\n            linear_layer(\r\n                data=layer3_output_flatten,\r\n                weights_size=full_conn_w_shape,\r\n                biases_size=full_conn_b_shape\r\n            )\r\n        )\r\n\r\n    with tf.variable_scope(\"output\") as output_layer5:\r\n        output = linear_layer(\r\n            data=layer4_output,\r\n            weights_size=out_w_shape,\r\n            biases_size=out_b_shape\r\n        )\r\n    print(data)\r\n    return output\r\n\r\ndef train_facedata(dataset, model_dir,model_path):\r\n\r\n    # train_set_x = data[0][0]\r\n    # train_set_y = data[0][1]\r\n    # valid_set_x = data[1][0]\r\n    # valid_set_y = data[1][1]\r\n    # test_set_x = data[2][0]\r\n    # test_set_y = data[2][1]\r\n    # X = tf.placeholder(tf.float32, shape=(None, None), name=\"x-input\")  # \u8f93\u5165\u6570\u636e\r\n    # Y = tf.placeholder(tf.float32, shape=(None, None), name='y-input')  # \u8f93\u5165\u6807\u7b7e\r\n\r\n    batch_size = 40\r\n\r\n    # train_set_x, train_set_y = dataset[0]\r\n    # valid_set_x, valid_set_y = dataset[1]\r\n    # test_set_x, test_set_y = dataset[2]\r\n    train_set_x = dataset[0][0]\r\n    train_set_y = dataset[0][1]\r\n    valid_set_x = dataset[1][0]\r\n    valid_set_y = dataset[1][1]\r\n    test_set_x = dataset[2][0]\r\n    test_set_y = dataset[2][1]\r\n\r\n    X = tf.placeholder(tf.float32, [batch_size, 57 * 47])\r\n    Y = tf.placeholder(tf.float32, [batch_size, 40])\r\n\r\n    predict = convolutional_neural_network(X)\r\n    cost_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=Y))\r\n    optimizer = tf.train.AdamOptimizer(1e-2).minimize(cost_func)\r\n\r\n    # \u7528\u4e8e\u4fdd\u5b58\u8bad\u7ec3\u7684\u6700\u4f73\u6a21\u578b\r\n    saver = tf.train.Saver()\r\n    #model_dir = './model'\r\n    #model_path = model_dir + '/best.ckpt'\r\n\r\n    with tf.Session() as session:\r\n\r\n        # \u82e5\u4e0d\u5b58\u5728\u6a21\u578b\u6570\u636e\uff0c\u9700\u8981\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\r\n        if not os.path.exists(model_path + \".index\"):\r\n\r\n            session.run(tf.global_variables_initializer())  ##\r\n\r\n            best_loss = float('Inf')\r\n            for epoch in range(20):\r\n\r\n                epoch_loss = 0\r\n                for i in range((int)(np.shape(train_set_x)[0] / batch_size)):\r\n                    x = train_set_x[i * batch_size: (i + 1) * batch_size]\r\n                    y = train_set_y[i * batch_size: (i + 1) * batch_size]\r\n                    _, cost = session.run([optimizer, cost_func], feed_dict={X: x, Y: y})\r\n                    epoch_loss += cost\r\n\r\n                print(epoch, ' : ', epoch_loss)\r\n                if best_loss > epoch_loss:\r\n                    best_loss = epoch_loss\r\n                    if not os.path.exists(model_dir):\r\n                        os.mkdir(model_dir)\r\n                        print(\"create the directory: %s\" % model_dir)\r\n                    save_path = saver.save(session, model_path)\r\n                    print(\"Model saved in file: %s\" % save_path)\r\n\r\n        # \u6062\u590d\u6570\u636e\u5e76\u6821\u9a8c\u548c\u6d4b\u8bd5\r\n        saver.restore(session, model_path)\r\n        correct = tf.equal(tf.argmax(predict,1), tf.argmax(Y,1))\r\n        valid_accuracy = tf.reduce_mean(tf.cast(correct,'float'))\r\n        print('valid set accuracy: ', valid_accuracy.eval({X: valid_set_x, Y: valid_set_y}))\r\n\r\n        test_pred = tf.argmax(predict, 1).eval({X: test_set_x})\r\n        test_true = np.argmax(test_set_y, 1)\r\n        test_correct = correct.eval({X: test_set_x, Y: test_set_y})\r\n        incorrect_index = [i for i in range(np.shape(test_correct)[0]) if not test_correct[i]]\r\n        for i in incorrect_index:\r\n            print('picture person is %i, but mis-predicted as person %i'\r\n                %(test_true[i], test_pred[i]))\r\n\r\n\r\ndef main():\r\n    dataset_path = \"olivettifaces.gif\"\r\n    data = load_data(dataset_path)\r\n    model_dir = './model'\r\n    model_path = model_dir + '/best.ckpt'\r\n    print(len(data))\r\n    train_facedata(data, model_dir, model_path)\r\n\r\nif __name__ == \"__main__\" :\r\n    main()\r\n\r\n**I am sure I have set the bias and the convince of the conv_layer3, but it calls me they are not exist.**", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 14751, "title": "update how_tos/reading_data to use Dataset API", "body": "Since the Dataset API moved from .contrib.data into .data (core) update the MNIST example to use Dataset over queues.\r\n\r\nThis was the first page I've found when searching how to best get data into TF and I thought it should reflect the current best practice. Also, if I understand correctly, queues might be deprecated in favor of Datasets some time in the future.", "comments": ["Can one of the admins verify this patch?", "Any thoughts on this PR? Should I write a separate how to rather than updating the existing one?", "IMO the changes look great, but I will defer to my colleague @MarkDaoust, who has been revising these tutorials and might have stronger opinions.", "I agree with @mrry, This is a great change.\r\n\r\nJust note that there is a long-term plan to eventually merge most of the examples directory into tensorflow/models, and some of the code in this sub-directory might not survive the transition. \r\n\r\nSo while I encourage you to continue sending PRs, you might want to avoid this sub-directory.\r\n\r\nThanks.", "@MarkDaoust What do you think about the docs update in this PR? Do you prefer it like this or rather have the link deleted completely?\r\nI personally find it a bit odd to declare a \"standard format\" and than not provide at least one full implementation example, thats why I didn't simply remove the link.", "Jenkins, test this please.", "Jenkins, test this please.", "Looks like the tests keep failing, not just for me but across all PRs and it doesn't seem to be code I've changed. Would it help if I merge the latest master into this PR? ", "@tensorflow-jenkins test this please"]}, {"number": 14750, "title": "Test case with multiple threads do not work in TF-1.4", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRedHat7.2\r\n- **TensorFlow installed from (source or binary)**:\r\ntensorflow_gpu binary\r\n- **TensorFlow version (use command below)**:\r\nr1.4\r\n- **Python version**: \r\npython 2.7\r\n- **CUDA/cuDNN version**:\r\nCUDA-8.0 CUDNN-6.0\r\n- **GPU model and memory**:\r\nNVIDIA-K80\r\n- **Exact command to reproduce**:\r\npython tensorflow/python/training/sync_replicas_optimizer_test.py\r\n\r\n### Describe the problem\r\n\r\nWhen I start a test case for sync optimizer, it always shows that ps:0,ps:1,worker:1 are not ready.\r\nIt seems this only happens in TF-1.4\r\n\r\n### Source code / logs\r\n\r\nSource Code:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer_test.py\r\n\r\nLogs:\r\n2017-11-21 18:37:58.627374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:                                                                                                                [6/1673]\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2017-11-21 18:37:58.874015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 1 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:07:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2017-11-21 18:37:58.874455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Device peer to peer matrix\r\n2017-11-21 18:37:58.874488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1051] DMA: 0 1 \r\n2017-11-21 18:37:58.874499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 0:   Y Y \r\n2017-11-21 18:37:58.874505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1061] 1:   Y Y \r\n2017-11-21 18:37:58.874523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:58.874532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\r\nE1121 18:37:59.075062187   32596 ev_epoll1_linux.c:1051]     grpc epoll fd: 31\r\n2017-11-21 18:37:59.081712: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\r\n2017-11-21 18:37:59.081747: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\r\n2017-11-21 18:37:59.083912: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:15405\r\n2017-11-21 18:37:59.084202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.084222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.090916: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\r\n2017-11-21 18:37:59.090943: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\r\n2017-11-21 18:37:59.091077: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:17501\r\n2017-11-21 18:37:59.091268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.091286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.097254: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\r\n2017-11-21 18:37:59.097277: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\r\n2017-11-21 18:37:59.097399: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:17358\r\n2017-11-21 18:37:59.097548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.097566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:07:00.0, compute capability: 3.7)\r\n2017-11-21 18:37:59.104080: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:17358, 1 -> localhost:24102}\r\n2017-11-21 18:37:59.104127: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:15405, 1 -> localhost:17501}\r\n2017-11-21 18:37:59.104281: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:24102\r\n2017-11-21 18:38:09.222879: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-11-21 18:38:09.222931: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\r\n2017-11-21 18:38:09.222941: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2017-11-21 18:38:19.223062: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-11-21 18:38:19.223102: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\r\n2017-11-21 18:38:19.223110: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2017-11-21 18:38:29.223253: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-11-21 18:38:29.223284: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\r\n2017-11-21 18:38:29.223291: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n2017-11-21 18:38:39.223379: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n2017-11-21 18:38:39.224059: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:ps/replica:0/task:1\r\n2017-11-21 18:38:39.224068: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:1\r\n", "comments": ["Sorry I find my problem. I find that http_proxy effects the grpc server/client. When I unset my http_proxy, It works. Issue closed"]}, {"number": 14748, "title": "[XLA] Use the specific float list rather than a hard coded list", "body": "This change makes the test use the list of supported types for the backend under test, rather than a hard coded list.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "could you pull rebase and push again?", "sure thing.", "actually - it's ok.  this change has already been fixed in:\r\n\r\n8a98563eb6d552f0bd0931f83837640481c1f938\r\n", "closed as already fixed"]}, {"number": 14747, "title": "change bazel-mirror to mirror.bazel", "body": "grep -v bazel-mirror:\r\ndo you mean to not use mirror to download ? \r\n\r\nmirror links in workspace.bzl is marked as mirror.bazel , not bazel-mirror. \r\nafter this change , the download speed enhanced.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "\r\n\r\ni just signed, wish to give my individual contributions to tensorflow.\r\n\r\ni'm a software engineer now cross compiling tensorflow and packing it to my own C++ libs.\r\n\r\n\r\nAgreement | Name | Date Signed | Manage\r\n-- | -- | -- | --\r\nGoogle Individual CLA | ZhenLiu | Nov 21, 2017 00:37 PST | Edit Contact Information\r\n\r\n"]}, {"number": 14746, "title": "How to use `tf.nn.dropout` to implement embedding dropout", "body": "Recent papers in language modeling use a specific form of embedding dropout that was proposed in [this paper](https://arxiv.org/pdf/1512.05287.pdf). The paper also proposed variational recurrent dropout which was discussed already in [this issue](https://github.com/tensorflow/tensorflow/issues/7927).\r\n\r\nIn embedding dropout, the same dropout mask is used at each timestep and entire words are dropped (i.e. the whole word vector of a word is set to zero). This behavior can be achieved by providing a `noise_shape` to `tf.nn.dropout`.  In addition, the same words are dropped throughout a sequence: \r\n\r\n\"Since we repeat the same mask at each time step, we drop the same words throughout the sequence \u2013 i.e. we drop word types at random rather than word tokens (as an example, the sentence \u201cthe dog and the cat\u201d might become \u201c\u2014 dog and \u2014 cat\u201d or \u201cthe \u2014 and the cat\u201d, but never \u201c\u2014 dog and the cat\u201d). \"\r\n\r\nI couldn't find a way to implement this functionality of embedding dropout efficiently. Are there any plans to incorporate these advances?", "comments": ["@zotroneneis did you figure out how to do it? ", "Yes, I did\r\n", "@zotroneneis can you show a code example?", "Sure! You just have to drop random rows of the embedding matrix (i.e. set them to zero). This corresponds to dropping certain words in the input sequence.\r\n\r\n```\r\n# initialize random embedding matrix\r\nwith tf.variable_scope('embedding'):\r\n   self.embedding_matrix = tf.get_variable( \"embedding\", shape=[self.vocab_size, self.embd_size], dtype=tf.float32, initializer=self.initializer)\r\n\r\nwith tf.name_scope(\"embedding_dropout\"):\r\n   self.embedding_matrix = tf.nn.dropout(self.embedding_matrix, keep_prob=self.embedding_dropout, noise_shape=[self.vocab_size,1])\r\n\r\nwith tf.name_scope('input'):\r\n   self.input_batch = tf.placeholder(tf.int64, shape=(None, None))\r\n   self.inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_batch)\r\n\r\n...\r\n\r\n\r\n```\r\n\r\n  \r\n  ", "@zotroneneis But this drops the same words for the entire batch -- this might make learning more noisy, correct?", "supposed to be like this? \r\n`\r\n\\# initialize random embedding matrix\r\nwith tf.variable_scope('embedding'):\r\n   self.embedding_matrix = tf.get_variable( \"embedding\", shape=[self.vocab_size, self.embd_size], dtype=tf.float32, initializer=self.initializer)\r\n\r\nwith tf.name_scope('input'):\r\n   self.input_batch = tf.placeholder(tf.int64, shape=(None, None))\r\n   self.inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_batch)\r\n\r\nwith tf.name_scope('word_dropout'):\r\n   self.inputs = tf.nn.dropout(self.inputs, keep_prob=self.word_dropout, noise_shape=[tf.shape(self.inputs)[0], tf.shape(self.inputs)[1], 1]) \r\n\r\n`\r\n", "@makai281 No, this would be dropout of the embedding vector (which is also a good idea, but not what we referred to here). We want to dropout inputs (= word types) efficiently.", "Directly drops the whole embedding matrix is inefficient for large vocabulary, since it requires `vocab_size*batch_size` random variable evaluation.\r\nThe implementation below cuts number of random variable evaluation to `num_uniq_words_in_batch * batch_size`\r\n```\r\n# preparing inputs\r\nkeep_prob = 0.8\r\nids = tf.convert_to_tensor([[1,2,3], [3,2,1]])\r\nbatch_size = tf.shape(ids)[0]\r\nmaxlen = tf.shape(ids)[1]\r\nembed_matrix = tf.ones([10, 20])\r\nembed_ids = tf.nn.embedding_lookup(embed_matrix, ids)\r\n\r\nuniq_ids, indices = tf.unique(tf.reshape(ids, [-1]))\r\n# generate random mask for each uniq_id\r\n# independent sample for each instance\r\nrand_mask = tf.random_uniform([batch_size, tf.size(uniq_ids)], dtype=embeddings.dtype)\r\n\r\n# prepare indices for tf.gather_nd\r\nbatch_wise = tf.broadcast_to(tf.expand_dims(tf.range(batch_size), axis=-1), [batch_size, maxlen])\r\nuniq_ids_wise = tf.reshape(indices, [batch_size, maxlen])\r\n\r\n# gather mask and convert it to binary mask\r\nmask_indices = tf.stack([batch_wise, uniq_ids_wise], axis=-1)\r\nbinary_mask = tf.floor(tf.gather_nd(rand_mask, mask_indices) + keep_prob)\r\n\r\n# apply mask and scale\r\ndropped_embeddings = embed_ids * tf.expand_dims(binary_mask, axis=-1) / keep_prob\r\n```\r\n\r\nAnd here is the result:\r\n```\r\nIn [52]: dropped_embeddings.eval()\r\nOut[52]: \r\narray([[[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\r\n         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\r\n\r\n       [[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\r\n         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]]],\r\n      dtype=float32)\r\n\r\nIn [53]: dropped_embeddings.eval()\r\nOut[53]: \r\narray([[[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]],\r\n\r\n       [[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]]],\r\n      dtype=float32)\r\n```\r\n\r\n", "> Sure! You just have to drop random rows of the embedding matrix (i.e. set them to zero). This corresponds to dropping certain words in the input sequence.\r\n> \r\n> ```\r\n> # initialize random embedding matrix\r\n> with tf.variable_scope('embedding'):\r\n>    self.embedding_matrix = tf.get_variable( \"embedding\", shape=[self.vocab_size, self.embd_size], dtype=tf.float32, initializer=self.initializer)\r\n> \r\n> with tf.name_scope(\"embedding_dropout\"):\r\n>    self.embedding_matrix = tf.nn.dropout(self.embedding_matrix, keep_prob=self.embedding_dropout, noise_shape=[self.vocab_size,1])\r\n> \r\n> with tf.name_scope('input'):\r\n>    self.input_batch = tf.placeholder(tf.int64, shape=(None, None))\r\n>    self.inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_batch)\r\n> \r\n> ...\r\n> ```\r\n\r\nThe biggest problem I see with this, is that typically only a tiny fraction of the overall vocabulary exists in a given batch. Dropping a uniformly random (instead of word frequency-informed) 20% of the vocabulary will most times do nothing. In order to have a non-negligible effect, one would need to increase the dropout to very high values (e.g. 70%), at which point the danger is that some sentences (or batches, since the same words will be dropped for the entire batch) will be dropped almost entirely. So my impression is that training will become noisy. The solution is to drop *existing* word IDs, preferably separately for each sample in the batch."]}, {"number": 14745, "title": "execute command properly in bash.exe on windows", "body": "On windows using bazel:\r\n\r\n```\r\n# doesn't work\r\nC:\\Windows\\system32>C:\\msys64\\usr\\bin\\bash.exe -c \"patch --help\"\r\n/usr/bin/bash: patch: command not found\r\n\r\n# works properly\r\nC:\\Windows\\system32>C:\\msys64\\usr\\bin\\bash.exe -l -c \"patch --help\"\r\n```\r\n\r\nAfter adding `-l`, it works. \r\n\r\n```\r\n...\r\n\r\nE:\\>cd tensorflow_dev\r\n\r\nE:\\tensorflow_dev>cd tensorflow\r\n\r\nE:\\tensorflow_dev\\tensorflow>cd test_bazel\r\n\r\nE:\\tensorflow_dev\\tensorflow\\test_bazel>bazel build :test_bazel\r\n...............\r\n____Loading package: tensorflow/test_bazel\r\n____Loading package: @bazel_tools//tools/cpp\r\n____Loading package: @bazel_tools//tools/jdk\r\n____Loading package: @local_config_xcode//\r\n____Loading package: @local_jdk//\r\nDEBUG: C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/baze\r\nl_tools/tools/cpp/lib_cc_configure.bzl:37:3:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest\r\nVisual C++ installed.\r\n\r\n____Loading package: @local_config_cc//\r\nDEBUG: C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/baze\r\nl_tools/tools/cpp/lib_cc_configure.bzl:37:3:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variabl\r\nes,eg. VS140COMNTOOLS\r\n\r\nDEBUG: C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/baze\r\nl_tools/tools/cpp/lib_cc_configure.bzl:37:3:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x8\r\n6)\\Microsoft Visual Studio 14.0\\VC\\\r\n\r\n____Loading complete.  Analyzing...\r\n____Downloading https://mirror.bazel.build/github.com/google/protobuf/archive/b0\r\n4e5cba356212e4e8c66c61bbe0c3a20537c5b9.tar.gz: 713,130 bytes\r\n...\r\nERROR: E:/tensorflow_dev/tensorflow/test_bazel/BUILD:3:1: error loading package\r\n'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no\r\nsuch package '@protobuf_archive//': Traceback (most recent call last):\r\n        File \"E:/tensorflow_dev/tensorflow/workspace.bzl\", line 119\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"E:/tensorflow_dev/tensorflow/workspace.bzl\", line 111, in _apply_p\r\natch\r\n                _execute_and_check_ret_code(repo_ctx, cmd)\r\n        File \"E:/tensorflow_dev/tensorflow/workspace.bzl\", line 92, in _execute_\r\nand_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(127) when executing 'C:\\msys64\\usr\\bin\\bash.exe -c patch -p\r\n1 -d C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/protob\r\nuf_archive -i E:/tensorflow_dev/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: /usr/bin/bash: patch: command not found\r\n and referenced by '//tensorflow/test_bazel:test_bazel'.\r\nERROR: E:/tensorflow_dev/tensorflow/test_bazel/BUILD:3:1: error loading package\r\n'tensorflow': Encountered error while reading extension file 'protobuf.bzl': no\r\nsuch package '@protobuf_archive//': Traceback (most recent call last):\r\n        File \"E:/tensorflow_dev/tensorflow/workspace.bzl\", line 119\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"E:/tensorflow_dev/tensorflow/workspace.bzl\", line 111, in _apply_p\r\natch\r\n                _execute_and_check_ret_code(repo_ctx, cmd)\r\n        File \"E:/tensorflow_dev/tensorflow/workspace.bzl\", line 92, in _execute_\r\nand_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(127) when executing 'C:\\msys64\\usr\\bin\\bash.exe -c patch -p\r\n1 -d C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/protob\r\nuf_archive -i E:/tensorflow_dev/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: /usr/bin/bash: patch: command not found\r\n and referenced by '//tensorflow/test_bazel:test_bazel'.\r\nERROR: Analysis of target '//tensorflow/test_bazel:test_bazel' failed; build abo\r\nrted: error loading package 'tensorflow': Encountered error while reading extens\r\nion file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most\r\nrecent call last):\r\n        File \"E:/tensorflow_dev/tensorflow/workspace.bzl\", line 119\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"E:/tensorflow_dev/tensorflow/workspace.bzl\", line 111, in _apply_p\r\natch\r\n                _execute_and_check_ret_code(repo_ctx, cmd)\r\n        File \"E:/tensorflow_dev/tensorflow/workspace.bzl\", line 92, in _execute_\r\nand_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(127) when executing 'C:\\msys64\\usr\\bin\\bash.exe -c patch -p\r\n1 -d C:/users/win7-vm/appdata/local/temp/_bazel_win7-vm/9apswmfr/external/protob\r\nuf_archive -i E:/tensorflow_dev/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: /usr/bin/bash: patch: command not found\r\n.\r\n____Elapsed time: 20.658s\r\n\r\nE:\\tensorflow_dev\\tensorflow\\test_bazel>\r\n```", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I tested this and found that -lc is required for ```GNU bash, version 4.4.12(1)-release-(x86_64-pc-msys)``` for MSYS2"]}, {"number": 14744, "title": "Quantization: error during quantizing inception v3 model", "body": "Hi experts,\r\n\r\nI met issues during quantizing retained inception v3 model. could someone help take a look? Thanks!\r\n\r\n**Environment:** tensorflow-1.4.0, python 3.5.2(Anaconda 4.2.0)\r\n**Issue description:**\r\nI am using the downloaded inception v3 model, and try to follow the guide written by @petewarden [,](https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/). always met the error below:\r\n```\r\nValueError: No inputs to quantize for op: name: \"conv/Conv2D\"\r\nop: \"Conv2D\"\r\ninput: \"Mul\"\r\ninput: \"conv/conv2d_params\"\r\n...\r\n```\r\nmy python script is as below:\r\n```\r\nfrom tensorflow.contrib.quantize.python import quantize_graph\r\nfrom tensorflow import gfile, GraphDef,import_graph_def,get_default_graph\r\n\r\nmodel_file=\"classify_image_graph_def.pb\"\r\nwith gfile.FastGFile(model_file,'rb') as f:\r\n    graph_def = GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    import_graph_def(graph_def,name='')\r\n\r\n    graph=get_default_graph()\r\n    q_graph=quantize_graph.create_eval_graph(graph)\r\n    with gfile.FastGFile(\"output.pb\",'wb') as f1:\r\n        f1.write(q_graph.as_graph_def().SerializeToString())\r\n```\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Hi @bikongyouran Did you solve this problem?", "Just came across the same problem with error message : ValueError: No inputs to quantize for op: name: \"prefix/conv/Conv2D\" with tensorflow 1.4.1 and python 2.7.5", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "sorry for the delayed response. i have switched to another model, and the problem is gone. i will close this issue.", "What was different about the new model? Did you figure out the reason why the error came up from the first place?\r\n\r\nthanks!"]}, {"number": 14743, "title": "The API doc for tensorflow.keras.backend.set_learning_phase is wrong.", "body": "### System information\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\nPython 2.7.13 :: Anaconda, Inc.\r\n\r\n### Describe the problem\r\nWhen I implemented Custom Estimator API with tf.keras, I countered following error: \r\n\r\n```python\r\nTypeError: `pred` must be a Tensor, a Variable, or a Python bool.\r\n```\r\n\r\nThis happens if I set {0, 1} to tensorflow.keras.backend.set_learning_phase, and doesn't happen if I set {False, True} instead of {0, 1}. However, [the API doc](https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_learning_phase) for tensorflow.keras.backend.set_learning_phase says the method is supposed to take {0, 1}. So, I think that the API doc should be modified.\r\n\r\n### Source code / logs\r\n- Source code (You can find more details [here](https://github.com/tensorflow/tensorflow/files/1490186/How.to.integrate.keras.into.Experiment.pdf))\r\n\r\n```python\r\ndef inference(images, mode):\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    tf.keras.backend.set_learning_phase(1) # this should be True\r\n  else:\r\n    tf.keras.backend.set_learning_phase(0) # this should be False\r\n        \r\n  model = tf.keras.models.Sequential()\r\n  # Define input tensor in Keras world.\r\n  model.add(tf.keras.layers.InputLayer(input_tensor=images))\r\n\r\n  # The first convolutional layer.\r\n  model.add(tf.keras.layers.Conv2D(\r\n      filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\r\n  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))\r\n\r\n  # The second convolutional layer.\r\n  model.add(tf.keras.layers.Conv2D(\r\n      filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\r\n  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))\r\n  model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n  # The third convolutional layer\r\n  model.add(tf.keras.layers.Conv2D(\r\n      filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\r\n\r\n  # The fourth convolutional layer\r\n  model.add(tf.keras.layers.Conv2D(\r\n      filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\r\n  model.add(tf.keras.layers.Dropout(0.25))\r\n\r\n  model.add(tf.keras.layers.Flatten())\r\n  model.add(tf.keras.layers.Dense(512, activation='relu'))\r\n  model.add(tf.keras.layers.Dropout(0.5))\r\n  model.add(tf.keras.layers.Dense(NUM_CLASSES))\r\n  logits = model.output\r\n  return logits\r\n```\r\n\r\n\r\n\r\n\r\n- Error logs\r\n```python\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-38-4dbe7b3f6667> in <module>()\r\n     20   schedule='train_and_evaluate',\r\n     21   run_config=run_config,\r\n---> 22   hparams=hparams\r\n     23 )\r\n     24 \r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.pyc in run(experiment_fn, output_dir, schedule, run_config, hparams)\r\n    216   schedule = schedule or _get_default_schedule(run_config)\r\n    217 \r\n--> 218   return _execute_schedule(experiment, schedule)\r\n    219 \r\n    220 \r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.pyc in _execute_schedule(experiment, schedule)\r\n     44     logging.error('Allowed values for this experiment are: %s', valid_tasks)\r\n     45     raise TypeError('Schedule references non-callable member %s' % schedule)\r\n---> 46   return task()\r\n     47 \r\n     48 \r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.pyc in train_and_evaluate(self)\r\n    623                   hooks=self._eval_hooks)\r\n    624           ]\r\n--> 625       self.train(delay_secs=0)\r\n    626 \r\n    627     # If the checkpoint_and_export flag and appropriate estimator configuration\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.pyc in train(self, delay_secs)\r\n    365     return self._call_train(input_fn=self._train_input_fn,\r\n    366                             max_steps=self._train_steps,\r\n--> 367                             hooks=self._train_monitors + extra_hooks)\r\n    368 \r\n    369   def evaluate(self, delay_secs=None, name=None):\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.pyc in _call_train(self, _sentinel, input_fn, steps, hooks, max_steps)\r\n    805                                    steps=steps,\r\n    806                                    max_steps=max_steps,\r\n--> 807                                    hooks=hooks)\r\n    808     else:\r\n    809       return self._estimator.fit(input_fn=input_fn,\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    300 \r\n    301     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 302     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    303     logging.info('Loss for final step: %s.', loss)\r\n    304     return self\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)\r\n    709       with ops.control_dependencies([global_step_read_tensor]):\r\n    710         estimator_spec = self._call_model_fn(\r\n--> 711             features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n    712       # Check if the user created a loss summary, and add one if they didn't.\r\n    713       # We assume here that the summary is called 'loss'. If it is not, we will\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _call_model_fn(self, features, labels, mode, config)\r\n    692     if 'config' in model_fn_args:\r\n    693       kwargs['config'] = config\r\n--> 694     model_fn_results = self._model_fn(features=features, **kwargs)\r\n    695 \r\n    696     if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):\r\n\r\n<ipython-input-29-a5666390b8b0> in cifar10_model_fn(features, labels, mode, params)\r\n     10 \r\n     11   # Calculate logits through CNN\r\n---> 12   logits = inference(images, mode)\r\n     13 \r\n     14   # Get predictions\r\n\r\n<ipython-input-37-23187df0ff6c> in inference(images, mode)\r\n     20 \r\n     21     # NOTE: Dropout is not working with model_fn in TF1.4\r\n---> 22     model.add(tf.keras.layers.Dropout(0.25))\r\n     23 \r\n     24     # The third convolutional layer\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/models.pyc in add(self, layer)\r\n    499           output_tensors=self.outputs)\r\n    500     else:\r\n--> 501       output_tensor = layer(self.outputs[0])\r\n    502       if isinstance(output_tensor, list):\r\n    503         raise TypeError('All layers in a Sequential model '\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.pyc in __call__(self, inputs, **kwargs)\r\n    250     \"\"\"\r\n    251     # Actually call the layer (optionally building it).\r\n--> 252     output = super(Layer, self).__call__(inputs, **kwargs)\r\n    253 \r\n    254     # Update learning phase info.\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\r\n    573         if in_graph_mode:\r\n    574           self._assert_input_compatibility(inputs)\r\n--> 575         outputs = self.call(inputs, *args, **kwargs)\r\n    576 \r\n    577         if outputs is None:\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/layers/core.pyc in call(self, inputs, training)\r\n    116     if training is None:\r\n    117       training = K.learning_phase()\r\n--> 118     output = super(Dropout, self).call(inputs, training=training)\r\n    119     if training is K.learning_phase():\r\n    120       output._uses_learning_phase = True  # pylint: disable=protected-access\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.pyc in call(self, inputs, training)\r\n    298     return utils.smart_cond(training,\r\n    299                             dropped_inputs,\r\n--> 300                             lambda: array_ops.identity(inputs))\r\n    301 \r\n    302 \r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/utils.pyc in smart_cond(pred, fn1, fn2, name)\r\n    201     raise TypeError('`fn2` must be callable.')\r\n    202 \r\n--> 203   pred_value = constant_value(pred)\r\n    204   if pred_value is not None:\r\n    205     if pred_value:\r\n\r\n/usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/utils.pyc in constant_value(pred)\r\n    231     pred_value = tensor_util.constant_value(pred)\r\n    232   else:\r\n--> 233     raise TypeError('`pred` must be a Tensor, a Variable, or a Python bool.')\r\n    234   return pred_value\r\n\r\nTypeError: `pred` must be a Tensor, a Variable, or a Python bool.\r\n```", "comments": ["tf.keras.backend.set_learning_phase takes 0 and 1 as argument, would it be possible for you to write a small example to reproduce the same error?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing it for the lack of any response.", "I also meet this problem,when I use tensorflow.contrib.keras.applications.ResNet50. And it need the learning phase, so I use K.set_learning_phase(1) and I countered the same problem."]}, {"number": 14742, "title": "foldl and foldr gives different results on gpu vs cpu in tensorflow 1.4", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: - Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: - Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: - source\r\n- **TensorFlow version (use command below)**: v1.4.0-3-g5addbae, 1.4.0\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**:  0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: default \r\n- **GPU model and memory**: 1080ti (11GB)\r\n- **Exact command to reproduce**:  \r\nSee code below\r\n\r\n### Describe the problem\r\nWhile testing foldl and foldr, I get the expected result when run on cpu, but get a zero result when running on gpu.  \r\n\r\n### Source code / logs\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n    els = tf.constant([1.0,2.0,3.0])\r\n    f = tf.foldl(lambda a, x: a + x, els)\r\n\r\nwith tf.Session() as sess:\r\n    print tf.GIT_VERSION,tf.VERSION\r\n    print sess.run([els,f])\r\n\r\n------------\r\nResult: \r\nv1.4.0-3-g5addbae 1.4.0\r\n[array([ 1.,  2.,  3.], dtype=float32), 0.0]\r\n\r\nThe last number should be 6.0, ie the sum of the input array.  I get this if I change the device to /cpu:0\r\n\r\n", "comments": ["Testing further, this seems to be related to this error message:\r\ntensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 72.44M (75956224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n\r\nIt looks like a memory error happens, but this does not cause the task to fail.  It just causes in incorrect result.  \r\n\r\nI would suggest that it is better that the job fail than have a wrong answer", "@dloyer do you have the stack trace? It should definitely propagate up.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "There is no error thrown to the client, just a incorrect answer, so no stack trace.  ", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 14741, "title": "Have tf-nightly depend on tb-nightly", "body": "TensorBoard now has an automated nightly release process!\r\n\r\nhttps://pypi.python.org/pypi/tb-nightly/", "comments": ["@yifeif Failures look unrelated. Would it be possible to merge today, so this finds its way into tf-nightly asap? Users of `tf-nightly` are reporting issues using TensorBoard. See https://github.com/tensorflow/tensorboard/issues/764.", "PTAL"]}, {"number": 14740, "title": "Tensorflow lite doesn't support Gather Op with multiple dims?", "body": "Hello,\r\n\r\nFollowing up this SO question which didn't get too much attention:\r\nhttps://stackoverflow.com/questions/47321911/cant-convert-model-to-tensorflows-lite-format\r\nI'm filling this form as a question/feature request.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS, builing and trying to use TF Lite for iOS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Latest, made a pull from HEAD 3 days ago\r\n- **Python version**:  2.7.14\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: clang 9.0.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n\r\n`bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/Users/valentinradu/Playgrounds/char-rnn-tensorflow/remote_save/latest/graph_frz.pb' '--output_file=/Users/valentinradu/Playgrounds/char-rnn-tensorflow/remote_save/latest/graph.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--input_type=FLOAT' '--inference_type=FLOAT' '--input_shapes=1,128:1,50,50' '--input_arrays=state_in,data_in' '--output_arrays=state_out,data_out'`\r\n\r\n### Describe the problem\r\nI have a trained rnn that I try to use on mobile. Problem is, when I use toco to convert my .pb file to .tflite it fails with the following error message. Having a look over the source code that generated that exception, I think it's because of the toco's lack of support for multidimensional inputs. But I'm not sure. If so, will this be added later?\r\n\r\n```\r\nWARNING: Config values are not defined in any .rc file: opt.\r\nINFO: Found 1 target...\r\nTarget //tensorflow/contrib/lite/toco:toco up-to-date:\r\n  bazel-bin/tensorflow/contrib/lite/toco/toco\r\nINFO: Elapsed time: 0.287s, Critical Path: 0.00s\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/Users/valentinradu/Playgrounds/char-rnn-tensorflow/remote_save/latest/graph_frz.pb' '--output_file=/Users/valentinradu/Playgrounds/char-rnn-tensorflow/remote_save/latest/graph.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--input_type=FLOAT' '--inference_type=FLOAT' '--input_shapes=1,128:1,50,50' '--input_arrays=state_in,data_in' '--output_arrays=state_out,data_out'\r\n2017-11-16 06:48:00.156091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Fill\r\n2017-11-16 06:48:00.156811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Fill\r\n2017-11-16 06:48:00.156821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Pack\r\n2017-11-16 06:48:00.156829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Pack\r\n2017-11-16 06:48:00.156841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Unpack\r\n2017-11-16 06:48:00.156856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: StridedSlice\r\n2017-11-16 06:48:00.156872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: StridedSlice\r\n2017-11-16 06:48:00.157260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Pack\r\n2017-11-16 06:48:00.157277: I tensorflow/contrib/lite/toco/import_tensorflow.cc:937] Converting unsupported operation: Pack\r\n2017-11-16 06:48:00.158053: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 40 operators, 64 arrays (0 quantized)\r\n2017-11-16 06:48:00.158141: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:793] Check failed: indices_shape.dimensions_count() == 1 (2 vs. 1)\r\n```\r\n\r\n### Source code / logs\r\nThe repository I user to train the model can be found in full here:\r\nhttps://github.com/valentinradu/char-rnn-tensorflow/blob/master/char_rnn/model.py\r\n", "comments": ["Answered on StackOverflow: https://stackoverflow.com/questions/47321911/cant-convert-model-to-tensorflows-lite-format", "@gargn You mean my own answer on SO can be confirmed? No support to date?", "I also met this problem\r\n`\r\n2018-03-04 13:24:20.587520: I tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:961] input array name is MobilenetV1/Embeding/embedding\r\n2018-03-04 13:24:20.587527: I tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:962] indices array name is input\r\n2018-03-04 13:24:20.587569: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:983] Check failed: indices_shape.dimensions_count() == 1 (2 vs. 1)\r\nAbort trap: 6\r\n`"]}, {"number": 14739, "title": "Eager: Warn with invalid policy", "body": "If a user accidentally writes `tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_WARN)` instead of the correct `tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_WARN)`, they won't get an error until later in their program.\r\n\r\nFor example, `tfe.num_gpus()` after  the incorrect enable call produces\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-71d6509178f5> in <module>()\r\n----> 1 tfe.num_gpus()\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py in num_gpus()\r\n    458     The number of available GPU devices.\r\n    459   \"\"\"\r\n--> 460   return context().num_gpus()\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py in num_gpus(self)\r\n    286   def num_gpus(self):\r\n    287     \"\"\"The number of GPUs available to execute operations.\"\"\"\r\n--> 288     self._initialize_handle_and_devices()\r\n    289     return self._num_gpus\r\n    290 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py in _initialize_handle_and_devices(self)\r\n    121         with errors.raise_exception_on_not_ok_status() as status:\r\n    122           if self._config is not None:\r\n--> 123             config_str = self._config.SerializeToString()\r\n    124             pywrap_tensorflow.TFE_ContextOptionsSetConfig(\r\n    125                 opts, config_str, len(config_str), status)\r\n\r\nAttributeError: 'int' object has no attribute 'SerializeToString'\r\n```\r\n\r\nI'd think it makes more sense to throw an error immediately after the incorrect `enable_eager_execution`. \r\n\r\nThis is on `master` (ab00df9).", "comments": ["@asimshankar could you please take a look into this.", "Sounds like a fair request :), will send out a change to address this."]}, {"number": 14738, "title": "Add str(Label(...)) to bazel macros", "body": "This is a similar pull request to \r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/14737\r\n\r\nbut for r1.4 branch, not master.\r\n\r\nWhen using tensorflow as a submodule in bazel, importing bazel rules from tensorflow.bzl is not working.\r\nThe main reason is that some of the dependencies are hard coded instead of using str(Label(...)) and this makes bazel to generate dependencies like\r\n\r\n//tensorflow\r\n\r\ninstead of\r\n\r\n@org_tensorflow//tensorflow.\r\n\r\nMost of bzl files under tensorflow use Label mechanism to allow supermodule to import tensorflow bazel rules like tf_cc_test, etc, but there were two files that did not use the Label mechanism and these two files block supermodule to import tensorflow bazel rules. This change updates those files to unblock bazel rule importing.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14737, "title": "Add str(Label(...)) to bazel macros", "body": "When using tensorflow as a submodule in bazel, importing bazel rules from tensorflow.bzl is not working.\r\nThe main reason is that some of the dependencies are hard coded instead of using str(Label(...)) and this makes bazel to generate dependencies like\r\n\r\n//tensorflow\r\n\r\ninstead of\r\n\r\n@org_tensorflow//tensorflow.\r\n\r\nMost of bzl files under tensorflow use Label mechanism to allow supermodule to import tensorflow bazel rules like tf_cc_test, etc, but there were two files that did not use the Label mechanism and these two files block supermodule to import tensorflow bazel rules. This change updates those files to unblock bazel rule importing.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14736, "title": "Does Tensorflow support Graphic for AMD (GPU also)  like NVIDIA (GPU)", "body": "Dear All\r\n\r\nI have laptop lenovo G50-80 I7 5500 with AMD RADEON R5 M230-2GB for graphic (it is also gpu based on this link ( https://www.futuremark.com/hardware/gpu/AMD+Radeon+R5+M230/review ), does tensorflow support this amd for computing  the same like what tensorflow did with GPU from NVIDIA ?. if it has to be configured from the source, what is the setting of tensorflow that when i compile it it will run the gpu?\r\n\r\nThx", "comments": ["@MyAusweis I believe so (experimental though). Please see #22 for updates on AMD/OpenCL support", "Yes, it's not supported and OpenCL is not fully ready.", "yes,   it supports newer version of AMD GPUs, such as GFX8 GPU's (Fiji & Polaris Family) and GFX9 (Vega).  I tried AMD RX 480 and works great !  See link below:\r\n\r\nhttps://github.com/ROCmSoftwarePlatform/hiptensorflow/blob/hip/README.ROCm.md\r\n ", "@charlie-ruan do you have windows version of the GPU installation in amd.\r\n", "@TEJAPS , according to AMD hiptensorflow documentation, hiptensorflow only supports Linux for now.", "You definitively need to support AMD's GPU natively asap in my opinion:\r\n- they are leader on free remote GPU clustering\r\n- there are thousands of crypto mining datacenters that will be converted to lowcost GPU farms in the next following years (they all use AMD's GPU)\r\nNot supporting AMD's GPU is not strategic if your aim is massive adoption of tensorflow.", "We need AMD GPUs support, \r\n\r\n- Their cards present amazing value for the money..\r\n- They offer more VRam than GTX cards..\r\n- HBM2 is way faster than DDR6. As u may know, memory bandwidth is the general bottleneck in DL tasks\r\n\r\n", "Recently I heard the announcement of Stadia, the new Google Game platform.\r\nIt is supported by nVidia GPUs.\r\nIt think that Google is definitively focussing, and consequently Tensorflow, on nVidia GPUs support to reduce the integration effort in all its services that require GPUs.\r\nCorporations strategies are not often fair for everyone...", "How is support for the new macbooks, both for TF and TFJS?\r\n\r\nhttps://www.apple.com/uk/shop/buy-mac/macbook-pro/15-inch-space-grey-2.3ghz-8-core-processor-with-turbo-boost-up-to-4.8ghz-512gb#\r\n\r\n\r\nRadeon Pro 560X with 4GB of GDDR5 memory\r\n- \u00a3225.00\r\n\r\nRadeon Pro Vega 16 with 4GB of HBM2 memory\r\n\r\nRadeon Pro Vega 20 with 4GB of HBM2 memory", "Looks like OpenCI support is WIP by google atm.\r\n\r\nTFJS abstracts this stuff away I think so since I use TFJS then these macs should be fine?", "AMD radeon R 5 GPU is that ok if i install tensorflow on my laptop?? will it support or will it be compatible? I am new in this field pardon if my question didn't make any sense!!"]}, {"number": 14735, "title": "R1.1", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 14734, "title": "Fixing download_dependencies.sh bugs for generating TFLite iOS exmaples", "body": "This is the 2nd try for #14631\r\n\r\nTo verify this:\r\n* `git clean -fdx` to clean all local files. \r\n* run `tensorflow/contrib/lite/download_dependencies.sh`\r\n* Verify that you see \"download_dependencies.sh completed successfully\", so the script is completed. \r\n* Verify these files are downloaded to correct location. \r\n * tensorflow/contrib/lite/examples/ios/camera/data/labels.txt\r\n * tensorflow/contrib/lite/examples/ios/camera/data/mobilenet_quant_v1_224.tflite\r\n * tensorflow/contrib/lite/examples/ios/simple/data/labels.txt\r\n * tensorflow/contrib/lite/examples/ios/simple/data/mobilenet_v1_1.0_224.tflite\r\n* Run `tensorflow/contrib/lite/build_ios_universal_lib.sh` to verify the library can be built. ", "comments": ["Can one of the admins verify this patch?", "download_dependencies.sh completed successfully, and examples are downloaded,  but run build_ios_universal_lib.sh failed: \r\n+ set -e\r\n+ make -f tensorflow/contrib/lite/Makefile TARGET=IOS IOS_ARCH=x86_64 -j 8\r\nmake: tensorflow/contrib/lite/Makefile: No such file or directory\r\nmake: *** No rule to make target `tensorflow/contrib/lite/Makefile'.  Stop.", "@llyyun For now, you need to execute this script at the git root directory. \r\nWe will improve these scripts and make it executable from anywhere. "]}, {"number": 14733, "title": "tf.contrib.ffmpeg.decode_audio console flood", "body": "When we evaluate the tensor returned by `tensorflow.contrib.ffmpeg.decode_audio()`, the ffmpeg log shows up in the terminal, leading to a flood of messages when decoding a large number of files.\r\n\r\nAsked [here](https://stackoverflow.com/questions/47361507/tf-contrib-ffmpeg-decode-audio-verbosity) as well. I could not find an easy way such as an environment variable for ffmpeg to turn off the output log, there is only a command-line argument `loglevel` but TF's `decode_audio()` does not support it.\r\n\r\nCurrently using `ffmpeg 3.4 (gcc 7.2.0)` and `tensorflow 1.4.0` on linux.\r\n\r\nIssue filed as instructed [here](https://github.com/tensorflow/tensorflow/issues/11339#issuecomment-345836714). @rryan @fredbertsch ", "comments": ["PR #14582 is pending review. I think the PR is the fix for this issue?", "Yes, indeed, the PR addresses the same problem I have. Now I see that I could not find it because it refers to `tf.contrib.ffmpeg.decode_*` instead of the complete function name."]}, {"number": 14732, "title": "Eager: Eager execution of tf.data pipelines", "body": "# System information\r\nTensorflow version:\r\n\r\n1.5.0-dev20171120\r\n\r\nPython version:\r\n\r\npython 3.6.3 |Anaconda, Inc.| (default, Nov  8 2017, 15:10:56) [MSC v.1900 64 bit (AMD64)]\r\n\r\n# Problem\r\n\r\nWhen debugging, calling the `numpy()` method on a `Tensor` object results in `AttributeError: 'Tensor' object has no attribute 'numpy' ` in certain situations.\r\n\r\n# Steps to reproduce\r\n\r\n1.  Put this code in a script:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom collections import defaultdict, Counter\r\n\r\ntfe.enable_eager_execution()\r\n\r\nclass_probs = dict(\r\n    a=0.15,\r\n    b=0.3,\r\n    c=0.8,\r\n    d=0.9,\r\n    e=0.2,\r\n    f=0.02\r\n)\r\nnum_classes = len(class_probs)\r\n\r\nclass_probs = {k: v / sum(class_probs.values()) for k, v in class_probs.items()}\r\nclass_mapping = {n: i for i, n in enumerate(class_probs.keys())}\r\n\r\nclass_names = list(class_probs.keys())\r\nclass_weights = list(class_probs.values())\r\nsampled_dataset = np.random.choice(class_names, size=1000, p=class_weights)\r\n\r\ndataset_data = defaultdict(list)\r\nfor i, d in enumerate(sampled_dataset):\r\n    dataset_data['class_name'].append(d)\r\n    dataset_data['class_id'].append(class_mapping[d])\r\n    dataset_data['data'].append(np.array([i]))\r\n    dataset_data['class_prob'].append(class_probs[d])\r\n\r\n    dataset_data['class_target_prob'].append(1 / num_classes)\r\n\r\nfor k, v in dataset_data.items():\r\n    dataset_data[k] = np.array(dataset_data[k])\r\n\r\nclass_counts = Counter(sampled_dataset)\r\n\r\noversampling_coef = 0.9\r\n\r\n\r\ndef oversample_classes(example):\r\n    \"\"\"\r\n    Returns the number of copies of given example\r\n    \"\"\"\r\n    class_prob = example['class_prob']\r\n    class_target_prob = example['class_target_prob']\r\n    prob_ratio = tf.cast(class_target_prob / class_prob, dtype=tf.float32)\r\n\r\n    prob_ratio = prob_ratio ** oversampling_coef\r\n\r\n    prob_ratio = tf.maximum(prob_ratio, 1)\r\n    # Breakpoint 1\r\n    repeat_count = tf.floor(prob_ratio)\r\n\r\n    repeat_residual = prob_ratio - repeat_count  # a number between 0-1\r\n    residual_acceptance = tf.less_equal(\r\n        tf.random_uniform([], dtype=tf.float32), repeat_residual\r\n    )\r\n\r\n    residual_acceptance = tf.cast(residual_acceptance, tf.int64)\r\n    repeat_count = tf.cast(repeat_count, dtype=tf.int64)\r\n\r\n    return repeat_count + residual_acceptance\r\n\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(dict(dataset_data))\r\n\r\ndataset = dataset.flat_map(\r\n    lambda x: tf.data.Dataset.from_tensors(x).repeat(oversample_classes(x))\r\n)\r\n\r\ni = tfe.Iterator(dataset)\r\nx = i.next()['class_id']\r\n\r\n# Breakpoint 2\r\nprint('end')\r\n```\r\n\r\n2. Insert the breakpoints in the lines following the comments Breakpoint 1 and Breakpoint 2\r\n\r\n3. Debug the script\r\n\r\n4. When breakpoint 1 is reached, evaluate the following:\r\n     `prob_ratio.numpy()`\r\n     This will result in the attribute error message.\r\n\r\n5. When breakpoint 2 is reached, evaluate the following:\r\n     `x.numpy()`\r\n     This will not result in the attribute error message.", "comments": ["@asimshankar Is there any workaround while waiting for this bug to be fixed?", "Apologies for the late response, I was out. Long story short: This behavior is intended and it isn't a bug :)\r\n\r\nThe `tf.data` API defines input processing as a dataflow graph that is executed by the TensorFlow runtime. In the code sample above, the call to `dataset.flat_map` expects a function that constructs the dataflow graph defining the transformations to be performed. As a result, you'll observe that breakpoint 1 will be triggered by the call to `dataset.flat_map` and not by the call to `i.next()`. In a program that iterates over the dataset, you'll observe that the breakpoint is hit exactly once, and not once per call to `i.next()`.\r\n\r\nExpressing the input pipeline as a dataflow graph enables efficient background input processing - allowing multiple input tensors to be processed concurrently (e.g., multiple threads for [`tf.data.Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)), and for input processing to be conducted concurrently with model computation. By doing so, input processing isn't bottlenecked on the single Python interpreter thread.\r\n\r\nThat said, a workaround would be to use something like a `tf.py_func` which will bring control to the Python interpreter. @akshayka is actively working on a `tfe.py_func` which will make this possible (at an efficiency cost of sharing the single Python interpreter thread).\r\n\r\nPlease let me know if the comment makes sense.\r\n\r\nI'm going to keep the issue open and we can close it when `tfe.py_func` is in place and we can provide you with a sample of how to use it in your input pipeline.\r\n\r\nFYI @mrry @jsimsa ", "@asimshankar Thank you for the explanation. It makes total sense to me.", "@asimshankar, on a related note. Are `lookup_ops` supported inside the `tf.data.Dataset.map` in the eager mode?\r\n\r\nI'm getting a similar Error when using `lookup_ops` in it:\r\n~~~\r\nValueError: Tried to convert 'table_handle' to a tensor and failed. \r\nError: Resource handles are not convertible to numpy.\r\n~~~\r\nin a `.map()` function:\r\n~~~python\r\ntf_ds = tf_ds.map(parse_tfrecord_dataset)\r\n~~~\r\nwhile the cause of the exception are the following lines in the `parse_tfrecord_dataset`:\r\n~~~python\r\nexample = tf.cond(tf.equal(example, tf.convert_to_tensor('NA')),\r\n                           lambda: tf.convert_to_tensor(-1, tf.int64),\r\n                           lambda: example_table.lookup(example))\r\n~~~\r\n\r\nAre there any workarounds for including lookup tables into eager data pipelines?", "@MtDersvan : Very sorry for the late reply, somehow missed this. This is related to (as you probably figured out #15212) - ops that have some implicit state stored in a `DT_RESOURCE` tensor need some work. We're looking into that.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @asimshankar, @akshayka: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @asimshankar, @akshayka: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@hsm207 \r\n\r\nHere's the promised solution; by wrapping the logic of `oversample_classes` into a `tfe.py_func`, the below code ensures that the logic is always executed eagerly. Sorry for the delay!\r\n\r\n```python\r\ndef oversample_classes(example):\r\n    \"\"\"\r\n    Returns the number of copies of given example\r\n    \"\"\"\r\n    class_prob = example['class_prob']\r\n    class_target_prob = example['class_target_prob']\r\n    def oversample_logic(class_prob, class_target_prob):\r\n      prob_ratio = tf.cast(class_target_prob / class_prob, dtype=tf.float32)\r\n\r\n      prob_ratio = prob_ratio ** oversampling_coef\r\n\r\n      prob_ratio = tf.maximum(prob_ratio, 1)\r\n      # Breakpoint 1: You can verify that prob_ratio.numpy() returns a NumPy scalar.\r\n      repeat_count = tf.floor(prob_ratio) \r\n      repeat_residual = prob_ratio - repeat_count  # a number between 0-1\r\n      residual_acceptance = tf.less_equal(\r\n          tf.random_uniform([], dtype=tf.float32), repeat_residual\r\n      )\r\n\r\n      residual_acceptance = tf.cast(residual_acceptance, tf.int64)\r\n      repeat_count = tf.cast(repeat_count, dtype=tf.int64)\r\n\r\n      return repeat_count + residual_acceptance\r\n    return tfe.py_func(func=oversample_logic,\r\n                       inp=[class_prob, class_target_prob], Tout=tf.int64)\r\n```", "@akshayka thank you for sharing the solution.", "Nagging Assignees @asimshankar, @akshayka: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 14731, "title": "Tensorflow lite - object detection - ssd-mobilenet-v1", "body": "Hi guys,\r\n\r\nI have trained a custom ssd-mobilenet-v1 (300x300 input) and currently running it via Tensorflow Android demo (Tensorflow mobile). I would love to convert this model to the lite format and possibly quantize it and run it via Tensorflow Lite to see how much has the performance improved. Currently the inference takes around 400-500ms on Google Pixel (version 1). \r\n\r\nCould you please let me know what's the best way to deploy my custom model for object detection?\r\n\r\nThank you very much in advance!\r\n\r\nMartin Peniak ", "comments": ["Answered here: https://github.com/tensorflow/tensorflow/issues/14761.", "I'm currently looking into ssd-mobilenet support, will leave this open to track.", "That's awesome, thanks!", "@andrewharp I'm more than happy to follow your lead and spend time to contribute to the ssd-mobilenet support. Or you got this and you just need someone to help testing, I can do that too. BTW, is it going to be available soon? Do you have an ETA? Thanks so very much!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Yes, is the ssd-mobilenet support there already or not yet?\r\n", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any update on this?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi, It would be very helpful if we are given an update on this work. Thanks so much!", "@yucheeling", "This is functional internally now; should have something out in the next week or two.", "This is awesome! Again, thanks so much!\r\n\r\nOh! one more thing, will this be in float and 8-bit?", "This is awesome.\r\nJust a side thought, is there any interest in providing TF Lite for other platforms like the python client  that run for server side inference?\r\n\r\nIt seems to me that the performance gains and memory footprint of TFLite would be welcome in many web based inference scenarios. Not to mention dare I say a JavaScript WebGL based implementation down the track (i bet my left arm you guys already have this in the works).\r\n\r\n\r\n", "@mpeniak Also do you mind if I ask how many images per class did you find gave you good enough results for transfer learning to new categories?", "@andrewharp   Is ssd-mobilenet on tfLite ok now?   How can I  use it ?", "@andrewharp - I'm also wondering if you have any update on where it is being released?  Any information would be greatly appreciated.", "@andrewharp Anything we can do to help? Buy you a coffee or some pizza? \ud83d\ude01Sorry i'm sure extra @ msg's aren't helping. \ud83d\ude2c", "@andrewharp any update on this?  ", "I've got a commit for porting the entire TensorFlow Android demo currently under internal review, including SSD object detection, so with any luck should be out in the next few days!", "Andrew, \r\n\r\nDo you have the code deployed for Tensorflow Lite?   I am new to this framework and want to figure out how to do object detection (both identification and bounding box info) not just identification probabilities for entire image.    Can you advise?  I am trying to understand how this alters for Tensorflow Lite  as the demo shows using the tensorflow lite class (org.tensorflow.lite.Interpreter) for classification (but, not localization). ", "SSD object detection in TF Lite is live now! See [this comment](https://github.com/tensorflow/tensorflow/issues/15633#issuecomment-377652630) for details.", "how to define own model for object_detection? like mobilenet_ssd v2\r\n\r\n", "> Hi guys,\r\n> \r\n> I have trained a custom ssd-mobilenet-v1 (300x300 input) and currently running it via Tensorflow Android demo (Tensorflow mobile). I would love to convert this model to the lite format and possibly quantize it and run it via Tensorflow Lite to see how much has the performance improved. Currently the inference takes around 400-500ms on Google Pixel (version 1).\r\n> \r\n> Could you please let me know what's the best way to deploy my custom model for object detection?\r\n> \r\n> Thank you very much in advance!\r\n> \r\n> Martin Peniak\r\n\r\nHie @mpeniak  \r\nI trained MobilenetSSDV2 lite model, the size of tflite model is 3MB which is less than the one they used for COCO SSD. There is no other problem else the inference time which is 500ms for my custom model. Could you please help in in this ?\r\n\r\nThank you\r\nRashmi Sharma"]}, {"number": 14730, "title": "Only install enum34 on Python <3.4 versions", "body": "Python 3.6 sometimes has issues with enum34 because the standard library\r\nrelies on enum features not in enum34 (see\r\nhttps://bitbucket.org/stoneleaf/enum34/issues/19/enum34-isnt-compatible-with-python-36\r\nfor more details).\r\n\r\ncc @macat", "comments": ["Can one of the admins verify this patch?", "With the discussion in #14418 we decided using this is the best way forward:\r\nhttps://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies\r\n\r\n@yifeif based on the above context, could you take a look?", "I've updated `enum34` (along with `mock` and `backports.weakref`) to use that syntax.", "@yifeif Done (sorry about the rebase -- that was an accident on my end).", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "I... have no idea why the build failed. Is there something up with the Jenkins set-up?", "@alanhdu yea it is due to some Jenkins infra issue. But this change should be good to merge :)", "Looks like this is not working as expected, see #14779 I will create a rollback for this, and we can figure out why the platform specific dependencies are not working."]}]