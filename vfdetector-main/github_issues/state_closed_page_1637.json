[{"number": 3798, "title": "More easier setup for building and using Tensorflow from source for mods and adds", "body": "Why does tensorflow use bazel instead of ./configure; make; make install?\n\nCould you please make the building from source much more streamlined?\n\nThanks!\n", "comments": ["We use bazel because it is an open source version similar to our internal build tool. As such, we can have one set of build files for our internal and external version. Generally speaking, it is the simplest way to keep our build working between open source and internal and supporting many build files would divert development time from actual core improvements. We do, however, provide many binary methods including docker. There is some limited support for Makefiles and CMake, but I think you will find that even less streamlined.\n\nIt is worth noting ./configure is only streamlined for projects with very few dependencies. Many ./configure scripts require --with-xxx= to provide paths to libraries, etc. \n\nIf there are any particular problems that are making bazel difficult to use, could you enumerate them?\n", "Automatically closing due to lack of recent activity. Please reopen when additional information becomes available.\n", "Reasons why I don't like Bazel?\n1. I haven't gotten tensorflow to compile and run from source.\n"]}, {"number": 3797, "title": "RMSProp errors with embedding_lookup", "body": "I've been trying to train an LSTM with a word embedding and got some errors that seem related to https://github.com/tensorflow/tensorflow/issues/1117\n### Environment info\n\nOperating System:\nIf installed from binary pip package, provide:\n\nLinux CPU Only\n0.10.0rc0\n### Steps to reproduce\n\nThis is the code I've been running, which should repro the issue:\n\n```\nimport tensorflow as tf\nimport numpy as np\nimport pandas\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\n#from preprocessing import VocabularyProcessor\nVocabularyProcessor = learn.preprocessing.VocabularyProcessor\n\ndef partition_length(x_train, y_train):\n  #Partition the training data by length of sequences\n  x_train_dict = {}\n  y_train_dict = {}\n  for i in range(x_train.shape[0]):\n    x = x_train[i]\n    y = y_train[i]\n\n    l = len(x)\n    if l not in x_train_dict:\n      x_train_dict[l] = []\n      y_train_dict[l] = []\n    x_train_dict[l].append(x)\n    y_train_dict[l].append(y)\n\n\n  x_train_np = {}\n  y_train_np = {}\n  for l in x_train_dict:\n    x_train_np[l] = np.asarray(x_train_dict[l])\n    y_train_np[l] = np.asarray(y_train_dict[l])\n\n  return (x_train_np, y_train_np)\n\nn_words = 0\nMAX_DOCUMENT_LENGTH = 10\nEMBEDDING_SIZE = 50\n\n# Prepare training and testing data\ndbpedia = learn.datasets.load_dataset('dbpedia')\n\nx_train = pandas.DataFrame(dbpedia.train.data)[1]\ny_train = dbpedia.train.target\nx_test = pandas.DataFrame(dbpedia.test.data)[1]\ny_test = dbpedia.test.target\n\nprint x_train.shape, y_train.shape\nprint x_test.shape, y_test.shape\n\n# Process vocabulary\nvocab_processor = VocabularyProcessor(10)\nx_train = np.array(list(vocab_processor.fit_transform(x_train)))\nx_test = np.array(list(vocab_processor.transform(x_test)))\nn_words = len(vocab_processor.vocabulary_)\nprint('Total words: %d' % n_words)\n\nprint x_train.shape, y_train.shape\n\n(x_train_part, y_train_part) = partition_length(x_train, y_train)\n\ndata = tf.placeholder(tf.int32, [None, None], name='data')\nword_vectors = learn.ops.categorical_variable(data, n_classes=n_words,\n  embedding_size=EMBEDDING_SIZE, name='words')\n\nprint('data: ', data.get_shape())\nprint('word_vectors: ', word_vectors.get_shape())\nprint('word_vectors: ', tf.shape(word_vectors))\n\ntarget = tf.placeholder(tf.int32, [None], name='target')\none_hot = tf.one_hot(target, 15, 1.0, 0.0, dtype=tf.float32)\n\nprint('target: ', target.get_shape())\nprint('one_hot: ', one_hot.get_shape())\n\nnum_hidden = 250\n\n_, state = tf.nn.dynamic_rnn(\n    tf.nn.rnn_cell.GRUCell(num_hidden),\n    word_vectors,\n    dtype=tf.float32,\n)\n\nprint('state: ', state.get_shape())\n\nin_size, out_size = (num_hidden, 15)\n\nweight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.01))\nbias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n\nprediction = tf.nn.softmax(tf.matmul(state, weight) + bias)\n#prediction = tf.contrib.losses.softmax_cross_entropy(logits, onehot_labels, weight=1.0, label_smoothing=0, scope=None)\nprediction_idx = tf.argmax(prediction, 1)\none_hot_idx = tf.argmax(one_hot, 1)\n\nprint('prediction: ', prediction.get_shape())\nprint('prediction_idx: ', prediction_idx.get_shape())\nprint('one_hot_idx: ', one_hot_idx.get_shape())\n\nmistakes = tf.not_equal(one_hot_idx, prediction_idx)\nerror =  tf.reduce_mean(tf.cast(mistakes, tf.float32))\n\ncross_entropy = -tf.reduce_sum(one_hot * tf.log(prediction))\n\nlearning_rate = 0.003\noptimizer = tf.train.RMSPropOptimizer(learning_rate)\noptimize = optimizer.minimize(cross_entropy)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\n\n\nbatch_size = 5\nfor length in x_train_part:\n  x_train = x_train_part[length]\n  y_train = y_train_part[length]\n\n  print \"Training on length \", length, \" # elements: \", x_train.shape[0]\n\n  error_pct = sess.run(error, {data: x_test, target: y_test})\n  print('Epoch {:2d} error {:3.1f}%'.format(0, 100 * error_pct))\n\n  for epoch in range(10):\n    print('Epoch {:2d}'.format(epoch+1))\n    for i in range(x_train.shape[0]/batch_size):\n      sess.run(optimize, {data: x_train[i:i+batch_size], target: y_train[i:i+batch_size]})\n\n    error_pct = sess.run(error, {data: x_test, target: y_test})\n    print('Epoch {:2d} error {:3.1f}%'.format(epoch + 1, 100 * error_pct))\n\n```\n### What have you tried?\n\nUsing any non-RMSProp Optimizer works\n### Logs or other output that would be helpful\n\nThis is the exception I get:\n\n```\nTraceback (most recent call last):\n  File \"lstm.py\", line 131, in <module>\n    sess.run(optimize, {data: x_train[i:i+batch_size], target: y_train[i:i+batch_size]})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: var and grad do not have the same shape[7664,50] [50,50]\n         [[Node: RMSProp/update_words/words_embeddings/SparseApplyRMSProp = SparseApplyRMSProp[T=DT_FLOAT, Tindices=DT_INT32, _class=[\"loc:@words/words_embeddings\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](words/words_embeddings, words/words_embeddings/RMSProp, words/words_embeddings/RMSProp_1, RMSProp/learning_rate, RMSProp/decay, RMSProp/momentum, RMSProp/epsilon, gradients/words/embedding_lookup/embedding_lookup_grad/Reshape, gradients/words/embedding_lookup/embedding_lookup_grad/Reshape_1)]]\nCaused by op u'RMSProp/update_words/words_embeddings/SparseApplyRMSProp', defined at:\n  File \"lstm.py\", line 111, in <module>\n    optimize = optimizer.minimize(cross_entropy)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 198, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 313, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py\", line 122, in _apply_sparse\n    use_locking=self._use_locking)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.py\", line 664, in sparse_apply_rms_prop\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n```\n", "comments": ["As you are asking for help using TensorFlow, please ask this question on StackOver Flow. GitHub issues are for bugs and feature enhancements only. Thank you.\n", "@aselle this could entirely be user error, but it smells like a bug due to the fact that it is an optimizer-specific issue\n", "I understand, but StackOverflow is probably the easiest way to get advice on finding out if you are doing something wrong or it is a bug.  If you are convinced it is a bug, please try to reduce it to a simpler test case.\n", "@vrv this looks like a bug. It seems you wrote SparseApplyRMSProp. It seems to work with many other optimizer. I tried AtomOptimizer, MomentumOptimizer, GradientDescentOptimizer and they all worked.\n", "I did not write SparseApplyRMSProp -- I think it was contributed by someone externally.  Let me find the commit...\n", "From https://github.com/tensorflow/tensorflow/pull/2564\n\n@suiyuan2009 do you think you can take a look?\n", "let me take a look.\n", "@kuza55 I'm sorry, check process in SparseApplyRMSProp is not correct, I'll make a quick fix.\n", "Thanks for the quick response @suiyuan2009.!\n", "Thanks @suiyuan2009 :)\n", "Hi @vrv, @suiyuan2009 - a quick hello! I am still facing this problem implementing a GRU with word embedding and RMSProp. As originally noted by @kuza55, other optimizers - Adam, Adagrad, FTRL in my case - work. With RMSProp, I get the same issue. I am running TF on AWS g2.2xlarge with Python 3.4.3. \n\nAfter seeing this thread today, I upgraded TensorFlow but that didn't fix the problem. Has the fix been uploaded?\n\nKindly advise!\n", "what version did you upgrade to? a nighlty build? or did you build from source?\n", "I am guessing its the nightly build as I didn't build from source. I did a sudo pip3 install -- upgrade\n", "try running `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n", "Its 0.10.0rc0\n", "install --upgrade doesn't fetch the nightlies, you should get the nightly\nlink off the main github page.\n\nOn Wed, Aug 24, 2016 at 3:20 PM, Jagannath Rajagopal <\nnotifications@github.com> wrote:\n\n> Its 0.10.0rc0\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3797#issuecomment-242226590,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAcTeZvz2K55P5PJx5dIEWXpDh6RDAyZks5qjMOtgaJpZM4JjvIk\n> .\n", "Ok thanks - I'll give it a try\n", "Right - that worked - thanks a lot :-)!\n", "Any reliable fix on this issue yet - made part of the release? I'm also getting the \"embedding_lookup\" exception when using RMSprop in a LSTM network. It works fine with the Adam optimizer. "]}, {"number": 3796, "title": "ERROR [PATH]/tensorflow/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command ", "body": "Hello,\n\nI get the following errors when compiling:\n\n____[110 / 2,641] Compiling external/png_archive/libpng-1.2.53/pngerror.c\nexternal/protobuf/src/google/protobuf/util/internal/field_mask_utility.cc:47:14: warning: 'google::protobuf::util::Status google::protobuf::util::converter::{anonymous}::CreatePublicError(google::protobuf::util::error::Code, const string&)' defined but not used [-Wunused-function]\n util::Status CreatePublicError(util::error::Code code,\nexternal/protobuf/src/google/protobuf/util/internal/field_mask_utility.cc:47:14: warning: 'google::protobuf::util::Status google::protobuf::util::converter::{anonymous}::CreatePublicError(google::protobuf::util::error::Code, const string&)' defined but not used [-Wunused-function]\n util::Status CreatePublicError(util::error::Code code,\nexternal/protobuf/src/google/protobuf/util/internal/field_mask_utility.cc:47:14: warning: 'google::protobuf::util::Status google::protobuf::util::converter::{anonymous}::CreatePublicError(google::protobuf::util::error::Code, const string&)' defined but not used [-Wunused-function]\n util::Status CreatePublicError(util::error::Code code,\n                         == )  // Compilation error with CHECK_EQ(NULL, x)?\n                         == )  // Compilation error with CHECK_EQ(NULL, x)?\nERROR: /home/user/Downloads/tensorflow/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \ntensorflow/stream_executor/cuda/cuda_dnn.cc:266:10: error: 'CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING' was not declared in this scope\ntensorflow/stream_executor/cuda/cuda_dnn.cc:284:10: error: 'CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING' was not declared in this scope\ntensorflow/stream_executor/cuda/cuda_dnn.cc:942:7: error: 'CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING' was not declared in this scope\ntensorflow/stream_executor/cuda/cuda_dnn.cc:947:4: error: no matching function for call to 'std::vector<long long int>::assign(<brace-enclosed initializer list>)'\ntensorflow/stream_executor/cuda/cuda_dnn.cc:958:7: error: 'CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING' was not declared in this scope\ntensorflow/stream_executor/cuda/cuda_dnn.cc:963:4: error: no matching function for call to 'std::vector<long long int>::assign(<brace-enclosed initializer list>)'\ntensorflow/stream_executor/cuda/cuda_dnn.cc:166:39: error: too many arguments to function\n\nComplete log of build: http://pastebin.com/NzpiwmGm\n\nThe compilation command that I am using is:\nbazel build --verbose_failures --genrule_strategy=standalone --spawn_strategy=standalone -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer >& build.log &\n\nMy systems information is as follows\n### Environment info\n\nOperating System:\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.1 LTS\nRelease:    16.04\nCodename:   xenial\n\nInstalled version of CUDA and cuDNN:\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0\n\nIf installed from source, provide \n1. The commit hash: bf31051225ce53c1c88fd45ee117a49645153770\n2. The output of `bazel version`\n   Build time: Thu Jan 01 00:00:00 1970 (0)\n   Build timestamp: Thu Jan 01 00:00:00 1970 (0)\n   Build timestamp as int: 0\n\nI installed cudatoolkit, and cudann following this instructions:\n\n> sudoapt-get install nvidia-cuda-toolkit\n> sudo apt-get install nvidia-cuda-361-updates\n> sudo apt-get install nvidia-nsight\n> sudo apt-get install nvidia-profiler\n> sudo apt-get install libcupti-dev zlib1g-dev\n> # Put symlinks in /usr/local/cuda\n> \n> sudo mkdir /usr/local/cuda\n> cd /usr/local/cuda\n> sudo ln -s  /usr/lib/x86_64-linux-gnu/ lib64\n> sudo ln -s  /usr/include/ include\n> sudo ln -s  /usr/bin/ bin\n> sudo ln -s  /usr/lib/x86_64-linux-gnu/ nvvm\n> sudo mkdir -p extras/CUPTI\n> cd extras/CUPTI\n> sudo ln -s  /usr/lib/x86_64-linux-gnu/ lib64\n> sudo ln -s  /usr/include/ include\n> # Install cudann\n> # http://askubuntu.com/questions/767269/how-can-i-install-cudnn-on-ubuntu-16-04\n> # Download cudann as detailed above and extract\n> \n> cd ~/Downloads/cuda\n> sudo cp include/cudnn.h /usr/include\n> sudo cp lib64/libcudnn\\* /usr/lib/x86_64-linux-gnu/\n> sudo chmod a+r /usr/lib/x86_64-linux-gnu/libcudnn*\n> # ... Install TensorFlow from source ...\n\ninstruction found at  https://devtalk.nvidia.com/default/topic/936212/cuda-setup-and-installation/tensorflow-cannot-find-cudnn-ubuntu-16-04-cuda7-5-/post/4880549/#4880549\n\nI ecounter the problems described in issue  #1066. I solved it using the solution described by @chrisburr: @drufat I ran into a similar issue when compiling on Arch and found the solution in #1346. I'm unsure if -D__STRICT_ANSI__ is actually required but the following patch worked for me:\n\ndiff --git a/third_party/gpus/crosstool/CROSSTOOL b/third_party/gpus/crosstool/CROSSTOOL\nindex dfde7cd..15fa9fd 100644\n--- a/third_party/gpus/crosstool/CROSSTOOL\n+++ b/third_party/gpus/crosstool/CROSSTOOL\n@@ -46,6 +46,9 @@ toolchain {\n   # Use \"-std=c++11\" for nvcc. For consistency, force both the host compiler\n   # and the device compiler to use \"-std=c++11\".\n   cxx_flag: \"-std=c++11\"\n-  cxx_flag: \"-D_MWAITXINTRIN_H_INCLUDED\"\n-  cxx_flag: \"-D_FORCE_INLINES\"\n-  cxx_flag: \"-D__STRICT_ANSI__\"\n  linker_flag: \"-lstdc++\"\n  linker_flag: \"-B/usr/bin/\"\n\nNext, I got the issue #698.\n\nSolved it by compiling with --genrule_strategy=standalone --spawn_strategy=standalone, as suggested by @damienmg \n", "comments": ["I think you are using too old a version of cudnn, please upgrade to v4 or v5\n", "Closing as I think the other bugs are duplicates of other open bugs. Try searching issues!\n"]}, {"number": 3795, "title": "Unable to connect jupyter notebook within tensorflow container in AWS", "body": "Currently we found that the [tensorflow/tensorflow](https://hub.docker.com/r/tensorflow/tensorflow/) does not work in AWS. I can access the port and open the jupyter website but could not `connect to the kernel`.\n\n![image](https://cloud.githubusercontent.com/assets/2715000/17644038/a3160972-61ae-11e6-8ef6-1bbca7804cc2.png)\n\n![image](https://cloud.githubusercontent.com/assets/2715000/17644040/a9745ed6-61ae-11e6-8cf0-c9b772cc9f0e.png)\n\nThe logs of that container look like these.\n\n```\nroot# docker run -p 30100:8888 tensorflow/tensorflow\n[I 15:09:48.070 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\n/usr/local/lib/python2.7/dist-packages/widgetsnbextension/__init__.py:30: UserWarning: To use the jupyter-js-widgets nbextension, you'll need to update\n    the Jupyter notebook to version 4.2 or later.\n  the Jupyter notebook to version 4.2 or later.\"\"\")\n[W 15:09:48.114 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.\n[W 15:09:48.114 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using authentication. This is highly insecure and not recommended.\n[I 15:09:48.120 NotebookApp] Serving notebooks from local directory: /notebooks\n[I 15:09:48.120 NotebookApp] 0 active kernels\n[I 15:09:48.120 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:8888/\n[I 15:09:48.120 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[I 15:09:59.446 NotebookApp] 302 GET / (10.2.201.72) 0.79ms\n[I 15:10:25.276 NotebookApp] Writing notebook-signing key to /root/.local/share/jupyter/notebook_secret\n[W 15:10:25.283 NotebookApp] Notebook 3_mnist_from_scratch.ipynb is not trusted\n[W 15:10:25.611 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20160813150948 (10.2.201.72) 7.44ms referer=http://10.69.1.246:30100/notebooks/3_mnist_from_scratch.ipynb\n[I 15:10:28.037 NotebookApp] Kernel started: b2421ec2-2411-4b3a-9d49-83f4ec41a3ed\n```\n\nIt doesn't work even if I try with `--net=host`. But it works for my local Linux server. The logs look like similar and I'm not sure if it is related to AWS's network or jupyter's configuration.\n\n```\n\u279c  sudo docker run -p 30100:8888 tensorflow/tensorflow\n[I 15:16:47.063 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\n/usr/local/lib/python2.7/dist-packages/widgetsnbextension/__init__.py:30: UserWarning: To use the jupyter-js-widgets nbextension, you'll need to update\n    the Jupyter notebook to version 4.2 or later.\n  the Jupyter notebook to version 4.2 or later.\"\"\")\n[W 15:16:47.100 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.\n[W 15:16:47.100 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using authentication. This is highly insecure and not recommended.\n[I 15:16:47.106 NotebookApp] Serving notebooks from local directory: /notebooks\n[I 15:16:47.106 NotebookApp] 0 active kernels\n[I 15:16:47.106 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:8888/\n[I 15:16:47.106 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[I 15:17:25.033 NotebookApp] 302 GET / (10.235.212.35) 1.53ms\n[I 15:17:35.355 NotebookApp] Writing notebook-signing key to /root/.local/share/jupyter/notebook_secret\n[W 15:17:35.361 NotebookApp] Notebook 3_mnist_from_scratch.ipynb is not trusted\n[W 15:17:35.408 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20160813151647 (10.235.212.35) 5.66ms referer=http://tomi:30100/notebooks/3_mnist_from_scratch.ipynb\n[I 15:17:37.307 NotebookApp] Kernel started: 952cac82-1a79-4446-b73e-728d5505d8a5\n```\n### Environment info\n\nOperating System: AWS ubuntu 14.04\n\nTensorFlow 0.9\n### Steps to reproduce\n1. `docker run -p 30100:8888 tensorflow/tensorflow` in AWS instances\n2. Go to `$ip:30100` in the browser\n3. Click any notebook\n### What have you tried?\n1. Trying in local Linux server rather than AWS instances should work.\n", "comments": ["Same problem even if I deploy Kubernetes cluster with`SkyDNS`.\n\n![image](https://cloud.githubusercontent.com/assets/2715000/17655745/7c1f10e8-62e4-11e6-9a95-ecdc40014d19.png)\n", "If you run it on your Linux local server, can you open it from another machine?\n", "This may be the issue of [jupyter/notebook](https://github.com/jupyter/notebook) because I have the same issue when running pure notebook container in AWS.\n\nI will start another issue to tract that.\n", "By default, a notebook server runs locally at 127.0.0.1:8888 and is accessible only from localhost.\nhttp://jupyter-notebook.readthedocs.io/en/latest/public_server.html\n", "Thanks @yaroslavvb . I can access the container from other machine in my local server.\n\nTensorFlow has customized the [jupyter configure file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/jupyter_notebook_config.py) and it should work for other clients. And I can access the index page which means the ip and port are correct.\n\nAnyway, the pure jupyter notebook has the same issue so I will close this and please trace that in https://github.com/jupyter/notebook/issues/1681 if you're interested.\n"]}, {"number": 3794, "title": "Unable to pull 0.10.0 Docker tag for gcr.io/tensorflow/tensorflow", "body": "Looks like there is no Docker tag `0.10.0`:\n\n```\n\u279c  ~ docker pull gcr.io/tensorflow/tensorflow:0.10.0\nPulling repository gcr.io/tensorflow/tensorflow\nTag 0.10.0 not found in repository gcr.io/tensorflow/tensorflow\n```\n\nWhile for example `docker pull gcr.io/tensorflow/tensorflow:0.7.0` (and `0.7.0-gpu`, `0.7.0-devel`, `0.7.0-devel-gpu`) works great.\n", "comments": ["Looks like `latest` is set to `0.8.0`:\n\n```\n\u279c  ~ docker images | grep aeff5a9860a3\ngcr.io/tensorflow/tensorflow                       0.8.0               aeff5a9860a3        3 months ago        714.2 MB\ngcr.io/tensorflow/tensorflow                       latest              aeff5a9860a3        3 months ago        714.2 MB\n```\n", "Downloaded all tags by `docker pull -a gcr.io/tensorflow/tensorflow` which proves that `0.10.0` or `r0.10rc0` is missing:\n\n```\ngcr.io/tensorflow/tensorflow                       r0.9rc0-gpu          51890333fd6d        9 weeks ago         2.083 GB\ngcr.io/tensorflow/tensorflow                       r0.9rc0              24fb64a91ba8        9 weeks ago         764.6 MB\ngcr.io/tensorflow/tensorflow                       r0.9rc0-devel-gpu    bff7093a7715        9 weeks ago         4.09 GB\ngcr.io/tensorflow/tensorflow                       r0.9rc0-devel        8298a524d293        9 weeks ago         2.517 GB\ngcr.io/tensorflow/tensorflow                       0.8.0-devel-gpu      9e12b89c50bb        3 months ago        3.883 GB\ngcr.io/tensorflow/tensorflow                       latest-devel-gpu     9e12b89c50bb        3 months ago        3.883 GB\ngcr.io/tensorflow/tensorflow                       0.8.0-devel          c3efccc5f94f        3 months ago        2.38 GB\ngcr.io/tensorflow/tensorflow                       latest-devel         c3efccc5f94f        3 months ago        2.38 GB\ngcr.io/tensorflow/tensorflow                       0.8.0-gpu            7f09e75cdc12        3 months ago        1.289 GB\ngcr.io/tensorflow/tensorflow                       latest-gpu           7f09e75cdc12        3 months ago        1.289 GB\ngcr.io/tensorflow/tensorflow                       0.8.0                aeff5a9860a3        3 months ago        714.2 MB\ngcr.io/tensorflow/tensorflow                       latest               aeff5a9860a3        3 months ago        714.2 MB\ngcr.io/tensorflow/tensorflow                       0.8.0rc0-devel-gpu   1f69ab8a5941        4 months ago        3.836 GB\ngcr.io/tensorflow/tensorflow                       0.8.0rc0-gpu         16bcc675cd5d        4 months ago        1.28 GB\ngcr.io/tensorflow/tensorflow                       0.8.0rc0-devel       0fd7d511ef5c        4 months ago        2.332 GB\ngcr.io/tensorflow/tensorflow                       0.8.0rc0             efac421416df        4 months ago        704.9 MB\ngcr.io/tensorflow/tensorflow                       0.7.1-devel          b24b6540547f        5 months ago        2.043 GB\ngcr.io/tensorflow/tensorflow                       0.7.1-gpu            68c30c286eda        5 months ago        1.199 GB\ngcr.io/tensorflow/tensorflow                       0.7.1                7e0be98eae06        5 months ago        669.6 MB\ngcr.io/tensorflow/tensorflow                       0.7.0-devel-gpu      a48812e48f9f        5 months ago        3.48 GB\ngcr.io/tensorflow/tensorflow                       0.7.0-devel          b5b028bd8bbd        5 months ago        2.048 GB\ngcr.io/tensorflow/tensorflow                       0.7.0-gpu            55943cfec113        5 months ago        1.132 GB\ngcr.io/tensorflow/tensorflow                       0.7.0                002f882bde74        5 months ago        674.3 MB\ngcr.io/tensorflow/tensorflow                       0.6.0-gpu            6765eddea9aa        8 months ago        1.095 GB\ngcr.io/tensorflow/tensorflow                       0.6.0                a78f841c310f        8 months ago        653.1 MB\ngcr.io/tensorflow/tensorflow                       0.6.0-devel-gpu      7c6138126d3a        8 months ago        3.406 GB\ngcr.io/tensorflow/tensorflow                       0.5.0-devel          32cde1dd0251        9 months ago        2.284 GB\ngcr.io/tensorflow/tensorflow                       0.5.0                17d8e0240de2        9 months ago        652.6 MB\n```\n", "@caisq, feel free to reassign. Thanks!\n", "I pull tensorflow/tensorflow:0.10.0rc0 from [dockerhub](https://hub.docker.com/r/tensorflow/tensorflow/) is ok.\n", "Currently able to successfully run `docker pull gcr.io/tensorflow/tensorflow:0.10.0`.\n"]}, {"number": 3793, "title": "will tensorflow support building and running model on tensorboard", "body": "It may be too far ahead, but It'll be great to support building and running model on tensorboard directly. Writing code for deep learning tasks is interesting, but It costs much time even if you just need to copy and paste from other's code or modify on other's code. Since Deep learning programs share much similarity to each other, and tensorflow's internal mechanism is suitable for visual programming, it's not hard to do this. It'll benefit many kinds of users. Researchers and engineers don't need to spend a lot of time to code a model already exists or design a new model, a person not skilled at coding can do cool things just like a skilled programer. \n", "comments": ["We've discussed this a lot internally. At the moment, it is not on the roadmap. Of course, an open source contributor is welcome to develop it and send a pull request. :)\n"]}, {"number": 3792, "title": "Fix Eigen-related compilation error when using -mavx2.", "body": "Since updating to use a newer version of Eigen [1], compilation with\n-mavx2 causes an error caused by an undefined type. Eigen removed\nthe type `scalar_multipl2_op` [2] so `scalar_product_op` should now\nbe used.\n\nThis commit replaces uses of `scalar_multiple2_op` similarly to how\n[2] did this.\n\n[1] See chage 127264575, commit 10211a6c.\n[2] See https://bitbucket.org/eigen/eigen/commits/03556a17eb548275bc9404d7cda8303ff6ca5c13\n", "comments": ["Can one of the admins verify this patch?\n", "It would be good if someone familiar with Eigen could review this. This fixes #3349.\n", "Jenkins, test this please.\n", "@benoitsteiner there seems to be problem with Jenkins infrastructure, which is also happening to other PRs. \n", "Does normal testing ever set -mavx2? \n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please\n\nI don't think all of our test machines have AVX2, so yes, we don't test with all possible architectures, sadly.\n"]}, {"number": 3791, "title": "Python 2 pip install fails with CERTIFICATE_VERIFY_FAILED - fixed by using http instead of https", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed. \n\nThe install is failing from CentOS 7 with:\n\n```\nStep 10 : RUN pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n ---> Running in 882f8c6e9398\nCollecting tensorflow==0.10.0rc0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\nException:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/pip/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"/usr/lib/python2.7/site-packages/pip/commands/install.py\", line 299, in run\n    requirement_set.prepare_files(finder)\n  File \"/usr/lib/python2.7/site-packages/pip/req/req_set.py\", line 370, in prepare_files\n    ignore_dependencies=self.ignore_dependencies))\n  File \"/usr/lib/python2.7/site-packages/pip/req/req_set.py\", line 587, in _prepare_file\n    session=self.session, hashes=hashes)\n  File \"/usr/lib/python2.7/site-packages/pip/download.py\", line 810, in unpack_url\n    hashes=hashes\n  File \"/usr/lib/python2.7/site-packages/pip/download.py\", line 649, in unpack_http_url\n    hashes)\n  File \"/usr/lib/python2.7/site-packages/pip/download.py\", line 842, in _download_http_url\n    stream=True,\n  File \"/usr/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 487, in get\n    return self.request('GET', url, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/pip/download.py\", line 378, in request\n    return super(PipSession, self).request(method, url, *args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 475, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 585, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py\", line 46, in send\n    resp = super(CacheControlAdapter, self).send(request, **kw)\n  File \"/usr/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py\", line 477, in send\n    raise SSLError(e, request=request)\nSSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:765)\n```\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n\nTensorflow was not installed.\n### Steps to reproduce\n1. \n   `pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl`\n### What have you tried?\n1. The fix for me was to use **http://** instead of **https://**\n\n```\nStep 10 : RUN pip install --upgrade http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n ---> Running in 064f19cad960\nCollecting tensorflow==0.10.0rc0 from http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n  Downloading http://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl (33.3MB)\nRequirement already up-to-date: six>=1.10.0 in /usr/lib/python2.7/site-packages (from tensorflow==0.10.0rc0)\nCollecting protobuf==3.0.0b2 (from tensorflow==0.10.0rc0)\n  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB)\nCollecting wheel (from tensorflow==0.10.0rc0)\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\nCollecting mock>=2.0.0 (from tensorflow==0.10.0rc0)\n  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\nRequirement already up-to-date: numpy>=1.8.2 in /usr/lib64/python2.7/site-packages (from tensorflow==0.10.0rc0)\nRequirement already up-to-date: setuptools in /usr/lib/python2.7/site-packages (from protobuf==3.0.0b2->tensorflow==0.10.0rc0)\nRequirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in /usr/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==0.10.0rc0)\nRequirement already up-to-date: pbr>=0.11 in /usr/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==0.10.0rc0)\nInstalling collected packages: protobuf, wheel, mock, tensorflow\nSuccessfully installed mock-2.0.0 protobuf-3.0.0b2 tensorflow-0.10.0rc0 wheel-0.29.0\n ---> a374951fbf58\n```\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Some more debugging help:\n\n```\n$ openssl s_client -connect storage.googleapis.com:443\nCONNECTED(00000003)\n139978079770488:error:140790E5:SSL routines:SSL23_WRITE:ssl handshake failure:s23_lib.c:184:\n---\nno peer certificate available\n---\nNo client certificate CA names sent\n---\nSSL handshake has read 0 bytes and written 207 bytes\n---\nNew, (NONE), Cipher is (NONE)\nSecure Renegotiation IS NOT supported\nCompression: NONE\nExpansion: NONE\n---\n```\n", "This is not a TensorFlow problem, it is a problem with pip's configuration (possibly specifically to CentOS). you can fix it by telling it to use the openssl certificate authorites. Googling the error yields:\n\nhttp://stackoverflow.com/questions/19377045/pip-cert-failed-but-curl-works\nhttp://stackoverflow.com/questions/32772895/python-pip-install-error-ssl-certificate-verify-failed\n\nI wouldn't recommend using http instead of https as then pip will not authenticate the package source.\n", "For those looking in the future it's not a pip problem. \n\nIt was the cert on storage.googleapis.com failing. \n\nLooks good now: \n\n```\n$ openssl s_client -connect storage.googleapis.com:443\nCONNECTED(00000003)\ndepth=3 C = US, O = Equifax, OU = Equifax Secure Certificate Authority\nverify return:1\ndepth=2 C = US, O = GeoTrust Inc., CN = GeoTrust Global CA\nverify return:1\ndepth=1 C = US, O = Google Inc, CN = Google Internet Authority G2\nverify return:1\ndepth=0 C = US, ST = California, L = Mountain View, O = Google Inc, CN = *.storage.googleapis.com\nverify return:1\n---\nCertificate chain\n 0 s:/C=US/ST=California/L=Mountain View/O=Google Inc/CN=*.storage.googleapis.com\n   i:/C=US/O=Google Inc/CN=Google Internet Authority G2\n 1 s:/C=US/O=Google Inc/CN=Google Internet Authority G2\n   i:/C=US/O=GeoTrust Inc./CN=GeoTrust Global CA\n 2 s:/C=US/O=GeoTrust Inc./CN=GeoTrust Global CA\n   i:/C=US/O=Equifax/OU=Equifax Secure Certificate Authority\n---\nServer certificate\n-----BEGIN CERTIFICATE-----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\ni0onAvuz\n-----END CERTIFICATE-----\nsubject=/C=US/ST=California/L=Mountain View/O=Google Inc/CN=*.storage.googleapis.com\nissuer=/C=US/O=Google Inc/CN=Google Internet Authority G2\n---\nNo client certificate CA names sent\nServer Temp Key: ECDH, prime256v1, 256 bits\n---\nSSL handshake has read 4113 bytes and written 333 bytes\n---\nNew, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES128-GCM-SHA256\nServer public key is 2048 bit\nSecure Renegotiation IS supported\nCompression: NONE\nExpansion: NONE\nSSL-Session:\n    Protocol  : TLSv1.2\n    Cipher    : ECDHE-RSA-AES128-GCM-SHA256\n    Session-ID: 6B5BF03E3F6362912A14AFA0C5ABF9D9DA9E93F49937ACDB715FD3576FAD0AF4\n    Session-ID-ctx: \n    Master-Key: B023D51CECAA588515119296C4C564673844E312EAAD5CD93D13867E5F2C29AC7E536A056C91038C0918CCB622F5196B\n    Key-Arg   : None\n    Krb5 Principal: None\n    PSK identity: None\n    PSK identity hint: None\n    TLS session ticket lifetime hint: 100800 (seconds)\n    TLS session ticket:\n    0000 - c2 77 a0 2d ce 0a c3 c0-9a 5e 7a 43 69 32 a7 74   .w.-.....^zCi2.t\n    0010 - 9e f6 b3 2c aa ba 02 bf-e1 af 5b 89 34 c4 7d 2b   ...,......[.4.}+\n    0020 - ca 90 28 b7 6a 50 04 83-fb d9 14 6a 8a a6 b6 e8   ..(.jP.....j....\n    0030 - 85 53 be 45 4d 64 fa 74-d4 5f 72 cc 5b e4 31 70   .S.EMd.t._r.[.1p\n    0040 - 0d 9d bd df 1a c1 20 51-ad b5 34 3f 6c 94 67 c1   ...... Q..4?l.g.\n    0050 - f7 7f f4 fa eb d2 e4 dc-6a 88 ab 3f c7 f4 95 37   ........j..?...7\n    0060 - b4 73 47 2f ca 5d b9 e2-9a ef a2 7e 31 9d b5 3d   .sG/.].....~1..=\n    0070 - a7 8c c1 a4 e2 ea 68 6a-94 a7 f3 30 86 a7 67 b7   ......hj...0..g.\n    0080 - 6a 02 22 da 4d 4b 26 75-62 80 b7 a5 50 be 94 db   j.\".MK&ub...P...\n    0090 - fe 22 b8 68 a3 16 c0 a9-f9 17 c4 50 92 6e b2 04   .\".h.......P.n..\n    00a0 - c3 f7 57 df                                       ..W.\n\n    Start Time: 1471559044\n    Timeout   : 300 (sec)\n    Verify return code: 0 (ok)\n---\n```\n", "Thanks for the post clarifying the issue. Sorry for misdirecting you!  Glad you got it to work.\n"]}, {"number": 3790, "title": "Issue: install ", "body": "I am trying to install tensorflow on Fedora 23. I use python 3.4. So, I follow the guide:\n$export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp34-cp34m-linux_x86_64.whl\n$pip3 install --upgrade $TF_BINARY_URL\n\nBut it doesn't work. Here is the log:\n\nCollecting tensorflow==0.10.0rc0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp34-cp34m-linux_x86_64.whl\n  Using cached https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp34-cp34m-linux_x86_64.whl\nCollecting numpy>=1.8.2 (from tensorflow==0.10.0rc0)\n  Using cached numpy-1.11.1.zip\nRequirement already up-to-date: wheel>=0.26 in /usr/lib/python3.4/site-packages (from tensorflow==0.10.0rc0)\nRequirement already up-to-date: six>=1.10.0 in /usr/lib/python3.4/site-packages (from tensorflow==0.10.0rc0)\nRequirement already up-to-date: protobuf==3.0.0b2 in /usr/lib/python3.4/site-packages (from tensorflow==0.10.0rc0)\nCollecting setuptools (from protobuf==3.0.0b2->tensorflow==0.10.0rc0)\n  Using cached setuptools-25.2.0-py2.py3-none-any.whl\nBuilding wheels for collected packages: numpy\n  Running setup.py bdist_wheel for numpy\n  Complete output from command /usr/bin/python3 -c \"import setuptools;**file**='/tmp/pip-build-vtpn0vsk/numpy/setup.py';exec(compile(open(**file**).read().replace('\\r\\n', '\\n'), **file**, 'exec'))\" bdist_wheel -d /tmp/tmpo1ev1azppip-wheel-:\n  blas_opt_info:\n  blas_mkl_info:\n    libraries mkl,vml,guide not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']\n    NOT AVAILABLE\n\n  openblas_info:\n    libraries openblas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']\n    NOT AVAILABLE\n\n  atlas_3_10_blas_threads_info:\n  Setting PTATLAS=ATLAS\n    libraries tatlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']\n    NOT AVAILABLE\n\n  atlas_3_10_blas_info:\n    libraries satlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']\n    NOT AVAILABLE\n\n  atlas_blas_threads_info:\n  Setting PTATLAS=ATLAS\n    libraries ptf77blas,ptcblas,atlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']\n    NOT AVAILABLE\n\n  atlas_blas_info:\n    libraries f77blas,cblas,atlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']\n    NOT AVAILABLE\n\n  blas_info:\n  customize UnixCCompiler\n  C compiler: gcc -pthread -Wno-unused-result -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC\n\n  creating /tmp/tmpwhgndlax/tmp\n  creating /tmp/tmpwhgndlax/tmp/tmpwhgndlax\n  compile options: '-I/usr/local/include -I/usr/include -c'\n  gcc: /tmp/tmpwhgndlax/source.c\n  gcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory\n  gcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory\n    libraries blas not found in ['', '']\n  Runtime library blas was not found. Ignoring\n    FOUND:\n      libraries = ['blas', 'blas']\n      library_dirs = ['/usr/lib64']\n\n```\nFOUND:\n  define_macros = [('NO_ATLAS_INFO', 1)]\n  libraries = ['blas', 'blas']\n  library_dirs = ['/usr/lib64']\n```\n\n  non-existing path in 'numpy/distutils': 'site.cfg'\n  F2PY Version 2\n  lapack_opt_info:\n  openblas_lapack_info:\n    libraries openblas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']\n    NOT AVAILABLE\n\n  lapack_mkl_info:\n  mkl_info:\n    libraries mkl,vml,guide not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']\n    NOT AVAILABLE\n\n```\nNOT AVAILABLE\n```\n\n  atlas_3_10_threads_info:\n  Setting PTATLAS=ATLAS\n    libraries tatlas,tatlas not found in /usr/local/lib64\n    libraries lapack_atlas not found in /usr/local/lib64\n    libraries tatlas,tatlas not found in /usr/local/lib\n    libraries lapack_atlas not found in /usr/local/lib\n    libraries tatlas,tatlas not found in /usr/lib64/sse2\n    libraries lapack_atlas not found in /usr/lib64/sse2\n    libraries tatlas,tatlas not found in /usr/lib64\n    libraries lapack_atlas not found in /usr/lib64\n    libraries tatlas,tatlas not found in /usr/lib/sse2\n    libraries lapack_atlas not found in /usr/lib/sse2\n    libraries tatlas,tatlas not found in /usr/lib\n    libraries lapack_atlas not found in /usr/lib\n    libraries tatlas,tatlas not found in /usr/lib/sse2\n    libraries lapack_atlas not found in /usr/lib/sse2\n    libraries tatlas,tatlas not found in /usr/lib/\n    libraries lapack_atlas not found in /usr/lib/\n  <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>\n    NOT AVAILABLE\n\n  atlas_3_10_info:\n    libraries satlas,satlas not found in /usr/local/lib64\n    libraries lapack_atlas not found in /usr/local/lib64\n    libraries satlas,satlas not found in /usr/local/lib\n    libraries lapack_atlas not found in /usr/local/lib\n    libraries satlas,satlas not found in /usr/lib64/sse2\n    libraries lapack_atlas not found in /usr/lib64/sse2\n    libraries satlas,satlas not found in /usr/lib64\n    libraries lapack_atlas not found in /usr/lib64\n    libraries satlas,satlas not found in /usr/lib/sse2\n    libraries lapack_atlas not found in /usr/lib/sse2\n    libraries satlas,satlas not found in /usr/lib\n    libraries lapack_atlas not found in /usr/lib\n    libraries satlas,satlas not found in /usr/lib/sse2\n    libraries lapack_atlas not found in /usr/lib/sse2\n    libraries satlas,satlas not found in /usr/lib/\n    libraries lapack_atlas not found in /usr/lib/\n  <class 'numpy.distutils.system_info.atlas_3_10_info'>\n    NOT AVAILABLE\n\n  atlas_threads_info:\n  Setting PTATLAS=ATLAS\n    libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib64\n    libraries lapack_atlas not found in /usr/local/lib64\n    libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib\n    libraries lapack_atlas not found in /usr/local/lib\n    libraries ptf77blas,ptcblas,atlas not found in /usr/lib64/sse2\n    libraries lapack_atlas not found in /usr/lib64/sse2\n    libraries ptf77blas,ptcblas,atlas not found in /usr/lib64\n    libraries lapack_atlas not found in /usr/lib64\n    libraries ptf77blas,ptcblas,atlas not found in /usr/lib/sse2\n    libraries lapack_atlas not found in /usr/lib/sse2\n    libraries ptf77blas,ptcblas,atlas not found in /usr/lib\n    libraries lapack_atlas not found in /usr/lib\n    libraries ptf77blas,ptcblas,atlas not found in /usr/lib/sse2\n    libraries lapack_atlas not found in /usr/lib/sse2\n    libraries ptf77blas,ptcblas,atlas not found in /usr/lib/\n    libraries lapack_atlas not found in /usr/lib/\n  <class 'numpy.distutils.system_info.atlas_threads_info'>\n    NOT AVAILABLE\n\n  atlas_info:\n    libraries f77blas,cblas,atlas not found in /usr/local/lib64\n    libraries lapack_atlas not found in /usr/local/lib64\n    libraries f77blas,cblas,atlas not found in /usr/local/lib\n    libraries lapack_atlas not found in /usr/local/lib\n    libraries f77blas,cblas,atlas not found in /usr/lib64/sse2\n    libraries lapack_atlas not found in /usr/lib64/sse2\n    libraries f77blas,cblas,atlas not found in /usr/lib64\n    libraries lapack_atlas not found in /usr/lib64\n    libraries f77blas,cblas,atlas not found in /usr/lib/sse2\n    libraries lapack_atlas not found in /usr/lib/sse2\n    libraries f77blas,cblas,atlas not found in /usr/lib\n    libraries lapack_atlas not found in /usr/lib\n    libraries f77blas,cblas,atlas not found in /usr/lib/sse2\n    libraries lapack_atlas not found in /usr/lib/sse2\n    libraries f77blas,cblas,atlas not found in /usr/lib/\n    libraries lapack_atlas not found in /usr/lib/\n  <class 'numpy.distutils.system_info.atlas_info'>\n    NOT AVAILABLE\n\n  lapack_info:\n    libraries lapack not found in ['', '']\n  Runtime library lapack was not found. Ignoring\n    FOUND:\n      libraries = ['lapack', 'lapack']\n      library_dirs = ['/usr/lib64']\n      language = f77\n\n```\nFOUND:\n  define_macros = [('NO_ATLAS_INFO', 1)]\n  libraries = ['lapack', 'lapack', 'blas', 'blas']\n  library_dirs = ['/usr/lib64']\n  language = f77\n```\n\n  running bdist_wheel\n  running build\n  running config_cc\n  unifing config_cc, config, build_clib, build_ext, build commands --compiler options\n  running config_fc\n  unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\n  running build_src\n  build_src\n  building py_modules sources\n  creating build\n  creating build/src.linux-x86_64-3.4\n  creating build/src.linux-x86_64-3.4/numpy\n  creating build/src.linux-x86_64-3.4/numpy/distutils\n  building library \"npymath\" sources\n  customize Gnu95FCompiler\n  Found executable /usr/bin/gfortran\n  customize Gnu95FCompiler\n  customize Gnu95FCompiler using config\n  C compiler: gcc -pthread -Wno-unused-result -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC\n\n  compile options: '-Inumpy/core/src/private -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.4m -c'\n  gcc: _configtest.c\n  gcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory\n  gcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory\n  failure.\n  removing: _configtest.c _configtest.o\n  Running from numpy source directory.\n  /tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/system_info.py:1646: UserWarning:\n      Atlas (http://math-atlas.sourceforge.net/) libraries not found.\n      Directories to search for the libraries can be specified in the\n      numpy/distutils/site.cfg file (section [atlas]) or by setting\n      the ATLAS environment variable.\n    warnings.warn(AtlasNotFoundError.__doc__)\n  /tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/system_info.py:1548: UserWarning:\n      Atlas (http://math-atlas.sourceforge.net/) libraries not found.\n      Directories to search for the libraries can be specified in the\n      numpy/distutils/site.cfg file (section [atlas]) or by setting\n      the ATLAS environment variable.\n    warnings.warn(AtlasNotFoundError.**doc**)\n  /usr/lib64/python3.4/distutils/dist.py:260: UserWarning: Unknown distribution option: 'define_macros'\n    warnings.warn(msg)\n  Traceback (most recent call last):\n    File \"<string>\", line 1, in <module>\n    File \"/tmp/pip-build-vtpn0vsk/numpy/setup.py\", line 386, in <module>\n      setup_package()\n    File \"/tmp/pip-build-vtpn0vsk/numpy/setup.py\", line 378, in setup_package\n      setup(**metadata)\n    File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/core.py\", line 169, in setup\n      return old_setup(**new_attr)\n    File \"/usr/lib64/python3.4/distutils/core.py\", line 148, in setup\n      dist.run_commands()\n    File \"/usr/lib64/python3.4/distutils/dist.py\", line 955, in run_commands\n      self.run_command(cmd)\n    File \"/usr/lib64/python3.4/distutils/dist.py\", line 974, in run_command\n      cmd_obj.run()\n    File \"/usr/lib/python3.4/site-packages/wheel/bdist_wheel.py\", line 179, in run\n      self.run_command('build')\n    File \"/usr/lib64/python3.4/distutils/cmd.py\", line 313, in run_command\n      self.distribution.run_command(command)\n    File \"/usr/lib64/python3.4/distutils/dist.py\", line 974, in run_command\n      cmd_obj.run()\n    File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/build.py\", line 47, in run\n      old_build.run(self)\n    File \"/usr/lib64/python3.4/distutils/command/build.py\", line 126, in run\n      self.run_command(cmd_name)\n    File \"/usr/lib64/python3.4/distutils/cmd.py\", line 313, in run_command\n      self.distribution.run_command(command)\n    File \"/usr/lib64/python3.4/distutils/dist.py\", line 974, in run_command\n      cmd_obj.run()\n    File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/build_src.py\", line 147, in run\n      self.build_sources()\n    File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/build_src.py\", line 158, in build_sources\n      self.build_library_sources(*libname_info)\n    File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/build_src.py\", line 293, in build_library_sources\n      sources = self.generate_sources(sources, (lib_name, build_info))\n    File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/build_src.py\", line 376, in generate_sources\n      source = func(extension, build_dir)\n    File \"numpy/core/setup.py\", line 654, in get_mathlib_info\n      raise RuntimeError(\"Broken toolchain: cannot link a simple C program\")\n  RuntimeError: Broken toolchain: cannot link a simple C program\n\n---\n\nFailed to build numpy\nInstalling collected packages: numpy, tensorflow, setuptools\n  Running setup.py install for numpy\n    Complete output from command /usr/bin/python3 -c \"import setuptools, tokenize;**file**='/tmp/pip-build-vtpn0vsk/numpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(**file**).read().replace('\\r\\n', '\\n'), **file**, 'exec'))\" install --record /tmp/pip-ong2pih1-record/install-record.txt --single-version-externally-managed --compile:\n\n```\nNote: if you need reliable uninstall behavior, then install\nwith pip instead of using `setup.py install`:\n\n  - `pip install .`       (from a git repo or downloaded source\n                           release)\n  - `pip install numpy`   (last Numpy release on PyPi)\n\n\nblas_opt_info:\nblas_mkl_info:\n  libraries mkl,vml,guide not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']\n  NOT AVAILABLE\n\nopenblas_info:\n  libraries openblas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']\n  NOT AVAILABLE\n\natlas_3_10_blas_threads_info:\nSetting PTATLAS=ATLAS\n  libraries tatlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']\n  NOT AVAILABLE\n\natlas_3_10_blas_info:\n  libraries satlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']\n  NOT AVAILABLE\n\natlas_blas_threads_info:\nSetting PTATLAS=ATLAS\n  libraries ptf77blas,ptcblas,atlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']\n  NOT AVAILABLE\n\natlas_blas_info:\n  libraries f77blas,cblas,atlas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64/sse2', '/usr/lib64', '/usr/lib/sse2', '/usr/lib', '/usr/lib/sse2', '/usr/lib/']\n  NOT AVAILABLE\n\nblas_info:\ncustomize UnixCCompiler\nC compiler: gcc -pthread -Wno-unused-result -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC\n\ncreating /tmp/tmpf19ke2cs/tmp\ncreating /tmp/tmpf19ke2cs/tmp/tmpf19ke2cs\ncompile options: '-I/usr/local/include -I/usr/include -c'\ngcc: /tmp/tmpf19ke2cs/source.c\ngcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory\ngcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory\n  libraries blas not found in ['', '']\nRuntime library blas was not found. Ignoring\n  FOUND:\n    libraries = ['blas', 'blas']\n    library_dirs = ['/usr/lib64']\n\n  FOUND:\n    libraries = ['blas', 'blas']\n    define_macros = [('NO_ATLAS_INFO', 1)]\n    library_dirs = ['/usr/lib64']\n\nnon-existing path in 'numpy/distutils': 'site.cfg'\nF2PY Version 2\nlapack_opt_info:\nopenblas_lapack_info:\n  libraries openblas not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']\n  NOT AVAILABLE\n\nlapack_mkl_info:\nmkl_info:\n  libraries mkl,vml,guide not found in ['/usr/local/lib64', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/']\n  NOT AVAILABLE\n\n  NOT AVAILABLE\n\natlas_3_10_threads_info:\nSetting PTATLAS=ATLAS\n  libraries tatlas,tatlas not found in /usr/local/lib64\n  libraries lapack_atlas not found in /usr/local/lib64\n  libraries tatlas,tatlas not found in /usr/local/lib\n  libraries lapack_atlas not found in /usr/local/lib\n  libraries tatlas,tatlas not found in /usr/lib64/sse2\n  libraries lapack_atlas not found in /usr/lib64/sse2\n  libraries tatlas,tatlas not found in /usr/lib64\n  libraries lapack_atlas not found in /usr/lib64\n  libraries tatlas,tatlas not found in /usr/lib/sse2\n  libraries lapack_atlas not found in /usr/lib/sse2\n  libraries tatlas,tatlas not found in /usr/lib\n  libraries lapack_atlas not found in /usr/lib\n  libraries tatlas,tatlas not found in /usr/lib/sse2\n  libraries lapack_atlas not found in /usr/lib/sse2\n  libraries tatlas,tatlas not found in /usr/lib/\n  libraries lapack_atlas not found in /usr/lib/\n<class 'numpy.distutils.system_info.atlas_3_10_threads_info'>\n  NOT AVAILABLE\n\natlas_3_10_info:\n  libraries satlas,satlas not found in /usr/local/lib64\n  libraries lapack_atlas not found in /usr/local/lib64\n  libraries satlas,satlas not found in /usr/local/lib\n  libraries lapack_atlas not found in /usr/local/lib\n  libraries satlas,satlas not found in /usr/lib64/sse2\n  libraries lapack_atlas not found in /usr/lib64/sse2\n  libraries satlas,satlas not found in /usr/lib64\n  libraries lapack_atlas not found in /usr/lib64\n  libraries satlas,satlas not found in /usr/lib/sse2\n  libraries lapack_atlas not found in /usr/lib/sse2\n  libraries satlas,satlas not found in /usr/lib\n  libraries lapack_atlas not found in /usr/lib\n  libraries satlas,satlas not found in /usr/lib/sse2\n  libraries lapack_atlas not found in /usr/lib/sse2\n  libraries satlas,satlas not found in /usr/lib/\n  libraries lapack_atlas not found in /usr/lib/\n<class 'numpy.distutils.system_info.atlas_3_10_info'>\n  NOT AVAILABLE\n\natlas_threads_info:\nSetting PTATLAS=ATLAS\n  libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib64\n  libraries lapack_atlas not found in /usr/local/lib64\n  libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib\n  libraries lapack_atlas not found in /usr/local/lib\n  libraries ptf77blas,ptcblas,atlas not found in /usr/lib64/sse2\n  libraries lapack_atlas not found in /usr/lib64/sse2\n  libraries ptf77blas,ptcblas,atlas not found in /usr/lib64\n  libraries lapack_atlas not found in /usr/lib64\n  libraries ptf77blas,ptcblas,atlas not found in /usr/lib/sse2\n  libraries lapack_atlas not found in /usr/lib/sse2\n  libraries ptf77blas,ptcblas,atlas not found in /usr/lib\n  libraries lapack_atlas not found in /usr/lib\n  libraries ptf77blas,ptcblas,atlas not found in /usr/lib/sse2\n  libraries lapack_atlas not found in /usr/lib/sse2\n  libraries ptf77blas,ptcblas,atlas not found in /usr/lib/\n  libraries lapack_atlas not found in /usr/lib/\n<class 'numpy.distutils.system_info.atlas_threads_info'>\n  NOT AVAILABLE\n\natlas_info:\n  libraries f77blas,cblas,atlas not found in /usr/local/lib64\n  libraries lapack_atlas not found in /usr/local/lib64\n  libraries f77blas,cblas,atlas not found in /usr/local/lib\n  libraries lapack_atlas not found in /usr/local/lib\n  libraries f77blas,cblas,atlas not found in /usr/lib64/sse2\n  libraries lapack_atlas not found in /usr/lib64/sse2\n  libraries f77blas,cblas,atlas not found in /usr/lib64\n  libraries lapack_atlas not found in /usr/lib64\n  libraries f77blas,cblas,atlas not found in /usr/lib/sse2\n  libraries lapack_atlas not found in /usr/lib/sse2\n  libraries f77blas,cblas,atlas not found in /usr/lib\n  libraries lapack_atlas not found in /usr/lib\n  libraries f77blas,cblas,atlas not found in /usr/lib/sse2\n  libraries lapack_atlas not found in /usr/lib/sse2\n  libraries f77blas,cblas,atlas not found in /usr/lib/\n  libraries lapack_atlas not found in /usr/lib/\n<class 'numpy.distutils.system_info.atlas_info'>\n  NOT AVAILABLE\n\nlapack_info:\n  libraries lapack not found in ['', '']\nRuntime library lapack was not found. Ignoring\n  FOUND:\n    libraries = ['lapack', 'lapack']\n    library_dirs = ['/usr/lib64']\n    language = f77\n\n  FOUND:\n    libraries = ['lapack', 'lapack', 'blas', 'blas']\n    define_macros = [('NO_ATLAS_INFO', 1)]\n    library_dirs = ['/usr/lib64']\n    language = f77\n\nrunning install\nrunning build\nrunning config_cc\nunifing config_cc, config, build_clib, build_ext, build commands --compiler options\nrunning config_fc\nunifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\nrunning build_src\nbuild_src\nbuilding py_modules sources\nbuilding library \"npymath\" sources\ncustomize Gnu95FCompiler\nFound executable /usr/bin/gfortran\ncustomize Gnu95FCompiler\ncustomize Gnu95FCompiler using config\nC compiler: gcc -pthread -Wno-unused-result -DDYNAMIC_ANNOTATIONS_ENABLED=1 -DNDEBUG -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC\n\ncompile options: '-Inumpy/core/src/private -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/include/python3.4m -c'\ngcc: _configtest.c\ngcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory\ngcc: error: /usr/lib/rpm/redhat/redhat-hardened-cc1: No such file or directory\nfailure.\nremoving: _configtest.c _configtest.o\nRunning from numpy source directory.\n/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/system_info.py:1646: UserWarning:\n    Atlas (http://math-atlas.sourceforge.net/) libraries not found.\n    Directories to search for the libraries can be specified in the\n    numpy/distutils/site.cfg file (section [atlas]) or by setting\n    the ATLAS environment variable.\n  warnings.warn(AtlasNotFoundError.__doc__)\n/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/system_info.py:1548: UserWarning:\n    Atlas (http://math-atlas.sourceforge.net/) libraries not found.\n    Directories to search for the libraries can be specified in the\n    numpy/distutils/site.cfg file (section [atlas]) or by setting\n    the ATLAS environment variable.\n  warnings.warn(AtlasNotFoundError.__doc__)\n/usr/lib64/python3.4/distutils/dist.py:260: UserWarning: Unknown distribution option: 'define_macros'\n  warnings.warn(msg)\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/tmp/pip-build-vtpn0vsk/numpy/setup.py\", line 386, in <module>\n    setup_package()\n  File \"/tmp/pip-build-vtpn0vsk/numpy/setup.py\", line 378, in setup_package\n    setup(**metadata)\n  File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/core.py\", line 169, in setup\n    return old_setup(**new_attr)\n  File \"/usr/lib64/python3.4/distutils/core.py\", line 148, in setup\n    dist.run_commands()\n  File \"/usr/lib64/python3.4/distutils/dist.py\", line 955, in run_commands\n    self.run_command(cmd)\n  File \"/usr/lib64/python3.4/distutils/dist.py\", line 974, in run_command\n    cmd_obj.run()\n  File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/install.py\", line 62, in run\n    r = self.setuptools_run()\n  File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/install.py\", line 36, in setuptools_run\n    return distutils_install.run(self)\n  File \"/usr/lib64/python3.4/distutils/command/install.py\", line 539, in run\n    self.run_command('build')\n  File \"/usr/lib64/python3.4/distutils/cmd.py\", line 313, in run_command\n    self.distribution.run_command(command)\n  File \"/usr/lib64/python3.4/distutils/dist.py\", line 974, in run_command\n    cmd_obj.run()\n  File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/build.py\", line 47, in run\n    old_build.run(self)\n  File \"/usr/lib64/python3.4/distutils/command/build.py\", line 126, in run\n    self.run_command(cmd_name)\n  File \"/usr/lib64/python3.4/distutils/cmd.py\", line 313, in run_command\n    self.distribution.run_command(command)\n  File \"/usr/lib64/python3.4/distutils/dist.py\", line 974, in run_command\n    cmd_obj.run()\n  File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/build_src.py\", line 147, in run\n    self.build_sources()\n  File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/build_src.py\", line 158, in build_sources\n    self.build_library_sources(*libname_info)\n  File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/build_src.py\", line 293, in build_library_sources\n    sources = self.generate_sources(sources, (lib_name, build_info))\n  File \"/tmp/pip-build-vtpn0vsk/numpy/numpy/distutils/command/build_src.py\", line 376, in generate_sources\n    source = func(extension, build_dir)\n  File \"numpy/core/setup.py\", line 654, in get_mathlib_info\n    raise RuntimeError(\"Broken toolchain: cannot link a simple C program\")\nRuntimeError: Broken toolchain: cannot link a simple C program\n\n----------------------------------------\n```\n\nWell, I also installed blas-devel and lapack-devel. \nThanks for your help.\n", "comments": ["Done. I just need to install numpy from dnf.\n"]}, {"number": 3789, "title": "Fix tensorboard CSV download in Python 3", "body": "In Python 3, the `csv` module cannot be used with `six.BytesIO` (`io.BytesIO` in Py3) as demonstrated by this example:\n\n```\nfrom six import BytesIO\nimport csv\n\nout = BytesIO()\nwriter = csv.writer(out)\nwriter.writerow(('A', 'B'))\n```\n\nWhich under Python 3 gives:\n\n```\nTraceback (most recent call last):\n  File \"bug.py\", line 6, in <module>\n    writer.writerow(('A', 'B'))\nTypeError: a bytes-like object is required, not 'str'\n```\n\nThis same problem happens when downloading CSV files from Tensorboard under Python 3.\n\nThis is a well-documented behavior and the solution is to change to `six.StringIO` ([1](http://stackoverflow.com/questions/37866883/unable-to-write-byte-like-string-using-csv-writer-in-python3), [2](http://stackoverflow.com/questions/9157314/python-write-data-into-csv-format-as-string-not-file?noredirect=1&lq=1), [3](http://stackoverflow.com/questions/13120127/how-can-i-use-io-stringio-with-the-csv-module?noredirect=1&lq=1)). This solution works for both Python 2 and Python 3.\n\nThere was a related problem with `BaseHTTPRequestHandler` not accepting unicode. This is also addressed in this PR by adding a `compat.as_bytes` before the `wfile.write` call.\n\nNote: I think the `BytesIO` usage `in _send_gzip_response` is fine, which is why I did not remove the `BytesIO` import.\n", "comments": ["Can one of the admins verify this patch?\n", "Looks good to me.\n", "@tensorflow-jenkins Test this please\n", "I must have cloned a broken build. I rebased it and that actually eliminated the need for the `as_bytes` one. It's ready to be tested again (can anyone ask @tensorflow-jenkins, or is that an admin privilege?).\n", "@tensorflow-jenkins Test this please\n", "@danmaane I guess Jenkins only listens to admins, could you run this again?\n\nBy the way, why not let anyone ask Jenkins to run tests? Or at least the author of a PR after some form of admin approval?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3788, "title": "building issue with cuda 8.0RC and computing capability 6.1", "body": "```\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(279): error: identifier \"h2log1p\" is undefined\n\n1 error detected in the compilation of \"/tmp/tmpxft_00000170_00000000-9_depthtospace_op_gpu.cu.compute_61.cpp1.ii\".\nERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/kernels/BUILD:1529:1: output 'tensorflow/core/kernels/_objs/depth_space_ops_gpu/tensorflow/core/kernels/depthtospace_op_gpu.cu.pic.o' was not created.\nERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/kernels/BUILD:1529:1: not all outputs were created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 112.094s, Critical Path: 108.68s\n```\n\nso `error: identifier \"h2log1p\" is undefined` is the cause, there must be something new in the Eigen library\n", "comments": ["recommend this [issue ](https://github.com/tensorflow/tensorflow/issues/2109#issuecomment-233219528). I have succeed on a GTX 1080 (capability 6.1) and cuda8.0. \n", "@suiyuan2009 sorry, I tried this but it cannot help this bug\n", "Please can you follow the template for reporting an issue.  It is impossible to triage if you do not provide software versions and adequate information to reproduce.\n", "This appears to be a duplicate of other reports.  I had this issue and it was resolved a few days ago at the HEAD.\n", "Yes, I believe it was fixed already.\n", "@vrv is the fix going to be cherry-picked in the r0.10 branch?\n", "Hm, it should be.\n\nhttps://github.com/tensorflow/tensorflow/commit/2ddc543401efe02a521f6ce9c3a41753faa8c774 is the one, right?\n", "https://github.com/tensorflow/tensorflow/commit/5d3492df515b059c7b5ccfb72d3a3f3088594f52 I just cherry-picked it -- can you let me know if it works now?\n", "@vrv it works fine now, thanks a lot for your prompt answer!\n"]}, {"number": 3787, "title": "Remove unnecessary exception and abstract property in BaseEstimator", "body": "These are not necessary anymore due to the addition of `Evaluable` and `Trainable`'s abstract methods. \n", "comments": ["can you fix the pylint errors?\n", "Seems like they existed before. Let's try again. @tensorflow-jenkins Test this please\n"]}, {"number": 3786, "title": "Issue installing.", "body": "I get the following error when I try to build the latest tensorflow from source.  I am on Ubuntu 16.04.  I am using GCC 5.4 with CUDA 8.0 RC (GCC 5.4 patch installed).\n\n> external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(279): error: identifier \"h2log1p\" is undefined\n> \n> 1 error detected in the compilation of \"/tmp/tmpxft_00007e16_00000000-7_depthtospace_op_gpu.cu.cpp1.ii\".\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\n> Use --verbose_failures to see the command lines of failed build steps.\n> INFO: Elapsed time: 270.294s, Critical Path: 152.97s\n", "comments": ["I have been seeing the same problem with CUDA enabled on the latest build.  I have disabled CUDA to compile the latest code base.\n", "I have the same problem except I'm using gcc 4.9 on Ubuntu 16.04 with CUDA 8.0 RC.\n", "#3788 same here\n", "I have the same problem, using cuDNN 5.1 and CUDA 8.0. The error is here:\nEigen/src/Core/arch/CUDA/PacketMathHalf.h(279)\n", "Hitting the same issue on 16.04, using CUDA 8.0RC with the 5.4 patch installed. Using cuDNN 5.0\nI'm trying to build against compute version 6.1\n\nThe header template looks like this:\n\n```\n#if defined __CUDACC_VER__ && __CUDACC_VER__ >= 80000 && defined __CUDA_ARCH__ && __CUDA_ARCH__ >= 530\n\n...\n\ntemplate<>  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE\nhalf2 plog1p<half2>(const half2& a) {\n  return h2log1p(a);\n}\n\n...\n\n```\n\nOn the previous versions it would default to\n\n```\ntemplate<> EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE half2 plog1p<half2>(const half2& a) {\n  float a1 = __low2float(a);\n  float a2 = __high2float(a);\n  float r1 = log1pf(a1);\n  float r2 = log1pf(a2);\n  return __floats2half2_rn(r1, r2);\n}\n\n```\n\n`h2log1p` does not show up anywhere else in the Eigen source as far as I can tell (also checked against their current dev branch)\n", "I have faced with the same error of:\n`external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(279): error: identifier \"h2log1p\" is undefined`\n\nGTX 1080 with NVIDIA Driver 367.35, CUDA 8.0rc, CuDNN 5.1 and the current master branch of TF.\n\nEDIT:\nI could successfully compile with the branch r0.10\nIs there any substantial difference between the r0.10 and the master branch?\n", "@ast0414 What compute version are you building against? I had no issues with 5.2, but 6.1 isn't building for me.\n", "FYI - I noticed on the MNIST model / convultional.py in tensorflow, if you run it on a Pascal chip set (ie 1080) and are using the v4 of cuDNN with the tensorflow GPU Linux binary, the resulting model is incorrect (90% testing error). Should be checked once the compile issue is resolved.\n", "With v5 of cuDNN with a Titan X Pascal (compiled targeting compute version 5.2) I got the following on the MNIST model:\n\n```\nRun #1\nStep 8500 (epoch 9.89), 4.1 ms\nMinibatch loss: 1.604, learning rate: 0.006302\nMinibatch error: 0.0%\nValidation error: 0.9%\nTest error: 0.8%\n\nRun #2\nStep 8500 (epoch 9.89), 4.2 ms\nMinibatch loss: 1.595, learning rate: 0.006302\nMinibatch error: 0.0%\nValidation error: 0.8%\nTest error: 0.8%\n```\n", "[RESOLVED] Same error, same setup: Ubuntu 16.04, gcc 5.4, GTX 1080 - 367.35,  CUDA 8.0.27.1, CuDNN 5.1.5, and current master of TF.   capability v6.1\n\nSearching for h2log1p reveals it isn't there ..\n grep -rnw /path/to/cuda/include -e \"h2log1p\"\n\nbut in /path/to/cuda/include/cuda_fp16.h you can find the declarations for h2log ... which is why the declare statement in PacketMathHalf.h was probably added \n#if defined **CUDACC_VER** && **CUDACC_VER** >= 80000 && defined **CUDA_ARCH** && **CUDA_ARCH** >= 530.\n\nSteps:\n1. _git clone --recurse-submodules https://github.com/tensorflow/tensorflow_\n2. edit tensorflow/third_party/gpus/crosstool/CROSSTOL and add:  \n   **`cxx_builtin_include_directory: \"path/to/cuda/include\"`**     (note: symlink did not work, use actual path)\n3. _bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package_\n4. This will take awhile but eventually fail due to above error \"undefined h2log1p\"\n5. locate PacketMathHalf.h and edit it\n6. comment out:   **`// return h2log1p(a);`**\n7. paste in:\n   **`\n   float a1 = __low2float(a);\n   float a2 = __high2float(a);\n   float r1 = log1pf(a1);\n   float r2 = log1pf(a2);\n   return __floats2half2_rn(r1, r2);\n   `**\n8. _bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package_\n9. _bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg_\n10. The name of the .whl file will depend on your platform.\n11. _sudo pip install /tmp/tensorflow_pkg/tensorflow-0.10.0rc0-py2-none-any.whl_\n12. test with\n    cd tensorflow/models/image/mnist\n    python convolutional.py\n", "@Brennan-VanderLaan Sorry for the late response. I set the computing capability as 6.1 and there was no error when I built on r0.10 branch while I failed to build on the master branch\n", "@wurrego is it possible to make your workaround to a Pull Request on the master branch\n@vrv can we solve this in generating `PacketMathHalf.h`\n", "@ibab @benoitsteiner can you guys look at this?\n\nLooks like this was added in https://bitbucket.org/eigen/eigen/commits/e531f13bce9d8bfcf9df54eb757bd81e94ee105e\n", "@vrv Seems like this one is my fault.\nWhen I was testing the changes, that section was never compiled since I wasn't using CUDA 8.\n\nThe right solution is to take the function out of the if/else macro and use the fallback version.\n\n**Edit:** I've created an Eigen PR with a fix here: https://bitbucket.org/eigen/eigen/pull-requests/218/fix-compilation-on-cuda-8-due-to-missing/diff.\nOnce the fix is in Eigen, the Eigen commit hash in `tensorflow/workspace.bzl` will need to be updated.\n", "@wurrego thank you so much. After many days trying, tried your solution and it works like a charm.\n", "@ibab ,thanks. If this has been fixed, please update me :smile: \n", "@fayeshine \nThe fix has now been merged into TensorFlow.\nCan someone verify that the compile error is gone with the newest commit on `master`?\n", "I just tried it, and the error is still there, but h2log1p is not present in any PacketMathHalf.h files on my machine (i.e the result of `cat $(locate 'PacketMathHalf') | grep 'h2log1p'` is empty)\n\nMy current git commit hash is `008bcaea38815f46804fc3f56492f4dd93837a56` and I can see your  commit (from Igor Babuschkin) `f0cff57` that should fix the CUDA 8 compilation error in my git log\n\n```\nINFO: From Compiling tensorflow/core/kernels/batchtospace_op_gpu.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/CUDA/PacketMathHalf.h(279): error: identifier \"h2log1p\" is undefined\n\n1 error detected in the compilation of \"/tmp/tmpxft_000046de_00000000-10_batchtospace_op_gpu.cu.compute_61.cpp1.ii\".\nERROR: /mnt/pccfs/downloads/tensorflow/tensorflow/core/kernels/BUILD:1509:1: output 'tensorflow/core/kernels/_objs/batchtospace_op_gpu/tensorflow/core/kernels/batchtospace_op_gpu.cu.pic.o' was not created.\nERROR: /mnt/pccfs/downloads/tensorflow/tensorflow/core/kernels/BUILD:1509:1: not all outputs were created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 89.445s, Critical Path: 33.07s\n```\n", "~~Can you try running `bazel clean --expunge` to get rid of the cached packages before compiling?~~\n\n**Edit:** Seems like my pull request to Eigen didn't actually make it in. Instead, it's been mysteriously replaced by a close branch commit: https://bitbucket.org/eigen/eigen/commits/5fea06b2707993a43c5b31105b8224266ecc2c8f?at=default\nSeems like I'll have to make another pull request. :(\n", "This issue has been fixed for me at the latest head:\nhttps://github.com/tensorflow/tensorflow/commit/39513573b40b61417ef859bdd8c6578d61e4312d\n\nVersions installed:\nCUDA 8.0\ncuDNN 5.0\nUbuntu 16.04\nGTX 1080\nNvidia Driver 367.35.0\n", "Yay!\n", "@ibab have you made the pull request\n", "seems ok now.\n", "@fayeshine: Yes, it's #3831.\n"]}, {"number": 3785, "title": "Branch 130152575", "body": "", "comments": []}, {"number": 3784, "title": "Remove duplicated NanLossDuringTrainingError in graph_action", "body": "", "comments": ["Sanity checks and TF.org failed. Testing again. \n@tensorflow-jenkins Test this please\n", "I don't think the test failures are related with my changes. Could someone else take a look?\n", "@tensorflow-jenkins test this please  \n(looks like the lint errors were pre-existing, maybe they've been updated, trying again)\n"]}, {"number": 3783, "title": "ZlibInputBuffer::Inflate() incorrectly considers Z_STREAM_END an error", "body": "When reading compressed records produced by something other than TFRecordWriter it is possible to produce valid files that are not readable by Tensorflow.\n\nThis fix adds a test that creates a deflate file using the builtin zlib module that previously caused TFRecordReader to raise an exception:\n\n`DataLossError: inflate() failed with error 1`\n\nFull stack is below:\n\n```\n======================================================================\nERROR: testZLibFlushRecord (__main__.TFRecordWriterZlibTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/source/tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py\", line 495, in testZLibFlushRecord\n    k, v = sess.run([key, value])\n  File \"/Users/nhowell/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/Users/nhowell/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/Users/nhowell/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/Users/nhowell/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\nDataLossError: inflate() failed with error 1\n     [[Node: ReaderRead = ReaderRead[_class=[\"loc:@fifo_queue\", \"loc:@test_reader\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](test_reader, fifo_queue)]]\n```\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks!\n\n@tensorflow-jenkins test this please\n", "Looks like this doesn't fully work< I get the following errors from the log:\n\nFAIL: //tensorflow/python/kernel_tests:reader_ops_test (see /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/tensorflow/python/kernel_tests/reader_ops_test/test.log).\nINFO: From Testing //tensorflow/python/kernel_tests:reader_ops_test:\n==================== Test output for //tensorflow/python/kernel_tests:reader_ops_test:\n../var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:315: ResourceWarning: unclosed file <_io.BufferedWriter name='/tmp/reader_ops_test/fixed_length_record.0.txt'>\n  f = open(fn, \"wb\")\n/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:323: ResourceWarning: unclosed file <_io.BufferedWriter name='/tmp/reader_ops_test/fixed_length_record.1.txt'>\n  files = self._CreateFiles()\n.....W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Could not parse state for IdentityReader 'test_reader': \\001\\020\\001\\030\\001\\\"\\001X\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Could not parse state for IdentityReader 'test_reader': \\010\\001\\020\\001\\030\\001\\\"\\001\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Could not parse state for IdentityReader 'test_reader': \\010\\001\\020\\001\\030\\001\\\"\\001XExtraJunk\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Could not parse state for IdentityReader 'test_reader': PREFIX\\010\\001\\020\\001\\030\\001\\\"\\001X\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Could not parse state for IdentityReader 'test_reader': BOGUS\\001\\\"\\001X\n........E./var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:244: ResourceWarning: unclosed file <_io.BufferedWriter name='/tmp/reader_ops_test/text_line.0.txt'>\n  f = open(fn, \"wb\")\n/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:275: ResourceWarning: unclosed file <_io.BufferedWriter name='/tmp/reader_ops_test/text_line.1.txt'>\n  self._testOneEpoch(self._CreateFiles(crlf=True))\n./var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:272: ResourceWarning: unclosed file <_io.BufferedWriter name='/tmp/reader_ops_test/text_line.1.txt'>\n  self._testOneEpoch(self._CreateFiles(crlf=False))\n./var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:278: ResourceWarning: unclosed file <_io.BufferedWriter name='/tmp/reader_ops_test/text_line.1.txt'>\n  files = self._CreateFiles()\n../var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:182: ResourceWarning: unclosed file <_io.BufferedWriter name='/tmp/reader_ops_test/whole_file.0.txt'>\n  open(fn, \"wb\").write(c)\n/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:182: ResourceWarning: unclosed file <_io.BufferedWriter name='/tmp/reader_ops_test/whole_file.1.txt'>\n  open(fn, \"wb\").write(c)\n/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py:182: ResourceWarning: unclosed file <_io.BufferedWriter name='/tmp/reader_ops_test/whole_file.2.txt'>\n  open(fn, \"wb\").write(c)\n\n# ...\n\n## ERROR: testZLibFlushRecord (**main**.TFRecordWriterZlibTest)\n\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py\", line 464, in testZLibFlushRecord\n    writer.write(\"small record\")\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/lib/io/tf_record.py\", line 112, in write\n    self._writer.WriteRecord(record)\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 156, in WriteRecord\n    def WriteRecord(self, *args): return _pywrap_tensorflow.PyRecordWriter_WriteRecord(self, *args)\nTypeError: expected bytes, str found\n", "Here's an attempt at a fix... if this doesn't work I'll actually setup a python3 environment for testing \ud83d\ude08 \n", "@tensorflow-jenkins test this please\n", "Sadly still doesn't work :(\n\n## ERROR: testZLibFlushRecord (**main**.TFRecordWriterZlibTest)\n\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py\", line 476, in testZLibFlushRecord\n    output += compressor.compress(c)\nTypeError: 'int' does not support the buffer interface\n", "I'll test it on py3 today and get it fixed.\n\nOn Aug 15, 2016 9:14 AM, \"Vijay Vasudevan\" notifications@github.com wrote:\n\n> Sadly still doesn't work :(\n> ERROR: testZLibFlushRecord (_main_.TFRecordWriterZlibTest)\n> \n> Traceback (most recent call last):\n> File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-\n> python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/\n> eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-\n> out/local-py3-opt/bin/tensorflow/python/kernel_tests/reader_ops_test.\n> runfiles/org_tensorflow/tensorflow/python/kernel_tests/reader_ops_test.py\",\n> line 476, in testZLibFlushRecord\n> output += compressor.compress(c)\n> TypeError: 'int' does not support the buffer interface\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3783#issuecomment-239847561,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAKbTX4D7o9d06UN7XQm0gUmqqWgRRd6ks5qgJBtgaJpZM4JjigA\n> .\n", "Okay, I managed to find a fix that works on py2 and py3. Tests pass locally now... can you kick off another Jenkins run?\n", "sounds good!  kicking one off\n\n@tensorflow-jenkins test this please\n", "Great!!\n"]}, {"number": 3782, "title": "Tensorboard overlays the legend on top of curve, making it hard to see the curve itself", "body": "**Feature Request** (This is not super high-priority, but would be nice to have)\nTensorboard overlays the legend on top of the graph, sometimes making it hard to see parts of the graph. It would be better that the legend location can be at the top/bottom depending on the mouse position, so it does not overlap on the graph itself. Example:\n![screen shot 2016-08-10 at 10 01 25 am](https://cloud.githubusercontent.com/assets/1893429/17637456/7b4db6ae-6098-11e6-9458-93a5beea19c8.png)\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: CUDA 7.5.18, cuDNN 5\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): 056db850614e0a06ce6f65b30f877c5789ab74f5\n2. The output of `bazel version`: Build label: 0.3.0\n", "comments": ["This has been fixed in master branch, now the tooltip is shown on the bottom of the chart to not overlap with the graph.\n"]}, {"number": 3781, "title": "Revert \"Merge branch 'r0.10' of https://github.com/tensorflow/tensorf\u2026", "body": "Commit a3539967e2e68cbf4a948d7d2bd3c984f7f4092c was pushed by mistake, reverting 3c65e0d5222a40b92ca176ea2e95d19df0b55a80.\n\nThis PR reverts the effects, and reinstates Josh's fixes.\n\nThis reverts commit a3539967e2e68cbf4a948d7d2bd3c984f7f4092c, reversing\nchanges made to ee221cb625bef24b09059d701c12d4e7c48a11c6.\n", "comments": []}, {"number": 3780, "title": "Added hungarian layer.", "body": "Please see initial discussion [here](https://github.com/Russell91/TensorBox/issues/47) for reference. I am submitting a custom layer that is necessary to implement [this paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Stewart_End-To-End_People_Detection_CVPR_2016_paper.pdf) from CVPR 2016. Users of [Tensorbox](https://github.com/Russell91/TensorBox) currently must compile Tensorflow from source to use these features, so accepting a variant of this pull request would at least help out those existing users. It would ideally be useful to others as well.\n\nThat being said, there are 2 known problems with the pull request:\n\n1) The code is messy\n2) The hungarian algorithm library itself is not something I can contribute under the Tensorflow license. There is no publicly available hungarian implementation that would satisfy this constraint either. I am holding out hope that Google may have a private implementation of the hungarian algorithm that could be packaged into the repository and satisfy the license constraints. If not, and the only way forward is to write a new implementation, someone (perhaps me) could write a novel hungarian c++ implementation from the details on wikipedia.\n\nProblem (2) has prevented me from really worrying about problem (1).\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for the pull request.  Unfortunately the licensing issues are a blocker.\n\nPerhaps we can sidestep this issue?  I think we now have a new way of supporting user-implemented ops now -- we should no longer need the user_ops directory to load custom ops, and we should figure out a way for your users to be able to build and load your custom op without having it be added to the official TF repository.  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/adding_an_op/index.md#with-tensorflow-binary-installation should be the way you can get this done.\n\nAssigning to @keveman so he can help with custom ops issues.\n", "(Marking as CLA: no so this doesn't get merged by accident)\n", "Okay, cool that there is an additional pathway under development. It seems that each user would still have to compile the op from source on their machine (and install bazel) though?\n\nAlso, were there no CLA issues, what would be the probability of getting this sort of op in? I didn't mean to imply in my initial post that recoding a hungarian algorithm would be some sort of intractable effort. Everything outside that contiguous region of code is 100% owned by me.\n", "Well, the hope is that users don't have to install bazel -- just need to run 'gcc' on the custom op that, say, lives in the TensorBox library (which you could package up via pip yourself, etc).  That's the situation I'd love to be in :)\n\nAs for its suitability in TF: we could get a set of opinions on this, but speaking as someone who has to essentially forever maintain any code that gets into TF core, I'm always wary of having to maintain (and handle bug reports for, debug) code for ideas that aren't \"core\" functions of TensorFlow.  I guess I'd have to understand how generally this op could be used.  One idea is that, we leave this external for now, and as more feature requests come in that could be solved by having this op in the core, we add it to core at that point.  What do you think?\n", "@vrv Makes sense to me. I guess we'll just have to keep an eye on this as the extension system matures?\n", "@Russell91, perhaps you didn't fully grok what @vrv was trying to say. The extension system is indeed quite mature. Look [here](https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html) for complete instructions on how to add an op to TensorFlow. You have two options, depending on how you want your op to be compiled and used.\n- Add it to tensorflow/contrib and send a PR. See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/linear_optimizer/BUILD) for an example BUILD file that compiles a custom op and makes it available TensorFlow users via `tensorflow.contrib`.\n- Maintain your op's source code independent of TensorFlow's source code. In this case, you would need the users to install the binary version of TensorFlow via the PIP package. Look [here](https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html#with-tensorflow-binary-installation) for how to compile your custom op without using the Bazel build system, say, using plain vanilla Makefile.\n\nLet me know if you need help with either of those.\n", "@keveman: tensorflow.contrib is for stuff that is likely to go into core, and as discussed, we're not yet ready for that.\n\nBut that second option is what I was trying to suggest, yes.\n", "Whoops sorry for the confusion.\nWhat I had in mind was the ability to use the Bazel build system to build the custom op. @Russell91, you can follow the model used by some projects in tensorflow/models, look [here](https://github.com/tensorflow/models/tree/master/syntaxnet)  for example. You setup your custom op as an independent git repository with its own BUILD file, and use TensorFlow as an external project.\n", "I still think he would prefer to use a Makefile in his own project to build the custom op, which is also possible.\n", "@kaveman My quote \n\n> I guess we'll just have to keep an eye on this as the extension system matures?\n\nwas a reaction to the comment of @vrv that \n\n> Well, the hope is that users don't have to install bazel -- just need to run 'gcc' on the custom op that, say, lives in the TensorBox library (which you could package up via pip yourself, etc). That's the situation I'd love to be in :)\n\nI guess I'll have to look into exactly what is possible with the current system, but you have to understand that I'm not only interested in a system which is _possible_ for users to add the hungarian layer, but one where it is _easy_. That's just a much higher bar, and say, even requiring them to install bazel would miss the mark entirely. Bazel is so rapidly developed that there's at least a 20% chance of an install taking > 15 minutes due to versioning problems. So I'm happy to learn about a bazel extension system, and it may indeed be much better than forcing them to recompile tensorflow from scratch (as they currently have to with my project). But @vrv is absolutely correct that I would prefer to just have my users say \"make hungarian\", which would call the native gcc to create a .so, and link that in at runtime with some python magic than to have them muck around with a brand new build system (bazel). Note also that my project currently suggests people to get started by ignoring the hungarian layer and just use a simpler model for exactly these complexity reasons.\n\nEdit: Okay, yes it seems that your link here: https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html#with-tensorflow-binary-installation is what I was looking for. Thanks guys!\n"]}, {"number": 3779, "title": "Allows running \"-m tensorflow.tensorboard\"", "body": "This is a suggested change that moves the main entry point of Tensorboard from `tensorboard.py` to `__main__.py`, allowing tensorboard to be run as a package:\n\n```\npython -m tensorflow.tensorboard --logdir=/path\n```\n\nThis can be really useful on environments with multiple versions of python. Of course, virtualenv is a much better way of dealing with this, but I still find this option to be useful at times.\n\nFor instance, if you maintain both Python 2 and Python 3 and do a `pip install --user` install of tensorflow for both versions. The python files will be neatly separated by the version in `~/.local/lib/pythonX.X/site-packages`. However, there is no such mechanism for `~/.local/bin`, so the second pip install will override the first one. You can also get around this by running the `tensorboard.py` file directly, but this is not nearly as convenient and safe as letting python specify the location of the package.\n\nAnother less drastic option here is to keep `tensorboard.py` and add a new file for `__main__.py` that simply imports `main` from `tensorboard.py` and runs it. I'm happy to arrange such a PR too.\n", "comments": ["Can one of the admins verify this patch?\n", "I think I would prefer the last approach, instead of moving the contents from tensorboard.py.\n\nAssigning to Dan\n", "Closing in favor of #3811.\n"]}, {"number": 3778, "title": "Reduce memory usage and increase performance for convolution on iOS", "body": "We've had lots of problems with large convolutions hitting memory limits on iOS. This new implementation of the operator breaks the work into chunks so we never use more than 16 MB, and uses Apple's Accelerate framework to optimize the matrix multiplication.\n\nTesting shows that it's between 5% to 10% faster than the existing implementation on various models, and keeps memory usage to a minimum.\n", "comments": ["For quantized models will be used gemmlowp also on IOS? /cc @wangyida\n", "@bhack gemmlowp can be used on iOS, though we haven't investigated optimizing it for those devices in particular so I expect we'll need to do more work there. This is primarily a fix for memory issues when running float models.\n", "Jenkins, test this please.\n", "Could be interesting to benchmark this against [BNNS](https://developer.apple.com/reference/accelerate/1912851-bnns).\n", "@bhack I can apply gemm on FC layer in tiny-dnn of IOS platform now, some memory issue seems related to the batch size and network structure rather than the parametric model itself.\n", "Looks good to me (my latest round of comments were minor)...\n", "Once the tests have passed, could the admins merge this since we have LGTMs?\n"]}, {"number": 3777, "title": "Problems in TensorFlow Linear Model Tutorial", "body": "I believe the [TensorFlow Linear Model tutorial](https://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html) incorrectly points to [wide_n_deep_tutorial.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py) code. For example that code does not put in place the [L1 and L2 regularization](https://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#adding-regularization-to-prevent-overfitting) that is mentioned (and for which the code is shown) in the tutorial text.\n\nI assume the tutorial was written up based on a different python script?\n\nAlso, [wide_n_deep_tutorial.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py) will not work on Python 3 as it calls `urllib.urlretrieve` directly. Perhaps it should use the `maybe_download` function from `tensorflow.contrib.learn.python.learn.datasets.base`?\n", "comments": ["@xmbrst Could you please take a look at this?\n", "Both the Linear Model and the Wide and Deep tutorials use wide_n_deep_tutorial.py. You use flags, like \" --model_type=wide\", to switch between the two types of models. I'll take a look at the regularization issue.\n\nI've a filed a separate issue, #3956, about the Python 3 problem.\n", "Has this issue been resolved? I'm seeing problems with wide_n_deep in TF 0.12.0 also in Windows 10 (new  version)", "@chitro, do you think your Windows 10 issue is related to the Python 3 problem described by @tiagonj? If so, please update #3956 with a description of the problem. Otherwise, you might want to create a new issue.\r\n\r\nI'm closing this issue about the text of the tutorial, which is working as intended. (The regularization code is framed in the text as a modification you can make to the existing code.)"]}, {"number": 3776, "title": "error with wide_n_deep_tutorial.py", "body": "I'm running into problems when trying to run the wide and deep tutorial code example. Here's my info:\n### Environment info\n\nOperating System: OSX 10.11.4 \n\nInstalled version of CUDA and cuDNN: CUDA-7.5,\n-rwxr-xr-x@ 2 root  wheel      8280 Apr 12 23:02 /usr/local/cuda/lib/libcuda.1.dylib\n-rwxr-xr-x@ 2 root  wheel      8280 Apr 12 23:02 /usr/local/cuda/lib/libcuda.dylib\nlrwxr-xr-x@ 1 root  wheel        45 Apr 12 23:03 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a\nlrwxr-xr-x@ 1 root  wheel        50 Apr 12 23:03 /usr/local/cuda/lib/libcudart.7.5.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib\nlrwxr-xr-x@ 1 root  wheel        46 Apr 12 23:03 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib\nlrwxr-xr-x@ 1 root  wheel        49 Apr 12 23:03 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a\n-rwxr-xr-x@ 1 root  admin  58270280 Aug  2 21:26 /usr/local/cuda/lib/libcudnn.5.dylib\n-rwxr-xr-x@ 1 root  admin  58270280 Aug  2 21:26 /usr/local/cuda/lib/libcudnn.dylib\n-rw-r--r--@ 1 root  admin  55551064 Aug  2 21:26 /usr/local/cuda/lib/libcudnn_static.a\n### Steps to reproduce\n1. Open the script, set the \"model_type\" FLAG to \"deep\" or to \"wide_n_deep\", and run interactively with an open Python compiler.\n### What have you tried?\n1. Running it with FLAGS.model_type = \"wide\" works fine. \n### Logs or other output that would be helpful\n\nHere is the log from running it with FLAGS.model_type = \"deep\"\n\n```\nTraining data is downloaded to /var/folders/_x/ssxr2t2144v_2vr8w0yywpvw0000gn/T/tmpbVSoKV\nTest data is downloaded to /var/folders/_x/ssxr2t2144v_2vr8w0yywpvw0000gn/T/tmp8ojlXf\nmodel directory = .\nWARNING:tensorflow:Setting feature info to {'hours_per_week': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False), 'workclass': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'relationship': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'gender': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'age': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False), 'marital_status': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'race': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'education_num': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False), 'native_country': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'capital_loss': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False), 'education': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'capital_gain': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False), 'occupation': TensorSignature(dtype=tf.string, shape=None, is_sparse=True)}\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(32561)]), is_sparse=False)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\nW tensorflow/core/framework/op_kernel.cc:936] Not found: Tensor name \"hiddenlayer_1/biases/Adagrad\" not found in checkpoint files ./model.ckpt-200-?????-of-00001\n     [[Node: save/restore_slice_12 = RestoreSlice[dt=DT_FLOAT, preferred_shard=0, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_12/tensor_name, save/restore_slice_12/shape_and_slice)]]\nW tensorflow/core/framework/op_kernel.cc:936] Not found: Tensor name \"hiddenlayer_1/biases/Adagrad\" not found in checkpoint files ./model.ckpt-200-?????-of-00001\n     [[Node: save/restore_slice_12 = RestoreSlice[dt=DT_FLOAT, preferred_shard=0, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_12/tensor_name, save/restore_slice_12/shape_and_slice)]]\nW tensorflow/core/framework/op_kernel.cc:936] Not found: Tensor name \"hiddenlayer_1/biases/Adagrad\" not found in checkpoint files ./model.ckpt-200-?????-of-00001\n     [[Node: save/restore_slice_12 = RestoreSlice[dt=DT_FLOAT, preferred_shard=0, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_12/tensor_name, save/restore_slice_12/shape_and_slice)]]\nE tensorflow/core/client/tensor_c_api.cc:485] Tensor name \"hiddenlayer_1/biases/Adagrad\" not found in checkpoint files ./model.ckpt-200-?????-of-00001\n     [[Node: save/restore_slice_12 = RestoreSlice[dt=DT_FLOAT, preferred_shard=0, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_12/tensor_name, save/restore_slice_12/shape_and_slice)]]\n     [[Node: save/Assign_22/_84 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_179_save/Assign_22\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 23, in train_and_eval\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 240, in fit\n    max_steps=max_steps)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 578, in _train_model\n    max_steps=max_steps)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 276, in _supervised_train\n    scaffold=scaffold) as super_sess:\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/supervised_session.py\", line 212, in __init__\n    self._sess = recoverable_session.RecoverableSession(self._create_session)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/recoverable_session.py\", line 46, in __init__\n    WrappedSession.__init__(self, sess_factory())\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/supervised_session.py\", line 232, in _create_session\n    init_fn=self._scaffold.init_fn)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 164, in prepare_session\n    max_wait_secs=max_wait_secs, config=config)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 224, in recover_session\n    saver.restore(sess, ckpt.model_checkpoint_path)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1129, in restore\n    {self.saver_def.filename_tensor_name: save_path})\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.NotFoundError: Tensor name \"hiddenlayer_1/biases/Adagrad\" not found in checkpoint files ./model.ckpt-200-?????-of-00001\n     [[Node: save/restore_slice_12 = RestoreSlice[dt=DT_FLOAT, preferred_shard=0, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_12/tensor_name, save/restore_slice_12/shape_and_slice)]]\n     [[Node: save/Assign_22/_84 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_179_save/Assign_22\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nCaused by op u'save/restore_slice_12', defined at:\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 23, in train_and_eval\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 240, in fit\n    max_steps=max_steps)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 578, in _train_model\n    max_steps=max_steps)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 252, in _supervised_train\n    keep_checkpoint_max=keep_checkpoint_max)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/supervised_session.py\", line 152, in __init__\n    lambda: training_saver.Saver(sharded=True,\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/supervised_session.py\", line 164, in _get_or_default\n    op = default_constructor()\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/supervised_session.py\", line 153, in <lambda>\n    max_to_keep=keep_checkpoint_max))\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 861, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 515, in build\n    filename_tensor, per_device, restore_sequentially, reshape)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 312, in _AddShardedRestoreOps\n    name=\"restore_shard\"))\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 272, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 187, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 203, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n\n```\n", "comments": ["This looks to me like the code it trying to restore a saved checkpoint.  Did you perhaps run with one configuration, long enough to save a checkpoint, and then start with different parameters (but without deleting the old checkpoints?)\n", "Automatically closing due to lack of recent activity. Please reopen if further information becomes available.\n"]}, {"number": 3775, "title": "reduce_mean is not numerically accurate on GPU", "body": "For smaller size matrix tf.reduce_mean works fine. \nBut for larger size matrix floating point results are not accurate.\n\nI am running on CUDA 7.0, Titan X.\n\n``` python\nimport tensorflow as tf\n\nmat3 = tf.get_variable(\"mat3\", [4500,4500], initializer=tf.constant_initializer(-1.))\nreduce_mat3_ref = tf.reduce_mean(mat3)\nwith tf.Session('') as sess:\n    tf.initialize_all_variables().run()\n    reduce1 = sess.run(reduce_mat3_ref)\n    print(reduce1) # on CPU this is -1.0, on GPU this produces -1.00006\n    assert(reduce1 == -1.) \n```\n", "comments": ["Also on CUDA 8.0, GTX 980/GTX 1080, result is off by 5.86509705e-05\n", "Relevant issue: #2625 \nAlthough I don't think numpy uses any of those stable methods, and it still gives exact result on the problem above, so something else is going on\n", "There's a PR for Eigen to support double at the moment:\n\nhttps://bitbucket.org/eigen/eigen/pull-requests/216/enable-efficient-tensor-reduction-for/diff\n\nYou can add yourself to the interest list of that PR. As soon as that's available, you can explicitly make your input tensor to be of dtype double and get the accuracy you prefer. How does that sound?\n", "the problem using both float on GPU and CPU, the results does not agree on each other. and this example is trivial enough, all the numbers are -1 in the entry. Float's precision should have been sufficient.\n", "I believe this is a duplicate of #3103, which was closed as \"working as intended\".\n\nThe basic problem is that there's a speed/accuracy tradeoff for reduce_mean.  Even though your example seems trivial, with the entire matrix initialized to -1.0, there are 20M+ entries (4500*4500).  Note that float32 can represent integral numbers in the range [-16M,+16M] without loss of precison (24 bits):\nhttps://en.wikipedia.org/wiki/Single-precision_floating-point_format\n\nSo we expect to see some error if we accumulate all of the -1.0 entries and divide at the end to compute the mean.  This also explains why smaller matrices didn't exhibit the problem.\n\nHope this answers your question!\n"]}, {"number": 3774, "title": "add int32 support for param seq_length of op reverse_sequence", "body": "#3528\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "what does `Forward declarations of the functor specializations for GPU.` mean ? so many code for gpu specializations.\n", "Jenkins, test this please.\n", "@ebrevdo backwards_compatibility is fixed now.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3773, "title": "upgrade to cudnn5", "body": "", "comments": ["Jenkins, test this please.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3772, "title": "Branch 130102794", "body": "", "comments": []}, {"number": 3771, "title": "Please consolidate the builder API's", "body": "In my opinion, a simple builder API is essential to TensorFlow programming because the procedure of constructing Neural Networks is otherwise complicated, repetitive and error-prone. But there are at least 5 different builder API's being developed in parallel:\n- tf.contrib.layers\n  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/layers\n- tf.contrib.slim\n  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\n- tf.contrib.learn (aka. TF Learn)\n  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn\n- TF Learn (same name, but apparently different from the above):\n  https://github.com/tflearn/tflearn\n- Pretty Tensor\n  https://github.com/google/prettytensor\n\nAll of these add-on packages seem to be more or less in the early stages of development and lacking good documentation. But the API's appear to be very similar. So why not consolidate them into a single builder API that can become an integrated part of TensorFlow? It is very confusing for new users the way it is done now, and the development would get much further if people would work on a single builder API rather than 5 almost identical ones. The builder API is how most people will use TensorFlow so it shouldn't be in limbo like this.\n\nFurthermore, I'm trying to put together some tutorials on YouTube, but I'm concerned that the API I'm using (Pretty Tensor) might become deprecated at some point.\n\nI hope that a consolidated and official builder API will receive more attention from the dev-team. \n\nThanks!\n", "comments": ["Thanks for the great summary. We are in fact in the process of consolidating. Martin will elaborate.\n", "contrib/layers and contrib/learn are APIs at different levels of\nabstraction. They will be moved to TensorFlow core and will be part of\nTensorFlow and hence supported indefinitely. If you're worried about\nsupport and alignment with TensorFlow, this is your best bet.\n\ncontrib/slim as well as PrettyTensor are actively maintained by\nteams/people at Google. Both will likely not disappear.\n\nI cannot speak to external projects' prospects. tflearn.org is an\nunfortunate name collision, we have no involvement in the project.\nOn Mon, Aug 15, 2016 at 17:08 Sherry Moore notifications@github.com wrote:\n\n> Thanks for the great summary. We are in fact in the process of\n> consolidating. Martin will elaborate.\n> \n> \u2014\n> You are receiving this because you were assigned.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3771#issuecomment-239966151,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_SrlutxiCelNBUd7iUnZSEZt4PlKks5qgP9vgaJpZM4JjKxP\n> .\n", "Thanks for the clarification.\n\nWill there be a change in how contrib/layers and contrib/learn will be imported and used once they're moved to TensorFlow core? If so, when do you expect to move them to core, so I can wait doing my tutorials until then?\n", "We will maintain stubs in contrib for a while so importing from the old\nlocation should continue to work. However until the code is moved to core\nit is not subject to stability guarantees so it's very possible that the\ninterfaces change (although I believe the largest changes are behind us).\nOn Thu, Aug 25, 2016 at 01:49 Hvass-Labs notifications@github.com wrote:\n\n> Thanks for the clarification.\n> \n> Will there be a change in how contrib/layers and contrib/learn will be\n> imported and used once they're moved to TensorFlow core? If so, when do you\n> expect to move them to core, so I can wait doing my tutorials until then?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3771#issuecomment-242319976,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_WOIcm4T31ofgF0LbzYjLTFO4cKCks5qjVczgaJpZM4JjKxP\n> .\n"]}, {"number": 3770, "title": "Forward port the \"reuse WorkerInterface\" fix to r0.10", "body": "This is the rollback-to-the-rollback of cf02fa256092d27158966daea428a7942952a19c (which had a bug and was rolled back in e61fb28461cc5f7b09fdd2504fb6ac6ec5bf2c3d.\n\nIt fixes a memory leak in the distributed runtime, as discovered in #3470.\n", "comments": []}, {"number": 3769, "title": "Assigning Multiple Embeddings per Word in Tensorflow", "body": "I am trying to reproduce the results described in the EMNLP 2014 paper \"Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space\". The authors proposed a way of handling multiple senses per word through the use of multiple embeddings associated with it. Each word is assigned a global context embedding and several sense-specific embeddings, and which sense the word belongs to in the current context is determined by the argmax index of the cosine similarity between the target word's sense-specific embeddings and the average of the global vectors in the context window.\n\nWhile the authors have made their Scala code publicly available on their website, I have found no similar implementations with a high level deep learning library like Tensorflow. Since I am new to Tensorflow, I am not sure whether such model can be built with the Python API. My main concern is how one can use the argmax operation as an intermediate layer in Tensorflow, and then to decide which embedding to use in the following layers. In addition, as each word might have different number of senses, is there an efficient way to handle this using embedding_lookup?\n\nDoes anyone have ideas?\n\nRegards,\nZenong\n", "comments": ["Yes, you can do that.  If you want details, please use StackOverflow; Github issues are for code bugs and feature requests.\n"]}]