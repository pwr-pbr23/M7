[{"number": 15987, "title": "Documentation for placeholder does not explain when shape is (), [] or [None]", "body": "### System information\r\n\r\nNot necessary.\r\n\r\n### Describe the problem\r\n\r\nThe documentation for `placeholder` does not explain the case when its shape is `()`, `[]` or `[None]`.\r\n\r\n### Possible solution\r\n\r\nAdd the explanations in [this SO  answer](https://stackoverflow.com/a/46941087/3924118) to the documentation of `placeholder`, including the example !!\r\n\r\n  ", "comments": ["IMO this is well documented in Tensor Ranks, Shapes and Types. Closing this.\r\n  ", "@shivaniag Can you please link me to a place where it is documented?"]}, {"number": 15986, "title": "Add new internal release notes that were missed in the previous iteration.", "body": null, "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Whoops, I messed up this PR invocation -- closing."]}, {"number": 15985, "title": "Feature Request: Dense to Sparse and Dense to Sparse Tensor Ops", "body": "I think it would be helpful if there is a dense_to_sparse op in Tensorflow for ops like `ctc_loss` that requires sparse labels. I'm not really sure where else it can be used aside from that but in case only `ctc_loss` uses it, I think it would help if dense labels can be passed into `ctc_loss` and do the conversion within.", "comments": ["@mrry does this seems to you something we would want to add?", "Perhaps, or perhaps it would make more sense for `tf.nn.ctc_loss()` to accept dense labels and do the conversion internally? I think @ebrevdo created that part of the code, so he'll know best.", "One reason I would like this feature to exist is to be able to use Tensorflow\u2019s Estimator.fit without having to covert the labels to a SparseTensor outside the fit function. ", "+1 for having denaified versions of the CTC loss and decoders. You'll need\nto pass an additional Sequence lengths for the labels.  PRs to\ntf.contrib.layers welcome.\n\nOn Tue, Jan 9, 2018, 6:34 PM Jerome <notifications@github.com> wrote:\n\n> One reason I would like this feature to exist is to be able to use\n> Tensorflow\u2019s Estimator.fit without having to covert the labels to a\n> SparseTensor outside the fit function.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15985#issuecomment-356481613>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2iDTptB5DrgGWkY7tJbE7p7iR22ks5tJCGtgaJpZM4RYleR>\n> .\n>\n", "I\u2019ll see if what I can do. I plan on basing this on the sparse_to_dense op.", "How is the sequence_lengths required by ctc_loss right now different from that of the to-be-implenented-densified_ctc_loss?", "I'm not sure if this is how it's done but here's what I got so far: https://stackoverflow.com/questions/39838234/sparse-matrix-from-a-dense-one-tensorflow\r\n\r\n```\r\n# labels is a dense tensor\r\nindices = tf.where(tf.not_equal(labels, 0))\r\nvalues = tf.gather_nd(labels, indices)\r\nshape = ???\r\nsparse_labels = tf.SparseTensor(indices, values, dense_shape=shape)\r\n```\r\n\r\nI've tried the following:\r\n\r\n1. If shape = labels.get_shape(), I get `ValueError: Cannot convert a partially known TensorShape to a Tensor: (?, 33)`\r\n2. If shape = (1, 33) or tf.shape(labels, out_type=tf.int64), I get `ValueError: Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss') with input shapes: [?,?,80], [?,2], [?], [].`", "Turns out, the conversion from dense to sparse is successful with this code:\r\n\r\n```\r\n#labels is a dense tensor\r\nindices = tf.where(tf.not_equal(labels, 0))\r\nvalues = tf.gather_nd(labels, indices)\r\nshape = tf.shape(labels, out_type=tf.int64)\r\nsparse_labels = tf.SparseTensor(indices, values, dense_shape=shape)\r\n```\r\n\r\nNow the only concern here is the ValueError thrown by ctc_loss:\r\n\r\n`ValueError: Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss') with input shapes: [?,?,80], [?,2], [?], [].`", "I tried using this to function to create a sparse matrix from a dense one:\r\n\r\n```\r\ndef convert_to_sparse(labels, dtype=np.int32):\r\n    indices = []\r\n    values = []\r\n\r\n    for n, seq in enumerate(labels):\r\n        indices.extend(zip([n] * len(seq), range(len(seq))))\r\n        values.extend(seq)\r\n\r\n    indices = np.asarray(indices, dtype=dtype)\r\n    values = np.asarray(values, dtype=dtype)\r\n    shape = np.asarray([len(labels), np.asarray(indices).max(0)[1] + 1], dtype=dtype)\r\n\r\n    return indices, values, shape\r\n```\r\n\r\nWhen I placed the indices, values, and shape to create a `SparseTensor`:\r\n\r\n```\r\nprint(tf.SparseTensor(\r\n        indices=sparse_y_train[0],\r\n        values=sparse_y_train[1],\r\n        dense_shape=sparse_y_train\r\n    ))\r\n```\r\n\r\nAnd compared it to a `SparseTensor` created from a `Tensor`:\r\n\r\n```\r\nindices = tf.where(tf.not_equal(labels, tf.constant(0, dtype=tf.int32)))\r\nvalues = tf.gather_nd(labels, indices)\r\nsparse_labels = tf.SparseTensor(indices, values, dense_shape=tf.shape(labels, out_type=tf.int64))\r\nprint(sparse_labels)\r\n```\r\n\r\nI got these respectively:\r\n\r\n```\r\nSparseTensor(indices=Tensor(\"SparseTensor/indices:0\", shape=(33, 2), dtype=int64), values=Tensor(\"SparseTensor/values:0\", shape=(33,), dtype=int32), dense_shape=Tensor(\"SparseTensor/dense_shape:0\", shape=(2,), dtype=int64))\r\n\r\nSparseTensor(indices=Tensor(\"Where:0\", shape=(?, 2), dtype=int64), values=Tensor(\"GatherNd:0\", shape=(?,), dtype=int32), dense_shape=Tensor(\"Shape:0\", shape=(2,), dtype=int64))\r\n```\r\n\r\nI guess this is the case of ctc_loss not being able to handle dynamic shapes for indices, values, and dense_shape for its labels parameter.", "+1 for having densified versions of the ctc_loss and decoders. ", "I'm stuck right now D:", "I already got it working! Turns out, the `sequence_length` parameter I was passing is a scalar but it should be a 1-d vector according to the [documentation](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/connectionist_temporal_classification__ctc_#ctc_loss):\r\n\r\n> sequence_length: 1-D int32 vector, size [batch_size]. The sequence lengths. \r\n\r\nThe code for converting a dense tensor into a sparse one is as follows:\r\n\r\n```\r\ndef dense_to_sparse(dense_tensor, out_type):\r\n    indices = tf.where(tf.not_equal(dense_tensor, tf.constant(0, dense_tensor.dtype)\r\n    values = tf.gather_nd(dense_tensor, indices)\r\n    shape = tf.shape(dense_tensor, out_type=out_type)\r\n    return tf.SparseTensor(indices, values, shape)\r\n```\r\n\r\nNow the questions is whether to have this op exist or just put it inside `tf.nn.ctc_loss`.,, or maybe both. Thoughts, anyone?\r\n\r\n@Holded ", "Your code makes the assumption that 0 signifies the EOS token, right?  I\nsuppose that's arbitrary, and would make that a required argument of the\nfunction.  I'd put it in tf.contrib.layers as ctc_loss_dense_labels or\nsomething.\n\nOn Sat, Jan 13, 2018 at 3:26 PM, Jerome <notifications@github.com> wrote:\n\n> I already got it working! Turns out, the sequence_length parameter I was\n> passing is a scalar but it should be a 1-d vector according to the\n> documentation\n> <https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/connectionist_temporal_classification__ctc_#ctc_loss>\n> :\n>\n> sequence_length: 1-D int32 vector, size [batch_size]. The sequence lengths.\n>\n> The code for converting a dense tensor into a sparse one is as follows:\n>\n> def dense_to_sparse(dense_tensor, out_type):\n>     indices = tf.where(tf.not_equal(dense_tensor, tf.constant(0, dense_tensor.dtype)\n>     values = tf.gather_nd(dense_tensor, indices)\n>     shape = tf.shape(dense_tensor, out_type=out_type)\n>     return tf.SparseTensor(indices, values, shape)\n>\n> Now the questions is whether to have this op exist or just put it inside\n> tf.nn.ctc_loss. Thoughts, anyone?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15985#issuecomment-357475661>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6sVq2LHUGsRN2LjQf5iOrxmY6usks5tKTu7gaJpZM4RYleR>\n> .\n>\n", "What do you mean by EOS? ", "End of sentence\n\nOn Sun, Jan 14, 2018, 2:41 AM Jerome <notifications@github.com> wrote:\n\n> What do you mean by EOS?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15985#issuecomment-357502700>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-nEIRbIcGjm7vdKIPrrbcPuP-NZks5tKdntgaJpZM4RYleR>\n> .\n>\n", "Apparently, that number is not arbitrary according to this comment in this SO [post](https://stackoverflow.com/questions/39838234/sparse-matrix-from-a-dense-one-tensorflow/41676814?noredirect=1#comment83489373_41676814):\r\n\r\n> A sparse tensor is defined through its nonzero values (in TensorFlow, but also in about any algebra package that defines sparse structures). A matrix or tensor is said to be sparse if most of its elements are zero, and so this representation is much more economical (although operating with it is generally more complex).", "Yes but when you start with a dense tensor, the zero value can mean\nanything when it comes to CTC labels.  0 does not always mean end of\nsentence or padding. For example, -1 is also a perfectly reasonable padding\nlabel value.\n\nOn Sun, Jan 14, 2018, 6:10 PM Jerome <notifications@github.com> wrote:\n\n> Apparently, that number is not arbitrary according to this comment in this\n> SO post\n> <https://stackoverflow.com/questions/39838234/sparse-matrix-from-a-dense-one-tensorflow/41676814?noredirect=1#comment83489373_41676814>\n> :\n>\n> A sparse tensor is defined through its nonzero values (in TensorFlow, but\n> also in about any algebra package that defines sparse structures). A matrix\n> or tensor is said to be sparse if most of its elements are zero, and so\n> this representation is much more economical (although operating with it is\n> generally more complex).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15985#issuecomment-357565573>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4-9nwJRy2W2ifW3_BonK6BuxEcUks5tKrOngaJpZM4RYleR>\n> .\n>\n", "Ohhh, I see. I'll work on that Pull Request now ", "Pull request made. https://github.com/tensorflow/tensorflow/pull/16119", "Is EOS the blank label index for ctc?", "Nope; EOS is not the blank label index.  The blank label index is an index\nemitted by logits to signify \"skip emitting a character this time step\".\nIt should never appear in the label tensor.  The EOS signifies the end of a\nsentence.  Now that I think about it, EOS is usually part of the target\nlabel; and padding should be some other value (not EOS, and obviously not\nthe blank label index).\n\nOn Tue, Jan 16, 2018 at 1:07 AM, Jerome <notifications@github.com> wrote:\n\n> Is EOS the blank label index for ctc?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15985#issuecomment-357896332>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxIrkv3UY989UEs46RHSB60H2xh8ks5tLGbagaJpZM4RYleR>\n> .\n>\n", "Let's say I have a target label \"apple\". If I were to encode it, should it be like this:\r\n\r\n`[0, 15, 15, 12, 5, 27, 26, 26, 26]` # 27 is the eos_token, 26 is the padding\r\n\r\nIs it correct to say that there are 27 classes for ctc (the letters + the blank label) and the eos token should not be in those classes?", "\"the eos token should not be in those classes\"?  no.  the padding token\nshould not be in those classes.  the eos token should be in them.\n\nvalid label:\n\n[0, 15, 15, 12, 5, 27, -1, -1, -1]  # 27 is EOS token, -1 is padding\n\n29 classes for ctc logits outputs: 0, ..., 27 (eos), 28 (blank)\n\nOn Wed, Jan 17, 2018 at 3:34 PM, Jerome <notifications@github.com> wrote:\n\n> Let's say I have a target label \"apple\". If I were to encode it, should it\n> be like this:\n>\n> [0, 15, 15, 12, 5, 27, 26, 26, 26] # 27 is the eos_token, 26 is the\n> padding\n>\n> Is it correct to say that there are 27 classes for ctc (the letters + the\n> blank label) and the eos token should not be in those classes?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15985#issuecomment-358485804>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3Pm6x8pXMRwH-3hTl9Mbe_RG6Y-ks5tLoOhgaJpZM4RYleR>\n> .\n>\n", "Oh, I see. Thank you for clearing it to me :D ", "I am using the following code as a workaround.\r\n\r\n```\r\ndef dense_to_sparse(dense_tensor, sequence_length):\r\n    indices = tf.where(tf.sequence_mask(sequence_length))\r\n    values = tf.gather_nd(dense_tensor, indices)\r\n    shape = tf.shape(dense_tensor, out_type=tf.int64)\r\n    return tf.SparseTensor(indices, values, shape)\r\n```\r\n", "What does sequence_length mean here? ", "sequence_length is the 1-D tensor containing the lengths of the labels in the batch.\r\n\r\nsequence_length may be of shape [batch_size], and labels (dense_tensor) may be of shape [batch_size, max_length_of_the_labels].\r\n\r\nThe code has already been tested.\r\n", "I have already implemented `dense_to_sparse` that passes an `eos_token`. What do you intend to gain from passing `sequence_length` instead of an `eos_token`? @pozhijisi ", "@ebrevdo -1 is not really allowed as padding. ctc_loss throws this error: `All labels must be nonnegative integers`", "Also, having an eos_token assumes that an eos_token should be appended to each label.", "Proposed implementations raises error with tensor of rank 0", "Which proposed implementation?", "Nothing to do right now.\r\nIt seems that scalar could not be represented within SparseTensor. Unfortunately that is SparseTensor behaviour.\r\n", "I guess error handling can be applied such that scalars can't be accepted into `dense_to_sparse`.", "@selcouthlyBlue I am using an implementation very much like @pozhijisi. I need this functionality for something unrelated to CTCs (namely, tf.edit_distance). I think it makes sense for the function to use either `eos_token` or `sequence_lengths` (or even a mask directly, as that is what I actually have)."]}, {"number": 15984, "title": "Fix r1.5 tests.", "body": "PiperOrigin-RevId: 181212111", "comments": []}, {"number": 15983, "title": "Feature request: Reduce learning rate on plateau", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes. But applies to stock examples as well.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04.\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary.\r\n- **TensorFlow version (use command below)**:\r\nv1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: \r\nPython 3.5.4\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nCUDA: V8.0.61\r\ncuDNN: 6.0.21\r\n- **GPU model and memory**:\r\nGTX 1080Ti 11GB running driver version 384.98\r\n- **Exact command to reproduce**:\r\nN/A. However, I am using the Experiment, Estimator and Dataset APIs in order to do training.\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI would like to reduce the learning rate during training. However, I do not want to treat this as another tunable hyperparameter, so I would like this to be based on performance plateauing. In Keras, it is easy to implement learning rate reduction by monitoring the validation loss using the ReduceLROnPlateau callback function, but in TensorFlow this does not seem to be the case/easy. I certainly haven't found any implementation of this, so I propose this as a feature request.\r\n\r\nFeel free to close this if this is not the correct forum.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@martinwicke does it seems likes a feature we would want to have or there is any workaround for it?", "The answer depends on how you are using TensorFlow. \r\n\r\nIf you are writing plain low-level TensorFlow code and your own training loop (e.g., using MonitoredTrainingSessions interleaved with validation, this is very straightforward. The learning rate is a parameter to the optimizers, which is a Tensor. The Tensor could be computed any way you like to, including any sort of plateau detection (presubmably, by comparing two rolling averages of the loss), and if it triggers, reducing the learning rate. We may not have a function that does exactly what you want, but it is easy to write. \r\n\r\nIf you are using an Estimator, you do not have easy access to the validation loss inside the training loop. This is because in distributed training, evaluation is performed on a separate worker, and in an isolated environment, while training continues on the other workers. If you want to implement learning rate decay based on evaluation loss there, you would write a hook which reads the loss from the evaluation event file, does whatever logic it needs to, and assigns a new learning rate to a training variable, which in turn is used as the learning rate in the optimizer. This is not hard to do, but it is not done.\r\n\r\nOf course, you can also use tf.keras if that works for you, that's why it's there.", "@martinwicke Thanks for the nice response. I am using the Experiment/Estimator combo, though I am not planning to do distributed training. \r\n\r\nHowever, wouldn't it be possible to create a loop over a training and evaluation phase using Estimator alone, grab the loss from the evaluate() call, determine learning rate decay and then reset the learning rate for the subsequent training/evaluation phases?\r\n\r\nSomething like this pseudo:\r\n```\r\n# Define model\r\nmodel = tf.estimator.Estimator(...)\r\n\r\n# Cycle over epochs\r\nfor cycle in num_cycles:\r\n    model.train(...)\r\n\r\n    # Grab loss\r\n    loss = model.evaluate(...)\r\n\r\n    # Determine if hit plateau:\r\n    if hit_plateau:\r\n        reduce_learning_rate()\r\n```\r\nWhere of course the `reduce_learning_rate()` function would impute the new learning rate value to the learning rate tensor. Would that be a possible workaround?\r\n\r\nEDIT: by the way, I am not finding a lot of documentation on how to use the tf.keras module. Is it possible to use the TFRecords API with tf.keras yet, or is there no significant performance gain to this application?", "TFRecords and Keras is possible, but not necessarily simple.\r\n\r\nThe pseudocode you describe is possible. You can add learning rate to the params arg (which is passed to the model_fn). \r\n\r\nThen, make a new estimator in each iteration of the loop. The Estimator will read the last state from disk anyway every time you call one of its methods. \r\n\r\nIt would look like this:\r\n\r\n```\r\ndef model_fn(..., params):\r\n    # Do something with params['learning_rate']\r\n    ...\r\n\r\nlr = INITIAL_LEARNING_RATE\r\n\r\n# Cycle over epochs\r\nfor cycle in num_cycles:\r\n    # Define model\r\n    model = tf.estimator.Estimator(model_fn=model_fn, ..., params={'learning_rate': lr})\r\n\r\n    model.train(...)\r\n\r\n    # Grab loss\r\n    loss = model.evaluate(...)\r\n\r\n    # Determine if hit plateau:\r\n    if hit_plateau:\r\n        lr = reduce_learning_rate(lr)\r\n```\r\n\r\nNote that constructing the Estimator is effectively free. Calling train is a loop is not something I recommend, but for local training it's ok, especially if you only call it once per epoch.", "Hey,\r\nIs there any intention to implement ReduceLROnPlateau that is integrated with tf.estimators? or your recommendation is to use tf.keras in order to achieve that behavior?\r\nThanks", "+1"]}, {"number": 15982, "title": "Add broadcasting support for `tf.where`", "body": "Adds where_v2 (which will be where in TF 2.0), which has numpy's broadcasting semantics.\r\n\r\nThis fix fixes #9284.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@aselle WDYT?", "@ebrevdo The PR has been updated with `broadcast` attribute removed. Please take a look.", "@ebrevdo The PR has been rebase to merge conflict.\r\n\r\nHowever, after reviewing the code again, I realized that there is still one scenario where exiting `tf.where` is not compatible with `np.where` (below is the output before this PR):\r\n```\r\n$ python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18)\r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> import numpy as np\r\n>>>\r\n>>> x = np.arange(4)\r\n>>> y = np.zeros((4, 4))\r\n>>> z = np.ones((4, 4))\r\n>>>\r\n>>> np.where(x > 1, y, z)\r\narray([[1., 1., 0., 0.],\r\n       [1., 1., 0., 0.],\r\n       [1., 1., 0., 0.],\r\n       [1., 1., 0., 0.]])\r\n>>> v = tf.where(x > 1, y, z)\r\n>>> tf.Session().run(v)\r\narray([[1., 1., 1., 1.],\r\n       [1., 1., 1., 1.],\r\n       [0., 0., 0., 0.],\r\n       [0., 0., 0., 0.]])\r\n>>>\r\n```\r\n\r\nAs is shown from the above example, when x has the shape `(4)` and y/z has the shape `(4, 4)`, the broadcast orientation is different.\r\n\r\nBecause of that, the current PR will fail several test cases.\r\n\r\nUnfortunately, I couldn't think of a way to make the proposed broadcasting changes in `tf.where` backward-compatible.\r\n\r\nI am wondering maybe it would be better to name a new op (e.g., `tf.where_v2`) so that it is compatible with numpy and not breaking existing users?\r\n", "Status?", "Any updates? Also as in the issue someone mentioned:\r\n\r\n> There is also not full support for broadcasting in the other way, when condition is smaller than x or y; as indicated in the docs it only works if condition is a vector, but it could be extended to work with condition matching an arbitrary number of first dimensions of x and y (I'm not sure if this is considered also \"broadcasting\") and/or broadcasting singleton dimensions.\r\n\r\nWould this also be implemented?", "I think if you'd like the interface to exactly match np.where, it does make sense to create a new op + kernel, v2.  you would want to expose it in tf.contrib somewhere (not in core).", "\"If x and y are vectors of higher rank, then condition must be either a vector with size matching the first dimension of x, or must have the same shape as x.\"\r\n\r\nCould we deprecate this behavior from `tf.where` (start issuing `FutureWarning`) and change it in some future API breaking release?\r\n\r\nTo ease the transition, we could safely add broadcasting support when the total number of dimensions match between all arguments, or add a function to contrib (temporarily) with the appropriate behavior.\r\n\r\nDeviating from NumPy's broadcasting rules feels like a design mistake to me, and I suspect this will be a repeated source of confusion in the future.", "Sorry for the delay. The PR has been updated. Now a new op `tf.contrib.framework.where` is exposed so that the broadcast rule follows numpy conventions. The original `tf.where` remains intact. Please take a look.", "I rebased the PR to resolve the merge conflict though it looks like there are some build failures after that. Will take a look and update the PR shortly to fix the build.", "Status guys?", "The PR has been rebased with build error fixed. All test passed now. Sorry for the long wait.", "@ebrevdo Please take a look and see if all issues have been addressed.", "Would love to see this feature added if possible.", "+Martin for API review", "Sorry for the delay!\r\n\r\nFor API review:\r\n\r\nChanges to contrib do not require API review. That being said, we'd like to understand what is going on here. Are TF's broadcasting rules in general different than numpy or just for `where`? Did `where` have some custom broadcasting logic instead of using the same broadcasting library that other TF ops use? If so why is `where` special (e.g. is the difference that `where` takes three arguments and most of our ops that support broadcasting take two)?\r\n\r\nThanks!", "I think generally tf had very little broadcasting. As part of making things more numpy like, we changed select to be called where. Thus would reasonably expect broadcasting to happen. There are lots of corner cases of ops that don't support broadcasting in the same way. Another is tf.constant() which takes a shape but the tf behavior is when the shape doesn't match, it just pads with the last value where as in numpy constant will use broadcasting semantics to match the shape. ", "For API review:\r\n\r\nContrib is on the chopping block. If we want this behavior, we can't make it in v1.x because it is a backwards-incompatible change. Proposal: export this using: `@tf_export(v1=\"where_v2\", v2=\"where\")` and add an `@deprecation` to the current version of `where`.", "I'd like to see how one would do batch major broadcasting with this version\nof tf.where.  that's by far the most useful version and is the one we\ncurrently have.\n\nOn Wed, Oct 3, 2018, 1:31 PM josh11b <notifications@github.com> wrote:\n\n> For API review:\n>\n> Contrib is on the chopping block. If we want this behavior, we can't make\n> it in v1.x because it is a backwards-incompatible change. Proposal: export\n> this using: @tf_export(v1=\"where_v2\", v2=\"where\") and add an @deprecation\n> to the current version of where.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15982#issuecomment-426789882>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2Lr2KqipOaBfD9UiDmNj2nQ7aZiks5uhR6FgaJpZM4RYgtJ>\n> .\n>\n", "I see... if we believe that the two broadcasting modes should be\naccessible, we could alternative introduce an argument which switches the\nbehavior. In 1.x, the default must be the current behavior, in 2.0 we could\ndecide to switch the behavior, if we think consistency with numpy outweighs\nthe utility argument.\n", "You could do \"batch major broadcasting\" the same way you do it for other\nTensorFlow ops: you would explicitly expand dimensions first, e.g., with\ntf.new_axis or tf.reshape.\nOn Thu, Oct 4, 2018 at 12:18 AM Martin Wicke <notifications@github.com>\nwrote:\n\n> I see... if we believe that the two broadcasting modes should be\n> accessible, we could alternative introduce an argument which switches the\n> behavior. In 1.x, the default must be the current behavior, in 2.0 we could\n> decide to switch the behavior, if we think consistency with numpy outweighs\n> the utility argument.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15982#issuecomment-426822330>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABKS1nRkOn5--7LUNBXnir6wgveDDBRjks5uhTehgaJpZM4RYgtJ>\n> .\n>\n", "This SGTM.  We'll have to update a number of internal users, especially\nsome RNN code, when moving to TF 2.0 if we make this the default.\n\nOn Wed, Oct 3, 2018 at 10:16 PM, Stephan Hoyer <notifications@github.com>\nwrote:\n\n> You could do \"batch major broadcasting\" the same way you do it for other\n> TensorFlow ops: you would explicitly expand dimensions first, e.g., with\n> tf.new_axis or tf.reshape.\n> On Thu, Oct 4, 2018 at 12:18 AM Martin Wicke <notifications@github.com>\n> wrote:\n>\n> > I see... if we believe that the two broadcasting modes should be\n> > accessible, we could alternative introduce an argument which switches the\n> > behavior. In 1.x, the default must be the current behavior, in 2.0 we\n> could\n> > decide to switch the behavior, if we think consistency with numpy\n> outweighs\n> > the utility argument.\n> >\n> > \u2014\n> > You are receiving this because you commented.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/pull/\n> 15982#issuecomment-426822330>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/ABKS1nRkOn5--\n> 7LUNBXnir6wgveDDBRjks5uhTehgaJpZM4RYgtJ>\n>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15982#issuecomment-426888065>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzeCkUUyVwmQ6CGi35ZmFjCEEJQQks5uhZm-gaJpZM4RYgtJ>\n> .\n>\n", "can you add a test:\n\npred = np.array([1, 0, 0]).reshape((3, 1))\na = np.random.randn((3, 4))\nb = np.random.randn((3, 4))\n\nout = self.evaluate(pred, a, b)\n\nassertAllClose(out, np.where(pred, a, b))\n\nOn Thu, Oct 4, 2018 at 10:17 AM, Eugene Brevdo <ebrevdo@google.com> wrote:\n\n> This SGTM.  We'll have to update a number of internal users, especially\n> some RNN code, when moving to TF 2.0 if we make this the default.\n>\n> On Wed, Oct 3, 2018 at 10:16 PM, Stephan Hoyer <notifications@github.com>\n> wrote:\n>\n>> You could do \"batch major broadcasting\" the same way you do it for other\n>> TensorFlow ops: you would explicitly expand dimensions first, e.g., with\n>> tf.new_axis or tf.reshape.\n>> On Thu, Oct 4, 2018 at 12:18 AM Martin Wicke <notifications@github.com>\n>> wrote:\n>>\n>> > I see... if we believe that the two broadcasting modes should be\n>> > accessible, we could alternative introduce an argument which switches\n>> the\n>> > behavior. In 1.x, the default must be the current behavior, in 2.0 we\n>> could\n>> > decide to switch the behavior, if we think consistency with numpy\n>> outweighs\n>> > the utility argument.\n>> >\n>> > \u2014\n>> > You are receiving this because you commented.\n>> > Reply to this email directly, view it on GitHub\n>> > <https://github.com/tensorflow/tensorflow/pull/15982#\n>> issuecomment-426822330>,\n>> > or mute the thread\n>> > <https://github.com/notifications/unsubscribe-auth/\n>> ABKS1nRkOn5--7LUNBXnir6wgveDDBRjks5uhTehgaJpZM4RYgtJ>\n>>\n>> > .\n>> >\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/15982#issuecomment-426888065>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtimzeCkUUyVwmQ6CGi35ZmFjCEEJQQks5uhZm-gaJpZM4RYgtJ>\n>> .\n>>\n>\n>\n", "@ebrevdo The PR has been updated with additional unit test. Please take a look.", "@ebrevdo @martinwicke Thanks for the help, the PR has been rebased to resolve the merge conflict. \r\n\r\nPlease take a look and see if any additional change is needed.", "Thanks @martinwicke for the review. The PR has been updated with where_v2 exposed in @tf_export(v1=[\"where_v2\"], v2=[\"where\"]). The deprecation has also been added.\r\n\r\nThe issue with the `select_v2` was that I didn't add the `HIDDEN` in the api refs (the api systems have changed over time, so it is different from the time this PR is created). I have updated the api defs with `HIDDEN` added.\r\n\r\nPlease take a look and let me know if there are any further issues.\r\n", "Ah. v2= does nothing. @annarev, we should probably error out if we find a\nnamed arg that's not useful.\n\nChange the second export to tf_export(\"where\", v1=[\"where_v2\"]), then it\nshould work.\n", "Thanks @martinwicke. The PR has been updated with `tf_export(\"where\", v1=[\"where_v2\"])` used for where_v2 and `tf_export(v1=[\"where\"])` used for where (legacy) implementations. Please take a look.", "@martinwicke @rthadur Wondering if there is any update on this PR? It would be good to see this PR in for v2, so that the broadcasting rule is consistent across different ops.", "@dksb Do you think it makes sense to scrub all PRs assigned to someone not on your team and assign one of your team to make sure they don't get lost like this one?", "> @dksb Do you think it makes sense to scrub all PRs assigned to someone not on your team and assign one of your team to make sure they don't get lost like this one?\r\n\r\n@yongtang @martinwicke @dksb  will take care of this PR. Feel free to assign me if you have such PR", "Oh, once I find the PR it's fine. It's more about the ones which I don't find. I usually find them only after the author pings me (or the PR), and that's usually after a long delay.", "Rasmus, can you review the kernel code? I'll review the API bits", "Thanks @alextp for the review. There is a slight different in a corner case between v1 and v2. In v2, the broadcasting rule matches `np.where` and default broadcast rule in tensorflow. The v1 version of where is slightly different:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nx = np.arange(4)\r\ny = np.zeros((4, 4))\r\nz = np.ones((4, 4))\r\n# where_v2 matches np.where:\r\nnp.where(x > 1, y, z)\r\n\r\n# Output:\r\narray([[1., 1., 0., 0.],\r\n       [1., 1., 0., 0.],\r\n       [1., 1., 0., 0.],\r\n       [1., 1., 0., 0.]])\r\n\r\n# where (v1):\r\nv = tf.where(x > 1, y, z)\r\ntf.Session().run(v)\r\n\r\n# Output:\r\narray([[1., 1., 1., 1.],\r\n       [1., 1., 1., 1.],\r\n       [0., 0., 0., 0.],\r\n       [0., 0., 0., 0.]])\r\n```", "Another way of looking at it is the old one broadcasts by batch and the new\none by depth (and more generally)\n\nOn Fri, Mar 29, 2019, 3:33 PM Yong Tang <notifications@github.com> wrote:\n\n> Thanks @alextp <https://github.com/alextp> for the review. There is a\n> slight different in a corner case between v1 and v2. In v2, the\n> broadcasting rule matches np.where and default broadcast rule in\n> tensorflow. The v1 version of where is slightly different:\n>\n> import tensorflow as tfimport numpy as np\n> x = np.arange(4)\n> y = np.zeros((4, 4))\n> z = np.ones((4, 4))# where_v2 matches np.where:\n> np.where(x > 1, y, z)\n> # Output:\n> array([[1., 1., 0., 0.],\n>        [1., 1., 0., 0.],\n>        [1., 1., 0., 0.],\n>        [1., 1., 0., 0.]])\n> # where (v1):\n> v = tf.where(x > 1, y, z)\n> tf.Session().run(v)\n> # Output:\n> array([[1., 1., 1., 1.],\n>        [1., 1., 1., 1.],\n>        [0., 0., 0., 0.],\n>        [0., 0., 0., 0.]])\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15982#issuecomment-478169199>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7lthGHjADx_Jc2xysvFRmv05XG2ks5vbpTAgaJpZM4RYgtJ>\n> .\n>\n", "> Looks good to me. @rmlarsen , please approve if the kernel code looks good to you\r\n\r\n@rmlarsen gentle ping ", "@rmlarsen if we're gonna get this in we should do it soon.  TF-Agents relies on this broadcasting code and we're gonna have to make changes for tf2.0.", "@martinwicke @rmlarsen @alextp Any chance to see this PR in 2.0? Really would like to see the consistency of broadcast rules across the board. (The 1.x tf.where broadcast rule is not compatible with the rest of the ops, and that could be some confusion for users).", "Looking now.", "@rmlarsen Thanks for the review. The PR has been updated. Please take a look and let me know if there are any issues.", "Since this has been going on for so long, I'd favor a cherry pick for this.\n\nOn Fri, May 3, 2019, 16:01 Alexandre Passos <notifications@github.com>\nwrote:\n\n> *@alextp* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/array_ops.py\n> <https://github.com/tensorflow/tensorflow/pull/15982#discussion_r280951066>\n> :\n>\n> > @@ -3177,6 +3181,46 @@ def where(condition, x=None, y=None, name=None):\n>      raise ValueError(\"x and y must both be non-None or both be None.\")\n>\n>\n> +@tf_export(\"where\", v1=[\"where_v2\"])\n> +def where_v2(condition, x=None, y=None, name=None):\n>\n> Since I wrote this message the tf v2 API has been frozen for the 1.14\n> release. This means we'll need to export this symbol as where_v2 both in\n> tf1 and in tf2 :-/\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15982#discussion_r280951066>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAEM57N3MKTMXHZBAVTHJQDPTS737ANCNFSM4ELCBNEQ>\n> .\n>\n", "We had to roll this back because the tests had no adequate coverage and missed things such as the broadcasting selectv2 op having no gradient defined for it.", "@alextp sorry about that. I will take a look to add grad and resubmit the PR later.", "I have a suggestion for the gradient, haven't tested it but maybe it gets you started.\r\n\r\n```python\r\n\r\n@ops.RegisterGradient(\"SelectV2\")\r\ndef _SelectGrad(op, grad):\r\n  c = op.inputs[0]\r\n  x = op.inputs[1]\r\n  y = op.inputs[2]\r\n  zeros = array_ops.zeros([], dtype=grad.dtype.base_dtype)\r\n  gx = array_ops.where_v2(c, grad, zeros)\r\n  gx_shape = array_ops.shape(gx)\r\n  x_shape = array_ops.shape(x)\r\n  rankdiff_x = array_ops.rank(gx) - array_ops.rank(x)\r\n  # Reduce away broadcasted leading dims.\r\n  gx = math_ops.reduce_sum(gx, axis=math_ops.range(rankdiff_x))\r\n  # Reduce but keep x's 1-valued dims which were broadcast.\r\n  gx = math_ops.reduce_sum(\r\n      gx, keepdims=1, axis=array_ops.where(grad_shape[rankdiff_x:] > x_shape))\r\n\r\n  gy = array_ops.where_v2(c, zeros, grad)\r\n  gy_shape = array_ops.shape(gy)\r\n  y_shape = array_ops.shape(y)\r\n  rankdiff_y = array_ops.rank(gy) - array_ops.rank(y)\r\n  # Reduce away broadcasted leading dims.\r\n  gy = math_ops.reduce_sum(gy, axis=math_ops.range(rankdiff_y))\r\n  # Reduce but keep y's 1-valued dims which were broadcast.\r\n  gy = math_ops.reduce_sum(\r\n      gy, keepdims=1, axis=array_ops.where(grad_shape[rankdiff_y:] > y_shape))\r\n\r\n  return (None, gx, gy)\r\n```", "@yongtang any update on this? This was rolled back ", "It's supported in tf.compat.v2.where\n\nOn Wed, Dec 11, 2019, 7:43 AM Tejas Lodaya <notifications@github.com> wrote:\n\n> @yongtang <https://github.com/yongtang> any update on this? This was\n> rolled back\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15982?email_source=notifications&email_token=AFJFSIY2TJ6NWWAGUSFZJDLQYDN5VA5CNFSM4ELCBNE2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGS672A#issuecomment-564522984>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFJFSI3P5IYRBTGAJK6GOSLQYDN5VANCNFSM4ELCBNEQ>\n> .\n>\n", "@tejaslodaya It was rolled back, then rolled forward (with the help from @brianwa84 for providing the gradient op \ud83d\udc4d \u2764\ufe0f ). It is now available in 2.0.", "I tried and it works! For anyone coming to this PR, here's how you do it\r\n\r\nBefore (TF 1.x)-\r\n```python\r\nwith tf.Session() as sess:  \r\n    col = tf.convert_to_tensor([1,2,3,4,5,6,7,8,9,10,11,12])    \r\n    print(tf.where(tf.math.greater(col, 10),\r\n                  tf.zeros_like(col),\r\n                  tf.ones_like(col)).eval())\r\n```\r\n> [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\r\n\r\nAfter (TF 2.x)-\r\n```python\r\nimport tensorflow as tf\r\ncol = [1,2,3,4,5,6,7,8,9,10,11,12]\r\nprint(tf.where(tf.math.greater(col, 10),\r\n              tf.zeros([1]),\r\n               tf.ones([1])))\r\n```\r\n> tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.], shape=(12,), dtype=float32)\r\n\r\n\r\nNotice, I had to do zeros_like and broadcast it to the shape of the column to make it work in 1.x\r\n\r\nThanks @yongtang , great work!\r\n\r\n\r\n\r\n\r\n"]}, {"number": 15981, "title": "tf.Estimator creates loss and loss_1 for eval/train", "body": "- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.12\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.4\r\n- **Python version**: \r\n2.7\r\n\r\n### Describe the problem\r\n\r\nWhen using the tf.Estimator, the summary files save out summaries for the loss variable evaluated every checkpoint.  The summary for the training, is saved as 'loss_1' .  I got this tensorboard by running the ciphar10_estimator code located: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator/\r\n\r\nThis makes it difficult to compare the eval/train loss on the same graph in tensorboard.  What causes this naming issue and what can be done to fix it?\r\n\r\nThanks!\r\n\r\n![screen shot 2018-01-09 at 12 02 53 pm](https://user-images.githubusercontent.com/22623388/34740703-311a2fc6-f535-11e7-9f88-ab16f65052ee.png)\r\n\r\n  ", "comments": ["So you think \u02cbloss\u02cb and \u02cbloss_1\u02cb are not good name? What name do you prefer? ", "loss is a fine name,  I think they should both be 'loss' so that they appear on the same plot and are able to see how training loss compares to test loss.   I found a fix for the ciphar code by adding a name-scope thats based on the training mode.  the loss_1 name was coming in due to the 'loss' op already existing.", "So you request to plot `loss` and `loss_1` in the same figure, right? \r\nIt seems not easy after I check #7089 #329", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15980, "title": "While loop randomly doesn't evaluate tensors", "body": "Hello!\r\nI believe to have found a bug in Tensorflow when running the code below. I am currently trying to build a neural transducer, and have stumbled across TF sometimes not returning any values for a tensor. I have not had the chance yet to test this out on another machine (no GPU, TF 1.4.1, Ubuntu 17.10). The code is redacted a bit to highlight only the parts that fail. [I've also posted to StackOverflow](https://stackoverflow.com/questions/48081063/tensorflow-non-deterministic-behaviour-with-large-model-using-while-loop) but didn't get any response there.\r\n\r\nNotes:\r\n\r\n- I believe the bug occurs around line 160, in the body of the while loop in the function run_full_transducer\r\n- The session is returning [encoder_outputs, transducer_outputs]\r\n- I do not use random functions\r\n- As far as I can tell, if I remove the Print OP in line 164, the output is always 0\r\n\r\nExample of a correct return value (more or less):\r\n```\r\narray([[[ 0.00811536, -0.00200322, -0.01177037,  0.03676344, -0.01909475,\r\n             -0.03157664,  0.026092  ,  0.02367685, -0.01894805,  0.02832799,\r\n              0.0377345 , -0.02583589, -0.02908566,  0.0299024 ,  0.00518877,\r\n             -0.00064737,  0.01431572, -0.01053502, -0.01783628, -0.00382657,\r\n              0.00076749, -0.02705991,  0.00112415, -0.0193013 ,  0.02346764,\r\n              0.03014467,  0.02663364,  0.02503882,  0.03362656, -0.01877708,\r\n              0.01859642,  0.02460729, -0.01395229, -0.03033791,  0.01177907,\r\n             -0.03049169, -0.00389978,  0.02221515, -0.00073605,  0.01248251,\r\n              0.00424051,  0.01070387,  0.02818898,  0.0321721 , -0.02462685,\r\n              0.03495178, -0.02408989, -0.02742486,  0.00331823, -0.02311424,\r\n             -0.01327039,  0.01095297,  0.02584363,  0.02083527, -0.01588045,\r\n              0.02837921,  0.02100117,  0.00918638,  0.00109535, -0.02965789,\r\n              0.01040822, -0.03240473,  0.00453057, -0.00603903]],\r\n    \r\n           [[ 0.01053647, -0.00457577, -0.01939731,  0.06317309, -0.03113565,\r\n             -0.05525927,  0.04647589,  0.04213476, -0.03498235,  0.04962765,\r\n              0.05989208, -0.04340284, -0.04777668,  0.05346756,  0.00395604,\r\n             -0.0005207 ,  0.02079381, -0.01424338, -0.02584206, -0.00530154,\r\n             -0.00031365, -0.04966826, -0.00091683, -0.03025239,  0.04526306,\r\n              0.0595435 ,  0.0463665 ,  0.04578522,  0.05916505, -0.031725  ,\r\n              0.03164144,  0.04257958, -0.02865831, -0.04795898,  0.01856991,\r\n             -0.05512668, -0.00730711,  0.03953242,  0.00017992,  0.01710426,\r\n              0.00754557,  0.01975578,  0.0469296 ,  0.05237873, -0.04435374,\r\n              0.05924731, -0.04474678, -0.04605344,  0.00947831, -0.04284734,\r\n             -0.01979787,  0.02003288,  0.04196753,  0.03900779, -0.02887472,\r\n              0.05130195,  0.03419674,  0.0105699 ,  0.001114  , -0.0524303 ,\r\n              0.01738651, -0.06084244,  0.01364262, -0.01153531]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]\r\n```\r\nIncorrect:\r\n```\r\n [array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\r\n              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\r\n              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\r\n              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\r\n              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\r\n    \r\n           [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\r\n              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\r\n              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\r\n              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\r\n              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]\r\n```\r\n\r\nCode:\r\n``` python\r\n import tensorflow as tf\r\n    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\r\n    from tensorflow.python.layers import core as layers_core\r\n    import numpy as np\r\n    # NOTE: Time major\r\n    \r\n    # Constants\r\n    input_dimensions = 1\r\n    vocab_size = 3\r\n    input_embedding_size = 20\r\n    encoder_hidden_units = 64\r\n    inputs_embedded = True\r\n    transducer_hidden_units = 64\r\n    batch_size = 1\r\n    GO_SYMBOL = vocab_size - 1  # TODO: Make these constants correct\r\n    END_SYMBOL = vocab_size\r\n    input_block_size = 2\r\n    log_prob_init_value = 0\r\n    \r\n    \r\n    # ---------------- Helper classes -----------------------\r\n    \r\n    \r\n    # ----------------- Model -------------------------------\r\n    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\r\n    \r\n    \r\n    class Model(object):\r\n        def __init__(self):\r\n            self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \\\r\n            self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()\r\n            self.encoder_raw_outputs, self.trans_hidden_state, self.transducer_amount_outputs, \\\r\n            self.transducer_hidden_state_new, self.logits, self.decoder_prediction = self.build_transducer_model()\r\n    \r\n        def build_encoder_model(self):\r\n            encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),\r\n                                         dtype=tf.float32, name='encoder_inputs', trainable=False)\r\n            encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,\r\n                                                name='encoder_inputs_length', trainable=False)\r\n            encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,\r\n                                               name='encoder_hidden_state')  # Save the state as one tensor\r\n    \r\n            if inputs_embedded is True:\r\n                encoder_inputs_embedded = encoder_inputs\r\n            else:\r\n                encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\r\n    \r\n            # Build model\r\n            encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\r\n    \r\n            # Build previous state\r\n            encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)\r\n            encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])\r\n            encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])\r\n            encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)\r\n    \r\n            #   encoder_outputs: [max_time, batch_size, num_units]\r\n            encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(\r\n                encoder_cell, encoder_inputs_embedded,\r\n                sequence_length=encoder_inputs_length, time_major=True,\r\n                dtype=tf.float32, initial_state=encoder_hidden_state_t)\r\n    \r\n            # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.\r\n            encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)\r\n            encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])\r\n    \r\n            return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new\r\n    \r\n        def build_transducer_model(self):\r\n            encoder_raw_outputs = tf.Variable(tf.zeros(shape=(input_block_size, 1, encoder_hidden_units)),\r\n                                              dtype=tf.float32,\r\n                                              name='encoder_raw_outputs')\r\n            trans_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, transducer_hidden_units)),\r\n                                             dtype=tf.float32,\r\n                                             name='trans_hidden_state')  # Save the state as one tensor\r\n            transducer_amount_outputs = tf.Variable(0, dtype=tf.int32, name='transducer_amount_outputs',\r\n                                                    trainable=False)\r\n    \r\n            # Model building\r\n            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\r\n                embedding=embeddings,\r\n                start_tokens=tf.tile([GO_SYMBOL], [batch_size]),\r\n                end_token=END_SYMBOL)\r\n    \r\n            attention_states = tf.transpose(encoder_raw_outputs,\r\n                                            [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]\r\n    \r\n            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n                encoder_hidden_units, attention_states)\r\n    \r\n            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n                tf.contrib.rnn.LSTMCell(transducer_hidden_units),\r\n                attention_mechanism,\r\n                attention_layer_size=transducer_hidden_units)\r\n    \r\n            projection_layer = layers_core.Dense(vocab_size, use_bias=False)\r\n    \r\n            # Build previous state\r\n            trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)\r\n            trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, transducer_hidden_units])\r\n            trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, transducer_hidden_units])\r\n            trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)\r\n    \r\n            decoder = tf.contrib.seq2seq.BasicDecoder(\r\n                decoder_cell, helper,\r\n                decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),\r\n                output_layer=projection_layer)\r\n    \r\n            outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,\r\n                                                                                        output_time_major=True,\r\n                                                                                        maximum_iterations=transducer_amount_outputs)\r\n            logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]\r\n            decoder_prediction = outputs.sample_id  # For debugging\r\n    \r\n            # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.\r\n            transducer_hidden_state_new = tf.concat(\r\n                [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],\r\n                axis=0)\r\n            transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,\r\n                                                     shape=[2, -1, transducer_hidden_units])\r\n    \r\n            return encoder_raw_outputs, trans_hidden_state, transducer_amount_outputs, transducer_hidden_state_new, \\\r\n                   logits, decoder_prediction\r\n    \r\n    \r\n    model = Model()\r\n    \r\n    \r\n    # ----------------- Alignment -------------------------\r\n    \r\n    # ----------------- Training --------------------------\r\n    \r\n    def run_full_transducer():\r\n        # Inputs\r\n        max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')\r\n        inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,\r\n                                         name='inputs_full_raw')\r\n        transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,\r\n                                                 name='transducer_list_outputs')  # amount to output per block\r\n    \r\n        # Turn inputs into tensor which is easily readable\r\n        inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])\r\n    \r\n        # Outputs\r\n        outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)\r\n    \r\n        # Hidden states\r\n        # TODO: make these correct\r\n        encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))\r\n        trans_hidden_init = tf.ones(shape=(2, 1, transducer_hidden_units))\r\n    \r\n        init_state = (0, outputs_ta, encoder_hidden_init, trans_hidden_init)\r\n    \r\n        def cond(current_block, outputs_int, encoder_hidden, trans_hidden):\r\n            return current_block < max_blocks\r\n    \r\n        def body(current_block, outputs_int, encoder_hidden, trans_hidden):\r\n            # Process encoder\r\n            model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])\r\n            model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])\r\n            model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)\r\n    \r\n            # TODO: Error is SOMETIMES gone when using tf.Print\r\n            current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')\r\n            #current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')\r\n    \r\n            # Flow data from encoder to transducer\r\n            model.encoder_raw_outputs = model.encoder_raw_outputs.assign(model.encoder_outputs)\r\n            model.trans_hidden_state = model.trans_hidden_state.assign(trans_hidden)\r\n            model.transducer_amount_outputs = model.transducer_amount_outputs.assign(transducer_list_outputs[current_block])\r\n    \r\n            # Note the outputs\r\n            outputs_int = outputs_int.write(current_block, model.logits)\r\n    \r\n            return current_block + 1, outputs_int, model.encoder_hidden_state_new, model.transducer_hidden_state_new\r\n    \r\n        _, outputs_final, _, _ = tf.while_loop(cond, body, init_state)\r\n    \r\n        # Process outputs\r\n        outputs = outputs_final.stack()  # Now the outputs are of shape [block, amount_of_trans_out, batch_size, vocab]\r\n        outputs = tf.reshape(outputs, shape=(-1, 1, vocab_size))  # And now its [amount_outputs, batch_size, vocab]\r\n    \r\n        model.encoder_outputs = tf.Print(model.encoder_outputs, [model.encoder_outputs], message='Current block enc out: ')\r\n    \r\n        return max_blocks, inputs_full_raw, transducer_list_outputs, outputs, model.encoder_outputs\r\n    \r\n    # ---------------------- Testing -----------------------------\r\n    \r\n    \r\n    # ---------------------- Management -----------------------------\r\n    \r\n    init = tf.global_variables_initializer()\r\n    \r\n    with tf.Session() as sess:\r\n        sess.run(init)\r\n    \r\n        inp_max_blocks, inp_inputs_full_raw, inp_trans_list_out, out_outputs, enc_out = run_full_transducer()\r\n    \r\n        print sess.run([enc_out, out_outputs], feed_dict={\r\n            inp_max_blocks: 3,\r\n            inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions)),\r\n            inp_trans_list_out: [1, 3, 2]\r\n        })\r\n```\r\nSystem information:\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10 (Artful Aardvark)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  1.4.1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Execute the code block as a python file a few times\r\n\r\nThanks!\r\nNikita\r\n  \r\n  \r\n  ", "comments": ["I've just tested it on a different machine (Ubuntu, GPU enabled, TF 1.4.1) and I also get the same errors.", "@ebrevdo do you have any suggestions?", "@nikita68 this is a very involved example.  can you provide a much smaller, minimal, failure case?", "at least try running your while_loops with `parallel_iterations=1` since it looks like you're assigning values inside your body and this is going to happen concurrently and mess everything up :-p", "@ebrevdo I completely forgot about that parameter! Unfortunately, the error still persists. I'm trying to make a really small example showing where the code fails, but in the meantime here is a minimum version of the code above (the error is in the run_full_transducer.body function):\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\r\nfrom tensorflow.python.layers import core as layers_core\r\nimport numpy as np\r\n\r\n# NOTE: Time major\r\n\r\n# Constants\r\ninput_dimensions = 1\r\nvocab_size = 3\r\ninput_embedding_size = 20\r\nencoder_hidden_units = 64\r\nbatch_size = 1\r\ninput_block_size = 2\r\n\r\n# ----------------- Model -------------------------------\r\n\r\nclass Model(object):\r\n    def __init__(self):\r\n        self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \\\r\n        self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()\r\n\r\n    def build_encoder_model(self):\r\n        encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),\r\n                                     dtype=tf.float32, name='encoder_inputs', trainable=False)\r\n        encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,\r\n                                            name='encoder_inputs_length', trainable=False)\r\n        encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,\r\n                                           name='encoder_hidden_state')  # Save the state as one tensor\r\n\r\n        encoder_inputs_embedded = encoder_inputs\r\n\r\n        # Build model\r\n        encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\r\n\r\n        # Build previous state\r\n        encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)\r\n        encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])\r\n        encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])\r\n        encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)\r\n\r\n        #   encoder_outputs: [max_time, batch_size, num_units]\r\n        encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(\r\n            encoder_cell, encoder_inputs_embedded,\r\n            sequence_length=encoder_inputs_length, time_major=True,\r\n            dtype=tf.float32, initial_state=encoder_hidden_state_t)\r\n\r\n        # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.\r\n        encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)\r\n        encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])\r\n\r\n        return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new\r\n\r\n\r\nmodel = Model()\r\n\r\n\r\n# ----------------- Training --------------------------\r\n\r\ndef run_full_transducer():\r\n    # Inputs\r\n    max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')  # How often to run the encoder\r\n    inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,\r\n                                     name='inputs_full_raw')\r\n\r\n    # Turn inputs into tensor which is easily readable\r\n    inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])\r\n\r\n    # Hidden states\r\n    encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))\r\n\r\n    init_state = (0, encoder_hidden_init)\r\n\r\n    def cond(current_block, encoder_hidden):\r\n        return current_block < max_blocks\r\n\r\n    def body(current_block, encoder_hidden):\r\n        # Process encoder\r\n        model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])\r\n        model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])\r\n        model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)\r\n\r\n        # TODO: Error is SOMETIMES gone when using tf.Print. If you comment out the next 2 lines the return val is 0.\r\n        current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')\r\n        current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')\r\n        return current_block + 1, model.encoder_hidden_state_new\r\n\r\n    _, final_enc_state = tf.while_loop(cond, body, init_state, parallel_iterations=1)\r\n\r\n    return max_blocks, inputs_full_raw, model.encoder_outputs, final_enc_state\r\n\r\n\r\n# ---------------------- Management -----------------------------\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n\r\n    inp_max_blocks, inp_inputs_full_raw, enc_out, fin_enc_state = run_full_transducer()\r\n\r\n    out, _ = sess.run([enc_out, fin_enc_state], feed_dict={\r\n        inp_max_blocks: 3,  # How often to run the encoder\r\n        inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions))  # Full inputs\r\n    })\r\n    print 'Encoder outputs: ' + str(out)\r\n```", "@ebrevdo I've tried to make to make a smaller fail case, but I can't seem to find a different way to repeat than in the code in my previous comment. Though it does seem as though that the tensors of the encoder are called and evaluated once at the start in the while loop, and the other results are just the previous values without reevaluation.\r\nEDIT: I've read up on while loops, and realized it is not possible to evaluate tensors defined outside of the loop. Due to this I will probably rewrite my model, and then this bug becomes obsolete for my case. Due to the obscure conditions needed for this bug, I'm closing the issue."]}, {"number": 15979, "title": "Branch 181341793", "body": "", "comments": ["@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please."]}, {"number": 15978, "title": "No documentation for ConfigProto", "body": "### System information\r\n\r\nNot necessary in this case.\r\n\r\n### Describe the problem\r\n\r\nNo documentation for the `ConfigProto` class in the TF website. Specifically, in neither of the following pages\r\n\r\n- https://www.tensorflow.org/api_docs/python/tf/ConfigProto\r\n- https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/ConfigProto\r\n- https://www.tensorflow.org/versions/master/api_docs/python/tf/ConfigProto\r\n  \r\n### Possible solutions\r\n\r\nThe following article https://www.tensorflow.org/tutorials/using_gpu contains info about `ConfigProto`. Either the docs for `ConfigProto` can be written based on that info or, at least, a link to that article should be added to the `ConfigProto` docs.", "comments": ["@josh11b it looks like you originally wrote those lines of the tutorial. Can you take a look or assign someone more appropriate, please?", "As with all protobufs on tensorflow.org, we (now) link to the definition file itself, so you can click on the provided link see the documentation where the proto is defined. Even if we were to generate a more extensive field listing, it would be generated from the proto file's comments.\r\n\r\nIf we feel strongly that the protobuf should link to this doc on tensorflow.org, it should be linked to tensorflow.org in the proto definition itself in a comment.  \r\n\r\nI'll leave this open for now and see if that's a practical thing to do.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@harshini-gadige why you close it? Look at _tensorflowbutler_ last post:\r\n\r\n> It has been 142 days with no activity and this issue has an assignee.\r\n\r\nSo, the problem is still present and issue is inactive bacause assignee does nothing. May be you should change an assignee to fix the problem?", "@RomanSteinberg did you check `using_gpu` [page](https://www.tensorflow.org/guide/using_gpu) that has good number of examples on `ConfigProto`. I am closing this issue as it is stale issue. Please feel free to open the issue if there are not sufficient examples and tutorials on `ConfigProto`.  Thanks! ", "@jvishnuvardhan Well, the using_gpu page is not the documentation of ConfigProto.  I think this issue should remain open.", "@nbro Are you interested in contributing through changing the documentation? You could raise  a Pull-Request. Thanks!", "The TF2 launch is coming soon, and TF2 does not expose ConfigProto except as `tf.compat.v1.ConfigProto`. So it might be reasonable to close this as \"obsolete\".\r\n\r\n@jaingaurav, you wrote the TF2 version of \"Using GPU\" do you know the status of ConfigProto? is it dead? should we close this?", "@MarkDaoust: ConfigProto has been replaced by many `tf.config` APIs for 2.0. Under the hood it uses ConfigProto, but that is an implementation detail. Please let me know if there is any known functionality missing from the `tf.config` APIs, as not all fields may have been exposed in the new APIs.", "Okay, so this is mentioned in the migration guide now.  Closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=15978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=15978\">No</a>\n"]}, {"number": 15977, "title": "Improve video input pipeline (using TFRecord files)", "body": "I am building a video input pipeline for DeepMind's [Kinetics dataset](https://deepmind.com/research/open-source/open-source-datasets/kinetics/) using TFRecord files. Since the dataset is large (200k videos) my TFRecord files store the frames as compressed JPG images; otherwise it would require too much space on disk. Each `tf.train.Example` has the following structure:\r\n\r\n```\r\nExample {\r\n  'num_frames': tf.int64,\r\n  'label': tf.int64,\r\n  'frames/0001': tf.string,\r\n  'frames/0002': tf.string,\r\n  ...\r\n}\r\n```\r\n\r\nWhere all the frames store a JPG image as compressed bytes. Using `tf.data.TFRecordDataset` and `tf.image.decode_jpg` I am able to load the images and decode from JPG into `tf.uint8` tensors (full code can be found [here](https://github.com/tomrunia/TF_VideoInputPipeline/blob/master/kinetics/input_pipeline.py)):\r\n\r\n```\r\ndef decode(serialized_example):\r\n  \r\n    # Prepare feature list; read encoded JPG images as bytes\r\n    features = dict()\r\n    features[\"class_label\"] = tf.FixedLenFeature((), tf.int64)\r\n    for i in range(64):\r\n        features[\"frames/{:04d}\".format(i)] = tf.FixedLenFeature((), tf.string)\r\n\r\n    # Parse into tensors\r\n    parsed_features = tf.parse_single_example(serialized_example, features)\r\n\r\n    # Decode the encoded JPG images\r\n    images = []\r\n    for i in range(64):\r\n        images.append(tf.image.decode_jpeg(parsed_features[\"frames/{:04d}\".format(i)]))\r\n\r\n    # Pack the frames into one big tensor of shape (N,H,W,3)\r\n    images = tf.stack(images)\r\n    label  = tf.cast(parsed_features['class_label'], tf.int64)\r\n\r\n    return images, label\r\n```\r\nTwo things currently seem impossible with the current features of TFRecord files:\r\n\r\n1. There seems to be no way to take a random sample of frames. The code example now takes the first 64 frames from the TFRecord, but what is often preferred is taking a random sample of consecutive frames. In one of my failed attempts I have tried to accomplish this along the lines of:\r\n\r\n```\r\nnum_frames = tf.cast(parsed_features['num_frames'], tf.int64)\r\noffset = tf.random_uniform(shape=(), minval=0, maxval=label, dtype=tf.int64)\r\n```\r\n\r\n2. The number of frames in the video example seems impossible to access in TensorFlow. It can be obtained using ` tf.train.Example.FromString` as given [here](https://stackoverflow.com/a/42402484/3419427), but that does not help me in this case. If this was possible I could just load all the video frames into a tensor (at increased cost...) and than use `tf.random_crop` to sample a random number of frames from the video. \r\n\r\nMy overall question is whether the input pipeline for videos using TFRecord files can be improved? This needs to consider speed of reading data and compression options to limit file size for enormous  datasets. It would be convenient to directly use mp4 streams with TFRecord files, however decoding this is problably much slower than decoding JPG images (**EDIT**: this pull request is related: https://github.com/tensorflow/tensorflow/pull/13242)\r\n\r\nNote that there are many ways to setup the data pipeline for videos. I have described some of them in this post on StackOverflow and motivated why I chose for TFRecord files. This post also describes the problem described here, so it may be informative: https://stackoverflow.com/questions/48101576/tensorflow-read-video-frames-from-tfrecords-file\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Done, although quite irrelevant.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@mrry @saeta can you comment or redirect? Thanks!", "I posted an [answer on Stack Overflow](https://stackoverflow.com/q/48101576/3574081). What you're trying do do seems possible, but it's not super easy (or efficient), and I suggested a workaround in the answer. I dare say the experience for working with video could be improved, and we'd welcome feature requests and/or contributions in this regard.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@mrry Thank you for the helpful comment on StackOverflow. For now, your suggestion seems to be the way to go. If I have some more insights or feedback down the road I will let you know. ", "Thanks for letting us know! I'll close this issue for now, but please feel free to reopen it if you have any more requests for improving video processing with `tf.data`.", "What if I wanna pick a random number of images from a random class, where images are not a segment of video but from random frame indices? @tomrunia @mrry "]}, {"number": 15976, "title": "iOS Camera Example not able to fit to iphone 6 screen ", "body": "### System information\r\n- **OS Platform and Distribution macOS High Sierra **:\r\n- **TensorFlow installed from source**:\r\n- **TensorFlow version 1.4**:\r\n\r\n### Describe the problem\r\nWhen I run the tensorflow camera example on xcode it runs fine but only in dimensions that fit an iphone 4. I changed the build settings, have turned on autolayout, and even tried messing around with the constraints. It seems like no matter what I try the viewcontroller won't get any larger.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15975, "title": "MKL: Fix for a compilation error caused by a previous commit", "body": "", "comments": ["Can one of the admins verify this patch?", "@andydavis1 Can you approve this?", "@yuefengz Can you merge this PR? The build is broken for Intel MKL.", "@zheng-xq can you approve?", "Jenkins, test this please."]}, {"number": 15974, "title": "Estimator.predict always loads model checkpoint preventing partially loading checkpoints", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\npip\r\n- **TensorFlow version (use command below)**:\r\n1.4\r\n- **Python version**: \r\n3.5\r\n\r\n### Describe the problem\r\nWhen using Tensorflow's Estimator to do predictions, the Estimator always loads the checkpoint in the model_dir. As this is done after the model_fn is called, there is no way to partially load a checkpoint for predictions. For training, I partially load the initial checkpoint in the model_fn which works fine.\r\n\r\nI also tried not specifying the model_dir for the Estimator. As the [documentation states](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#__init__), this results in the Estimator using a temporary folder. However, as the temporary folder does not contain a checkpoint, I get the error `Could not find trained model in model_dir`.\r\n\r\nIt looks like there is no way to only partially load a checkpoint for prediction. If so, please provide a way to do this. For me, this is important, because I have a large model with several outputs. For different datasets, some of the outputs have different sizes. However, some of them are the same for all datasets. That's why I want to load them with the same code and only partially, because I don't need to load and run the whole network for this prediction. ", "comments": ["@martinwicke @ispirmustafa is there a way for @andreas-eberle to do what they want, or is this a feature request?", "Hi @andreas-eberle \r\nYou can create your graph partially based on params. something like:\r\n```\r\ndef model_fn(..., params):\r\n  if params['model'] == 'all':\r\n   create all graph\r\n  elif params['model'] == 'small':\r\n   create small graph\r\n\r\nparams = {'model': 'all'}\r\nmain_estimator = tf.estimator.Estimator(model_fn, params=params)\r\nmain_estimator.train(...)\r\n\r\nparams = {'model': 'small'}\r\nsmall_estimator = tf.estimator.Estimator(model_fn, model_dir=main_estimator.model_dir, params=params)\r\nsmall_estimator.predict(...)\r\n```\r\nHaving said that, making your prediction graph different then the graph you trained looks error prone to me.", "Same problem: model_dir specified but got **Could not find trained model in model_dir**"]}, {"number": 15973, "title": "How to change the model, without any change into android APK file", "body": "Hello,\r\n\r\nI want to make an android app in this way, like we can change model file anytime in future, and it will not require any change into application code, means no need to generate new APK file of application, on any change into model.\r\nIn short I want to know, is there anyway to place model file other then assets folder. So that I can refer updated model file anytime from app.\r\n\r\nThanks,\r\nSumeet Guha.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hello,\r\n\r\nPlease find the required details as below:\r\n\r\nHave I written custom code : N/A\r\nOS Platform and Distribution : Android 5.0 and above\r\nTensorFlow installed from: Used TensorFlow for Android library\r\nTensorFlow version : N/A\r\nBazel version : N/A\r\nCUDA/cuDNN version : N/A\r\nGPU model and memory : N/A\r\nExact command to reproduce : N/A\r\n\r\nThanks,\r\nSumeet Guha.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15972, "title": "Maven Version of tensorflow Java API jar wrongly updated in Documentation", "body": "### System information\r\nHave I written custom code : N/A\r\nOS Platform and Distribution : N/A\r\nTensorFlow installed from : N/A\r\nTensorFlow version : N/A\r\nBazel version : N/A\r\nCUDA/cuDNN version : N/A\r\nGPU model and memory : N/A\r\nExact command to reproduce : N/A\r\n\r\n### Describe the problem\r\nhttps://www.tensorflow.org/install/install_java shows maven version as 1.4.1 \r\n\r\n```\r\n<dependency>\r\n  <groupId>org.tensorflow</groupId>\r\n  <artifactId>tensorflow</artifactId>\r\n  <version>1.4.1</version>\r\n</dependency>\r\n```\r\nHowever, this version is not available in public maven Repositories.\r\nhttps://mvnrepository.com/artifact/org.tensorflow/tensorflow\r\nOnly versions  1.3.0 , 1.4.0, 1.4.0-rc0 and 1.5.0-rc0 are available.\r\nPlease correct documentation or release 1.4.1 Versions.\r\n\r\n### Source code / logs\r\nN/A\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Modified Post as per template, although it was not applicable.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Thanks for bringing that to our attention.", "The instructions should be updated now, to version 1.5.0. "]}, {"number": 15971, "title": "Fix local path for hexagon_graph_execution in sample script", "body": "As Android arch is supported since r1.5, the local path must also be changed.\r\nIf not, and error occurs that the file can not be found.\r\n\r\nSigned-off-by: MyungSung Kwak <yesmung@gmail.com>", "comments": ["Can one of the admins verify this patch?", "Hello, I don't know merge process well because this is my first contribution.\r\nSo, I have any questions as below. Please check if you possible : )\r\n\r\n1. How long does it usually take to complete of verification above (e.g, cla, sanity, cmake tests...) ?\r\n2. Should I add reviewer? ", "I want to contribute commit link as below, too.\r\n- https://goo.gl/sHdJMC (440c2ca3819ad5aff8722067a64a2ffe2d095b4b)\r\n\r\nBut the parent commit is my first contribute commit on this request.\r\nSo, I cannot create new request page. \r\n\r\nMy second commit is distinct from my previous 1ebe6dd10f1b09244c0c9439bbcddfb4fffa2468\r\n\r\nPlease guide if I need to create new pull request.", "@satok16 , @tensorflower-gardener \r\nCould you review my commits ? And Please check my question above.\r\n- https://github.com/tensorflow/tensorflow/commit/440c2ca3819ad5aff8722067a64a2ffe2d095b4b\r\n- https://github.com/tensorflow/tensorflow/commit/1ebe6dd10f1b09244c0c9439bbcddfb4fffa2468", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Okay. I checked. Thank you for all support."]}, {"number": 15970, "title": "Fix local path for hexagon_graph_execution in sample script", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15969, "title": "Fix variable property of DropoutWrapper", "body": "Fix #15810.", "comments": ["Can one of the admins verify this patch?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you modify the test to check the variables property after the instance\nis used via proper __call__, not just via add_variable?\n\nOn Sat, Mar 3, 2018, 12:05 AM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Assignee @ebrevdo <https://github.com/ebrevdo>: It has been 14\n> days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15969#issuecomment-370128770>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzYtVZuaUwaE8IVG-V8LnfboLiOiks5tak5ZgaJpZM4RXWdY>\n> .\n>\n", "It seems that DropoutWrapper doesn't create varaible, hence variables property is always empty list via call, not add_variable explicitly. ", "So what's the purpose of this PR?\n\nOn Sun, Mar 4, 2018, 3:14 PM Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> It seems that DropoutWrapper doesn't create varaible, hence variables\n> property is always empty list via call, not add_variable explicitly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15969#issuecomment-370273593>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-2QMeo7ocAxLon9BTq7oZp_o3Z5ks5tbHVpgaJpZM4RXWdY>\n> .\n>\n", "Oh, I found that #16006 has fixed #15810. Let's close it."]}, {"number": 15968, "title": "Imperfect implementation of tf.losses.mean_pairwise_squared_error", "body": "### System information\r\n- **TensorFlow version**: 1.4.0, 1.4.1, and 1.5.0-rc0 (checked)\r\n- **Have I written custom code**: N/A\r\n- **OS Platform and Distribution**: N/A\r\n- **TensorFlow installed from**: N/A\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nThe implementation of `tf.losses.mean_pairwise_squared_error` looks imperfect.\r\nFor example, as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error)\r\n> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are three pairs of differences are summed to compute the loss: loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3\r\n\r\nlet me put the following data as `labels` and `predictions`:\r\n```\r\nlabels = tf.constant([[0., 0.5, 1.]])\r\npredictions = tf.constant([[1., 1., 1.]])\r\ntf.losses.mean_pairwise_squared_error(labels, predictions)\r\n```\r\nIn this case, the result should be `[(0-0.5)^2+(0-1)^2+(0.5-1)^2]/3=0.5`, but tensorflow returns different value 0.3333333134651184.\r\n\r\n### Suggestion to fix the source code\r\n[tensorflow/python/ops/losses/losses_impl.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py)\r\n\r\nIf the loss function `mean_pairwise_squared_error` measures the differences between pairs of corresponding elements of `predictions` and `labels` as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error), here is a simple patch:\r\n> (lines 520-521 need to be changed as)\r\n> `term1 = 2.0 * _safe_div(sum_squares_diff_per_batch, num_present_per_batch-1)`\r\nand\r\n> (lines 525-526 need to be changed as)\r\n> `term2 = 2.0 * _safe_div(math_ops.square(sum_diff), math_ops.multiply(num_present_per_batch, num_present_per_batch-1))`\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15967, "title": "Make graph transform tool accessible via command line for pip install.", "body": "RELNOTE: Make graph transform tool available from command line as\r\n`transform_graph` for pip package.\r\nFix  #13287.", "comments": ["@gunan can you take another look?"]}, {"number": 15966, "title": "remove write_version=saver_pb2.SaverDef.V1", "body": "This PR fixes the failed testAdditionalHooks and testRestoredModelPerformance test for PR #14341", "comments": ["Can one of the admins verify this patch?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "FYI @sguada this seems like a trivial slim change safe to merge? It affects a test."]}, {"number": 15965, "title": "unsupported operand type(s) for /: 'Tensor' and 'float", "body": "Tensorflow version is :  tensorflow-gpu (1.4.1)\r\nPython version is:    Python 3.5.4 :: Anaconda custom (64-bit)\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15964, "title": "DownloadfileTask Failed", "body": "try projrct as https://www.tensorflow.org/mobile/android_build#android_sample_apps,but downloadtask failed,  then solve it ,may be you shoule change the \r\n\r\n> download-models.gradle  classpath 'de.undercouch:gradle-download-task:3.2.0' to 3.3.0", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15963, "title": "how to disable do_constant_folding  of OptimizerOptions for my custom ops ", "body": "I write two ops named own_send and own_recv in c++ \r\nown_send write data to an buffer while wait the buffer flag is 0 ,then set the buffer flag to 1\r\nown_recv read the buffer while wait the buffer flag is 1 and set the flag to 0\r\nbut when I run my model I got the error own_recv is waiting the buffer flag is 1 but the flag is 0 when own_send is done rightly.\r\nI finally found the GraphOptimizer class would run own_recv  previously ends with my flag is changed before my model really process.\r\nI run the code following get the error own_recv is always  waiting the flag is 1 thile the flag is 0.\r\n`input = [tf.ones([3136,1024])]`\r\n`send = own_send(input)`\r\n`recv = own_recv([tf.ones([1])])`\r\n`sess = tf.Session()`\r\n`sess.run(recv)`\r\n\r\nthen I try to run with opt_level is L0, it works:\r\n\r\n`input = [tf.ones([3136,1024])]`\r\n`send = own_send(input)`\r\n`recv = own_recv([tf.ones([1])])`\r\n`config = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))`\r\n`sess = tf.Session(config = config)`\r\n`sess.run(recv)`\r\n\r\nI want to know how to disable the optimizer options just for my own custiom ops leaving the other ops optimied\r\nIs there some path to disable option for specified ops \uff1f\r\nsomething like  add some opt in my BUILD ?\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux  Ubuntu 14.04.4 LTS \r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.4.0-rc1\r\n- **Python version**: Python 2.7.6\r\n- **Bazel version (if compiling from source)**: 0.5.4", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15962, "title": "Model diverges with NaN if the class label exceeds the expected number of classes", "body": "Tried in Tensorflow v1.4\r\n\r\nHave I written custom code - Yes\r\nOS Platform and Distribution - CentOS Linux release 7.4.1708\r\nTensorFlow installed from - Not from source\r\nTensorFlow version - v1.4.1\r\nGPU model and memory - Tesla P100 16GB\r\n\r\nIf the class label exceeds the expected number of classes (dimensions of losses and given label won't match), TensorFlow simply errors out with `loss diverged with a NaN`. Ideally it should warn the user that the class label provided exceeded the expected number of classes.\r\n\r\nIt is easy to hit this error and spend lot of time in debugging. For Ex: If there are 101 number of classes, TensorFlow expects the labels to be in `[0,100]`, but if the user has labels `[1,101]`, any attempts to train would simply error out with `loss diverged with a NaN`.\r\n\r\nCan we have some warning specifically for this case?\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15961, "title": "Adding cuda_config.h to the pip package.", "body": "", "comments": ["http://ci.tensorflow.org/job/test_cuda/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/7/artifact/pip_test/whl/tensorflow_gpu-1.5.0rc0-cp27-cp27mu-manylinux1_x86_64.whl", "Breakages are unrelated."]}, {"number": 15960, "title": "Branch 181239691", "body": "", "comments": ["@tensorflow-jenkins, test this please.", "@tensorflow-jenkins, test this please."]}, {"number": 15959, "title": "Adding an install sources line for 1.5.0-rc0. Earlier we only updated\u2026", "body": "\u2026 this for official.", "comments": ["@gunan for Windows we list the cmake version as that is what is technically supported.", "But we should still add all the compiler, cmake and CUDA/cuDNN versions we use to build TF on windows.", "Ah my mistake. "]}, {"number": 15958, "title": "Undefined Symbol gen_parsing_ops_py_wrappers_cc when building pip package", "body": "\r\n### System information\r\n\r\ngreg@salt:~/code/tensorflow$ more tf_env.txt \r\n\r\n== cat /etc/issue ===============================================\r\nLinux salt 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"17.10 (Artful Aardvark)\"\r\nVERSION_ID=\"17.10\"\r\nVERSION_CODENAME=artful\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\n(configure set to gcc-6)\r\nc++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux salt 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.0)\r\nprotobuf (3.5.1)\r\ns2clientprotocol (1.1)\r\ntensorflow-gpu (1.4.0)\r\ntensorflow-serving-api (1.4.0)\r\ntensorflow-tensorboard (0.4.0rc3)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64:/opt/tensorlibs/lib\r\nDYLD_LIBRARY_PATH /usr/local/cuda/lib64:/opt/tensorlibs/lib\r\n\r\n== nvidia-smi ===================================================\r\nMon Jan  8 13:25:47 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 00000000:01:00.0  On |                  N/A |\r\n| 24%   34C    P8     9W / 151W |    596MiB /  8112MiB |      3%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       988      G   /usr/lib/xorg/Xorg                           373MiB |\r\n|    0      1384      G   /usr/bin/compiz                              116MiB |\r\n|    0      8602      G   ...-token=09D12A9300046A211380F83BAD0A60C6    46MiB |\r\n|    0     15983      G   ...-token=29A09CA037A6A6610BD563090AA5B5DA    56MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.1/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.103\r\n\r\n\r\n### Describe the problem\r\nBuild command below fails w/ undefined symbol.  \r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\nERROR: /home/greg/code/tensorflow/tensorflow/python/BUILD:1398:1: Executing genrule //tensorflow/python:parsing_ops_pygenrule failed (Exit 127): bash failed: error executing command \r\n  (cd /home/greg/.cache/bazel/_bazel_greg/ca148a14f6c24000015970c3c0a435f7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-9.1 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc-6 \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/opt/tensorlibs/lib \\\r\n    PATH=/opt/ant/bin:.:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-9-oracle/bin:/usr/lib/jvm/java-9-oracle/db/bin:/opt/gephi/bin:/opt/eclipse:/opt/gradle/bin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION=9.1 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /bin/bash bazel-out/k8-py3-opt/genfiles/tensorflow/python/parsing_ops_pygenrule.genrule_script.sh)\r\nbazel-out/host/bin/tensorflow/python/gen_parsing_ops_py_wrappers_cc: symbol lookup error: bazel-out/host/bin/tensorflow/python/gen_parsing_ops_py_wrappers_cc: undefined symbol: _ZN10tensorflow17ParseExampleAttrs10FinishInitEv\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 63.574s, Critical Path: 19.07s\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n### Source code / logs\r\ngreg@salt:~/code/tensorflow$ dpkg -l | grep cuda\r\nii  cuda                                       9.1.85-1                                    amd64        CUDA meta-package\r\nii  cuda-9-1                                   9.1.85-1                                    amd64        CUDA 9.1 meta-package\r\n....\r\nii  libcudnn7                                  7.0.5.15-1+cuda9.1                          amd64        cuDNN runtime libraries\r\nii  libcudnn7-dev                              7.0.5.15-1+cuda9.1                          amd64        cuDNN development libraries and headers\r\nii  libcudnn7-doc                              7.0.5.15-1+cuda9.1                          amd64        cuDNN documents and samples\r\n\r\n\r\n  ", "comments": ["@martinwicke @javiribera @jart PTAL. ", "Apologies, this was a false alarm.  After posting, I noticed the /opt/tensorlibs which was an old directory where I used static libraries to get around a previous syntaxnet gcc linking error.  I also had tensorflow 1.5 and tensorflow-gpu 1.4.1 both installed.  I removed that /opt/tensorlibs directori, did a pip3 uninstall for both tensorflow and tensorflow-gup,  and did a bazel clean.  That solved the issue.  "]}]