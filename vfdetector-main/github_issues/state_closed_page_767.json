[{"number": 30546, "title": "Add `maximize` method to Optimizer base class.", "body": "In some fields, such as reinforcement learning, the optimization problem is posed as maximization of reward. By only supporting minimization, TensorFlow optimizers force a slight mismatch between code and math.\r\n\r\nConsider translating the objective `maximize f1 + f2` to code. One could make the following careless error:\r\n\r\n    opt_op = adam.minimize(-f1 + f2)\r\n\r\nAdding a `maximize` method comes at almost no implementation cost, and it removes a (minor) potential source of errors when translating math to code, so it could be worth including.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30546) for more info**.\n\n<!-- need_sender_cla -->", "Thanks for your contribution , can you please sign CLA.", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30546) for more info**.\n\n<!-- ok -->", "IMO In reinforcement learning the objective you're trying to maximize or minimize is not a direct measure. The gradient of that objective happens to be policy gradient. So even we have a maximize method it wouldn't be reasonable to expect the 'loss' will keep increasing even the policy is getting better."]}, {"number": 30545, "title": "MSVC compiler gives warning C4190 when include Tensorflow header", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nNo\r\n- TensorFlow version (use command below):\r\nTensorFlow 1.10\r\n- Python version:\r\nNone\r\n- Bazel version (if compiling from source):\r\nNone\r\n- GCC/Compiler version (if compiling from source):\r\nNone\r\n- CUDA/cuDNN version:\r\nCUDA 10.0 / cuDNN 7.5\r\n- GPU model and memory:\r\nGTX 1070\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen include tensorflow header to use the C API in my project, it always gives me warnings warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C. It is a bug introduced by the Tensorflow C API header.\r\n\r\n**Describe the expected behavior**\r\nNo warnings should be come from the Tensorflow header. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Got same WARNING but everything seems work fine.\r\nThe structure is defined as below\r\n```\r\ntypedef struct TF_WhileParams {\r\n  // The number of inputs to the while loop, i.e. the number of loop variables.\r\n  // This is the size of cond_inputs, body_inputs, and body_outputs.\r\n  const int ninputs;\r\n\r\n  // The while condition graph. The inputs are the current values of the loop\r\n  // variables. The output should be a scalar boolean.\r\n  TF_Graph* const cond_graph;\r\n  const TF_Output* const cond_inputs;\r\n  TF_Output cond_output;\r\n\r\n  // The loop body graph. The inputs are the current values of the loop\r\n  // variables. The outputs are the updated values of the loop variables.\r\n  TF_Graph* const body_graph;\r\n  const TF_Output* const body_inputs;\r\n  TF_Output* const body_outputs;\r\n\r\n  // Unique null-terminated name for this while loop. This is used as a prefix\r\n  // for created operations.\r\n  const char* name;\r\n} TF_WhileParams;\r\n```\r\n\r\nI think it's because C does not support const member and there are a lot of those members in this structure, so `TF_WhileParams` is treated as a C++ structure. Maybe you could ignore them?", "Can you send a PR removing const from this struct?", "```\r\ntypedef struct TF_WhileParams {\r\n  // The number of inputs to the while loop, i.e. the number of loop variables.\r\n  // This is the size of cond_inputs, body_inputs, and body_outputs.\r\n  int ninputs;\r\n\r\n  // The while condition graph. The inputs are the current values of the loop\r\n  // variables. The output should be a scalar boolean.\r\n  TF_Graph* cond_graph;\r\n  const TF_Output* cond_inputs;\r\n  TF_Output cond_output;\r\n\r\n  // The loop body graph. The inputs are the current values of the loop\r\n  // variables. The outputs are the updated values of the loop variables.\r\n  TF_Graph* body_graph;\r\n  const TF_Output* body_inputs;\r\n  TF_Output* body_outputs;\r\n\r\n  // Unique null-terminated name for this while loop. This is used as a prefix\r\n  // for created operations.\r\n  const char* name;\r\n} TF_WhileParams;\r\n```\r\n\r\nI removed all top-level consts in this struct and there are no more warnings, and the code seems work, to be tested though.", "@yijiew We see that you're using TF v1.x which is not actively supported .Could you please try on the  latest stable TF v2.6.0 and let us know if it is still an issue ? Please refer to the [comment](https://github.com/tensorflow/tensorflow/issues/30545#issuecomment-536443799) and let us know if it helps?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30545\">No</a>\n"]}, {"number": 30544, "title": "Bring release notes from recent releases back into master", "body": "Somehow our release process missed this step before, now I think it's fixed but we should still bring the old notes to master.", "comments": []}, {"number": 30543, "title": "Fix the related docs for MatchingFilesOp", "body": "This PR fixes #30436. As `MatchingFilesOp` sorts the matched files [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matching_files_op.cc#L63), the order of filenames returned is deterministic. This PR revises the related docs. ", "comments": []}, {"number": 30542, "title": "[INTEL MKL] Added support for common utility functions used by MKL-DNN enabled kernels for MKL-DNN v1.0", "body": "", "comments": ["Since this PR builds up on https://github.com/tensorflow/tensorflow/pull/30186, it will be reopened once https://github.com/tensorflow/tensorflow/pull/30186 gets merged into the master. "]}, {"number": 30541, "title": "Add pip_new.sh as our release scripts now use it.", "body": "If we don't have this, `macos/*/pip` and `ubuntu/*/pip` release jobs all fail due to missing script.", "comments": []}, {"number": 30540, "title": "Refactor TextLineDatasetOp with tests", "body": "This PR refactors `TextLineDatasetOp` using name_utils and add the tests.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa Thanks for your review! The comments are addressed in this commit (https://github.com/tensorflow/tensorflow/pull/30540/commits/ad249152a4b7653b14d62d75a4694239f62cebe5). Could you please have a look at the change when you get a chance?", "@jsimsa The comments are addressed here(https://github.com/tensorflow/tensorflow/pull/30540/commits/7df87194e2335b33b8085ae06fb5f4f3fbef3ea9). Please take another look!", "@jsimsa The default values for `CompressionParams` are added now (https://github.com/tensorflow/tensorflow/pull/30540/commits/f6e973eb9ffaac771c57fac24ab03cdb658343f0).", "@jsimsa The internal check failed. Could you please help check the logs and paste them here?", "> @jsimsa The internal check failed. Could you please help check the logs and paste them here?\r\n\r\n@feihugis changes are submitted internally , waiting for tool to auto merge , thank you", "> @feihugis changes are submitted internally , waiting for tool to auto merge , thank you\r\n\r\nThanks @rthadur! When I checked the tests, the status shows `failed`, which may be caused by that the test is still running. Now it is updated to `passed`.\r\n"]}, {"number": 30539, "title": "Error importing tensorflow on windows 10 - OSError: [WinError 126] The specified module could not be found", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): cmd \r\n- TensorFlow GPU  version:  1.14.0\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10.1, 7.6.0\r\n- GPU model and memory: GTX 970 8GB RAM\r\n\r\nHey i've recently tried to installed tensorflow-gpu and before that i made sure that I have all Environment variables set up cuDNN, CUDA, drivers. \r\neventually i installed tensorflow-gpu using pip (pip install --ignore-installed -- upgrade tensorflow-gpu\r\n\r\nafter the Installation I tried importing tensorflow as tf and It came up with this error:\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n    ctypes.WinDLL(build_info.cudart_dll_name)\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n>>> import tensorflow as tf\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive\r\n\r\n**it says it cannot find 'cudart64_100.dll' but I have in my cuda folder the following dll 'cudnn64_7'**\r\n\r\nHere is a pic of my Environment variables : https://imgur.com/a/UXub3LR", "comments": ["TF 1.14 GPU supports cuda 10.0 please switch to cuda 10.0 and update the environment variables. Thanks!", "> TF 1.14 GPU supports cuda 10.0 please switch to cuda 10.0 and update the environment variables. Thanks!\r\n\r\nThanks I did it but now I get this error:\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 83, in <module>\r\n    from tensorflow.python import keras\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\", line 26, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\activations\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.activations import elu\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\__init__.py\", line 21, in <module>\r\n    from tensorflow.python.keras._impl.keras import activations\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\activations.py\", line 23, in <module>\r\n    from tensorflow.python.keras._impl.keras import backend as K\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py\", line 37, in <module>\r\n    from tensorflow.python.layers import base as tf_base_layers\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 25, in <module>\r\n    from tensorflow.python.keras import backend\r\n  File \"C:\\Users\\\u05d9\u05e8\u05d9\u05df\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\backend\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.backend import abs\r\nImportError: cannot import name 'abs'", "Probably this can help,\r\nhttps://github.com/tensorflow/tensorflow/issues/20778#issuecomment-410962482", "Yay I got it to work by uninstalling and installing tensor flow!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 30538, "title": "[R1.14] Use environment markers for enum34", "body": "\r\n**NOTE: This PR is against R1.14 branch and is a cherry-pick of #30255.**\r\n\r\nThis fix tries to addres the issue raised in #30200 where\r\nenum34 caused poetry breaks due to the conditional sys.version_info\r\n\r\nThis fix changes to use environment markers for enum34 (conform to PEP508)\r\n\r\nThis fix fixes #30200.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 30537, "title": "Colocation error with SparseApply* ops and ResourceVariables", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0/7.4.1\r\n- GPU model and memory: GeForce GTX 980 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nTensorFlow attempts to colocate ops with incompatible devices (when the ops are related to `SparseApply*`, e.g. `SparseApplyRMSProp`, operations), which results in an error.  This only occurs when using ResourceVariables (not RefVariables).\r\n\r\n**Describe the expected behavior**\r\n\r\nThere should be no error, and ops should be assigned to their appropriate devices (CPU vs GPU).\r\n\r\n**Code to reproduce the issue**\r\n``` python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndo_error = True\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    my_var = tf.Variable(np.ones(5), use_resource=True)\r\n    with tf.device(\"/gpu:0\" if do_error else None):\r\n        gather = tf.gather(my_var, [0, 2, 4])\r\n    opt_op = tf.train.MomentumOptimizer(0.1, 0.1).minimize(gather)\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(opt_op)\r\n```\r\n\r\n**Other info / logs**\r\nColocation debug info:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation Variable/IsInitialized/VarIsInitializedOp: Could not satisfy explicit device specification '' because the node node Variable/IsInitialized/VarIsInitializedOp (defined at tmp.py:7) placed on device No device assignments were active during op 'Variable/IsInitiali\r\nzed/VarIsInitializedOp' creation.  was colocated with a group of nodes that required incompatible device '/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0].\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]\r\nConst: GPU CPU\r\nVarHandleOp: GPU CPU\r\nAssignVariableOp: GPU CPU\r\nVarIsInitializedOp: GPU CPU\r\nResourceGather: GPU CPU\r\nReadVariableOp: GPU CPU\r\nStridedSlice: GPU CPU\r\nUnique: GPU CPU\r\nShape: GPU CPU\r\nUnsortedSegmentSum: GPU CPU\r\nCast: GPU CPU\r\nResourceSparseApplyMomentum: CPU\r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  Variable/Initializer/initial_value (Const)\r\n  Variable (VarHandleOp)\r\n  Variable/IsInitialized/VarIsInitializedOp (VarIsInitializedOp)\r\n  Variable/Assign (AssignVariableOp)\r\n  Variable/Read/ReadVariableOp (ReadVariableOp)\r\n  Gather (ResourceGather) /device:GPU:0\r\n  Variable/Momentum/Initializer/zeros (Const)\r\n  Variable/Momentum (VarHandleOp)\r\n  Variable/Momentum/IsInitialized/VarIsInitializedOp (VarIsInitializedOp)\r\n  Variable/Momentum/Assign (AssignVariableOp)\r\n  Variable/Momentum/Read/ReadVariableOp (ReadVariableOp)\r\n  Momentum/update_Variable/Unique (Unique)\r\n  Momentum/update_Variable/Shape (Shape)\r\n  Momentum/update_Variable/strided_slice/stack (Const)\r\n  Momentum/update_Variable/strided_slice/stack_1 (Const)\r\n  Momentum/update_Variable/strided_slice/stack_2 (Const)\r\n  Momentum/update_Variable/strided_slice (StridedSlice)\r\n  Momentum/update_Variable/UnsortedSegmentSum (UnsortedSegmentSum)\r\n  Momentum/update_Variable/Cast (Cast)\r\n  Momentum/update_Variable/Cast_1 (Cast)\r\n  Momentum/update_Variable/ResourceSparseApplyMomentum (ResourceSparseApplyMomentum)\r\n\r\n         [[node Variable/IsInitialized/VarIsInitializedOp (defined at tmp.py:7) ]]Additional information about colocations:No node-device colocations were active during op 'Variable/IsInitialized/VarIsInitializedOp' creation.\r\nNo device assignments were active during op 'Variable/IsInitialized/VarIsInitializedOp' creation.\r\n```", "comments": ["Seems related to https://github.com/tensorflow/tensorflow/issues/1310, but the last comment there indicates that that issue was fixed back in 2016.", "I have tried on colab with TF version 1.14 and was able to reproduce the issue.Thanks!", "The following code works:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndo_error = True\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    my_var = tf.Variable(np.ones(5), use_resource=True)\r\n    # Moved tf.gather outside the tf.device block.\r\n    gather = tf.gather(my_var, [0, 2, 4])\r\n    with tf.device(\"/gpu:0\" if do_error else None):\r\n        gather = tf.identity(gather)\r\n    opt_op = tf.train.MomentumOptimizer(0.1, 0.1).minimize(gather)\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(opt_op)\r\n```\r\n\r\nThe only change here is that the `ResourceGather` op is no longer placed on GPU. This makes sense since we want to place all ops touching a resource on the same device. If this is the desired behavior we could change `sparse_read` to place the `ResourceGather` op on the same device as the handle.\r\nHowever, I tried removing the optimizer in the original code, and it started to work. So I am not sure whether my understanding is correct. @alextp or @mrry may have more ideas.", "cc @ezhulenev ", "I think this is \u201cworking as intended\u201d, but the error message is a little overwhelming. The root of the problem is that `ResourceSparseApplyMomentum` only has a CPU implementation, and it must run on the same device as where the variable is stored. For efficiency, `tf.gather(my_var, \u2026)` uses a `ResourceGather` op that also must run on the same device as where the variable is stored. However, if you explicitly request that `tf.gather(my_var, \u2026)` be placed on GPU, we are left with an unsatisfiable set of constraints.\r\n\r\nSaurabh\u2019s workaround [above](https://github.com/saxenasaurabh) is correct, but it has the unfortunate effect of copying the entirety of `my_var` from CPU to GPU on each step, which is likely to impair performance.\r\n\r\nAs a short-term workaround, you can enable \u201csoft placement\u201d, which should fall back to placing the ops on the CPU when no GPU implementation is available, by passing `config=tf.ConfigProto(allow_soft_placement=True)` to `tf.Session`.\r\n\r\nLonger term, it would be useful to add GPU implementations of `ResourceSparseApply*`, wherever possible. I think we\u2019d welcome contributions for those.", "Thanks for the explanation @mrry. Do you think it makes sense to change [`sparse_read`](https://github.com/tensorflow/tensorflow/blob/ec64a3e0c7d86a52ee166e4998a376e1112fc3c6/tensorflow/python/ops/resource_variable_ops.py#L638) to place the `ResourceGather` on the same device as the resource handle just like we do for [`read_value`](https://github.com/tensorflow/tensorflow/blob/ec64a3e0c7d86a52ee166e4998a376e1112fc3c6/tensorflow/python/ops/resource_variable_ops.py#L632) and ignore the device context? That way this won't error out.", "> Do you think it makes sense to change sparse_read to place the ResourceGather on the same device as the resource handle just like we do for read_value and ignore the device context? \r\n\r\nThat probably makes sense! Historically we\u2019d have used `with ops.colocate_with(my_var):` for that. Technically the `with ops.device(self._handle.device):` [in `read_value()`](https://github.com/tensorflow/tensorflow/blob/ec64a3e0c7d86a52ee166e4998a376e1112fc3c6/tensorflow/python/ops/resource_variable_ops.py#L632) isn\u2019t sufficient because if `self._handle.device` isn\u2019t a complete device specification, then you wouldn\u2019t be guaranteed to colocate them.", "True. I believe [placer](https://github.com/tensorflow/tensorflow/blob/4db2480e44dc82de7ff22bb5ffbb562d24fa9930/tensorflow/core/common_runtime/colocation_graph.cc#L614) might colocate them though so maybe the undefined specification is better than an incorrect one. I'll send out a fix.", "The placer should already force-colocate the sparseread and the resource\nhandle, both in eager and graph modes.\n\nOn Fri, Jul 12, 2019 at 12:59 PM Saurabh Saxena <notifications@github.com>\nwrote:\n\n> True. I believe placer\n> <https://github.com/tensorflow/tensorflow/blob/4db2480e44dc82de7ff22bb5ffbb562d24fa9930/tensorflow/core/common_runtime/colocation_graph.cc#L614>\n> might colocate them though so maybe the undefined specification is better\n> than an incorrect one. I'll send out a fix.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30537?email_source=notifications&email_token=AAABHRP2APUGKEVVZV4F73LP7DPDDA5CNFSM4H7HLLZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ2XAIQ#issuecomment-511012898>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKYVFUNKGBOE6W455TP7DPDDANCNFSM4H7HLLZQ>\n> .\n>\n\n\n-- \n - Alex\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30537\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30537\">No</a>\n"]}, {"number": 30536, "title": "Transformer training error with evaluation/test dataset newstest2014.de ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (only changed batch/filter size on modelparams.py)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): https://hub.docker.com/r/tensorflow/tensorflow nightly-gpu-py3\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA RTX 8000 x2 (wNVLINK)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen following the transformer [walkthrough](https://github.com/tensorflow/models/tree/master/official/transformer#detailed-instructions) There seems to be an issue with the newstest2014.de 'Download and preprocess datasets' step the newstest2014.de doesn't appear to be in valid text format. When its unpacked it cannot be opened as a text file (looks like another archive file) as the newstest2014.en can and was in previous versions. \r\n\r\nFor example: when I run the following command (after completing the rest of the requisite steps): \r\n\r\n_python3 transformer_main.py --data_dir=$DATA_DIR --model_dir=$MODEL_DIR --vocab_file=$VOCAB_FILE --param_set=$PARAM_SET --bleu_source=$DATA_DIR/newstest2014.en --bleu_ref=$DATA_DIR/newstest2014.de --train_steps=250000 --steps_between_evals=10000 --export_dir=$EXPORT_DIR_ \r\n\r\nI get the error ( see attached/below for traceback) **UnicodeDecodeError: 'utf8' codec can't decode byte 0x8b in position 1: invalid start byte**\r\n\r\n**Describe the expected behavior**\r\ncompute_bleu.py should work, but does not (only when i replace newstest2014.de with an older version (text format)\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nFollow the example walkthrough, you'll get invalid newstest2014.de (as of  7/8/19)\r\n\r\n**Other info / logs** see attached for traceback as well\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAs it currently stands, training only worked when i replaced the invalid newstest2014.de file with the older version, therefore i think thats the error.\r\n\r\nI0708 18:55:13.764734 140267571169088 translate.py:133] Writing to file /tmp/tmpie7053oh\r\nTraceback (most recent call last):\r\n  File \"transformer_main.py\", line 670, in <module>\r\n    absl_app.run(main)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"transformer_main.py\", line 664, in main\r\n    run_transformer(flags.FLAGS)\r\n  File \"transformer_main.py\", line 644, in run_transformer\r\n    vocab_file=flags_obj.vocab_file)\r\n  File \"transformer_main.py\", line 361, in run_loop\r\n    estimator, bleu_source, bleu_ref, vocab_file)\r\n  File \"transformer_main.py\", line 238, in evaluate_and_log_bleu\r\n    estimator, subtokenizer, bleu_source, bleu_ref)\r\n  File \"transformer_main.py\", line 222, in translate_and_compute_bleu\r\n    uncased_score = compute_bleu.bleu_wrapper(bleu_ref, tmp_filename, False)\r\n  File \"/datasets/datasets/models/official/transformer/compute_bleu.py\", line 91, in bleu_wrapper\r\n    tf.io.gfile.GFile(ref_filename).read()).strip().splitlines()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py\", line 128, in read\r\n    pywrap_tensorflow.ReadFromStream(self._read_buf, length))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py\", line 98, in _prepare_value\r\n    return compat.as_str_any(val)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/compat.py\", line 117, in as_str_any\r\n    return as_str(value)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/compat.py\", line 87, in as_text\r\n    return bytes_or_text.decode(encoding)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\r\n\r\n \r\n[traceback1.txt](https://github.com/tensorflow/tensorflow/files/3373946/traceback1.txt)\r\n", "comments": ["moved to tensorflow/models issues, should be there, closing. "]}, {"number": 30535, "title": "Argument to force CuDNN implementation for tf.keras.layers.LSTM", "body": "**System information**\r\n- TensorFlow version (you are using): 2 beta1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nBoolean `force_cudnn` argument for tf.keras.layers.LSTM. When True, would raise an error if the user's configurations or usage (masking) are incompatible with the CuDNN implementation. \r\n\r\nCurrently, the LSTM layer logic dynamically uses the CuDNN implementation depending on the given arguments to the `__init__` and `call` functions. It does not report which it ends up using.\r\n\r\n**Will this change the current api? How?**\r\n\r\nThis will add an argument to the tf.keras.layers.LSTM `__init__` function.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone for whom the CuDNN implementation is both essential and viable and who is debugging their architecture for this reason. For example... \"My model is very slow, but I haven't run it before. I wonder if the LSTM implementation is optimized for GPU? I wish I could verify that it is.\"", "comments": ["@jkamalu,\r\nSorry for the delayed response. As per the documentation of [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), it uses **`GPU`** if the below conditions are satisfied.\r\n\r\n> activation == tanh\r\n> recurrent_activation == sigmoid\r\n> recurrent_dropout == 0\r\n> unroll is False\r\n> use_bias is True\r\n> Inputs, if use masking, are strictly right-padded.\r\n> Eager execution is enabled in the outermost context.\r\n\r\nCan you please let us know if this is what you are looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 30534, "title": "[TF 2.0] Error using VGG-based loss with keras model.compile", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: cudatoolkit 10.0.130, cudnn 7.6.0\r\n- GPU model and memory: NVIDIA GeForce GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\nThe following error (full traceback below) is observed during the  keras compile call when using a pretrained VGG model in my loss function.\r\n\r\n> AttributeError: 'Tensor' object has no attribute '_cpu_nograd'\r\n\r\nThis error only occurs in TF 2.0, and does not occur in TF 1.14.\r\n\r\nIt is worth noting that no error occurs when using a similar loss function in non-eager execution as in https://www.tensorflow.org/beta/tutorials/generative/style_transfer\r\n \r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import models, optimizers, metrics\r\nfrom tensorflow.keras.layers import Conv2D\r\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\r\nimport numpy as np\r\n\r\n#Build Simple Model\r\ninputs = keras.Input(shape=(32,32,3))\r\nx = Conv2D(16, 3, padding = 'same', activation='relu')(inputs)\r\nx = Conv2D(16, 3, padding = 'same', activation='relu')(x)\r\no = Conv2D( 3, 3, padding = 'same', activation='relu')(x)\r\nmodel = keras.Model(inputs=inputs, outputs=o)\r\n\r\n#Generate dummy data\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\r\nx_train = x_train.astype('float32') / 255.\r\ny_train = x_train.copy()\r\nx_train = x_train+0.1*np.random.randn(*x_train.shape).astype('f')\r\ntrain_ds = tf.data.Dataset.from_tensor_slices(\r\n    (x_train, y_train)).shuffle(10000).batch(128)\r\n\r\n#Load VGG model\r\nvgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape = [32,32,3])\r\nvgg.trainable = False\r\ncontent_layers = 'block5_conv2'\r\n\r\nlossModel = models.Model([vgg.input], vgg.get_layer(content_layers).output, name = 'vggL')\r\n\r\ndef lossVGG(X,Y):\r\n    Xt = preprocess_input(X*255)\r\n    Yt = preprocess_input(Y*255)\r\n    vggX = lossModel(Xt)\r\n    vggY = lossModel(Yt)\r\n    return tf.reduce_mean(tf.square(vggY-vggX))\r\n\r\noptimizer = optimizers.Adam(learning_rate=0.0005)\r\nloss = [lossVGG]\r\nmodel.compile(optimizer = optimizer, loss = loss)\r\nhistory = model.fit(train_ds, epochs = 20)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-6822dc73d458> in <module>\r\n     40 optimizer = optimizers.Adam(learning_rate=0.0005)\r\n     41 loss = [lossVGG]\r\n---> 42 model.compile(optimizer = optimizer, loss = loss)\r\n     43 history = model.fit(train_ds, epochs = 20)\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    456     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    457     try:\r\n--> 458       result = method(self, *args, **kwargs)\r\n    459     finally:\r\n    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    335 \r\n    336       # Creates the model loss and weighted metrics sub-graphs.\r\n--> 337       self._compile_weights_loss_and_weighted_metrics()\r\n    338 \r\n    339       # Functions for train, test and predict will\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    456     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    457     try:\r\n--> 458       result = method(self, *args, **kwargs)\r\n    459     finally:\r\n    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\r\n   1492       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n   1493       #                   layer losses.\r\n-> 1494       self.total_loss = self._prepare_total_loss(masks)\r\n   1495 \r\n   1496   def _prepare_skip_target_masks(self):\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _prepare_total_loss(self, masks)\r\n   1552 \r\n   1553           if hasattr(loss_fn, 'reduction'):\r\n-> 1554             per_sample_losses = loss_fn.call(y_true, y_pred)\r\n   1555             weighted_losses = losses_utils.compute_weighted_loss(\r\n   1556                 per_sample_losses,\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py in call(self, y_true, y_pred)\r\n    213       Loss values per sample.\r\n    214     \"\"\"\r\n--> 215     return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n    216 \r\n    217   def get_config(self):\r\n\r\n<ipython-input-6-6822dc73d458> in lossVGG(X, Y)\r\n     34 \r\n     35     vggX = lossModel(Xt)\r\n---> 36     vggY = lossModel(Yt)\r\n     37 \r\n     38     return tf.reduce_mean(tf.square(vggY-vggX))\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    594     # framework.\r\n    595     if build_graph and base_layer_utils.needs_keras_history(inputs):\r\n--> 596       base_layer_utils.create_keras_history(inputs)\r\n    597 \r\n    598     # Clear eager losses on top level model call.\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py in create_keras_history(tensors)\r\n    197     keras_tensors: The Tensors found that came from a Keras Layer.\r\n    198   \"\"\"\r\n--> 199   _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n    200   return created_layers\r\n    201 \r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops, created_layers)\r\n    241             constants[i] = op_input\r\n    242           else:\r\n--> 243             constants[i] = backend.function([], op_input)([])\r\n    244       processed_ops, created_layers = _create_keras_history_helper(\r\n    245           layer_inputs, processed_ops, created_layers)\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py in __call__(self, inputs)\r\n   3517     # (otherwise it's just a no-op.)\r\n   3518     return nest.pack_sequence_as(\r\n-> 3519         self._outputs_structure, [x._cpu_nograd()._numpy() for x in outputs],  # pylint: disable=protected-access\r\n   3520         expand_composites=True)\r\n   3521 \r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py in <listcomp>(.0)\r\n   3517     # (otherwise it's just a no-op.)\r\n   3518     return nest.pack_sequence_as(\r\n-> 3519         self._outputs_structure, [x._cpu_nograd()._numpy() for x in outputs],  # pylint: disable=protected-access\r\n   3520         expand_composites=True)\r\n   3521 \r\n\r\nAttributeError: 'Tensor' object has no attribute '_cpu_nograd'\r\n```", "comments": ["I have tried on colab with TF version 2.0 beta1 and was able to reproduce the issue.Thanks!", "@mketcha Need to add function decoration @tf.function() before custom loss function.\r\nPlease check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/dfd2cf563b1a3e255455b3c54e64d3b8/tf_30534_customloss.ipynb#scrollTo=e3TFz1m2n1sj). Thanks!", "Thank you! Including the @tf.function() does work in the provided sample code.  However, if I change the code to now load the model within the function as in:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import models, optimizers, metrics\r\nfrom tensorflow.keras.layers import Conv2D\r\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\r\nimport numpy as np\r\n\r\n#Build Simple Model\r\ninputs = keras.Input(shape=(32,32,3))\r\nx = Conv2D(16, 3, padding = 'same', activation='relu')(inputs)\r\nx = Conv2D(16, 3, padding = 'same', activation='relu')(x)\r\no = Conv2D( 3, 3, padding = 'same', activation='relu')(x)\r\nmodel = keras.Model(inputs=inputs, outputs=o)\r\n\r\n#Generate dummy data\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\r\nx_train = x_train.astype('float32') / 255.\r\ny_train = x_train.copy()\r\nx_train = x_train+0.1*np.random.randn(*x_train.shape).astype('f')\r\ntrain_ds = tf.data.Dataset.from_tensor_slices(\r\n    (x_train, y_train)).shuffle(10000).batch(128)\r\n\r\n\r\n@tf.function()\r\ndef lossVGG(X,Y):\r\n    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape = [32,32,3])\r\n    vgg.trainable = False\r\n    content_layers = 'block5_conv2'\r\n\r\n    lossModel = models.Model([vgg.input],vgg.get_layer(content_layers).output,name = 'vggL')\r\n\r\n    Xt = preprocess_input(X*255)\r\n    Yt = preprocess_input(Y*255)\r\n    \r\n    vggX = lossModel(Xt)\r\n    vggY = lossModel(Yt)\r\n    \r\n    return tf.reduce_mean(tf.square(vggY-vggX))\r\n\r\noptimizer = optimizers.Adam(learning_rate=0.0005)\r\nloss = [lossVGG]\r\nmodel.compile(optimizer = optimizer, loss = loss)\r\nhistory = model.fit(train_ds, epochs = 20)\r\n```\r\n\r\nI now get the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-34d952c19226> in <module>\r\n     42 optimizer = optimizers.Adam(learning_rate=0.0005)\r\n     43 loss = [lossVGG]\r\n---> 44 model.compile(optimizer = optimizer, loss = loss)\r\n     45 history = model.fit(train_ds, epochs = 20)\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    456     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    457     try:\r\n--> 458       result = method(self, *args, **kwargs)\r\n    459     finally:\r\n    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    335 \r\n    336       # Creates the model loss and weighted metrics sub-graphs.\r\n--> 337       self._compile_weights_loss_and_weighted_metrics()\r\n    338 \r\n    339       # Functions for train, test and predict will\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n    456     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    457     try:\r\n--> 458       result = method(self, *args, **kwargs)\r\n    459     finally:\r\n    460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _compile_weights_loss_and_weighted_metrics(self, sample_weights)\r\n   1492       #                   loss_weight_2 * output_2_loss_fn(...) +\r\n   1493       #                   layer losses.\r\n-> 1494       self.total_loss = self._prepare_total_loss(masks)\r\n   1495 \r\n   1496   def _prepare_skip_target_masks(self):\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _prepare_total_loss(self, masks)\r\n   1552 \r\n   1553           if hasattr(loss_fn, 'reduction'):\r\n-> 1554             per_sample_losses = loss_fn.call(y_true, y_pred)\r\n   1555             weighted_losses = losses_utils.compute_weighted_loss(\r\n   1556                 per_sample_losses,\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py in call(self, y_true, y_pred)\r\n    213       Loss values per sample.\r\n    214     \"\"\"\r\n--> 215     return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n    216 \r\n    217   def get_config(self):\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    426         # Lifting succeeded, so variables are initialized and we can run the\r\n    427         # stateless function.\r\n--> 428         return self._stateless_fn(*args, **kwds)\r\n    429     else:\r\n    430       canon_args, canon_kwds = \\\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1332   def __call__(self, *args, **kwargs):\r\n   1333     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n-> 1334     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   1335     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1336 \r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   1646       graph_function = self._function_cache.primary.get(cache_key, None)\r\n   1647       if graph_function is None:\r\n-> 1648         graph_function = self._create_graph_function(args, kwargs)\r\n   1649         self._function_cache.primary[cache_key] = graph_function\r\n   1650       return graph_function, args, kwargs\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   1539             arg_names=arg_names,\r\n   1540             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 1541             capture_by_value=self._capture_by_value),\r\n   1542         self._function_attributes)\r\n   1543 \r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    714                                           converted_func)\r\n    715 \r\n--> 716       func_outputs = python_func(*func_args, **func_kwargs)\r\n    717 \r\n    718       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n    307         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    308         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 309         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    310     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    311 \r\n\r\n~\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in wrapper(*args, **kwargs)\r\n    704           except Exception as e:  # pylint:disable=broad-except\r\n    705             if hasattr(e, \"ag_error_metadata\"):\r\n--> 706               raise e.ag_error_metadata.to_exception(type(e))\r\n    707             else:\r\n    708               raise\r\n\r\nValueError: in converted code:\r\n\r\n    <ipython-input-1-34d952c19226>:28 lossVGG  *\r\n        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape = [32,32,3])\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\__init__.py:70 wrapper\r\n        return base_fun(*args, **kwargs)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\vgg19.py:32 VGG19\r\n        return vgg19.VGG19(*args, **kwargs)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\keras_applications\\vgg19.py:112 VGG19\r\n        name='block1_conv1')(img_input)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:616 __call__\r\n        self._maybe_build(inputs)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1966 _maybe_build\r\n        self.build(input_shapes)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py:165 build\r\n        dtype=self.dtype)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:389 add_weight\r\n        aggregation=aggregation)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:713 _add_variable_with_custom_getter\r\n        **kwargs_for_getter)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_utils.py:154 make_variable\r\n        shape=variable_shape if variable_shape else None)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:260 __call__\r\n        return cls._variable_v1_call(*args, **kwargs)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:221 _variable_v1_call\r\n        shape=shape)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:60 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    C:\\Users\\ketchm3\\AppData\\Local\\Continuum\\miniconda3\\envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:364 invalid_creator_scope\r\n        \"tf.function-decorated function tried to create \"\r\n\r\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\r\n```\r\n\r\nI can work around it, but still worth noting", "@mketcha What is your intention behind adding load_model under custom loss function? \r\n\r\nThis `lossVGG` will be called every epoch which mean you are trying to create `vgg` and other variables multiple times in the graph. That is the reason behind `ValueError: tf.function-decorated function tried to create variables on non-first call.`. Thanks!", "I agree, adds unnecessary overhead. Was an error that came about while attempting to update TF1.X scripts that I found to TF2.0.\r\n\r\nThanks for the help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30534\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30534\">No</a>\n"]}, {"number": 30533, "title": "[TF2.0] trainable=True on tf.keras.Embedding result in \"constant folding failed: Invalid argument: Unsupported type: 21\"", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: CUDA 10.0.130_411.31 / cuDNN 7.5.1.10\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen enabling training on Embedding layer using keras subclassed model, an error is raised during model.fit():\r\n\r\n`2019-07-09 11:00:33.231719: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21`\r\n\r\nThis has a huge impact on training speed, that scales with the amount of data, in my full code, each epoch take more than 1 minute to complete(dataset of 869 tokenized strings, for train an encoder/decoder model), the code for reproduce this error dont have a noticeable impact because there is only one sample, already tokenized.\r\n\r\nThe error only is showed when an Embedding layer with trainable=True, is followed by an LSTM/GRU layer\r\n\r\n**Describe the expected behavior**\r\nLSTM/GRU supporting zero masking, and having no impact of training speed\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass lstm(tf.keras.Model):\r\n    def __init__(self):\r\n        super(lstm, self).__init__()\r\n\r\n        self.embedding = tf.keras.layers.Embedding(300, 2, mask_zero=True, trainable=True)\r\n        self.encoder = tf.keras.layers.LSTM(2, return_sequences=True, return_state=False)\r\n\r\n    def call(self, inputs):\r\n        output = self.embedding(inputs)\r\n        output = self.encoder(output)\r\n        return output[0]\r\n\r\ninput_questions = np.array([[5, 12, 13, 189, 10, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)\r\noutput = np.array([[-0.00299482, 0.00096033]])\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((input_questions, output)).shuffle(2).batch(1)\r\n\r\nmodel = lstm()\r\nmodel.compile(tf.keras.optimizers.Adadelta(1.0), tf.keras.losses.MeanSquaredError())\r\nmodel.fit(dataset, epochs=10, verbose=2)\r\nfor sample, target in dataset.take(1):\r\n    model(sample)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nWhat the minimal code example above show in the console for me\r\n\r\n```\r\n2019-07-09 11:10:25.468004: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll                                                                                                      2019-07-09 11:10:26.569016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:    name: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.733                                                    pciBusID: 0000:01:00.0                                                                                                  2019-07-09 11:10:26.588454: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.                                                                                    2019-07-09 11:10:26.604012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0      2019-07-09 11:10:26.619218: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2                                                                       2019-07-09 11:10:26.643472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:    name: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.733                                                    pciBusID: 0000:01:00.0                                                                                                  2019-07-09 11:10:26.670895: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.                                                                                    2019-07-09 11:10:26.688869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0      2019-07-09 11:10:28.438683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:                                                                                            2019-07-09 11:10:28.466078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0                             2019-07-09 11:10:28.480348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N                             2019-07-09 11:10:28.500656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4712 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)  \r\n                                                                              \r\nEpoch 1/10                                                                                                             \r\n\r\nWARNING: Logging before flag parsing goes to stderr.                                                                    \r\nW0709 11:10:30.293739 15940 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3868: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.                                                                             \r\nInstructions for updating:                                                                                              \r\nUse tf.where in 2.0, which has the same broadcast rule as np.where                                                      \r\n\r\n2019-07-09 11:10:38.997986: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21                                                                                        \r\n\r\n2019-07-09 11:10:39.653444: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21      \r\n                                                                                  \r\n1/1 - 13s - loss: 1.5416e-05                                                                                            \r\nEpoch 2/10                                                                                                              \r\n1/1 - 1s - loss: 1.1537e-05                                                                                             \r\nEpoch 3/10                                                                                                              \r\n1/1 - 1s - loss: 8.7570e-06                                                                                             \r\nEpoch 4/10                                                                                                              \r\n1/1 - 1s - loss: 6.8737e-06                                                                                             \r\nEpoch 5/10                                                                                                              \r\n1/1 - 1s - loss: 5.6433e-06                                                                                             \r\nEpoch 6/10                                                                                                              \r\n1/1 - 1s - loss: 4.8554e-06                                                                                             \r\nEpoch 7/10                                                                                                              \r\n1/1 - 1s - loss: 4.3538e-06                                                                                             \r\nEpoch 8/10                                                                                                              \r\n1/1 - 1s - loss: 4.0314e-06                                                                                             \r\nEpoch 9/10                                                                                                              \r\n1/1 - 1s - loss: 3.8191e-06                                                                                             \r\nEpoch 10/10                                                                                                             \r\n1/1 - 1s - loss: 3.6735e-06                                                                                             \r\n\r\nPress any key to continue . . .  \r\n```\r\n\r\nEdit 01: i think is somewhat ralated to [this](https://github.com/tensorflow/tensorflow/issues/29525#issuecomment-509154149) on https://github.com/tensorflow/tensorflow/issues/29525", "comments": ["I manage to throw out some warnings, leaving exclusively: \r\n\r\n`2019-07-09 17:19:34.150417: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21`\r\n\r\nThe training speed impact of this error is still there, and the unique way i found for disable it completely is making trainable=False on the layer constructor:\r\n\r\n`self.embedding = tf.keras.layers.Embedding(300, 2, trainable=False)`\r\n\r\nhere is the new code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass lstm(tf.keras.Model):\r\n    def __init__(self):\r\n        super(lstm, self).__init__()\r\n\r\n        self.masking = tf.keras.layers.Masking()\r\n        self.embedding = tf.keras.layers.Embedding(300, 2, trainable=True)\r\n        self.encoder = tf.keras.layers.LSTM(2, \"sigmoid\", return_sequences=True, return_state=False)\r\n\r\n    def call(self, inputs):\r\n        output = self.masking(inputs)\r\n        output = self.embedding(output)\r\n        output = self.encoder(output)\r\n        return output[0]\r\n\r\ninput_questions = np.array([[5, 12, 13, 189, 10, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)\r\noutput = np.array([[-0.00299482, 0.00096033]])\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((input_questions, output)).shuffle(2).batch(1)\r\n\r\nmodel = lstm()\r\nmodel.compile(tf.keras.optimizers.Adadelta(1.0), tf.keras.losses.MeanSquaredError())\r\nmodel.fit(dataset, epochs=10, verbose=2)\r\nfor sample, target in dataset.take(1):\r\n    model(sample)\r\n\r\n```\r\n\r\nAs you can see, now instead of use mask_zeros=True in the Embedding layer constructor, i leave it false (as default), and use the tf.keras.layers.Masking, this give me the masking feature without any errors. The only one still there is the topic of the Issue.", "@ElPapi42 I tried reproducing the issue and i do not see any errors while fitting the model. Please, see https://colab.sandbox.google.com/drive/11g3kQiNYk4XTzp4h550-gPGCbUdemGGR#scrollTo=ewDZ7_374EJO for your reference.Thanks!", "@ravikyram I tried reproducing the issue too, the reported warnings can be found in the collab runtime logs", "> @ElPapi42 I tried reproducing the issue and i do not see any errors while fitting the model. Please, see https://colab.sandbox.google.com/drive/11g3kQiNYk4XTzp4h550-gPGCbUdemGGR#scrollTo=ewDZ7_374EJO for your reference.Thanks!\r\n\r\nI dont have access to the link, request an account with access permisions\r\n\r\n", "![results](https://user-images.githubusercontent.com/51902062/61029611-0cd49580-a3d9-11e9-93a7-608bef26a063.png)\r\nPlease, see screenshot.Thanks!", "> @ravikyram I tried reproducing the issue too, the reported warnings can be found in the collab runtime logs\r\n\r\n@ravikyram Please look at the runtime logs. The fact that this kind of issue will only show up in logs when ran in collab has already been noted in #30263 - and this is an actual issue, reported on multiple occasions but seemingly unsolved as of now...", "Adding Eugene who is the expert of grappler.", "@rmlarsen rings any bells?", "Any advance on this? its really annoying ", "> I manage to throw out some warnings, leaving exclusively:\r\n> \r\n> `2019-07-09 17:19:34.150417: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21`\r\n> \r\n> The training speed impact of this error is still there, and the unique way i found for disable it completely is making trainable=False on the layer constructor:\r\n> \r\n> `self.embedding = tf.keras.layers.Embedding(300, 2, trainable=False)`\r\n> \r\n> here is the new code:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> import numpy as np\r\n> \r\n> class lstm(tf.keras.Model):\r\n>     def __init__(self):\r\n>         super(lstm, self).__init__()\r\n> \r\n>         self.masking = tf.keras.layers.Masking()\r\n>         self.embedding = tf.keras.layers.Embedding(300, 2, trainable=True)\r\n>         self.encoder = tf.keras.layers.LSTM(2, \"sigmoid\", return_sequences=True, return_state=False)\r\n> \r\n>     def call(self, inputs):\r\n>         output = self.masking(inputs)\r\n>         output = self.embedding(output)\r\n>         output = self.encoder(output)\r\n>         return output[0]\r\n> \r\n> input_questions = np.array([[5, 12, 13, 189, 10, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)\r\n> output = np.array([[-0.00299482, 0.00096033]])\r\n> \r\n> dataset = tf.data.Dataset.from_tensor_slices((input_questions, output)).shuffle(2).batch(1)\r\n> \r\n> model = lstm()\r\n> model.compile(tf.keras.optimizers.Adadelta(1.0), tf.keras.losses.MeanSquaredError())\r\n> model.fit(dataset, epochs=10, verbose=2)\r\n> for sample, target in dataset.take(1):\r\n>     model(sample)\r\n> ```\r\n> \r\n> As you can see, now instead of use mask_zeros=True in the Embedding layer constructor, i leave it false (as default), and use the tf.keras.layers.Masking, this give me the masking feature without any errors. The only one still there is the topic of the Issue.\r\n\r\nI discover something, in the code above, i make sigmoid the activation of the lstm for get rid of:\r\n\r\n`2019-07-19 09:01:52.787220: W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_395_571' and '__inference___backward_cudnn_lstm_395_571_specialized_for_Adadelta_gradients_lstm_lstm_1_StatefulPartitionedCall_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_1552' both implement 'lstm_ca0befc9-b0cc-4e66-adc9-7da601846478' but their signatures do not match`\r\n\r\nactivation=\"sigmoid\" avoid use the cdnn implementation of the LSTM layer. Playing around with the code i notice that leaving activation=\"tanh\" (allowing cDDN implementation), suppress the error Unsupported Type 21, but creates a new one related to the signatures, the one that push me before to make activation=\"sigmoid\" on lstm layer\r\n\r\nhere is the new code, that not produce Unsupported Type 21 error\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass lstm(tf.keras.Model):\r\n    def __init__(self):\r\n        super(lstm, self).__init__()\r\n\r\n        self.masking = tf.keras.layers.Masking()\r\n        self.embedding = tf.keras.layers.Embedding(300, 2, trainable=True)\r\n        self.encoder = tf.keras.layers.LSTM(2, \"tanh\", return_sequences=True, return_state=False)\r\n\r\n    def call(self, inputs):\r\n        output = self.masking(inputs)\r\n        output = self.embedding(output)\r\n        output = self.encoder(output)\r\n        return output[0]\r\n\r\ninput_questions = np.array([[5, 12, 13, 189, 10, 95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)\r\noutput = np.array([[-0.00299482, 0.00096033]])\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((input_questions, output)).shuffle(2).batch(1)\r\n\r\nmodel = lstm()\r\nmodel.compile(tf.keras.optimizers.Adadelta(1.0), tf.keras.losses.MeanSquaredError())\r\nmodel.fit(dataset, epochs=10, verbose=2)\r\nfor sample, target in dataset.take(1):\r\n    model(sample)\r\n```\r\n\r\nThis can point that the Unsupported Type 21 error is produced on the Standard LSTM/GRU implementation of Tensorflow\r\n\r\nThe problem with the new error is that tells skip the optimization completely, and that sounds bad\r\n\r\nPD: as you can see, the embedding layer has trainable=True and Unsupported type 21 is not showing  ", "Obviously the Unsupported Type 21 Error is something that must be checked, there is something broke", "This was fixed in https://github.com/tensorflow/tensorflow/commit/24174643a75e819b8ce01fd70d45d03616e50071, should be in nightly build", "For the implementation selector problems I think there is work in progress by @qlzh727.", "Can you give the pip command for install tf2.0 nightly build? ", "any advances on this issue?", "Any patches or solutions ? please we need some help . thank per advance", "> This was fixed in 2417464, should be in nightly build\r\n\r\n`pip install tf-nightly-gpu-2.0-preview` or `pip install tf-nightly-2.0-preview` (depending whether you want GPU support or not ; you could also want and get a nightly build of tf 1.15 instead)\r\n\r\nThat being said, the fix that was deployed three weeks ago merely avoids the warning message (which saves a little time and avoids wary print outs), but I believe the underlying issue has not been fixed yet (hopefully this is indeed work in progress).", "@ElPapi42 Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. I ran with `TF2.0` and don't see any constant folding error. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/6ebdf409b24e112f7979fc0e31d5ee8e/untitled541.ipynb) Thanks!", "I think this was solved, but i dont have time to check right now. The project where this appeared is in an unusable state right now, must work on it for test.", "I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30533\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30533\">No</a>\n"]}, {"number": 30532, "title": "How can i make sure the hash is consistent when saving the same model?", "body": "I'm using the Keras model.save('my_model.h5') function. Every time i save it, the hash changes, even though no changes were made to the weights or settings etc. \r\n\r\nOn this page: [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saved_model/save.py), it shows there's a KERAS_CACHE_KEY, is that the culprit? or is some other change/timestamp happening on save? \r\n\r\nI need to identify the specific model in the future and really just want to store it's hash for verification, but if i save the same model twice, i'll never know it was the same one. Can you please help point me in the right direction? ", "comments": ["@bojangus Please have a look at [link](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/keras/save_and_restore_models.ipynb). Thanks! ", "@gadagashwini Thank you for the response, but this link is a tutorial on how to save a model. I have no problem saving the models. And i have no problem restoring the models and using them again either. \r\n\r\nI am running the saved models through a hashing function (eg: from the [hashlib](https://docs.python.org/3/library/hashlib.html) module), and the same model comes out with a different hash every time it is saved, even if you re-save it just 5 seconds later. (the weights and parameters haven't changed).\r\n\r\nThe scenario i'm trying to reach is, the same model is saved at multiple different times, but has the same hash. Any idea on how i can achieve this?", "Hi @k-w-w , anything i can do to help clarify this further? ", "Hi @k-w-w , I realise you're busy, if you could just give me some advice on how to work around it myself, or where to look, that would be great.", "@bojangus,\r\nSorry for the delayed response. I think, recommendation in this case is to use [**`Tensorflow Extended, TFX`**](https://www.tensorflow.org/tfx), which effectively handles **`End-To-End Machine Learning Pipelines`**. In your case, the versioning will be taken care by **`Tensorflow Serving`**, a sub system of **`TFX`**.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30532\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30532\">No</a>\n"]}, {"number": 30531, "title": "tf.keras becuase Tensorflow Version Update to 1.14.0 Not Running", "body": "Tensorflow Version: 1.14.0\r\nException : \r\n```\r\n---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n<ipython-input-14-dcf9102c1887> in <module>()\r\n----> 1 trainModel()\r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in __init__(self, cpu_model, strategy)\r\n   1426           self._cpu_model.sample_weight_mode,\r\n   1427           self._cpu_model._compile_weighted_metrics,\r\n-> 1428           self._cpu_model.target_tensors,\r\n   1429       )\r\n   1430 \r\n\r\nAttributeError: 'Model' object has no attribute 'target_tensors'\r\n```\r\nBut My Code in Tensorflow Version 1.13.1 is Normal Working.\r\n\r\nMy Code:\r\n```\r\n\u00b7\u00b7\u00b7\r\n\ttuneModel = Model(inputs=base_model.input, outputs = output)\r\n\ttuneModel.compile(loss='sparse_categorical_crossentropy', optimizer= tf.train.RMSPropOptimizer(learning_rate=1e-4) ,metrics=['accuracy'])\r\n\r\n\u00b7\u00b7\u00b7\r\n```", "comments": ["Working By Colab TPU mode", "Please provide details about what platform you are using (operating system, architecture). \r\n\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks", "My Computer: MacBook Pro (Retina, 13-inch, Early 2015)\r\nBut I isn't run tensorflow on my computer, is in Google Colab TPU mode.\r\nCode snippet  in this issue first commit.", "Please provide us complete code to reproduce the issue.Thanks!", "> \u8bf7\u63d0\u4f9b\u5b8c\u6574\u7684\u4ee3\u7801\u4ee5\u91cd\u73b0\u8be5\u95ee\u9898\u3002\u8c22\u8c22\uff01\r\nFull Code(Colab TPU Runtime):\r\n```\r\nimport os\r\nimport datetime\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras import optimizers\r\nfrom tensorflow.keras.applications.vgg16 import VGG16\r\nfrom tensorflow.keras.preprocessing import image\r\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard\r\nfrom tensorflow.keras.backend import set_session\r\n\r\nfrom google.colab import drive\r\nfrom google.colab import files\r\ndrive.mount('/dataset')\r\n!mkdir /dataset1\r\n!ln -s /dataset/'My Drive'/newdataset /dataset1\r\n\r\nprint(\"Tensorflow Version:\",tf.__version__)\r\nprint(\"Tensorflow Keras Version:\",tf.keras.__version__)\r\n\r\ntrain_dir = \"/dataset1/newdataset/train\"\r\ntest_dir = \"/dataset1/newdataset/test\"\r\n!mkdir /dataset1/newdataset/checkpoints\r\n!mkdir /dataset1/newdataset/log\r\ntrain_pic_gen=ImageDataGenerator(rescale=1./255,rotation_range=20,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.5,horizontal_flip=True,fill_mode='nearest')\r\ntest_pic_gen=ImageDataGenerator(rescale=1./255)\r\ntrain_iter=train_pic_gen.flow_from_directory(train_dir,(224,224),batch_size=128*2,class_mode='categorical')\r\nval_iter=test_pic_gen.flow_from_directory(test_dir,(224,224),batch_size=128*2,class_mode='categorical')\r\n\r\nbase_model = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\r\n#\u5728vgg16\r\nout = base_model.layers[-1].output\r\nflatten  = layers.Flatten(name=\"flatten\")(out)\r\nfc_1  = layers.Dense(1024, activation='relu')(flatten)\r\nfc_1_droupout = layers.Dropout(0.5)(fc_1)\r\nfc_2 = layers.Dense(512, activation='relu')(fc_1_droupout)\r\nfc_2_droupout = layers.Dropout(0.3)(fc_2)\r\noutput = layers.Dense(6, activation='softmax',name=\"output\")(fc_2_droupout)\r\ntuneModel = Model(inputs=base_model.input, outputs = output)\r\ntuneModel.compile(loss='categorical_crossentropy', optimizer= tf.train.RMSPropOptimizer(learning_rate=1e-4) ,metrics=['accuracy'])\r\n#TPU\r\nTPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\ntpu_model = tf.contrib.tpu.keras_to_tpu_model(tuneModel,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\r\n\r\nhistory = tpu_model.fit_generator(generator=train_iter,steps_per_epoch=100,epochs=300,validation_data=val_iter,validation_steps=32)\r\n```\r\n\r\nRunning After Exception:\r\n```\r\nDrive already mounted at /dataset; to attempt to forcibly remount, call drive.mount(\"/dataset\", force_remount=True).\r\nmkdir: cannot create directory \u2018/dataset1\u2019: File exists\r\nln: failed to create symbolic link '/dataset1/newdataset': File exists\r\nTensorflow Version: 1.14.0\r\nTensorflow Keras Version: 2.2.4-tf\r\nmkdir: cannot create directory \u2018/dataset1/newdataset/checkpoints\u2019: File exists\r\nmkdir: cannot create directory \u2018/dataset1/newdataset/log\u2019: File exists\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0712 01:30:42.246924 139705002051456 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nFound 1411 images belonging to 6 classes.\r\nFound 326 images belonging to 6 classes.\r\nW0712 01:30:44.299897 139705002051456 lazy_loader.py:50] \r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nW0712 01:30:45.307133 139705002051456 keras_support.py:217] Keras support is now deprecated in support of TPU Strategy. Please follow the distribution strategy guide on tensorflow.org to migrate to the 2.0 supported version.\r\nW0712 01:30:45.323995 139705002051456 deprecation.py:323] From <ipython-input-1-d1779d017a30>:46: tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is deprecated and will be removed after 2019-02-20.\r\nInstructions for updating:\r\nSwitch to tf.contrib.distribute.TPUStrategy. https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/DistributionStrategy\r\nW0712 01:30:49.196088 139705002051456 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0712 01:30:49.197774 139705002051456 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0712 01:30:51.592882 139705002051456 keras_support.py:1394] Keras support is now deprecated in support of TPU Strategy. Please follow the distribution strategy guide on tensorflow.org to migrate to the 2.0 supported version.\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-d1779d017a30> in <module>()\r\n     44 #TPU\r\n     45 TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n---> 46 tpu_model = tf.contrib.tpu.keras_to_tpu_model(tuneModel,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\r\n     47 #TensorBoard\r\n     48 #log_dir=\"/dataset1/newdataset/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in __init__(self, cpu_model, strategy)\r\n   1426           self._cpu_model.sample_weight_mode,\r\n   1427           self._cpu_model._compile_weighted_metrics,\r\n-> 1428           self._cpu_model.target_tensors,\r\n   1429       )\r\n   1430 \r\n\r\nAttributeError: 'Model' object has no attribute 'target_tensors'\r\n\r\n```", "I have the same problem. I'm running on a regular google cloud TPU from a VM, not colab.", "Same error here in this colab.\r\nhttps://colab.research.google.com/github/samwit/TPU/blob/master/Guide_to_tf_keras_TPU_MNIST.ipynb\r\n", "I made it work by instead of using `tf.contrib.tpu.keras_to_tpu_model`, I created the model within the TPU strategy scope.\r\n\r\n```python\r\nstrategy = tf.distribute.experimental.TPUStrategy(\r\n    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS))\r\nwith strategy.scope():\r\n  Inp, output = build_model()\r\n  opt = tf.train.AdamOptimizer(learning_rate)\r\n  tpu_model = tf.keras.Model(inputs=[Inp], outputs=[output])\r\n  tpu_model.compile(\r\n      optimizer=opt,\r\n      loss='categorical_crossentropy',\r\n      metrics=['acc'])\r\n```", "@blackhu did you get a chance to check the solution provided by rzilleruelo. Also, it looks drive is not mounted correctly in colab so you are getting problem with directories.Please, check. Thanks!", "> I made it work by instead of using `tf.contrib.tpu.keras_to_tpu_model`, I created the model within the TPU strategy scope.\r\n\r\nThank You rzilleruelo,This way i have try.But tips ```NotImplementedError: `fit_generator` is not supported for models compiled with tf.distribute.Strategy.```", "> ravikyram\r\n\r\nHi ravikyram\uff0cMy Google Drive Mount is Not problems. Output cue some \"File exists\",just i mounted driver on run this code before. I used rzilleruelos' way,but have a new problem.Detail Infomation:\r\n```\r\nimport os\r\nimport datetime\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras import optimizers\r\nfrom tensorflow.keras.applications.vgg16 import VGG16\r\nfrom tensorflow.keras.preprocessing import image\r\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard\r\nfrom tensorflow.keras.backend import set_session\r\n\r\nfrom google.colab import drive\r\nfrom google.colab import files\r\ndrive.mount('/dataset')\r\n!mkdir /dataset1\r\n!ln -s /dataset/'My Drive'/newdataset /dataset1\r\n\r\nprint(\"Tensorflow Version:\",tf.__version__)\r\nprint(\"Tensorflow Keras Version:\",tf.keras.__version__)\r\n\r\ntrain_dir = \"/dataset1/newdataset/train\"\r\ntest_dir = \"/dataset1/newdataset/test\"\r\n!mkdir /dataset1/newdataset/checkpoints\r\n!mkdir /dataset1/newdataset/log\r\ntrain_pic_gen=ImageDataGenerator(rescale=1./255,rotation_range=20,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.5,horizontal_flip=True,fill_mode='nearest')\r\ntest_pic_gen=ImageDataGenerator(rescale=1./255)\r\ntrain_iter=train_pic_gen.flow_from_directory(train_dir,(224,224),batch_size=128*2,class_mode='categorical')\r\nval_iter=test_pic_gen.flow_from_directory(test_dir,(224,224),batch_size=128*2,class_mode='categorical')\r\n\r\n#TPU\r\nTPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\nstrategy = tf.distribute.experimental.TPUStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\r\nwith strategy.scope():\r\n\tbase_model = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\r\n\tout = base_model.layers[-1].output\r\n\tflatten  = layers.Flatten(name=\"flatten\")(out)\r\n\tfc_1  = layers.Dense(1024, activation='relu')(flatten)\r\n\tfc_1_droupout = layers.Dropout(0.5)(fc_1)\r\n\tfc_2 = layers.Dense(512, activation='relu')(fc_1_droupout)\r\n\tfc_2_droupout = layers.Dropout(0.3)(fc_2)\r\n\toutput = layers.Dense(6, activation='softmax',name=\"output\")(fc_2_droupout)\r\n\ttuneModel = Model(inputs=base_model.input, outputs = output)\r\n\ttuneModel.compile(loss='categorical_crossentropy', optimizer= tf.train.RMSPropOptimizer(learning_rate=1e-4) ,metrics=['accuracy'])\r\n\r\nhistory = tuneModel.fit_generator(generator=train_iter,steps_per_epoch=100,epochs=300,validation_data=val_iter,validation_steps=32)\r\n```\r\n\r\nOutput:\r\n```\r\nDrive already mounted at /dataset; to attempt to forcibly remount, call drive.mount(\"/dataset\", force_remount=True).\r\nmkdir: cannot create directory \u2018/dataset1\u2019: File exists\r\nln: failed to create symbolic link '/dataset1/newdataset': File exists\r\nTensorflow Version: 1.14.0\r\nTensorflow Keras Version: 2.2.4-tf\r\nmkdir: cannot create directory \u2018/dataset1/newdataset/checkpoints\u2019: File exists\r\nmkdir: cannot create directory \u2018/dataset1/newdataset/log\u2019: File exists\r\nFound 1420 images belonging to 6 classes.\r\nFound 329 images belonging to 6 classes.\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-7-a15528b76f39> in <module>()\r\n     46         tuneModel.compile(loss='categorical_crossentropy', optimizer= tf.train.RMSPropOptimizer(learning_rate=1e-4) ,metrics=['accuracy'])\r\n     47 \r\n---> 48 history = tuneModel.fit_generator(generator=train_iter,steps_per_epoch=100,epochs=300,validation_data=val_iter,validation_steps=32)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1412     \"\"\"\r\n   1413     if self._distribution_strategy:\r\n-> 1414       raise NotImplementedError('`fit_generator` is not supported for '\r\n   1415                                 'models compiled with tf.distribute.Strategy.')\r\n   1416     _keras_api_gauge.get_cell('train').set(True)\r\n\r\nNotImplementedError: `fit_generator` is not supported for models compiled with tf.distribute.Strategy.\r\n```", "@ravikyram I also tried the other solution and ran into an error. Am not in front of a computer right now so can't give full code. Basically I get an assertion error saying that I'm not using a DatasetV2.\r\n\r\n```\r\n  File \"/home/lminer_gmail_com/anaconda3/envs/separate/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2408, in _distribution_standardize_user_data\r\n    assert isinstance(x, dataset_ops.DatasetV2)\r\nAssertionError\r\n```", "> Thank You rzilleruelo,This way i have try.But tips `` NotImplementedError: `fit_generator` is not supported for models compiled with tf.distribute.Strategy. ``\r\n\r\nDid you find how to solve this?", "> > Thank You rzilleruelo,This way i have try.But tips `` NotImplementedError: `fit_generator` is not supported for models compiled with tf.distribute.Strategy. ``\r\n> \r\n> Did you find how to solve this?\r\n\r\nI am waiting for the reply from Tensorflow Project RD", "There are some examples of code for TPUs here: https://github.com/tensorflow/tpu/tree/master/models/experimental. Cheers.\r\n\r\n**Edit**: it's also possible in Colab to revert to an earlier version by running\r\n```\r\n%pip install tensorflow==1.13.1\r\n```\r\nand then resetting your run time.", "So is it solved or Just only can be solved with pip install tensorflow==1.13.1???\r\n@BrianHung", "@appleyuchi I think you still have to use `1.13.1` considering `1.14` is still the latest release of tensorflow.", "@BrianHung \r\nThanks for your replies~!\r\nGot it~!", "Is `fit_generator` still not supported?", "how to solve the below error in google colab...\r\n\r\nRuntimeError: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14.", "> how to solve the below error in google colab...\r\n> \r\n> RuntimeError: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14.\r\n\r\nI did it using\r\n%tensorflow_version 1.x\r\nand restarting runtime.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30531\">No</a>\n"]}, {"number": 30530, "title": "Request for ComplexAbs and RFFT operations in tf.lite for Tensorflow 2.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.0.0-dev20190709\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, DIV, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOG, MAXIMUM, MUL, PACK, PAD, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: ComplexAbs, RFFT.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\nFull traceback:\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-23-ddedc174c27b> in <module>\r\n      4 #converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n      5 \r\n----> 6 tflite_model = converter.convert()\r\n\r\n~/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    438         input_tensors=input_tensors,\r\n    439         output_tensors=output_tensors,\r\n--> 440         **converter_kwargs)\r\n    441 \r\n    442     if self._is_calibration_quantize():\r\n\r\n~/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)\r\n    409   data = toco_convert_protos(model_flags.SerializeToString(),\r\n    410                              toco_flags.SerializeToString(),\r\n--> 411                              input_data.SerializeToString())\r\n    412   return data\r\n    413 \r\n\r\n~/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    170       stderr = _try_convert_to_unicode(stderr)\r\n    171       raise ConverterError(\r\n--> 172           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    173   finally:\r\n    174     # Must manually cleanup files.\r\n\r\nConverterError: TOCO failed. See console for info.\r\n2019-07-09 16:34:24.077376: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: RFFT\r\n2019-07-09 16:34:24.077408: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: ComplexAbs\r\n2019-07-09 16:34:24.078122: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 47 operators, 85 arrays (0 quantized)\r\n2019-07-09 16:34:24.078475: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 47 operators, 85 arrays (0 quantized)\r\n2019-07-09 16:34:24.079060: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 40 operators, 70 arrays (0 quantized)\r\n2019-07-09 16:34:24.079501: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 39 operators, 69 arrays (0 quantized)\r\n2019-07-09 16:34:24.079910: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 39 operators, 69 arrays (0 quantized)\r\n2019-07-09 16:34:24.080126: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 39 operators, 69 arrays (0 quantized)\r\n2019-07-09 16:34:24.080490: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 384 bytes, theoretical optimal value: 192 bytes.\r\n2019-07-09 16:34:24.080931: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, DIV, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOG, MAXIMUM, MUL, PACK, PAD, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: ComplexAbs, RFFT.\r\nTraceback (most recent call last):\r\n  File \"/Users/ben/miniconda3/envs/wakeword/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/ben/miniconda3/envs/wakeword/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, DIV, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOG, MAXIMUM, MUL, PACK, PAD, RANGE, REDUCE_MAX, REDUCE_MIN, RESHAPE, SHAPE, SPLIT_V, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: ComplexAbs, RFFT.\r\n```\r\n\r\nI'm trying to convert a model with tf.lite and running into this error. There is another issue https://github.com/tensorflow/tensorflow/issues/27303 that requests the RFFT operator as well, but it seems to be for Tensorflow 1.x. There is a commit https://github.com/tensorflow/tensorflow/commit/c77e7e56de56c624116cf9eea340b4f96f032c85#diff-ed4b7d597384e8e4b1210b7558a16640 that whitelists the RFFT operation, however my conversion fails. Is this only implemented in Tensorflow 1.x right now?\r\n\r\nI'm converting my model using this code:\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(test_model)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\nI've tried using `converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]` and `converter.target_ops = [tf.lite.OpsSet.SELECT_TF_OPS]` but nothing changes.", "comments": ["In order to use select ops in 2.0 please use the following code (detailed [here](https://www.tensorflow.org/lite/r2/convert/python_api#changes_to_tfliteconverter_attributes)):\r\n```\r\nconverter.target_spec.supported_ops = set([tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS])\r\n```", "Thanks @gargn, that worked!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30530\">No</a>\n"]}, {"number": 30529, "title": "How to know about the length of the data, TF2.0?", "body": "I have been following these two tutorials, [1](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) & [2](https://www.tensorflow.org/beta/tutorials/load_data/csv) in order to read a csv file to train a NN model. However, In [tutorial 1](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches), [here exactly](https://www.tensorflow.org/beta/tutorials/load_data/text#load_text_into_datasets), I did not understand how the author chose the values for **BUFFFER** and **TAKE**. It is not documented how to find the total number of examples/elements in the txt files because some file might consisting less or more examples/elements. \r\nThank you!", "comments": ["This would indeed be a useful feature (I do not believe that it is implemented as of now).\r\n\r\nThat being said, in most cases you have some knowledge about your data and set up those constants accordingly... You may however run independent python operations to gather files' lengths (in this tutorial, you simply need to sum the number of rows in each file, which is easily obtained - see below) or, provided you are using Eager execution, do `n_samples = sum(1 for _ in iter(all_labeled_data))`.\r\nTo count rows (which works when you do not have Eager execution on):\r\n```\r\nn_samples = 0\r\nfor file_name in FILE_NAME:\r\n    with open(os.path.join(parent_dir, file_name)) as file:\r\n        n_samples += sum(1 for _ in file)\r\n```\r\nI hope this helps :)", "Hi Paul, \r\nthank you for this suggestion. This is the problem that one has to do it separately, to look into my file (in my case, a csv file) I had to execute a separate script consisting Pandas DF. Neat and clean but there is nothing in TF(2.0). I request Tensorflow's team to provide such feature, if possible.\r\n ", "Hi,\r\nI get that, and personally agree that it would be a useful feature. Let's hope @jvishnuvardhan (and/or other TF people) can make it happen!\r\nFYI, you can count rows in a csv files using the same trick as I posted yesterday (merely opening files with `open` and counting how much rows there are, possibly substracting 1 if you have a header), so as not to depend on pandas (and also not to load the data in memory with it, which is RAM-costly) :)", "I did not know about the not loading data into memory. Thank you, I will try this inexpensive way. :)", "To be more precise, `open` creates a generator that yields the data as it is read. By iterating over the resulting object, you load the data in memory, but only one row at a time, which as you do not attempt to keep it further is also discarded between iterations (well, when the garbage collector passes, which may not be at _every_ iteration, but you get idea). Thus, instead of loading all of the file's content (plus parsing it) with pandas, you iteratively load and discard small chunks (the rows) and only keep track of how many times you were able to do that (thus how many rows there are in the csv file, including the header if any, hence my mentioning substracting one).", "thank you for the nice explanation. :)\ud83d\udc4d ", "The tf.data API provides the [cardinality](https://www.tensorflow.org/api_docs/python/tf/data/experimental/cardinality) operation which returns the size of a tf.data dataset. The caveat is that the operation can return \"unknown\" if it cannot be done in a constant time with respect to the number of elements of the dataset. In particular, the cardinality operation will not attempt to enumerate all elements stored in a file.\r\n\r\nFor an arbitrary (finite) `dataset`, you can always count the number of its elements as follows:\r\n\r\n``` \r\nnum_elements = 0\r\nfor element in dataset:\r\n  num_elements += 1\r\n```\r\n\r\n", "@jsimsa Thank you for pointing `tf.data.experimental.cardinality` out!\r\n\r\n(note: the second solution is similar to the `n_samples = sum(1 for _ in iter(all_labeled_data))` line I pointed out in my initial answer - and it requires Eager execution, which is becoming the norm in tf 2.0)", "Hi @jsimsa , thank you for this information. I tried [cardinality](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/cardinality) but its output is `tf.Tensor(-2, shape=(), dtype=int64)` on my dataset consisting two columns, **Descritption** and **Label**. I am not sure how to get length from this output. However, \r\n\r\n> num_elements = 0\r\nfor element in dataset:\r\n      num_elements += 1\r\n\r\nbehaves as expected. \r\nAlso, Since my dataset has many classes (approx. 100000), I sometimes want to test it on lets' say first 50000 classes. This dataset is sorted on the basis of classes like from class 0,1, 2...100000. Now I want to know the number of elements until class number 50000. Is there a way in TF 2.0, to count elements only until class 50000?. \r\nI can do it using Pandas DF, where I just select classes until 50000, something like `dataset[dataset['label']<51000]`", "@rishabhsahrawat can you share the code for which you receive `-2` (which by the way is a constant used to identify unknown cardinality)\r\n\r\nUnlike Pandas DF, tf.data provide a streaming abstraction and thus supports datasets that do not necessarily fit into memory. If you would like to limit the dataset to its first `X` elements, you can do so through the `take` transformation. If you would like to filter a subset of a dataset based on values of each element, you can do so through the `filter` transformation.", "Hi, here is the code.\r\n`dataset = tf.data.experimental.make_csv_dataset('name_of_file.csv', batch_size = 1, select_columns = ['description', 'Label'], label_name = 'Label', num_epochs = 1 ,shuffle = True)`\r\n`print 'Cardinality: ',tf.data.experimental.cardinality(dataset)`\r\nAs I said, this csv file has two columns, **description** and **Label**.  Total number of elements are **1265814**. \r\nIf you need any other information, please let me know.\r\n\r\nthank you for the information about `tf.filter`, I will test it out.", "Your dataset is read from a file. As per my original response, \"the `cardinality` operation will not attempt to enumerate all elements stored in a file.\" because that's not a constant time operation with respect to the size of the dataset.", "okay, I see. So, there is no other way to find the number of elements in the file except the `for` loop you mentioned earlier.?. Also then how and where to use `cardinality`?", "Correct. `cardinality` works with datasets created through `from_tensors` or `from_tensor_slices`.", "Okay, clear. \r\nI am playing with `filter`, mentioned earlier by you, does it also work on datasets created through `from_tensors` or `from_tensor_slices`? Because in the example [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#filter), the dataset is created using `from_tensor_slices`."]}, {"number": 30528, "title": "tf.keras in TPU working callbacks call TensorBoard is ERROR", "body": "**Tensorflow Version:** 1.13.1\r\n**Working Platom:** colab TPU\r\n\r\n**Use TensorBoard  Happen Exception\uff1a**\r\n\u201cAttributeError: 'TPUFunction' object has no attribute 'fetches'\u201d\r\n\r\n**Exception Detail\uff1a**\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)\r\n   1139       # pylint: disable=protected-access\r\n   1140       # add the histogram summary op if it should run this epoch\r\n-> 1141       if self.merged not in self.model._eval_function.fetches:\r\n   1142         self.model._eval_function.fetches.append(self.merged)\r\n   1143         self.model._eval_function.fetch_callbacks[\r\n```\r\n\r\n**My Code Fragment\uff1a**\r\n\r\n```\r\n#TensorBoard\r\nlog_dir=\"logs/fit/\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,write_graph=True, write_images=True)\r\n\r\ncallbacks_list = [checkpoint,tensorboard_callback]\r\n\r\n#Learning\r\nhistory = tpu_model.fit_generator(generator=train_iter,epochs=100,validation_data=val_iter,callbacks=callbacks_list)\r\n```\r\n", "comments": ["Can you please provide the colab link to reproduce the issue. Thanks!", "> \u4f60\u80fd\u5426\u63d0\u4f9bcolab\u94fe\u63a5\u6765\u91cd\u73b0\u8fd9\u4e2a\u95ee\u9898\u3002\u8c22\u8c22\uff01\r\n\r\nhttps://colab.research.google.com/drive/1VYboO5LMbrcHgTeGmh7Y1Sb4SC_GlwoW#scrollTo=7MgrKf1mvvex", "I am not able to access the Colab link provided. Request you to grant me the access to reproduce the issue.Thanks!", "> I am not able to access the Colab link provided. Request you to grant me the access to reproduce the issue.Thanks!\r\n\r\nhttps://colab.research.google.com/drive/1VYboO5LMbrcHgTeGmh7Y1Sb4SC_GlwoW", "> I am not able to access the Colab link provided. Request you to grant me the access to reproduce the issue.Thanks!\r\n\r\nFull Code:\r\n```\r\nimport os\r\nimport datetime\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras import optimizers\r\nfrom tensorflow.keras.applications.vgg16 import VGG16\r\nfrom tensorflow.keras.preprocessing import image\r\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard\r\nfrom tensorflow.keras.backend import set_session\r\n\r\nfrom google.colab import drive\r\nfrom google.colab import files\r\ndrive.mount('/dataset')\r\n!mkdir /dataset1\r\n!ln -s /dataset/'My Drive'/newdataset /dataset1\r\n\r\nprint(\"Tensorflow Version:\",tf.__version__)\r\nprint(\"Tensorflow Keras Version:\",tf.keras.__version__)\r\n\r\ntrain_dir = \"/dataset1/newdataset/train\"\r\ntest_dir = \"/dataset1/newdataset/test\"\r\n!mkdir /dataset1/newdataset/checkpoints\r\n!mkdir /dataset1/newdataset/log\r\ntrain_pic_gen=ImageDataGenerator(rescale=1./255,rotation_range=20,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.5,horizontal_flip=True,fill_mode='nearest')\r\ntest_pic_gen=ImageDataGenerator(rescale=1./255)\r\ntrain_iter=train_pic_gen.flow_from_directory(train_dir,(224,224),batch_size=128*2,class_mode='categorical')\r\nval_iter=test_pic_gen.flow_from_directory(test_dir,(224,224),batch_size=128*2,class_mode='categorical')\r\n\r\nbase_model = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\r\n#\u5728vgg16\r\nout = base_model.layers[-1].output\r\nflatten  = layers.Flatten(name=\"flatten\")(out)\r\nfc_1  = layers.Dense(1024, activation='relu')(flatten)\r\nfc_1_droupout = layers.Dropout(0.5)(fc_1)\r\nfc_2 = layers.Dense(512, activation='relu')(fc_1_droupout)\r\nfc_2_droupout = layers.Dropout(0.3)(fc_2)\r\noutput = layers.Dense(6, activation='softmax',name=\"output\")(fc_2_droupout)\r\ntuneModel = Model(inputs=base_model.input, outputs = output)\r\ntuneModel.compile(loss='categorical_crossentropy', optimizer= tf.train.RMSPropOptimizer(learning_rate=1e-4) ,metrics=['accuracy'])\r\n#TPU\r\nTPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\ntpu_model = tf.contrib.tpu.keras_to_tpu_model(tuneModel,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\r\n#TensorBoard\r\nlog_dir=\"/dataset1/newdataset/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\r\ncallbacks_list = [tensorboard_callback]\r\n\r\nhistory = tpu_model.fit_generator(generator=train_iter,steps_per_epoch=100,epochs=300,validation_data=val_iter,validation_steps=32,callbacks=callbacks_list)\r\n```\r\n\r\nRunning After Exception:\r\n```\r\nDrive already mounted at /dataset; to attempt to forcibly remount, call drive.mount(\"/dataset\", force_remount=True).\r\nmkdir: cannot create directory \u2018/dataset1\u2019: File exists\r\nln: failed to create symbolic link '/dataset1/newdataset': File exists\r\nTensorflow Version: 1.13.1\r\nTensorflow Keras Version: 2.2.4-tf\r\nmkdir: cannot create directory \u2018/dataset1/newdataset/checkpoints\u2019: File exists\r\nmkdir: cannot create directory \u2018/dataset1/newdataset/log\u2019: File exists\r\nFound 1411 images belonging to 6 classes.\r\nFound 326 images belonging to 6 classes.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nINFO:tensorflow:Querying Tensorflow master (grpc://10.26.221.130:8470) for TPU system metadata.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 7621372480413121604)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16455206848570694857)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3055192024204661033)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2959337379146398183)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 8179524105136487244)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7965734704452416620)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5051508239641363858)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 7532269395245277437)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 9200636892927262848)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 12745496176264895538)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16231103514035382418)\r\nWARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-996aea90e789> in <module>()\r\n     50 callbacks_list = [tensorboard_callback]\r\n     51 \r\n---> 52 history = tpu_model.fit_generator(generator=train_iter,steps_per_epoch=100,epochs=300,validation_data=val_iter,validation_steps=32,callbacks=callbacks_list)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)\r\n   1139       # pylint: disable=protected-access\r\n   1140       # add the histogram summary op if it should run this epoch\r\n-> 1141       if self.merged not in self.model._eval_function.fetches:\r\n   1142         self.model._eval_function.fetches.append(self.merged)\r\n   1143         self.model._eval_function.fetch_callbacks[\r\n\r\nAttributeError: 'TPUFunction' object has no attribute 'fetches'\r\n\r\n```", "@blackhu setting histogram_freq = 1 was making it compute histograms at every epoch. That is the cause of error. Please take a look at this [issue](https://stackoverflow.com/questions/55085661/unable-to-attach-tensorboard-callback-in-tensorflow-keras) where the solution is described and let me know if it helps. Thanks! \r\nAlso please note that this platform is only for bug/performance, feature request or related issues. Please post these kind of issues on Stack overflow as there is a bigger community to help. Thanks!"]}, {"number": 30527, "title": "I want to connect two networks,but No gradients provided for any variable", "body": "I trained a network, and I want to add another network before the trained network\r\n\r\n`with tf.name_scope('inNN'):\r\n    T_lamb = tf.placeholder(tf.float32, [None, 2], name='inNN_inputs')\r\n    Ts = tf.placeholder(tf.float32, [None, 1], name='T_labels')\r\n    Ds = tf.placeholder(tf.float32, [None, 4], name='D_labels')\r\n    DLs = tf.placeholder(tf.float32, [None, 5], name='D_lamb')\r\n\r\nweight, bias = get_w_b()\r\nD_logits_op = inNN(T_lamb, None)\r\nprediction = fwNN(DLs, None, weight, bias)\r\nwith tf.variable_scope('T_loss'):\r\n    T_loss_op = losses(prediction, Ts)\r\nwith tf.variable_scope('D_loss'):\r\n    D_loss_op = losses(D_logits_op, Ds)\r\nglobal_step = tf.Variable(0, trainable=0)\r\n\r\nvalid_op = evaluation(prediction, Ts)\r\nwith tf.variable_scope('train_step'):\r\n    train_step_op = training(T_loss_op, LEARNING_RATE, global_step)  # error: No gradients provided for any variable, check your graph for ops that do not support gradients`\r\n\r\nwhen I choose the D_loss_op in the training. The network works", "comments": ["This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30526, "title": "not show `raise ValueError` but `unsupported operand` error when using modular arithmetic as minus integer in TF 1.13.1, 1.14.0", "body": "1. tf 1.13.1\r\n![image](https://user-images.githubusercontent.com/10525011/60887589-bc055580-a28f-11e9-8cfd-c08e960a227e.png)\r\n\r\n2. tf1.14.0\r\n![image](https://user-images.githubusercontent.com/10525011/60888239-4bf7cf00-a291-11e9-9f37-4c9781883e86.png)\r\n\r\n\r\n3. tf2.0.0a0\r\n![image](https://user-images.githubusercontent.com/10525011/60887966-a5133300-a290-11e9-9831-9470b875d5fe.png)\r\n\r\nWhy used `NotImplemented`in `__mod__` and `__rmod__` https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_shape.py#L221? **It makes to be confused '%' operand is not working**\r\n\r\nwhen people try modular arithmetic with minus integer, error with `raise ValueError(\"Dimension %d must be >= 0\" % self._value)` is feel more good instead of error with `TypeError: unsupported operand type(s) for %:`\r\n\r\nWhat else?\r\n```python\r\n  def __mod__(self, other):\r\n    \"\"\"Returns `self` modulo `other`.\r\n\r\n    Dimension moduli are computed as follows:\r\n\r\n    ```python\r\n    tf.Dimension(m)    % tf.Dimension(n)    == tf.Dimension(m % n)\r\n    tf.Dimension(m)    % tf.Dimension(None) == tf.Dimension(None)\r\n    tf.Dimension(None) % tf.Dimension(n)    == tf.Dimension(None)\r\n    tf.Dimension(None) % tf.Dimension(None) == tf.Dimension(None)\r\n    ```\r\n\r\n    Args:\r\n      other: Another Dimension, or a value accepted by `as_dimension`.\r\n\r\n    Returns:\r\n      A Dimension whose value is `self` modulo `other`.\r\n    \"\"\"\r\n    other = as_dimension(other)\r\n    if self._value is None or other.value is None:\r\n      return Dimension(None)\r\n    else:\r\n      return Dimension(self._value % other.value)\r\n```\r\nThanks", "comments": ["Added PR #30563 for the fix.", "@yoneken Hello. This is my first time leave issue in tensorflow. I dont know how contribution system working, so I leave issue before pull request and check comment for this issue, then I have plan to pull request. I think it's have more priority for who finding issue How do you think about it? Thanks", "Thank you for the issue and the PR.\r\n\r\nAs the Pr is merged, closing the issue"]}, {"number": 30525, "title": "how to apply quant to bert", "body": "my bert is slow for prediction using cpu, can quant speed up its forward process? how to use?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Do you mean mobile cpu? You can try: https://www.tensorflow.org/lite/performance/post_training_quantization\r\n\r\nFor gpu, you can try to cast embedding variables to fp16 to enable full fp16 inference.\r\nFor tpu, casting to bfloat16 may keep better accuracy.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30524, "title": "Failed to convert ssdlite_mobilenet_v2_coco_2018_05_09 to tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colaboratory\r\n- TensorFlow installed from (source or binary): idk\r\n- TensorFlow version (or github SHA if from source): 1.14\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nConverterError                            Traceback (most recent call last)\r\n\r\n<ipython-input-5-fc0e59056dc1> in <module>()\r\n----> 1 tflite_model = converter.convert()\r\n      2 open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n2 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    902           input_arrays_with_shape=self._input_arrays_with_shape,\r\n    903           output_arrays=self._output_arrays,\r\n--> 904           **converter_kwargs)\r\n    905 \r\n    906     if self._is_calibration_quantize():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_graph_def(input_data, input_arrays_with_shape, output_arrays, *args, **kwargs)\r\n    371   data = toco_convert_protos(model_flags.SerializeToString(),\r\n    372                              toco_flags.SerializeToString(),\r\n--> 373                              input_data.SerializeToString())\r\n    374   return data\r\n    375 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)\r\n    170       stderr = _try_convert_to_unicode(stderr)\r\n    171       raise ConverterError(\r\n--> 172           \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    173   finally:\r\n    174     # Must manually cleanup files.\r\n\r\nConverterError: TOCO failed. See console for info.\r\n2019-07-09 12:08:44.812119: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2019-07-09 12:08:44.833564: F tensorflow/lite/toco/tooling_util.cc:918] Check failed: GetOpWithOutput(model, output_array) Specified output array \"detection_boxes\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.\r\nAborted (core dumped)\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible:\r\nhttps://drive.google.com/open?id=1HJ0bnrOok4Brd5IcB30HYn80itE4dH8_\r\n\r\n**Any other info / logs**\r\n\r\nI'm using model ssdlite_mobilenet_v2_coco_2018_05_09 from zoo. Have trained it and converted ckpt with command:\r\n`export_tflite_ssd_graph.py --pipeline_config_path training/ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-19780 --output_directory tflite_graph --add_postprocessing_op True`\r\n\r\nI'm using Python API for conversion of model. My code:\r\n```\r\nimport tensorflow as tf\r\ngraph_def_file = \"/content/tflite_graph.pb\"\r\ninput_arrays = [\"normalized_input_image_tensor\"]\r\noutput_arrays = [\"detection_boxes\", \"detection_classes\", \"detection_scores\", \"num_boxes\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file, \r\n  input_arrays, \r\n  output_arrays, \r\n  input_shapes={'normalized_input_image_tensor':[1, 300, 300, 3]}\r\n  )\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```", "comments": ["Hi @AlexZot, I'm unable to access the model you linked, would you mind updating?\r\n\r\nWhat happens if you change `output_arrays` to:\r\n```\r\noutput_arrays = [\r\n        'TFLite_Detection_PostProcess', 'TFLite_Detection_PostProcess:1',\r\n        'TFLite_Detection_PostProcess:2', 'TFLite_Detection_PostProcess:3'\r\n    ]\r\n```", "Hi @AlexZot, \r\n\r\ntry `output_arrays = ['raw_outputs/box_encodings','raw_outputs/class_predictions']`\r\n\r\nI'm facing the same issue. But I want to convert the ssdlite model from model zoo directly without changing the model. (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)\r\n\r\nI tried using export_tflite_ssd_graph.py this way:\r\n`python export_tflite_ssd_graph.py --pipeline_config_path pipeline.config --trained_checkpoint_prefix model.ckpt --output_directory my_frozen_graph --add_postprocessing_op True`\r\n\r\nI want a model with 4 outputs, it should be `output_arrays = ['detection_boxes', 'detection_classes', 'detection_scores', 'num_boxes']` but I get `output_arrays = ['raw_outputs/box_encodings','raw_outputs/class_predictions']`. How export_tflite_ssd_graph.py has to be used to get a model with the expected 4 outputs?", "@jdduke Here is link: https://drive.google.com/open?id=1HJ0bnrOok4Brd5IcB30HYn80itE4dH8_\r\n@val9299 I know that is working with raw outputs, but it means that I have to decode predictions manually. ", "Hey Alex, can you try adding `converter.allow_custom_ops = True` ?", "I opened a new issue for specifically my case: https://github.com/tensorflow/tensorflow/issues/31015\r\nThanks for helping!", "@srjoglekar246 omg, that's worked! Thank you good man! How does this array: ['detection_boxes', 'detection_classes', 'detection_scores', 'num_boxes'] corresponds to this: [TFLite_Detection_PostProcess', 'TFLite_Detection_PostProcess:1', 'TFLite_Detection_PostProcess:2', 'TFLite_Detection_PostProcess:3']\r\n", "I also tried the suggestion. But it looks like the new tflite model has raw outputs just in another format. I thought the goal would be having processed outputs like detection_boxes, detection_classes,...? To get a model like this, something already has to be changed in the freezing process with export_tflite_ssd_graph.py but I also would like to know what has to be changed to get it working.", "@AlexZot The `TFLite_Detection_PostProcess:1` value lets the converter know that `detection_classes` is the output from the post-processing op at index 1. ", "> Hey Alex, can you try adding `converter.allow_custom_ops = True` ?\r\n\r\nSorry to bother, I meet this `Converting unsupported operation: TFLite_Detection_PostProcess` too. Where should I add this `converter.allow_custom_ops = True`?", "> @srjoglekar246 omg, that's worked! Thank you good man! How does this array: ['detection_boxes', 'detection_classes', 'detection_scores', 'num_boxes'] corresponds to this: [TFLite_Detection_PostProcess', 'TFLite_Detection_PostProcess:1', 'TFLite_Detection_PostProcess:2', 'TFLite_Detection_PostProcess:3']\r\n\r\nSo how to solve this problem `Converting unsupported operation: TFLite_Detection_PostProcess` exactly? \r\n(1)export_tflite_ssd_graph.py --pipeline_config_path training/ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-19780 --output_directory tflite_graph --add_postprocessing_op True. ---->  add_postprocessing_op True or Flase\r\n", "@kangkang59812 Please look at the following snippet in the first comment:\r\n\r\n```\r\nimport tensorflow as tf\r\ngraph_def_file = \"/content/tflite_graph.pb\"\r\ninput_arrays = [\"normalized_input_image_tensor\"]\r\noutput_arrays = [\"detection_boxes\", \"detection_classes\", \"detection_scores\", \"num_boxes\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file, \r\n  input_arrays, \r\n  output_arrays, \r\n  input_shapes={'normalized_input_image_tensor':[1, 300, 300, 3]}\r\n  )\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nThere, we need to add `converter.allow_custom_ops = True` for the TFLite SSD inference to work.\r\n\r\nAnd about your second comment: Yes, we need to set `--add_postprocessing_op True`", "@srjoglekar246  The produced  `converted_model.tflite` is same as the .tflite produced by `bazel` command.  What makes me confused is that the result is different  between `converted_model.tflite` and `tflite_graph.pb`.  I think it still can't convert the `postprocessing_op` sucessfully. For example, the bbox has negative values in `converted_model.tflite`'s result, but the values are nomal in 'tflite_graph.pb''s result. So the `postprocessing_op` may not work well as  `tf.image.non_max_suppression`. In [running_on_mobile_tensorflowlite.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md), it says \"add_postprocessing flag enables the model to take advantage of a custom optimized detection post-processing operation which can be thought of as a replacement for tf.image.non_max_suppression\". But the result is totally different. How can solve it ?", "The result, mathematically, shouldn't be that different with our custom op (atleast for 10 detections or so). The reason we do this, is that using the custom op is much easier/faster than using multi-class Non-Max-Suppression that tensorflow does in its graphs.\r\n\r\nThere is one way to do this with our experimental new converter, which supports builtin NMS ops without using the custom op.\r\n\r\nCould you try the following:\r\n1. Modify `pipeline config` as follows:\r\n\r\nBefore:\r\n```\r\n   post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.300000011921\r\n        iou_threshold: 0.600000023842\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n```\r\n\r\nAfter:\r\n```\r\npost_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.300000011921\r\n        iou_threshold: 0.600000023842\r\n        max_detections_per_class: 10\r\n        max_total_detections: 10\r\n        use_class_agnostic_nms: true\r\n        use_static_shapes: true\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n```\r\n\r\nThis reduces the complexity of the NMS graph for TFLite inference.\r\nAlso remove this from the config, if present:\r\n`batch_norm_trainable: true`\r\n\r\n2. Then, export the inference graph using [TF Object Detection's script](https://pythonprogramming.net/testing-custom-object-detector-tensorflow-object-detection-api-tutorial/) with the following params:\r\n```\r\n--input_type image_tensor --input_shape 1,<height>,<width>,3 --pipeline_config_path /path/to/pipeline.config --trained_checkpoint_prefix /path/to/model.ckpt --output_directory /path/to/output\r\n```\r\n\r\n3. Then convert using our experimental converter:\r\nAdd this before conversion:\r\n`converter.experimental_new_converter = True`\r\n\r\nUse these output arrays:\r\n```\r\ndetection_boxes,Postprocessor/BatchMultiClassNonMaxSuppression/stack_3,detection_scores,num_detections\r\n```\r\nUse this input array:\r\n```\r\nPreprocessor/sub\r\n```\r\nInput data type:\r\n```\r\nDT_FLOAT\r\n```\r\n", "@srjoglekar246  Thanks for your patient . I followed the below steps:\r\n1. Following your advice, I change the pipline.config. ( I trained the model use the default config.)\r\n2. I use \r\n```\r\npython export_inference_graph.py \\\r\n--pipeline_config_path=$CONFIG_FILE \\\r\n--trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n--output_directory=$OUTPUT_DIR \\\r\n--input_type=image_tensor \\\r\n--input_shape=1,300,300,3 \\\r\n``` \r\n\r\n to get the frozen_inference_graph.pb file\r\n\r\n\r\n3.  Then  use tf 1.13.0-dev20190101(I trained the model with tf 1.12, and it doesn't have tf.lite)\r\n```\r\nimport tensorflow as tf\r\ngraph_def_file = \"/Users/kangkang/Documents/code/convert/frozen_inference_graph.pb.pb\"\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file,\r\n    input_arrays=['Preprocessor/sub'],\r\n    output_arrays=['detection_boxes', 'Postprocessor/BatchMultiClassNonMaxSuppression/stack_3',\r\n                   'detection_scores', 'num_detections'],\r\n    input_shapes={'Preprocessor/sub': [1, 300, 300, 3]})\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\ntf_model_files = 'newpost22.tflite'\r\nopen(tf_model_files, \"wb\").write(tflite_model)\r\n```\r\nThats all what I have done.  \r\n\r\n(1)Where should I insert your `Input data type:DT_FLOAT`? \r\n(2) I got the error \r\n```\r\nF tensorflow/contrib/lite/toco/import_tensorflow.cc:2137] Check failed: status.ok() Neither input_content (0) nor float_val (1915) have the right dimensions (1917) for this float tensor\r\n(while processing node 'Postprocessor/Decode/get_center_coordinates_and_sizes/add')\r\n```\r\nAnd when I use tf 1.14.0, the error is changed\r\n`Check failed: status.ok() Neither input_content (0) nor float_val (7667) have the right dimensions (7668) for this float tensor (while processing node 'Postprocessor/Reshape')`\r\nI am  Very confused about the input and output according to different config.\r\n\r\n(3)I upload the [checkpoint](https://drive.google.com/drive/folders/1oaShwaiXsbkPfwG1wa-vDcnFwO9RILCP) on google drive. The model(based on ssd_mobilenet_v2_coco_2018_03_29) is trained on my datasets(55 classes) and the results are good enough on PC  using `saved_model.pb` . I Just want the same result on `detect.tflite`. \r\n\r\n(4)Before your advice, I use `export_tflite_ssd_graph.py` to get `tflite.pb `and then use `bazel` to get the `detect.tflite`. But the result is different except Top1.\r\n\r\nSo should I use` export_inference_graph.py` to get the `frozen_inference_graph.pb` to produce `detect.tflite` or use `export_tflite_ssd_graph.py` to get the `tflite.pb` to produce `detect.tflite`? And at the same time, the result is same on .pb and .tflite. \r\n\r\nThanks sincerely!!!", "If latency isn't an issue for you, just use `export_tflite_ssd_graph.py` with the extra param `--use_regular_nms=true`. Does that improve the accuracy?", "@srjoglekar246 No, the .tflite result on Android is still not the same as .pb result on PC. I have tried `allow_custom_ops ` \u3001`--use_regular_nms=true` and other methods in the other issues. "]}, {"number": 30523, "title": "Keras Tensorflow R  Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 home/Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA = 10.1; cuDNN = 7.6\r\n- GPU model and memory: Nvidia RTX 2060 6GB; RAM = 16GB; CPU = Intel I7 8750h \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nHi all,\r\n\r\nI know this problem has arisen on here but I have yet to find a discussion around the problem with regards to R; all I see are python issues and solutions. In short I am using the Keras library with Tensorflow GPU as a backend in Rstudio for deep learning, specifically convnets. After a bit of trouble installing Cuda/cuDNN etc. I managed to get it all working, and installed Tensorflow-gpu in Anaconda (I am working within an Anaconda environment). This all worked fine and yesterday I was running my models on my GPU in R with no problems. \r\n\r\nWithout making any changes whatsoever, I boot up today and run the same models and get an error telling me that cuDNN cannot initialise as Tensorflow could not create a cuDNN handle. I am not sure why this has emerged all of a sudden as I was having no problems yesterday.\r\n\r\n**Describe the expected behavior**\r\n\r\nAs mentioned, yesterday the models were running perfectly fine and I expected the same to happen today as I have made no changes to my software/hardware in between.\r\n\r\n**Code to reproduce the issue**\r\n\r\nAny code that attempts to compile a Keras model produces this error. However I will provide below the exact code I used:\r\n\r\n`img_height <- 32\r\nimg_width <- 32\r\nchannels <- 3\r\n\r\n### Full convolutional model\r\nmodel <- keras_model_sequential() %>%\r\n  \r\n  ### input layer\r\n  layer_conv_2d(filters = 96,\r\n                kernel_size = c(3,3),\r\n                activation = \"relu\",\r\n                padding = \"same\",\r\n                input_shape = c(img_height, img_width, channels)) %>%\r\n  layer_dropout(0.2) %>%\r\n\r\n  ### hidden block 1\r\n  layer_conv_2d(filters = 96,\r\n                kernel_size = c(3,3),\r\n                activation = \"relu\",\r\n                padding = \"same\") %>%\r\n  layer_conv_2d(filters = 96,\r\n                kernel_size = c(3,3),\r\n                activation = \"relu\",\r\n                padding = \"same\",\r\n                strides = 2) %>%\r\n  layer_dropout(0.5) %>%\r\n  \r\n  ## hidden block 2\r\n  layer_conv_2d(filters = 192,\r\n                kernel_size = c(3,3),\r\n                activation = \"relu\",\r\n                padding = \"same\") %>%\r\n  layer_conv_2d(filters = 192,\r\n                kernel_size = c(3,3),\r\n                activation = \"relu\",\r\n                padding = \"same\") %>%\r\n  layer_conv_2d(filters = 192,\r\n                kernel_size = c(3,3),\r\n                activation = \"relu\",\r\n                padding = \"same\",\r\n                strides = 2) %>%\r\n  layer_dropout(0.5) %>%\r\n  \r\n  ### hidden block 3\r\n  layer_conv_2d(filters = 192,\r\n                kernel_size = c(3,3),\r\n                padding = \"same\") %>%\r\n  layer_activation(\"relu\") %>%\r\n  layer_conv_2d(filters = 192,\r\n                kernel_size = c(1,1),\r\n                padding = \"valid\") %>%\r\n  layer_activation(\"relu\") %>%\r\n  layer_conv_2d(filters = 200,\r\n                kernel_size = c(1,1),\r\n                padding = \"valid\") %>%\r\n  \r\n  ### global pooling layer (in place of flattening)\r\n  layer_global_average_pooling_2d() %>%\r\n  \r\n  ### softmax output (takes as input the filters from the last conv layer in place of FC layer)\r\n  layer_activation(\"softmax\")\r\n  \r\n\r\n### Compiler Architecture:\r\n\r\nmodel %>% compile(\r\n  optimizer = optimizer_adam(lr = 0.1), # high learning rate for large classes\r\n  loss = \"categorical_crossentropy\",\r\n  metrics = c(\"accuracy\")\r\n)\r\n\r\n### view model architecture\r\nmodel\r\n\r\n### data augmentation\r\ndatagen = image_data_generator(\r\n  rescale = 1/255,\r\n  rotation_range = 40,\r\n  width_shift_range = 0.2,\r\n  height_shift_range = 0.2,\r\n  shear_range = 0.2,\r\n  zoom_range = 0.2,\r\n  horizontal_flip = TRUE\r\n)\r\n\r\n### validation data generator (should not be augmented)\r\nvalidation_datagen = image_data_generator(\r\n  rescale = 1/255\r\n)\r\n\r\n### define batch size \r\nbatch_size <- 32\r\n\r\n### define no. epochs\r\nepochs <- 50\r\n\r\n### define class mode\r\nclass_mode <- \"categorical\"\r\n\r\n### create generator for training data\r\ntrain_generator <- flow_images_from_directory(\r\n  train_dir,\r\n  datagen,\r\n  target_size = c(img_height, img_width),\r\n  batch_size = batch_size,\r\n  classes = classes,\r\n  class_mode = class_mode\r\n)\r\n\r\nvalidation_generator <- flow_images_from_directory(\r\n  validation_dir,\r\n  validation_datagen,\r\n  target_size = c(img_height, img_width),\r\n  batch_size = (batch_size/9), # validation data is 1/9th of training so reduce the batch size\r\n  classes = classes,\r\n  class_mode = class_mode\r\n)\r\n\r\n### define no. training samples\r\ntrain_samples <- train_generator$n # 7000 training data\r\n\r\n### define no. validation samples\r\nvalid_samples <- validation_generator$n # 800 validation data\r\n\r\n### fit the model to the data\r\nhistory <- model %>% fit_generator(\r\n  train_generator,\r\n  steps_per_epoch = as.integer(train_samples / batch_size),\r\n  epochs = epochs,\r\n  validation_data = validation_generator,\r\n  validation_steps = as.integer(valid_samples / (batch_size/9))\r\n)`\r\n\r\nNB. This model is using the CUB200-2011 dataset.\r\n\r\n**Other info / logs**\r\nFull traceback of the issue is provided in the log below:\r\n\r\n2019-07-09 10:56:45.085345: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2019-07-09 10:56:45.328872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.2\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.89GiB\r\n2019-07-09 10:56:45.329507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-07-09 10:56:46.571437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-09 10:56:46.571725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-07-09 10:56:46.571847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-07-09 10:56:46.573101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4620 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-07-09 10:56:49.550878: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2019-07-09 10:56:49.551567: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n Show Traceback\r\n \r\n Rerun with Debug\r\n Error in py_call_impl(callable, dots$args, dots$keywords) : \r\n  UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node conv2d_9/Conv2D}}]]\r\n\t [[{{node ConstantFoldingCtrl/loss/activation_5_loss/broadcast_weights/assert_broadcastable/AssertGuard/Switch_0}}]] \r\n\r\nI have also attempted to run this on Ubuntu too in case the problem is related to Windows' compilation, with the exact same error:\r\n\r\n2019-07-09 11:22:49.942666: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: \r\nSSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-07-09 11:22:49.966646: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz\r\n2019-07-09 11:22:49.967415: \r\nI tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5611f27510e0 executing computations on platform Host. Devices:\r\n2019-07-09 11:22:49.967431: \r\nI tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-09 11:22:50.051695: \r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, \r\nso returning NUMA node zero\r\n2019-07-09 11:22:50.051950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: \r\nGeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.2\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.79GiB freeMemory: 5.43GiB\r\n2019-07-09 11:22:50.051963: \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-07-09 11:22:50.052384: \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-09 11:22:50.052392: \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-07-09 11:22:50.052396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   \r\nN \r\n2019-07-09 11:22:50.052445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5259 MB memory) \r\n-> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-07-09 11:22:50.053557: I tensorflow/compiler/xla/service/service.cc:150] \r\nXLA service 0x5611f28776a0 executing computations on platform CUDA. Devices:\r\n2019-07-09 11:22:50.053569: I tensorflow/compiler/xla/service/service.cc:158]   \r\nStreamExecutor device (0): GeForce RTX 2060, Compute Capability 7.5\r\n2019-07-09 11:22:54.658481: I tensorflow/stream_executor/dso_loader.cc:152] \r\nsuccessfully opened CUDA library libcublas.so.10.0 locally\r\n2019-07-09 11:23:28.099635: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] \r\nCould not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-07-09 11:23:28.114124: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] \r\nCould not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n Show Traceback\r\n \r\n Rerun with Debug\r\n Error in py_call_impl(callable, dots$args, dots$keywords) : \r\n  \r\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\n\t [[{{node conv2d/Conv2D}}]]\r\n\t [[{{node loss/mul}}]] \r\n\r\nI have also provided an output of nvidia-smi to show the driver versions and installations of CUDA. Likewise, this also demonstrates how Tensorflow is detecting my GPU in Rstudio to show how the problem may not be related to that:\r\n\r\nnvidia-smi\r\nTue Jul 09 11:05:43 2019\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 425.25       Driver Version: 425.25       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 2060   WDDM  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   41C    P8     4W /  N/A |   5256MiB /  6144MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     13156    C+G   ...eam\\bin\\cef\\cef.win7\\steamwebhelper.exe N/A      |\r\n|    0     16016      C   ...tensorflow_gpu\\Library\\bin\\rsession.exe N/A      |\r\n+-----------------------------------------------------------------------------+\r\n\r\nHere I offer a more complete overview of the software used for this:\r\n\r\nSoftware:\r\nOS - Windows 10 Home/ Ubuntu 18.04 (Kabuntu)\r\nGPU Drivers - 425.25 (Windows) /430.25 (Ubuntu)\r\nCuda - 10.1 (Windows) / 10.2 (Ubuntu)\r\nCudnn - 7.6 \r\nTF - 1.13.1 (GPU version)\r\nKeras - 2.2.4 \r\nPython - 3.6.8 \r\nRstudio - 1.1.456\r\nBase R - 3.6.1\r\n\r\nI hope this is enough to help others understand my problem. As I said, I know many have reported this issue but I have yet to find a thread where the discussion centres around the R version of Keras rather than Python and would like to know if there is anything anyone knows of to help.\r\nThanks in advance :)\r\n", "comments": ["@Matt-Peters97 Were you able to run any simple tensorflow-gpu models? Was there any change in NVIDIA drivers after restart? I think it is not related to Keras. It will be more related to NVIDIA driver, CUDA, cuDNN drivers. Please let us know what you think. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30522, "title": "[MICRO] Fix for custom operator", "body": "Changed micro_mtable_op_resolver.cc\r\nto use BuiltinOperator_CUSTOM.", "comments": []}, {"number": 30521, "title": "Loading Keras model: TypeError: __init__() missing 1 required positional argument: 'fn'", "body": "- Installed with `pip install tensorflow-gpu`, and also tried with `pip install tensorflow` which yielded the same issue.\r\n- tensorflow 1.14\r\n- Keras-Applications 1.0.8 \r\n- Keras-Preprocessing 1.1.0\r\n\r\nAfter saving model with `tf.keras.models.save_model(model=self.predict_model, filepath=path)`, we try to load with `tf.keras.models.load_model(path, custom_objects={'tf': tf})` and it yields the error that I reported in the title.\r\nWhat possible conflict of packages may I have? Or is this a known bug?\r\n", "comments": ["All packages in the environment:\r\n```\r\nPackage              Version  Location\r\n-------------------- -------- ------------------------\r\nabsl-py              0.7.1\r\nastor                0.8.0\r\nbackcall             0.1.0\r\nboto3                1.9.184\r\nbotocore             1.12.184\r\ncycler               0.10.0\r\ndecorator            4.4.0\r\ndocutils             0.14\r\ngast                 0.2.2\r\ngoogle-pasta         0.1.7\r\ngrpcio               1.22.0\r\nh5py                 2.9.0\r\nipython              7.6.1\r\nipython-genutils     0.2.0\r\njedi                 0.14.0\r\njmespath             0.9.4\r\njoblib               0.13.2\r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.0\r\nkiwisolver           1.1.0\r\nMarkdown             3.1.1\r\nmatplotlib           3.1.1\r\nnetworkx             2.3\r\nnltk                 3.4.4\r\nnumpy                1.16.4\r\npandas               0.24.2\r\nparso                0.5.0\r\npexpect              4.7.0\r\npickleshare          0.7.5\r\npip                  19.1.1\r\nprompt-toolkit       2.0.9\r\nprotobuf             3.8.0\r\nptyprocess           0.6.0\r\nPygments             2.4.2\r\npyparsing            2.4.0\r\npython-dateutil      2.8.0\r\npytz                 2019.1\r\nPyYAML               5.1.1\r\nrecordclass          0.11.1\r\ns3transfer           0.2.1\r\nscikit-learn         0.21.2\r\nscipy                1.3.0\r\nseaborn              0.9.0\r\nsetuptools           41.0.1\r\nsix                  1.12.0\r\nsklearn              0.0\r\ntensorboard          1.14.0\r\ntensorflow           1.14.0\r\ntensorflow-estimator 1.14.0\r\ntermcolor            1.1.0\r\ntorch                1.1.0\r\ntqdm                 4.32.2\r\ntraitlets            4.3.2\r\nurllib3              1.25.3\r\nwcwidth              0.1.7\r\nWerkzeug             0.15.4\r\nwheel                0.33.4\r\nwordninja            0.1.5\r\nwrapt                1.11.2\r\n```", "In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Just create any Keras' model, save it and load with the commands I mentioned", "@JoaoLages \r\nCan you please check attached zip folder.I am able to save and load the Keras model.Please, let me know if i am wrong.Thanks!\r\n[keras.zip](https://github.com/tensorflow/tensorflow/files/3381132/keras.zip)\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "The problem occurs when saving a Keras model (`tf.keras.models.save_model` function) in tensorflow-gpu==1.14.0 and loading it in tensorflow==1.14 (`tf.keras.models.load_model` function)\r\n", "@JoaoLages \r\nI tried with tensorflow-gpu==1.14.0 and able to save and load successfully.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/014783bb590e13b1884256509b83d0d8/untitled249.ipynb).Thanks!", "@ravikyram can you try to load it with `tensorflow==1.14`? (and not tensorflow-gpu) ", "@JoaoLages \r\nLoaded using `!pip install tensorflow==1.14`in the below attachment . Request you to have a look on this and let me know if the issue still persists.Thanks!\r\n[keras.zip](https://github.com/tensorflow/tensorflow/files/3690276/keras.zip)\r\n\r\n", "I am closing the issue since I am not able to reproduce this error outside of my environment. ", "Solved!\r\n`self.model = tf.keras.models.load_model(model_file, custom_objects={'tf': tf}, compile=False)` - not able to use the loss function but works for prediction"]}, {"number": 30520, "title": "[XLA] Add option to tfcompile HLO proto", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30520) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30520) for more info**.\n\n<!-- ok -->", "Ok, thanks for review. My first TF PR so want to make sure I have the style/conventions right \ud83d\ude42", "This breaks some internal tests.  You can reproduce the error `bazel test //tensorflow/compiler/aot/tests:all_tests -c opt`, we don't test these by default since that would end up building LLVM twice, once for the host build and once for the target build.\r\n\r\nRight now `//tensorflow/compiler/aot/tests:all_tests` is broken for some unrelated reasons, but I have a fix in flight that should land soon.", "@sanjoy  Any update on this PR, please. ", "> Right now `//tensorflow/compiler/aot/tests:all_tests` is broken for some unrelated reasons, but I have a fix in flight that should land soon.\r\n\r\nThis is fixed now so @rjeli should be able to run the AOT tests with this patch and fix them.", "@rjeli Could you please address the reviewer comments. Thanks!", "Can one of the admins verify this patch?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 30519, "title": "Fix linking error of CoreFoundation for iOS", "body": "It appears linking error while building iOS package for arm64 only.\r\ntensorflow/contrib/makefile/build_all_ios.sh -a arm64", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30519) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30519) for more info**.\n\n<!-- ok -->", "Can you rebase this on current master? Somehow this doesn't seem to see the changes in #30279 but GitHub doesn't report any conflict.", "> Can you rebase this on current master? Somehow this doesn't seem to see the changes in #30279 but GitHub doesn't report any conflict.\r\n\r\nDone. Could you check it again?", "Thank you"]}, {"number": 30518, "title": "[XLA] Add XLA test macros to iota tests", "body": "This simply allows these tests to be disabled using the test manifest system.", "comments": ["it seems impossible that changing the test macros in the XLA tests directory can make the Linux GPU fail.  Likewise the windows one.  i can't see the details of the failures though for some reason.", "thanks :)", "Changes have been merged in to master by commit 6e03f94 , so closing this PR."]}, {"number": 30517, "title": "Removed deprecated API from tensorflow/python/ops/distributions", "body": "", "comments": ["@alextp , can you re-approve the PR, \r\n\r\nRegards\r\nAmit", "@alextp , sorry for the trouble, fixed the python indentation problem, can you please re-approve the PR.\r\n\r\nRegards\r\nAmit", "@gbaned , can you pls help to get this PR merged.\r\n\r\nRegards\r\nAmit"]}]