[{"number": 36251, "title": "Symmetric quantization with activations 16-bit and weights 8-bit: interface", "body": "In this PR we add a new option TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8 to enable quantization with 16-bit activations and weights 8-bit.\r\n\r\nWhen it is set, then we do post-training symmetric quantization with 16-bit activations and 8-bit weights. The bias is 64-bit in this case. It behaves the same way as TFLITE_BUILTINS_INT8.\r\n\r\nExample of usage:\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\nThe name is quite long even it is explanatory. \r\nMay be, we should use something like  TFLITE_BUILTINS_INT16x8.\r\nAny suggestions are welcome.\r\n\r\nImplementations of reference kernels are submitted in other PRs.\r\n", "comments": ["Hi @renjie-liu , Could you please review this PR ? Thanks!", "Sorry for the late response, adding Suharsh to take a look. thanks!", "@suharshs Thanks for the review!\r\nI will correct according to the suggestions and re-test internally.", "Hi @suharshs, I added \"non-strict\" mode to this PR in the last commit. Now all changes to introduce 16x8 mode is here. Could you please take a look when you have a time ? Thanks", "This overall looks good now.\r\n\r\nBefore submitting this change, we need to ensure that the kernels are in, and then in one change we need to bump the version number of all the 16 bit ops, and ensure that this tool uses that new version when applying 16 bit operations. \r\n\r\nLet's first get the rest of the kernels submitted, then send a single PR that updates the version for all the kernels, and finally update this PR with those version numbers.", "Thanks for the review! The plan sounds good to me.", "@wwwind Can you please resolve conflicts? Thanks!", "@gbaned conflicts are resolved", "> This overall looks good now.\r\n> \r\n> Before submitting this change, we need to ensure that the kernels are in, and then in one change we need to bump the version number of all the 16 bit ops, and ensure that this tool uses that new version when applying 16 bit operations.\r\n> \r\n> Let's first get the rest of the kernels submitted, then send a single PR that updates the version for all the kernels, and finally update this PR with those version numbers.\r\n\r\n@suharshs dependent changes are submitted internally or there is a PR ?", "Adding Feng to speak to how to add support in the mlir code path too.", "Hi @liufengdb , I tested all my models with the flag experimental_new_converter = True (default) \r\nand all models converted without problems to 16x8 (activations int16, weights int8) mode, with description \"MLIR converted\", etc.\r\n\r\nI know that there is _experimental_new_quantizer flag, but this path is in the active development, so I keep an eye on it, but it looks that it is not needed for now.\r\n\r\nAll our essential changes are in quantize_model.cc.\r\nPlease let me know if something need to be done in addition.\r\n\r\n*There is merge conflict on this PR - changes to resolve it are under testing right now.\r\n\r\nThanks!", "Hi @wwwind, it is great to see the patch works well. Internally we have decided to migrate to the new quantizer, and, of course, we want to support this 16 bits activation feature in the new quantizer. My only concern is how to verify the results, because most of the tests in this patch seems tied to the old quantizer.", "Thanks @liufengdb for the support in the new quantizer.\r\n\r\n1. Yes, we have not added a lot of tests here, but I am ready to add unit tests with models that cover all our operators. Like in quantize_model_test.cc. Is it okay ? Perhaps, should I do this in the additional PR, because this one is too big already.\r\n\r\n2. Internally we have a set of end2end dummy models for all reference kernels that we have implemented:\r\nwe create one layer model and quantize it to 16x8 and do some checks\r\n+ proper accuracy testing on classic models with 16x8 mode.\r\nIn all these tests we use installed patched tf.\r\nIs there any place in tensorflow where I can add similar tests?\r\n\r\n3. What is the timeline for the migration to the new quantizer ? When does it make sense for us to start testing it ?\r\n\r\nThanks!\r\n ", "> add unit tests with models that cover all our operators. \r\n> Internally we have a set of end2end dummy models for all reference kernels that we have \r\n> implemented. we create one layer model and quantize it to 16x8 and do some checks\r\n\r\nA good place for these per-op tests can be extent from these tests:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/testing/op_tests\r\n\r\n> Is there any place in tensorflow where I can add similar tests?\r\n\r\nThere are some tests are using import tf directly: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/testing/generate_examples.py#L32\r\n\r\n> What is the timeline for the migration to the new quantizer? When does it make sense for us to \r\n> start testing it ?\r\n\r\nMy plan is to switch the flag internally by the end of this quarter and make it public externally in the middle of next quarter. We have some internal users with the new converter, and most of the features are ready, except this 16 bits. I can add the 16bits support next week, and later on, you add the tests to `tensorflow/lite/testing/op_tests`? ", "Thanks! This sounds good. I will prepare PR with these tests.\r\n", "Hi @liufengdb ! I created a set of tests for 16x8 quantization in ops_test:\r\nhttps://github.com/tensorflow/tensorflow/pull/39543\r\n\r\nMy concern is that these tests are for the old converter: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/testing/toco_convert.py#L123\r\n\r\nIf the function QuantizeModel stays the same for the new quantizer, then\r\nin this PR I have parametrized with int16 all tests [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/quantize_model_test.cc#L127).\r\nThis should be a good cover for 16x8 case.\r\n\r\nThanks!", "I will switch the zip tests to use the new converter. Then you can submit #39543.", "@wwwind can you please resolve conflicts and check sanity errors ", "@liufengdb I updated - please take a look\r\n\r\nThere is a failure in CI with \r\n//tensorflow/tools/api/tests:api_compatibility_test\r\n\r\n  ```\r\nFile \"/home/elezhe01/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py\", line 23, in <module>\r\n    from tensorflow.python.feature_column import dense_features\r\nImportError: cannot import name 'dense_features'\r\n```\r\n\r\nBut I checked that the same error is in master, so it's not specific to this PR.", "I tested this PR on our set of models - everything is working as should be when \r\nexperimental_new_converter = True\r\nInference is broken now, when experimental_new_converter = False.\r\nDo we need to support this case ?", "> I tested this PR on our set of models - everything is working as should be when\r\nexperimental_new_converter = True\r\nInference is broken now, when experimental_new_converter = False.\r\nDo we need to support this case ?\r\n\r\nNo, but we should:\r\n 1) Document this restriction in the API docs (and eventual guide)\r\n 2) Throw an exception if trying to use this flag when the new converter is disabled", "Hi @rthadur Thanks! I pushed a fix for these errors.\r\n\r\n@jdduke, the option is renamed as suggested. ", "Hi @rthadur \r\nThere is a failure of the test in Ubuntu CPU checks:\r\n//tensorflow/tools/api/tests:api_compatibility_test\r\nI checked master and this test fails their as well\r\nThanks", "@wwwind  Can you please address Ubuntu Sanity errors? Thanks!", "Hi @gbaned \r\nHonestly, I dont know how to fix them, because \r\nI run this test on master and it was failing with the same message.\r\nFAILED: //tensorflow/tools/api/tests:api_compatibility_test (Summary)\r\nSo, this failure is not relevant to my changes.", "Hi @rthadur, Could you please re-approve this PR ?\r\nI found a problem and fixed this CI failure finally.\r\nThanks! ", "Hi @jdduke I added description to API as requested.\r\nPlease take a look.\r\nThanks!", "Hi @rthadur Sorry to bother you, but could you please re-approve this PR?\r\nI had to push fixes to pylint errors.\r\nThanks!", "Hi @jdduke ! Thanks for the review! Documentation is merged into this PR and comment is corrected, + small fix for pylint.\r\nCould you please re-approve ? \r\nThanks!\r\n", "Adding the API review label for discussion and approval (hopefully by EOW).", "For @tensorflow/api-owners, this looks good.", "Hi @jdduke Could you please help me with this PR ?\r\nThere are CI failures on this PR but they don't look relevant to my PR.\r\nI tried these targets locally and they are green:\r\n//tensorflow/lite/tools/optimize/calibration:logging_op_resolver (Linux GPU)\r\n//tensorflow/tools/ci_build/builds:gen_win_out (Windows bazel)\r\nIs it possible to re-run CI somehow ?\r\nThanks!\r\n\r\n", "I run the failing target locally:\r\nbazel test //tensorflow/lite/tools/optimize/calibration:logging_op_resolver\r\nand it's green locally.", ">  run the failing target locally:\r\nbazel test //tensorflow/lite/tools/optimize/calibration:logging_op_resolver\r\nand it's green locally.\r\n\r\nRight, I think there was a broken build at head, so probably just needed a rebase.", "Hi @jdduke I pushed a fix. Could you please re-approve ?\r\nThanks!", "The build error in \"MacOS CPU Python3\" is reproducable in master branch", "@rthadur can you help push this through internal migration?", "Hi @rthadur I have corrected. Could you please re-approve ? Thanks!", "Hi @jdduke Could you please re-approve this PR? I pushed a small change as requested. \r\nThanks!", "Hi @rthadur there is again failure on this PR in \"MacOS CPU Python3\".\r\nIt is the same as before and it is reproducible in master branch for me locally.\r\nthanks", "@wwwind will you be able to fix the error ?", "Hi @rthadur No, it's not obvious to me why this test is failing and what can I do.\r\nThis is Mac specific error and it fails consistently on Ubuntu, but it is excluded on Ubuntu.", "Hi, I will try to patch this internally to resolve the tests. Thanks."]}, {"number": 36250, "title": "Tensorflow keras metrics cannot be used straight into the keras compile method", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu\r\n- TensorFlow installed from (source or binary): using pip\r\n- TensorFlow version (use command below):  2.1.0\r\n- Keras version: 2.3.1\r\n- Python version: 3.6.4\r\n\r\ntensorflow.version.GIT_VERSION, tensorflow.version.VERSION\r\n('v2.1.0-rc2-17-ge5bf8de', '2.1.0')\r\n\r\n**Describe the current behavior**\r\n\r\nI found an anomalous behavior when specifying tensorflow.keras.metrics directly into the Keras compile API:\r\n```\r\nfrom tensorflow.keras.metrics import Recall, Precision\r\nmodel.compile(..., metrics=[Recall(), Precision()]\r\n```\r\nWhen looking at the history track the precision and recall plots at each epoch (using keras.callbacks.History) I observe very similar performances to both the training set and the validation set.\r\nThe weirdest thing is that both Recall and Precision increase at each epoch while the loss is clearly not improving anymore.\r\n\r\nI found the issue to be related to the statefulness of the Tensorflow metrics objects.\r\nEverytime you call the metric object it will append a new batch of data that get mixed with both training and validation data and cumulates at each epoch.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behavior is that the metrics object should be stateless and do not depend on previous calls. Each time we calculate the metric (precision, recall or anything else), the function should only depend on the specified y_true and y_pred.\r\n\r\nTo workaround the issue we need to have either have Keras to be smart enough to re-instantiate the metric object at every call or to provide a tensorflow wrapper that is stateless. Maybe a decorator?\r\n\r\n**Code to reproduce the issue**\r\n\r\n```from tensorflow.keras.metrics import Recall, Precision, AUC, TopKCategoricalAccuracy, PrecisionAtRecall\r\nrecall=Recall()\r\n\r\ny_train =[[0, 1, 0, 1],\r\n         [1,0,0,0]]\r\n\r\ny_train_pred=[[0.1,0.50001,0.4,0.7],\r\n       [0.5,0.51,1,0]]\r\n\r\ny_test =[[1, 1, 0, 0],\r\n         [0,0,0,1]]\r\n\r\ny_test_pred=[[0.1,0.80,0.8,0.9],\r\n            [0.1,0.4,0.99,0]]\r\n\r\n\r\nprint(recall(y_train, y_train_pred))\r\nprint(recall(y_test, y_test_pred))\r\nrecall=Recall()\r\nprint(recall(y_test, y_test_pred))\r\n\r\nrecall=Recall()\r\nprint(recall(y_test, y_test_pred))\r\nprint(recall(y_train, y_train_pred))\r\n```\r\n\r\n**Other info / logs**\r\nThe code above will print:\r\n```\r\ntf.Tensor(0.6666667, shape=(), dtype=float32)\r\ntf.Tensor(0.5, shape=(), dtype=float32)\r\ntf.Tensor(0.33333334, shape=(), dtype=float32)\r\ntf.Tensor(0.33333334, shape=(), dtype=float32)\r\ntf.Tensor(0.5, shape=(), dtype=float32)\r\n```\r\nAs you can see the behavior is not stateless but is the concatenation of all of the apply calls since the object instantiation.\r\n", "comments": ["I have even tried wrapping the tensorflow metric instances in a sort of decorator:\r\n```\r\ndef tf_metric_stateless(name, metric_factory, *args, **kwargs):\r\n    \r\n    def metric_stateless(y_true, y_pred):\r\n        metric = metric_factory(*args, **kwargs)\r\n        return metric(y_true, y_pred)\r\n    \r\n    metric_stateless.__name__ = name\r\n    return metric_stateless\r\n\r\nmodel.compile(optimizer=optimizer, \r\n                           loss='binary_crossentropy', \r\n                           metrics=['accuracy',\r\n                                    'binary_accuracy',\r\n                                    tf_metric_stateless('tf_binary_accuracy', BinaryAccuracy),\r\n                                    tf_metric_stateless('tf_precision', Precision),\r\n                                    tf_metric_stateless('tf_recall', Recall),\r\n                                    tf_metric_stateless('tf_precision_at_recall', PrecisionAtRecall, 0.9),\r\n                                    tf_metric_stateless('tf_auc', AUC, multi_label=False)\r\n                                   ])\r\n```\r\n\r\nThe wrapped metrics instances work fine in eager mode in fact I can now get reproducible results when I calculate the recall in sequence on the toy data.\r\n\r\nNevertheless, when I collect the metrics calculated at each epoch via the History callback in Keras, the look like in the original case (without the wrapper). I believe it has something to do with the different execution modes. When the metric is compiled in the tensorflow graph, it becomes a singleton even if it is re-instantiated everytime from the python code. Am I wrong or missing something?", "I have tried to train the model by proving random validation labels (y_val) in order to force a visible gap between training and validation data.\r\nThe metrics calculated natively in keras makes sense (loss and accuracy):\r\n\r\n\r\n![loss-keras](https://user-images.githubusercontent.com/1311749/73252243-baaf6200-41ba-11ea-8d9c-ad2d8148e251.png)\r\n![accuracy-keras](https://user-images.githubusercontent.com/1311749/73252245-baaf6200-41ba-11ea-902d-729e903d2fb4.png)\r\n\r\nBut the tensorflow metrics looks fishy.\r\n\r\n![precision-tf](https://user-images.githubusercontent.com/1311749/73252240-ba16cb80-41ba-11ea-9abc-be8866436481.png)\r\n![recall-tf](https://user-images.githubusercontent.com/1311749/73252241-baaf6200-41ba-11ea-90ac-9b8c66655001.png)\r\n![accuracy-tf](https://user-images.githubusercontent.com/1311749/73252242-baaf6200-41ba-11ea-8ea4-2c47e3c1ffe0.png)", "Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/40d8b36766062b0c2a9101af93ae8464/36250.ipynb). Thanks!", "@gm-spacagna Thank you for the issue. \r\n\r\nFor some of the metrics such as MSE we have stateful and stateless versions:\r\nstateful listed as classes here: https://www.tensorflow.org/api_docs/python/tf/keras/metrics\r\nstateless listed as functions: https://www.tensorflow.org/api_docs/python/tf/keras/metrics#functions\r\n\r\nUsage with compile/fit API are always stateful. If you want to get batchwise values, you can write custom training loop using the `train_on_batch` API.\r\n\r\nFor metrics such as Precision/Recall there isn't really a stateless version. For standalone usage of these metrics, please use `reset_state` API for clearing the state between batches.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36250\">No</a>\n", "@pavithrasv your explanations are correct but there problem I think is elsewhere.\r\nI see two issues:\r\n\r\n1. It is hard to get aggregated metrics on the whole dataset instead of batchwise\r\n2. It is hard to isolate the metrics on training set and validation set\r\n\r\nYou can reset the state between batches but i guess it won't help on finding metric on the whole validation data separately from the training data.", "> It is hard to get aggregated metrics on the whole dataset instead of batchwise\r\n\r\nWith the stateful metrics you get the aggregated results across the entire dataset and not batchwise. \r\n\r\n> It is hard to isolate the metrics on training set and validation set\r\n\r\nCan you call evaluate separately for this use case?", "Same issue here. There are some case where it might be useful to have stateful metrics (if prior history of the metric is needed for the metric itself), but there should be a different state for validation and training. \r\nIs there any way to achieve this?"]}, {"number": 36249, "title": "Issue on download_and_extract URL DIR download_dependencies.sh", "body": "I follow [the build for arm64](https://www.tensorflow.org/lite/guide/build_arm64) and try  to execute `download_dependencies.sh` script but end up with issue:\r\n\r\n```\r\n~/work/tensorflow$ ./tensorflow/lite/tools/make/download_dependencies.sh\r\n./tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR\r\n```\r\n\r\nHow it can be fixed?\r\n", "comments": ["I think this was fixed in 0071950c073eb78e8935ad680f557e11f52b76a0 and 0438fe668efd9c3e3c3d4692f0c4af4f1dff6395, though it is not release in TensorFlow 2.1", "duplicate issue https://github.com/tensorflow/tensorflow/issues/36199 contains a fix.", "@peter197321, Were you able to resolve this issue. Thanks!", "@peter197321, Any update!", "It's fixed now.\r\nhttps://github.com/tensorflow/tensorflow/commit/f270180a6caa8693f2b2888ac7e6b8e69c4feaa8"]}, {"number": 36248, "title": "TF-Lite example label Image w shared library", "body": "Please update `label_image` example to be compilable using a shared library.\r\nIt means to use a shared library with `label_image` example.", "comments": ["Can you be a bit more explicit? Do you mean you'd like for the example to use one of the shared library targets (either the C or C++ one), rather than statically linking everything into the final binary?", "@peter197321 Could you please respond to the above comment, so we could help you resolve the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36247, "title": "XLA Warning and could not interpret set environment", "body": "**System information**\r\n- OS Platform and Distribution:18.04\r\n- TensorFlow version: 1.14.0\r\n- Python version: 2.7.17\r\n\r\n`Couldn't interpret value =/home/hyadav/deephyp/lib/python2.7/site-packages/tensorflow/compiler/xla:--tf_xla_cpu_global_jit=/home/hyadav/deephyp/lib/python2.7/site-packages/tensorflow/compiler/xla:=--tf_xla_cpu_global_jit=--tf_xla_cpu_global_jit for flag tf_xla_cpu_global_jit.`\r\n\r\nI tried using the following export TF_XLA_FLAGS=--tf_xla_cpu_global_jit=/mytensorflowpath/tensorflow/compiler/xla:$TF_XLA_FLAGS=--tf_xla_cpu_global_jit from issue #30308 and get the above. ", "comments": ["@himanshisyadav, Could you provide the standalone code to reproduce the reported issue. Thanks!", "I am using the autoencoder example codes from [this](https://github.com/lloydwindrim/hyperspectral-autoencoders) github repo. Specifically running autoencoder_test_MLP_basic.py. \r\n\r\nI actually unset tf_xla_cpu_jit variable and the above error was solved. However, I was trying so many things simultaneously that I do not remember what did the trick. ", "Hi @himanshisyadav,\r\n\r\nThe person in the previous issue was using this flag wrong. It should be only `TF_XLA_FLAGS=--tf_xla_cpu_global_jit` This is not a key-value pair. It is just a flag.\r\n\r\nRegarding the XLA warning you mention, if you were seeing the same warning mentioned in the previous issue, it is fixed in newer versions of Tensorflow and will not be backported. If you don't want to use XLA in your code, just ignore the warning.", "@himanshisyadav Please let us know if the above comment helps resolve the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36247\">No</a>\n"]}, {"number": 36246, "title": "Hessian-vector product raises exception about Fetch argument", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.15.0-rc3-22-g590d6ee 1.15.0\r\n- Python version: Python 3.7.6\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nTrying to compute the Hessian-vector product of a graph with a linear dependence on a tensor produces a `TypeError` exception:\r\n```raw\r\nTypeError: Fetch argument None has invalid type <class 'NoneType'>` exception.\r\n```\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport itertools\r\nimport os\r\n\r\nimport numpy.random as rnd\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.gradients_impl import _hessian_vector_product\r\n\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\n\r\n\r\ndef main():\r\n    n = 10\r\n\r\n    x = tf.Variable(tf.zeros(n, dtype=tf.float64), name=\"x\")\r\n    y = tf.Variable(tf.zeros(n, dtype=tf.float64), name=\"y\")\r\n    z = tf.Variable(tf.zeros(n, dtype=tf.float64), name=\"z\")\r\n    function = tf.reduce_sum(x ** 2 + y + z ** 3)\r\n\r\n    arguments = (x, y, z)\r\n    zeros = [tf.zeros_like(argument) for argument in arguments]\r\n    hessian = _hessian_vector_product(function, arguments, zeros)\r\n\r\n    x, y, z = [rnd.randn(n) for _ in range(3)]\r\n    u, v, w = [rnd.randn(n) for _ in range(3)]\r\n\r\n    feed_dict = {\r\n        key: value\r\n        for key, value in zip(itertools.chain(arguments, zeros),\r\n                              (x, y, z, u, v, w))\r\n    }\r\n    with tf.Session() as session:\r\n        hessian_vector_product = session.run(hessian, feed_dict)\r\n\r\n    print(hessian_vector_product)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```", "comments": ["I did some more digging and realized it was due to the default behavior of `tf.gradients`, which replaces unconnected gradients with None. The straightforward fix is to copy the implementation of `_hessian_vector_product` and pass `unconnected_gradients=tf.UnconnectedGradients.ZERO` to any `tf.gradients` calls.", "Was able to reproduce the issue with Tf 1.15 on colab.\r\nPlease find the gist [here](https://colab.research.google.com/gist/gadagashwini/e149f31541c4106708a8c850761a39c3/untitled362.ipynb). Thanks!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36246\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36246\">No</a>\n"]}, {"number": 36245, "title": "Malformatted doc page for SGD optimizer", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD#apply_gradients\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe formatting in the \"References\" section is wrong; there is a ``nesterov = True,`` that has nothing to do with the reference.", "comments": ["\"http://jmlr.org/proceedings/papers/v28/sutskever13.pdf\"\r\nis this the only thing that should be present?", "I don't know, I just realised that there is something wrong with the formatting.", "@renatobellotti I agree there is some formatting required in this page. But, can you please explain what do you meant by \"that has nothing to do with the reference\"?. I think the reference explain about the importance of `nesterov`. Thanks!", "@jvishnuvardhan I haven't read the reference, I just saw that this piece of code was no reference, and I couldn't find where it belongs to, so I assumed it should not be in the reference box.\n\nSorry for my ambiguous formulation.", "Closing this issue as it was resolved. Thanks!"]}, {"number": 36244, "title": "Default training algorithm: When are the gradients applied?", "body": "## Description of issue (what needs changing):\r\n\r\nIt is not clear how a Keras model is trained by default. Are the gradients applied after each minibatch, or are they averaged over all minibatches, and then the average gradients are applied?", "comments": ["@renatobellotti, can you elaborate the issue with code snippet or any use case. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36243, "title": " RBE: Use exec_properties to tag tests that require a GPU ", "body": "This change replaces all uses of the exec_compatible_with attribute for GPU tests with the exec_properties attribute. exec_properties are passed through as platform properties to the remote execution backend. This makes creating special platforms for GPU tests obsolete.\r\n\r\nThis change is rebased on https://github.com/tensorflow/tensorflow/pull/36239\r\n\r\n@r4nt @gunan ", "comments": ["@buchgr can you please check the sanity build failures here \r\nhttps://source.cloud.google.com/results/invocations/6e1be175-efd1-4139-8ca0-72fd15364340/log"]}, {"number": 36242, "title": "[MLIR][XLA] Add HLO FuncOp to LHLO FuncOp legalization.", "body": "The conversion includes a simple buffer assignment assuming that there are no branches.", "comments": []}, {"number": 36241, "title": "EarlyStopping TensorFlow 2.0 discrepancy", "body": "I am running a code using Python 3.7.5 with TensorFlow 2.0 for MNIST classification.\r\nI am using **EarlyStopping** from TensorFlow 2.0 and the callback I have for it is:\r\n\r\n    callbacks = [\r\n                 tf.keras.callbacks.EarlyStopping(\r\n                     monitor='val_loss', patience = 3,\r\n                     min_delta=0.001\r\n                 )\r\n    ]\r\n\r\n\r\nAccording to [EarlyStopping - TensorFlow 2.0][1] page, the definition of **min_delta** parameter is as follows:\r\n\r\n**min_delta: Minimum change in the monitored quantity to qualify as an\r\nimprovement, i.e. an absolute change of less than min_delta, will\r\ncount as no improvement.**\r\n\r\n\r\n\r\n\r\n> Train on 60000 samples, validate on 10000 samples\r\n> \r\n> Epoch 1/15 60000/60000 [==============================] - 10s\r\n> 173us/sample - loss: 0.2040 - accuracy: 0.9391 - val_loss: 0.1117 -\r\n> val_accuracy: 0.9648\r\n> \r\n> Epoch 2/15 60000/60000 [==============================] - 9s\r\n> 150us/sample - loss: 0.0845 - accuracy: 0.9736 - val_loss: 0.0801 -\r\n> val_accuracy: 0.9748\r\n> \r\n> Epoch 3/15 60000/60000 [==============================] - 9s\r\n> 151us/sample - loss: 0.0574 - accuracy: 0.9817 - val_loss: 0.0709 -\r\n> val_accuracy: 0.9795\r\n> \r\n> Epoch 4/15 60000/60000 [==============================] - 9s\r\n> 149us/sample - loss: 0.0434 - accuracy: 0.9858 - val_loss: 0.0787 -\r\n> val_accuracy: 0.9761\r\n> \r\n> Epoch 5/15 60000/60000 [==============================] - 9s\r\n> 151us/sample - loss: 0.0331 - accuracy: 0.9893 - val_loss: 0.0644 -\r\n> val_accuracy: 0.9808\r\n> \r\n> Epoch 6/15 60000/60000 [==============================] - 9s\r\n> 150us/sample - loss: 0.0275 - accuracy: 0.9910 - val_loss: 0.0873 -\r\n> val_accuracy: 0.9779\r\n> \r\n> Epoch 7/15 60000/60000 [==============================] - 9s\r\n> 151us/sample - loss: 0.0232 - accuracy: 0.9921 - val_loss: 0.0746 -\r\n> val_accuracy: 0.9805\r\n> \r\n> Epoch 8/15 60000/60000 [==============================] - 9s\r\n> 151us/sample - loss: 0.0188 - accuracy: 0.9936 - val_loss: 0.1088 -\r\n> val_accuracy: 0.9748\r\n\r\nNow if I look at the last three epochs viz., epochs 6, 7, and 8 and look at the validation loss ('val_loss'), their values are:\r\n\r\n0.0688, 0.0843 and 0.0847.\r\n\r\nAnd the differences between consecutive 3 terms are: 0.0155, 0.0004. But isn't the first difference greater than 'min_delta' as defined in the callback. \r\n\r\nThe code I came up with for EarlyStopping is as follows:\r\n\r\n    # numpy array to hold last 'patience = 3' values-\r\n    pv = [0.0688, 0.0843, 0.0847]\r\n    \r\n    # numpy array to compute differences between consecutive elements in 'pv'-\r\n    differences = np.diff(pv, n=1)\r\n    \r\n    differences\r\n    # array([0.0155, 0.0004])\r\n    \r\n    \r\n    # minimum change required for monitored metric's improvement-\r\n    min_delta = 0.001\r\n    \r\n    # Check whether the consecutive differences is greater than 'min_delta' parameter-\r\n    check = differences > min_delta\r\n    \r\n    check\r\n    # array([ True,  False])\r\n    \r\n    # Condition to see whether all 3 'val_loss' differences are less than 'min_delta'\r\n    # for training to stop since EarlyStopping is called-\r\n    if np.all(check == False):\r\n        print(\"Stop Training - EarlyStopping is called\")\r\n        # stop training \r\n\r\n\r\nBut according to the 'val_loss', the differences between the not ALL of the 3 last epochs are greater than 'min_delta' of 0.001. For example, the first difference is greater than 0.001 (0.0843 - 0.0688) while the second difference is less than 0.001 (0.0847 - 0.0843).\r\n\r\nAlso, according to **patience** parameter definition of \"EarlyStopping\":\r\n\r\n**patience: Number of epochs with no improvement after which training will be stopped.**\r\n\r\nSo, EarlyStopping should be called when there is no improvement for 'val_loss' for 3 consecutive epochs where the absolute change of less than 'min_delta' does not count as improvement.\r\n\r\nThen why is EarlyStopping called?\r\n\r\n\r\n\r\n\r\nAnother example is:\r\n\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/30\r\n60000/60000 [==============================] - 11s 179us/sample - loss: 0.2032 - accuracy: 0.9399 - val_loss: 0.1195 - val_accuracy: 0.9626\r\nEpoch 2/30\r\n60000/60000 [==============================] - 9s 155us/sample - loss: 0.0852 - accuracy: 0.9732 - val_loss: 0.0891 - val_accuracy: 0.9712\r\nEpoch 3/30\r\n60000/60000 [==============================] - 9s 154us/sample - loss: 0.0552 - accuracy: 0.9830 - val_loss: 0.0897 - val_accuracy: 0.9723\r\nEpoch 4/30\r\n60000/60000 [==============================] - 9s 153us/sample - loss: 0.0460 - accuracy: 0.9850 - val_loss: 0.0746 - val_accuracy: 0.9794\r\nEpoch 5/30\r\n60000/60000 [==============================] - 9s 152us/sample - loss: 0.0325 - accuracy: 0.9894 - val_loss: 0.0787 - val_accuracy: 0.9774\r\nEpoch 6/30\r\n60000/60000 [==============================] - 9s 154us/sample - loss: 0.0267 - accuracy: 0.9913 - val_loss: 0.0736 - val_accuracy: 0.9782\r\nEpoch 7/30\r\n60000/60000 [==============================] - 9s 153us/sample - loss: 0.0250 - accuracy: 0.9920 - val_loss: 0.0857 - val_accuracy: 0.9777\r\nEpoch 8/30\r\n60000/60000 [==============================] - 9s 153us/sample - loss: 0.0210 - accuracy: 0.9929 - val_loss: 0.0808 - val_accuracy: 0.9790\r\nEpoch 9/30\r\n60000/60000 [==============================] - 9s 153us/sample - loss: 0.0179 - accuracy: 0.9937 - val_loss: 0.1043 - val_accuracy: 0.9768\r\n\r\nHere, the last 3 'val_loss' are:\r\n0.08566837142373625, 0.08082119292882962 and 0.10430033170154183.\r\n\r\nAnd the consecutive differences between the 2 adjacent 'val_loss' are:\r\n0.00484718 and 0.02347914\r\n\r\nNow, both of these differences are greater than 'min_delta' of 0.001. But, 'EarlyStopping' again halts training.\r\n\r\nWhat am I not getting?\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\r\n\r\n\r\nThe complete code GitHub URL:\r\nhttps://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/tfmot_sparsity_experiment.ipynb", "comments": ["@arjun-majumdar,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "@amahendrakar , added GitHub repo URL in post\r\n\r\nhttps://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/tfmot_sparsity_experiment.ipynb", "It is because the delta is calculated from the best value, on your second example it would be epoch 6 with val_loss 0.0736.\r\nThe next 3 epochs don't get better values than that. 0.0856, 0.0808 and 0.1043 are all greater than 0.0736 so they showed no improvement. The purpose is to improve on your best epoch not the latest.", "@arjun-majumdar,\r\nCould you please verify if @bfs15's answer resolves your query?  Thanks!", "Thanks, it solves my problem"]}, {"number": 36240, "title": "TF 2.1 Keras: Cached datasets use 40% more memory than in TF 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: 10.1 / 7.6.5.32\r\n- GPU model and memory: NVIDIA V100\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running training with cached repeated datasets in Keras, TensorFlow 2.1 (and `tf-nightly==2.2.0.dev20200124`) use 40-50% more memory than TensorFlow 2.0.\r\n\r\nE.g. a large cached dataset like [ImageNet](https://www.tensorflow.org/datasets/catalog/imagenet2012) will require **~230 GB** of memory where as in TensorFlow 2.0 the same code only required **155 GB**. This is a 75 GB increase in memory usage which makes it hard to cache large datasets in memory.\r\n\r\n```python\r\nimport math\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow_datasets as tfds\r\n\r\n\r\nbatch_size = 1024\r\n\r\ndataset, info = tfds.load(\r\n    \"imagenet2012:5.0.*\",\r\n    decoders={\"image\": tfds.decode.SkipDecoding()},\r\n    split=\"train\",\r\n    with_info=True,\r\n)\r\n\r\nval_dataset = tfds.load(\r\n    \"imagenet2012:5.0.*\",\r\n    decoders={\"image\": tfds.decode.SkipDecoding()},\r\n    split=\"validation\",\r\n)\r\n\r\nsteps_per_epoch = math.ceil(info.splits[\"train\"].num_examples / batch_size)\r\nval_steps = math.ceil(info.splits[\"validation\"].num_examples / batch_size)\r\n\r\n\r\ndef _decode_and_center_crop(image_bytes):\r\n    \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\r\n    shape = tf.image.extract_jpeg_shape(image_bytes)\r\n    image_height = shape[0]\r\n    image_width = shape[1]\r\n    image_size = 224\r\n\r\n    padded_center_crop_size = tf.cast(\r\n        (\r\n            (image_size / (image_size + 32))\r\n            * tf.cast(tf.minimum(image_height, image_width), tf.float32)\r\n        ),\r\n        tf.int32,\r\n    )\r\n\r\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\r\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\r\n    crop_window = tf.stack(\r\n        [offset_height, offset_width, padded_center_crop_size, padded_center_crop_size]\r\n    )\r\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\r\n    image = tf.compat.v1.image.resize_bicubic([image], [image_size, image_size])[0]\r\n    return image\r\n\r\n\r\ndef preprocessing(data):\r\n    return tf.cast(_decode_and_center_crop(data[\"image\"]), tf.float32), data[\"label\"]\r\n\r\n\r\ndataset = (\r\n    dataset.cache()\r\n    .shuffle(10 * batch_size)\r\n    .repeat()\r\n    .map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    .batch(batch_size)\r\n    .prefetch(1)\r\n)\r\n\r\nval_dataset = (\r\n    val_dataset.cache()\r\n    .repeat()\r\n    .map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    .batch(batch_size)\r\n    .prefetch(1)\r\n)\r\n\r\nmodel = keras.models.Sequential(\r\n    [\r\n        keras.layers.GlobalMaxPool2D(input_shape=(224, 224, 3)),\r\n        keras.layers.Dense(1000, activation=\"softmax\",),\r\n    ]\r\n)\r\n\r\nmodel.compile(\r\n    optimizer=\"adam\",\r\n    loss=\"sparse_categorical_crossentropy\",\r\n    metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"],\r\n)\r\n\r\nmodel.fit(\r\n    dataset,\r\n    epochs=5,\r\n    steps_per_epoch=steps_per_epoch,\r\n    validation_data=val_dataset,\r\n    validation_steps=val_steps,\r\n)\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo increased memory requirements when switching from TensorFlow 2.0 to 2.1\r\n\r\n**Other info / logs**\r\n\r\nThis issue doesn't appear when not using a validation dataset:\r\n```python\r\nmodel.fit(dataset, epochs=5, steps_per_epoch=steps_per_epoch)\r\n```\r\n\r\nThe issues also doesn't appear if the dataset is not repeated. However then we would run into #36126 which logs error messages after each epoch:\r\n```python\r\ndataset = (\r\n    dataset.cache()\r\n    .shuffle(10 * batch_size)\r\n    .map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    .batch(batch_size)\r\n    .prefetch(1)\r\n)\r\n\r\nval_dataset = (\r\n    val_dataset.cache()\r\n    .map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    .batch(batch_size)\r\n    .prefetch(1)\r\n)\r\n\r\nmodel = keras.models.Sequential(\r\n    [\r\n        keras.layers.GlobalMaxPool2D(input_shape=(224, 224, 3)),\r\n        keras.layers.Dense(1000, activation=\"softmax\",),\r\n    ]\r\n)\r\n\r\nmodel.compile(\r\n    optimizer=\"adam\",\r\n    loss=\"sparse_categorical_crossentropy\",\r\n    metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"],\r\n)\r\n\r\nmodel.fit(dataset, epochs=5, validation_data=val_dataset)\r\n```\r\n\r\nJudging from the observations above it looks like the iteration of the validation dataset is broken inside Keras, so it doesn't show up when the dataset iterator is recreated when used without `steps_per_epoch`.\r\n\r\nWhat is the recommended way of using `tf.data.Datasets` with Keras models?", "comments": ["I am using TF 2.0/2.1/2.2 nightly for regular benchmarking ImageNet models and I did not find this issue loading data from disks. Btw I have not used tensorflow datasets; maybe it is better for you to raise the question there.", "@byronyi This issue only appears when caching the dataset using `.cache()`, have you tried it with caching? Also I am reading the data from `GCS` which might change the story a bit.", "@lgeiger can you reproduce the issue when you store a copy of ImageNet locally?", "> @lgeiger can you reproduce the issue when you store a copy of ImageNet locally?\r\n\r\nI currently don't have a good way to test this. Though the experiment can be reproduced with a smaller dataset like [`oxford_flowers102`](https://www.tensorflow.org/datasets/catalog/oxford_flowers102), though there the memory difference is harder to see.", "@jsimsa I tried to reproduce the issue with MNIST by caching on disk and didn't see any difference between TensorFlow versions. It looks like this issue only appears when caching in memory.", "Thank you @lgeiger. What I was suggesting is that you copy the data to your local file system and then run your program caching data in memory. I am trying to rule out that this is not due to reading from `GCS` (and it will also be easier to reproduce if it can be run locally).", "> Thank you @lgeiger. What I was suggesting is that you copy the data to your local file system and then run your program caching data in memory. I am trying to rule out that this is not due to reading from GCS (and it will also be easier to reproduce if it can be run locally).\r\n\r\n@jsimsa I just verified that I can reproduce this also when reading the data locally.\r\n\r\nSince this only happens when when running validation, I suspect that this might be a problem on the keras side.", "This can also be reproduced on `tf-nightly-2.2.0.dev20200205`. It looks like the memory only exceeds the normal requirements once the second epoch starts.", "Thank you for the follow up, I am going to ask the Keras team to investigate.\r\n\r\n@karmel this looks like Keras regression between TF 2.0 and TF 2.1.", "@yhliang2018 @karmel @jsimsa Any updates on this?", "@lgeiger I took a closer look and also suspect that the same issue occur in our environment. I am still investigating...will keep you posted.", "Just for reference, #36126 doesn't show up in `tf-nightly==2.2.0.dev20200218` anymore so one can workaround this issue by not repeating the dataset.", "@byronyi @yhliang2018 Any updates on this?", "It seems to be fixed by #37877, which is included in 2.2.0-rc3. Would you mind to try again?", "I was testing this earlier today with rc3 and without repeating the dataset as mentioned in https://github.com/tensorflow/tensorflow/issues/36240#issuecomment-588180783 and it ran out of memory for larger batch sizes. This is already better that 2.1 which crashed irrespective of the batch size, but there is still something going wrong compared to TensorFlow 2.0.0. I'll properly test it later today and will report back to see if I can find a workaround. For now it seams that 2.2 still has a lot higher memory usages than 2.0.", "One of my colleagues reported similar issues that the memory consumption increases after each epoch, and it might be due to creating a new validation dataset for each evaluation. See here for some details: https://github.com/tensorflow/tensorflow/issues/33340#issuecomment-569458253\r\n\r\nI never tested caching validation data before. Would you mind not to cache the validation dataset and see if you could reproduce the issue?", "@byronyi Thanks for the fast response. I am debugging as we speek, and indeed I can observe that the memory usage increases every epoch. It is not as severe as it was in 2.1, but still very noticable. I am testing this currently without repeating the dataset. Even when caching both training and validation datasets the memory usage increases during validation and drops sharply once the next epoch starts. This suggests that indeed the validation set is regenerated each time but unfortunately the memory usage doesn't fully drop to the original value and therfore builds up each epoch. This means even if the validation dataset is cached the cache is disposed every epoch which makes caching the validation dataset pointless. When not caching the validation set it seems that memory usage seems stable, but I need to run it for longer to be sure.\r\n\r\nOne more concerning observation I made (which would block us upgrading from 2.0 to 2.2) is that XLA in our case requires 1.5 - 2x the GPU memory as it did in 2.0. This means I need to halve the batch size in order to be able to run on 4-GPUs with XLA, which makes XLA essentially unusable. The training code works without a problem when XLA is disabled though. Is there already an issue where this is tracked?", "> It seems to be fixed by #37877, which is included in 2.2.0-rc3. Would you mind to try again?\r\n\r\nIt seems the original issue has been fixed, but unfortunately I am now running out of GPU memory when using larger batch sizes which for some reason doesn't only happens when using `steps_per_epoch`.", "Thanks for reporting this. @tomerk is looking into the memory issue, and may help on this.", "> One of my colleagues reported similar issues that the memory consumption increases after each epoch, and it might be due to creating a new validation dataset for each evaluation. See here for some details: #33340 (comment)\r\n\r\n@byronyi I can reproduce this issue when caching is enabled for the validation dataset. It looks like this is an issue with the [thread pool dataset](https://github.com/tensorflow/tensorflow/blob/8e0eecc8e396f8c1859b1b3954a89a41da8b5b45/tensorflow/python/data/ops/dataset_ops.py#L360-L362).\r\nTensorFlow Datasets sets [`experimental_threading.private_threadpool_size = 16` by default](https://github.com/tensorflow/datasets/blob/8277548d5bdbc264a50d84ef702adc15bee8d4ae/tensorflow_datasets/core/tfrecords_reader.py#L62) which leads to an increase in memory consumption after each epoch for me. Disabling this option keeps the memory usage stable.\r\n\r\nI can write down a more structured summary of the various issues tomorrow.", "Hi @lgeiger, do you mind to open a new issue with the thread pool dataset? So tf.data team can take a closer look at it. If you can provide a minimal repro example and some detailed summaries you already have, that will be very helpful to us! \r\n\r\nAt the same time, can we close this issue as it's already fixed? Thanks!", "Thanks for the help!\r\n\r\nI still have some issues with the datasets, but it is likely that they are not related to the original description of the issue anymore. I will run a few more tests and will try to provide a structured description in a new issue tomorrow. I hope we can get this fixed soon.", "For reference I opened a new issue at #38617 explaining the behaviour."]}, {"number": 36239, "title": "toolchains: migrate platforms to use exec_properties", "body": "- remote_execution_properties has been deprecated in favor\r\nof exec_properties [1].\r\n\r\n- have gpu platforms inherit from the cpu platform in order\r\nto avoid duplication.\r\n\r\n[1] https://docs.bazel.build/versions/master/be/platform.html#platform.exec_properties\r\n\r\n@r4nt @gunan ", "comments": []}, {"number": 36238, "title": "AutoGraph issue", "body": "**System information**\r\n- OS Platform and Distribution : CentOS Linux release 7.7.1908 (Core) \r\n- TensorFlow version: 1.15.0-dev20190718\r\n- Python version: 3.4\r\n \r\nI am trying to save a custom built LSTM model.\r\nThe issue arises when saving the lstm model as:\r\nfinal_model.save(\"complete_model\",save_format='tf'), whereas it is doesn't throw an error upon saving it as final_model.save(\"complete_model.h5\"). \r\n\r\nI tried to downgrade gast to version 0.2.2 as suggested by ticket**(TF2.0 AutoGraph issue\r\n#32377)** but still doesn't work.\r\nThe error message is as follows:\r\n\r\nW0127 16:22:09.873542 140113418938176 ag_logging.py:146] Entity <function Function._initialize_uninitialized_variables..initialize_variables at 0x7f6de43a8048> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\r\nW0127 16:22:09.882860 140113418938176 ag_logging.py:146] Entity <function _wrap_call_and_conditional_losses..call_and_return_conditional_losses at 0x7f6de41e8048> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\r\nW0127 16:22:09.911009 140113418938176 save.py:129] Skipping full serialization of object <tensorflow.python.keras.layers.recurrent.LSTM object at 0x7f6e00539f60>, because an error occurred while tracing layer functions. Error message: in converted code:\r\nrelative to /usr/local/lib/python3.4/site-packages/tensorflow_core/python/keras:\r\n\r\nsaving/saved_model/save.py:535 call_and_return_conditional_losses\r\n    return layer_call(inputs, training=training), layer.get_losses_for(inputs)\r\nlayers/recurrent.py:2535 call\r\n    inputs, mask=mask, training=training, initial_state=initial_state)\r\nlayers/recurrent.py:745 call\r\n    zero_output_for_mask=self.zero_output_for_mask)\r\nbackend.py:3794 rnn\r\n    input_time_zero, tuple(initial_states) + tuple(constants))\r\nlayers/recurrent.py:730 step\r\n    output, new_states = self.cell.call(inputs, states, **kwargs)\r\n\r\nTypeError: wrapped_call() takes 1 positional argument but 2 were given\r\nW0127 16:22:09.916115 140113418938176 ag_logging.py:146] Entity <function _wrap_call_and_conditional_losses..call_and_return_conditional_losses at 0x7f6de41e8488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\r\nW0127 16:22:09.943473 140113418938176 save.py:129] Skipping full serialization of object <tensorflow.python.keras.engine.training.Model object at 0x7f6dff6e65c0>, because an error occurred while tracing layer functions. Error message: in converted code:\r\nrelative to /usr/local/lib/python3.4/site-packages/tensorflow_core/python/keras:\r\n\r\nsaving/saved_model/save.py:535 call_and_return_conditional_losses\r\n    return layer_call(inputs, training=training), layer.get_losses_for(inputs)\r\nengine/network.py:737 call\r\n    return self._run_internal_graph(inputs, training=training, mask=mask)\r\nengine/network.py:879 _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\nlayers/recurrent.py:621 __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\nengine/base_layer.py:713 __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\nlayers/recurrent.py:2535 call\r\n    inputs, mask=mask, training=training, initial_state=initial_state)\r\nlayers/recurrent.py:745 call\r\n    zero_output_for_mask=self.zero_output_for_mask)\r\nbackend.py:3794 rnn\r\n    input_time_zero, tuple(initial_states) + tuple(constants))\r\nlayers/recurrent.py:730 step\r\n    output, new_states = self.cell.call(inputs, states, **kwargs)\r\n\r\nTypeError: wrapped_call() takes 1 positional argument but 2 were given\r\n2020-01-27 16:22:09.957051: W tensorflow/python/util/util.cc:288] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nW0127 16:22:09.982703 140113418938176 ag_logging.py:146] Entity <function Layer._handle_weight_regularization.._loss_for_variable at 0x7f6e017b6d90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\r\nW0127 16:22:09.992292 140113418938176 ag_logging.py:146] Entity <function Function._initialize_uninitialized_variables..initialize_variables at 0x7f6de43a8400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output. Cause: No module named 'tensorflow_core.estimator\r\n\r\n\r\n", "comments": ["@ashwin17083, Can you provide the standalone code replicate the reported issue. Thanks!", "Hi,\r\nThanks for your response.\r\nPlease find the attached zip folder.\r\nCompilation command: python3 PAKLSTMRepValCommented.py\r\n[lstm.zip](https://github.com/tensorflow/tensorflow/files/4127786/lstm.zip)\r\n", "@ashwin17083, Will it be possible to provide the simple standalone code to analyze the issue. Thanks!", "@gadagashwini: Hi,\r\nI have attached a modified code which includes a simple model description,training and saving.\r\nThe issue occurs in line number 26 when trying to save the model in TensorFlow SavedModel format.\r\nThe same model could be saved in .h5 format as in line number 25.\r\n[PAKLSTMRepValCommented.txt](https://github.com/tensorflow/tensorflow/files/4153207/PAKLSTMRepValCommented.txt)\r\nThanks!\r\n\r\n", "@ashwin17083, I tried on colab and on Linux system it is working as expected no error. Please find the gist [here](https://colab.research.google.com/gist/gadagashwini/3cb8a2d5006ae9da0ff9775c8bc481af/untitled377.ipynb). Can you try with gast==0.2.2 again and check. Create virtual env and install Tensorflow with gast version 0.2.2.  Thanks!", "Hi @gadagashwini, I tried it on conda 4.8.0 and it worked. \ud83d\udc4d \r\nReally appreciate your help \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36238\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36238\">No</a>\n"]}, {"number": 36237, "title": "Layer Names tf.keras.applications vs keras.applications not matching", "body": "Not sure if this is a documentation issue or a functional bug. \r\n\r\n## Description of issue:\r\nThe layer names in 'tf.keras.applications' are not consistent with the layer names in 'keras.applications'.\r\n\r\n## Example:\r\n`keras.applications.resnet50.ResNet50(weights='imagenet').summary()` prints the following layers:\r\n\r\n> (...)\r\n> __________________________________________________________________________________________________\r\n> **conv1_pad** (ZeroPadding2D)       (None, 230, 230, 3)  0           input_3[0][0]                    \r\n> __________________________________________________________________________________________________\r\n> **conv1** (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \r\n> __________________________________________________________________________________________________\r\n> **bn_conv1** (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \r\n> __________________________________________________________________________________________________\r\n> **activation_50** (Activation)      (None, 112, 112, 64) 0           bn_conv1[0][0]                   \r\n> __________________________________________________________________________________________________\r\n> **pool1_pad** (ZeroPadding2D)       (None, 114, 114, 64) 0           activation_50[0][0]              \r\n> __________________________________________________________________________________________________\r\n> (...)\r\n\r\n                           \r\n`tf.keras.applications.resnet50.ResNet50(weights='imagenet').summary()` prints the following layers:\r\n\r\n> (...)\r\n> __________________________________________________________________________________________________\r\n> **conv1_pad** (ZeroPadding2D)       (None, 230, 230, 3)  0           input_6[0][0]                    \r\n> __________________________________________________________________________________________________\r\n> **conv1_conv** (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \r\n> __________________________________________________________________________________________________\r\n> **conv1_bn** (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \r\n> __________________________________________________________________________________________________\r\n> **conv1_relu** (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \r\n> __________________________________________________________________________________________________\r\n> **pool1_pad** (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \r\n> __________________________________________________________________________________________________\r\n> (...)\r\n\r\nThis can lead to errors if code relying on layer names is migrated from `keras` to `tf.keras`. Even after looking for it for quite a bit, I have not found any documentation explaining the change of layer names. Also, I manually needed to map `keras `layer names to `tf.keras` layer names, which would be avoidable with some nice documentation.\r\n\r\n## URL(s) with the issue:\r\nProbably this should be mentioned in the migration docs or the applications docs.\r\nhttps://www.tensorflow.org/guide/migrate#a_note_on_slim_contriblayers\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications\r\n\r\n\r\n## Usage example\r\n\r\nThe following code snippet runs when applications is imported from `keras `but not with `tf.keras`, as the layer is not found.\r\n\r\n```\r\nloaded_model = applications.ResNet50(weights='imagenet')\r\n\r\npartial_model = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer('res5c_branch2c').output)\r\n```\r\n\r\nSome easily findable documentation should be explaining why the layer is not found (renamed layer names) and where to find the mapping from `keras` layer names to `tf.keras` layer names (i.e., the mapping from `res5c_branch2c ` to `conv5_block3_3_conv`, which is the layer name used in tf.keras).\r\n\r\n## Used versions in these examples\r\ntensorflow 2.1.0\r\nKeras 2.3.1\r\n\r\n## Final Remark\r\nI'm not sure if renaming the layers was that useful... While the tf.keras layer names might be better readable, papers and tutorials often refer to the keras label names - and users now seem to have to find a mapping themself. \r\n", "comments": ["Similar issues: #33459, #35336", "Please check https://github.com/keras-team/keras-applications/pull/173", "See https://github.com/tensorflow/tensorflow/issues/33459#issuecomment-592758846. \r\n\r\nIn short, keras-team/keras-application was exporting an old version of implementation, and TF is exporting a new version. Both of them should behave correctly, but the implementation detail is little bit different. You should rely on tf.keras.applications for the latest implementation.", "This may explain the related issue of broken weights. However, it seems no to resolve this issue (which is a documentation issue).\r\n\r\nWe still have the situation that the Resnet from `tf.keras` has to be handled differently than previous keras versions. There does not seem to be any documentation explaining how to migrate, which I think many will want to do when switching from tf1 to tf2.\r\n\r\nI do see that is is more a keras than a tf.keras issue. Still, doc would not hurt...\r\n\r\nDo you agree?\r\n", "@qlzh727 Would you consider reopening this **documentation** issue?", "Sure. Sorry for the late reply.", "I don't think this is an issue anymore. Please [see this gist here](https://colab.research.google.com/gist/nikitamaia/ef5a0faad77c562d22a717f12622d67f/issue_36237.ipynb), which prints `keras.applications.resnet50.ResNet50(weights='imagenet').summary()`. \r\nSeem to be the same as `tf.keras` now. Note that the usage example provided will now also fail for `keras` as well as `tf.keras` since the layer names are now consistent.\r\n\r\nClosing this issue now since it not longer seems relevant (though please add a note if I missed something)."]}, {"number": 36236, "title": "GradientTape.gradient fails when tf.gather is used after LSTM/GRU in tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Stretch**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.1.0** and **2.0.0**\r\n- Python version: **3.6**\r\n\r\n**Describe the current behavior**\r\nConsider the following simple model running a GRU, tf.gather, and a linear regression:\r\n```python\r\nimport tensorflow as tf\r\n\r\ninputs = tf.keras.layers.Input(shape=[None, 1], dtype=tf.float32)\r\nhidden = tf.keras.layers.GRU(10)(inputs)\r\nhidden = tf.gather(hidden, [0])\r\noutput = tf.keras.layers.Dense(1)(hidden)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=output)\r\n\r\n@tf.function\r\ndef train(x, y):\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(x, training=True)\r\n        loss = tf.losses.mean_squared_error(y, predictions)\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n\r\ntrain(tf.constant([[[1], [2], [3]]], dtype=tf.float32), tf.constant([[1]], dtype=tf.float32))\r\n```\r\nBoth TF 2.0.0 and TF 2.1.0 fails, with a similar error message (the following is TF 2.1):\r\n```\r\n    ValueError: All inputs to `ConcreteFunction`s must be Tensors; on invocation of __backward_standard_gru_918, the 0-th input (IndexedSlices(indices=Tensor(\"Reshape_2:0\", shape=(1,), dtype=int32), values=Tensor(\"Reshape_1:0\", shape=(1, 10), dtype=float32), dense_shape=Tensor(\"Cast_2:0\", shape=(2,), dtype=int32))) was not a Tensor.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code should work.\r\n\r\n**Other info / logs**\r\n\r\nThe problem is caused by `tf.gather` generating `tf.IndexedSlices` as a derivative. However, current LSTM/GRU are graph functions and the backprop algorithm for graph functions assumes the inputs must be `tf.Tensor`s.\r\n\r\nOne possible workaround is not to use `tf.function` and use eager. Then the code works.\r\n\r\nAnother solution is to force conversion of the derivatives from `tf.IndexedSlices` to `tf.Tensor`. The easiest solution I found is to use `* 1`, which adds operation `Mul` to the computation graph, for which the conversion from `tf.IndexedSlices` to `Tensor` works. So the following works:\r\n```python\r\ninputs = tf.keras.layers.Input(shape=[None, 1], dtype=tf.float32)\r\nhidden = tf.keras.layers.GRU(10)(inputs)\r\nhidden = tf.gather(hidden * 1, [0])\r\noutput = tf.keras.layers.Dense(1)(hidden)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=output)\r\n\r\n@tf.function\r\ndef train(x, y):\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(x, training=True)\r\n        loss = tf.losses.mean_squared_error(y, predictions)\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n\r\ntrain(tf.constant([[[1], [2], [3]]], dtype=tf.float32), tf.constant([[1]], dtype=tf.float32))\r\n```\r\n\r\n**Possible solution**\r\nI assume an automatic conversion of `tf.IndexedSlices` to `tf.Tensor` should be performed. As a proof of concept, I changed the line https://github.com/tensorflow/tensorflow/blob/1f404dc482e3b7e4244cc1603725e555f06f8b9b/tensorflow/python/eager/function.py#L1731\r\nto\r\n```python\r\n  tensor_inputs.append(ops.convert_to_tensor(arg))\r\n```\r\nand the code then works as expected (converting the `tf.IndexedSlices` to `tf.Tensor`). However, a proper fix is definitely more complicated.", "comments": ["Issue replicating for [tf-nightly](https://colab.sandbox.google.com/gist/oanush/8de62f184ff385f8c95404fa6b8d36f4/36236.ipynb), thanks!", "I am having the same problem.  Removing all tf.function decorators causes the problem to go away.", "This is severe for performance, experiencing troubles as well!", "Well, the `* 1` on strategic places (as described above) makes the `tf.functions` work again and there is no slowdown (a proper fix will do a very similar thing from the performance point of view).\r\n\r\nSo what you can try is after every call to LSTM/GRU (or after an enclosing Bidirectional) write `* 1`, until the bug is fixed.", "> Well, the `* 1` on strategic places (as described above) makes the `tf.functions` work again and there is no slowdown (a proper fix will do a very similar thing from the performance point of view).\r\n> \r\n> So what you can try is after every call to LSTM/GRU (or after an enclosing Bidirectional) write `* 1`, until the bug is fixed.\r\n\r\nWell I am using a significantly more complex model. It feels painful to do this, I am looking forward to the fix.", "I keep running into this issue myself; it's a horrific regression relative to TF1.x, where no such problems existed.\r\n\r\nIt would be really great if the TF team would fix this in the next release, for example by auto-converting to a dense tensor on entry into the backwards function. Yes, there might be more performant options, but the current behaviour is simply breaking code.", "Any updates on this?\r\nChanging this [line](https://github.com/tensorflow/tensorflow/blob/eab9701d541f5b8d54a898f81c3344c7b7d672b4/tensorflow/python/eager/function.py#L1971) to\r\n```\r\n  tensor_inputs.append(ops.convert_to_tensor(arg))\r\n```\r\nsolves the issue on 2.2.0-rc3 but just wondering if there is a proper change from TF team.", "A gentle bump after another month -- still not working on current TF-nightly 2.3.0-dev20200602.", "I've raised a similar issue  #31952 before, which got fixed. However that fix only works when the grads are calculated outside a graph ( i.e. in eager mode). It might shed some light on how to fix it in graph mode though. cc the author of that fix: @alextp \r\n", "Can you send a PR changing the line and adding a test?", "@alextp I can easily do it, but I am not sure it is a correct solution. Looking at the current master:\r\nhttps://github.com/tensorflow/tensorflow/blob/9221044560b9ed34d60a4b07dc0895552d2540c5/tensorflow/python/eager/function.py#L1873-L1906\r\nI assume it would make sense to test specifically for IndexedSlices only, and perform the same shape check as in the case of tf.Tensor? Hm, I think the best solution would be to do something like\r\n```python\r\n      elif isinstance(arg, (ops.Tensor, ops.IndexedSlices)): \r\n        if isinstance(arg, ops.IndexedSlices):\r\n          tensor_inputs.append(ops.convert_to_tensor(arg))\r\n        else\r\n          tensor_inputs.append(arg)\r\n```\r\nand then continue with the shape-checking code.\r\n\r\nLooking even deeper, maybe we should do the rewrite for the backward pass only? Here\r\nhttps://github.com/tensorflow/tensorflow/blob/9221044560b9ed34d60a4b07dc0895552d2540c5/tensorflow/python/eager/function.py#L737-L747\r\nthe gradients in the backward pass are collected -- it would be enough to convert the IndexedSlices to Tensors here. So that the least number of silent conversions would happen (and the backward pass is the only problematic one).\r\n\r\nPersonally I would do the rewrite only for the backward pass -- what do you think @alextp? If you think it is fine, I will prepare the patch (including a test).", "I have simplified the problematic input not to reference LSTM/GRU:\r\n```python\r\n@tf.function\r\ndef summing_rnn(inputs):\r\n  return tf.reduce_sum(inputs, axis=1)\r\n\r\n@tf.function\r\ndef gradients(inputs):\r\n  with tf.GradientTape() as tape:\r\n    tape.watch(inputs)\r\n    hidden = summing_rnn(inputs)\r\n    hidden = tf.gather(hidden, tf.constant([0]))\r\n    loss = tf.reduce_mean(hidden)\r\n  return tape.gradient(loss, inputs)\r\n\r\ngradients(tf.constant([[[1.0], [2.0]]])) # No error is raised\r\n```\r\n\r\nI am also creating a pull request.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36236\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36236\">No</a>\n"]}, {"number": 36235, "title": "Dll load failed", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@angelineaish ,\r\nHi, please provide the information asked in the template.\r\nAlso please refer the [comment1](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577874481) and [comment2](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from @mihaimaruseac for similar issue and let us know if it helps.", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36235\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36235\">No</a>\n", "Hello so should I have a GPU model to solve this problem?\r\n", "No. Solution is at \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\nIf neither of those apply, please open a new issue,fill in template and mention that these steps don't work and why"]}, {"number": 36234, "title": "SpeechRecognition android example", "body": "**System information**\r\n- OS Platform - macOS mojave:\r\n- TensorFlow version - master\r\n\r\n\r\n**Failure details**\r\nim trying to build example from tensorflow/tensorflow/examples/speech_commands/\r\nso i save model by freeze.py and get savedModel then i build android app from tensorflow/tensorflow/examples/android/\r\nso my quesion is how to use savedModel in mobile in this example?\r\n\r\nanother question is how to convert savedModel to Tflite?\r\n\r\n**Command used to run the converter**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nthis code produce some errors\r\n", "comments": ["@B0yma \r\nCan you please go through this [link](https://www.tensorflow.org/lite/guide/ops_select), [link2 ](https://www.tensorflow.org/lite/guide/android) and see if it helps you. Thanks!", "> @B0yma\r\n> Can you please go through this [link](https://www.tensorflow.org/lite/guide/ops_select), [link2 ](https://www.tensorflow.org/lite/guide/android) and see if it helps you. Thanks!\r\n\r\n version of Tensorflow :\r\nName: tensorflow\r\nVersion: 1.15.0\r\n\r\nfrom the 1st link:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nafter above code I get error :\r\n```\r\npython tensorflow/examples/speech_commands/freezemobile.py\r\n2020-01-29 21:15:58.919574: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9db96ebfe0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-29 21:15:58.919603: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/speech_commands/freezemobile.py\", line 3, in <module>\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(\"/tmp\")\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/lite/python/lite.py\", line 762, in from_saved_model\r\n    output_arrays, tag_set, signature_key)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/lite/python/convert_saved_model.py\", line 187, in freeze_saved_model\r\n    meta_graph = get_meta_graph_def(saved_model_dir, tag_set)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/lite/python/convert_saved_model.py\", line 60, in get_meta_graph_def\r\n    return loader.load(sess, tag_set, saved_model_dir)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 269, in load\r\n    return loader.load(sess, tags, import_scope, **saver_kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 422, in load\r\n    **saver_kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 349, in load_graph\r\n    meta_graph_def = self.get_meta_graph_def_from_tags(tags)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/python/saved_model/loader_impl.py\", line 327, in get_meta_graph_def_from_tags\r\n    \"\\navailable_tags: \" + str(available_tags))\r\nRuntimeError: MetaGraphDef associated with tags set(['serve']) could not be found in SavedModel. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\r\navailable_tags: [set([])]\r\n```\r\n\r\nand after this code:\r\n```\r\ntflite_convert \\\r\n  --output_file=/tmp/foo.tflite \\\r\n  --graph_def_file=/tmp/foo.pb \\\r\n  --input_arrays=input \\\r\n  --output_arrays=MobilenetV1/Predictions/Reshape_1 \\\r\n  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS\r\n```\r\n\r\nget error:\r\n```\r\n2020-01-29 21:20:13.917241: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb909751660 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-29 21:20:13.917270: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 515, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python2.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python2.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 511, in run_main\r\n    _convert_tf1_model(tflite_flags)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 124, in _convert_tf1_model\r\n    converter = _get_toco_converter(flags)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 111, in _get_toco_converter\r\n    return converter_fn(**converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/lite/python/lite.py\", line 705, in from_frozen_graph\r\n    sess.graph, input_arrays)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow_core/lite/python/util.py\", line 122, in get_tensors_from_tensor_names\r\n    \",\".join(invalid_tensors)))\r\nValueError: Invalid tensors 'input' were found.\r\n```", "Your input_array argument is not correct. You may want to use tensorboard and visualize what your inputs look like.\r\nAnother approach can be to try https://github.com/tensorflow/examples/tree/master/lite/examples/speech_commands/android#tensorflow-lite-speech-command-recognition-android-demo"]}, {"number": 36233, "title": "object_detection", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36232, "title": "Convert TensorArray to RaggedTensor", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TensorFlow-gpu 1.14+\r\n- Are you willing to contribute it (Yes/No): No.\r\n  Not this stage.\r\n\r\n**Describe the feature and the current behavior/state.**\r\n  As tf.keras.model could take a raggedtensor as an input, there is a need to convert a TensorArray to a RaggedTensor.\r\n\r\n**Will this change the current api? How?**\r\n   Yes, it requires a new api to TensorArray (say, to_ragged).\r\n\r\n**Who will benefit with this feature?**\r\n   This allows the output (no rectangle tensors of same shape) of tf.map_fn of batch-based slicing operation as an input to a tf.keras.layers.Conv2D seamlessly. \r\n\r\n**Any Other info.**\r\n  No.\r\n\r\nThank you", "comments": ["Extending TensorArray to allow an op similar to stack() that generates a ragged tensor seems reasonable.  But if your goal is to use `tf.map_fn` with ragged outputs, then note that `map_fn` already supports this in `tf_nightly`.  You just need to use `fn_output_signature` to specify what the output of your function looks like.  E.g.:\r\n\r\n```\r\n>>> x = tf.constant([5, 3, 0, 4])\r\n>>> tf.map_fn(tf.range, x,\r\n...           fn_output_signature=tf.RaggedTensorSpec(shape=[None], dtype=tf.int32))\r\n<tf.RaggedTensor [[0, 1, 2, 3, 4], [0, 1, 2], [], [0, 1, 2, 3]]>\r\n```\r\n", "I'm hitting this use case too, and in my case the goal is the ragged tensor itself.\r\n\r\nI'm writing some code to recursively parse protobufs in the TF graph from their descriptors as part of the input pipeline (using `tf.io.decode_proto`). My input data is highly structured, so the goal is to automatically produce nested named tuples based on the different proto messages that allow me to keep data structured in `tf.data.Dataset` objects.\r\n\r\nFor repeated message fields I stack together the tensors contained by each message. Since their size depends on the data, I'm forced to use `tf.TensorArray` to do the stacking. However, if the messages I'm stacking happen contain other repeated data with different sizes, regular stacking will fail due to mismatching shapes. For that I need to stack them into a ragged tensor if appropriate.\r\n\r\nSince it seems like a reasonable feature with no clear alternative for the cases where you're forced to use `tf.TensorArray`, would it be possible to add it?", "After many attempts, I've managed to get my use case working with `tf.map_fn` and `fn_output_signature` applied recursively over my proto parsing function. However, in the process I've found a couple of issues worth sharing. @edloper : you might want to take a look.\r\n\r\nLet me start with a example that works fine so I can show small variations of it.\r\n```python\r\nx = tf.constant([1, 2, 3], tf.int32)\r\ntf.map_fn(tf.ragged.range, x, fn_output_signature=tf.RaggedTensorSpec([1, None], tf.int32))\r\n```\r\nThis correctly returns `<tf.RaggedTensor [[[0]], [[0, 1]], [[0, 1, 2]]]>` as expected.\r\n\r\nHowever, if I pass an empty tensor I get not an exception but a segmentation fault. This seems to happen every time I use a `tf.RaggedTensorSpec` (no matter its shape) in `fn_output_signature` with an empty tensor passed to `tf.map_fn`.\r\n```python\r\nx = tf.zeros([0], tf.int32)\r\ntf.map_fn(tf.ragged.range, x, fn_output_signature=tf.RaggedTensorSpec([1, None], tf.int32))\r\n```\r\n\r\nI also found a more subtle issue when using regular tensors.\r\n```python\r\nx = tf.zeros([1], tf.int32)\r\ntf.map_fn(tf.range, x, fn_output_signature=tf.TensorSpec([None], tf.int32))\r\n```\r\nThis returns `tf.Tensor([], shape=(1, 0), dtype=int32)`.\r\n\r\nHowever, if the tensor is empty, you're forced to change the shape of the tensor spec.\r\n```python\r\nx = tf.zeros([0], tf.int32)\r\ntf.map_fn(tf.range, x, fn_output_signature=tf.TensorSpec([], tf.int32))\r\n```\r\nThis returns `tf.Tensor([], shape=(0,), dtype=int32)` but only if the shape is `[]`. If you leave it as `[None]` you get the following exception:\r\n> ValueError: Tensor's shape (0,) is not compatible with supplied shape (None, None)\r\n\r\nThis means that the correct `tf.TensorSpec` to pass depends not only on the number of dimensions of the data, but on the data itself. This sounds conceptually wrong to me.", "Should we put a note about the segmentation fault in the ragged tensor api documentation/guide somewhere? I ran into the problem @leandro-gracia-gil was having, and it was kind of a hard issue to track down/google. The reason I'm not sure it's worth the effort is because it's not clear how much time the warning will be relevant after the docs update gets merged.\r\n\r\nWould it be possible to add tags or something to confirmed GitHub issues that correspond to different documentation pages, then have a \"known issues\" section at the bottom of docs pages that pulls from the GitHub API? Or at least a link to the open issues for that page? I bet it would be useful, and maybe cut down on duplicate issues. Sorry this is kind of more general than is directly relevant to the thread.", "@leandro-gracia-gil Thanks for pointing out these issues!  I'll look into things further, but it looks to me like the root cause for both the problems you describe may be the same: `tf.TensorArray` is returning tensors with the wrong rank when it can't infer a complete shape for the result.  E.g.:\r\n\r\n```python\r\n>>> ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=False, element_shape=[5, 3])\r\n>>> print(ta.stack())  # Good: we get the expected shape.\r\ntf.Tensor([], shape=(0, 5, 3), dtype=int32)\r\n>>> ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=False, element_shape=[None, 3])\r\n>>> print(ta.stack())  # Bad: shape is not consistent w/ `element_shape`.\r\ntf.Tensor([], shape=(0,), dtype=int32)\r\n```\r\n\r\n(Note that `ta.stack()` doesn't really have a good way to decide what the size of the second dimension in its return tensor should be -- i.e., it could return a tensor with shape `[0, 0, 3]` or `[0, 1000, 3]`, or `[0, 124131, 3]`, and all of those would be consistent with what we asked for.  But it probably should return a tensor with shape `[0, 0, 3]`.)\r\n\r\nThe reason that this causes problems for `RaggedTensors` is that the `TensorArray` is giving us a tensor with an unexpected rank, and that's causing problems when we try to unpack it.  (It shouldn't be giving a segfault -- that's always a bug if it occurs in TensorFlow -- but it should be raising an exception.)\r\n\r\n@kentslaney Sorry that this issue was hard to track down, and that it's been left unaddressed/unresolved for so long.\r\n", "@leandro-gracia-gil I just wrote a fix for the segfault, and it should hopefully show up in nightly later this week.  \r\n\r\nUnfortunately, because the underlying kernel that's building the RaggedTensor doesn't have access to the expected shape info, we'll have the same kind of issue that you saw with dense tensors.  I.e., if you do something like:\r\n\r\n```\r\n>>> x = tf.zeros([0], tf.int32)\r\n>>> tf.map_fn(lambda x: tf.zeros(x, 10), x, fn_output_signature=tf.RaggedTensorSpec([None, 10], tf.int32))\r\n```\r\n\r\nThen the result will be a `RaggedTensor` with shape `[0, 0]`, whereas the result *should* be a `RaggedTensor` with shape `[0, ?, 10]` (where `?` could technically be any value, but should probably default to zero).  Fixing that will can't be done without modifying the API of the RaggedTensorFromVariant kernel, which can't be done in a backwards-compatible way -- so we'll need to introduce a RaggedTensorFromVariantV2 to fix this.  I'll try to get to that eventually, but at least for now you won't get a segmentation fault, and the behavior for RaggedTensors will be consistent w/ the behavior for dense tensors.", "Sounds great. Thanks!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 36231, "title": "How to save weights of part of a network in TF2.0?", "body": "TF provides `save_weights` API which saves the weights of a network. However, it does not allow one to save only part of the network. For instance, I have \r\n\r\n```\r\nclass X(Model):\r\n    ...\r\n    group1 = [layer_a, layer_b]\r\n    group2 = [...]\r\n\r\nmodel = X()\r\n```\r\n\r\nand I just want to save the weights of `group1`. PyTorch has the `ModuleList` API which is able to turn `group1` into a subnetwork and allows one to just call `X.group1.save_weights(...)`. What about TF? How to do so in TF2.0?\r\n\r\n", "comments": ["@yxchng \r\nCan you please go through the [link](https://www.tensorflow.org/tutorials/keras/save_and_load) , [link2](https://www.tensorflow.org/guide/keras/save_and_serialize) and see if it helps you. Thanks!", "@ravikyram I have went through these two links and there is no mention of saving part of a model in these two links. Have you really read them?", "Please go through this [issue](https://stackoverflow.com/questions/39450046/save-specific-weights-in-tensorflow) incombination with this [doc](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Saver) and let me know if it helps. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@yxchng Are you still facing the issue?", "@yxchng You can actually try to save weights of few select layers as saving a part of the network is not possible.", "Closing this issue as it has been inactive for more than 3 weeks. Please add additional comments and we can open the issue again. Thanks!"]}, {"number": 36230, "title": "Fix spelling errors", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36230) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36230) for more info**.\n\n<!-- ok -->", "Changes have been made."]}, {"number": 36229, "title": "external/pybind11\\include/pybind11/pybind11.h(139): error C2783: \u201cstd::tuple<pybind11::detail::type_caster<pybind11::handle,void>>::tuple(void) noexcept(<expr>)\u201d: \u672a\u80fd\u4e3a\u201c__formal\u201d\u63a8\u5bfc \u6a21\u677f \u53c2\u6570", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 1903\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.15.0\r\n- Python version: Python 3.6.8rc1\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source):0.25.0\r\n- GCC/Compiler version (if compiling from source):VS2017 MSVC 14.16.27023\r\n- CUDA/cuDNN version:10.2/7.0\r\n- GPU model and memory: GTX1070 8.0GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\u4f7f\u7528bazel\u7f16\u8bd1TensorFlow\u65f6\u63d0\u793a \r\nexternal/pybind11\\include/pybind11/pybind11.h(139): error C2783: \u201cstd::tuple<pybind11::detail::type_caster<pybind11::handle,void>>::tuple(void) noexcept(<expr>)\u201d: \u672a\u80fd\u4e3a\u201c__formal\u201d\u63a8\u5bfc \u6a21\u677f \u53c2\u6570\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\u6309\u7167https://www.tensorflow.org/install/source_windows?hl=zh-cn#cpu\u914d\u7f6e\r\n\r\n**Any other info / logs**\r\n\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   F:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Tools\\MSVC\\14.16.27023\\include\\sstream\r\nexternal/pybind11\\include/pybind11/pybind11.h(139): error C2783: \u201cstd::tuple<pybind11::detail::type_caster<pybind11::handle,void>>::tuple(void) noexcept(<expr>)\u201d: \u672a\u80fd\u4e3a\u201c__formal\u201d\u63a8\u5bfc \u6a21\u677f \u53c2\u6570\r\nF:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Tools\\MSVC\\14.16.27023\\include\\tuple(429): note: \u53c2\u89c1\u201cstd::tuple<pybind11::detail::type_caster<pybind11::handle,void>>::tuple\u201d\u7684\u58f0\u660e\r\nexternal/pybind11\\include/pybind11/pybind11.h(73): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684\u51fd\u6570 \u6a21\u677f \u5b9e\u4f8b\u5316\u201cvoid pybind11::cpp_function::initialize<_Ty,R,pybind11::handle,pybind11::is_method>(Func &&,Return (__cdecl *)(pybind11::handle),const pybind11::is_method &)\u201d\u7684\u5f15\u7528\r\n        with\r\n        [\r\n            _Ty=pybind11::detail::enum_base::init::<lambda_46073d681fcd766eb481a3d0374dc63b>,\r\n            R=pybind11::str,\r\n            Func=pybind11::detail::enum_base::init::<lambda_46073d681fcd766eb481a3d0374dc63b>,\r\n            Return=pybind11::str\r\n        ]\r\nexternal/pybind11\\include/pybind11/pybind11.h(1416): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684\u51fd\u6570 \u6a21\u677f \u5b9e\u4f8b\u5316\u201cpybind11::cpp_function::cpp_function<pybind11::detail::enum_base::init::<lambda_46073d681fcd766eb481a3d0374dc63b>,pybind11::is_method,void>(Func &&,const pybind11::is_method &)\u201d\u7684\u5f15\u7528\r\n        with\r\n        [\r\n            Func=pybind11::detail::enum_base::init::<lambda_46073d681fcd766eb481a3d0374dc63b>\r\n        ]\r\nc:\\users\\reshiram\\_bazel_reshiram\\qnykyiqk\\execroot\\org_tensorflow\\external\\pybind11\\include\\pybind11\\cast.h(1556): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684 \u7c7b \u6a21\u677f \u5b9e\u4f8b\u5316 \"pybind11::detail::descr<8>\" \u7684\u5f15\u7528\r\nc:\\users\\reshiram\\_bazel_reshiram\\qnykyiqk\\execroot\\org_tensorflow\\external\\pybind11\\include\\pybind11\\cast.h(1554): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684 \u7c7b \u6a21\u677f \u5b9e\u4f8b\u5316 \"pybind11::detail::descr<5>\" \u7684\u5f15\u7528\r\nc:\\users\\reshiram\\_bazel_reshiram\\qnykyiqk\\execroot\\org_tensorflow\\external\\pybind11\\include\\pybind11\\cast.h(1109): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684 \u7c7b \u6a21\u677f \u5b9e\u4f8b\u5316 \"pybind11::detail::descr<7>\" \u7684\u5f15\u7528\r\nexternal/pybind11\\include/pybind11/pybind11.h(139): error C2783: \u201cstd::tuple<pybind11::detail::type_caster<pybind11::object,void>,pybind11::detail::type_caster<pybind11::object,void>>::tuple(void) noexcept(<expr>)\u201d: \u672a\u80fd\u4e3a\u201c__formal\u201d\u63a8\u5bfc \u6a21\u677f \u53c2\u6570\r\nF:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Tools\\MSVC\\14.16.27023\\include\\tuple(429): note: \u53c2\u89c1\u201cstd::tuple<pybind11::detail::type_caster<pybind11::object,void>,pybind11::detail::type_caster<pybind11::object,void>>::tuple\u201d\u7684\u58f0\u660e\r\nexternal/pybind11\\include/pybind11/pybind11.h(73): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684\u51fd\u6570 \u6a21\u677f \u5b9e\u4f8b\u5316\u201cvoid pybind11::cpp_function::initialize<_Ty,R,pybind11::object,pybind11::object,pybind11::is_method>(Func &&,Return (__cdecl *)(pybind11::object,pybind11::object),const pybind11::is_method &)\u201d\u7684\u5f15\u7528\r\n        with\r\n        [\r\n            _Ty=pybind11::detail::enum_base::init::<lambda_ea06563a4979ca80a55a8ff1d879382b>,\r\n            R=bool,\r\n            Func=pybind11::detail::enum_base::init::<lambda_ea06563a4979ca80a55a8ff1d879382b>,\r\n            Return=bool\r\n        ]\r\nexternal/pybind11\\include/pybind11/pybind11.h(1474): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684\u51fd\u6570 \u6a21\u677f \u5b9e\u4f8b\u5316\u201cpybind11::cpp_function::cpp_function<pybind11::detail::enum_base::init::<lambda_ea06563a4979ca80a55a8ff1d879382b>,pybind11::is_method,void>(Func &&,const pybind11::is_method &)\u201d\u7684\u5f15\u7528\r\n        with\r\n        [\r\n            Func=pybind11::detail::enum_base::init::<lambda_ea06563a4979ca80a55a8ff1d879382b>\r\n        ]\r\nexternal/pybind11\\include/pybind11/pybind11.h(139): error C2783: \u201cstd::tuple<pybind11::detail::type_caster<pybind11::object,void>>::tuple(void) noexcept(<expr>)\u201d: \u672a\u80fd\u4e3a\u201c__formal\u201d\u63a8\u5bfc \u6a21\u677f \u53c2\u6570\r\nF:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Tools\\MSVC\\14.16.27023\\include\\tuple(429): note: \u53c2\u89c1\u201cstd::tuple<pybind11::detail::type_caster<pybind11::object,void>>::tuple\u201d\u7684\u58f0\u660e\r\nexternal/pybind11\\include/pybind11/pybind11.h(73): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684\u51fd\u6570 \u6a21\u677f \u5b9e\u4f8b\u5316\u201cvoid pybind11::cpp_function::initialize<_Ty,R,pybind11::object,pybind11::is_method>(Func &&,Return (__cdecl *)(pybind11::object),const pybind11::is_method &)\u201d\u7684\u5f15\u7528\r\n        with\r\n        [\r\n            _Ty=pybind11::detail::enum_base::init::<lambda_909c581f503d5f259cf84551105b4f30>,\r\n            R=pybind11::int_,\r\n            Func=pybind11::detail::enum_base::init::<lambda_909c581f503d5f259cf84551105b4f30>,\r\n            Return=pybind11::int_\r\n        ]\r\nexternal/pybind11\\include/pybind11/pybind11.h(1507): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684\u51fd\u6570 \u6a21\u677f \u5b9e\u4f8b\u5316\u201cpybind11::cpp_function::cpp_function<pybind11::detail::enum_base::init::<lambda_909c581f503d5f259cf84551105b4f30>,pybind11::is_method,void>(Func &&,const pybind11::is_method &)\u201d\u7684\u5f15\u7528\r\n        with\r\n        [\r\n            Func=pybind11::detail::enum_base::init::<lambda_909c581f503d5f259cf84551105b4f30>\r\n        ]\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 767.769s, Critical Path: 62.28s\r\nINFO: 2518 processes: 2518 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@ZTZ-99 \r\n\r\nMake sure[ long paths are enabled ](https://superuser.com/questions/1119883/windows-10-enable-ntfs-long-paths-policy-option-missing)on Windows.Thanks!", "\u6211\u4fee\u6539 long paths \u7684\u8bbe\u7f6e\u4e3aenable\u540e\uff0c\u65e7\u9519\u8bef\u6ca1\u6709\u518d\u51fa\u73b0\uff0c\u4f46\u662f\u51fa\u73b0\u4e86\u65b0\u7684\u9519\u8bef\r\n\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   bazel-out/x64_windows-py2-opt/bin\\tensorflow/compiler/mlir/lite/ir/tfl_ops.h.inc\r\n.\\tensorflow/compiler/mlir/lite/quantization/quantization_traits.h(98): error C2976: \u201cmlir::OpTrait::TFL::QuantizationSpecTraitBase\u201d: \u6a21\u677f \u53c2\u6570\u592a\u5c11\r\n.\\tensorflow/compiler/mlir/lite/quantization/quantization_traits.h(33): note: \u53c2\u89c1\u201cmlir::OpTrait::TFL::QuantizationSpecTraitBase\u201d\u7684\u58f0\u660e\r\nexternal/local_config_mlir/include\\mlir/IR/OpDefinition.h(875): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684 \u7c7b \u6a21\u677f \u5b9e\u4f8b\u5316 \"mlir::OpTrait::TFL::AccumulatorUniformScale<2,0,1>::Impl<ConcreteType>\" \u7684\u5f15\u7528\r\n        with\r\n        [\r\n            ConcreteType=mlir::TFL::Conv2DOp\r\n        ]\r\nbazel-out/x64_windows-py2-opt/bin\\tensorflow/compiler/mlir/lite/ir/tfl_ops.h.inc(456): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684 \u7c7b \u6a21\u677f \u5b9e\u4f8b\u5316 \"mlir::Op<mlir::TFL::Conv2DOp,mlir::OpTrait::OneResult,mlir::OpTrait::HasNoSideEffect,mlir::OpTrait::TFL::AccumulatorUniformScale<2,0,1>::Impl,mlir::OpTrait::NOperands<3>::Impl>\" \u7684\u5f15\u7528\r\n.\\tensorflow/compiler/mlir/lite/quantization/quantization_traits.h(98): error C2955: \u201cmlir::OpTrait::TFL::QuantizationSpecTraitBase\u201d: \u4f7f\u7528 \u7c7b \u6a21\u677f \u9700\u8981 \u6a21\u677f \u53c2\u6570\u5217\u8868\r\n.\\tensorflow/compiler/mlir/lite/quantization/quantization_traits.h(33): note: \u53c2\u89c1\u201cmlir::OpTrait::TFL::QuantizationSpecTraitBase\u201d\u7684\u58f0\u660e\r\n.\\tensorflow/compiler/mlir/lite/ir/tfl_traits.h(39): error C2976: \u201cmlir::OpTrait::TraitBase\u201d: \u6a21\u677f \u53c2\u6570\u592a\u5c11\r\nexternal/local_config_mlir/include\\mlir/IR/OpDefinition.h(372): note: \u53c2\u89c1\u201cmlir::OpTrait::TraitBase\u201d\u7684\u58f0\u660e\r\nexternal/local_config_mlir/include\\mlir/IR/OpDefinition.h(875): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684 \u7c7b \u6a21\u677f \u5b9e\u4f8b\u5316 \"mlir::OpTrait::TFL::StatefulOperands<18,19>::Impl<ConcreteType>\" \u7684\u5f15\u7528\r\n        with\r\n        [\r\n            ConcreteType=mlir::TFL::LSTMOp\r\n        ]\r\nbazel-out/x64_windows-py2-opt/bin\\tensorflow/compiler/mlir/lite/ir/tfl_ops.h.inc(1148): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684 \u7c7b \u6a21\u677f \u5b9e\u4f8b\u5316 \"mlir::Op<mlir::TFL::LSTMOp,mlir::OpTrait::OneResult,mlir::OpTrait::TFL::StatefulOperands<18,19>::Impl,mlir::OpTrait::NOperands<24>::Impl>\" \u7684\u5f15\u7528\r\n.\\tensorflow/compiler/mlir/lite/ir/tfl_traits.h(39): error C2955: \u201cmlir::OpTrait::TraitBase\u201d: \u4f7f\u7528 \u7c7b \u6a21\u677f \u9700\u8981 \u6a21\u677f \u53c2\u6570\u5217\u8868\r\nexternal/local_config_mlir/include\\mlir/IR/OpDefinition.h(372): note: \u53c2\u89c1\u201cmlir::OpTrait::TraitBase\u201d\u7684\u58f0\u660e\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  external/local_config_mlir/include\\mlir/Dialect/StandardOps/Ops.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   bazel-out/x64_windows-py2-opt/bin/external/local_config_mlir/include\\mlir/Dialect/StandardOps/Ops.h.inc\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  external/local_config_mlir/include\\mlir/IR/Matchers.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  external/local_config_mlir/include\\mlir/IR/OpImplementation.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   external/llvm/include\\llvm/Support/SMLoc.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:    F:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Tools\\MSVC\\14.16.27023\\include\\cassert\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:     F:\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt\\assert.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   external/llvm/include\\llvm/Support/raw_ostream.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:    F:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Tools\\MSVC\\14.16.27023\\include\\cassert\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:     F:\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt\\assert.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  external/local_config_mlir/include\\mlir/IR/PatternMatch.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  external/local_config_mlir/include\\mlir/IR/TypeUtilities.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  .\\tensorflow/compiler/mlir/tensorflow/ir/tf_types.h\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   .\\tensorflow/compiler/mlir/tensorflow/ir/tf_types.def\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:   .\\tensorflow/compiler/mlir/tensorflow/ir/tf_types.def\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  bazel-out/x64_windows-py2-opt/bin\\tensorflow/compiler/mlir/lite/ir/tfl_ops.cc.inc\r\n\u6ce8\u610f: \u5305\u542b\u6587\u4ef6:  bazel-out/x64_windows-py2-opt/bin\\tensorflow/compiler/mlir/lite/ir/tfl_ops.cc.inc\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3304.075s, Critical Path: 622.61s\r\nINFO: 2350 processes: 2350 local.\r\nFAILED: Build did NOT complete successfully", "\u6211\u5207\u6362\u5230\u4e86r1.12\u7248\u672c\uff0c\u5e76\u6253\u5f00long paths\u914d\u7f6e\u540e\uff0c\u7f16\u8bd1\u8fc7\u7a0b\u53ef\u4ee5\u987a\u5229\u5b8c\u6210\u4e86\uff0c\u4f46\u662f\u5728\u6700\u540e\u751f\u6210\u7ed3\u679c\u65f6\u62a5\u544a\uff1a\r\nERROR: C:/tensorflow/tensorflow/BUILD:533:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 127): bash.exe failed: error executing command\r\n  cd C:/users/reshiram/_bazel_reshiram/xv6zejqw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=F:/CUDA10.2\r\n    SET CUDNN_INSTALL_PATH=F:/CUDA10.2\r\n    SET PATH=F:\\msys64\\usr\\bin;F:\\msys64\\bin;F:\\CUDA10.2\\bin;F:\\CUDA10.2\\libnvvp;D:\\Program Files (x86)\\NetSarang\\Xshell 6\\;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;F:\\Python27;F:\\Program Files\\CMake\\bin;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.5.0\\;F:\\CUDA10.2\\lib\\x64;F:\\Python27\\Scripts;F:\\OpenCV4.2.0\\opencv\\lib\\install\\x64\\vc16\\bin;F:\\OpenCV4.2.0\\opencv\\build\\bin;F:\\CUDA10.2\\include;F:\\Python36\\Scripts;F:\\Python36;F:\\Program Files\\Git\\cmd;F:\\ROS\\swigwin-4.0.1;F:\\ROS\\bazel;F:\\msys64\\usr\\bin;F:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Tools\\MSVC\\14.16.27023\\include;C:\\Users\\Reshiram\\AppData\\Local\\Microsoft\\WindowsApps\r\n    SET PYTHON_BIN_PATH=F:/Python36/python3.exe\r\n    SET PYTHON_LIB_PATH=F:/Python36/lib/site-packages\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  F:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/create_tensorflow.python_api_1.exe --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/x64_windows-opt/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow._api.v1 bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/activations/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/backend/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/callbacks/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/constraints/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/estimator/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/initializers/__init__.p/_api/v1/keras/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/models/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/optimizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/genfiles/keras/regularizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/wrappers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/wrappers/scikit_learn/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/user_ops/__init__.py\r\n2020-01-29 19:18:49.719983: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 19.691s, Critical Path: 7.35s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\u63d0\u793a\u65e0\u6cd5\u8fdb\u5165\u76ee\u5f55\uff0c\u8fd9\u4e2a\u76ee\u5f55\u786e\u5b9e\u5b58\u5728\u5e76\u4e14MSYS2\u4f7f\u7528cd\u6307\u4ee4\u53ef\u4ee5\u76f4\u63a5\u8fdb\u5165\uff0c\u6211\u4e0d\u77e5\u9053\u8fd9\u662f\u4ec0\u4e48\u95ee\u9898\u5bfc\u81f4\u7684\uff0c\u800c\u4e14\u5728\u8fd9\u4e4b\u524d\u5df2\u7ecf\u6b63\u786e\u751f\u6210\u4e86\u4e00\u4e9b\u7f16\u8bd1\u7ed3\u679c\u3002", "@ZTZ-99 We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. please refer to **[build from source](https://www.tensorflow.org/install/source_windows)** Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36229\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36229\">No</a>\n"]}, {"number": 36228, "title": "tensorflow java GPU compute capabilties 6.0 instead of 3.7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Linux 18.06\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.0, 1.13.1, 1.13.2, 1.14.0, 1.15.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Nope\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: K80\r\n\r\n**Describe the problem**\r\nI am loading a model in a Java Server using the Java API to do inference. The inference is working but not running on the GPU. I get an error message provided below.\r\n\r\nWe have been using the python version of the same tensorflow release on the same GPU (K80) without an issue. This is only a problem with the java driver. \r\n\r\nI believe the culprit is this line:\r\nhttps://github.com/tensorflow/tensorflow/blob/aa50d2b624c7e8d56b4b1644c4ccf489d8e8c55c/tensorflow/tools/ci_build/presubmit/ubuntu_16/gpu_py36_full/build.sh#L41-L44\r\n\r\nThe comment seems to indicate that this env variable is not used. I don't really understand the setup but it looks like the Java build is picking it up.\r\n\r\nNo other build seems to be requiring compute 6.0, everything is set to 3.7 (K80) which would make sense since K80 are probably the cheapest GPUs available on cloud.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/tools/ci_build/xla/linux/gpu/run_py3.sh#L31\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/938c08e5e612c5a581262e409cda08db61b25d25/tensorflow/tools/ci_build/Dockerfile.gpu.ppc64le#L28\r\n\r\n[Search of all  TF_CUDA_COMPUTE_CAPABILITIES in the Repo](https://github.com/tensorflow/tensorflow/search?q=TF_CUDA_COMPUTE_CAPABILITIES&unscoped_q=TF_CUDA_COMPUTE_CAPABILITIES)\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nUse the Java Driver on GPU.\r\n\r\n**Any other info / logs**\r\n```\r\n 2020-01-25 23:17:43.458562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1717] Ignoring visible gpu device (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7) with Cuda compute capability 3.7. The min \u2502\r\n\u2502 imum required Cuda capability is 6.0.                                                                                                                                                                                                      \u2502\r\n\u2502 2020-01-25 23:17:43.519712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:                                                                                       \u2502\r\n\u2502 2020-01-25 23:17:43.519748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0                                                                                                                                                \u2502\r\n\u2502 2020-01-25 23:17:43.519762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N\r\n```\r\n\r\nCI Logs that contain compute restriction:\r\nhttps://source.cloud.google.com/results/invocations/37ab790b-7136-4d77-a415-f03b9909e3b2/targets/tensorflow%2Fgithub%2Fubuntu_16%2Fgpu_py36_full%2Fcontinuous/log\r\n", "comments": ["> I believe the culprit is this line:\r\n> https://github.com/tensorflow/tensorflow/blob/aa50d2b624c7e8d56b4b1644c4ccf489d8e8c55c/tensorflow/tools/ci_build/presubmit/ubuntu_16/gpu_py36_full/build.sh#L41-L44\r\n> \r\n> The comment seems to indicate that this env variable is not used. I don't really understand the setup but it looks like the Java build is picking it up.\r\n\r\nHow did you triage the problem to that line?  Is it because that's the only place that uses 6.0 as a compute capability?  If so, that might be a red herring.\r\n\r\nIIUC the TF JNI GPU packages are uploaded here https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow_jni-gpu-linux-x86_64.tar.gz.\r\n\r\n@gunan Do you know how these JNI packages are built and uploaded?", "The person who used to build these has left the team. So I do not know who owns them now.\r\n@goldiegadde any ideas?", "Has there been any update on this issue? I have encountered the same problem.\r\nRegards,\r\nCarlos", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36228\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36228\">No</a>\n"]}, {"number": 36227, "title": "Poor performance and warning for GradientTape.jacobian with experimental_use_pfor=False", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nExtremely slow gradient calculation. \r\nAlso the following warning: \r\n```\r\nWARNING:tensorflow:From /home/grad3/cmich/miniconda3/envs/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.identity instead.\r\n```\r\n\r\n**Describe the expected behavior**\r\nThere most be a much faster way of getting these gradients. After all the TensorFlow optimization must use these and those evaluations do not take so long. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self,):\r\n        super(Model, self).__init__()\r\n        self.layers_list = [tf.keras.layers.Dense(2)] + \\\r\n            [tf.keras.layers.Dense(1000)] + \\\r\n            [tf.keras.layers.Dense(500)]\r\n\r\n    def call(self, x):\r\n        for layer in self.layers_list:\r\n            x = layer(x)\r\n        return x\r\n\r\nmodel = Model()\r\nmodel.build((1,2))\r\nweights = model.trainable_variables\r\n\r\ninput = np.zeros([1,2])\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    ts = time.time()\r\n    output = model(input)\r\n    print(f'Forward took {time.time()-ts:.2f}s')\r\n\r\nts = time.time()\r\ngradients = tape.jacobian(output, weights, experimental_use_pfor=False)\r\nprint(f'Backward took {time.time()-ts:.2f}s')\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nOutput:\r\n```\r\nForward took 0.01s\r\nWARNING:tensorflow:From /home/grad3/cmich/miniconda3/envs/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.identity instead.\r\nBackward took 5.75s\r\n```\r\nThe gradient calculation takes several orders of magnitude more time (for my real case the respective times are about 0.03s and 18s which is too much). \r\n\r\nIs this the best way of doing this or this a faster way? Also should I care about the warning?", "comments": ["Any reasons why you are setting `experimental_use_pfor=False`? `pfor` means \"parallel for\" and it brings greater parallelism.", "@byronyi Indeed I had to use both the ``experimental_use_pfor=False`` and the ``persistent=True`` options for my real case (would get an error and a message saying I needed to). But I did not check if this toy case would run without them. I will check in more detail tomorrow when I am back in front of a computer and update the toy case if necessary to better reflect my real case.    ", "@byronyi I want to use LeakyReLU activation. When I add this layer then I get the following error:\r\n```\r\nEncountered an exception while vectorizing the jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.\r\n```\r\n\r\n\r\nThe modified toy case is:\r\n```\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self,):\r\n        super(Model, self).__init__()\r\n        self.layers_list = [tf.keras.layers.Dense(2)] + \\\r\n            [tf.keras.layers.Dense(1000)] + \\\r\n            [tf.keras.layers.LeakyReLU(alpha=0.1)] + \\\r\n            [tf.keras.layers.Dense(500)]\r\n\r\n    def call(self, x):\r\n        for layer in self.layers_list:\r\n            x = layer(x)\r\n        return x\r\n\r\nmodel = Model()\r\nmodel.build((1,2))\r\nweights = model.trainable_variables\r\n\r\ninput = np.zeros([1,2], dtype='float32')\r\n\r\n# with tf.GradientTape(persistent=True) as tape:\r\nwith tf.GradientTape() as tape:\r\n    ts = time.time()\r\n    output = model(input)\r\n    print(f'Forward took {time.time()-ts:.2f}s')\r\n\r\nts = time.time()\r\n# gradients = tape.jacobian(output, weights, experimental_use_pfor=False)\r\ngradients = tape.jacobian(output, weights)\r\nprint(f'Backward took {time.time()-ts:.2f}s')\r\n```", "Also, with the original toy case (without LeakyReLU) if I don't use of the ``experimental_use_pfor=False`` I get the following:\r\n\r\n```\r\nForward took 0.01s\r\n2020-01-27 10:35:40.909307: E tensorflow/core/common_runtime/executor.cc:642] Executor failed to create kernel. Internal: No function library\r\n\t [[{{node loop_body/MatMul_4/pfor/cond}}]]\r\n2020-01-27 10:35:40.950468: E tensorflow/core/common_runtime/executor.cc:642] Executor failed to create kernel. Internal: No function library\r\n\t [[{{node loop_body/MatMul_3/pfor/cond}}]]\r\n2020-01-27 10:35:40.991867: E tensorflow/core/common_runtime/executor.cc:642] Executor failed to create kernel. Internal: No function library\r\n\t [[{{node loop_body/MatMul_1/pfor/cond}}]]\r\nBackward took 1.03s\r\n``` ", "It looks like you are computing jacobian on an output of size 500. Without pfor, it would translate to 500 evaluations of gradient, so a slowdown of 500x is as expected. It looks like pfor vectorization provides a 5x speedup on top of an iterative strategy which also sounds in line with expectation. So I'd say that this is working as intended.\r\n\r\nAs an aside, If you plan on using a batch size larger than 1, I'd recommend batch_jacobian instead of jacobian.", "@cmichelenstrofer Please let me know if the issue has been resolved using the above comment. Thanks!", "If this is how slow it is supposed to be than I guess there's nothing to do. Unfortunately, I cannot use batch normalization for my case. I was just surprised it was so slow when the optimization/training in tensorflow is not this slow and it requires gradient calculation. But from what I've been reading it seems tensorflow never calculates the full jacobian when doing back-propagation/gradient descent. ", "@agarwal-ashish Can you PTAL? Thanks!", "@cmichelenstrofer can you clarify the requirement ? Typically networks have a scalar loss at the top which makes the jacobian computation cheap. For non-scalar output, batch jacobian would be faster if you can use it (batch entries are independently transformed). If you compute full jacobian with non-scalar outputs, then yes, it will be exepected to be slow.", "@agarwal-ashish The requirement is for non-scalar output and no batches. Very high-dimensional input and output spaces. And I need the Jacobian of all outputs w.r.t. all inputs (no batch Jacobian). I was under the impression that the full Jacobian needed to be computed as an intermediate step even when using a scalar output (a scalar output is after all one extra step in the forward model). This seems to not be the case and you use a numerical trick to avoid computing the large intermediate Jacobian (is this correct?). So counter-intuitively adding an extra step to your forward model makes the back-propagation cheaper. If this is indeed the case, it sounds like there is no way around the jacobian being so expensive. ", "Gradient functions are implemented as vector-jacobian products, and they often use tricks for more efficient computation which avoids creating jacobian matrices.\r\nGradientTape.Jacobian still builds upon those tricks, but effectively it is now running many more vector-jacobian computations, which it vectorizes for 5-10x speedups."]}, {"number": 36226, "title": "How is data laid out in TfLiteTensor.data.uint8[]?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.19.0-6-amd64 #1 SMP Debian 4.19.67-2+deb10u2 (2019-11-11) x86_64 GNU/Linux\r\n- TensorFlow installed from (source or binary):  source at https://github.com/tensorflow/tensorflow/archive/v2.1.0.tar.gz\r\n- TensorFlow version (or github SHA if from source): 2.1.0\r\n\r\nI am loading a MobileNet V2 in tensorflow-lite and then getting a reference to a tensor with index `4` as follows:\r\n```\r\nTfLiteTensor *avgpool_input = interpreter->tensor(4);\r\n```\r\nThe dimension of this tensor is: `avgpool_input->dims->data = [1 7 7 1280]`. I know that the data of this tensor is stored in `avgpool_input.data.uint8[]`. I do not know how the 4-D tensor data is laid out in this flat array. I looked into the `tensorflow/lite/c/c_api_internal.h` file but I could not find any documentation about the layout of data. Can someone please help me understand the layout of data or point me out to relevant parts of the source code?\r\n", "comments": ["I am trying to guess the layout using the code below:\r\n```\r\nfor(uint32_t k=0; k<1280; ++k) {\r\n    uint32_t sum = 0;\r\n    for(uint32_t i=0; i<7; ++i) {\r\n        for(uint32_t j=0; j<7; ++j) {\r\n            sum += avgpool_input->data.uint8[k*1280 + i*7 + j];\r\n        }\r\n    }\r\n    uint32_t avgpool_output_k = sum / 49;\r\n}\r\n```\r\nIf this were the data layout, my calculations should match the output of the average pool layer. But this does not match.", "Hi,\r\n\r\nI think you are right that we don't have great documentation here. The layout of the tensor data is such that the values in the dimension of highest index \"change fastest\" as you iterate through the data. So, with typical 4D tensors in MobileNet models labeled as [batch, filter height, filter width, channels], you step through the channel dimension as you iterate through the data. You can see this in the code if you look at the AveragePool reference implementation in TF Lite:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/pooling.h#L95-L128\r\n\r\nThere you can see that we iterate over channel in the innermost loop. The batch dimension is the outermost for loop.\r\n\r\nYou can also investigate the tensors as numpy arrays. I believe you are referring to the MobileNet V2 quantized model available for download here:\r\n\r\nhttps://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz\r\n\r\nAfter unzipping/untaring you can load the model like this:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.lite.python.interpreter import Interpreter\r\ninterp = Interpreter(\"./mobilenet_v2_1.0_224_quant.tflite\")\r\n```\r\n\r\nOne of the weights tensors is at index 19\r\n\r\n```\r\ntensor = interp.get_tensor(19)\r\nIn [106]: tensor.shape                                                                                                                             \r\nOut[106]: (1, 3, 3, 96)\r\n\r\nIn [107]: tensor[0,0,0,0:5]                                                                                                                        \r\nOut[107]: array([ 96, 124, 112, 120, 114], dtype=uint8)\r\n```\r\n\r\nSo those are the first five values of the tensor in the channel dimension. Now we can look at the raw data in memory (as hex values) and convert it back to ints. Here I will list out the first 10 hex values and convert them back to ints:\r\n\r\n```\r\nIn [108] memview = tensor.data.hex()\r\n\r\nIn [109]: [int('0x' + memview[i:i+2], 16) for i in range(0,10,2)]                                                                                  \r\nOut[109]: [96, 124, 112, 120, 114]\r\n```\r\n\r\nSo we confirmed that the raw memory layout is as expected. A good tip, in general, is to check out the reference implementations for each op. You can see how the math is done, but also get an idea of how the data is laid out. For example, if you look at the quantized Average Pool, you can see that it's a bit more complex than what you have. We have to worry about rounding/clamping.\r\n\r\n", "I am grateful for you support in improving my understanding of the code. Thank you."]}, {"number": 36225, "title": "download_and_extract URL DIR download_dependencies.sh", "body": "I follow [the build for arm64](https://www.tensorflow.org/lite/guide/build_arm64) and try  to execute `download_dependencies.sh` script but end up with issue:\r\n\r\n```\r\n~/work/tensorflow$ ./tensorflow/lite/tools/make/download_dependencies.sh\r\n./tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR\r\n```\r\n\r\nAnybody can advise on this issue?\r\n", "comments": ["@peter197321, Can you please fill the [Template](https://github.com/tensorflow/tensorflow/issues/new/choose) to analyze the issue. Thanks!"]}, {"number": 36224, "title": "tf.keras NaN loss when using multiple GPUs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: 2x NVIDIA GTX 1080 Ti 11GB\r\n- Driver Version: 440.33.01\r\n\r\n\r\nI am currently using Tensorflow 2.0 (Python) and the` tf.keras` library to train a CNN. However, I am encountering an issue when I try to train my model by calling `model.fit()`. \r\n\r\nAfter I begin training, the loss is normal for 1 ~ 2 steps for the first epoch. But after that, it suddenly becomes **NaN** loss.\r\n\r\nThis issue only happens when using multiple GPUs. The code I'm using works perfectly fine on a single GPU. I have wrapped all of my code inside the scope of a `tf.distribute.MirroredStrategy ` using `with strategy.scope():`. I am feeding my network with data from a `tf.data.Dataset` (though this error occurs regardless of the data format).\r\n\r\nI then ran some tests:\r\n\r\n1) I tried to replace the data in my dataset with random number, but the loss still went to **NaN**.\r\n\r\n2) I also tried feeding the numpy arrays directly to `.fit()`, but that didn't solve the issue.\r\n\r\n3) I tried using different optimizers (Adam, RMSprop, SGD), batch sizes (4, 8, 16, 32), and learning rates, none of which helped to solve this problem.\r\n\r\n4) I swapped out my network for a simple Multi-layer Perceptron, but the error persisted.\r\n\r\nThis doesn't appear to be an OOM issue, since the data is relatively small and running `watch -n0.1 nvidia-smi` reveals that memory usage never exceeds 30% on either of my GPUs. There doesn't seem to be any warning or error in the console output that might hint at the issue either.\r\n\r\nAny help is appreciated", "comments": ["@rodyt Could you please provide us with simple standalone code to reproduce the issue in our environment, Thanks.", "Hi @Saduf2019 ,\r\n\r\nThanks for responding. I am able to reproduce the error with the sample MNIST code provided on the Tensorflow website:\r\nhttps://www.tensorflow.org/tutorials/distribute/keras\r\n\r\nThis is the sample code from the tutorial that I tried to run:\r\n```\r\n\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\ntfds.disable_progress_bar()\r\nimport os\r\n\r\ndatasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\n\r\nBUFFER_SIZE = 10000\r\n\r\nBATCH_SIZE_PER_REPLICA = 64\r\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * 2\r\n\r\ndef scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n\r\n    return image, label\r\n\r\ntrain_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\neval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    \r\n    model.compile(loss='sparse_categorical_crossentropy',\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\n    model.fit(\r\n        train_dataset, \r\n        epochs=12\r\n    )\r\n\r\n```\r\nThis is the output when I run `.fit` using `MirroredStrategy`:\r\n\r\n> Epoch 1/12\r\n>\r\n> INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n> INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n> INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n> INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n>      16/Unknown - 7s 423ms/step - loss: **nan** - accuracy: 0.0932\r\n\r\nThis is the output of the model when I run _without_ `MirroredStrategy`:\r\n\r\n> \r\n> Epoch 1/12\r\n> 469/469 [==============================] - 2s 4ms/step - loss: 0.0480 - accuracy: 0.9853\r\n> Epoch 2/12\r\n> 469/469 [==============================] - 2s 3ms/step - loss: 0.0361 - accuracy: 0.9892\r\n> Epoch 3/12\r\n> 469/469 [==============================] - 2s 3ms/step - loss: 0.0302 - accuracy: 0.9908\r\n> Epoch 4/12\r\n> 469/469 [==============================] - 2s 3ms/step - loss: 0.0249 - accuracy: 0.9922\r\n> Epoch 5/12\r\n> 469/469 [==============================] - 2s 3ms/step - loss: 0.0197 - accuracy: 0.9944\r\n> Epoch 6/12\r\n> 469/469 [==============================] - 2s 3ms/step - loss: 0.0163 - accuracy: 0.9950\r\n> Epoch 7/12\r\n> 469/469 [==============================] - 1s 3ms/step - loss: 0.0122 - accuracy: 0.9966\r\n> Epoch 8/12\r\n> 469/469 [==============================] - 2s 3ms/step - loss: 0.0102 - accuracy: 0.9970\r\n> Epoch 9/12\r\n> 469/469 [==============================] - 1s 3ms/step - loss: 0.0082 - accuracy: 0.9977\r\n> Epoch 10/12\r\n> 469/469 [==============================] - 1s 3ms/step - loss: 0.0087 - accuracy: 0.9975\r\n> Epoch 11/12\r\n> 469/469 [==============================] - 2s 3ms/step - loss: 0.0060 - accuracy: 0.9983\r\n> Epoch 12/12\r\n> 469/469 [==============================] - 1s 3ms/step - loss: 0.0051 - accuracy: 0.9984", "I am unable to repro the issue in a colab using the standalone snippet you provided.  Can you try using [tf.debugging.enabled_check_numerics](https://www.tensorflow.org/api_docs/python/tf/debugging/enable_check_numerics) to help pinpoint the op that is causing this?", "I'm sorry, I don't have TF 2.1. What is the alternative to this?", "> I'm sorry, I don't have TF 2.1. What is the alternative to this?\r\n\r\nWell, I can tell you that this problem have not been sloved at tensorflow 2.1....\r\n", "Same here with TF 2.1, have been using the same example and also get the NaN-Values while the training on 2 gpu's is much slower than on one", "Is there any other way I can provide debug information on TF 2.0? It's incredibly annoying to not be able to use all my GPUs. @anj-s  @jvishnuvardhan ", "@rodyt I have got them same issue while using Keras in multiple GPUs. Have you solved your problem? \r\n![Screenshot from 2020-02-06 16-35-29](https://user-images.githubusercontent.com/15938335/73924270-ca3b5300-48fe-11ea-8cdf-0f55fdc91137.png)\r\n", "@rodyt we are unable to reproduce this problem with the code snippet you provided. Is it possible that there is a driver/hardware issue with one of the GPUs? \r\n\r\n@VyBui please file a separate ticket for your issues - there could be many different reasons for getting a NaN. If you file a separate ticket with your code to repro, setup etc, that can help us look into that issue. ", "@rodyt  Closing this issue as we are unable to reproduce this problem with the code snippet you provided.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36224\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36224\">No</a>\n"]}, {"number": 36223, "title": "tensor flow installing error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia GEFORCE GTX 1050\r\n\r\n\r\n\r\n**Describe the problem**\r\nIm trying to demo image detection in darkflow and it lead to this error\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**:\r\n(darkflow-env) C:\\Binod\\Deep_Learning\\Object_Detection\\YOLO_Detection\\darkflow-master>python flow --model cfg --load bin\\yolov2.weights --imgdir sample_dog --gpu 0.7\r\nTraceback (most recent call last):\r\n  File \"flow\", line 4, in <module>\r\n    from darkflow.cli import cliHandler\r\n  File \"C:\\Binod\\Deep_Learning\\Object_Detection\\YOLO_Detection\\darkflow-master\\darkflow\\cli.py\", line 3, in <module>\r\n    from .net.build import TFNet\r\n  File \"C:\\Binod\\Deep_Learning\\Object_Detection\\YOLO_Detection\\darkflow-master\\darkflow\\net\\build.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\darkflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\darkflow\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\darkflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\darkflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\darkflow\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\darkflow\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 47, in <module>\r\n    import numpy as np\r\n  File \"C:\\darkflow\\lib\\site-packages\\numpy\\__init__.py\", line 140, in <module>\r\n    from . import _distributor_init\r\n  File \"C:\\darkflow\\lib\\site-packages\\numpy\\_distributor_init.py\", line 34, in <module>\r\n    from . import _mklinit\r\nImportError: DLL load failed: The specified module could not be found.\r\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Closing as duplicate"]}, {"number": 36222, "title": "Could we have conv2d_transpose for C++?", "body": "**System information**\r\n- TensorFlow version : 2.1\r\n\r\nIn python we have conv2d_transpose to upsample. Could we have something similar for the C++ api inaddition to ResizeBilinear, ResizeNearestNeighbor and ResizeBicubic and ResizeArea?", "comments": ["As an alternative to Conv2DTranspose you could also  use Upsample + conv2D for achieving deconvolution/learnable upsampling .", "@tensyq, Please fill the issue [Template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!", "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): No\r\n**Describe the feature and the current behavior/state.**\r\n**Will this change the current api? How?**\r\nPossibly similar to conv2d with padding\r\nI am trying to implement upsampling + convolution in tensorflow c++ like this:\r\n....\r\n```\r\n Scope scope_conv3 = t_root.NewSubScope(\"Conv3_layer\");\r\n     in_channels = out_channels;\r\n     out_channels = 16;\r\n     auto pool3 = AddConvolutionLayer(\"C1\", scope_conv3, in_channels, out_channels, filter_side, pool2);\r\n     int new_side_3 = ceil((float)new_side_2 / 2);\r\n\r\nauto upsample_1 = ResizeBilinear(i_root.WithOpName(\"upsample_1\"), pool3, Const(i_root, {new_side_2, new_side_2}));\r\n     Scope scope_conv_4 = t_root.NewSubScope(\"Convolution4_layer\");\r\n```\r\n\r\nwhere AddConvolutionLayer does something like this:\r\n```\r\n     auto conv = Conv2D(scope.WithOpName(\"Convolution\"), input, weights_variables[\"W\"+idx], {1, 1, 1, 1}, \"SAME\");\r\n     auto bias = BiasAdd(scope.WithOpName(\"Bias\"), conv, bias_variables[\"Bias\"+idx]);\r\n     auto relu = Relu(scope.WithOpName(\"Relu\"), bias);\r\n     return MaxPool(scope.WithOpName(\"Pooling\"), relu, {1, 2, 2, 1}, {1, 2, 2, 1}, \"SAME\");\r\n```\r\n\r\nThe error I get is:\r\n`Non-OK-status: t_session->Run(ops_to_run, nullptr) status: Invalid argument: Node 'Convolution4_layer/Conv': Unknown input node 'upsample_1'`\r\n\r\nIf instead conv2d_transpose was provided I would be able to do exactly what I did with Conv2D plus  I do not need to worry about the different ways of resizing.\r\n"]}]