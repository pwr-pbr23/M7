[{"number": 9214, "title": "Fixed jemalloc build on linux ppc64le when it is enabled while configure", "body": "", "comments": ["Can one of the admins verify this patch?", "@jhseu @jart any suggestion?", "This PR can not be merged as is. @npanpaliya are these select() expression changes meant to enable jemalloc when `n` is answered to the `./configure` prompt asking if jemalloc should be enabled?", "@jart @jhseu I'm another member of @npanpaliya 's team.  We don't want to force jemalloc to always build on ppc, but we couldn't think of a good way to work around the fact that the current with_jemalloc config_setting in tensorflow/BUILD is limited to k8.\r\n\r\nLooking at the suggestions in the thread now...  Would it be a good solution to:\r\n\r\n- modify existing K8-specific config_setting rule to be \"with_jemalloc_k8\"\r\n- add new ppc64le-specific config_setting rule \"with_jemalloc_ppc64le\"\r\n- update the selects in build_config.bzl / tf_additional_lib_defines and tf_additional_lib_deps to recognize both with_jemalloc_k8 and with_jemalloc_ppc64le", "I see what you're saying. In that case could we say:\r\n\r\n```py\r\nconfig_setting(\r\n    name = \"with_jemalloc\",\r\n    values = {\"define\": \"with_jemalloc=true\"},\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n```\r\n\r\nAnd then:\r\n\r\n```py\r\nselect({\r\n    \"//tensorflow:with_jemalloc\": select({\r\n        \"//tensorflow:linux_x86_64\": [...],\r\n        \"//tensorflow:linux_ppc64le\": [...],\r\n        \"//conditions:default\": [],\r\n```", "@hartb Yeah, that works.\r\n\r\n@jart Nesting selects doesn't work in Bazel, unfortunately:\r\nhttps://github.com/bazelbuild/bazel/issues/1623", "In that case I'd recommend one of the following:\r\n\r\n1. Keep current approach and rename `with_jemalloc` to `with_jemalloc_on_linux_x86_64` and rename `linux_ppc64le` to `with_jemalloc_on_linux_ppc64le`.\r\n\r\n2. Investigate if jemalloc can just be enabled for all platforms if `with_jemalloc` is `true`. When I coded it this way IIRC I was erring on the side of caution and inheriting previous decisions. I haven't investigated what platforms jemalloc does and does not support.\r\n\r\nI would prefer (2) but it's probably more work.", "#2 is a decent amount of work, so let's stick with #1\n\nOn Apr 14, 2017 12:42 PM, \"Justine Tunney\" <notifications@github.com> wrote:\n\n> In that case I'd recommend one of the following:\n>\n>    1.\n>\n>    Keep current approach and rename with_jemalloc to\n>    with_jemalloc_on_linux_x86_64 and rename linux_ppc64le to\n>    with_jemalloc_on_linux_ppc64le.\n>    2.\n>\n>    Investigate if jemalloc can just be enabled for all platforms if\n>    with_jemalloc is true. When I coded it this way IIRC I was erring on\n>    the side of caution and inheriting previous decisions. I haven't\n>    investigated what platforms jemalloc does and does not support.\n>\n> I would prefer (2) but it's probably more work.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9214#issuecomment-294221910>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAKYw8uo7Cw0rjJ5F01bgFduYACdPiEeks5rv8wogaJpZM4M9jTv>\n> .\n>\n", "@jart @jhseu Thank you both!  @npanpaliya will re-work the PR.\r\n\r\nWe'll go with approach 1; we just don't have access to most of the other platforms to test.\r\n\r\nDo you have any suggestions for the change in third_party/jemalloc.BUILD?  There we want to substitute in a page size indicator depending on the architecture (and really, OS and arch).  Is there a way we could set an intermediate variable based on linux_ppc64le / otherwise, and the substitute that into 'cmd'?", "We can add select() expressions inside jemalloc.BUILD using the \"@%ws%//tensorflow:FOO\" config_setting label hack.", "@jart Thank you; we'll take a look at that. \r\n\r\nI also see some cases of select() used for genrule / cmd in third_party/jpeg/jpeg.BUILD.  Maybe we can do something like that.", "Thank you all for your suggestions. I've implemented the suggested approach. Kindly review and let me know your feedback.", "Jenkins, test this please.", "Jenkins, test this please"]}, {"number": 9213, "title": "System hangs when computing gradients on GPU", "body": "### Problem description\r\n\r\nMy system hangs when computing gradients on the GPU.  I am able to compute gradients on the CPU without issue.  I can compute all the nodes in my graph, including my loss function, on the GPU without issue.  My system hangs when computing gradients on the GPU irrespective of which optimizer I use.  The code that produces this issue is:\r\n\r\n```{python}\r\nsess = tf.Session()\r\n\r\n# ...\r\n\r\ncnn = CNNClass()\r\n\r\n#...\r\n\r\nstep_size = tf.placeholder(tf.float32, name=\"step_size\") \r\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False) \r\noptimizer = tf.train.MomentumOptimizer(step_size, 0.9)\r\n\r\n#...\r\n\r\n# CRASHES WHEN I RUN THIS\r\ngrads_and_vars = optimizer.compute_gradients(cnn.loss)\r\nsess.run([grads_and_vars], feed_dict)\r\n\r\n# DOES NOT CRASH WHEN I RUN THIS\r\nsess.run([cnn.loss])\r\n```\r\n\r\n### Further information\r\n\r\nI tried setting `gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333)` in my ConfigProto but that didn't help.  \r\n\r\nLaunching `watch -n 0.1 nvidia-smi` before running the code shows Volatile GPU-Util goes to 100% just before hanging.\r\n\r\nComputing gradients on the CPU and printing them out shows normal output (I'm not getting any NaNs or 0's or anything like that).\r\n\r\n### System information\r\n\r\n- System: Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.0.0-65-g4763edf-dirty 1.0.1\r\n- CUDA version: 8.0, V8.0.61\r\n- cuDNN version: 5.1\r\n- GPU model and memory: NVidia GeForce GTX TITAN X (12GB)\r\n- Video card driver: 375.39", "comments": ["The code runs on the GPU after scaling down the model.  \r\n\r\nThere appears to be significant overhead in starting computations on the GPU.  I can see this when I train an intermediate-size model: Volatile GPU-Util goes to 100%  for a while without completing any evaluations, then Volatile GPU-Util drops to around %30 and the evaluation loop begins running fairly quickly.  This wouldn't be an issue except for the fact that my system is almost completely unusable until Volatile GPU-Util drops back down to 30%.\r\n\r\nI assume overhead is somehow associated with putting data on the GPU.  Can someone explain why this happens?  Is there a way to prevent TensorFlow from hogging all Volatile GPU during the upstart phase?"]}, {"number": 9212, "title": "Provide wheel packages at pypi for python 3.6 on windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **TensorFlow installed from (source or binary)**: binary(expect)\r\n\r\n### Describe the problem\r\n\r\nProviding wheel packages at pypi for python 3.6 on windows would be helpful. As it is the latest anaconda python version.\r\n", "comments": ["Please see #6999"]}, {"number": 9211, "title": "Enabled LLVM to be built on Linux ppc64le when XLA is enable\u2026", "body": "Added ppc64le's support while building LLVM when it is enabled through configure option. ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Ignoring mac failure from XLA graph, and coordinator timeout."]}, {"number": 9210, "title": "Feature: Sparse matrix multiplications for Tensors with rank > 2", "body": "## System Information:\r\nWindows 10, x64, Tensorflow 1.1.0.rc1\r\n\r\n## Description:\r\nThe 3-D sparse tensor (placeholder) multiply with 3-D dense tensor has bug, the operation will failed.\r\n\r\n```python\r\nx = tf.sparse_placeholder(tf.float32, shape=[None, 2, 2])\r\ny = tf.constant(np.ones([3, 2, 1]), dtype=tf.float32)\r\nz = tf.matmul(x, y, a_is_sparse=True)\r\n\r\nindices = [[1, 1, 1], [2, 0, 0], [3, 0, 1]]\r\nvalues = [1.0, 2.0, 3.0]\r\ndense_shape = [3, 2, 2]\r\nx_val = tf.SparseTensorValue(indices, values, dense_shape)\r\n\r\nwith tf.Session() as sess:\r\n  res = sess.run(z, feed_dict={x: x_val})\r\n  print(res)\r\n```\r\nexpected result(3x2x1):\r\n```\r\n[[[ 0.][ 1.]]\r\n [[ 1.][ 0.]]\r\n [[ 1.][ 0.]]]\r\n```\r\nbut output some errors actually :\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"D:/Learning/master_project/clinicalText/SourceCode/Python/DNN_CWS/seg_dnn.py\", line 369, in <module>\r\n    cws = SegDNN(constant.VOCAB_SIZE, embed_size, constant.DNN_SKIP_WINDOW)\r\n  File \"D:/Learning/master_project/clinicalText/SourceCode/Python/DNN_CWS/seg_dnn.py\", line 76, in __init__\r\n    self.loss = tf.reduce_sum(tf.matmul(self.slim_map_matrix,tf.expand_dims(tf.transpose(self.word_score),2),a_is_sparse=True))\r\n  File \"E:\\IntelPython35\\envs\\tensorflow-intel\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1755, in matmul\r\n    a = ops.convert_to_tensor(a, name=\"a\")\r\n  File \"E:\\IntelPython35\\envs\\tensorflow-intel\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 639, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"E:\\IntelPython35\\envs\\tensorflow-intel\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 704, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"E:\\IntelPython35\\envs\\tensorflow-intel\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 113, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"E:\\IntelPython35\\envs\\tensorflow-intel\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 102, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"E:\\IntelPython35\\envs\\tensorflow-intel\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 444, in make_tensor_proto\r\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\r\n  File \"E:\\IntelPython35\\envs\\tensorflow-intel\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 444, in <listcomp>\r\n    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\r\n  File \"E:\\IntelPython35\\envs\\tensorflow-intel\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 65, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x00000195FA86B1D0>\r\n\r\n```\r\n\r\nchange the `z` to \r\n```python\r\nz = tf.sparse_tensor_dense_matmul(x,y)\r\n```\r\nalso failed because the shape of sparse must 2-D,but `x`and`b`has 3-D", "comments": ["Note that `tf.matmul` requires [dense tensors as both arguments](https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/matmul). The `a_is_sparse` and `b_is_sparse` arguments are used only to choose a possibly more efficient algorithm when `a` (and respectively `b`) have a lot zeros. The error message perhaps could be improved though.\r\n\r\nIf you're using the `SparseTensor` class, then [`tf.sparse_tensor_dense_matmul`](https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_dense_matmul) is indeed what you want. Support for tensors with rank > 2 in that is not something that anyone is actively working on, contributions are welcome!\r\n\r\n(Long story short, this isn't a bug but arguably a missing feature for which contributions are welcome)", "FWIW, the error message was improved in commit 3e239dc36147ef2d730ff4de50de59d9acfe0181, but alas that is not part of the 1.1 release, so it will be improved in 1.2", "Would a function like `sparse_tensor_dense_tensortdot` resolve this issue? I was thinking of implementing this function in `python`, similarly to `tensordot`.\r\n\r\nI took a look at the linked feature request which seems to do this in `C++`. But just like `tensordot` is implemented purely in `python`, I think this is possible too.\r\n\r\nThoughts? If someone thinks it's a good idea, I'll go about implementing it.", "@kojino I just found this page because I wanted to do a sparse-dense tensordot operation. If you're willing the implement it (even as a custom Op) I would certainly much appreciate it! I'm sure we can't be the only two who would be interested in this! Cheers", "I implemented a function that is based off of `tf.tensordot`. It's long, but I think it works.\r\n\r\n```\r\ndef sparse_tensor_dense_tensordot(sp_a, b, axes, name=None):\r\n    r\"\"\"Tensor contraction of a and b along specified axes.\r\n    Tensordot (also known as tensor contraction) sums the product of elements\r\n    from `a` and `b` over the indices specified by `a_axes` and `b_axes`.\r\n    The lists `a_axes` and `b_axes` specify those pairs of axes along which to\r\n    contract the tensors. The axis `a_axes[i]` of `a` must have the same dimension\r\n    as axis `b_axes[i]` of `b` for all `i` in `range(0, len(a_axes))`. The lists\r\n    `a_axes` and `b_axes` must have identical length and consist of unique\r\n    integers that specify valid axes for each of the tensors.\r\n    This operation corresponds to `numpy.tensordot(a, b, axes)`.\r\n    Example 1: When `a` and `b` are matrices (order 2), the case `axes = 1`\r\n    is equivalent to matrix multiplication.\r\n    Example 2: When `a` and `b` are matrices (order 2), the case\r\n    `axes = [[1], [0]]` is equivalent to matrix multiplication.\r\n    Example 3: Suppose that \\\\(a_{ijk}\\\\) and \\\\(b_{lmn}\\\\) represent two\r\n    tensors of order 3. Then, `contract(a, b, [[0], [2]])` is the order 4 tensor\r\n    \\\\(c_{jklm}\\\\) whose entry\r\n    corresponding to the indices \\\\((j,k,l,m)\\\\) is given by:\r\n    \\\\( c_{jklm} = \\sum_i a_{ijk} b_{lmi} \\\\).\r\n    In general, `order(c) = order(a) + order(b) - 2*len(axes[0])`.\r\n    Args:\r\n        a: `SparseTensor` of type `float32` or `float64`.\r\n        b: `Tensor` with the same type as `a`.\r\n        axes: Either a scalar `N`, or a list or an `int32` `Tensor` of shape [2, k].\r\n         If axes is a scalar, sum over the last N axes of a and the first N axes\r\n         of b in order.\r\n         If axes is a list or `Tensor` the first and second row contain the set of\r\n         unique integers specifying axes along which the contraction is computed,\r\n         for `a` and `b`, respectively. The number of axes for `a` and `b` must\r\n         be equal.\r\n        name: A name for the operation (optional).\r\n    Returns:\r\n        A `Tensor` with the same type as `a`.\r\n    Raises:\r\n        ValueError: If the shapes of `a`, `b`, and `axes` are incompatible.\r\n        IndexError: If the values in axes exceed the rank of the corresponding\r\n            tensor.\r\n    \"\"\"\r\n\r\n    def _tensordot_reshape(a, axes, flipped=False):\r\n        \"\"\"Helper method to perform transpose and reshape for contraction op.\r\n        This method is helpful in reducing `math_tf.tensordot` to `math_tf.matmul`\r\n        using `tf.transpose` and `tf.reshape`. The method takes a\r\n        tensor and performs the correct transpose and reshape operation for a given\r\n        set of indices. It returns the reshaped tensor as well as a list of indices\r\n        necessary to reshape the tensor again after matrix multiplication.\r\n        Args:\r\n            a: `Tensor`.\r\n            axes: List or `int32` `Tensor` of unique indices specifying valid axes of\r\n             `a`.\r\n            flipped: An optional `bool`. Defaults to `False`. If `True`, the method\r\n                assumes that `a` is the second argument in the contraction operation.\r\n        Returns:\r\n            A tuple `(reshaped_a, free_dims, free_dims_static)` where `reshaped_a` is\r\n            the tensor `a` reshaped to allow contraction via `matmul`, `free_dims` is\r\n            either a list of integers or an `int32` `Tensor`, depending on whether\r\n            the shape of a is fully specified, and free_dims_static is either a list\r\n            of integers and None values, or None, representing the inferred\r\n            static shape of the free dimensions\r\n        \"\"\"\r\n        if a.get_shape().is_fully_defined() and isinstance(axes, (list, tuple)):\r\n            shape_a = a.get_shape().as_list()\r\n            axes = [i if i >= 0 else i + len(shape_a) for i in axes]\r\n            free = [i for i in range(len(shape_a)) if i not in axes]\r\n            free_dims = [shape_a[i] for i in free]\r\n            prod_free = int(np.prod([shape_a[i] for i in free]))\r\n            prod_axes = int(np.prod([shape_a[i] for i in axes]))\r\n            perm = list(axes) + free if flipped else free + list(axes)\r\n            new_shape = [prod_axes, prod_free] if flipped else [prod_free, prod_axes]\r\n            reshaped_a = tf.reshape(tf.transpose(a, perm), new_shape)\r\n            return reshaped_a, free_dims, free_dims\r\n        else:\r\n            if a.get_shape().ndims is not None and isinstance(axes, (list, tuple)):\r\n                shape_a = a.get_shape().as_list()\r\n                axes = [i if i >= 0 else i + len(shape_a) for i in axes]\r\n                free = [i for i in range(len(shape_a)) if i not in axes]\r\n                free_dims_static = [shape_a[i] for i in free]\r\n            else:\r\n                free_dims_static = None\r\n            shape_a = tf.shape(a)\r\n            rank_a = tf.rank(a)\r\n            axes = tf.convert_to_tensor(axes, dtype=tf.int32, name=\"axes\")\r\n            axes = tf.cast(axes >= 0, tf.int32) * axes + tf.cast(\r\n                    axes < 0, tf.int32) * (\r\n                            axes + rank_a)\r\n            free, _ = tf.setdiff1d(tf.range(rank_a), axes)\r\n            free_dims = tf.gather(shape_a, free)\r\n            axes_dims = tf.gather(shape_a, axes)\r\n            prod_free_dims = tf.reduce_prod(free_dims)\r\n            prod_axes_dims = tf.reduce_prod(axes_dims)\r\n            perm = tf.concat([axes_dims, free_dims], 0)\r\n            if flipped:\r\n                perm = tf.concat([axes, free], 0)\r\n                new_shape = tf.stack([prod_axes_dims, prod_free_dims])\r\n            else:\r\n                perm = tf.concat([free, axes], 0)\r\n                new_shape = tf.stack([prod_free_dims, prod_axes_dims])\r\n            reshaped_a = tf.reshape(tf.transpose(a, perm), new_shape)\r\n            return reshaped_a, free_dims, free_dims_static\r\n\r\n    def _tensordot_axes(a, axes):\r\n        \"\"\"Generates two sets of contraction axes for the two tensor arguments.\"\"\"\r\n        a_shape = a.get_shape()\r\n        if isinstance(axes, compat.integral_types):\r\n            if axes < 0:\r\n                raise ValueError(\"'axes' must be at least 0.\")\r\n            if a_shape.ndims is not None:\r\n                if axes > a_shape.ndims:\r\n                    raise ValueError(\"'axes' must not be larger than the number of \"\r\n                                                     \"dimensions of tensor %s.\" % a)\r\n                return (list(range(a_shape.ndims - axes, a_shape.ndims)),\r\n                                list(range(axes)))\r\n            else:\r\n                rank = tf.rank(a)\r\n                return (range(rank - axes, rank, dtype=tf.int32),\r\n                                range(axes, dtype=tf.int32))\r\n        elif isinstance(axes, (list, tuple)):\r\n            if len(axes) != 2:\r\n                raise ValueError(\"'axes' must be an integer or have length 2.\")\r\n            a_axes = axes[0]\r\n            b_axes = axes[1]\r\n            if isinstance(a_axes, compat.integral_types) and \\\r\n                    isinstance(b_axes, compat.integral_types):\r\n                a_axes = [a_axes]\r\n                b_axes = [b_axes]\r\n            if len(a_axes) != len(b_axes):\r\n                raise ValueError(\r\n                        \"Different number of contraction axes 'a' and 'b', %s != %s.\" %\r\n                        (len(a_axes), len(b_axes)))\r\n            return a_axes, b_axes\r\n        else:\r\n            axes = tf.convert_to_tensor(axes, name=\"axes\", dtype=tf.int32)\r\n        return axes[0], axes[1]\r\n\r\n    def _sparse_tensordot_reshape(a, axes, flipped=False):\r\n        \"\"\"Helper method to perform transpose and reshape for contraction op.\r\n        This method is helpful in reducing `math_tf.tensordot` to `math_tf.matmul`\r\n        using `tf.transpose` and `tf.reshape`. The method takes a\r\n        tensor and performs the correct transpose and reshape operation for a given\r\n        set of indices. It returns the reshaped tensor as well as a list of indices\r\n        necessary to reshape the tensor again after matrix multiplication.\r\n        Args:\r\n            a: `Tensor`.\r\n            axes: List or `int32` `Tensor` of unique indices specifying valid axes of\r\n             `a`.\r\n            flipped: An optional `bool`. Defaults to `False`. If `True`, the method\r\n                assumes that `a` is the second argument in the contraction operation.\r\n        Returns:\r\n            A tuple `(reshaped_a, free_dims, free_dims_static)` where `reshaped_a` is\r\n            the tensor `a` reshaped to allow contraction via `matmul`, `free_dims` is\r\n            either a list of integers or an `int32` `Tensor`, depending on whether\r\n            the shape of a is fully specified, and free_dims_static is either a list\r\n            of integers and None values, or None, representing the inferred\r\n            static shape of the free dimensions\r\n        \"\"\"\r\n        if a.get_shape().is_fully_defined() and isinstance(axes, (list, tuple)):\r\n            shape_a = a.get_shape().as_list()\r\n            axes = [i if i >= 0 else i + len(shape_a) for i in axes]\r\n            free = [i for i in range(len(shape_a)) if i not in axes]\r\n            free_dims = [shape_a[i] for i in free]\r\n            prod_free = int(np.prod([shape_a[i] for i in free]))\r\n            prod_axes = int(np.prod([shape_a[i] for i in axes]))\r\n            perm = list(axes) + free if flipped else free + list(axes)\r\n            new_shape = [prod_axes, prod_free] if flipped else [prod_free, prod_axes]\r\n            reshaped_a = tf.sparse_reshape(tf.sparse_transpose(a, perm), new_shape)\r\n            return reshaped_a, free_dims, free_dims\r\n        else:\r\n            if a.get_shape().ndims is not None and isinstance(axes, (list, tuple)):\r\n                shape_a = a.get_shape().as_list()\r\n                axes = [i if i >= 0 else i + len(shape_a) for i in axes]\r\n                free = [i for i in range(len(shape_a)) if i not in axes]\r\n                free_dims_static = [shape_a[i] for i in free]\r\n            else:\r\n                free_dims_static = None\r\n            shape_a = tf.shape(a)\r\n            rank_a = tf.rank(a)\r\n            axes = tf.convert_to_tensor(axes, dtype=tf.int32, name=\"axes\")\r\n            axes = tf.cast(axes >= 0, tf.int32) * axes + tf.cast(\r\n                    axes < 0, tf.int32) * (\r\n                            axes + rank_a)\r\n            print(sess.run(rank_a), sess.run(axes))\r\n            free, _ = tf.setdiff1d(tf.range(rank_a), axes)\r\n            free_dims = tf.gather(shape_a, free)\r\n            axes_dims = tf.gather(shape_a, axes)\r\n            prod_free_dims = tf.reduce_prod(free_dims)\r\n            prod_axes_dims = tf.reduce_prod(axes_dims)\r\n            perm = tf.concat([axes_dims, free_dims], 0)\r\n            if flipped:\r\n                perm = tf.concat([axes, free], 0)\r\n                new_shape = tf.stack([prod_axes_dims, prod_free_dims])\r\n            else:\r\n                perm = tf.concat([free, axes], 0)\r\n                new_shape = tf.stack([prod_free_dims, prod_axes_dims])\r\n            reshaped_a = tf.sparse_reshape(tf.sparse_transpose(a, perm), new_shape)\r\n            return reshaped_a, free_dims, free_dims_static\r\n\r\n    def _sparse_tensordot_axes(a, axes):\r\n        \"\"\"Generates two sets of contraction axes for the two tensor arguments.\"\"\"\r\n        a_shape = a.get_shape()\r\n        if isinstance(axes, tf.compat.integral_types):\r\n            if axes < 0:\r\n                raise ValueError(\"'axes' must be at least 0.\")\r\n            if a_shape.ndims is not None:\r\n                if axes > a_shape.ndims:\r\n                    raise ValueError(\"'axes' must not be larger than the number of \"\r\n                                                     \"dimensions of tensor %s.\" % a)\r\n                return (list(range(a_shape.ndims - axes, a_shape.ndims)),\r\n                                list(range(axes)))\r\n            else:\r\n                rank = tf.rank(a)\r\n                return (range(rank - axes, rank, dtype=tf.int32),\r\n                                range(axes, dtype=tf.int32))\r\n        elif isinstance(axes, (list, tuple)):\r\n            if len(axes) != 2:\r\n                raise ValueError(\"'axes' must be an integer or have length 2.\")\r\n            a_axes = axes[0]\r\n            b_axes = axes[1]\r\n            if isinstance(a_axes, compat.integral_types) and \\\r\n                    isinstance(b_axes, compat.integral_types):\r\n                a_axes = [a_axes]\r\n                b_axes = [b_axes]\r\n            if len(a_axes) != len(b_axes):\r\n                raise ValueError(\r\n                        \"Different number of contraction axes 'a' and 'b', %s != %s.\" %\r\n                        (len(a_axes), len(b_axes)))\r\n            return a_axes, b_axes\r\n        else:\r\n            axes = tf.convert_to_tensor(axes, name=\"axes\", dtype=tf.int32)\r\n        return axes[0], axes[1]\r\n\r\n    with tf.name_scope(name, \"SparseTensorDenseTensordot\", [sp_a, b, axes]) as name:\r\n#         a = tf.convert_to_tensor(a, name=\"a\")\r\n        b = tf.convert_to_tensor(b, name=\"b\")\r\n        sp_a_axes, b_axes = _sparse_tensordot_axes(sp_a, axes)\r\n        sp_a_reshape, sp_a_free_dims, sp_a_free_dims_static = _sparse_tensordot_reshape(sp_a, sp_a_axes)\r\n        b_reshape, b_free_dims, b_free_dims_static = _tensordot_reshape(\r\n                b, b_axes, True)\r\n        ab_matmul = tf.sparse_tensor_dense_matmul(sp_a_reshape, b_reshape)\r\n        if isinstance(sp_a_free_dims, list) and isinstance(b_free_dims, list):\r\n            return tf.reshape(ab_matmul, sp_a_free_dims + b_free_dims, name=name)\r\n        else:\r\n            sp_a_free_dims = tf.convert_to_tensor(sp_a_free_dims, dtype=tf.int32)\r\n            b_free_dims = tf.convert_to_tensor(b_free_dims, dtype=tf.int32)\r\n            product = tf.reshape(\r\n                    ab_matmul, tf.concat([sp_a_free_dims, b_free_dims], 0), name=name)\r\n            if sp_a_free_dims_static is not None and b_free_dims_static is not None:\r\n                product.set_shape(sp_a_free_dims_static + b_free_dims_static)\r\n            return product\r\n```\r\n\r\n", "I ended up hacking something together using loops and sparse_slice but I'll take a look at using this instead (sparse_slice doesn't have a gradient)", "To matrix multiply two 3-D tensors, one sparse and one dense, where the first dimension is the batch size, you can do this very succinctly using tf.map_fn():\r\n\r\n```\r\ndef sparse_dense_matmult_batch(sp_a, b):\r\n\r\n    def map_function(x):\r\n        i, dense_slice = x[0], x[1]\r\n        sparse_slice = tf.sparse.reshape(tf.sparse.slice(\r\n            sp_a, [i, 0, 0], [1, sp_a.dense_shape[1], sp_a.dense_shape[2]]),\r\n            [sp_a.dense_shape[1], sp_a.dense_shape[2]])\r\n        mult_slice = tf.sparse.matmul(sparse_slice, dense_slice)\r\n        return mult_slice\r\n\r\n    elems = (tf.range(0, sp_a.dense_shape[0], delta=1, dtype=tf.int64), b)\r\n    return tf.map_fn(map_function, elems, dtype=tf.float32, back_prop=True)\r\n```\r\n\r\nThis has the benefit that tf.map_fn() allows iterations to run in parallel as opposed to a sequential for loop. I was using tf.while_loop before to do this, but that resulted in a SparseTensor being added to a collection as a result of the while_context. This was a problem since SparseTensor doesn't have a \"name\" field and can't be added to any collections whose to_proto() implementation expects one (I needed to save and load the meta graph).\r\n", "@bkutt Brilliant thank you very much, I'll give this a go. Parallelizing the multiplies will certainly be a big help!\r\n\r\n", "It also has support for backprop", "I tried out @bkutt's [proposed implementation](https://github.com/tensorflow/tensorflow/issues/9210#issuecomment-497889961) in a custom `keras.layers.Layer`'s `call` method and tensorflow raises an error saying:\r\n\r\n> OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\r\ndecorating the `map_function` function doesn't ends up raising the same error.\r\n\r\nSo in short, a fully functional implementation cannot rely on iterating through slices of the `SparseTensor`.", "I'm using Tensorflow2.4.1 and I've revised @bkutt's solution as below:\r\n\r\n```\r\ndef sparse_dense_matmult_batch(sp_a, b):\r\n    return tf.map_fn(\r\n        lambda x: tf.sparse.sparse_dense_matmul(x[0], x[1]),\r\n        elems=(sp_a, b), fn_output_signature=tf.float32\r\n    )\r\n```\r\nIt looks much more straightforward now!\r\nHowever, if we calculate it in this way,\r\nthe gradient is not defined yet in TF2.4.1.\r\nTherefore, this way can only be used for purpose other than training such as evaluation.", "Hi @supercoderhawk! Inline with @BoyoChen's suggestion , Have you tried  migrating your code base to latest stable version TF 2.6  yet? Please create a new issue if the issue is replicating in newer version. Thanks!", "> Hi @supercoderhawk! Inline with @BoyoChen's suggestion , Have you tried migrating your code base to latest stable version TF 2.6 yet? Please create a new issue if the issue is replicating in newer version. Thanks!\r\n\r\nThe project required this feature has ended, so this issue can be closed now.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 9209, "title": "[Windows - Bazel] ERROR undeclared inclusion(s) in rule '//tensorflow/core/kernels:fake_quant_ops_gpu'", "body": "Windows 10\r\nCUDA 8.0\r\ncuDNN 5.1\r\nTensorFlow at e8a34420ce06c7f99049b4e4fec4308aff8dd058\r\nBazel at [e565230](https://github.com/bazelbuild/bazel/commit/eecd7128f420b2d404ed2f42d549dea3bd198d9d)\r\nMsys2\r\n\r\nDuring build Bazel failed with\r\n\r\n```\r\nERROR: C:/users/adriano/documents/tensorflow/tensorflow/core/kernels/BUILD:2809:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:fake_quant_ops_gpu':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/fake_quant_ops_gpu.cu.cc':\r\n  'C:/windows/temp/tmpxft_0000426c_00000000-8_fake_quant_ops_gpu.cu.cudafe1.stub.c'\r\n  'C:/windows/temp/tmpxft_0000426c_00000000-6_fake_quant_ops_gpu.cu.fatbin.c'\r\nc:\\tmp\\_bazel_adriano\\piryq3n9\\execroot\\tensorflow\\external\\eigen_archive\\eigen\\src/Core/products/Parallelizer.h(20): warning: variable \"m_maxThreads\" was set but never used\r\nc:\\tmp\\_bazel_adriano\\piryq3n9\\execroot\\tensorflow\\external\\eigen_archive\\eigen\\src/Core/ArrayWrapper.h(93): warning: __declspec attributes ignored\r\nc:\\tmp\\_bazel_adriano\\piryq3n9\\execroot\\tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/SimpleThreadPool.h(139): warning: \"constexpr\" is ignored here in Microsoft mode\r\nc:\\tmp\\_bazel_adriano\\piryq3n9\\execroot\\tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/NonBlockingThreadPool.h(151): warning: \"constexpr\" is ignored here in Microsoft mode\r\nc:\\tmp\\_bazel_adriano\\piryq3n9\\execroot\\tensorflow\\external\\eigen_archive\\eigen\\src/Core/products/Parallelizer.h(20): warning: variable \"m_maxThreads\" was set but never used\r\nc:\\tmp\\_bazel_adriano\\piryq3n9\\execroot\\tensorflow\\external\\eigen_archive\\eigen\\src/Core/ArrayWrapper.h(93): warning: __declspec attributes ignored\r\nc:\\tmp\\_bazel_adriano\\piryq3n9\\execroot\\tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/SimpleThreadPool.h(139): warning: \"constexpr\" is ignored here in Microsoft mode\r\nc:\\tmp\\_bazel_adriano\\piryq3n9\\execroot\\tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/ThreadPool/NonBlockingThreadPool.h(151): warning: \"constexpr\" is ignored here in Microsoft mode\r\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2078.109s, Critical Path: 104.55s\r\nFAILED: Build did NOT complete successfully\r\n+ exit 1\r\n\r\n```\r\n\r\n#### Steps to reproduce\r\n- configure TensorFlow accordingly without XLA\r\n- `export BUILD_OPTS='--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=/w --verbose_failures --experimental_ui'`\r\n- `./tensorflow/tools/ci_build/windows/gpu/pip/build_tf_windows.sh    `\r\n\r\n", "comments": ["@mrry Guy with the correct Windows CUDA [versions](https://www.tensorflow.org/install/install_windows#requirements_to_run_tensorflow_with_gpu_support) is encountering what appears to be either a Bazel sandbox or strict deps issue.", "Alas support for Bazel on Windows is still limited, and it will probably be easier to build with CMake.\r\n\r\n@meteorcloudy Have you seen this error before?", "I was expecting to leverage gcc but was warned by @meteorcloudy I might face issues if building TF. \r\nI am aware bazel on Windows is experimental but decided to post the issue anyways to make the problem known for future improvements.", "Hmm, did those errors appear when using GCC as the compiler on Windows? (Like @meteorcloudy, I'd be surprised if that worked, especially for the GPU build, but also for the various dependent libraries that might not have GCC-aware build configurations for Windows in our build files.)", "In https://github.com/bazelbuild/bazel/issues/2753#issuecomment-292818313 @meteorcloudy warned me it wouldn't probably work to build TF, but as I mentioned before I just wanted to report if maybe in the future it happens to be something to be addressed. I will close it as it's not really relevant and to be supported. It'd be cool though if MSVC wasn't so limited when comes to SIMD optimizations. ", "@mrry Someone else had a compiling issue building with GPU supp. related to `fake_quant_ops_gpu` as well but on Ubuntu, seems something with AVX though\r\nhttp://stackoverflow.com/questions/43482642/tesnsorflow-1-1-build-breaks-on-cuda-code\r\n", "Thanks for pointing that out! @cwhipkey Are you aware of any issues with building `fake_quant_ops_gpu` (which I think you added, but feel free to redirect :)...)?", "(Looks like this has also surfaced in #9296.)", "I don't know of any issues.  The code in those kernels hasn't changed for months.\r\n\r\nIt's not clear to me if it's an issue with fake_quant kernel, or whether that's the kernel that happens to hit the error first.  I guess we could try compiling a single other kernel library to see.\r\n\r\n(agree that the other issue, #9296, looks more like an avx512 issue, as it's a failure compiling avx512fintrin.h)\r\n"]}, {"number": 9208, "title": "Update vgg_16 ref in slim readme", "body": "Fixes #9187 ", "comments": []}, {"number": 9207, "title": "Cherrypick", "body": "Fixing the gradient test flakiness.", "comments": ["Jenkins, test this please."]}, {"number": 9206, "title": "rc2 RELEASE file update", "body": "Updating RELEASE.md to include keras fixes and TensorBoard text dashboard.", "comments": ["Yes, in the rc0 update.", "Jenkins, test this please."]}, {"number": 9205, "title": "Results are corrupted when running multiple sessions on one GPU ", "body": "Tensorflow generates corrupted results when running two sessions concurrently on gpu. Each session has a separate graph.\r\n\r\nCode (thx to @bnoodle):\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom threading import Thread, Event\r\n\r\nsize = 10240\r\ndef myfunc(sess, name):\r\n  values = set()\r\n  count = 0\r\n  while True:\r\n    count += 1\r\n    v = sess.run(name + \"/matmul4:0\", feed_dict={name + \"/input:0\": np.ones((1,1))})\r\n    v = float(np.squeeze(v))\r\n    old = len(values)\r\n    values.add(v)\r\n    if len(values) != old:\r\n      print(values, name, count)\r\n\r\ndef create_graph(sess, name):\r\n  with sess.graph.as_default():\r\n    with tf.variable_scope(name):\r\n      input = tf.placeholder(tf.float32, shape=[1,1], name = \"input\")\r\n      tf.set_random_seed(1)\r\n\r\n      matrix1 = tf.Variable(tf.truncated_normal([1, size]), name = 'matrix1')\r\n      matrix2 = tf.Variable(tf.truncated_normal([size, size]), name = 'matrix2')\r\n      matrix4 = tf.Variable(tf.truncated_normal([size, 1]), name = 'matrix4')\r\n\r\n      matmul1 = tf.matmul(input, matrix1, name = 'matmul1')\r\n      matmul2 = tf.matmul(matmul1, matrix2, name = 'matmul2')\r\n      matmul4 = tf.matmul(matmul2, matrix4, name = \"matmul4\")\r\n      sess.run(tf.global_variables_initializer())\r\n\r\nsess1 = tf.Session()\r\nwith tf.device(\"/gpu:0\"):\r\n  create_graph(sess1, \"s1\")\r\n\r\nsess2 = tf.Session()\r\nwith tf.device(\"/gpu:0\"):\r\n  create_graph(sess2, \"s2\")\r\n\r\nt1 = Thread(target=myfunc, args=(sess1, 's1'))\r\nt1.start()\r\n\r\nt2 = Thread(target=myfunc, args=(sess2, 's2'))\r\nt2.start()\r\n```\r\nsess1 should output -17430.388671875, sess2 should output -968.17529296875. But the output sets are nondeterministically growing:\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GRID K520\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\r\npciBusID 0000:00:03.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.91GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\r\n((set([set([-968.17529296875]), 's2', 1-17430.388671875]))\r\n, 's1', 1)\r\n(set([-968.17529296875, -903.794921875]), 's2', 2)\r\n(set([-17430.388671875, -17302.84375]), 's1', 2)\r\n(set([-968.17529296875, -903.794921875, 2173.232177734375]), 's2', 511)\r\n(set([-968.17529296875, -903.794921875, 2173.232177734375, 841.5855712890625]), 's2', 1723)\r\n(set([-17430.388671875, -17302.84375, -211.60272216796875]), 's1', 1961)\r\n(set([-968.17529296875, -903.794921875, 2173.232177734375, 841.5855712890625, -30.6038818359375]), 's2', 2180)\r\n(set([-17430.388671875, -17302.84375, -1592.9022216796875, -211.60272216796875]), 's1', 2722)\r\n(set([-968.17529296875, -903.794921875, 2173.232177734375, -287.991455078125, -30.6038818359375, 841.5855712890625]), 's2', 3337)\r\n...\r\n```\r\n\r\nTensorflow serving has a similar corrupted results issue https://github.com/tensorflow/serving/issues/335, but this seems to be a tensorflow gpu memory problem. With @yaroslavvb memory_util.py, it seems when sess1 and sess2 interleaving in memory allocation/deallocation and gpu memory address (the second to last column in the log) is reused, results will be corrupted. The shortest trace I found is\r\n```\r\n(set([-968.17529296875]), 's2', 1)\r\n('**************', 's2', 2)\r\n(set([-17430.388671875]), 's1', 1)\r\n('**************', 's1', 2)\r\n(set([-968.17529296875, -875.1435546875]), 's2', 2)\r\n       11                     s2/matmul1(44-gpu_bfc)       40960       40960 gpu_bfc 30103334400 MemoryLogTensorAllocation\r\n       23                     s1/matmul1(46-gpu_bfc)       40960       81920 gpu_bfc 30103375360 MemoryLogTensorAllocation\r\n       26                     s2/matmul2(47-gpu_bfc)       40960      122880 gpu_bfc 30103416320 MemoryLogTensorAllocation\r\n       28                     s2/matmul1(44-gpu_bfc)      -40960       81920 gpu_bfc -1 MemoryLogTensorDeallocation\r\n       31                     s2/matmul2(47-gpu_bfc)      -40960       40960 gpu_bfc -1 MemoryLogTensorDeallocation\r\n       35                     s1/matmul2(49-gpu_bfc)       40960       81920 gpu_bfc 30103334400 MemoryLogTensorAllocation\r\n       37                     s1/matmul1(46-gpu_bfc)      -40960       40960 gpu_bfc -1 MemoryLogTensorDeallocation\r\n       40                     s1/matmul2(49-gpu_bfc)      -40960           0 gpu_bfc -1 MemoryLogTensorDeallocation\r\n       61                     s2/matmul1(52-gpu_bfc)       40960       40960 gpu_bfc 30103334400 MemoryLogTensorAllocation\r\n       66                     s2/matmul2(53-gpu_bfc)       40960       81920 gpu_bfc 30103170560 MemoryLogTensorAllocation\r\n       71                     s2/matmul1(52-gpu_bfc)      -40960       40960 gpu_bfc -1 MemoryLogTensorDeallocation\r\n       80                     s2/matmul2(53-gpu_bfc)      -40960           0 gpu_bfc -1 MemoryLogTensorDeallocation\r\n       81                     s1/matmul1(56-gpu_bfc)       40960       40960 gpu_bfc 30103334912 MemoryLogTensorAllocation\r\n       85                     s1/matmul2(57-gpu_bfc)       40960       81920 gpu_bfc 30103170560 MemoryLogTensorAllocation\r\n       87                     s1/matmul1(56-gpu_bfc)      -40960       40960 gpu_bfc -1 MemoryLogTensorDeallocation\r\n       90                     s1/matmul2(57-gpu_bfc)      -40960           0 gpu_bfc -1 MemoryLogTensorDeallocation\r\n```\r\n\r\n### System Information\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: Yes\r\n- *OS Platform and Distribution (i.e. Linux Ubuntu 16.0)*: Ubuntu 14.04\r\n- *TensorFlow installed from (source or binary)?*: binary\r\n- *TensorFlow version* (use command below): 1.0.1\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*: cuda 8.0/cudnn5.1.5\r\n- *GPU Model and Memory*: GRID K520, 4GB\r\n- *Exact command to reproduce*: python multi_session.py\r\n\r\n", "comments": ["*Note: Please let us triagers take care of tagging team members for you.*\r\n\r\nI'm noticing that the example code breaks the contract mentioned in session.h:\r\n\r\n```c\r\n/// A Session allows concurrent calls to Run(), though a Session must\r\n/// be created / extended by a single thread.\r\n///\r\n/// Only one thread must call Close(), and Close() must only be called\r\n/// after all other calls to Run() have returned.\r\n```\r\n\r\nIIUC tf.Session is meant to be [thread safe](https://www.tensorflow.org/programmers_guide/threading_and_queues) in the sense that you're allowed to use a tf.Session object from multiple threads. I'm not aware of the documentation making any promises for thread safety with multiple tf.Session objects. There's also parameters like [intra_op_parallelism_threads](http://stackoverflow.com/questions/34389945/changing-the-number-of-threads-in-tensorflow-on-cifar10) that can help scale sessions to multiple threads.\r\n\r\nDoes your example work if the session objects are creating within the spawned threads, rather than being created in the main thread and handed off?\r\n\r\nIf not, then what is your use case for having multiple sessions running in parallel?", "Thx @jart. The main use case to have multiple sessions is in tensorflow serving inference. Each saved model will be loaded and a new session is created. When a model is no longer used, it can be offloaded and the corresponding session is deleted. As in https://github.com/tensorflow/serving/issues/335, multiple people have this use case and see wrong results.\r\n\r\nI also modified the multi_session.py to call create_graph in the spawned threads. This still generates corrupted results.\r\n\r\nFor thread safety of session, I noticed running multiple sessions on CPU using multiple serving threads in C++ has no issues.", "@nfiedel https://github.com/tensorflow/serving/issues/335 sounds concerning. Based on my cursory glance, this seems like an issue in Serving. Do you believe this is actionable on TensorFlow?", "Issue is fixed! Thanks to tensorflow commit https://github.com/caisq/tensorflow/commit/fccaac3d1bf1391756fae67f1979afe598d10ed1, I rerun the reported python test program, results are consistent. cc @caisq", "@dongwang218 Could you share the snippet of your code to make it work?\r\nIt seems now multiple sessions can be run on the same gpu avoiding interference each other.\r\nAnd, cannot find the documentation for HOW.\r\n\r\nThank you in advance.", "@hyeokhyen caisq's fix is in tensorflow >=1.4, my test python code in the first post works afterwards."]}, {"number": 9204, "title": "deleted", "body": "", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9203, "title": "Updated the link to tensorflow.org to use https", "body": "This is a very minor change. \r\n\r\nI just updated the link to tensorflow.org so that it uses https instead of http, and users aren't redirected. \r\n\r\n^ _ ^", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks!\r\n\r\nJenkins, test this please.", "Thanks! \ud83d\ude04 "]}, {"number": 9202, "title": "Add support for matrix square root", "body": "Please consider adding a [matrix square root](https://en.wikipedia.org/wiki/Square_root_of_a_matrix) operation, with gradients.\r\nIt will make it possible to implement [stable whitening](https://arxiv.org/abs/1512.00809), which could be broadly useful, in addition to being useful for my particular problem :)\r\nNote, Cholesky whitening is currently supported in TensorFlow, but I'm not aware of any guarantees it provides regarding the correspondences between whitened and non-whitened data.\r\n\r\nIt appears Eigen [already has a matrix square root function](https://eigen.tuxfamily.org/dox/unsupported/group__MatrixFunctions__Module.html#matrixbase_sqrt), so this might not be too hard to implement.", "comments": ["CC @rmlarsen @agarwal-ashish in case they have any comments here.\r\n\r\nGenerally sounds like a good idea, contributions welcome.", "Has this been claimed? I am looking to make my first contribution and I think this would be a good one to get started with.\r\n\r\n@rmlarsen @agarwal-ashish @asimshankar ", "@rmlarsen @agarwal-ashish @asimshankar \r\n\r\nI was hoping to essentially redo the Eigen square root function. But, I had a quick question as a newer contributor:\r\nShould the PR only be made when the function is fully supported in all C, C++, and Python? ", "@bhairavmehta95 same question here.\r\n\r\nI'd say that you can provide the op in c++ for instance and an other contributor will add the python part...\r\nAnd the issue will be closed when all parts are implemented.", "@theflofly thanks for the response. I guess I will assign myself informally to the Python implementation, and move on from there!", "BTW, for symsqrt I've been using function below. No gradients, but will be after SVD gradients are supported as in https://github.com/tensorflow/tensorflow/issues/6503\r\n\r\n```\r\ndef symsqrt(mat, eps=1e-7):\r\n  \"\"\"Symmetric square root.\"\"\"\r\n  s, u, v = tf.svd(mat)\r\n  # sqrt is unstable around 0, just use 0 in such case\r\n  si = tf.where(tf.less(s, eps), s, tf.sqrt(s))\r\n  return u @ tf.diag(si) @ tf.transpose(v)\r\n```\r\nIf you just want to whiten data, and backprop through it, you can use Cholesky.\r\nWhitening is accomplished by using modified Cholesky Decomposition which is just a couple of arithmetic ops on top of Cholesky, here's a [mini tutorial](https://wolfr.am/lbQxdRUK)", "@yaroslavvb thanks for the mini tutorial; it looks like it should be more efficient than just computing `cholesky(inverse(covariance_matrix))`.\r\n\r\nHowever, for a dataset with dimension > 1 there are infinitely many whitening matrices, because a rotation of white data is still white.\r\nThe Cholesky approach produces just one of these possible matrices.\r\nAs mentioned in the first post, a matrix square root operation would make it possible to produce matrices with more desirable properties, e.g. a matrix which maximizes correlation between whitened and non-whitened data.", "@emchristiansen The`symsqrt` function above will produce whitening matrix of maximum trace", "BTW, this [paper](https://arxiv.org/pdf/1512.00809.pdf) talks about constraints enforced by Cholesky whitening which are indeed different from square root whitening. The `symsqrt` version above should be equivalent to matrix square root when the matrix is invertible (it's \"ZCA whitening\" in the paper)", "@bhairavmehta95 do you intend to implement this?\r\nIf not I might give it a shot or try another approach.\r\n\r\n@yaroslavvb that's the paper I linked to in the first post :)\r\n\r\n", "Is someone currently working on this issue at the moment @bhairavmehta95 @emchristiansen? I intent to start contributing to tensorflow. I have tried to figure out the roadmap to implement the matrix_sqrt feature and this is what I have come up with:\r\n\r\n- [ ] Create `matrix_sqrt_op.cc` in `tensorflow/tensorflow/core/kernels`\r\n- [ ] Add `matrix_sqrt_op` entry to `tensorflow/tensorflow/core/kernels/BUILD`\r\n- [ ] Write a test function in `tensorflow/tensorflow/core/ops/linalg_ops_test.cc`\r\n- [ ] Register op in `tensorflow/tensorflow/core/ops/linalg_ops.cc`\r\n- [ ] Add a `MatrixSqrt` entry to `tensorflow/tensorflow/core/ops/ops.pbtxt`\r\n- [ ] Define `_MatrixSqrtGrad(op, grad)` in `tensorflow/tensorflow/python/ops/linalg_grad.py`\r\n- [ ] Write a test function in `tensorflow/python/kernel_tests/linalg_grad_test.py`\r\n- [ ] Write a go wrapper in `tensorflow/go/op/wrappers.go`\r\n\r\nPlease let me know if what I have figured out is correct or if there are any changes that should be made to this plan.", "@lakshayg I'm not currently working on this, and I'd guess @bhairavmehta95 isn't working on it either. So I think it's yours if you want it :)\r\n\r\nI'm not familiar enough with the C++ side of TensorFlow to meaningfully comment on your plan, but it seems like a good start at least.\r\nIf I were you (and I'm guessing this is what you did), I'd just model the code on another unary matrix op, e.g. matrix inverse.", "It looks like over a year has passed since the last comment so I went ahead and added matrix square root op support in PR #20611. Any feedback would be much appreciated!", "Closing as this is resolved"]}, {"number": 9200, "title": "Fix build warning about comparing signed/unsigned types", "body": "This fixes a harmless (but annoying) build warning about comparing signed/unsigned types.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I have signed the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks!\r\n\r\nJenkins, test this please."]}, {"number": 9199, "title": "Improvements to ISSUE_TEMPLATE.md", "body": "Removed the asterisks from the \"must complete\" section and the shell tag because when users see this template, they will see the unformatted markdown, not the formatted version.\r\n\r\nFixed the last sentence as well.\r\n\r\nSee also https://github.com/tensorflow/models/pull/1340. Will submit an update to that issue template as well.", "comments": ["The point of the asterisks is to make it easier for the triager to see if they have added answers to the questions.", "I see, makes sense. Added them back.", "I'm very confused about this pull request. Did it not make it in properly? I don't see the changes in https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md nor in [the history](https://github.com/tensorflow/tensorflow/commits/master/ISSUE_TEMPLATE.md).", "So you can look at\r\n```\r\ngit log --graph --oneline --decorate --all\r\n```\r\nI traced a survivor `fdbeae662` and its lineage dies in `a5b1fb8e5` (aka #9289), which was a forward push.\r\nI'm hoping it was just a bad merge.\r\n@caisq did you see any merge errors?", "@nealwu @drpngx it is possible that I took only internal and ignored external when resolve the merge conflicts. Sorry if that's the case. In any case, I sent out another PR to fix it: https://github.com/tensorflow/tensorflow/pull/9385", "Thank you @caisq! As long as it's not a script issue, we're good."]}, {"number": 9198, "title": "Cherrypicks for rc2", "body": "Conflict resolved on keras merge.py if someone wants to take a quick glance over.\r\n\r\nUpdating version to 1.1.0-rc2.", "comments": ["Have we updated Release note? Can we still add the change mentioned in #7664 (change moving rnn back to core from '1.1' to '1.2')? Thanks!", "@yifeif the release md PR happens after all the tests pass. I'll send that with your included line shortly.", "@av8ramit SG, thanks!"]}, {"number": 9197, "title": "HDF5 QueueOp", "body": "I wrote this internally and got the okay to release it. There are some changes I would need to make before it is ready for general use (naming, passing in a file name, etc.), but generally it works.\r\n\r\nThe way I made it work was to have HDF5 structure like `file/group/queue_output0` so if there are many datasets, they get returned as different outputs of the queue. I can easily change this to make it more generic.\r\n\r\nIf there is interest, I can make the fixes and get it up to standard. One general concern that you might have is that I spawn std::threads inside the operation to facilitate access to the HDF5 file and I also have a FIFOQueue as a member variable so I can populate two different queues with the write and read requests.\r\n\r\nLet me know what should be changed it you would like an HDF5 reader op", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 9196, "title": "Branch 153075650", "body": "Need 152990262 for release.", "comments": []}, {"number": 9195, "title": "Enable protobuf's --with-pic option", "body": "This PR only applies to **contrib/makefile** project.\r\n\r\n`libtensorflow-core.a` static library is built with position independent code (i.e., -fPIC option), however, `libprotobuf.a` static library is not. This MR enables the protobuf's `--with-pic` option in order to be able to use `libprotobuf.a` when building a shared library.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 9194, "title": "XLA: Help understanding compute path for HLO graph", "body": "@aidan-plenert-macdonald and I are trying to figure out how the HLO graph is being passed around during xla computations. I'm moving computations over to a new device and am attempting to intercept the graph to see how it's represented, in order to replicate the structure.\r\n\r\nA list of the files where I added print statements is included below. Almost none of them print (only Registrar initialization in hlo_graph_dumper.cc, the Transfer functions in client.cc and service.cc, and the CpuCompiler and XpuCompiler initializations), and I've added statements in almost every function in the attached files.\r\n\r\nWhere is the HLO graph being assembled and dumped? How do I access the HLO graph?\r\n\r\nNote: the 'xpu' folder/files are for my new device - they're replicas of the 'cpu' folder/files in compiler/xla/service, with all mentions of 'cpu' changed to 'xpu'. List of files with print statements:\r\n```\r\n./compiler/aot/compile.cc\r\n./compiler/jit/encapsulate_subgraphs_pass.cc\r\n./compiler/jit/mark_for_compilation_pass.cc\r\n./compiler/tf2xla/kernels/batch_matmul_op.cc\r\n./compiler/tf2xla/kernels/gather_op.cc\r\n./compiler/tf2xla/xla_compiler.cc\r\n./compiler/xla/client/client.cc\r\n./compiler/xla/service/cpu/cpu_compiler.cc\r\n./compiler/xla/service/hlo_computation.cc\r\n./compiler/xla/service/hlo_graph_dumper.cc\r\n./compiler/xla/service/layout_assignment.cc\r\n./compiler/xla/service/service.cc\r\n./compiler/xla/service/user_computation.cc\r\n./compiler/xla/service/xpu/xpu_compiler.cc\r\n./compiler/xla/service/xpu/xpu_executable.cc\r\n./compiler/xla/tests/hlo_test_base.cc\r\n./core/common_runtime/function.cc\r\n./core/common_runtime/graph_optimizer.cc\r\n```", "comments": ["You might be able to trace the HLO graph dumping from the handling of the [`TF_XLA_FLAGS`](https://github.com/tensorflow/tensorflow/blob/f94e20f2c704f037a113d350fd7c431e3fd75df8/tensorflow/compiler/xla/legacy_flags/parse_flags_from_env.h#L20) environment variable and the [`xla_dump_*`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/legacy_flags/service_flags.cc#L75) command-line flags.\r\n\r\nAs XLA is in early stages, documentation (and indeed the implementation) of these details is not something that is finalized and thus easily accessible yet. \r\n\r\nDoes that help?\r\n\r\n(CC @tatatodd @eliben in case they have something to add)", "I'm running with TF_XLA_FLAGS=--xla_generate_hlo_graph=.* normally. I just ran with TF_XLA_FLAGS=--xla_dump_* (if that's what you meant) and nothing changed", "We have a not terribly well publicized Google group for developers working on the XLA implementation, you may wish to post there:\r\nhttps://groups.google.com/forum/#!forum/xla-dev\r\n\r\nOne possibility is you aren't actually using XLA at all. Can you show the Python code you are attempting to run? How are you enabling XLA?\r\n\r\nIn general, you can get out HLO graphs by the environment variable TF_XLA_FLAGS=--xla_generate_hlo_graph=.* . \r\n\r\nIf you are using the Tensorflow LOG() macros to print things, you also need to make sure logging is enabled. There are two more environment variables TF_CPP_MIN_LOG_LEVEL and TF_CPP_MIN_VLOG_LEVEL that control that.", "The larger point is that we  can't seem to exercise the compiler at all.  Here is what happenes when I  reproduce this with the mnist_softmax_xla.py file from the TensorFlow tutorial:\r\n\r\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py\r\n\r\nI should expect this to break at CpuCompiler::Compile, and have checked via printing that line 63 of the python script, which sets ON_1, is executed.\r\n\r\n``\r\nrdubielzig@swpl-000224:~/tflow$ lldb -- python3 mnist_softmax_xla.py \r\n(lldb) target create \"python3\"\r\nCurrent executable set to 'python3' (x86_64).\r\n(lldb) settings set -- target.run-args  \"mnist_softmax_xla.py\"\r\n(lldb) b CpuCompiler::Compile\r\nBreakpoint 1: no locations (pending).\r\nWARNING:  Unable to resolve breakpoint to any actual locations.\r\n(lldb) r\r\nProcess 10331 launched: '/usr/bin/python3' (x86_64)\r\n2 locations added to breakpoint 1\r\nProcess 10331 stopped and restarted: thread 1 received signal: SIGCHLD\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n2017-04-13 15:35:52.612498: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-04-13 15:35:52.616628: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x2b38780 executing computations on platform Host. Devices:\r\n2017-04-13 15:35:52.616642: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\r\n0.9195\r\nProcess 10331 exited with status = 0 (0x00000000)\r\n``", "Thanks! I'll check out the group.\r\n\r\nI'm fairly sure I am using XLA, the code I'm running is modified/simplified from the mnist_softmax_xla example:\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport sys \r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.python.client import timeline\r\n\r\nFLAGS = None\r\n\r\n\r\ndef main(_):\r\n    config = tf.ConfigProto(log_device_placement=True)\r\n    jit_level = 0 \r\n    if FLAGS.xla:\r\n        # Turns on XLA JIT compilation.\r\n        jit_level = tf.OptimizerOptions.ON_1\r\n        print('XLA flag on')\r\n\r\n    config.graph_options.optimizer_options.global_jit_level = jit_level\r\n    run_metadata = tf.RunMetadata()\r\n    # Creates a session with log_device_placement set to True.\r\n        # Creates a graph.\r\n    with tf.device('/job:localhost/replica:0/task:0/device:XLA_XPU:0'):\r\n        with tf.Session(config=config) as sess:\r\n       # with tf.device('/device:CPU:0'):\r\n            a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], \r\n                            shape=[2, 3], name='a')\r\n\r\n            b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], \r\n                            shape=[3, 2], name='b')\r\n            c = tf.square(tf.tanh(tf.matmul(a, b)))\r\n        # Runs the op.\r\n        print(sess.run(c, \r\n              options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\r\n              run_metadata=run_metadata))\r\n        trace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n        with open('timeline.ctf.json', 'w') as trace_file:\r\n            trace_file.write(trace.generate_chrome_trace_format())\r\n            trace_file.flush()\r\n        sess.close()\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--data_dir', type=str, \r\n            default='/tmp/tensorflow/mnist/input_data',\r\n            help='Directory for storing input data')\r\n    parser.add_argument(\r\n              '--xla', type=bool, default=True, help='Turn xla via JIT on')\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\nI also have manually gone into the code and set VLOG_IS_ON to always return true (that seems to deal with the MIN_LOG_LEVEL issues)", "The text at the top of the group suggests that the question is better suited for stackoverflow, so that is where I have posted the question:\r\n\r\nhttp://stackoverflow.com/questions/43403089/how-to-exercise-xla-compiler", "@lwogulis : `xla_dump_graph*` was meant to refer to a set of flags, like `xla_dump_hlo_text_to` etc. (see the link in my previous comment: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/legacy_flags/service_flags.cc#L75)\r\n\r\nClosing this issue out since it seems that the xla-dev@ mailing list would be a more appropriate forum for XLA (which is still \"alpha/experimental\") specific questions.", "Ah! I see your problem. We don't think the CPU JIT is ready for wide use yet, so it does not get enabled globally via OptimizerOptions. (The *GPU* JIT can be enabled via OptimizerOptions.)\r\n\r\nTry marking some operators to compile using the \"manual\" approach described here:\r\nhttps://www.tensorflow.org/performance/xla/jit", "Have you succeeded in moving computations over to a new device by XLA? Could you please tell me about your method in detail? @lwogulis ", "@yuchen112358 Unfortunately, neither @lwogulis nor I work at that company anymore. We did manage to get the computation to run on a new \"virtual\" device (new bindings to the CPU). As far as I am aware, XLA has yet to be mapped to the device. We have normal TF running on the device. @RichDubielzig is the current lead "]}, {"number": 9193, "title": "Support int32 idxs in sparse_tensor_dense_matmul", "body": "The `sparse_tensor_dense_mat_mul` operator requires indices to be stored in 64bit precision, which can end up consuming a lot of memory for large arrays.  This adds support for 32bit indices.\r\n\r\nNote that I only changed `gen_sparse_ops._sparse_tensor_dense_mat_mul`, not `sparse_ops.sparse_tensor_dense_matmul`.  This makes this change fairly invisible, only accessible to those who know to look for it.  Changing the public facing function would require changing `SparseTensor` to accept 32bit indices, which might be a good thing to do but would be a much larger change.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Pushed an update that I think should fix that backwards compatibility error.", "Wait what broke the backwards compat?\r\n\r\nJenkins, test this please.", "It was because the new attribute I added (Tindices) didn't have a default value (this line https://github.com/tensorflow/tensorflow/pull/9193/commits/7adcd0d2dba35e7802591a64e8d025e19f951668#diff-ac07c69dd7c613a69fab85a90cae89f7R137)", "SG.\r\n\r\n@vrv checking if there are any concerns with this change.", "seems reasonable to me since it's backwards compatible and doesn't deal with the potential mess of SparseTensor casting :)"]}, {"number": 9192, "title": "Revert \"NADAM Optimizer (#8405)\"", "body": "We need to put this in contrib instead.\r\n\r\nThis reverts commit 86d3891ba6612552347beaabe27edac11f5758d7.", "comments": ["/cc: @martinwicke @josh11b ", "/cc: @louiehelm ", "Is there a new PR with this moved to contrib?", "No, we're considering a merge with https://github.com/tensorflow/tensorflow/pull/9175 for cleaner code.\r\n"]}, {"number": 9191, "title": "TensorFlow crashing when batching audio", "body": "I have tried to create an audio processing model using tensorflow.contrib.ffmpeg. The code I wrote consistently crashes the python process on my OS X. I have provided both the code and a piece of crash report on this stackoverflow question. \r\n\r\nhttp://stackoverflow.com/questions/43377986/batching-audio-data-in-tensorflow/\r\n\r\nIs this a bug or am I doing something wrong?", "comments": ["Please see issue tracker policy posted [here](https://github.com/tensorflow/tensorflow/issues/new)."]}, {"number": 9190, "title": "SpaceToBatchND / BatchToSpaceND Documentation Error", "body": "\r\nI think there is an error in the *documentation* for SpaceToBatchND and BatchToSpaceND.  (The op itself seems to be working fine.)\r\n\r\nIn example (3) from SpaceToBatchND:\r\n\r\n> (3) For the following input of shape [1, 4, 4, 1], block_shape = [2, 2], and paddings = [[0, 0], [0, 0]]:\r\n> \r\n> ```prettyprint x = [[[[1], [2], [3], [4]], [[5], [6], [7], [8]], [[9], [10], [11], [12]], [[13], [14], [15], [16]]]] ```\r\n> \r\n> The output tensor has shape [4, 2, 2, 1] and value:\r\n> \r\n> ```prettyprint x = [[[[1], [3]], [[5], [7]]], [[[2], [4]], [[10], [12]]], [[[5], [7]], [[13], [15]]], [[[6], [8]], [[14], [16]]]] ```\r\n\r\nNote how [[5],[7]] appears twice in the result and [[9],[11]] is missing.\r\n\r\nThe example (3) in BatchToSpaceND has the same problem in reverse.\r\n\r\nIf I actually try this op in TensorFlow, I get results that differ from the documentation but make sense to me:\r\n\r\n```\r\n\r\n>>> bshape = [2,2]\r\n>>> paddings = [[0,0],[0,0]]\r\n>>> x  = [[[[1], [2], [3], [4]], [[5], [6], [7], [8]], [[9], [10], [11], [12]], [[13], [14], [15], [16]]]]\r\n>>> s2btest = tf.space_to_batch_nd(x,bshape,paddings)\r\n>>> sess.run(s2btest)\r\narray([[[[ 1],\r\n         [ 3]],\r\n\r\n        [[ 9],\r\n         [11]]],\r\n\r\n\r\n       [[[ 2],\r\n         [ 4]],\r\n\r\n        [[10],\r\n         [12]]],\r\n\r\n\r\n       [[[ 5],\r\n         [ 7]],\r\n\r\n        [[13],\r\n         [15]]],\r\n\r\n\r\n       [[[ 6],\r\n         [ 8]],\r\n\r\n        [[14],\r\n         [16]]]], dtype=int32)\r\n\r\n```\r\n\r\n\r\nExample tried with TensorFlow from source / Linux Ubuntu Xenial / v1.1.0-rc1-168-g0054c39 1.1.0-rc1\r\n.\r\n\r\nDocumentation from https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/batch-to-space-n-d and https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/space-to-batch-n-d \r\n\r\nLooking at the doc string this may be fixed in master (d1ba01f8) and just not updated on the 1.0 branch web docs yet.\r\n\r\n", "comments": ["Thanks for pointing this out. As you found out, this was fixed in #7080 and will be reflected in the next release (you can preview the docs for 1.1 that show this fix at https://www.tensorflow.org/versions/r1.1/api_docs/cc/class/tensorflow/ops/space-to-batch-n-d)\r\n\r\nClosing this out, since this has been fixed - just not reflected to the website yet, which will be soon."]}, {"number": 9189, "title": "A replacement for tf.cond", "body": "This is a prototype for https://github.com/tensorflow/tensorflow/issues/9188.\r\n\r\nTwo new ops, `Mux` and `Demux` are introduced by this patch. `Mux` op takes `[i, input_0, input_1, ...]` as input, and produces `input_i` as output. It is meant to replace `tf.case` and `tf.cond`. `Demux` op is a generalized version of `Switch`. It takes `[i, input]` as input, and produces `[output_0, ..., output_N]` where `output_i` is set to `input` and other outputs are dead.\r\n\r\nThis patch also contains an algorithm that rewrites the graph before execution so that `Mux` ops are efficient just like `tf.cond`.\r\n\r\nExample:\r\n\r\n```\r\ni = tf.placeholder(tf.int32, [])\r\na = tf.placeholder(tf.int32, [])\r\nb = tf.placeholder(tf.int32, [])\r\nc = tf.select(i, [a+1, b*2]) # Mux op is renamed to select in python api.\r\n\r\nsess.run(c, feed_dict={i: 0, a: 1, b: 2}) # returns 2, trace to see that \"Mul\" node is not executed.\r\nsess.run(c, feed_dict={i: 1, a: 1, b: 2}) # returns 4, trace to see that \"Add\" node is not executed.\r\n```\r\n\r\nI wouldn't be surprised if this patch causes problem somewhere. In particular, the graph rewriting algorithm may produce valid but cyclic graph. It also lacks proper handling of special nodes like `Enter` `NextIteration` etc. However, I would like to know if this approach is promising. I can work on this if there is no fundamental difficulty.", "comments": ["Can one of the admins verify this patch?", "@sherrym @yuanbyu any thoughts on this?\r\n\r\n@gaohuazuo we should probably try this in contrib first.", "This seems really cool, I'd be happy to see something like this in TensorFlow.", "Jenkins, test this please.", "The approach looks interesting. One area that you probably need to pay some attention to is stateful ops. One good test is to replace all the conds in third_party/tensorflow/python/kernel_tests/control_flow_ops_py_test.py with yours and make sure all the tests still pass.", "@gaohuazuo friendly ping!  This is indeed a very interesting change that we'd love to see more work on!", "(My suggestion would be to leave the C++ op in core, but have the 'select' API in contrib)", "I'm working on it. I will push to github once I have checked all tests in `control_flow_ops_py_test`.\r\n\r\nProgress:\r\nThe graph rewriting algorithm now play nice with while loops. However, optimization does not cross loop or iteration boundary.\r\n\r\nTODOs:\r\nCheck all tests in `control_flow_ops_py_test`.\r\nImplement gradient.\r\nPython api improvements (move to contrib, add a drop-in replacement for `tf.cond`, docs).\r\n", "## `control_flow_ops_py_test` Result\r\n\r\n`bazel test -c dbg --config cuda //tensorflow/python/kernel_tests:control_flow_ops_py_test`\r\n\r\n* <del>testCondGrad_1</del>\r\n    <del>`LookupError: No gradient defined for operation 'select/Mux' (op type: Mux)`</del>\r\n\r\n* <del>testCondGrad_2</del>\r\n    <del>`LookupError: No gradient defined for operation 'select/Mux' (op type: Mux)`</del>\r\n\r\n* <del>testCondGrad_Gather</del>\r\n    <del>`LookupError: No gradient defined for operation 'select/Mux' (op type: Mux)`</del>\r\n\r\n* <del>testCondIndexedSlices</del>\r\n    <del>`ValueError: Tensor conversion requested for IndexedSlices without dense_shape: IndexedSlices(indices=Tensor(\"Const_1:0\", shape=(), dtype=int32, device=/device:CPU:0), values=Tensor(\"Add:0\", shape=(), dtype=int32, device=/device:CPU:0))`</del>\r\n\r\n* testCondIndexedSlicesDifferentTypes\r\n    <del>`ValueError: Tensor conversion requested for IndexedSlices without dense_shape: IndexedSlices(indices=Tensor(\"one:0\", shape=(), dtype=int32, device=/device:CPU:0), values=Tensor(\"Add:0\", shape=(), dtype=int32, device=/device:CPU:0))`</del>\r\n\r\n    `TypeError: Tensors in list passed to 'inputs' of 'Mux' Op have types [int64, int32] that don't all match.`\r\n\r\n* <del>testCondRef</del>\r\n    <del>`ValueError: Input tensors must have the same dtype.`</del>\r\n\r\n* <del>testCondSparseTensor</del>\r\n    <del>`TypeError: Expected binary or unicode string, got <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fa52815a898>`</del>\r\n\r\n* <del>testCondWhile_3</del>\r\n    <del>`LookupError: No gradient defined for operation 'select/Mux' (op type: Mux)`</del>\r\n\r\n* <del>testNestedCond_Simple</del>\r\n    <del>`LookupError: No gradient defined for operation 'select_1/Mux' (op type: Mux)`</del>\r\n\r\n* <del>testNestedWhileCondWhileGrad</del>\r\n    <del>`LookupError: No gradient defined for operation 'while/select/Mux' (op type: Mux)`</del>\r\n\r\n* <del>testWhileCondGrad_Simple</del>\r\n    <del>`LookupError: No gradient defined for operation 'while/select/Mux' (op type: Mux)`</del>\r\n\r\n* <del>testWhileCondGrad_UnknownShape</del>\r\n    <del>`LookupError: No gradient defined for operation 'while/select/Mux' (op type: Mux)`</del>\r\n\r\n* <del>testWhileGrad_Square</del>\r\n    <del>`LookupError: No gradient defined for operation 'select/Mux' (op type: Mux)`</del>\r\n\r\n* testCond_4\r\n    ```\r\n    AssertionError: Tuples differ: () != (0,)\r\n\r\n    Second tuple contains 1 additional elements.\r\n    First extra element 0:\r\n    0\r\n\r\n    - ()\r\n    + (0,) : Shape mismatch: expected (), got (0,).\r\n    ```\r\n\r\n### Summary\r\n\r\n* <del>`tf.select` and `tf.cond_v2` does not handle `IndexedSlice` and `SparseTensor` properly.</del>\r\n    <del>Accounts for `testCondIndexedSlices`,`testCondIndexedSlicesDifferentTypes` and `testCondSparseTensor`.</del>\r\n   <del>I am not very familiar with `IndexedSlice` and `SparseTensor`, so I need to study further to provide a fix.</del>\r\n\r\n* <del>`tf.select` does not handle ref dtypes properly.</del>\r\n   <del>Accounts for `testCondRef`.</del>\r\n   <del>Possible fix would be adding a `RefMux` op and some handling in python `select` function.</del>\r\n\r\n* When an input is an `tf.Operation`, `tf.cond` produces a scalar bool tensor whereas `tf.cond_v2` produces an empty float tensor. This is just a minor change and any sane application should not rely on it.\r\n    Accounts for `testCond_4`.\r\n\r\n* <del>`Mux` op has no gradient.</del>\r\n    <del>Accounts for other failed tests.</del>\r\n   <del>Possible fix would be adding a gradient function or modifying `tf.gradients`. Simply registering a gradient function should work, but is less efficient.</del>\r\n\r\n### Some questions\r\n\r\n* Some op kernels, `Merge` for example, require that all int32 inputs reside in host memory. Why? Should I do the same?\r\n\r\n* Which contrib module should I add `select` and `cond` to? Or should I create a new contrib module?", "@gaohuazuo I can't answer your questions, but thanks a lot for putting in all this effort!", "@yuanbyu  @vrv `cond_v2` has passed almost all tests in `ControlFlowTest` (see previous comment and https://github.com/tensorflow/tensorflow/pull/9189/commits/2d882220c3ccaf161e9d243a382121b030b762cf). I will complete docs and code comments soon. What else has to be done before I may ask for review? Additionally, which contrib module should I use?\r\n\r\nNotes on gradient:\r\nThe current naive approach is to register a gradient function for `Mux`, as if it were an ordinary op. It works, but the optimization that is enjoyed by `tf.cond` is missing. Even so, it is already more efficient in some scenarios, for example `y = do_something(tf.cond(tf.constant(False), lambda: x1, lambda: x2)); tf.gradients(y, x1)[0].eval()`.", "Can one of the admins verify this patch?", "@yuanbyu could you provide some guidance for the author here?", "Regarding questions from before:\r\n\r\n1. \"What else has to be done before I may ask for review?\"\r\n\r\nI think as long as the functionality is complete enough, with the tests, then we can start a review of the code.  Some things may come up during that review.\r\n\r\n2. \"Additionally, which contrib module should I use?\"\r\n\r\nDoes tf.contrib.control_flow seem okay?\r\n\r\n3. \"Some op kernels, Merge for example, require that all int32 inputs reside in host memory. Why? Should I do the same?\"\r\n\r\nSome background: Some operations are commonly used for operations on shape tensors, which tend to be int32 (although int64 is possible).  If those operations have int32 GPU kernels registered which put the memory on device, then placement of ops to devices will put them on GPU, but it's better to avoid the bouncing back-and-forth from device-to-host for those operations.  Currently, this is avoided by having the int32 GPU kernels request HostMemory for the inputs and outputs; iiuc, this mainly works because int32 is not used often for non-shape tensors on GPU.  So I would guess that your ops should match their switch/merge analogs with this behavior.", "Looks interesting!\r\n\r\n> Mux op is renamed to select in python api.\r\n\r\nI like `select`, it is a much better name! Could select or an incremented equivalent be considered for the C++ side as well?", "@gaohuazuo - Please let me know when I should review the code (I can review now, or wait for soem more moves and updates)", "@cwhipkey Thanks for the reply! I am planning to complete the comments and clean up the code. I don't think this PR in its current state is ready for merge. However, I am on a full time intern now, so it is going to take some time. I will update this thread when it is done.", "@ahundt There is a `RefSelect` op in C++. Although `RefSelect` do the same thing, I think it would be safer to use another name.", "> @ahundt There is a RefSelect op in C++. Although RefSelect do the same thing, I think it would be safer to use another name.\r\n\r\n@gaohuazuo Can the existing RefSelect be extended to work in both cases in a clear and concise manner that also has backwards compatibility?\r\n\r\nConsistent and clear naming is one of the most important things for users to make sense of and use the API, so please consider giving some additional thought on what naming approach will be clear between python, and C++, and RefSelect. :-)\r\n", "@gaohuazuo could you resolve conflicts and respond to comments?\r\n\r\nThanks\r\n", "Any updates?", "@vrv I think I won't have the spare time until October.", ":( I'm sorry to hear that but perhaps someone else will pick this up sooner, or it will be waiting for you otherwise.  Thanks for your effort, and I hope you learned something in the process too :)", "I'm curious as to the motivation for this change. Referenced issue talks about `tf.cond` awkwardness -- things need to be wrapped into a function. But this awkwardness can be fixed on the client level using existing Merge/Switch primitives. IE, a smarter `tf.cond` replacement could instead take existing TensorFlow tensors and ensure laziness by inserting Merge/Switch nodes into the graph like [here](https://gist.github.com/yaroslavvb/d67410e240369736fc4ba0267250ef27)\r\n", "@yaroslavvb I feel that the current `tf.cond` adds an unnecessary mechanism to the computational graph formulation. This pr tries to make `tf.cond` an ordinary op.", "Ah so it's for simplification, makes sense. Yeah, the amount of Python logic in existing tf.cond/tf.case ops is a bit scary"]}, {"number": 9188, "title": "Improving tf.cond", "body": "Currently `tf.cond` requires a function for each branch, which makes no difference semantically but serves as an optimization hint. Such requirement becomes quite inconvenient in some cases, for example,\r\n\r\n```\r\ndef build_graph(p1, p2):\r\n    a = a_fn() # Redundant when p1 and p2 are both false.\r\n    b1 = tf.cond(p1, lambda: a, b1_fn)\r\n    b2 = tf.cond(p2, lambda: a, b2_fn)\r\n    return b1, b2\r\n\r\n# An alternative\r\ndef build_graph(p1, p2):\r\n    # Redundant computation when p1 and p2 are both true.\r\n    b1 = tf.cond(p1, a_fn, b1_fn)\r\n    b2 = tf.cond(p2, a_fn, b2_fn)\r\n    return b1, b2\r\n```\r\n\r\nThough possible, it is not so straightforward to transform the above graph into an efficient one. In fact, we don't really need the optimization hint. Once target nodes are known, the exact execution condition for each node can be readily calculated. Furthermore, these conditions can be implemented using existing `Switch` and `Merge` ops. I have made a prototype to illustrate the idea at https://github.com/tensorflow/tensorflow/pull/9189.", "comments": ["Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 9187, "title": "README.md has a deprecated API call", "body": "### System Information\r\nTensorflow ('v1.0.0-65-g4763edf-dirty', '1.0.1')\r\n\r\n\r\n### Describe the problem clearly\r\nIn README.md line `predictions = vgg.vgg16(images, is_training=True)` should be `predictions = vgg.vgg_16(images, is_training=True)` (see [source code for vgg](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py)). \r\n", "comments": ["Thanks for bringing this to our attention. Did you notice any other deprecated parts of the README by chance?", "I've also come upon [this issue on SO](http://stackoverflow.com/questions/41808353/tensorflow-slim-module-object-has-no-attribute-sum-of-squares) while searching for fix for vgg.\r\n\r\nI believe `sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)\r\n` is still in README despite being deprecated, but I haven't checked it myself.\r\n\r\nI hope this helps."]}, {"number": 9186, "title": "TFRecordReader num_records_produced() and num_work_units_completed() do not work", "body": "Hey guys, since I've been dealing a lot with TFRecord files lately, I stumbled upon the following bug:\r\n\r\n```\r\nfile_queue = init_queue(dataset)\r\n\r\nrecords_reader = tf.TFRecordReader(name='...')\r\n\r\n_, record = records_reader.read(file_queue, name='...')\r\n\r\n #do something with record in a while loop\r\n```\r\n\r\n\r\n\r\n\r\nAnd when I call the two aforementioned functions they always return 0. I am sure I have used the records since I have seen the end results dumped to an excel spreadsheet.\r\n\r\nCheers, Kris\r\n\r\n", "comments": ["Can it be so that within a while loop one and the same parsed record gets used and this does not trigger queues? \r\n\r\nEDIT: consider this simple example: \r\n\r\n```\r\nl = [1,2,324,3,12,1,2,3]\r\nq = tf.train.input_producer(l, shuffle=False)\r\nx = q.dequeue()\r\n\r\nt = tf.TensorArray(dtype=tf.int32, size=5, dynamic_size=True, clear_after_read=True)\r\n\r\n_, t = tf.while_loop(cond=lambda i, a: tf.less(i, 5, name='less_op'),\r\nbody=lambda i, a: [i+1, a.write(i, [x])],\r\nloop_vars=[0, t])\r\n```\r\n\r\nit outputs [1 1 1 1 1], since the dequeue() is called only once. Please tell me how to trigger the dequeue operation on each iteration *before deleting this github thread*.\r\n\r\nThank you!\r\n\r\nCheers, Kris\r\n\r\n", "mrry already answered on stackoverflow. Many thanks and this thread can be closed now\r\n"]}, {"number": 9185, "title": "Windows: //tensorflow/python/estimator:estimator_test failing in Bazel build", "body": "http://ci.tensorflow.org/job/tf-master-win-bzl/751/consoleFull\r\n\r\nIt has been failing on ci for a while with:\r\n```\r\n22:36:33 ======================================================================\r\n22:36:33 ERROR: test_train_save_copy_reload (__main__.EstimatorTrainTest)\r\n22:36:33 ----------------------------------------------------------------------\r\n22:36:33 Traceback (most recent call last):\r\n22:36:33   File \"\\\\?\\c:\\tmp\\Bazel.runfiles_r2z2r52c\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\estimator\\estimator_test.py\", line 267, in test_train_save_copy_reload\r\n22:36:33     os.renames(model_dir1, model_dir2)\r\n22:36:33   File \"C:\\Program Files\\Anaconda3\\lib\\os.py\", line 288, in renames\r\n22:36:33     rename(old, new)\r\n22:36:33 PermissionError: [WinError 5] Access is denied: 'c:\\\\tmp\\\\tmp8f7qnomv\\\\model_dir1' -> 'c:\\\\tmp\\\\tmp8f7qnomv\\\\model_dir2'\r\n```\r\n@gunan Can we fix this test case on Windows? Otherwise we'd better disable this test on Windows.", "comments": ["Friendly ping @mrry re: state of Windows Jenkins build.", "Looks like the author of the broken test isn't on GitHub, so reassigning to @martinwicke as overall owner for the Estimator code.", "Is there something special about mkdtmp on Windows? Seems we don't have permissions to rename a file in such a directory?", "I think the main pitfall here is failing to close all of the files in the directory before trying to move it. We had various tests that used to be sloppy about closing temp files before opening them elsewhere... perhaps the Estimator code does this somewhere?"]}, {"number": 9184, "title": "tensorflow 1.0.1 + Python3 - TypeError: string argument expected, got 'NoneType'", "body": "Hi all,\r\nI am trying to install tensorflow 1.0.1 from source on RHEL 6.6 with CUDA 7.5 & CUDNN 5.0.5.\r\nI am repeatedly getting following error-\r\n\r\n$ pip3 install ./tensorflow-1.0.1-cp36-cp36m-linux_x86_64.whl --root=\"$INSTALL_ROOT_DIR\" \r\nProcessing ./tensorflow-1.0.1-cp36-cp36m-linux_x86_64.whl\r\nRequirement already satisfied: protobuf>=3.1.0 in /home/apps/PYTHONPACKAGES/3.6.0/ucs4/gnu/493/PROTOBUF/3.2.0/gnu/lib/python3.6/site-packages/protobuf-3.2.0-py3.6.egg (from tensorflow==1.0.1)\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/site-packages/pip/commands/install.py\", line 335, in run\r\n    wb.build(autobuilding=True)\r\n  File \"/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/site-packages/pip/wheel.py\", line 749, in build\r\n    self.requirement_set.prepare_files(self.finder)\r\n  File \"/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/site-packages/pip/req/req_set.py\", line 380, in prepare_files\r\n    ignore_dependencies=self.ignore_dependencies))\r\n  File \"/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/site-packages/pip/req/req_set.py\", line 666, in _prepare_file\r\n    check_dist_requires_python(dist)\r\n  File \"/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/site-packages/pip/utils/packaging.py\", line 48, in check_dist_requires_python\r\n    feed_parser.feed(metadata)\r\n  File \"/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/email/feedparser.py\", line 175, in feed\r\n    self._input.push(data)\r\n  File \"/home/soft/PYTHON/3.6.0/ucs4/gnu/447/lib/python3.6/email/feedparser.py\", line 103, in push\r\n    self._partial.write(data)\r\nTypeError: string argument expected, got 'NoneType'\r\n\r\n\r\nPlease let me know if any further information is required from my side to debug this issue.\r\nThough, I  have previously compiled tf-1.0.1 using python 2.7, but currently, I have to stick to python3 for this installation.\r\n", "comments": ["As requested in the new issue template, please include the exact command if possible to produce  the output included in your test case.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Hi,\r\nI have shared the exact error causing command and complete error log. \r\nDo i need to share the build log for \"tensorflow-1.0.1-cp36-cp36m-linux_x86_64.whl\" ?", "Unfortunately we don't support redhat 6 or python 3.5. We support centos/redhat 7 and python 3.5 only.", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}]