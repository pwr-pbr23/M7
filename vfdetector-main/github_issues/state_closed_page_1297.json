[{"number": 14205, "title": "TensorFlow r1.4 does not be compiled from 32bit environment.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: None\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Slackware 14.2 32bit\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.4\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 5.3.0\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package and set optimization flags to -march=i686\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nInstalling TensorFlow r1.4 on a Linux system with 32bit kernel does not work. I think the error comes from the configuration of the nsync, which does not care 32bit environment. \r\n\r\nI googled it, and found a workaround.\r\n[https://lengerrong.blogspot.kr/2017/09/fix-up-configurable-attribute-copts.html](https://lengerrong.blogspot.kr/2017/09/fix-up-configurable-attribute-copts.html)\r\n\r\nand added \r\n\r\n`\"//conditions\"default\": []'`\r\n\r\nto the appropriate place.\r\n\r\nHowever, I think that it will be much better if the official configure system of the TensorFlow supports the 32bit system.\r\n\r\nThanks.\r\nSungjin.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nNone.\r\n", "comments": ["Unfortunately, we don't have the resources to support a very large variety of platforms. As per https://www.tensorflow.org/install/ - the TensorFlow maintainers currently support only 64-bit platforms, with some limited support for 32-bit builds for ARM.\r\n\r\nWe rely on the community to support other configurations.\r\nIf you have a fix, we'd be happy to review and accept a pull request to fix the problem.\r\n\r\nThanks!", "Hi @daisylab ,Please follow instructions for installing Tensorflow from [here](https://www.tensorflow.org/install).\r\nWe also  see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/14205\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/14205\">No</a>\n"]}, {"number": 14204, "title": "Fix typos", "body": "This PR fixes some typos: `a same`, `specifed`, `signalled`, and `compatiblity`.", "comments": ["Can one of the admins verify this patch?", "it seems nice\r\n\r\nnice job", "Jenkins, test this please."]}, {"number": 14203, "title": "Convert keras model to estimator model", "body": "ValueError: ('Expected `model` argument to be a `Model` instance, got ', <keras.engine.training.Model object at 0x00000230F36F0E10>)\r\n", "comments": ["Also,I wonder how can I set some parms like epoch,batch_size,ckpt saving step...etc,Thanks a lot", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks, in particular detailed instructions to reproduce the problem. Thank you.", "@asimshankar can we please reopen this issue, I've encountered it as well and here's the details:\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: followed example code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macos 10.13.1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```\r\nfrom keras_contrib.applications.densenet import DenseNetImageNet161\r\nimport keras.optimizers\r\nimport tensorflow as tf\r\n\r\ntarget_img_size = (224, 224, 3)\r\nmodel = DenseNetImageNet161(input_shape=target_img_size)\r\nmodel.compile(optimizer=keras.optimizers.Adam(lr=1e-4), loss='categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(keras_model=model, model_dir='/path/to/dir')\r\n```\r\n\r\n### Describe the problem\r\nWhen I run the above script, it gives the error that @KirtoXX mentioned (see traceback below).\r\n\r\nIt seems to be [this exact issue referenced in keras](https://github.com/keras-team/keras/issues/9310). It seems that if the model was built with standard keras, then model_to_estimator() can't really be used, which would make model_to_estimator() not very helpful for using many pre-trained models.\r\n\r\n### Source code / logs\r\n...\r\nUsing TensorFlow backend.\r\n2018-03-09 00:46:36.080703: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nWeights for the model were loaded successfully\r\nTraceback (most recent call last):\r\n  File \"/Users/siyuyang/Source/Repos/AI4E_IARPAfMoW/scripts/temp.py\", line 15, in <module>\r\n    estimator = tf.keras.estimator.model_to_estimator(keras_model=model, model_dir='/path/to/dir')\r\n  File \"/Users/siyuyang/Source/Repos/AI4E_IARPAfMoW/env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 302, in model_to_estimator\r\n    _save_first_checkpoint(keras_model, est, custom_objects, keras_weights)\r\n  File \"/Users/siyuyang/Source/Repos/AI4E_IARPAfMoW/env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 231, in _save_first_checkpoint\r\n    custom_objects)\r\n  File \"/Users/siyuyang/Source/Repos/AI4E_IARPAfMoW/env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/estimator.py\", line 111, in _clone_and_build_model\r\n    model = models.clone_model(keras_model, input_tensors=input_tensors)\r\n  File \"/Users/siyuyang/Source/Repos/AI4E_IARPAfMoW/env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py\", line 1557, in clone_model\r\n    return _clone_functional_model(model, input_tensors=input_tensors)\r\n  File \"/Users/siyuyang/Source/Repos/AI4E_IARPAfMoW/env/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py\", line 1361, in _clone_functional_model\r\n    'to be a `Model` instance, got ', model)\r\nValueError: ('Expected `model` argument to be a `Model` instance, got ', <keras.engine.training.Model object at 0x10e616588>)", "I don't think there is an intention to have `model_to_estimator` work with `keras.Model`, it requires `tf.keras.Model`. However, since `tf.keras` implements the Keras API spec, you should be able to create the same using `tf.keras.applications`.\r\n\r\nThat said, I could be wrong, so CCing @fchollet and @anj-s for their input.", "Same problem here."]}, {"number": 14202, "title": "Fails to optimize MultivariateNormalFullCovariance. It breaks on Cholesky decomposition . I have cherry picked the mentioned commit to update gradient issue in  MultivariateNormalFullCovariance. Still it fails in optimization", "body": "I am trying to optimize a distribution with Mu=Nx4 and covariance matrix = Nx4x4 using MultivariateNormalFullCovariance. The optimization runs for few iterations, loss seems to be reducing (gradients are not exploding !). The code breaks with following   \r\n\r\nInvalidArgumentError (see above for traceback): Got info = 4 for batch index 0, expected info = 0. Debug_info =potrf\r\n\r\n[[Node: MultivariateNormalFullCovariance/init/Cholesky = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape_2)]]\r\n\t [[Node: gradients/AddN_30/_49 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_3080_gradients/AddN_30\", tensor_type=DT_FL\r\n\r\nI have debugged,  analyzing the covarince,pdf and gradients they all seem to be alright(giving expecting values). Suddenly at one iteration code breaks.", "comments": ["@ebrevdo can you please take a look at this issue? Thanks.\r\n", "If this is not relevant, I am sorry but I have a problem using MultivariateNormalFullCovariance in optimization, especially when I set covariance as Variable, Visual Code crashes as shown below.\r\n\r\nhttps://stackoverflow.com/questions/47182741/tensorflow-optimize-to-bring-two-multivariate-close-to-each-other-crashes", "Is it possible your optimizer is generating a non positive definite covariance?  Consider mvnchol and feeding your covariance variable through an op that enforces positive diagonals, i.e. matrix_set_diag with a softplus.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "The issue was during optimization the matrices were not positive semi definite. As correctly pointed out, by enforcing positive semi definite constraint it works. ", "Great!  Closing."]}, {"number": 14201, "title": "Update word2vec_basic.py in generate_batch", "body": "File \"/word2vec_***_basic.py\", line 139, in generate_batch\r\n    buffer[:] = data[:span]\r\nTypeError: sequence index must be integer, not 'slice'\r\n\r\ngot above messages running in python 2.7\r\n\r\nupdating line 139 from \"buffer[:] = data[:span]\" to :\r\n       buffer.extend(data[0:span])\r\nThe updating has passed my own test and can works.\r\n\r\nthanks.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "This looks like it is sent against the release branch.\r\nWe do not accept PRs against the release branch. Could you send this against the master branch?"]}, {"number": 14200, "title": "Fix formatting in readme.", "body": "@flx42 thanks for letting me know!", "comments": []}, {"number": 14199, "title": "Update For more information in README", "body": "Change from lowercase to uppercase", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 14198, "title": "Tensorflow 1.4 Keras issue with respect to model_fn", "body": "I have a model function which accepts Features, targets and mode but when I add tf.keras layers I'm currently getting Exception **`pred` must be a Tensor, a Variable, or a Python bool.**\r\n\r\nBut, When I run the same code with out using tf.keras but directly from keras(i.e. **from keras.layers**), It's working.\r\n\r\n**Code :**\r\n\r\n```python\r\ndef model_fn(features, labels, mode):\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        tf.keras.backend.set_learning_phase(1)\r\n    else:\r\n        tf.keras.backend.set_learning_phase(0)\r\n\r\n    input_feature = features['x']\r\n    table = lookup.index_table_from_file(vocabulary_file='vocab.txt', num_oov_buckets=1, default_value=-1)\r\n    text = tf.squeeze(input_feature, [1])\r\n    words = tf.string_split(text)\r\n    densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\r\n    numbers = table.lookup(densewords)\r\n    padding = tf.constant([[0, 0], [0, MAX_FEATURES]])\r\n    padded = tf.pad(numbers, padding)\r\n    sliced = tf.slice(padded, [0, 0], [-1, MAX_FEATURES])\r\n    print('words_sliced={}'.format(words))\r\n\r\n    #embeds = tf.keras.layers.Embedding(MAX_FEATURES, 50, input_length=MAX_FEATURES)(sliced)\r\n    embeds = tf.contrib.layers.embed_sequence(sliced, vocab_size=MAX_FEATURES, embed_dim=50)\r\n    print('words_embed={}'.format(embeds))\r\n\r\n    f1 = tf.keras.layers.Dropout(0.2)(embeds)\r\n    f1 = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(f1)\r\n    f1 = tf.keras.layers.GlobalAveragePooling1D()(f1)\r\n    # f1 = layers.BatchNormalization()(f1)\r\n    f1 = tf.keras.layers.Dense(hidden_dims)(f1)\r\n    f1 = tf.keras.layers.Dropout(0.5)(f1)\r\n    f1 = tf.keras.layers.Activation('relu')(f1)\r\n    logits = tf.keras.layers.Dense(11)(f1)\r\n\r\n    predictions_dict = {\r\n        'class': tf.argmax(logits, 1),\r\n        'prob': tf.nn.softmax(logits)\r\n    }\r\n\r\n    prediction_output = tf.estimator.export.PredictOutput({\"classes\": tf.argmax(input=logits, axis=1),\r\n                                                           \"probabilities\": tf.nn.softmax(logits,\r\n                                                                                          name=\"softmax_tensor\")})\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions_dict, export_outputs={\r\n            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: prediction_output\r\n        })\r\n\r\n    # one_hot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=11)\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits=logits)\r\n\r\n    if mode == tf.contrib.learn.ModeKeys.TRAIN:\r\n        train_op = tf.contrib.layers.optimize_loss(loss, tf.contrib.framework.get_global_step(), optimizer='Adam',\r\n                                                   learning_rate=0.001)\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\n    eval_metrics_ops = {\r\n        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions_dict['class'])\r\n    }\r\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics_ops)\r\n```\r\n\r\n**Exception:**\r\n\r\n  File \"D:/PyCharm-Workspace/Keras-Exp/finance-complaints/tensorflow/tf_finance_complaints.py\", line 149, in <module>\r\n    finance_classifier.train(input_fn=lambda: input_fn('dataset/train.csv', batch_size=32, repeat_count=5, shuffle=True))\r\n  File \"E:\\Programs\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 302, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"E:\\Programs\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 711, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"E:\\Programs\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 694, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"D:/PyCharm-Workspace/Keras-Exp/finance-complaints/tensorflow/tf_finance_complaints.py\", line 108, in model_fn\r\n    f1 = tf.keras.layers.Dropout(0.2)(embeds)\r\n  File \"E:\\Programs\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\topology.py\", line 252, in __call__\r\n    output = super(Layer, self).__call__(inputs, **kwargs)\r\n  File \"E:\\Programs\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"E:\\Programs\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\core.py\", line 118, in call\r\n    output = super(Dropout, self).call(inputs, training=training)\r\n  File \"E:\\Programs\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 300, in call\r\n    lambda: array_ops.identity(inputs))\r\n  File \"E:\\Programs\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\layers\\utils.py\", line 203, in smart_cond\r\n    pred_value = constant_value(pred)\r\n  File \"E:\\Programs\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\layers\\utils.py\", line 233, in constant_value\r\n    raise TypeError('`pred` must be a Tensor, a Variable, or a Python bool.')\r\nTypeError: `pred` must be a Tensor, a Variable, or a Python bool.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Okay @jart ", "hi @jart, i also got the same issue when `set_learning_phase(0)`, seems there is a problem in [1] `K.learning_phase()` is returning integer, however in [2] is Boolean.\r\n[1]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/layers/core.py#L115-L117\r\n[2]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/core.py#L295\r\n\r\nStack trace\r\n```\r\nTraceback (most recent call last):\r\n  File \".\\predict.py\", line 37, in <module>\r\n    model = load_model('./model.h5')\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\models.py\", line 245, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\models.py\", line 323, in model_from_config\r\n    return layer_module.deserialize(config, custom_objects=custom_objects)\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\serialization.py\", line 63, in deserialize\r\n    printable_module_name='layer')\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\utils\\generic_utils.py\", line 163, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\models.py\", line 1238, in from_config\r\n    model.add(layer)\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\models.py\", line 501, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\topology.py\", line 252, in __call__\r\n    output = super(Layer, self).__call__(inputs, **kwargs)\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 575, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\core.py\", line 118, in call\r\n    output = super(Dropout, self).call(inputs, training=training)\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 300, in call\r\n    lambda: array_ops.identity(inputs))\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\layers\\utils.py\", line 203, in smart_cond\r\n    pred_value = constant_value(pred)\r\n  File \"~\\venv\\lib\\site-packages\\tensorflow\\python\\layers\\utils.py\", line 233, in constant_value\r\n    raise TypeError('`pred` must be a Tensor, a Variable, or a Python bool.')\r\nTypeError: `pred` must be a Tensor, a Variable, or a Python bool.\r\n```", "Guys I also posted this question in [StackOverflow](https://stackoverflow.com/questions/47149076/tensorflow-1-4-keras-issue-with-respect-to-model-fn)", "Try using tf.keras.backend.set_learning_phase(True) or tf.keras.backend.set_learning_phase(False) seems to be working.", "@bshao001 That's correct using **set_learning_phase(True) or set_learning_phase(False)** is solving the problem. Updated the answer in [StackOverflow](https://stackoverflow.com/questions/47149076/tensorflow-1-4-keras-issue-with-respect-to-model-fn)"]}, {"number": 14197, "title": "Update head.py", "body": "Minor typo fix", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14196, "title": "Dataset memory bottleneck not showing in debug mode", "body": "im fitting word2vec models using distributed gpus which require me to assemble multiple towers and so copy my model several times over. to load data i am using the get_next method of a contrib.dataset iterator initialized with one_shot. before starting up i run into the 2gb protobuf memory bottleneck. so in debug mode i compared 3 models: one where dataset is given full data, another where dataset is given 30% of my data and a last where dataset is given 1% of my data. the first doesnt run. but most importantly, for the second two, i compared the size of all of my tensors and they are all the same. this makes debugging difficult. any thoughts? i suspect there's something going on with folding of constants? \r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "hi, \r\n\r\nIn sum: It would be nice if the tf debugging mode _did not fold constants_. \r\n\r\nI was running into the 2GB protobuff limit because I was trying to load my data onto the graph. It took me the longest while to figure out what was causing the memory overload because the tensor nodes which held my data were not showing up in debugging mode. I solved it by using initializable iterator and the feed_dict mechanism. \r\n\r\nfeel free to close this thread. \r\n\r\nThanks!\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Thank you @andrebeu for reporting back. Glad you solved the issue!"]}, {"number": 14195, "title": "Fix broken code tag in tf.contrib.data README", "body": "", "comments": []}, {"number": 14194, "title": "Make CMake on TensorFlow Incrementally Compile on Windows", "body": "I'm working with the tensorflow r1.4 branch on Windows 7 with Visual Studio 2015.   When I configure\r\n\r\n```\r\ncmake C:\\Users\\Kevin\\dev\\tensorflow-r1.4\\tensorflow\\contrib\\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/Users/Kevin/dev/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=C:/ProgramData/Anaconda3/python.exe -DPYTHON_LIBRARIES=C:/ProgramData/Anaconda3/libs/python35.lib -DPYTHON_INCLUDE_DIR=C:\\ProgramData\\Anaconda3\\include -DNUMPY_INCLUDE_DIR=C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\numpy\\core\\include -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\"\r\n```\r\nand run\r\n\r\n```\r\npowershell \"MSBuild /m /p:Configuration=Release tf_python_build_pip_package.vcxproj | tee msbuild.txt\"\r\n```\r\nthe build always compiles all the projects, even the projects already compiled.\r\n\r\nLooking at the first few lines of msbuild.txt I see\r\n\r\n```\r\nMicrosoft (R) Build Engine version 14.0.25420.1\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\nBuild started 11/2/2017 4:35:58 PM.\r\n     1>Project \"C:\\users\\kevin\\dev\\tensorflow-r1.4\\tensorflow\\contrib\\cmake\\tf_python_build_pip_package.vcxproj\" on node 1 (default targets).\r\n     1>Project \"C:\\users\\kevin\\dev\\tensorflow-r1.4\\tensorflow\\contrib\\cmake\\tf_python_build_pip_package.vcxproj\" (1) is building \"C:\\Users\\Kevin\\dev\\tensorflow-r1.4\\tensorflow\\contrib\\cmake\\ZERO_CHECK.vcxproj\" (2) on node 1 (default targets).\r\n     2>InitializeBuildStatus:\r\n         Creating \"x64\\Release\\ZERO_CHECK\\ZERO_CHECK.tlog\\unsuccessfulbuild\" because \"AlwaysCreate\" was specified.\r\n       CustomBuild:\r\n         Checking Build System\r\n         CMake is re-running because C:/Users/Kevin/dev/tensorflow-r1.4/tensorflow/contrib/cmake/CMakeFiles/generate.stamp is out-of-date.\r\n           the file 'C:/Users/Kevin/dev/tensorflow-r1.4/tensorflow/contrib/cmake/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/tf_core_gpu_kernels_generated_cwise_op_gpu_igammas.cu.cc.obj.depend'\r\n           is newer than 'C:/Users/Kevin/dev/tensorflow-r1.4/tensorflow/contrib/cmake/CMakeFiles/generate.stamp.depend'\r\n           result='-1'\r\n         -- Configuring done\r\n```\r\nThe issue isn't the multicore builds, I'm just using multicore builds to speed up the overall build, which takes 5 hours on one core.\r\n\r\nThe problem appears to be that the generate.stamp isn't what's expected.", "comments": ["This would definitely be a useful feature for anyone relying on the CMake build in regular development. However, since we only currently rely on CMake for CI builds on Windows, these are always from scratch, and we hope to shift that build to Bazel for Windows ASAP, it is unlikely that anyone on the TensorFlow team will spend time investigating the cause of the gratuitous rebuilds. Therefore I'm going to mark this issue as \"Contributions Welcome\", and I hope some member of the community heeds the call.\r\n\r\nI'd be happy to respond to questions on this thread about how the build is structured with a view to making it incremental, and very happy to review pull requests that improve the situation. \r\n\r\n/cc @gunan FYI.", "When will the Windows Bazel build be ready for prime-time?\r\n", "Hi @johnsrude , now it lacks tf.load_library() support. All stuffs in tf.contrib are missing.   For the others, it's almost perfect. ", "@johnsrude The CMake build appears to be configured to generate an new version file `tensorflow/core/util/version_info.cc` for *every* build, which is included in the low level `tf_core_framework`.  This is at least partly responsible for the rebuild problems.\r\n\r\nThere is a custom command that creates the version file from python + git here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/dcd820616d352d1e8844b1db504af4e859176cbf/tensorflow/contrib/cmake/tf_core_framework.cmake#L271-L283\r\n\r\nI *think* what we want, is to only update this file if the git version has changed.  I've added an extra `add_custom_command` to copy the output from `gen_git_source.py --raw_generate` over`tensorflow/core/util/version_info.cc` only if it has changed using CMake's `copy_if_different`, and that seems to address the unwanted full rebuild.\r\n\r\n```\r\n# Tricky setup to force always rebuilding\r\n# force_rebuild always runs forcing ${VERSION_INFO_CC} target to run\r\n# ${VERSION_INFO_CC} would cache, but it depends on a phony never produced\r\n# target.\r\nset(gen_git_source ${tensorflow_source_dir}/tensorflow/tools/git/gen_git_source.py)\r\nset(version_info_cc_tmp ${CMAKE_CURRENT_BINARY_DIR}/version_info.cc)\r\nadd_custom_target(force_rebuild_target ALL DEPENDS ${VERSION_INFO_CC})\r\nset_property(TARGET force_rebuild_target PROPERTY FOLDER \"custom\")\r\n\r\nadd_custom_command(OUTPUT __force_rebuild COMMAND ${CMAKE_COMMAND} -E echo)\r\nadd_custom_command(OUTPUT\r\n    ${version_info_cc_tmp}\r\n    COMMAND ${PYTHON_EXECUTABLE} ${gen_git_source} --raw_generate ${version_info_cc_tmp}\r\n    DEPENDS __force_rebuild)\r\n\r\nset(VERSION_INFO_CC ${tensorflow_source_dir}/tensorflow/core/util/version_info.cc)\r\nadd_custom_command(OUTPUT\r\n  \"${VERSION_INFO_CC}\"\r\n  DEPENDS \"${version_info_cc_tmp}\"\r\n  COMMAND ${CMAKE_COMMAND} -E copy_if_different \"${version_info_cc_tmp}\" \"${VERSION_INFO_CC}\"\r\n  ) \r\n  \r\nset(tf_version_srcs ${VERSION_INFO_CC})\r\n```\r\n\r\nI've tested this on OS X and it seems to be working.  Is there an active CMake author who can review this?\r\n", "@headupinclouds I'd be happy to review a PR for this. Thanks!", "I've pushed the above changes in a PR (tested in another branch) for now:\r\n\r\nhttps://github.com/headupinclouds/tensorflow/tree/pr.cmake.incremental.build\r\n\r\nI'm running into some unrelated build issues after pulling the latest from master.  I'll hold off on submitting this until I sort those out.", "This seems very useful. Every time I compile a user_op on windows it needs to re-compile everything.. ", "Hi @steven-hh-ding Forget cmake, Embrace bazel!", "@snnn Thanks! You mean I can build user_opr on windows with bazel? I will check it out!", "Wait, for custom user_op, if you build it as a dll, then you can put it into a separated project, which is not part of the tensorflow.sln. So you don't have to use either cmake or bazel. All the issues described here are gone.  \r\n\r\nIf you put your custom user_op inside tensorflow source tree, it's another story. Yes. you can build it with bazel, and it's very fast.", "Thank you @snnn ! I managed to skip the rebuild with /p:BuildProjectReferences=false. Will switch to bazel later. Thanks!", "I can understand the need for rebuild, but cann't know the reason why should we redownload gprc in every rebuild. Is there any suggestion?\r\nMy network is not good, and the gprc is really big, about 200M !!!", "with up-to-date sources (--source_dir and --git_tag_override):\r\n```\r\n# Tricky setup to force always rebuilding\r\n# force_rebuild always runs forcing ${VERSION_INFO_CC} target to run\r\n# ${VERSION_INFO_CC} would cache, but it depends on a phony never produced\r\n# target.\r\nset(gen_git_source ${tensorflow_source_dir}/tensorflow/tools/git/gen_git_source.py)\r\nset(version_info_cc_tmp ${CMAKE_CURRENT_BINARY_DIR}/version_info.cc)\r\nadd_custom_target(force_rebuild_target ALL DEPENDS ${VERSION_INFO_CC})\r\nset_property(TARGET force_rebuild_target PROPERTY FOLDER \"custom\")\r\n\r\nadd_custom_command(OUTPUT __force_rebuild COMMAND ${CMAKE_COMMAND} -E echo)\r\nadd_custom_command(OUTPUT\r\n    ${version_info_cc_tmp}\r\n    COMMAND ${PYTHON_EXECUTABLE} ${gen_git_source}\r\n    ARGS --raw_generate ${version_info_cc_tmp} --source_dir ${tensorflow_source_dir} --git_tag_override=${GIT_TAG_OVERRIDE}\r\n    DEPENDS __force_rebuild)\r\n\r\nset(VERSION_INFO_CC ${tensorflow_source_dir}/tensorflow/core/util/version_info.cc)\r\nadd_custom_command(OUTPUT\r\n  \"${VERSION_INFO_CC}\"\r\n  COMMAND ${CMAKE_COMMAND}\r\n  ARGS -E copy_if_different \"${version_info_cc_tmp}\" \"${VERSION_INFO_CC}\"\r\n  DEPENDS \"${version_info_cc_tmp}\"\r\n  ) \r\n  \r\nset(tf_version_srcs ${VERSION_INFO_CC})\r\n```", "@johnsrude It seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. since contrib has been depreciated in Tensorflow 2.x check the name of the module without the tf.contrib part to know it's new location, also please refer this [link](https://www.tensorflow.org/guide/migrate) ."]}, {"number": 14193, "title": "TypeError: Cannot interpret feed_dict key as Tensor: The name 'save/Const:0' refers to a Tensor which does not exist. The operation, 'save/Const', does not exist in the graph.", "body": "From this file: https://github.com/llSourcell/pong_neural_network_live/blob/master/RL.py\r\n\r\nI've updated the lines \r\n\r\n    #first convolutional layer. bias vector\r\n    #creates an empty tensor with all elements set to zero with a shape\r\n    W_conv1 = tf.Variable(tf.zeros([8, 8, 4, 32]) , name='W_conv1')\r\n    b_conv1 = tf.Variable(tf.zeros([32]), name='b_conv1')\r\n\r\n    W_conv2 = tf.Variable(tf.zeros([4, 4, 32, 64]), name='W_conv2')\r\n    b_conv2 = tf.Variable(tf.zeros([64]), name='b_conv2')\r\n\r\n    W_conv3 = tf.Variable(tf.zeros([3, 3, 64, 64]), name='W_conv3')\r\n    b_conv3 = tf.Variable(tf.zeros([64]), name='b_conv3')\r\n\r\n    W_fc4 = tf.Variable(tf.zeros([3136, 784]), name='W_fc4')\r\n    b_fc4 = tf.Variable(tf.zeros([784]), name='b_fc4')\r\n\r\n    W_fc5 = tf.Variable(tf.zeros([784, ACTIONS]), name='W_fc5')\r\n    b_fc5 = tf.Variable(tf.zeros([ACTIONS]), name='b_fc5')\r\n\r\nand:\r\n\r\n    saver.save(sess, './' + 'pong' + '-dqn', global_step = timestamp )\r\n\r\nand in:\r\n\r\n    def main():\r\n        # ////\r\n        tf.reset_default_graph()    \r\n        imported_meta = tf.train.import_meta_graph('./' + 'pong' + '-dqn-' + '48000' + '.meta')  \r\n        imported_meta.restore(sess, tf.train.latest_checkpoint('./'))\r\n        # ////\r\n\r\nTo try and restore the model but I get this error:\r\n\r\nTypeError: Cannot interpret feed_dict key as Tensor: The name 'save/Const:0' refers to a Tensor which does not exist. The operation, 'save/Const', does not exist in the graph.\r\n\r\nWhen I try this:\r\n\r\n     graph = tf.get_default_graph() \r\n     W_conv1 = graph.get_tensor_by_name(\"W_conv1:0\")\r\n     b_conv1 = graph.get_tensor_by_name(\"wb_conv1:0\") \r\n     W_conv2 = graph.get_tensor_by_name(\"W_conv2:0\")\r\n     b_conv2 = graph.get_tensor_by_name(\"wb_conv2:0\") \r\n     W_conv3 = graph.get_tensor_by_name(\"W_conv3:0\")\r\n     b_conv3 = graph.get_tensor_by_name(\"b_conv3:0\") \r\n     W_fc4 = graph.get_tensor_by_name(\"W_fc4:0\")\r\n     b_fc4 = graph.get_tensor_by_name(\"b_fc4:0\") \r\n     W_fc5 = graph.get_tensor_by_name(\"W_fc5:0\")\r\n     b_fc5 = graph.get_tensor_by_name(\"b_fc5:0\")  \r\n\r\nI get this error:\r\n\r\n\"The name 'W_conv1:0' refers to a Tensor which does not exist. The operation, 'W_conv1', does not exist in the graph.\r\n\r\nWhy is this happening? I've created my game in pygame and I'm trying to connect it to the RL. I'd like to make sure I can save and load my progress. I'm just having trouble with the logic of how to save and load.\r\n\r\nThanks in advance!\r\n\r\nWhy is this happening?\r\n", "comments": ["It's hard to help without the details asked for in the new issue template (which includes information about the version of TensorFlow being used) and detailed instructions to reproduce the problem (ideally ones which do not involve having to install additional libraries, scan through some other code  and manually merge snippets).\r\n\r\nI tried the following in TensorFlow 1.4.0 and 1.3.0 but was unable to reproduce the problem you observed:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef save():\r\n  W_conv1 = tf.Variable(tf.zeros([8, 8, 4, 32]), name='W_conv1')\r\n  saver = tf.train.Saver()\r\n  with tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    saver.save(sess, '/tmp/foo')\r\n\r\ndef load():\r\n  with tf.Graph().as_default():\r\n    saver = tf.train.import_meta_graph('/tmp/foo.meta')\r\n    with tf.Session() as sess:\r\n      saver.restore(sess, tf.train.latest_checkpoint('/tmp'))\r\n      \r\n      graph = tf.get_default_graph()\r\n      print(graph.get_tensor_by_name(\"W_conv1:0\"))\r\n\r\nsave()\r\ntf.reset_default_graph()\r\nload()\r\n```\r\n\r\nPerhaps this isn't accurately reproducing what you're trying. If you could provide a similar self-contained snippet, that would enable us to debug and help.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I am having a similar issue. I have an RNN that I'm running in a CV loop and each time through the loop I call a function to make the model. (If I reuse the same model, it continues training, which renders CV useless.) Just to be safe, I included `tf.reset_default_graph()` at the start of the model-making function and on the second time through the CV loop I get the error:\r\n\r\n`ValueError: Tensor Tensor(\"embedding_input:0\", shape=(?, ?), dtype=float32) is not an element of this graph.\r\n`\r\nand\r\n`\r\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"embedding_input:0\", shape=(?, ?), dtype=float32) is not an element of this graph.\r\n`", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 14192, "title": "Dockerfile.gpu: use the runtime cuDNN v6 image", "body": "The generated Docker image will be approximately 900 MB smaller.\r\n\r\nThe Dockerfile switched to the devel image a long time ago to\r\nworkaround a bug when looking up CUDA libraries. This problem has been\r\nfixed in the meantime.\r\n\r\n\r\n@gunan let me know if I'm missing something and there is a good reason to keep the devel image as a base.", "comments": ["Can one of the admins verify this patch?", "I cannot think of a good reason, thank you very much for the help!\r\nJenkins, test this please."]}, {"number": 14191, "title": "tf.nn.l2_normalize takes dim instead of axis", "body": "At master, `tf.nn.l2_normalize` still takes the axis parameter as `dim`.  Everything else has been standardized on `axis`, so it'd be nice if this one was `axis` too.  Since `dim` can't be changed for backwards compatibility reasons, presumably the right approach is adding an extra `axis` argument and requiring that at most one of them is set.\r\n\r\n@aselle: Is that right?", "comments": ["@girving PR #12716 changes dim to axis for `tf.nn.softmax` and `tf.nn.log_softmax`  (with backward-compatibility). The PR is pending review. I can update the PR to include changes of `tf.nn.l2_normalize` from `dim` to `axis` as well.", "@girving PR #12716 has been updated to change `dim` to `axis` for `tf.nn.l2_normalize` as well. Please take a look.", "@yongtang Thanks, looks perfect."]}, {"number": 14190, "title": "tf.contrib.boosted_trees still cannot be used in official 1.4.0", "body": "Hi, i have raised an [issue](https://github.com/tensorflow/tensorflow/issues/14087#event-1319398254) before, however after i upgrade to official 1.4.0, the problem is still the same.\r\n\r\nI am sorry about raising this again, but GBDT in TF seems so appealing to users.\r\n\r\nTF version: 1.4.0\r\nPy version: 2.7.10\r\nOS: Mac OS\r\n\r\nThe testing script i am using is:\r\n```python\r\nimport argparse\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeClassifier\r\nfrom tensorflow.contrib.boosted_trees.proto import learner_pb2\r\nfrom tensorflow.contrib.learn import learn_runner\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--batch_size\",type=int,default=1000)\r\nparser.add_argument(\"--depth\", type=int, default=4, help=\"Maximum depth of weak learners.\")\r\nparser.add_argument(\"--l2\", type=float, default=1.0, help=\"l2 regularization per batch.\")\r\nparser.add_argument(\"--learning_rate\",type=float,default=0.1)\r\nparser.add_argument(\"--examples_per_layer\", type=int, default=1000)\r\nparser.add_argument(\"--num_trees\", type=int, default=10)\r\nargs = parser.parse_args()\r\n\r\nlearner_config = learner_pb2.LearnerConfig()\r\nnum_classes = 10\r\nlearner_config.learning_rate_tuner.fixed.learning_rate = args.learning_rate\r\nlearner_config.num_classes = num_classes\r\nlearner_config.regularization.l1 = 0.0\r\nlearner_config.regularization.l2 = args.l2 / args.examples_per_layer\r\nlearner_config.constraints.max_tree_depth = args.depth\r\n\r\ngrowing_mode = learner_pb2.LearnerConfig.LAYER_BY_LAYER\r\nlearner_config.growing_mode = growing_mode\r\nlearner_config.multi_class_strategy = (\r\n    learner_pb2.LearnerConfig.DIAGONAL_HESSIAN)\r\n\r\nestimator = GradientBoostedDecisionTreeClassifier(\r\n    learner_config=learner_config,\r\n    n_classes=num_classes,\r\n    examples_per_layer=args.examples_per_layer,\r\n    num_trees=args.num_trees,\r\n    center_bias=False)\r\n\r\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nX_train = (X_train / 255.).reshape(-1, 28*28).astype(np.float32)\r\ny_train = y_train.astype(np.int32)\r\n\r\nestimator.fit(input_fn=tf.estimator.inputs.numpy_input_fn(\r\n    x={'_':X_train}, y=y_train, batch_size=args.batch_size, num_epochs=1, shuffle=True))\r\n```\r\nThe full error i am getting is:\r\n```\r\n(tf_1.4) Zhedongs-MacBook-Pro:desktop zhedongzheng$ python tf_boost_test.py\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpCFdPSc\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x110dac810>, '_model_dir': '/var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmpCFdPSc', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_tf_random_seed': None, '_save_summary_steps': 100, '_environment': 'local', '_num_worker_replicas': 0, '_task_id': 0, '_log_step_count_steps': 100, '_tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1\r\n}\r\n, '_evaluation_master': '', '_master': ''}\r\nINFO:tensorflow:Active Feature Columns: ['__0', '__1', '__2', '__3', '__4', '__5', '__6', '__7', '__8', '__9', '__10', '__11', '__12', '__13', '__14', '__15', '__16', '__17', '__18', '__19', '__20', '__21', '__22', '__23', '__24', '__25', '__26', '__27', '__28', '__29', '__30', '__31', '__32', '__33', '__34', '__35', '__36', '__37', '__38', '__39', '__40', '__41', '__42', '__43', '__44', '__45', '__46', '__47', '__48', '__49', '__50', '__51', '__52', '__53', '__54', '__55', '__56', '__57', '__58', '__59', '__60', '__61', '__62', '__63', '__64', '__65', '__66', '__67', '__68', '__69', '__70', '__71', '__72', '__73', '__74', '__75', '__76', '__77', '__78', '__79', '__80', '__81', '__82', '__83', '__84', '__85', '__86', '__87', '__88', '__89', '__90', '__91', '__92', '__93', '__94', '__95', '__96', '__97', '__98', '__99', '__100', '__101', '__102', '__103', '__104', '__105', '__106', '__107', '__108', '__109', '__110', '__111', '__112', '__113', '__114', '__115', '__116', '__117', '__118', '__119', '__120', '__121', '__122', '__123', '__124', '__125', '__126', '__127', '__128', '__129', '__130', '__131', '__132', '__133', '__134', '__135', '__136', '__137', '__138', '__139', '__140', '__141', '__142', '__143', '__144', '__145', '__146', '__147', '__148', '__149', '__150', '__151', '__152', '__153', '__154', '__155', '__156', '__157', '__158', '__159', '__160', '__161', '__162', '__163', '__164', '__165', '__166', '__167', '__168', '__169', '__170', '__171', '__172', '__173', '__174', '__175', '__176', '__177', '__178', '__179', '__180', '__181', '__182', '__183', '__184', '__185', '__186', '__187', '__188', '__189', '__190', '__191', '__192', '__193', '__194', '__195', '__196', '__197', '__198', '__199', '__200', '__201', '__202', '__203', '__204', '__205', '__206', '__207', '__208', '__209', '__210', '__211', '__212', '__213', '__214', '__215', '__216', '__217', '__218', '__219', '__220', '__221', '__222', '__223', '__224', '__225', '__226', '__227', '__228', '__229', '__230', '__231', '__232', '__233', '__234', '__235', '__236', '__237', '__238', '__239', '__240', '__241', '__242', '__243', '__244', '__245', '__246', '__247', '__248', '__249', '__250', '__251', '__252', '__253', '__254', '__255', '__256', '__257', '__258', '__259', '__260', '__261', '__262', '__263', '__264', '__265', '__266', '__267', '__268', '__269', '__270', '__271', '__272', '__273', '__274', '__275', '__276', '__277', '__278', '__279', '__280', '__281', '__282', '__283', '__284', '__285', '__286', '__287', '__288', '__289', '__290', '__291', '__292', '__293', '__294', '__295', '__296', '__297', '__298', '__299', '__300', '__301', '__302', '__303', '__304', '__305', '__306', '__307', '__308', '__309', '__310', '__311', '__312', '__313', '__314', '__315', '__316', '__317', '__318', '__319', '__320', '__321', '__322', '__323', '__324', '__325', '__326', '__327', '__328', '__329', '__330', '__331', '__332', '__333', '__334', '__335', '__336', '__337', '__338', '__339', '__340', '__341', '__342', '__343', '__344', '__345', '__346', '__347', '__348', '__349', '__350', '__351', '__352', '__353', '__354', '__355', '__356', '__357', '__358', '__359', '__360', '__361', '__362', '__363', '__364', '__365', '__366', '__367', '__368', '__369', '__370', '__371', '__372', '__373', '__374', '__375', '__376', '__377', '__378', '__379', '__380', '__381', '__382', '__383', '__384', '__385', '__386', '__387', '__388', '__389', '__390', '__391', '__392', '__393', '__394', '__395', '__396', '__397', '__398', '__399', '__400', '__401', '__402', '__403', '__404', '__405', '__406', '__407', '__408', '__409', '__410', '__411', '__412', '__413', '__414', '__415', '__416', '__417', '__418', '__419', '__420', '__421', '__422', '__423', '__424', '__425', '__426', '__427', '__428', '__429', '__430', '__431', '__432', '__433', '__434', '__435', '__436', '__437', '__438', '__439', '__440', '__441', '__442', '__443', '__444', '__445', '__446', '__447', '__448', '__449', '__450', '__451', '__452', '__453', '__454', '__455', '__456', '__457', '__458', '__459', '__460', '__461', '__462', '__463', '__464', '__465', '__466', '__467', '__468', '__469', '__470', '__471', '__472', '__473', '__474', '__475', '__476', '__477', '__478', '__479', '__480', '__481', '__482', '__483', '__484', '__485', '__486', '__487', '__488', '__489', '__490', '__491', '__492', '__493', '__494', '__495', '__496', '__497', '__498', '__499', '__500', '__501', '__502', '__503', '__504', '__505', '__506', '__507', '__508', '__509', '__510', '__511', '__512', '__513', '__514', '__515', '__516', '__517', '__518', '__519', '__520', '__521', '__522', '__523', '__524', '__525', '__526', '__527', '__528', '__529', '__530', '__531', '__532', '__533', '__534', '__535', '__536', '__537', '__538', '__539', '__540', '__541', '__542', '__543', '__544', '__545', '__546', '__547', '__548', '__549', '__550', '__551', '__552', '__553', '__554', '__555', '__556', '__557', '__558', '__559', '__560', '__561', '__562', '__563', '__564', '__565', '__566', '__567', '__568', '__569', '__570', '__571', '__572', '__573', '__574', '__575', '__576', '__577', '__578', '__579', '__580', '__581', '__582', '__583', '__584', '__585', '__586', '__587', '__588', '__589', '__590', '__591', '__592', '__593', '__594', '__595', '__596', '__597', '__598', '__599', '__600', '__601', '__602', '__603', '__604', '__605', '__606', '__607', '__608', '__609', '__610', '__611', '__612', '__613', '__614', '__615', '__616', '__617', '__618', '__619', '__620', '__621', '__622', '__623', '__624', '__625', '__626', '__627', '__628', '__629', '__630', '__631', '__632', '__633', '__634', '__635', '__636', '__637', '__638', '__639', '__640', '__641', '__642', '__643', '__644', '__645', '__646', '__647', '__648', '__649', '__650', '__651', '__652', '__653', '__654', '__655', '__656', '__657', '__658', '__659', '__660', '__661', '__662', '__663', '__664', '__665', '__666', '__667', '__668', '__669', '__670', '__671', '__672', '__673', '__674', '__675', '__676', '__677', '__678', '__679', '__680', '__681', '__682', '__683', '__684', '__685', '__686', '__687', '__688', '__689', '__690', '__691', '__692', '__693', '__694', '__695', '__696', '__697', '__698', '__699', '__700', '__701', '__702', '__703', '__704', '__705', '__706', '__707', '__708', '__709', '__710', '__711', '__712', '__713', '__714', '__715', '__716', '__717', '__718', '__719', '__720', '__721', '__722', '__723', '__724', '__725', '__726', '__727', '__728', '__729', '__730', '__731', '__732', '__733', '__734', '__735', '__736', '__737', '__738', '__739', '__740', '__741', '__742', '__743', '__744', '__745', '__746', '__747', '__748', '__749', '__750', '__751', '__752', '__753', '__754', '__755', '__756', '__757', '__758', '__759', '__760', '__761', '__762', '__763', '__764', '__765', '__766', '__767', '__768', '__769', '__770', '__771', '__772', '__773', '__774', '__775', '__776', '__777', '__778', '__779', '__780', '__781', '__782', '__783']\r\nTraceback (most recent call last):\r\n  File \"tf_boost_test.py\", line 52, in <module>\r\n    x={'_':X_train}, y=y_train, batch_size=args.batch_size, num_epochs=1, shuffle=True))\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 480, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 986, in _train_model\r\n    model_fn_ops = self._get_train_ops(features, labels)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1202, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1166, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/estimator_batch/model.py\", line 116, in model_builder\r\n    logits=logits)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1064, in create_model_fn_ops\r\n    enable_centered_bias=self._enable_centered_bias)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 648, in _create_model_fn_ops\r\n    batch_size, loss_fn, weight_tensor)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1923, in _train_op\r\n    train_op = train_op_fn(loss)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/estimator_batch/model.py\", line 105, in _train_op_fn\r\n    update_op = gbdt_model.train(loss, predictions_dict, labels)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/python/training/functions/gbdt_batch.py\", line 543, in train\r\n    hessian_list = self._diagonal_hessian(gradients, predictions)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/python/training/functions/gbdt_batch.py\", line 845, in _diagonal_hessian\r\n    aggregation_method=None)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py\", line 352, in _PreventGradientGrad\r\n    \"Gradient explicitly disabled. Reason: %s\" % op.get_attr(\"message\"))\r\nLookupError: Gradient explicitly disabled. Reason: Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation's interaction with tf.gradients()\r\n```", "comments": ["hi, @sshrdp, can you have a look? or the next release you are talking about is actually 1.5?", "Yes, the fix is in 1.5. The MNIST example that came with 1.4 does multi-class classification in that version which should be compatible with your installation. \r\nhttps://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/boosted_trees/examples/mnist.py", "Thanks @sshrdp, i have successfully run the boosted_tree in 1.4\r\n(though it is very slow building the graph)\r\n* Python 3 throws error, because they use ```xrange``` in the source code\r\n\r\nFor those who may be interested running GBT in TF, the minimal script is:\r\n```python\r\nimport argparse\r\nimport functools\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeEstimator\r\nfrom tensorflow.contrib.boosted_trees.proto import learner_pb2\r\nfrom tensorflow.contrib.boosted_trees.estimator_batch import custom_loss_head\r\nfrom tensorflow.contrib.boosted_trees.python.utils import losses\r\nfrom tensorflow.contrib import metrics as metrics_lib\r\nfrom tensorflow.python.ops import math_ops\r\n\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--batch_size\",type=int,default=1000)\r\nparser.add_argument(\"--depth\", type=int, default=4, help=\"Maximum depth of weak learners.\")\r\nparser.add_argument(\"--l2\", type=float, default=1.0, help=\"l2 regularization per batch.\")\r\nparser.add_argument(\"--learning_rate\",type=float,default=0.1)\r\nparser.add_argument(\"--examples_per_layer\", type=int, default=1000)\r\nparser.add_argument(\"--num_trees\", type=int, default=100)\r\nparser.add_argument(\"--num_classes\", type=int, default=10)\r\nargs = parser.parse_args()\r\n\r\nlearner_config = learner_pb2.LearnerConfig()\r\nlearner_config.learning_rate_tuner.fixed.learning_rate = args.learning_rate\r\nlearner_config.num_classes = args.num_classes\r\nlearner_config.regularization.l1 = 0.0\r\nlearner_config.regularization.l2 = args.l2 / args.examples_per_layer\r\nlearner_config.constraints.max_tree_depth = args.depth\r\n\r\nlearner_config.growing_mode = learner_pb2.LearnerConfig.LAYER_BY_LAYER\r\nlearner_config.multi_class_strategy = learner_pb2.LearnerConfig.DIAGONAL_HESSIAN\r\n\r\n\r\ndef _multiclass_metrics(predictions, labels, weights):\r\n    \"\"\"Prepares eval metrics for multiclass eval.\"\"\"\r\n    metrics = dict()\r\n    logits = predictions[\"scores\"]\r\n    classes = math_ops.argmax(logits, 1)\r\n    metrics[\"accuracy\"] = metrics_lib.streaming_accuracy(\r\n        classes, labels, weights)\r\n    return metrics\r\n\r\n\r\nhead = custom_loss_head.CustomLossHead(\r\n      loss_fn=functools.partial(losses.per_example_maxent_loss, num_classes=args.num_classes),\r\n      link_fn=tf.identity,\r\n      logit_dimension=args.num_classes,\r\n      metrics_fn=_multiclass_metrics)\r\n\r\nestimator = GradientBoostedDecisionTreeEstimator(\r\n    learner_config=learner_config,\r\n    head=head,\r\n    examples_per_layer=args.examples_per_layer,\r\n    num_trees=args.num_trees,\r\n    center_bias=False)\r\n\r\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nX_train = (X_train / 255.).reshape(-1, 28*28).astype(np.float32)\r\nX_test = (X_test / 255.).reshape(-1, 28*28).astype(np.float32)\r\ny_train = y_train.astype(np.int32)\r\ny_test = y_test.astype(np.int32)\r\n\r\nestimator.fit(input_fn=tf.estimator.inputs.numpy_input_fn(\r\n    x={'_':X_train}, y=y_train, batch_size=args.batch_size, num_epochs=1, shuffle=True))\r\n\r\nestimator.evaluate(input_fn=tf.estimator.inputs.numpy_input_fn(\r\n    x={'_':X_test}, y=y_test, batch_size=args.batch_size, shuffle=False))\r\n```", "What's your examples_per_layer and batch_size?\r\nHere is the values set in mnist.py comment:\r\n python tensorflow/contrib/boosted_trees/examples/mnist.py \\\r\n  --output_dir=\"/tmp/mnist\" --depth=4 --learning_rate=0.3 --batch_size=60000  \\\r\n  --examples_per_layer=60000 --eval_batch_size=10000 --num_eval_steps=1 \\\r\n  --num_trees=10 --l2=1\r\n", "hi @sshrdp, just default value as you can see from my code above.\r\n```\r\nparser.add_argument(\"--batch_size\",type=int,default=1000)\r\nparser.add_argument(\"--depth\", type=int, default=4, help=\"Maximum depth of weak learners.\")\r\nparser.add_argument(\"--l2\", type=float, default=1.0, help=\"l2 regularization per batch.\")\r\nparser.add_argument(\"--learning_rate\",type=float,default=0.1)\r\nparser.add_argument(\"--examples_per_layer\", type=int, default=1000)\r\nparser.add_argument(\"--num_trees\", type=int, default=100)\r\nparser.add_argument(\"--num_classes\", type=int, default=10)\r\n```\r\nYou can try my script on a single machine, it is very slow to build the graph.", "@zhedongzheng \r\nestimator.fit(input_fn=tf.estimator.inputs.numpy_input_fn(\r\n    x={'_':X_train}, y=y_train, batch_size=args.batch_size, num_epochs=1, shuffle=True))\r\n\r\nafter fit by GradientBoostedDecisionTreeEstimator , can the GBDT model be saved in JSON format, which can easily loaded by other language such as C++...   ", "@zhedongzheng - I can see you are able to make it work with GBDTEstimator class, but how about the `GradientBoostedDecisionTreeClassifier`?\r\nAny workarounds for classifier before 1.5?", "@Dahlasam using ```GradientBoostedDecisionTreeClassifier``` needs to be next release. If we really want to use it right now, we need to use GBDTEstimatror."]}, {"number": 14189, "title": "Cannot deepcopy index_table_from_file object", "body": "Tensorflow 1.4.0\r\nWindows 10\r\n\r\nCode:\r\n```python\r\nimport copy\r\nvt = tf.contrib.lookup.index_table_from_file('test.vocab', 0)\r\ncopy.deepcopy(vt)\r\n```\r\n\r\nError\r\n```\r\n.....\r\nFile \"C:\\Development\\Tools\\miniconda\\envs\\py36\\lib\\copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"C:\\Development\\Tools\\miniconda\\envs\\py36\\lib\\copy.py\", line 169, in deepcopy\r\n    rv = reductor(4)\r\nTypeError: can't pickle _thread.lock objects\r\n```\r\n\r\nIs it intended that this object cannot be deepcopied?", "comments": ["Yes, that is intentional (`Tensor`, `Operation`, and many other types) are not intended to be deepcopied.\r\n\r\nIn this particular case, it's because of the use of the `threading` module, which many of these classes use. For example, you'll probably get the same error with something like this:\r\n\r\n```python\r\nimport threading\r\nimport copy\r\n\r\nclass C(object):\r\n  def __init__(self):\r\n    self.lock = threading.Lock()\r\n\r\nc = C()\r\ncopy.deepcopy(c)\r\n```\r\n"]}, {"number": 14188, "title": "Retval[0] does not have value in a multithreaded context with FIFOQueue and  tf.scan(...)", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I provide a working script showing the buggy behaviour\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: GeForce GTX 280M 1GB\r\n\r\nNote: I reproduced the buggy behaviour on different platforms too: macOS High Sierra, openSUSE 42.3 and with Tensorflow 1.3\r\n\r\n###  problem context\r\nI found the buggy behaviour in one of the helper methods the queue loader of the DeepSpeech project (ctc_label_dense_to_sparse). That implementation is partially clumsy in particular when using tf.scan where a scan history context is not used. Nevertheless the algorithm should have worked under any circumstances. I could reduce the problem scenario to a small standalone example. This example uses a number of threads for filling a queue from which batches are requested by dequeu_up_to (the same buggy behaviour if replaced by dequeue_many). Afterwards the batch is postproccessed. For this postprocessing I provided the buggy version (function_buggy, using tf.scan) and an own implementation (function_ok). Both versions produce the same output data in cases there the buggy versions does not fail. \r\n\r\n### buggy behaviour description\r\nVery often, but not always a test with a larger batch_size and smaller thread_count leads to the following output: tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value. I include the complete log in the next section. With a larger thread_count and smaller batch_size it works. If the switch-Parameter --use_buggy_version is set to 0 (False) no choice of batch_size and thread_count produces a bug. This proves that the problem is located in the buggy function and the rest of the workflow is OK. I could prove that an implementation without the tf.scan works fine too. In experiments using the context in tf.scan instead of ignoring it didn't change the behaviour. Even as tf.scan doesn't make much sense in the context it was used for in DeepSpeech it  should not have failed and its correct function is essential for many tensorflow based projects. And maybe a similar flaw is hidden in the other \"Higher Order operators\" too: tf.map_fn, tf.foldl, tf.foldr\r\n\r\n### log\r\n\r\nusage: scan_bug_demo.py [-h] [--batch_size BATCH_SIZE]\r\n                        [--thread_count THREAD_COUNT]\r\n                        [--use_buggy_version USE_BUGGY_VERSION]\r\nbatch_size=15\r\nthread_count=1\r\nuse_buggy_version=True\r\nusing buggy implementation: True\r\nTraceback (most recent call last):\r\n  File \"scan_bug_demo.py\", line 127, in <module>\r\n    retval = do_it(batch_size, thread_count, use_buggy_version, rnd_seed)\r\n  File \"scan_bug_demo.py\", line 99, in do_it\r\n    coord.join(queue_threads)\r\n  File \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"scan_bug_demo.py\", line 92, in do_it\r\n    res = sess.run(batch)\r\n  File \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/uli/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n\r\n### bug demonstration python source\r\n\r\nimport tensorflow as tf\r\nfrom random import Random\r\nfrom threading import Thread\r\nimport numpy as np\r\nimport sys\r\nfrom argparse import ArgumentParser\r\n\r\n\r\ndef function_buggy(value_sequence_lengths):\r\n    max_len = tf.reduce_max(value_sequence_lengths)\r\n\r\n    max_value_seuence_lenght_tns = tf.expand_dims(max_len, 0)\r\n    init = tf.expand_dims(tf.cast(tf.fill(max_value_seuence_lenght_tns, 0), tf.bool), 0)\r\n\r\n    def scan_function(previous_state, current_input):\r\n        return tf.expand_dims(tf.range(max_len), 0) < current_input\r\n\r\n    retval = tf.squeeze(tf.scan(scan_function, value_sequence_lengths, initializer=init, parallel_iterations=1),\r\n                        axis=[1])\r\n\r\n    return retval\r\n\r\n\r\ndef function_ok(value_sequence_lengths):\r\n    max_len = tf.reduce_max(value_sequence_lengths)\r\n    value_sequence_lengths_shape = tf.shape(value_sequence_lengths)\r\n\r\n    x_repeated_lenghts = tf.tile(tf.expand_dims(value_sequence_lengths, 1), (1, max_len))\r\n    y_repeated_x_indices = tf.tile(tf.expand_dims(tf.range(max_len), 0), (value_sequence_lengths_shape[0], 1))\r\n\r\n    retval = y_repeated_x_indices < x_repeated_lenghts\r\n\r\n    return retval\r\n\r\n\r\nclass BatchProvider(object):\r\n    def __init__(self, batch_size, function, thread_count, rnd_seed):\r\n        self._coord = None\r\n        self.batch_size = batch_size\r\n        self._random = Random(rnd_seed)\r\n        self._capacity = 2 * batch_size\r\n        self._thread_count = thread_count\r\n        self._queue = tf.FIFOQueue(shapes=[[]], dtypes=[tf.int32], capacity=self._capacity)\r\n        self._y_length = tf.placeholder(tf.int32, [])\r\n        self._enqueue_op = self._queue.enqueue(self._y_length)\r\n        self._close_op = self._queue.close(cancel_pending_enqueues=True)\r\n        self._function = function\r\n\r\n    def start_queue_threads(self, session, coord):\r\n        self._coord = coord\r\n        batch_threads = [Thread(target=self._populate_batch_queue, args=(session,)) for i in range(self._thread_count)]\r\n        for batch_thread in batch_threads:\r\n            self._coord.register_thread(batch_thread)\r\n            batch_thread.daemon = True\r\n            batch_thread.start()\r\n        return batch_threads\r\n\r\n    def close_queue(self, session):\r\n        session.run(self._close_op)\r\n\r\n    def _populate_batch_queue(self, session):\r\n        while True:\r\n            length = self._random.randint(5, 10)\r\n            target_len = length\r\n            try:\r\n                self._enqueue_op.run(session=session, feed_dict={self._y_length: target_len})\r\n            except tf.errors.CancelledError:\r\n                return\r\n\r\n    def next_batch(self):\r\n        target_lengths = self._queue.dequeue_up_to(self.batch_size)\r\n        retval = self._function(target_lengths)\r\n        return retval\r\n\r\n\r\ndef do_it(batch_size, thread_count, use_buggy_version, rnd_seed):\r\n    function = function_buggy if use_buggy_version else function_ok\r\n\r\n    batch_provider = BatchProvider(batch_size=batch_size, function=function,\r\n                                   thread_count=thread_count, rnd_seed=rnd_seed)\r\n\r\n    sess = tf.Session()\r\n\r\n    coord = tf.train.Coordinator()\r\n    queue_threads = batch_provider.start_queue_threads(sess, coord)\r\n    batch = batch_provider.next_batch()\r\n\r\n    try:\r\n        for _ in range(1):\r\n            if coord.should_stop():\r\n                break\r\n            res = sess.run(batch)\r\n            print batch\r\n            print res\r\n    except Exception, ex:\r\n        coord.request_stop(ex)\r\n    finally:\r\n        batch_provider.close_queue(sess)\r\n        coord.join(queue_threads)\r\n        sess.close()\r\n\r\n    return res\r\n\r\n\r\nif __name__ == '__main__':\r\n    args = sys.argv[1:]\r\n    argparser = ArgumentParser()\r\n    argparser.add_argument(\"--batch_size\", dest=\"batch_size\", type=int, default=15)\r\n    argparser.add_argument(\"--thread_count\", dest=\"thread_count\", type=int, default=1)\r\n    argparser.add_argument(\"--use_buggy_version\", dest=\"use_buggy_version\", type=int, default=True)\r\n    if len(args) == 0:\r\n        argparser.print_usage()\r\n    parsed = argparser.parse_args(args=args)\r\n    batch_size = parsed.batch_size\r\n    thread_count = parsed.thread_count\r\n    use_buggy_version = parsed.use_buggy_version\r\n\r\n    print \"batch_size=\" + str(batch_size)\r\n    print \"thread_count=\" + str(thread_count)\r\n    print \"use_buggy_version=\" + str(use_buggy_version)\r\n\r\n    rnd_seed = Random().random()\r\n\r\n    print \"using buggy implementation: \" + str(use_buggy_version)\r\n    retval = do_it(batch_size, thread_count, use_buggy_version, rnd_seed)\r\n    print \"success\"\r\n", "comments": ["Thank you for the report. There's a lot of code here. Would you be able to distill a minimal reproducible example?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "The way of providing the training data in the problem context is not\nthe recommended one since TensorFlow 1.4. As there is the possibility\nthat using the now recomended new mechanisms could prevent that kind of\nproblem from occurring I think the bug could be closed and maybe\nreopened if it exists in the new setup too.\n\nAm Donnerstag, den 08.02.2018, 11:23 -0800 schrieb Alfred Sorten Wolf:\n> Nagging Awaiting Response: It has been 14 days with no activityand\n> the awaiting response label was assigned. Is this still an issue?\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "Agreed, closing for now but please reopen if it's present in non-deprecated mechanisms.", "> The way of providing the training data in the problem context is not the recommended one since TensorFlow 1.4. As there is the possibility that using the now recomended new mechanisms could prevent that kind of problem from occurring I think the bug could be closed and maybe reopened if it exists in the new setup too. Am Donnerstag, den 08.02.2018, 11:23 -0800 schrieb Alfred Sorten Wolf:\r\n> [\u2026](#)\r\n\r\nCan you, please, tell what exactly is not recommended?\r\nI am experiencing similar issue with tf 1.5 (I know there is 1.12 \ud83d\ude33\ud83e\udd26\u200d\u2642\ufe0f) with tf.map_fn which applies very slow function and without any prefetching technique. Current workaround is to increase parallel_iterations.\r\n\r\nI can not reproduce it with code simple enough to post for now :(.\r\n\r\nThank you. "]}, {"number": 14187, "title": "Remove `non-fused` version of `adjust_hue` as GPU kernel is already in place", "body": "Was looking into adding batch support for `tf.image.random_hue` (#8926) and noticed that the `non-fused` version of `adjust_hue` was still in place. The `non-fused` is for non-GPU support of `adjust_hue`.\r\n\r\nAs GPU kernel for `AdjustHue` has already been added in PR #6818, I think it makes sense to remove the non-fused version. Besides, the env `TF_ADJUST_HUE_FUSED` seems not in use anyway.\r\n\r\nThis fix removed `non-fused` version of `adjust_hue` as GPU kernel is already in place.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 14186, "title": "TF 1.4 build_all_android.sh fails with nsync.a", "body": "Running\r\n\r\n    NDK_ROOT=\"$HOME/Library/Android/sdk/ndk-bundle\" bash $MAKEFILE_DIR/build_all_android.sh\r\n\r\nresults in this error\r\n\r\n```\r\n/Users/era/Library/Android/sdk/ndk-bundle/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: tensorflow/contrib/makefile/downloads/nsync/builds/armeabi-v7a.android.c++11/nsync.a: malformed archive header name at 8\r\n/Users/era/Library/Android/sdk/ndk-bundle/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: tensorflow/contrib/makefile/downloads/nsync/builds/armeabi-v7a.android.c++11/nsync.a: malformed archive header name at 8\r\n/Users/era/Library/Android/sdk/ndk-bundle/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: fatal error: tensorflow/contrib/makefile/downloads/nsync/builds/armeabi-v7a.android.c++11/nsync.a: attempt to map 60 bytes at offset 67800 exceeds size of file; the file may be corrupt\r\ncollect2: error: ld returned 1 exit status\r\nmake: *** [.../tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1\r\n```\r\n\r\nNDK version 15c, Android Studio 3.0, macOS High Sierra 10.13.1", "comments": ["Also fails on `master`", "Searching the Internet, this type of error seems to happen quite rarely in general. Could you check if that nsync.a file exists? See https://github.com/openframeworks/openFrameworks/issues/4869#issuecomment-186440569 Were those files built with a different compiler? See [this question](https://stackoverflow.com/questions/13508332/how-to-import-non-standard-library-packages-use-gccgo).", "`nsync.a` exists. Everything was compiled with the NDK, not with a different compiler (unless one of the build scripts does that internal).\r\n\r\nAs I said, I only run the build normally with setting the `NDK_ROOT`. That's it.\r\n\r\n    NDK_ROOT=\"$HOME/Library/Android/sdk/ndk-bundle\" bash $MAKEFILE_DIR/build_all_android.sh", "I am also facing the same issue. Using the instructions on [README for Makefile](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#android). \r\n\r\nConfiguration:\r\nMacOS Sierra 10.12.6\r\nAndroid NDK r15c\r\n\r\n```android-ndk-r15c/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: tensorflow/contrib/makefile/downloads/nsync/builds/armeabi-v7a.android.c++11/nsync.a: malformed archive header name at 8\r\nandroid-ndk-r15c/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: tensorflow/contrib/makefile/downloads/nsync/builds/armeabi-v7a.android.c++11/nsync.a: malformed archive header name at 8\r\nandroid-ndk-r15c/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: fatal error: tensorflow/contrib/makefile/downloads/nsync/builds/armeabi-v7a.android.c++11/nsync.a: attempt to map 60 bytes at offset 68372 exceeds size of file; the file may be corrupt```\r\n\r\nAny resolution to this?", "Any update on this issue? I've exact same issue and `build_all_android.sh` never completes. I've tried changing NDK versions, tweaking some parts of the makefile, but this still persists.\r\n\r\nNot really sure where is the root of the problem at all, or how to even start figuring out if the problem is due to Makefile, or the script to build or something else entirely?", "I didn't encounter issues when building for `arm64-v8a`.", "It is the bug in tensorflow/contrib/makefile/compile_nsync.sh\r\nneed to override AR with android's AR , \r\n makefile='\r\n                        AR := ${NDK_ROOT}/toolchains/'\"$toolchain\"'/prebuilt/'\"$android_os_arch\"'/bin/'\"$bin_prefix\"'-ar\r\n                        CC=${CC_PREFIX} \\\r\n                           ${NDK_ROOT}/toolchains/'\"$toolchain\"'/prebuilt/'\"$android_os_arch\"'/bin/'\"$bin_prefix\"'-g++", "@keyongyu That brings to about 50 different\r\n`error: invalid argument '-std=c++11' not allowed with 'C/ObjC'`", "I was basically able to remove the error after removing a particular nsync build folder corresponding to the architecture I was trying to build. For example, I had problem with armeabi and so, I removed `./tensorflow/contrib/makefile/downloads/nsync/builds/armeabi.android.c++11` folder and it built fine after that for that arch\r\n\r\nAlso, btw, used r14 of NDK.", "I would be delighted if Google just for ONCE released a TF version that doesn't break anything on mobile. TF has been a non-stop self patching project for me on mobile since 1.0. Every new release just breaks about everything.", "Can confirm @keyongyu s solution does work. Overriding `AR` resolves this for me.", "Thanks @keyongyu. The fix works for me too", "@drpngx @jart when can we expected to see this be promoted to master?", "feel free to submit a PR and we can take a look", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "The same issue, any patches with it?", "This is another method I have confirmed. \r\nFirst, just compile it through build_all_android.sh script, then above err msg will print on the terminal.\r\nSecond, just execute the script \"compile_nsync.sh\" in the makefile dir, a perfect libnsync.a and nsync.a will be there.\r\nFinally,  we can link three libs: tensorflowe.a, protobuf.a, libnsync.a.\r\nGod be with you\uff01", "I suggest we try clang++, llvm, llvm-ar", "So far the best strategy is to build several times and comment out various sections of the build, which allows me to get the nsync built, then use it in the tensorflow make call. I think OSX is relatively hosed at the moment. If I come up with a fix I will post it or create a pull request. \r\n\r\nWhen are we moving to CMake?", "thansk! @keyongyu I follow your step.it works!\r\nfirst,I compile the build_all_android.sh,then I got the same error. then I remove nsync builds armeabi-v7a.android.c++11 folder ,update the compile_nsync.sh,  cover build_all_android.sh about dowloading and front of nsync. Finally compile build_all_android.sh again.here are my bash.\r\nsudo NDK_ROOT=/Users/a1707001/android-ndk-r14b sh tensorflow/contrib/makefile/build_all_android.sh\r\nbtw my tensorflow version is v1.4.0 ,ndk version is r14b.", "@drpngx Can you give a table of tf version and NDK version, when using tf.contrib.makefile?", "Hi @eaigner ,\r\nWe see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/14186\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/14186\">No</a>\n"]}, {"number": 14185, "title": "memory leak with tf.image.encode_jpeg", "body": "I encountered an issue with tf.image.encode_jpeg. I got a lot of images to preprocess. The code that caused the problem is like this:\r\n\r\nfor imagefile in image_list:\r\n  im = cv2.imread(imagefile)\r\n  image_data = tf.image.encode_jpeg(im, format='rgb')\r\n\r\nThe memory usage kept increasing as image.encode_jpeg was called. Eventually the 256G mem server ran out of memory.", "comments": ["So did I miss anything when called tf.image.encode_jpeg? ", "In regular TensorFlow, calling `tf.image.encode_jpeg()` (or any TensorFlow op function) in a loop will append it to a graph. Furthermore, each value of `im` will be embedded in the graph as a constant. There are a few options here:\r\n\r\n* Try the recently announced [eager execution](https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html) mode, which makes code like this work the way you would expect, and prevents your code from leaking memory.\r\n\r\n* Create a `tf.data` pipeline to do the conversion:\r\n\r\n  ```python\r\n  def generator():\r\n    for imagefile in image_list:\r\n      yield cv2.imread(imagefile)\r\n\r\n  dataset = tf.data.Dataset.from_generator(generator, tf.uint8).map(tf.image.encode_jpeg)\r\n  ```\r\n\r\n* Create a single `tf.image.encode_jpeg()` op that takes its input from a `tf.placeholder()` and feed that placeholder with each image.", "@mrry Thanks a lot! I will try your suggestions."]}, {"number": 14184, "title": "memory leak with tf.image.encode_jpeg", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing as duplicate of #14185."]}, {"number": 14183, "title": "improve production condition of debug log with VLOG", "body": "This patch improve a debug logging **VLOG** .\r\n\r\nNew environment variable ```TF_CPP_MAX_VLOG_LEVEL``` is added to suppress log production with VLOG.\r\nif ```TF_CPP_MAX_VLOG_LEVEL``` is set, VLOG only outputs log lower level than it.\r\n\r\nFor example, if an user sets environment variable ```TF_CPP_MAX_VLOG_LEVEL=2```, you can see the log only lower level than 2. And you can't see the log with level 1.\r\n\r\n### Use case:\r\n\r\n1. ```TF_CPP_MAX_VLOG_LEVEL=2``` and ```TF_CPP_MIN_VLOG_LEVEL=1```\r\n   * ```VLOG(1)``` and ```VLOG(2)``` will output log\r\n2. ```TF_CPP_MIN_VLOG_LEVEL=1```\r\n   * Only ```VLOG(1)``` will output\r\n3. ```TF_CPP_MAX_VLOG_LEVEL=3```\r\n   * ```VLOG(3)``` , ```VLOG(4)``` ...  will output log", "comments": ["Can one of the admins verify this patch?", "I'm not sure I understand the idea here -- you can now limit logging to see only, say, warning, errors, and fatals. With this new variable, you'd be able to restrict it to only show warnings, but not errors or fatals? \r\n\r\nWhat's the use case for this?", "VLOG is always output as INFO log level, so I think VLOG's level (1, 2 ...) is different from LOG's level (info, warning, errors, fatal).\r\nAnd there is highest level VLOG that is not important compared to the lower level.\r\nTherefore, it is useful to limit output log level from VLOG.", "I still don't understand why one would want both a minimum and a maximum. ", "I want to filter the output log with this improvement.\r\nthere is no method to filter the output log by a log level.\r\n\r\nbut Instead, i think that it is also good to display a log level on VLOG output.", "I'm sorry, I must be missing something really obvious -- there already is a minimum vlog level, this PR only adds a maximum. I don't understand why that is useful.", "Yes, I know there is a minimum VLOG level already.\r\nBut in the current interface,  I can not recognize what VLOG level is.\r\nIn order to recognize or filter the VLOG level, I added TF_CPP_MAX_VLOG_LEVEL which is the one of way to realize it.", "@nutti I think the intent of vlog is to output anything below that level. Having the ability to suppress levels at lower verbosity is probably not something we want, unless I am misunderstanding something.", "@drpngx \r\n\r\nYes, you are right. And I understand about the intention.\r\nBut the current VLOG does not output VLOG level, so I could not recognize what VLOG level is output.\r\nSo, I proposed to suppress the lower log level to recognize it. Of course, this is the best way to solve this problem.\r\n\r\nInstead, if you approve to add VLOG level to the log message, it can also solve the problem too.\r\n\r\nfor example:\r\n\r\ncode:\r\n```\r\nVLOG(2) << \"Debug\";\r\n```\r\n\r\nVLOG output(V2 will be added):\r\n2017-12-27 21:26:43.712063: I **V2** tensorflow/core/common_runtime/executor.cc:1557] Debug\r\n\r\nBut I did not which method is better.", "The code generating the actual warning message is in tensorflow/core/platform/default/logging.cc:78. Currently all VLOG messages are generated with INFO severity. Since the VLOG numbers don't map cleanly to INFO/WARNING etc., you'd have to add another field to LogMessage, set that field from the constructor used by VLOG, and then output what it contains. Adding this information would make more sense.\r\n\r\n", "Thanks for the instruction.\r\n\r\nI fixed my code to output VLOG level as shown previous comment.\r\nPlease check this revision.", "VLOG means something else, it's for `VLOG(x)` macros. In any case, I don't think we want both min and max. The max should include everything below it.", "Yes, I understood your intention.\r\n\r\nSo, I discarded previous commit which adds TF_CPP_MAX_VLOG_LEVEL.\r\nInstead, I added VLOG level to VLOG messages as mentioned before.\r\n\r\nThis will enable to distinguish which VLOG level is.\r\nAnd I also think it will be better way than adding max level.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks! While this seems OK, I'd rather we didn't change the log format to be different from what we have in glog and our internal versions.", "OK, thanks again for the feedback.\r\nInstead, how about to add VLOG Level to the tail or the head of messages?\r\nI only want to recognize what VLOG level it is, and it is boring to check VLOG level by using ```TF_CPP_MIN_VLOG_LEVEL``` repeatedly.\r\n\r\nExample 1:\r\n\r\n```\r\n2017-12-27 21:26:43.712063: I tensorflow/core/common_runtime/executor.cc:1557] Debug (VLOG:2)\r\n```\r\n\r\nExample 2:\r\n\r\n```\r\n2017-12-27 21:26:43.712063: I tensorflow/core/common_runtime/executor.cc:1557] (VLOG:2) Debug\r\n```", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@drpngx Do you think a change to the format as proposed (prefix, or suffix to the log message, not touching the [] part) would be acceptable? \r\n\r\n@nutti It would be nice to get information about VLOG level in the message, but log output is often processed by script etc., so it is very easy to break users with any change to it. ", "I guess `Example 2` will not break parsers. But now it makes it extra non-formalized information. I still don't think it's useful enough to change it for everyone.", "Thank you for your feedback.\r\n\r\nI make sense about the effects of the change of VLOG format.\r\nHow about the addition of the output switch parameter instead?\r\n\r\nFor example, environment variable ```TF_CPP_OUTPUT_VLOG_LEVEL``` is a switch parameter in this case.\r\nIf you specify ```TF_CPP_OUTPUT_VLOG_LEVEL=TRUE```, you will get the log with VLOG level.\r\nIf you don't specify ```TF_CPP_OUTPUT_VLOG_LEVEL```, VLOG level will not output.\r\nThis method will not break the exist users.\r\n\r\nIs there any other idea to output VLOG level?\r\nOr perhaps, this idea will not be acceptable?", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Ok, I will close this PR. It is truly regrettable that we have two separate and barely compatible logging mechanisms. I'm not sure whether this may change with absl? \r\n\r\n@yilei will absl-py logging use absl C++ logging, and provide a single mechanism for verbosity?", "Yes, we will make absl-py logging use absl C++ logging once it's released. [`logging.set_verbosity`](https://github.com/abseil/abseil-py/blob/ea8c4d2ddbf3fba610c4d613260561699b776db8/absl/logging/__init__.py#L259) or the -v/--verbosity flags are used to control verbosity.", "Thanks. I got it.\r\nI'm looking forward to release new verbosity mechanism."]}, {"number": 14182, "title": "Creating a specific 3.6 binary for Linux", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: \r\n- **Python version**: Python3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: -NA-\r\n- **CUDA/cuDNN version**: -NA-\r\n- **GPU model and memory**: -NA-\r\n- **Exact command to reproduce**: import tensorflow as tf\r\n\r\n**Environment capture text:**\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1437609/tf_env.txt)\r\n\r\nYou can obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nThis command also results in the same error.\r\n/home/raju/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n\r\n### Describe the problem\r\nWhen importing tensorflow , I get this error. I found some information on \"Feature request: nightly build for python 3.6 #12935\" --\"Yes, we unfortunately copy the 3.5 binary for 3.6 I'll look into creating a specific 3.6 binary for Linux.\"\r\n\r\n### Source code / logs\r\n$import tensorflow as tf\r\n**result is** \r\n/home/raju/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n\r\n", "comments": ["same issue on MacOS 10.12.6", "same problem on Linux Ubuntu 16.04", "@gunan : Do we have any updates on the resources we have to support Python 3.6 releases?", "@av8ramit looks like there were divergences which prevent us from using the same pip packages on python 3.5 and 3.6\r\nCan we modify our builds to also build 3.6 version from scratch?", "Same issue on Linux CentOS 7", "Same issue after upgrade Tensorflow from 1.3 to 1.4 on Mac High Sierra 10.13.1 using Python 3.6.2.\r\nTensorflow still works but give the above warning which is \"RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\"", "FWIW, python3.5 still works", "ANything that can be changed here?", "Same problem using virtualenv after upgrading from Ubuntu 17.04 to 17.10. \r\n`/home/wiebe/virtual/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)`", "Same on Ubuntu 14.04", "Same issue on Fedora 26 with Python 3.6.2.", "Same issue on OSX 10.12.5 with Python 3.6.3", "Same issue on OSX with Python 3.6.2", "Only posting because I didn't see my environment mentioned:\r\nClean 17.10 Ubuntu Server install on Python 3.6.3 (native `pip install tensorflow-gpu`)\r\nOutputs:\r\n```\r\n/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n```", "I have tried both 3.5 and 3.6 (macOS High Sierra). \r\n3.5 works perfect while 3.6 gives warns.\r\nWhile 3.6 still works.\r\n\r\nHope this helps.", "same problem on Linux Ubuntu 17.10\r\n\r\nClean 17.10 Ubuntu Server install on Python 3.6.3 (native pip install tensorflow)", "Same issue on OSX 10.12.6 with Python 3.6.3(virtualenv installation)\r\n\r\n>>>> import tensorflow as tf\r\n/Users/zhanghao/virtualenv/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)", "Same issue using this Docker file `https://github.com/christiangda/kerasvideo/blob/master/Dockerfile` \r\n\r\nerror:\r\n```\r\nimport tensorflow as tf\r\n\r\n/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n```", "Hi,\r\nCan you let me know if this has to be treated as a warning and ignored?\r\n**Furthermore  the error is**\r\nfrom PyQt5 import QtCore, QtGui, QtWidgets\r\nImportError: dlopen: cannot load any more object with static TLS\r\n\r\n**See the Traceback below**\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 51, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/home/raju/anaconda3/envs/tensorflow/models/research/object_detection/builders/model_builder.py\", line 29, in <module>\r\n    from object_detection.meta_architectures import ssd_meta_arch\r\n  File \"/home/raju/anaconda3/envs/tensorflow/models/research/object_detection/meta_architectures/ssd_meta_arch.py\", line 31, in <module>\r\n    from object_detection.utils import visualization_utils\r\n  File \"/home/raju/anaconda3/envs/tensorflow/models/research/object_detection/utils/visualization_utils.py\", line 24, in <module>\r\n    import matplotlib.pyplot as plt\r\n  File \"/home/raju/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 113, in <module>\r\n    _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()\r\n  File \"/home/raju/anaconda3/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 60, in pylab_setup\r\n    [backend_name], 0)\r\n  File \"/home/raju/anaconda3/lib/python3.6/site-packages/matplotlib/backends/backend_qt5agg.py\", line 16, in <module>\r\n    from .backend_qt5 import (\r\n  File \"/home/raju/anaconda3/lib/python3.6/site-packages/matplotlib/backends/backend_qt5.py\", line 18, in <module>\r\n    import matplotlib.backends.qt_editor.figureoptions as figureoptions\r\n  File \"/home/raju/anaconda3/lib/python3.6/site-packages/matplotlib/backends/qt_editor/figureoptions.py\", line 20, in <module>\r\n    import matplotlib.backends.qt_editor.formlayout as formlayout\r\n  File \"/home/raju/anaconda3/lib/python3.6/site-packages/matplotlib/backends/qt_editor/formlayout.py\", line 56, in <module>\r\n    from matplotlib.backends.qt_compat import QtGui, QtWidgets, QtCore\r\n  File \"/home/raju/anaconda3/lib/python3.6/site-packages/matplotlib/backends/qt_compat.py\", line 137, in <module>\r\n    **from PyQt5 import QtCore, QtGui, QtWidgets\r\nImportError: dlopen: cannot load any more object with static TLS**\r\n\r\nPls let me know ways to resolve this error. Or will it be resolved only when the warning is resolved!!", "Same issues on Ubuntu 16.04 64bit virtualenv python3", "Does it break stuff or can I ignore the warning?", "Quite strange since the prior version worked fine with 3.6 (and it's packages), so it would seem that this should be a pretty simple fix?", "same issue with the just released 1.4 version...", "Just to add to what @dsitnik said,  simple hello world to reproduce on Fedora 26 venv \r\n\r\n```\r\n$ cat requirements.txt \r\nbleach==1.5.0\r\nenum34==1.1.6\r\nhtml5lib==0.9999999\r\nMarkdown==2.6.9\r\nnumpy==1.13.3\r\nprotobuf==3.4.0\r\nsix==1.11.0\r\ntensorflow==1.4.0\r\ntensorflow-tensorboard==0.4.0rc2\r\nWerkzeug==0.12.2\r\n```\r\n\r\n```\r\n$ python --version\r\nPython 3.6.2\r\n```\r\n\r\n```\r\n$ cat app.py \r\nimport tensorflow as tf\r\n\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsession = tf.Session()\r\nprint(session.run(hello))\r\n```\r\n\r\n```\r\n$ python app.py \r\n/usr/lib64/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n2017-11-07 16:41:00.517173: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\r\nb'Hello, TensorFlow!'\r\n```\r\n", "Same issue on OSX 10.11.6, in an Anaconda virtual environment with Python 3.6.3 and attempting upgrade from TensorFlow  1.3 to 1.4. \r\nTemporarily routed around this issue by creating a new Anaconda environment with Python 3.5.4 and Tensorflow 1.4, which is working fine.", "same issue on MacOS high sierra\uff01how resolved it\uff01give me command \uff0cnot link\uff01", "Same issue on OSX 10.12.6 sierra,\r\n![image](https://user-images.githubusercontent.com/24262233/32562915-795d941c-c475-11e7-8f5f-314800f1c51e.png)\r\n\r\nthe version:\r\n![image](https://user-images.githubusercontent.com/24262233/32562950-9213c742-c475-11e7-9ade-84519916faa5.png)\r\n\r\nHope this helps.\r\n", "I am working on resolving both the Python3.6 binary as well as the git_version tag. ", "I took reference from \r\nhttps://github.com/tensorflow/tensorflow/issues/6533\r\n\r\n```\r\n$ python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl\r\nCollecting tensorflow==0.12.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl\r\n  Downloading https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl (38.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 38.4MB 33kB/s \r\nCollecting protobuf==3.1.0 (from tensorflow==0.12.0)\r\n  Downloading protobuf-3.1.0-py2.py3-none-any.whl (339kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348kB 1.4MB/s \r\nRequirement already up-to-date: six>=1.10.0 in ./virtualenvs/dl4cv/lib/python3.6/site-packages (from tensorflow==0.12.0)\r\nRequirement already up-to-date: numpy>=1.11.0 in ./virtualenvs/dl4cv/lib/python3.6/site-packages (from tensorflow==0.12.0)\r\nRequirement already up-to-date: wheel>=0.26 in ./virtualenvs/dl4cv/lib/python3.6/site-packages (from tensorflow==0.12.0)\r\nRequirement already up-to-date: setuptools in ./virtualenvs/dl4cv/lib/python3.6/site-packages (from protobuf==3.1.0->tensorflow==0.12.0)\r\nInstalling collected packages: protobuf, tensorflow\r\n  Found existing installation: protobuf 3.4.0\r\n    Uninstalling protobuf-3.4.0:\r\n      Successfully uninstalled protobuf-3.4.0\r\nSuccessfully installed protobuf-3.1.0 tensorflow-0.12.0\r\n\r\n$ python\r\nPython 3.6.3 (default, Oct  4 2017, 06:09:15) \r\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> \r\n\r\n$ python\r\nPython 3.6.3 (default, Oct  4 2017, 06:09:15) \r\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.37)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import keras\r\nUsing TensorFlow backend.\r\n>>> \r\n```", "Same issue on OSX 10.13.1 with Python 3.6.3", "same issue on osx 10.12.6 with Python 3.6.3\r\n```\r\n/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n```\r\n\r\nI someone has a solution I take it ! :) ", "setup a virtualenv with Python 3.5.0", "@aktivkohle We need binary for python 3.6 anyways", "same issue on macOS 10.13.1 with Python 3.6.3\r\n```\r\n/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n```", "got same issue on macOS 10.12.6. However, seem we can ignore this warning. Waiting for a fix!", "Same issue using Anaconda with python 3.6 on Ubuntu 16.04 when upgrading from tensorflow-gpu 1.3", "same issue on Ubuntu 17.10, python 3.6.\r\nCan we really ignore this warning ?", "Same issue on Anaconda, Ubuntu 16.04, Python 3.6, Tensorflow GPU", "Same issue on Anaconda, Ubuntu 16.04, Python 3.6, Tensorflow CPU", "Same issue. Going to try to come back to setup a virtualenv with Python 3.5", "I wish GitHub had a \"Me Too\" button... Anyways, me too.\r\n\r\nArch Linux, Pip env, and native GPU installation. Python 3.6.", "I see the same warning with Python 3.6 from conda. On Mac it's just a warning, and things seem to work OK. But on Linux I see that `import tensorflow` breaks `import scipy.special` in a weird way.\r\n```\r\npython -c 'import tensorflow as tf; import scipy.special'\r\n/d1/hfm/deil/software/anaconda/envs/image-style-transfer/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/d1/hfm/deil/software/anaconda/envs/image-style-transfer/lib/python3.6/site-packages/scipy/special/__init__.py\", line 648, in <module>\r\n    from ._ellip_harm import ellip_harm, ellip_harm_2, ellip_normal\r\n  File \"/d1/hfm/deil/software/anaconda/envs/image-style-transfer/lib/python3.6/site-packages/scipy/special/_ellip_harm.py\", line 7, in <module>\r\n    from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\r\nImportError: cannot import name '_ellipsoid'\r\n```\r\n\r\nI think my scipy is OK, at least this import works if I don't import tensorflow:\r\n```\r\npython -c 'import scipy.special'\r\n```\r\n\r\nScipy 1.0 was installed via Anaconda, tensorflow 1.4 via pip:\r\n```\r\n$ conda list\r\nscipy                     1.0.0            py36hbf646e7_0  \r\ntensorflow                1.4.0                     <pip>\r\n```", "Same issue on macOS 10.13 with Python 3.6.3\r\n\r\n`/Users/mcukingdom007/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)`", "Any workaround or timeline to solve this problem? Thanks a lot! ", "I am also facing the same issue on macOS 10.12.6 with Python 3.6\r\n`/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)`", "Maybe Tensorflow-1.4 does not support linux & macOS with Python 3.6, so there seems to be an error.\r\nif you downgrade from v1.4 to v1.3, then it does not cause an error.", "Same issue on macOS 10.13 with python 3.6.1", "I'm also having the same problem not anaconda install...just with pyenv and jupyter...on a macOS Sierra version 10.12.6 with python 3.6\r\n\r\nUsing TensorFlow backend.\r\n\r\n/Users/Create/.pyenv/versions/3.6.3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)", "Same issue on Ubuntu16.04.3 with python 3.6.3", "Switching to Python 3.5 has removed the warning. To do this, I basically had to remove python 3.6 from my machine completely since jupyter was still pointing towards 3.6 even when only installed in 3.5 packages.\r\n\r\nI followed [this answer](https://stackoverflow.com/questions/3819449/how-to-uninstall-python-2-7-on-a-mac-os-x-10-6-4/3819829#3819829) to remove python 3.6 from my machine (just replace all occurrences of 2.7 with 3.6 in that answer).\r\n\r\nI then made sure to set PYTHONPATH to 3.5 using\r\n`export PYTHONPATH=/Library/Frameworks/Python.framework/Versions/3.5/bin`\r\nThen removing 3.6 from PATH by\r\n`echo $PATH` then copy that entire path\r\n`export PATH=pasteInThePathAndRemovePython3.6`\r\n\r\nHopefully this helps in the mean time! Just remember that you'll have to reinstall all your packages for python 3.5!", "\"FIxed\" issue by forcing the installation of the 1.3.0 version of tensorflow\r\n```\r\n$~/Downloads$ pip3 install tensorflow==1.3.0\r\nCollecting tensorflow==1.3.0\r\n  Downloading tensorflow-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (43.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 43.6MB 42kB/s \r\nCollecting six>=1.10.0 (from tensorflow==1.3.0)\r\n  Using cached six-1.11.0-py2.py3-none-any.whl\r\nCollecting protobuf>=3.3.0 (from tensorflow==1.3.0)\r\n  Using cached protobuf-3.4.0-cp36-cp36m-manylinux1_x86_64.whl\r\nCollecting wheel>=0.26 (from tensorflow==1.3.0)\r\n  Using cached wheel-0.30.0-py2.py3-none-any.whl\r\nCollecting numpy>=1.11.0 (from tensorflow==1.3.0)\r\n  Using cached numpy-1.13.3-cp36-cp36m-manylinux1_x86_64.whl\r\nCollecting tensorflow-tensorboard<0.2.0,>=0.1.0 (from tensorflow==1.3.0)\r\n  Downloading tensorflow_tensorboard-0.1.8-py3-none-any.whl (1.6MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.6MB 1.2MB/s \r\nCollecting setuptools (from protobuf>=3.3.0->tensorflow==1.3.0)\r\n  Using cached setuptools-36.7.2-py2.py3-none-any.whl\r\nCollecting werkzeug>=0.11.10 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0)\r\n  Using cached Werkzeug-0.12.2-py2.py3-none-any.whl\r\nCollecting markdown>=2.6.8 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0)\r\nCollecting html5lib==0.9999999 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0)\r\nCollecting bleach==1.5.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0)\r\n  Using cached bleach-1.5.0-py2.py3-none-any.whl\r\nInstalling collected packages: six, setuptools, protobuf, wheel, numpy, werkzeug, markdown, html5lib, bleach, tensorflow-tensorboard, tensorflow\r\nSuccessfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9 numpy-1.13.3 protobuf-3.4.0 setuptools-36.7.2 six-1.11.0 tensorflow-1.3.0 tensorflow-tensorboard-0.4.0rc2 werkzeug-0.12.2 wheel-0.30.0\r\n$~/Downloads$ python3\r\nPython 3.6.3 (default, Oct  3 2017, 21:45:48) \r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import keras\r\nUsing TensorFlow backend.\r\n>>> \r\n```", "Great!!!", "TensorFlow v1.4 doesn't work with Python3.6.\r\nSo, we should use TensorFlow v1.3.\r\nThank you for your great information.", "This is the main guidance to installing which py version and tf version\r\nhttps://github.com/lakshayg/tensorflow-build\r\n", "I use Mac OS High Sierra.\r\nI tried to use TF v1.4 with Python3.6.3, but , RuntimeWarning is shown when I import TF.\r\nSo, if you use High Sierra , be careful.", "I am finding the GPU version (using Nvidia ) 1.4 works fine on Ubuntu Linux (py3.6) once past the warnings.. Have not run into other issues than the warnings..", "Here are working 3.6 wheels https://github.com/mind/wheels", "I have the same problem with clang 900.0.38,  python 3.6.3, tf 1.4.0. no idea how to fix it.", "## Here is a temporal \"fix\"\r\n\r\nFor anyone looking for a workaround we have several options:\r\n\r\n- Wait for an official binary to be released for Python 3.6\r\n- Ignore the warning, it seems to work.\r\n- Don't use tensorflow 1.4.0 instead use 1.3.0 `pip3 install tensorflow==1.3.0`.\r\n- Install an unofficial tensorflow binary compatible with Python 3.6 from [lakshayg/tensorflow-build](https://github.com/lakshayg/tensorflow-build) or from [mind/wheels](https://github.com/mind/wheels).\r\n\r\nExample in **Mac OS High Sierra** I do:\r\n\r\n```\r\n$ pip install https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.4.0-cp36-cp36m-macosx_10_12_x86_64.whl\r\n```\r\n\r\nNote: Be careful to choose the correct wheel for your platform and if it's with CPU or GPU support.", "Same issue on Ubuntu 16.04, Python 3.6.1, CPU version.\r\nForce install TensorFlow 1.3 can fix like @VictorGaiva said.", "When can we expect the official build of TF 1.4.0 for Python 3.6? The official docs reference `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp36-cp36m-linux_x86_64.whl` as the Python 3.6 version, but I am still getting the original warning.", "As a workaround (in virtualenv), we can use python 3.5 as below.\r\n\r\n**virtualenv --system-site-packages -p  /usr/bin/python3.5 ~/tensorflow/**\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n```\r\n\r\n\r\n", "In mac os ,this help me: python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl", "tf-nightly and tf-nightly-gpu now has a python3.6 binary built from scratch for Linux. \r\n1.5.0-dev20171206 and up\r\n\r\nThis should fix the warning", "Same issue on MacOS 10.11.6 with Python 3.6.1", "I installed tf_nightly_gpu-1.5.0.dev20171213-cp36-cp36m-manylinux1_x86_64.whl successfully.\r\nUbuntu17.10 x64.\r\nTensorflow is looking for cuda-9.0 but I have cuda-8.0 installed. does tf_1.5.0 works with cuda-8.0?\r\nI have cuda-8.0 installed. I can see it in the library path. Any help to resolve this issue will be much appreciated.\r\n\r\n![screenshot from 2017-12-15 01-44-45](https://user-images.githubusercontent.com/5642181/34030106-956b7260-e139-11e7-9f71-6f665a587cb1.png)\r\n\r\n![screenshot from 2017-12-15 01-42-30](https://user-images.githubusercontent.com/5642181/34030072-6fb10f58-e139-11e7-8697-3290121e340a.png)\r\n\r\n![screenshot from 2017-12-15 01-59-36](https://user-images.githubusercontent.com/5642181/34030630-cf648838-e13b-11e7-826d-3fe1888b0fce.png)\r\n\r\n\r\nFollowing is Traceback for error \r\n\r\nImportError                               Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n/usr/lib/python3.6/imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n/usr/lib/python3.6/imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\r\n     71 for some common reasons and solutions.  Include the entire stack trace\r\n     72 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 73   raise ImportError(msg)\r\n     74 \r\n     75 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\u200b\r\n", "Just installed python 3.6 on ubuntu 16.04. Then:\r\n\r\n~$ python3.6 -m pip install --user tensorflow\r\nCollecting tensorflow\r\n  Downloading tensorflow-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (41.2MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41.2MB 52kB/s \r\n\r\nand so on...\r\n\r\nThen:\r\n\r\n~$ ipython\r\nPython 3.6.3 (default, Oct  4 2017, 02:55:45) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow as tf\r\n/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n\r\nIn [2]: hello = tf.constant('Hello TensorFlow!')\r\n\r\nIn [3]: session = tf.Session()\r\n2017-12-15 15:53:35.523688: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n\r\nIn [4]: print(session.run(hello))\r\nb'Hello TensorFlow!'\r\n\r\n\r\nSo I'm going to assume that this works just fine and this warning will go away with 1.5 or perhaps a 1.4.2 version.\r\n\r\n\r\nBy the way, did not notice this warning in Windows7 using tensorflow-gpu version 1.4.0 with python 3.6.2", "Same issue on Ubuntu 16.04 LTS.", "Same issue.", "Same issue on Ubuntu 16.04 LTS.", "We are not offering a native python3.6 binary for mac anytime soon.\r\n\r\nAs for the cuda9 issue, tf-nightly is built from HEAD at master every night, and I believe the latest binaries require cuda9. Try using an older version of tf-nightly. (Maybe 1208 or something)\r\n\r\nWindows has a native python3.6 binary so you will not see this issue.\r\n\r\n", "Thanks @av8ramit \r\n\r\nFor those who have the (arbitrary) preference of only wanting to use official releases, the following is correct?\r\n- you'll need to wait for 1.5.x for this warning message to go away when using Python 3.6 on linux\r\n- in the meantime, you can use 1.4.1 with no problems, you'll just see the warning message", "The same issue appears while I use Anaconda 3-5.0.1-Linux-x86_64.sh(Python3.6) and TensorFlow 1.4.", "For what it's worth, the issue was discussed in a Hacker News thread about the 1.4 release (https://news.ycombinator.com/item?id=15647790). User allenlavoie responded:\r\n\r\n> It's just a small Cython-compiled utility throwing the warning, it works fine. Everything else is pure Python and so doesn't care about 3.5 vs 3.6. We will fix the warning, though (disclaimer: I work on TF and added fast_tensor_utils--sorry!).", "Dear Tensorflow QA team. Please add some unit tests!!! Happy NY", "You can create whl from source using following link steps. http://www.python36.com/install-tensorflow141-gpu/ . Let me informed if it worked.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Official releases now (1.5+) have a native python3.6 image build on Ubuntu 16.04. We do not build a python3.6 specific binary for mac. Windows has native 3.6 as well. ", "@av8ramit , I have 1.4.1 and you said that official releases *now* have .... yet,\r\n\r\nWhen I do:\r\n`python3.6 -m pip install --upgrade --user tensorflow`\r\n\r\nIt says that:\r\n`Requirement already up-to-date: tensorflow in ./.local/lib/python3.6/site-packages`", "You guys could get python3.6 specific binary for mac/linux/windows from here:\r\nhttps://pypi.python.org/pypi/tensorflow", "@av8ramit  I manually downloaded 1.5.0rc1 cp36 manylinux wheel and installed successfully. The warning message has disappeared when import tensorflow in python 3.6\r\n\r\nHowever, I think there is issue with pip. See below please. Do you need me to start a new issue, or am I just doing something wrong?\r\n\r\nWhen I do:\r\n`python3.6 -m pip install --upgrade --user tensorflow -vvv`\r\n\r\nIt does find the 1.5.0rc1 cp36 manylinux wheel, as well as rc0 and older version wheels (both rc and non-rc): \r\n```\r\nFound link https://pypi.python.org/packages/cd/e4/b2a8bcd1fa689489050386ec70c5c547e4a75d06f2cc2b55f45463cd092c/tensorflow-1.1.0-cp36-cp36m-manylinux1_x86_64.whl#md5=1f761290358dfb7fe4ec73140f4d282a (from https://pypi.python.org/simple/tensorflow/), version: 1.1.0\r\n...\r\nFound link https://pypi.python.org/packages/c6/1c/d64f9367cc3a50194e911ab9da865c2515cbc1fefa5e210ddebf85a61bac/tensorflow-1.2.0rc0-cp36-cp36m-manylinux1_x86_64.whl#md5=8a637f414bc16c86708c01f17c94f3f6 (from https://pypi.python.org/simple/tensorflow/), version: 1.2.0rc0\r\n...\r\nFound link https://pypi.python.org/packages/fe/7d/01c588ba060e5e7dffe64ab968745d143530c84da30296f9593f661ca27d/tensorflow-1.5.0rc0-cp36-cp36m-manylinux1_x86_64.whl#md5=6578302e0c843660464cc16ad45d6093 (from https://pypi.python.org/simple/tensorflow/), version: 1.5.0rc0\r\n...\r\nFound link https://pypi.python.org/packages/4e/50/87e3c6861957413deb5ccd62aee6cea85cc2eedcf6391183595191d3c87d/tensorflow-1.5.0rc1-cp36-cp36m-manylinux1_x86_64.whl#md5=501d5072275518056aa5a5f25a001015 (from https://pypi.python.org/simple/tensorflow/), version: 1.5.0rc1\r\n```\r\n\r\nbut at the end this does not register somehow with pip to be the latest version:\r\n```\r\nInstalled version (1.4.1) is most up-to-date (past versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.4.1)\r\nRequirement already up-to-date: tensorflow in ./.local/lib/python3.6/site-packages\r\n```\r\n\r\nI may be completely wrong, but it seems pip is not catching the versions with rc in the version number. I don't know if this is pip's error or tensorflow's error, but it seems to be an issue. Or am I wrong?", "@tylerlekang pip fetch the latest stable release by default (see https://pip.pypa.io/en/stable/reference/pip_install/#pre-release-versions) \r\nand the pypi package of tensorflow is still a release candidate (1.5.0rc1) \r\n\r\ntry using: \r\n`pip install 'tensorflow>=1.5.0rc1' `", "@attiasr  Oh! Well ... that's embarrassing!\r\n\r\nHow long does it take the rc to turn into stable, and are there usually any changes once released as rc? Well I guess I can answer my own question by fact that there has already been rc0 and rc1 of 1.5.0 .....\r\n\r\nThanks!", "@tylerlekang np\r\n", "python3.6 -m pip install https://github.com/lakshayg/tensorflow-build/raw/master/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl\r\n", "Thanks @Karthick333031 it works for me (MacOS 10.12.6)", "Running: `pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.10.0-py3-none-any.whl` solved the issue\r\n\r\nPython **3.6.5**\r\nTensorFlow **1.10.0** (Was previously **1.10.1**)\r\nMacOS **10.14**", "Thanks,\r\n\r\n @umang6891! It solved the problem for me. With the warning, my example hanged forever without failing, but now it runs. It looks like the problem is the version in pip.\r\n\r\n<img width=\"798\" alt=\"screenshot 2018-08-24 23 38 35\" src=\"https://user-images.githubusercontent.com/1203545/44615098-d741b080-a7f6-11e8-8548-8066e698ed5a.png\"> \ud83d\udc4d ", "@umang6891 's solution works good for me, and I am using a python3.6 virtualenv. "]}, {"number": 14181, "title": "Tensorflow or python having memory cleanup issues when using multiple models in iterative loop", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: Python 3.6.1 :: Anaconda 4.4.0 (64-bit)\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n\r\n== cat /etc/issue ===============================================\r\nLinux Bragi 4.10.0-37-generic #41-Ubuntu SMP Fri Oct 6 20:20:37 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"17.04 (Zesty Zapus)\"\r\nVERSION_ID=\"17.04\"\r\nVERSION_CODENAME=zesty\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 6.3.0-12ubuntu2) 6.3.0 20170406\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux Bragi 4.10.0-37-generic #41-Ubuntu SMP Fri Oct 6 20:20:37 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.8)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n\r\n### Describe the problem\r\nI am working on a tensorflow model which takes pretty much RAM. It is executed iteratively to process given tasks.\r\n\r\nHowever, with increasing time the whole process starts consuming more and more RAM although it should clean it up. This sounds like as if I'd keep data of one graph over the iterations, but I am almost sure that the graphs are cleanly separated.\r\n\r\nProblem\r\n-------\r\nI reduced the code to the following:\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    reps = 30\r\n    for i in range(reps):\r\n        with tf.Graph().as_default() as graph:\r\n            with tf.Session(graph=graph) as sess:\r\n                tf.constant(np.random.random((1000,1000,200,1)))\r\n\r\nI have 32GB RAM available, working on a ubuntu 17.04 with CPU Tensorflow 1.3. This will give following error message after about the 25th or 27th iteration:\r\n\r\n> terminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\n\r\nGiving the process some time after each iteration results in no improvement:\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    import time\r\n\r\n    reps = 30\r\n    for i in range(reps):\r\n        with tf.Graph().as_default() as graph:\r\n            with tf.Session(graph=graph) as sess:\r\n                tf.constant(np.random.random((1000,1000,200,1)))\r\n        time.sleep(1)\r\n\r\nHowever, it works if I force garbage collection invocation after each repetition:\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    import gc\r\n\r\n    reps = 30\r\n    for i in range(reps):\r\n        with tf.Graph().as_default() as graph:\r\n            with tf.Session(graph=graph) as sess:\r\n                tf.constant(np.random.random((1000,1000,200,1)))\r\n        gc.collect()\r\n\r\nQuestion\r\n--------\r\nNow I wonder why I need to force garbage collection to run even though tensorflow should have closed the session and de-referenced the graph object.\r\n\r\nBack to my original model I am not sure, yet, if the gc invocation actually helps. The memory usage grows pretty intense, especially when I am about to persist the model to disk.\r\n\r\nThanks for any insights.\r\n", "comments": ["First of all, thanks for filling in the the issue template with all the details and providing clear instructions to reproduce the problem. This is very helpful!\r\n\r\nI modified your snippet a bit to reduce iteration time and dump out memory stats inline (so avoid generating a new random numpy array on each iteration and I see issues even without creating a session):\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport resource\r\n\r\nreps = 30\r\nx = np.random.random((1000, 1000, 200, 1))\r\nfor i in range(reps):\r\n  with tf.Graph().as_default() as graph:\r\n    tf.constant(x)\r\n  print('Iteration ', i, ' maxrss: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\r\n```\r\n\r\nThe snippet above does show `maxrss` constantly increasing, while if I add the `gc` as you suggested:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport resource\r\nimport gc\r\n\r\nreps = 30\r\nx = np.random.random((1000, 1000, 200, 1))\r\nfor i in range(reps):\r\n  with tf.Graph().as_default() as graph:\r\n    tf.constant(x)\r\n  print('Iteration ', i, ' maxrss: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\r\n  gc.collect()\r\n```\r\n\r\nThen `maxrss` remains stable.\r\n\r\nThanks for the report.\r\n\r\n@allenlavoie has been looking at some other garbage generation and might have some thoughts.\r\n@skye might also know a bit about graph construction code.\r\n\r\n@allenlavoie @skye : Mind taking a look? Thanks!", "As long as we have reference cycles we're at the mercy of Python's garbage collection strategy. We could add some explicit `gc.collect()` calls in places (when popping the graph stack?), but for graph mode in TF 1.x I think that's the best we can do.\r\n\r\n`graph.get_operations()[0].graph` is one definite reference cycle. I don't think this is something we can solve with weakref for graph mode while maintaining API compatibility; we're pretty explicit about holding on to an operation being enough to execute it (needs the graph) and holding on to the graph being enough to execute it (needs the operation).\r\n\r\nSo eager mode may help, where we will (eventually) try to avoid most reference cycles and so will hopefully not need the garbage collector much. But there are definitely reference cycles there too at the moment.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Assigning to @allenlavoie, feel free to re-assign.", "I don't think this bug has a reasonable solution, so I'm going to close.\r\n\r\nI don't want to run gc.collect() manually. We make Graphs for Defuns (and therefore Dataset map_funcs) and every time a ResourceVariable is created. It requires a sweep through all objects and could slow down graph building significantly.\r\n\r\nIt is [possible to allow a graph's memory to be freed](https://github.com/tensorflow/tensorflow/blob/8a87518ae7074d5a0da779089e7024cd0920bba4/tensorflow/python/ops/resource_variable_ops.py#L77) without running the garbage collector, but it's sufficiently niche that I don't think it warrants a new \"manual graph delete\" API (but if someone wants to factor that into python.util as a non-public-API utility I'm happy to review a pull request).\r\n\r\nOtherwise eager execution allows continually redefining what gets executed without creating work for the garbage collector.\r\n\r\n", "I ran into this issue when I tried to run a bunch of experiments. So I tried calling `gc.collect()` manually with `sess.close()` and printing out `resource.getrusage(resource.RUSAGE_SELF).ru_maxrss` it just keeps increasing...\r\n\r\n"]}, {"number": 14180, "title": "error during install.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows10\r\n- **TensorFlow installed from (source or binary)**: anaconda 5.0.0\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\ni'm trying to install tensorflow through anaconda 5.0.0\r\nand i got this lines and i can't finish my install. plz help me\r\n\r\n### Source code / logs\r\nException:\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\commands\\install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\req\\req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\req\\req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\req\\req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\wheel.py\", line 323, in clobber\r\n    shutil.copyfile(srcfile, destfile)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\shutil.py\", line 121, in copyfile\r\n    with open(dst, 'wb') as fdst:\r\nPermissionError: [Errno 13] Permission denied: 'C:\\\\ProgramData\\\\Anaconda3\\\\Lib\\\\site-packages\\\\wheel\\\\archive.py'\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I had the same problem and solve it by closing spyder software before pip install"]}, {"number": 14179, "title": "tensorflow model convert to other dl tools?", "body": "are there some tools that can convert tensorflow model to caffe\u3001mxnet or other dl tools?", "comments": ["While we (the TensorFlow maintainers) do not have the bandwidth to provide such tools in the short term, we'd be happy to see community development of such tools.\r\n"]}, {"number": 14178, "title": "what is this error stands for.?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing as perhaps this issue was accidentally filed? (There is no information included - so I'm not sure what \"error\" is being talked about :)"]}, {"number": 14177, "title": "Getting wrong value with placeholder", "body": "### System information\r\n\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo, I have used code which is on the [link](https://www.tensorflow.org/get_started/get_started). \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\nInstalling with native pip (tensorflow-gpu)\r\n- **TensorFlow version (use command below)**:\r\n'1.3.0'\r\n- **Python version**: \r\n Python 2.7.12\r\n('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n- **CUDA/cuDNN version**:\r\nCUDA : release 8.0, V8.0.61\r\ncuDNN: CUDNN_MAJOR      6\r\n- **GPU model and memory**:\r\nGeForce 940MX \r\nMemory: 2GB\r\nDriver Version: 384.90\r\n\r\n\r\n\r\n### Describe the problem\r\nWhen I run at below code, I think i am getting the wrong result.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\na = tf.placeholder(tf.float32)\r\nb = tf.placeholder(tf.float32)\r\nadder_node = a + b\r\nprint(sess.run(adder_node, {a: 3, b: 4.5}))\r\nprint(sess.run(adder_node, {a: [1, 3], b: [2, 4]}))\r\n```\r\n\r\n> **My Output :**\r\n>  **3.0**\r\n> **[ 1. 3.]**\r\n\r\n> **I think the true result should be;**\r\n> **7.5**\r\n> **[ 3. 7.]**\r\n> \r\n\r\n### Logs\r\nAlso, when i type `sess = tf.Session()` , i am getting that output\r\n> 2017-11-02 12:09:04.184601: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-11-02 12:09:04.184689: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-11-02 12:09:04.184717: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-11-02 12:09:04.184753: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-11-02 12:09:04.184805: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-11-02 12:09:04.368504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2017-11-02 12:09:04.368855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\n> name: GeForce 940MX\r\n> major: 5 minor: 0 memoryClockRate (GHz) 1.2415\r\n> pciBusID 0000:01:00.0\r\n> Total memory: 1.96GiB\r\n> Free memory: 1.53GiB\r\n> 2017-11-02 12:09:04.368870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \r\n> 2017-11-02 12:09:04.368874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \r\n> 2017-11-02 12:09:04.368882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0)\r\n", "comments": ["Hmm....I'm unable to reproduce the error using TensorFlow 1.3.0 (I see the correct outputs of `7.5` and `[3, 7]`. The outputs you see make sense if the line was `adder_node = a` instead of `adder_node = a + b`!\r\n\r\nI don't have the same driver/GPU as you, though I doubt those would be to blame. But just to be extra sure, do you see the same errors when using just the CPU, say with:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\nwith tf.device('/cpu:0'):\r\n  a = tf.placeholder(tf.float32)\r\n  b = tf.placeholder(tf.float32)\r\n  adder_node = a + b\r\nprint(sess.run(adder_node, {a: 3, b: 4.5}))\r\nprint(sess.run(adder_node, {a: [1, 3], b: [2, 4]}))\r\n```\r\n\r\nThe log output you mentioned above is just informational logs. I didn't see any concerning notes in that.\r\n", "@asimshankar Thank you for your interest. When i run over CPU with at above code, i got the true result. `7.5` and `[3, 7]`. ", "Hmm...that's weird.\r\n\r\nDo you see the same problem with TensorFlow 1.4.0? Is it possible for you to try this on a different machine (specifically, one with a different driver/CUDA version/GPU or something)? Or, does upgrading CUDA/the NVIDIA driver make any difference?", "@asimshankar  . I have also tried with Python 3.5.2 and Tensorflow '1.4.0' which is installed with pip3 tensorflow - gpu. I got same wrong results. `3.0 [1. 3.]`  I think the problem is related to GPU, it is old and has small memory. Thank you for your all effort."]}, {"number": 14176, "title": "change set_tf_cunn_version to set_tf_cudnn_version", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}]