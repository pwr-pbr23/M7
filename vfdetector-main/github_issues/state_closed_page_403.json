[{"number": 41849, "title": "Exception: TensorFlow Lite currently doesn't support control flow ops: Merge, Switch.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):1.14\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2020-07-29 15:27:21.979509: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1402 operators, 2593 arrays (0 quantized)\r\n2020-07-29 15:27:22.011911: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1290 operators, 2425 arrays (0 quantized)\r\n2020-07-29 15:27:22.054821: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1290 operators, 2425 arrays (0 quantized)\r\n2020-07-29 15:27:22.098145: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1062 operators, 2194 arrays (0 quantized)\r\n2020-07-29 15:27:22.139738: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 1062 operators, 2194 arrays (0 quantized)\r\n2020-07-29 15:27:22.172354: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1062 operators, 2194 arrays (0 quantized)\r\n2020-07-29 15:27:22.219537: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 1228800 bytes, theoretical optimal value: 921600 bytes.\r\n2020-07-29 15:27:22.231094: E tensorflow/lite/toco/toco_tooling.cc:456] TensorFlow Lite currently doesn't support control flow ops: Merge, Switch.\r\nTraceback (most recent call last):\r\n  File \"/home/ps/anaconda3/envs/rknn/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ps/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ps/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/ps/.local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ps/.local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/ps/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: TensorFlow Lite currently doesn't support control flow ops: Merge, Switch.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["@apupu1 \r\nThis has already been reported and resolved, please refer to [this link](https://github.com/tensorflow/tensorflow/issues/41547#issuecomment-660781354).", "@Saduf2019 thanks"]}, {"number": 41848, "title": "GPU Installation instructions may need to be updated", "body": "I'm following the [GPU install instructions](https://www.tensorflow.org/install/gpu) for Ubuntu 18.04 and getting the following after running this:\r\n```\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-1 \\\r\n    libcudnn7=7.6.5.32+cuda10.1  \\\r\n    libcudnn7-dev=7.6.5.32-1+cuda10.1\r\n```\r\nI'm getting this:\r\n```\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Version '7.6.5.32+cuda10.1' for 'libcudnn7' was not found\r\n```\r\n `7.6.5.32+cuda10.1` may need to be changed to  `7.6.5.32-1+cuda10.1`  since the latter works.\r\n", "comments": ["Same\r\n", "There is 1 missing in libcudnn7 package, following works\r\n\r\nlibcudnn7=7.6.5.32-**1**+cuda10.1  \r\n"]}, {"number": 41847, "title": "Timeseries example is a Misnomer. It is NOT a \"timeseries\" it is rather a simple series.", "body": "https://www.tensorflow.org/tutorials/structured_data/time_series#part_2_forecast_a_multivariate_time_series\r\n\r\nI do not understand how this can be time series ? The data is \"equidistant\" with each other. And the time itself is not considered in predicting the values, but rather just as a series. When you include TIME AS A VECTOR, i would accept that it is a Timeseries.\r\n\r\nThe title is MISLEADING and also we need an example for vectorizing Time... for an actual Timeseries.", "comments": ["I do believe the docs go with following [definition ](https://www.merriam-webster.com/dictionary/time%20series) of the term, or close variation on it.", "Wow... are you saying that You go with Merriam Webster but not mathematical definition ? They are wrong. IT is a sequential data. If the data is time ordered or indexed only then it can be Time series. And Webster is not a definitive source for mathematical definitions. I will write to Webster about this discrepancy. \r\nMore over when \"Time\" itself is not considered how could this example be \"Timeseries\" ? IT is that simple actually.\r\nhttps://en.wikipedia.org/wiki/Time_series\r\n\r\nAre we stuck up on the definition or looking to have a future added ? I am confused here.  If you say Timeseries, how do i take a particular time and predict ? I can do this for a sequential data, for example a next step in the sequence.\r\n\r\nI want Tensor team to clearly state if they mean any sequential data is a \"Timed\" series data... And if they come around and say yes.. i have no words for it.", "I'm not sure if either of us misinterpreted the definition. \r\nBut I don't see how is what docs describe differs from either Webster or wiki. \r\n\r\nThe dictionary definition says:\r\n> a set of data collected sequentially usually at fixed intervals of time\r\nIt's hard not to see the implied sequence of the data points.  \r\n\r\nThe wiki definition on the other hand: \r\n\r\n> A time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\r\n\r\nAgain sequence, arguably more clearly stated compared to Webster. \r\nDocs example is rather straightforward too, in that it describes:\r\n\r\n> This tutorial uses a weather time series dataset recorded by the Max Planck Institute for Biogeochemistry.\r\n> \r\n> This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by Fran\u00e7ois Chollet for his book Deep Learning with Python.\r\n\r\nIf it's good enough for Chollet, the guy behind Keras, it's good enough for me. \r\n\r\nFinally the Statistics Engineering handbook [defines](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc41.htm) time series as:\r\n\r\n> Definition of Time Series: An ordered sequence of values of a variable at equally spaced time intervals.\r\n\r\nWhich again conforms with what I seen in provided example. \r\n", "@summa-code As mentioned in the above comment, the dataset that has been used here is a timeseries dataset. I don't really see a problem in here.", "@jpodivin ... I say WOW for the examples of the definition. Here is my point. I am not questioning about the DATA that is timeseries. But fortunately or unfortunately, they all fall under equi-distant timeseries... which in other words, simply a sequential data. But for the sack of the problem for \"Time\" series, the *solution* presented seems to me simply for a sequential data, not taking TIME into consideration.\r\nAnyhow, i am glad that Tensorflow team seems to have updated their Timeseries example. I have not gone through them fully. But i will in the coming days and see if it meets the criteria. Timeseries problems are not perfect, but i like to see some broader approach involving actual time.", "@summa-code As gowthamkpr said the data set is fine as it is. And I fail to see any point to prolonging this conversation. \r\nI am glad that you found my examples, which were numerous and sourced, to be sufficient and well sourced. \r\n\r\nIf you have further issues with the documentation I can only recommend providing your own sources in the first place, it will help your argument since, all things taken into account, the burden of proof does rest on you. ", "@jpodivin My statements are very clear, there is no confusion. The DATA does have times, but they are periodic equidistant time. The solution presented was simply for a sequential data, does not include time. Think of IoT data that comes out based on random timed event, and they are not equidistant time. They can not be simply be considered as series, because they happen at random times, so they are Timed series than a simple series. But the new example that is published seems to consider time as a cyclical feature. I will go over them and see if it makes sense. I still think that Time should be considered as a linear increasing value than cyclical event. \r\n\r\nI could not run the latest notebook. Not sure if they have tested these before publishing,\r\nhttps://github.com/tensorflow/tensorflow/issues/41987\r\nFiled a bug again, let me see if they fix this. ", "And then i could not run on an IoT data that does not fit in memory. These need to be converted to tensor graphs. :-(\r\nSo i can only test with simple sample code with this.", "@summa-code,\r\nIf the Data is huge, you can [Create_TF_Dataset](https://www.tensorflow.org/tutorials/structured_data/time_series#4_create_tfdatadatasets). Please let us know if you have any other concerns. Thanks!", "@summa-code,\r\nCan you please respond to the above comment. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Definitions are context dependent. This usage is consistent with most common usage. \r\n\r\nIf you have variable timestep data you can just include the time deltas as a feature.\r\n\r\nIf your data doesn't fit in memory then you'll want to use [tf.data](https://www.tensorflow.org/guide/data).\r\n\r\nBut anymore implementation questions should probably go on Stack Overflow. Github issues are for tracking problems in TensorFlow."]}, {"number": 41846, "title": "Missing \"model\" in visualize.py script: tflite model AttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model' ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8.1\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"` v1.12.1-37714-gac84c5eb81 2.4.0\r\n\r\n\r\n**I followed the [instructions](https://www.tensorflow.org/lite/guide/faq#how_do_i_inspect_a_tflite_file) in order to inspect a tflite file. Compilation and installation of the python whl was successful. pip freeze shows \r\ntensorflow @ file:///tmp/tensorflow_pkg/tensorflow-2.4.0-cp38-cp38-linux_x86_64.whl. Starting the python consul and importing tensorflow works as expected. \r\n\r\nHowever, running \r\n\r\npython visualize.py foo.tflite foo.html\r\n\r\nresults in a an error\r\n\r\nTraceback (most recent call last):\r\n  File \"visualize.py\", line 517, in <module>\r\n    main(sys.argv)\r\n  File \"visualize.py\", line 513, in main\r\n    CreateHtmlFile(tflite_input, html_output)\r\n  File \"visualize.py\", line 429, in CreateHtmlFile\r\n    data = CreateDictFromFlatbuffer(file_data)\r\n  File \"visualize.py\", line 414, in CreateDictFromFlatbuffer\r\n    model_obj = schema_fb.Model.GetRootAsModel(buffer_data, 0)\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\n\r\n\r\nIn addition, \r\n\r\nbazel run //tensorflow/lite/tools:visualize model.tflite visualized_model.html\r\n\r\nresults in a similar error\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=153\r\nINFO: Reading rc options for 'run' from /home/omri/src/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'run' from /home/omri/src/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'run' from /home/omri/src/tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/home/omri/.pyenv/versions/tf_src/bin/python3 --action_env PYTHON_LIB_PATH=/home/omri/.pyenv/versions/tf_src/lib/python3.8/site-packages --python_path=/home/omri/.pyenv/versions/tf_src/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/omri/src/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/omri/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/omri/src/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /home/omri/src/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/omri/src/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/lite/tools:visualize (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/tools:visualize up-to-date:\r\n  bazel-bin/tensorflow/lite/tools/visualize\r\nINFO: Elapsed time: 0.132s, Critical Path: 0.01s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Running command line: bazel-bin/tensorflow/lite/tools/visualize /home/omri/Downloads/mobilenet_thin_openpose_opt_fullint_tf1.tflite /home/omri/DownINFO: Build completed successfully, 1 total action\r\nTraceback (most recent call last):\r\n  File \"/home/omri/.cache/bazel/_bazel_omri/a9e9b87cb64d67149db4f28645a2ba4b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 517, in <module>\r\n    main(sys.argv)\r\n  File \"/home/omri/.cache/bazel/_bazel_omri/a9e9b87cb64d67149db4f28645a2ba4b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 513, in main\r\n    CreateHtmlFile(tflite_input, html_output)\r\n  File \"/home/omri/.cache/bazel/_bazel_omri/a9e9b87cb64d67149db4f28645a2ba4b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 429, in CreateHtmlFile\r\n    data = CreateDictFromFlatbuffer(file_data)\r\n  File \"/home/omri/.cache/bazel/_bazel_omri/a9e9b87cb64d67149db4f28645a2ba4b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/visualize.runfiles/org_tensorflow/tensorflow/lite/tools/visualize.py\", line 414, in CreateDictFromFlatbuffer\r\n    model_obj = schema_fb.Model.GetRootAsModel(buffer_data, 0)\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\n\r\n\r\nFinally, starting the python consul and trying to import `Model` from  'tensorflow.lite.python.schema_py_generated' results in the same error\r\n\r\n~/src/tensorflow/tensorflow/lite/tools$ python\r\nPython 3.8.1 (default, Mar  5 2020, 13:14:49) \r\n[GCC 7.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow.lite.python import schema_py_generated\r\n>>> schema_py_generated.Model\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\n\r\n**\r\n\r\n**The script should generate an HTML file**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@OmriTreidel \r\nPlease provide simple stand alone code to replicate the issue or if possible share colab gist with the error to analyse.", "@Saduf2019 The error can be reproduced by a single import 2 lines mentioned above\r\n\r\n```\r\nfrom tensorflow.lite.python import schema_py_generated\r\nschema_py_generated.Model\r\n```\r\n\r\nor alternatively by using bazel run in the terminal\r\n\r\n```\r\nbazel run //tensorflow/lite/tools:visualize model.tflite visualized_model.html\r\n``` \r\n\r\nboth result in the same error\r\n\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\n\r\n\r\nWhat else should I provide ?", "I am also facing the same issue. Any update on this ? \r\nKindly help @jvishnuvardhan ", "I can reproduce the issue. Investigating...", "Seems this's because 'schema_py_generated.py' is empty. By changing https://github.com/tensorflow/tensorflow/blob/9544dfa6063e37afd7aa35aa33e8622734acc69b/third_party/flatbuffers/build_defs.bzl#L373 from\r\n\r\n>  \"sed 's/from flatbuffers.compat import import_numpy/import numpy as np' |\" +\r\n\r\nto \r\n\r\n>  \"sed 's/from flatbuffers.compat import import_numpy/import numpy as np/g' |\" +\r\n\r\nshould fix the issue\r\n\r\n           ", "@Fruiter I have \"AttributeError: module 'flatbuffers' has no attribute 'encode'\" error with the change.\r\nAm I missing something?\r\n", "> @Fruiter I have \"AttributeError: module 'flatbuffers' has no attribute 'encode'\" error with the change.\r\n> Am I missing something?\r\n\r\nNot sure. I tested by 'bazel build //tensorflow/lite/python:schema_py' then replaced the empty schema_py_generated.py. And I didn't test the 'visualize' use case. But I have tested with tflite model conversion logic which has the same error without the fix.", "@lu-wang-g can you advise?", "It seems to be related to a recent change on FlatBuffer bazel rules. @MeghnaNatraj could you please double check the case?", "Looking into this now. I was able to reproduce the error. Will post an update when I find a fix. ", "Marking the issue as closed as it has been fixed. Feel free to re-open it if the issue is still unresolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41846\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41846\">No</a>\n", "F:\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\pythonschema_py_generated.py      There is no code, it is empty.   \r\nhow to solve this problem\uff1f", "@YANxu-666 what TF version are you using? Could you installing`tf-nightly` and try again?", "> @ YANxu-666\u60a8\u4f7f\u7528\u7684\u662f\u4ec0\u4e48TF\u7248\u672c\uff1f\u60a8\u53ef\u4ee5\u5b89\u88c5\u5e76\u91cd`tf-nightly`\u8bd5\u5417\uff1f\r\n\r\ntensorflow2.4.0  .I am also facing the same issue. Any update on this ?", "I am also facing the same issue. Any update on this ?", "Ran into this issue on Windows (maybe that ^^ bazel find/sed script does not run on Windows?). Created a new virtualenv in WSL2 (Ubuntu 18.04) and installed the same requirements.txt including `tf-nightly==2.5.0.dev20201115`. There the `schema_py_generated.py` was not empty... Because I wanted to continue on Windows, I copied the generated code to the empty file in my Windows environment and now it works like a charm. Hope it helps you (to fix it \ud83d\ude09 )?", "> \u5728Windows\u4e0a\u9047\u5230\u6b64\u95ee\u9898\uff08\u4e5f\u8bb8^^ bazel\u67e5\u627e/ sed\u811a\u672c\u4e0d\u80fd\u5728Windows\u4e0a\u8fd0\u884c\uff1f\uff09\u3002\u5728WSL2\uff08Ubuntu 18.04\uff09\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684virtualenv\uff0c\u5e76\u5b89\u88c5\u4e86\u76f8\u540c\u7684requirements.txt\uff0c\u5176\u4e2d\u5305\u62ec`tf-nightly==2.5.0.dev20201115`\u3002\u90a3\u91cc`schema_py_generated.py`\u4e0d\u662f\u7a7a\u7684\u2026\u2026\u56e0\u4e3a\u6211\u60f3\u5728Windows\u4e0a\u7ee7\u7eed\uff0c\u6240\u4ee5\u6211\u5c06\u751f\u6210\u7684\u4ee3\u7801\u590d\u5236\u5230Windows\u73af\u5883\u4e2d\u7684\u7a7a\u6587\u4ef6\u4e2d\uff0c\u73b0\u5728\u5b83\u5c31\u50cf\u4e00\u4e2a\u8d85\u7ea7\u6309\u94ae\u4e00\u6837\u5de5\u4f5c\u3002\u5e0c\u671b\u5bf9\u60a8\u6709\u5e2e\u52a9\uff08\u4fee\u590d\u5b83\uff09\ud83d\ude09 \uff09\uff1f\r\nI am running on Windows,and I downloaded all the files in the tensorflow directory to my computer.I don't know which file directory the schema_py_generated.py is in.\r\nCan you send me the code of schema_py_generated.py?\r\n  thanks\r\n", "> \u5728Windows\u4e0a\u9047\u5230\u6b64\u95ee\u9898\uff08\u4e5f\u8bb8^^ bazel\u67e5\u627e/ sed\u811a\u672c\u4e0d\u80fd\u5728Windows\u4e0a\u8fd0\u884c\uff1f\uff09\u3002\u5728WSL2\uff08Ubuntu 18.04\uff09\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684virtualenv\uff0c\u5e76\u5b89\u88c5\u4e86\u76f8\u540c\u7684requirements.txt\uff0c\u5176\u4e2d\u5305\u62ec`tf-nightly==2.5.0.dev20201115`\u3002\u90a3\u91cc`schema_py_generated.py`\u4e0d\u662f\u7a7a\u7684\u2026\u2026\u56e0\u4e3a\u6211\u60f3\u5728Windows\u4e0a\u7ee7\u7eed\uff0c\u6240\u4ee5\u6211\u5c06\u751f\u6210\u7684\u4ee3\u7801\u590d\u5236\u5230Windows\u73af\u5883\u4e2d\u7684\u7a7a\u6587\u4ef6\u4e2d\uff0c\u73b0\u5728\u5b83\u5c31\u50cf\u4e00\u4e2a\u8d85\u7ea7\u6309\u94ae\u4e00\u6837\u5de5\u4f5c\u3002\u5e0c\u671b\u5bf9\u60a8\u6709\u5e2e\u52a9\uff08\u4fee\u590d\u5b83\uff09\ud83d\ude09 \uff09\uff1f\r\nI found schema_py_generated.py but it is empty\r\nCan you send me the code of schema_py_generated.py?\r\nI really need it\r\n", "@YANxu-666 This issue should be fixed with the latest TF code (tf-nightly). Could you try with that let us know if it works?\r\n\r\nInstall it using pip as follows: \r\n```\r\npip install tf-nightly\r\n```\r\n\r\n(you need to restart your runtime if you are using https://colab.research.google.com)", "> Ran into this issue on Windows (maybe that ^^ bazel find/sed script does not run on Windows?). Created a new virtualenv in WSL2 (Ubuntu 18.04) and installed the same requirements.txt including `tf-nightly==2.5.0.dev20201115`. There the `schema_py_generated.py` was not empty... Because I wanted to continue on Windows, I copied the generated code to the empty file in my Windows environment and now it works like a charm. Hope it helps you (to fix it \ud83d\ude09 )?\r\n\r\nOn Windows, i still get empty scheme_py_generated.py\r\nTested it with clean pip installs of tf-nightly 2.5.0.dev20201116 and fresh released v2.4.0-rc2", "@hhass is it possible to post an end-to-end https://colab.research.google.com where I can reproduce this issue?", "Hi! I am encountering the same problem, my version of tensorflow is : tf_nightly-2.5.0.dev20201202. \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2020.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py\", line 1438, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2020.1\\plugins\\python-ce\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"C:/Users/brackman/Documents/Tensorflow/models/research/object_detection/ssd_mobilenet_quant/convertion.py\", line 39, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\brackman\\Anaconda3\\envs\\tf-nightly\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 767, in convert\r\n    result = _modify_model_io_type(result, **flags_modify_model_io_type)\r\n  File \"C:\\Users\\brackman\\Anaconda3\\envs\\tf-nightly\\lib\\site-packages\\tensorflow\\lite\\python\\util.py\", line 853, in modify_model_io_type\r\n    model_object = _convert_model_from_bytearray_to_object(model)\r\n  File \"C:\\Users\\brackman\\Anaconda3\\envs\\tf-nightly\\lib\\site-packages\\tensorflow\\lite\\python\\util.py\", line 579, in _convert_model_from_bytearray_to_object\r\n    model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\n```\r\n\r\nI am trying to quantize the SSD_MobileNetV2. Let me know if you need my code for review. Thank you!", "@ChowderII Yes, please do post an end-to-end [colab notebook](https://colab.research.google.com] so I can reproduce the issue for debugging.", "@MeghnaNatraj \r\nI am not sure how to upload my model there but I will give you the code that I ran. (The model is [SSD_MobileNetV2_FPN_lite](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz))\r\n\r\nI don't know how to put my images and my model there but here is the [colab](https://colab.research.google.com/drive/1T7anbKpXTxVc2h93e_eTSsG9MLdxHCr5?usp=sharing)", "@ChowderII If you can upload all your data (model and images) to GoogleDrive, mount it & run your colab to verify the error,  and finally share that folder (ensure it has with read permissions for everyone), it would make it very easy for us to debug it. \r\n\r\nHere's a Github issue where a user did this, and it enabled us to debug the issue quickly - https://github.com/tensorflow/tensorflow/issues/41840"]}, {"number": 41845, "title": "Tensorflow raises exception in eager mode but works in graph mode \"AttributeError: 'float' object has no attribute '_id'\"", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): `v2.1.0-rc2-17-ge5bf8de410 2.1.0`\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: V10.1.243\r\n- GPU model and memory: RTX2070 MaxQ, 8GB\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the following code in Eager mode, the line `tf.print(tape.gradient(y, x))` throws with exception  `AttributeError: 'float' object has no attribute '_id'`. In Graph mode it returns `None` gradient as expected.\r\n\r\n```python\r\n# @tf.function\r\ndef sign(x):\r\n    tf.debugging.assert_rank(x, 0)\r\n    \r\n    if x > 0:\r\n        return 1.0\r\n    else:\r\n        return -1.0\r\n    \r\nx = tf.constant(3.0, dtype=tf.float32)\r\n\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(x)\r\n    y = sign(x)\r\n    \r\ntf.print(y)\r\ntf.print(tape.gradient(y, x))\r\n```\r\n\r\nHowever, wrapping the return values in `tf.constant` does not throw an exception\r\n\r\n```python\r\ndef sign(x):\r\n    tf.debugging.assert_rank(x, 0)\r\n    \r\n    if x > 0:\r\n        return tf.constant(1.0)\r\n    else:\r\n        return tf.constant(-1.0)\r\n    \r\nx = tf.constant(3.0, dtype=tf.float32)\r\n\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(x)\r\n    y = sign(x)\r\n    \r\ntf.print(y)\r\ntf.print(tape.gradient(y, x))\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should either not throw an error in Eager mode or give a more verbose error message\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-f537fe9b7f6f> in <module>\r\n     15 \r\n     16 tf.print(y)\r\n---> 17 tf.print(tape.gradient(y, x))\r\n\r\nc:\\users\\windows\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n   1027         output_gradients=output_gradients,\r\n   1028         sources_raw=flat_sources_raw,\r\n-> 1029         unconnected_gradients=unconnected_gradients)\r\n   1030 \r\n   1031     if not self._persistent:\r\n\r\nc:\\users\\windows\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     75       output_gradients,\r\n     76       sources_raw,\r\n---> 77       compat.as_str(unconnected_gradients.value))\r\n\r\nAttributeError: 'float' object has no attribute '_id'\r\n```\r\n", "comments": ["I have tried in colab with TF 2.1, 2.3, nightly versions(`2.4.0-dev20200728`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/2c242efbd330d1be2aef5ff16cfb802e/untitled194.ipynb).Thanks!", "Hi @Ghost---Shadow, you'll notice that if you `print(type(y))` within your gradient tape, y is `<class 'float'>`. \r\n\r\n`tape.gradient` only works with tensors or variables, [as specified in the docs.](https://www.tensorflow.org/api_docs/python/tf/GradientTape#gradient) Hence why everything works when `sign(x)` returns tf.constant instead of float.\r\n\r\nThe reason your code works with the @tf.function decorator is because [Autograph is on by default](https://www.tensorflow.org/guide/function#autograph_transformations), and transforms your python eager code into graph-compatible TensorFlow ops.\r\nHope this helps clear up any confusion!", "@nikitamaia I am aware of that. However, I think it should either not throw an error in Eager mode or give a more verbose error message. \r\n\r\n> gradient function expects Tensor, received float\r\n\r\nor it should coerce to Tensor automatically.\r\n\r\n", "Removing the bug label, since there is no bug here. \r\nThat being said, there is always a lot of room to improve Tensorflow error messages and make everything more intuitive (especially when switching between eager and graph mode!). \r\nPlease file a feature request if you are proposing a different error message or functionality."]}, {"number": 41844, "title": "\"inputs\" method of tflite::Interpreter causes crash.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MatePad Pro\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): Android NDK 21.3.6528147\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nUsing TensorFlow Lite C++ API, when you call \"inputs\" method of tflite::Interpreter, you get the error `terminating with uncaught exception of type std::length_error: vector` and the program crashes.\r\n\r\nUntil the source code with the tag 2.3.0-rc2, it works and the source code with the tag 2.3.0 and later, it crashes.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo crashes and get the result of inputs.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@y-ich,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Can you provide a model that I can use to debug please?", "It was my mistake.\r\nI used different version of include path. That was the cause.\r\n\r\nI am sorry that I wasted your time.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41844\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41844\">No</a>\n"]}, {"number": 41843, "title": "tf.data.Dataset API with ImageGenerator => ValueError: as_list() is not defined on an unknown TensorShape.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): See the colab link below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.2.0\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: colab: CPU/GPU/TPU\r\n\r\n**Describe the current behavior**\r\nWhen using tf.data.Dataset API fit throws an exception:\r\nINFO:tensorflow:Error reported to Coordinator: as_list() is not defined on an unknown TensorShape.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 998, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 262, in wrapper\r\n    return converted_call(f, args, kwargs, options=options)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 418, in converted_call\r\n    return _call_unconverted(f, args, kwargs, options, False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 346, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 543, in train_step\r\n    self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py\", line 391, in update_state\r\n    self._build(y_pred, y_true)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py\", line 322, in _build\r\n    self._metrics, y_true, y_pred)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 1118, in map_structure_up_to\r\n    **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 1214, in map_structure_with_tuple_paths_up_to\r\n    *flat_value_lists)]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 1213, in <listcomp>\r\n    results = [func(*args, **kwargs) for args in zip(flat_path_list,\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 1116, in <lambda>\r\n    lambda _, *values: func(*values),  # Discards the path arg.\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py\", line 421, in _get_metric_objects\r\n    return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py\", line 421, in <listcomp>\r\n    return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py\", line 442, in _get_metric_object\r\n    y_t_rank = len(y_t.shape.as_list())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 1173, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n\r\n**Describe the expected behavior**\r\nNo exception.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1TnU5zWWyLeDnk2K-6aVeur0W9zEscWuX?usp=sharing\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem.https://colab.research.google.com/drive/1TnU5zWWyLeDnk2K-6aVeur0W9zEscWuX?usp=sharing\r\n\r\n**Other info / logs** \r\nPlease use **use_dataset_api** flag to toggle using dataset API vs ImageGenerator, which works", "comments": ["Found the solution by explicitly providing output_shapes output_shapes = ([None, 28, 28, 1],[None,10]) for the example above. However it raises the following questions, 1) why the output_shapes is optional? 2) why the shapes cannot be deduced from the provided generator? 3) The error message is very confusing and loosely referencing the source of the issue...\r\n`output_shapes = ([None, 28, 28, 1],[None,10]);\r\ntrain_generator = tf.data.Dataset.from_generator( make_generator, ( tf.float32, tf.float32 ),\r\n                    output_shapes = output_shapes )\r\n`", "Hi @vladbph, I get permission denied when I try to access the colab. Can you make it publicly accessible? Thanks.", "> Hi @vladbph, I get permission denied when I try to access the colab. Can you make it publicly accessible? Thanks.\r\n\r\nSorry for the delay. I did make it publicly available, just added anyone can edit rights... Could you please try it.\r\nhttps://colab.research.google.com/drive/1TnU5zWWyLeDnk2K-6aVeur0W9zEscWuX?usp=sharing\r\n", "Hi @vladbph, seems to me like this the same issue as #32912 and the error message is raised from `model.fit`. In that case it's less surprising that the `output_shapes` argument is optional, as there are other uses for the generator other than passing it to `model.fit`.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41843\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41843\">No</a>\n"]}, {"number": 41842, "title": "Added tstring fuzzer", "body": "@mihaimaruseac ", "comments": []}, {"number": 41841, "title": "Merge summary kernel with C API ", "body": "Implemented MergeSummary Op and Kernel with C API. Updated build dependencies for c++ and python builds, so tests use new MergeSummary op. \r\n@annarev @bmzhao ", "comments": ["@dnguyen28061 can you please check sanity build failures ?"]}, {"number": 41840, "title": "Issue with a specific dataset with tf lite full integer quantisation uint8", "body": "TensorFlow version: tf-nightly 2.4.0-dev20200728\r\nKeras: 2.4.3\r\nPython: 3.6.9\r\n\r\nIssue: I trained DenseNet-169 with my dataset, validation accuracy 67.8%. My dataset (10k training images, 2k test images) is made in this way: each picture is 3 different pictures 200x200 pixels int8 piled up together. In the google colab file training_densenet169 is it possible to find a tool to visualise the images. \r\nAfter training I convert the model with tf converter (see colab file tf_to_tf_lite converter) to tf lite full integer uint8 quantisation. The tf lite model is subsequently tested with the google colab file test_tf_lite_cpu. The test shows that the tf lite model doesn't work, it predicts always the same value.\r\n\r\nI made the following tests. \r\n1) I changed my dataset so to have only one channel (therefore not the three pictures pile up). I trainined it with accuracy 67.8%, converted to tf lite, and tested it. The test failed. \r\n2) I then took the cifar10 dataset,  multiply by 7 the size in order to have 224x224 pixels images with 3 channels, and trained densenet169, conerted to tf lite, and tested. It worked.\r\n\r\nMy ideas: \r\n1) cifar10 is similar to my dataset, and it works. Is it possible that maybe there is a bug so that my images size 200x200 don't work but cifar10 size 224x224 does?\r\n2) If you can, have a look to my dataset. Is there anything specific that could trigger a failure mode in the converter? I couldn't see anything specific.\r\n\r\nAdditional information: All of the other tf lite quantisations work perfectly, only the full integer quantisation doesn't work.\r\n\r\nEverything can be found in this google folder: https://drive.google.com/drive/u/0/folders/11XruNeJzdIm9DTn7FnuIWYaSalqg2F0B\r\n", "comments": ["Dear @MeghnaNatraj this is for you. I am sorry but I cannot find how to assign you to this issue, could you do it? ", "I looked into `test_tf_lite_cpu.ipynb` and it looks like what you're doing seems fine. \r\n\r\n```\r\n## Before we get started, for reference:\r\n\r\n## 1. dynamic range optimization\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n## 2. integer quantization with float fallback\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\n\r\n## 3. integer quantization\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8  # or tf.uint8\r\nconverter.inference_output_type = tf.uint8  # or tf.uint8\r\n```\r\n\r\nCould try this the following in `test_tf_lite_cpu.ipynb` and get back to me with the results? \r\n1. Create all 3 tflite models and run inference on them in the notebook on ~5 images. Have the code and outputs in notebook so I can review it. This will help us understand at which optimization it fails.  \r\n2. For 3. integer quantization - Is it possible to print an input image at different stages as given below?\r\n   1. input to the original model\r\n   2. input to the representative dataset function (while quantizing)\r\n   3. input to the interpreter (before manually quantizing)\r\n   We need to ensure that i, ii, iii have the exact same range of values.\r\n\r\n", "Hi @MeghnaNatraj I am currently on holiday but I'll get back to you as soon as possible. Please keep the issue open in the meantime.", "Dear @MeghnaNatraj, first of all, thanks for keeping the issue open, I appreciate that. \r\n \r\nI am working on that right now but a new issue about my google drive has appeared so it is taking a little more time.\r\n\r\nCould you please specify what you mean by \"input to the representative dataset function (while quantizing)\"?", "\"input to the representative dataset function (while quantizing)\" = when you specify the representative dataset function, you will define a small set of images that will be used to quantize the model during conversion. Print one of these inputs/images. The reason why we want to do this is because we need to verify that the images are preprocessed the same way as your training dataset. \r\n\r\nHope this helps. Let me know if you need more details.", "Hi,\r\n\r\nI've done what you asked me. Let me guide you through the results.\r\n\r\n1. you can find in the file tf_to_tflite_convert the code to create the 3 models (called DenseNet169_2020_07_27_dyn_range_debug, DenseNet169_2020_07_27_int_quant_float_debug, DenseNet169_2020_07_27_int_quant_uint_debug). You can find the tests on the code test_tf_lite_cpu. \r\n\r\nThese are the results calculated over 100 pics (tf model accuracy is 68.6% calculated over 2000 pics):\r\n\r\ndynamic range: time/inference 8.4 seconds, accuracy 35%\r\nfloat fallback: time/inference 9.3 seconds, accuracy 41%\r\ninteger: time/inference 9.3 seconds, accuracy 60%\r\n\r\nThese results are in contrast with what I found on the 16th of June, where my results were (with the same model and calculated over 2000 pics)\r\n\r\ndynamic range: time/inference 297 ms, accuracy 63.65%\r\nfloat fallback: time/inference 81.92 seconds, accuracy 68.6%\r\ninteger: time/inference 9.3 seconds, it predicted always the same value.\r\n\r\nSo it looks like dynamic range and float fallback have worsened their performance, being slower and way less accurate than before (but why?). On the other hand, full integer finally it looks like it is doing something sensible.\r\n\r\n2. You can find a visualisation of i and iii at the end of test_tf_lite_cpu and of ii in tf_to_tflite_converter.\r\n\r\np.s. you asked to print ~ 5 pics but you'll see the output of the inference is calculated over 100 pics. if you need to test it and you want less pictures, just play with the parameter number_pics.", "Dear @MeghnaNatraj , I would like to ask you if you happen to have any news.", "It looks like you have forgotten to set the input tensors when running inference for dynamic range and float fallback!\r\n\r\nReference: https://thinkmobile.dev/testing-tensorflow-lite-image-classification-model/\r\nSpecifically these parts:\r\n- TFLite Inference [Resize](https://gist.githubusercontent.com/frogermcs/41ba1572340d23e80ac58386c0d91037/raw/c5ea483e06ec64c7f28951aa8c52c1881480f992/resize_interpreter.py) \r\n- TFLite Inference [set tensors and run inference](https://gist.githubusercontent.com/frogermcs/ab75f6d8f56c967d4ddb2057a4604693/raw/75b5d08776a4c808b93b3b2bf305a296ea816e69/tflite_inference.py)\r\n\r\nYour code:\r\n```\r\n#inference\r\nt1_dynamic = time.time()\r\ninterpreter_dynamic.invoke()\r\nt2_dynamic = time.time()\r\n```\r\n\r\nExpected code:\r\n```\r\n#inference\r\ninterpreter_dynamic.set_tensor(input_details_dynamic[0]['index'], x_test_a_list)\r\nt1_dynamic = time.time()\r\ninterpreter_dynamic.invoke()\r\nt2_dynamic = time.time()\r\n```\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41840\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41840\">No</a>\n"]}, {"number": 41839, "title": "java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found Falling back to OpenGL TfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer. ", "body": "**System information**\r\n- Android 9.\r\n- TensorFlow Lite.\r\n- Google Pixel\r\n\r\nbuild.gradle:\r\n```\r\nimplementation('org.tensorflow:tensorflow-lite:2.2.0')\r\nimplementation ('org.tensorflow:tensorflow-lite-gpu:2.2.0')\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nException is thrown on Google Pixel while running Blazeface model taken from Mediapipe repo (https://github.com/google/mediapipe/blob/master/mediapipe/models/face_detection_front.tflite). But this model works successfully on Samsung S10 and Xiaomi MI8.\r\n\r\n```\r\n Internal error: Failed to run on the given Interpreter: Following operations are not supported by GPU delegate:\r\n    DEQUANTIZE: \r\n    164 operations will run on the GPU, and the remaining 0 operations will run on the CPU.\r\n    Can not open OpenCL library on this device - dlopen failed: library \"libOpenCL.so\" not found\r\n    Falling back to OpenGL\r\n    TfLiteGpuDelegate Invoke: GpuDelegate must run on the same thread where it was initialized.\r\n    Node number 164 (TfLiteGpuDelegateV2) failed to invoke.\r\n    \r\n    TfLiteGpuDelegate Invoke: GpuDele [DevelopReportingTree.log:18]\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Following operations are not supported by GPU delegate:\r\n    DEQUANTIZE: \r\n    164 operations will run on the GPU, and the remaining 0 operations will run on the CPU.\r\n    Can not open OpenCL library on this device - dlopen failed: library \"libOpenCL.so\" not found\r\n    Falling back to OpenGL\r\n    TfLiteGpuDelegate Invoke: GpuDelegate must run on the same thread where it was initialized.\r\n    Node number 164 (TfLiteGpuDelegateV2) failed to invoke.\r\n    \r\n    TfLiteGpuDelegate Invoke: GpuDele\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)\r\n```\r\n\r\nWhen I change tflite dependencies from nightly to v. 2.2 and modify interpreter initialization, error will be a bit different for Google Pixel:\r\n\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found\r\nFalling back to OpenGL\r\nTfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer. \r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nSuccessfully recognizes faces without exceptions\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n ```\r\n    private static final int BATCH_SIZE = 1;\r\n    private ByteBuffer imgData;\r\n    private int[] intValues;\r\n    private static final int PIXEL_SIZE = 3;\r\n\r\n       Interpreter tfLite = createTfliteInterpreter(assetManager, \"face_detection_front_blazefast.tflite\");\r\n         imgData = ByteBuffer.allocateDirect(BATCH_SIZE * 128 * 128 * PIXEL_SIZE * 4);\r\n        imgData.order(ByteOrder.nativeOrder());\r\n        intValues = new int[inputSize * inputSize];\r\n\r\n    public List<Recognition> recognizeImage(Bitmap bitmap) {\r\n        final int previewWidth = bitmap.getWidth();\r\n        final int previewHeight = bitmap.getHeight();\r\n\r\n        int sensorOrientation = 0;\r\n        Matrix frameToCropTransform =\r\n                getTransformationMatrix(\r\n                        previewWidth,\r\n                        previewHeight,\r\n                        128, 128,\r\n                        sensorOrientation,\r\n                        false\r\n                );\r\n\r\n        // to restore location object\r\n        cropToFrameTransform = new Matrix();\r\n        frameToCropTransform.invert(cropToFrameTransform);\r\n\r\n        // crop image\r\n        Canvas canvas = new Canvas(croppedBitmap);\r\n        canvas.drawBitmap(bitmap, frameToCropTransform, null);\r\n\r\n        convertBitmapToByteBuffer(\r\n                croppedBitmap,\r\n                imgData,\r\n                intValues,\r\n                128\r\n        );\r\n\r\n        Map<Integer, Object> outputs = provideOutput();\r\n        Object[] inputArray = {imgData};\r\n        try {\r\n            tfLite.runForMultipleInputsOutputs(inputArray, outputs);\r\n        } catch (Exception e) {\r\n            Timber.e(e);\r\n        }\r\n        return getDetections(bitmap, outputs);\r\n    }\r\n\r\n    public Map<Integer, Object> provideOutput() {\r\n        float[][][] boxesResult = new float[1][896][16];\r\n        float[][][] scoresResult = new float[1][896][1];\r\n        HashMap<Integer, Object> outputs = new HashMap<>();\r\n        outputs.put(0, boxesResult);\r\n        outputs.put(1, scoresResult);\r\n        return outputs;\r\n    }\r\n\r\n    Interpreter createTfliteInterpreter(AssetManager assetManager, String modelPath) {\r\n        try {\r\n            final Interpreter.Options options;\r\n                GpuDelegate.Options options1 = new GpuDelegate.Options()\r\n                        .setPrecisionLossAllowed(true)\r\n                        .setInferencePreference(GpuDelegate.Options.INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER);\r\n                GpuDelegate delegate = new GpuDelegate(options1);\r\n                options = new Interpreter.Options()\r\n                        .addDelegate(delegate);\r\n            return new Interpreter(\r\n                    loadModelFile(assetManager, modelPath),\r\n                    options\r\n            );\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n\r\n    public static void convertBitmapToByteBuffer(Bitmap bitmap, ByteBuffer imgData, int[] intValues, int mInputSize) {\r\n        bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n        imgData.rewind();\r\n        int pixel = 0;\r\n        for (int i = 0; i < mInputSize; ++i) {\r\n            for (int j = 0; j < mInputSize; ++j) {\r\n                final int pixelValue = intValues[pixel++];\r\n                imgData.putFloat(((pixelValue >> 16) & 0xFF) / 255.0f);\r\n                imgData.putFloat(((pixelValue >> 8) & 0xFF) / 255.0f);\r\n                imgData.putFloat((pixelValue & 0xFF) / 255.0f);\r\n            }\r\n        }\r\n    }\r\n\r\npublic static Matrix getTransformationMatrix(\r\n            final int srcWidth,\r\n            final int srcHeight,\r\n            final int dstWidth,\r\n            final int dstHeight,\r\n            final int applyRotation,\r\n            final boolean maintainAspectRatio) {\r\n        final Matrix matrix = new Matrix();\r\n\r\n        if (applyRotation != 0) {\r\n            if (applyRotation % 90 != 0) {\r\n                Timber.w(\"Rotation of %d % 90 != 0\", applyRotation);\r\n            }\r\n\r\n            // Translate so center of image is at origin.\r\n            matrix.postTranslate(-srcWidth / 2.0f, -srcHeight / 2.0f);\r\n\r\n            // Rotate around origin.\r\n            matrix.postRotate(applyRotation);\r\n        }\r\n\r\n        // Account for the already applied rotation, if any, and then determine how\r\n        // much scaling is needed for each axis.\r\n        final boolean transpose = (Math.abs(applyRotation) + 90) % 180 == 0;\r\n\r\n        final int inWidth = transpose ? srcHeight : srcWidth;\r\n        final int inHeight = transpose ? srcWidth : srcHeight;\r\n\r\n        // Apply scaling if necessary.\r\n        if (inWidth != dstWidth || inHeight != dstHeight) {\r\n            final float scaleFactorX = dstWidth / (float) inWidth;\r\n            final float scaleFactorY = dstHeight / (float) inHeight;\r\n\r\n            if (maintainAspectRatio) {\r\n                // Scale by minimum factor so that dst is filled completely while\r\n                // maintaining the aspect ratio. Some image may fall off the edge.\r\n                final float scaleFactor = Math.max(scaleFactorX, scaleFactorY);\r\n                matrix.postScale(scaleFactor, scaleFactor);\r\n            } else {\r\n                // Scale exactly to fill dst from src.\r\n                matrix.postScale(scaleFactorX, scaleFactorY);\r\n            }\r\n        }\r\n\r\n        if (applyRotation != 0) {\r\n            // Translate back from origin centered reference to destination frame.\r\n            matrix.postTranslate(dstWidth / 2.0f, dstHeight / 2.0f);\r\n        }\r\n\r\n        return matrix;\r\n    }\r\n```\r\n\r\n", "comments": ["This issue looks similar to https://github.com/tensorflow/tensorflow/issues/44659.  Just wondering does more recent tflite build fix the issue? Thx!", "@iglaweb This looks more similar to this [issue](https://github.com/tensorflow/tensorflow/issues/44659)  & Could you please execute your code using Latest Version 2.4.1 or 2.5 and let us know if the issue still persists? Thanks!", "> This looks more similar to this [issue](https://github.com/tensorflow/tensorflow/issues/44659) & Could you please execute your code using Latest Version 2.4.1 or 2.5 and let us know if the issue still persists? Thanks!\r\n\r\nAll delegates (CPU, GPU, Hexagon, NNAPI) do not throw errors now with TensorflowLite 2.5. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41839\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41839\">No</a>\n", "The same issue reoccurred on Android 12 while running the SSDtiny model on GPU.  The same code works fine on android 8. \r\n\r\nI am running tfLite 2.8.0 . Is there anything that needs to be upgraded in order for tfLite to use the available GPU resources on android 12 ? "]}, {"number": 41838, "title": "Segmentation fault: 11", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.14.6\r\n- TensorFlow installed from (source or binary): Using pip\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\n\r\n```\r\npython3 tensorflow/lite/python/tflite_convert.py --output_file=model-float.lite --output_format=TFLITE --inference_type=FLOAT --inference_input_type=FLOAT --input_shape=\"1,224,224,3\" --input_array=\"serving_default_input_1\" --output_array=\"StatefulPartitionedCall\" --mean_value=0 --std_dev_value=1 --saved_model_dir=/Users/z004njq/Projects/save_models/mobilenetv4-exp-1004-export/ --experimental_converter=True\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-07-28 13:29:05.763703: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-28 13:29:05.781147: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14dc421f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-28 13:29:05.781174: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-28 13:29:14.581147: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\r\n2020-07-28 13:29:14.581230: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-07-28 13:29:14.677045: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-07-28 13:29:14.677069: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 1835 nodes (1614), 2696 edges (2475), time = 64.086ms.\r\n2020-07-28 13:29:14.677092: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 2.02ms.\r\nI0728 13:29:16.315555 140734995711424 lite.py:624] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n2020-07-28 13:29:16.359263: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-07-28 13:29:16.359292: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007fff6b6d45c0 (most recent call first):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38 in wrapped_toco_convert\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 199 in toco_convert_protos\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 574 in toco_convert_impl\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 633 in convert\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 900 in convert\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1076 in convert\r\n  File \"tensorflow/lite/python/tflite_convert.py\", line 239 in _convert_tf2_model\r\n  File \"tensorflow/lite/python/tflite_convert.py\", line 623 in run_main\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py\", line 299 in run\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"tensorflow/lite/python/tflite_convert.py\", line 640 in main\r\n  File \"tensorflow/lite/python/tflite_convert.py\", line 644 in <module>\r\nSegmentation fault: 11\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n[mobilenetv3-exp-1004-export.zip](https://github.com/tensorflow/tensorflow/files/4991052/mobilenetv3-exp-1004-export.zip)\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you provide the model / a minimal reproducer please?", "I had included the saved model before, it's available at: https://github.com/tensorflow/tensorflow/files/4991052/mobilenetv3-exp-1004-export.zip", "Apologies, it was not showing properly in the original message due to formatting.", "Thank for your correcting the formatting, I am looking forward to solving the issue and understand the underlying cause.", "@dipendra009 Can you please share a standalone code to reproduce the issue? I am sharing a gist that you can use and update later part of the code. [Here](https://colab.research.google.com/gist/jvishnuvardhan/f7f66a336334c3f7cd5b343278c3bc8a/untitled.ipynb) is the gist with your model. Thanks!", "I have already uploaded the saved model file and the code I am using is tensorflow/lite/python/tflite_convert.py from TF 2.3.0 as mentioned in the issue. The model is successfully loaded by the converter but it gives the segmentation fault while trying to convert it to tf-lite format.", "Sorry for the late update. I can reproduce and working on a fix for the issue.\r\nThanks for reporting the issue and sorry for the delay", "Sorry for the late update. This should be fixed now on the master branch.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41838\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41838\">No</a>\n"]}, {"number": 41837, "title": "Update the release notes to fix some typos and missed changes.", "body": "", "comments": []}, {"number": 41835, "title": "FailedPreconditionError: Error while reading resource variable block1_conv2_30/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/block1_conv2_30/kernel/N10tensorflow3VarE does not exist. \t [[{{node block1_conv2_30/Conv2D/ReadVariableOp}}]]", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.1.1\r\n- Python version: 2.7.17\r\n\r\n\r\nTrying to model a neural style transfer using VGG16.\r\n\r\nError while reading resource variable block1_conv2_30/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/block1_conv2_30/kernel/N10tensorflow3VarE does not exist.\r\n\t [[{{node block1_conv2_30/Conv2D/ReadVariableOp}}]]\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/nickinack/NST-VGG16/blob/master/nst-vgg.ipynb\r\n\r\n**Other info/logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@nickinack,\r\nOn running the code with TF v1.15, I am facing an error stating `InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [1,320,480,3] vs. shape[2] = [1,480,320,3]`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c48ac409fd2513a2c0f4431b39e020f5/41835.ipynb). \r\n\r\nCould you please try running the code with TF v1.15 and check if you are still facing the issue. Thanks!", "Hey, thank you very much! It indeed seems to be a dimension based problem!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41835\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41835\">No</a>\n"]}, {"number": 41834, "title": "Histogram summary", "body": "Added HistogramSummary Op and Kernel using C API. Also updated BUILD dependencies to use new Op_Kernel. \r\nWaiting for merge of TF_OpKernelConstruction_GetName \r\n\r\n@annarev @bmzhao ", "comments": ["@dnguyen28061 can you please check sanity build failures ?"]}, {"number": 41833, "title": "Keras ImageDataGenerator Preprocessing-Function", "body": "I am quite new to Tensorflow and Keras and not sure if am getting something wrong or if there really is a difference between the documentation and the code. If any futher informations are necessarry I will add them quickly.\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://keras.io/api/preprocessing/image/#imagedatagenerator-class\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nAccording to the documentation the class `ImageDataGenerator` takes a function `preprocessing_function` when initializing it. In the discription it says that the function is executed AFTER the input has been rescaled:\r\n\"...The function will run after the image is resized and augmented...\"\r\nWhen going through the code I realized that in the class `ImageDataGenerator` within the function `standardize` the `preprocessing_function `is applied BEFORE rescaling the image.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? Yes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? Yes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? Yes\r\n\r\n### Submit a pull request? No\r\n", "comments": ["@marang7 \r\nCould you please fill in the issue template with the issue faced and details as requested for along with tf version [simple stand alone code and error logs or colab gist with the error or steps that you ran before you ran into the issue]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41832, "title": "Fix #41630: include max_seq_length in cudnn descriptor cache key", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41832) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41832) for more info**.\n\n<!-- ok -->", "> Actually, can you please also add a test case?\r\n\r\nI can't find any existing test file supporting what is in `tensorflow/core/kernels/cudnn_rnn_ops.cc`, and the coverage of `CONTRIBUTING.md` for tests is quite scarce, it just covers how to run them. Would you have some pointers on where I should add test case?", "> > Actually, can you please also add a test case?\r\n> \r\n> I can't find any existing test file supporting what is in `tensorflow/core/kernels/cudnn_rnn_ops.cc`, and the coverage of `CONTRIBUTING.md` for tests is quite scarce, it just covers how to run them. Would you have some pointers on where I should add test case?\r\n\r\nManaged to add something as a new `tf_cuda_cc_test`, so it required some new headers and moving the two structs into that. It gets more invasise of a patch than I hoped for at the begining, but locally tests runs well and catches the regression.", "I am wondering if it is possible to limit the test into the python test, like in the lstm_v2_test.py. All we need to do is to design a simple training as in the issue 41630 that uses [1, 2048, 2048, 1, **74**, 2, 2048] in the first step and store it to the cache but [1, 2048, 2048, 1, **75**, 2, 2048] in the second step. If this can be successfully trained on GPU, we can view it as passed. How do you think?", "> I am wondering if it is possible to limit the test into the python test, like in the lstm_v2_test.py. All we need to do is to design a simple training as in the issue 41630 that uses [1, 2048, 2048, 1, **74**, 2, 2048] in the first step and store it to the cache but [1, 2048, 2048, 1, **75**, 2, 2048] in the second step. If this can be successfully trained on GPU, we can view it as passed. How do you think?\r\n\r\nI can look into that, it would sounds like a better option than the current test I wrote.", ">  lstm_v2_test.py.\r\n\r\nDo you know if there's any magic to run python tests required? I've followed `CONTRIBUTING.md` section about running unit tests, but\r\n```\r\nbazel test -s --verbose_failures --config=noaws --config=nogcp --config=nohdfs --config=nonccl -c opt --config=cuda -k //tensorflow/python/keras/layers\r\n[...]\r\nINFO: Analyzed target //tensorflow/python/keras/layers:layers (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target and 0 test targets...\r\nTarget //tensorflow/python/keras/layers:layers up-to-date (nothing to build)\r\nINFO: Elapsed time: 0.336s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Build completed successfully, 1 total action\r\n```", "Have you tried to simply run `python lstm_v2_test.py`?", "> Have you tried to simply run `python lstm_v2_test.py`?\r\n\r\nEven simpler: ` //tensorflow/python/keras/layers:lstm_v2_test` bazel target, I assumed passing the directory would run all tests there.", "Making progresses:\r\n```\r\n  @test_util.run_gpu_only\r\n  def test_lstm_variable_sequence_length(self):\r\n    if test.is_built_with_rocm():\r\n      self.skipTest('Skipping the test as ROCm MIOpen does not '\r\n                    'support padded input yet.')\r\n    input_shape = 2048\r\n    rnn_state_size = 2048\r\n    batch = 2\r\n\r\n    timestep = 1\r\n\r\n    (x_train, y_train), _ = testing_utils.get_test_data(\r\n        train_samples=batch,\r\n        test_samples=0,\r\n        input_shape=(timestep, input_shape),\r\n        num_classes=rnn_state_size,\r\n        random_seed=random_seed.DEFAULT_GRAPH_SEED)\r\n\r\n    timestep = 1\r\n\r\n    (x2_train, y2_train), _ = testing_utils.get_test_data(\r\n        train_samples=batch,\r\n        test_samples=0,\r\n        input_shape=(timestep, input_shape),\r\n        num_classes=rnn_state_size,\r\n        random_seed=random_seed.DEFAULT_GRAPH_SEED)\r\n\r\n    inputs = keras.layers.Input(\r\n        shape=[timestep, input_shape], dtype=dtypes.float32)\r\n    masked_input = keras.layers.Masking()(inputs)\r\n\r\n    with test_util.device(use_gpu=True):\r\n      cudnn_layer = rnn.LSTM(rnn_state_size,\r\n                             activation='tanh',\r\n                             recurrent_activation='sigmoid',\r\n                             recurrent_dropout=0,\r\n                             unroll=False,\r\n                             use_bias=True)\r\n      cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))\r\n\r\n    cudnn_model.compile('rmsprop', 'mse')\r\n    cudnn_model.fit(x_train, y_train)\r\n    cudnn_model.fit(x2_train, y2_train)\r\n```\r\n\r\nWith that, I can assert that it is going through the layers and reaches `IsCompatibleWith`:\r\n```\r\n\r\nIsCompatibleWith -> [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 1, 2, 2048] \r\nIsCompatibleWith -> [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 1, 2, 2048] \r\nIsCompatibleWith -> [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 1, 2, 2048] \r\nIsCompatibleWith -> [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 1, 2, 2048] \r\nIsCompatibleWith -> [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 1, 2, 2048] \r\nIsCompatibleWith -> [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2048, 2048, 1, 1, 2, 2048] \r\n```\r\n\r\nHowever, changing `timestep` to be `74` on `x_train` and `75` on `x2_train` seems to bypass the GPU implementation, because I can't see any `IsCompatibleWith` output in the log but I see:\r\n```\r\nWARNING:tensorflow:Model was constructed with shape (None, 75, 2048) for input Tensor(\"input_1:0\", shape=(None, 75, 2048), dtype=float32), but it was called on an input with incompatible shape (None, 74, 2048).\r\nW0731 11:00:01.229415 139681924948352 functional.py:583] Model was constructed with shape (None, 75, 2048) for input Tensor(\"input_1:0\", shape=(None, 75, 2048), dtype=float32), but it was called on an input with incompatible shape (None, 74, 2048).\r\n```\r\n\r\nEven directly calling `CuDNNLSTM` from `tensorflow/python/keras/layers/cudnn_recurrent.py` I'm blocked at Keras level when adding the 74/75 dimensional difference.\r\n\r\nForcing eager execution using `@test_util.run_in_graph_and_eager_modes`,\r\n```\r\nValueError: Error when checking input: expected input_1 to have shape (75, 64) but got array with shape (74, 64)\r\n```\r\n\r\nAnd to the best of my knowledge, with current TensorFlow, since there is no more `tensorflow.contrib` Keras is the only way to build LSTM layers, right? So I'm unsure how to solve this :/", "So @kaixih I force-pushed removing the C++ test, and I'm unsure how to pursue and add a proper test, since the Keras layers appears to be blocking me, and that seems completely un-avoidable, and it does not seems that v2.x allows for very raw graph creation (and I have to be honest, it would take me a lot of time to get up to speed because all my reading points to the fact all the code we did cannot be used as a source of inspiration).\r\n\r\nIf there's a way to create the graph manually and load CuDNN-backed LSTM without going through Keras, I'd be happy to be pointed to that.", "@lissyx For the test you pasted above, I think giving the input tensors with different shapes ([xx, 74, xx] vs [xx, 75, xx] will trigger TF function retracing and creating different graphs for the two inputs. So, there is no cached rnn states in the second case.\r\n\r\nFor the cudnn_recurrent.py, I think it doesn't support the variable sequence lengths. So, maybe it is not for this case.\r\n\r\nIt seems that you can repro it with TF1 script. Can you share the TF code snippet to repro the issue? Then, we can see how to make it into python test.\r\n", "> Can you share the TF code snippet to repro the issue? Then, we can see how to make it into python test.\r\n\r\nUnfortunately, it's not just isolated as a snippet, one would need to re-write it from scratch in tensorflow 1.15, but if that helps, I'll do it", "I am wondering how hard it would be to extract it as a unit test. Actually, I attempted to make a new test by giving a graph containing one lstm node the 74 shape and then 75 shape, but it seems the graph can always create a new rnn descriptor for it. So, I am kind of curious how you hit the error and if this case can be extracted, that would be helpful.", "> I am wondering how hard it would be to extract it as a unit test. Actually, I attempted to make a new test by giving a graph containing one lstm node the 74 shape and then 75 shape, but it seems the graph can always create a new rnn descriptor for it. So, I am kind of curious how you hit the error and if this case can be extracted, that would be helpful.\r\n\r\nOk, that is what I was trying to do. We hit the error when running training using deepspeech, using repro material from https://github.com/tensorflow/tensorflow/issues/41630#issuecomment-662491897\r\n\r\nSo, as you can see by looking at the code, extracting it is not as trivial as we could hope :/", "Can we hope for a 1.15.4 to include that fix ?", "> Can we hope for a 1.15.4 to include that fix ?\r\n\r\nCC @goldiegadde ", "> > Can we hope for a 1.15.4 to include that fix ?\r\n> \r\n> CC @goldiegadde\r\n\r\nGentle ping, can we hope for 1.15.4 or should we direct people hitting this issue to use the magic reset state env variable?", "@sanjoy @kaixih @goldiegadde Gentle ping? There has been several commit pushed to r1.15, so it would really be nice to include this one if you are preparing a 1.15.4. I can send the PR if you'd like.", "@lissyx Sorry for the delayed response, can you please open a PR against the 1.15 branch. cc @mihaimaruseac as well. ", "Apologies, I missed this myself too. Yes, please, let's open a PR against the branch", "> @lissyx Sorry for the delayed response, can you please open a PR against the 1.15 branch. cc @mihaimaruseac as well.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/42634"]}, {"number": 41831, "title": "Issue when trying to import tensorflow in my conda environment right after I downloaded it", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory:N/A\r\n\r\nHere below is the error message:\r\n  \r\n\r\n**Describe the problem**\r\n_It fails to load Tensorflow runtime whenever I try \"import tensorflow\" or \"from tensorflow import keras\" right after I have installed(using pip) tensorflow 2.3.0 from my conda environment.... I used \"pip install tensorflow\"\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nInstalled Anaconda on my machine,\r\nI then successfully setup a new conda environment on Cmd right in the Users/home folder\r\nOnwards, I started downloading libraries I needed for ML/DL, specifically tensorflow\r\nAfter successful downloaded, I tried importing it, but it _failed__\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n from tensorflow.python._pywrap_tensorflow_interanl import keras\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\celestino\\anaconda3\\envs\\packt_exercises\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n_Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\celestino\\anaconda3\\envs\\packt_exercises\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\celestino\\anaconda3\\envs\\packt_exercises\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\celestino\\anaconda3\\envs\\packt_exercises\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\celestino\\anaconda3\\envs\\packt_exercises\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\celestino\\anaconda3\\envs\\packt_exercises\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\celestino\\anaconda3\\envs\\packt_exercises\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime._", "comments": ["@kcelestinomaria \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website.](https://www.tensorflow.org/install/source_windows)\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\nPlease, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "\r\n@kcelestinomaria \r\n\r\nSorry, but we don't provide support for issues with the conda environment.\r\n\r\nThis issue is more suitable on Continuum Anaconda [repo](https://github.com/ContinuumIO/anaconda-issues/issues) since its related to TF installation with Anaconda.\r\nPlease post it on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41831\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41831\">No</a>\n"]}, {"number": 41830, "title": "Tensorflow not working in pycharm", "body": "\r\n**System information**\r\n-   MacOs Version 10.14.6 (18G5033)\r\n- Installed through Pycharm CE \r\n- TensorFlow version 2.0 but I have also installed Tensorflow 1.14 on python hoping it would work but didn't work.\r\n- Python version: 3.7.4\r\n- Installed using: pip\r\n- 1.6 GHz Intel Core i5\r\n\r\n\r\n\r\n**The problem**\r\n\r\nToday is my first time trying learn Tensorflow through LinkedIn Learning. I followed tutorials which uses Pycharm CE. It is through Pycharm where I installed Tensorflow. However, when it came to running the model, the response I got is that **ModuleNotFoundError: No module named 'tensorflow.contrib'**\r\n\r\nI have read some debugging related to this question but they were unclear and unhelpful. Any assistance will be highly appreciated. Thank you.  \r\n\r\n\r\n", "comments": ["Looks like you are trying to import contrib module in TF 2 which raises the error. `contrib` is no longer part of TF 2 and above. ", "Thank you ymodak, what can I do?", "If you want to use `tf.contrib`, you need to use TF 1.15.\r\n\r\nIf you want to use TF newer than 2.0 (2.3 was released yesterday), then you need to change the code to not use `tf.contrib`.", "The tf.contrib module is not included in TensorFlow 2. Many of its submodules have been integrated into TensorFlow core, or spun-off into other projects like tensorflow_io, or tensorflow_addons. So probably you can still do the same things, you will just need to find the right locations for the function you want to use.", "Thank you. It's no doubt tough for a beginner who is watching tutorials. I have been scanning through internet to look at the available answers. One the debugs I came across is this, **python -m pip install --user --upgrade tensorflow==1.15**\r\n\r\nThat was meant to change my version from 2.20 to 1.15. I run the code in Pycharm terminal and it seems to have successfully installed. But when I try to check tensorflow version on Pycharm terminal, I can't get it. When I run the code, I get the original error of **ModuleNotFoundError: No module named 'tensorflow.contrib'**\r\n\r\nNot sure of what I should try out next. Thank you once again. ", "what shows when running `python -c 'import tensorflow as tf; print(tf.__version__)'` ?", "Thank you. It shows that I have tensorflow version 1.15. Could there be a way that I can connect it to Pycharm like the way it synchronizes python because now, I see, I have tensorflow installed, but Pycharm isn't getting it like the way is connects to python as the interpreter. Thank you once again for the help. ", "@blanshe,\r\nPlease take a look at [this guide](https://www.jetbrains.com/help/pycharm/installing-uninstalling-and-upgrading-packages.html?keymap=primary_macos) to install Python packages from PyCharm itself. Try installing TensorFlow using the guide and let us know if it works. \r\n\r\nSeems like PyCharm is using a different Python interpreter when compared to the terminal. Please refer to [this guide](https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html?keymap=primary_macos) for more information. Thanks!", "I am really grateful. It has worked. I went to python interpreter settings and downgraded tensorflow from 2.3 to 1.15 version. Thank you all. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41830\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41830\">No</a>\n", "Hello, i am trying to run tensorflow, with the following setup:\r\nmacbook mid 2009\r\nUbuntu 20.04\r\nPycharm latest version\r\npython 3.7.9, installed with pycharm\r\n\r\nnow if i use tensorflow 2.3.1 i get an error 132\r\nif i run tensorflow 1.5 ModuleNotFoundError: No module named 'tensorflow.python.platform' but it is install on the virtual environment.\r\n\r\nis there a fix or a work around?", "@fabiogeraci,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n"]}, {"number": 41829, "title": " Specified output array \"'TFLite_Detection_PostProcess'\" is not produced by any op in this graph.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip installed\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\ntflite_convert --graph_def_file=C:/tensorflow4/models/research/object_detection/TFLite_model/tflite_graph.pb --output_file=tflite/detect.tflite --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=127 --change_concat_input_ranges=false --allow_custom_ops\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n2020-07-28 18:33:11.363228: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-07-28 18:33:11.531507: F tensorflow/lite/toco/tooling_util.cc:935] Check failed: GetOpWithOutput(model, output_array) Specified output array \"'TFLite_Detection_PostProcess'\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x000023ec (most recent call first):\r\n  File \"c:\\users\\sreed\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 52 in execute\r\n  File \"c:\\users\\sreed\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\absl\\app.py\", line 250 in _run_main\r\n  File \"c:\\users\\sreed\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\absl\\app.py\", line 299 in run\r\n  File \"c:\\users\\sreed\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40 in run\r\n  File \"c:\\users\\sreed\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 89 in main\r\n  File \"C:\\Users\\Sreed\\Anaconda3\\envs\\tensorflow2\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7 in <module>\r\n  File \"c:\\users\\sreed\\anaconda3\\envs\\tensorflow2\\lib\\runpy.py\", line 85 in _run_code\r\n  File \"c:\\users\\sreed\\anaconda3\\envs\\tensorflow2\\lib\\runpy.py\", line 193 in _run_module_as_main\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n https://github.com/Dasinator21/Replicate-Error\r\n```\r\n# Put link here or attach to the issue.\r\n```https://github.com/Dasinator21/Replicate-Error\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Dasinator21 \r\nCould you please try removing quotes as below and let us know if it helps:\r\ninstead of using\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' Use:\r\n--output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\r\n\r\nas per issues with similar error:\r\n#34143 [link](https://stackoverflow.com/questions/58282082/error-when-covert-pb-to-tflite-ssd-mobilenet-v2-windows-10) \r\n\r\n", "> @Dasinator21\r\n> Could you please try removing quotes as below and let us know if it helps:\r\n> instead of using\r\n> --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' Use:\r\n> --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\r\n> \r\n> as per issues with similar error:\r\n> #34143 [link](https://stackoverflow.com/questions/58282082/error-when-covert-pb-to-tflite-ssd-mobilenet-v2-windows-10)\r\n\r\nIt seems to work, I will see I can run a code to run an inference with the tflite model. (Sorry for any misuse of terminology)", "It works thank you!", "@Dasinator21 \r\nHappy to help, moving this to closed status as the issue is resolved."]}, {"number": 41828, "title": "Please upload tensorflow-gpu-estimator 2.3 to pypi", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pypi\r\n- TensorFlow version: 2.3\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nPlease upload tensorflow-gpu-estimator 2.3 to Pypi.  There's tensorflow-estimator 2.3 but not tensorflow-gpu-estimator 2.3.  Pypi only have 2.2 as of now.\r\n\r\nI have TF 2.2, when I upgrade to 2.3, it failed because it could not find co-dependency: tensorflow-gpu-estimator 2.3\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`pip install tensorflow-gpu -U` results in:\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu-estimator<2.4.0,>=2.3.0 (from tensorflow-gpu) (from versions: 2.1.0, 2.2.0)\r\nERROR: No matching distribution found for tensorflow-gpu-estimator<2.4.0,>=2.3.0 (from tensorflow-gpu)\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Apologies for the delay. Should be done now: https://pypi.org/project/tensorflow-gpu-estimator/2.3.0/", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41828\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41828\">No</a>\n"]}, {"number": 41826, "title": "Hadoop Filesystem for C API Modular filesystem", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it: Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis issue is used to track the implementation of `hadoop` filesystem for c api modular filesystem\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nUsers\r\n\r\ncc @mihaimaruseac ", "comments": ["@mihaimaruseac \r\nI can not file any suitable place so I create an issue here. Currently, the `core` implementation of `hadoop` use 2 functions:\r\n- https://github.com/tensorflow/tensorflow/blob/e82d258f17bb47ce5fe8daa5c88cbbd7fcc0924b/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L73\r\n- https://github.com/tensorflow/tensorflow/blob/e82d258f17bb47ce5fe8daa5c88cbbd7fcc0924b/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L38\r\n\r\nBecause we can not build `libhdfs.so` ( C interface using JAVA and C ), we have to load it and get the symbols inside it. This is not possible because `C API` does not support these functions. So we have 2 options:\r\n- Add these functions to `C API`.\r\n- Build our own library [`libhdfspp.so`](https://github.com/apache/hadoop/tree/trunk/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp) ( C++ interface with C++ and Proto ). I prefer this approach but converting from `cmake` to `bazel` is troublesome and will take a lot of time ( of course I will continue to work until everything is done ).\r\n\r\nWhich one do you prefer ?", "I would go for the first option as it is reusable. We will need to load libraries in the future too.\r\n\r\nNote that we have one of them already (or very similar) https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/c_api.h;l=1431-1443;drc=24f835217fd27f21141ff0254a2b93ea3cfd7b6c", "@vnvo2409 \r\nPlease update as per above comment.", "That function return a `TF_Library*` https://github.com/tensorflow/tensorflow/blob/bf86c225f059bd6f8e1624001f4644cce2c18fe8/tensorflow/c/c_api.h#L1442\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bf86c225f059bd6f8e1624001f4644cce2c18fe8/tensorflow/c/c_api_internal.h#L65-L68\r\n\r\nI would like to ask whether `op_list` is the list of the symbols of the library or we have to add a new function `TF_GetSymbolFromLibrary` into `c_api.h`\r\n\r\n\r\n\r\n", "Hmm, no, the `op_list` is just the list of TF ops included in the library. It's a protobuf, not something we can use unfortunately.\r\n\r\nLooking around, I found that `TF_LoadLibrary` ends up calling [`tensorflow::LoadLibrary`](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/framework/load_library.cc;l=46;drc=d5e414d34ab4b9d918e1eadd5428caf3ef99d6fe) which is different from [`tensorflow::Env::LoadLibrary`](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/env.h;l=337;drc=791f4f2b773208b21ece71e1d37204a91720346d) that we need.\r\n\r\nI think now that we will actually need to expose both symbols from `tensorflow/core/platform/env.h` into `tensorflow/c/c_api.h`.", "I will send you a PR right now"]}, {"number": 41825, "title": "Tensorflow-Lite on NDK with C-API fails to provide output - no errors or warnings", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Based on example script\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10\r\n- TensorFlow Lite version: 2.2\r\n- Python version: 3.6\r\n- GCC/Compiler version (if compiling from source): clang++ version 7.0\r\n\r\n**Describe the current behavior**\r\n\r\nPreviously I was running TFLite on Android, with the Java API.\r\nRunning the .tflite model from Android's Java API works fine, and I get the expected output.\r\n\r\nNow I'm running TFLite on NDK, using the C-API.\r\nInvoking the interpreter in the NDK doesn't seem to do anything.\r\n\r\nI can verify the the model is loaded (does not return null) and setting everything up (interpreter / options / input and output tensors / etc) seems OK too.\r\n\r\nThe input and output are both float32 arrays:\r\n- I can verify input array data integrity before `TfLiteTensorCopyFromBuffer` (all float values are correct and none-zero).\r\n- Output array is a float array (in the proper size) initialized with zeros.\r\nExtracting the inference result with `TfLiteTensorCopyToBuffer` does not change the output array - it's still all zeros.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nInvoking the interpreter should provide an output array, a result.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nIn my native C++ file:\r\n\r\n```\r\n// Following the example in c_api.h\r\n\r\n// Input\r\n// input is just a single tensor in size\r\n// input is: vector<float> input_float_array;\r\n// accessing input values with input_float_array.at(x), for example,\r\n// provides float32 values - the first few are:\r\n// [ 43.36578, 72.9673, 98.4356, 12.7865, ... ]\r\n\r\n// Output\r\n// output is just a single tensor in size\r\n// output is: vector<float> output_float_array;\r\n// output is initialized with zeros:\r\n// [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]\r\n// and is not constant\r\n\r\nTfLiteModel * model = TfLiteModelCreateFromFile(\r\n                reinterpret_cast<const char *>(model_file_path));\r\n\r\n// checkpoint \"model\":\r\n// if (model == nullptr)\r\n// result: model is not null - model_file_path found\r\n// in model_file_path there's a .tflite file\r\n\r\nTfLiteInterpreterOptions * options = TfLiteInterpreterOptionsCreate();\r\nTfLiteInterpreterOptionsSetNumThreads(options, 2);\r\nTfLiteInterpreter * interpreter = TfLiteInterpreterCreate(model, options);\r\nTfLiteInterpreterAllocateTensors(interpreter);\r\nTfLiteTensor * input_tensor =\r\n        TfLiteInterpreterGetInputTensor(interpreter, 0);\r\n\r\n// checkpoint \"before\"\r\n// input_float_array has legitimate float values, at this point:\r\n// [ 43.36578, 72.9673, 98.4356, 12.7865, ... ]\r\n\r\nTfLiteTensorCopyFromBuffer(input_tensor,\r\n        input_float_array.data(),\r\n        input_float_array.size() * sizeof(float));\r\nTfLiteInterpreterInvoke(interpreter);\r\nconst TfLiteTensor * output_tensor =\r\n        TfLiteInterpreterGetOutputTensor(interpreter, 0);\r\nTfLiteTensorCopyToBuffer(output_tensor,\r\n        output_float_array.data(),\r\n        output_float_array.size() * sizeof(float));\r\n\r\n// checkpoint \"after\"\r\n// output_float_array remained all zeros, at this point:\r\n// [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]\r\n\r\nTfLiteInterpreterDelete(interpreter);\r\nTfLiteInterpreterOptionsDelete(options);\r\nTfLiteModelDelete(model);\r\n\r\n// some alternatives to access data and verify integrity:\r\n//\r\n// replacing \"input_float_array.data()\" with \"&(* input_float_array.begin())\"\r\n// and \"output_float_array.data()\" with \"&(* output_float_array.begin())\"\r\n//\r\n// initializing output_float_array with ones instead of zeros:\r\n// [ 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\r\n\r\n// I also tried to change a few models\r\n// (all with same input / output and characteristics)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nNo errors of any kind or anything to show.\r\nThe code runs smoothly, but nothing happens - the output array remains unchanged.\r\nThe exact same models, when run with Java API, work just fine.", "comments": ["### Update\r\n\r\nTo create the original model I used Keras.\r\nNow I've created a new model - this is the same model only I took out 2 operators from the original one:\r\n\r\n1. Conv2DTranspose\r\n2. BatchNormalization\r\n\r\nand I do get results from the model - it's not the expected result values, it's just a mess.\r\nBoth operators appear in [TF / TFL ops compatibility](https://www.tensorflow.org/lite/guide/ops_compatibility).", "@orangesomethingorange Do you std::cout the TfLiteTensorCopyFromBuffer() and TfLiteTensorCopyToBuffer() return value?They must be  kTfLiteOK.", "Hey @patterson163 ,\r\nYes - both `TfLiteTensorCopyFromBuffer` and `TfLiteTensorCopyToBuffer`'s return values are `kTfLiteOK`.", "### Update 2\r\n\r\nI created 2 more models:\r\n\r\n1. Identity model\r\n\r\n```\r\nin = Input(...)\r\nout = in\r\nmodel = Model(inputs = [in], outputs = [out])\r\n```\r\n\r\n2. Single operator (Conv2D) model\r\n\r\n```\r\nin = Input(...)\r\na = in\r\nb = Conv2D(...)(a)\r\nout = b\r\nmodel = Model(inputs = [in], outputs = [out])\r\n```\r\n\r\nWhen running inference with the identity model - the values are again unchanged (output float array was initialized with ones, and stays that way).\r\nWhen running inference with the single convolution model - I do see results (meaning non-zero float values in the output float array), but again - same as the first update, it's not the expected results but some mess.", "Running the same model in Android's Runtime (in Java) produce the expected results.\r\nIsn't Java only an envelope to the C-code?\r\n\r\nAre the input / output (byte) arrays expected to be in some unique state?", "Would you mind attaching one of the .tflite models that you're using? I'm assuming these results are being generated on the Android device? Did you use our prebuilt C API libs, or did you build from source? If the latter, what build command did you use (and which NDK version)?\r\n\r\nWe saw a similar issue reported previously (https://github.com/tensorflow/tensorflow/issues/40318), but it was an issue with using the wrong number of elements in the input/output.\r\n\r\n", "Hey @jdduke ,\r\n\r\nYes, the results are generated on an Android device.\r\nI used the prebuilt libraries (following the instructions [here](https://www.tensorflow.org/lite/guide/android#use_tflite_c_api)).\r\nI did not build from source, but still - NDK version is `android-ndk-r18b (ver 18.1.5063045)`.\r\n\r\nI only got all zeros on some models.\r\nFollowing your reference to [Issue #40318](https://github.com/tensorflow/tensorflow/issues/40318), I played with the input/output sizes passed to `TfLiteTensorCopyFromBuffer` / `TfLiteTensorCopyToBuffer`. Output was indeed changed, but still not a desired result - still only a mess of values.\r\n\r\nWhen using a model which provided non-zero results, I compared the following calculations:\r\n\r\n1. `size_t input_data_size_1 = input_float_array.size() * sizeof(float)`\r\n2. `size_t input_data_size_2 = TfLiteTensorByteSize(input_tensor)`\r\n3. `size_t input_data_size_3 = input_image_size * input_image_channels * sizeof(float)`\r\n\r\n(All variables are exactly what you think they are).\r\n** Did the same for the output.\r\n\r\nAll above data size calculations get the exact same (positive) value.\r\nIncreasing and decreasing that size does change the output, but still just a mess of float values.\r\n\r\nI'm attaching a link below with some files:\r\n\r\n- Two .tflite model files: one has a single operator and the other has multiple operators.\r\n- Some 512x512 input images (which were translated into input float arrays) that I got from the internet.\r\n- The expected results for an output image (in this case the green channel is colored, the result is 1-dim).\r\n- The output float array results (translated back into an image) that I got on the Android device.\r\n\r\n(*) Note that the input has 12 channels and the output just 1.\r\n\r\nSee [this link](https://gofile.io/d/t0HttZ) for the files.", "Thanks for the repro. And you used the prebuilt libs from the [TFLite 2.2 release](https://bintray.com/google/tensorflow/tensorflow-lite#files/org%2Ftensorflow%2Ftensorflow-lite%2F2.2.0)?", "> Thanks for the repro. And you used the prebuilt libs from the [TFLite 2.2 release](https://bintray.com/google/tensorflow/tensorflow-lite#files/org%2Ftensorflow%2Ftensorflow-lite%2F2.2.0)?\r\n\r\nYes", "Also, I created the single-operator model again, but I set the input patch-size to 4x4 (instead of 512x512) and it works fine (input sizes of 512x512 / 1200x1600 - I get the expected results).\r\n\r\nYou've mentioned the `memcpy` operation in the tensor buffer - does TFLite limit memory allocation by any chance?", "I've tried a local repro but don't observe the same issue.\r\n\r\n> You've mentioned the memcpy operation in the tensor buffer - does TFLite limit memory allocation by any chance?\r\n\r\nHere is the implementation: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api.cc#L215. We just validate that the provided byte length matches the tensor byte length.\r\n\r\nOne thing you could do is check the TfLiteTensor raw buffer value, that is, the `input_tensor->data.f` and `output_tensor->data.f` array, just to make sure it's not an issue with the CopyFrom/CopyTo buffer calls.\r\n\r\n> Some 512x512 input images (which were translated into input float arrays) that I got from the internet.\r\n\r\nCan you explain how you loaded the 512x512 RGB input image into a 12-channel (512x512x12) input tensor? \r\n\r\n> TfLiteInterpreterInvoke(interpreter);\r\n\r\nCan you also make sure that the return value for this call was kTfLiteOk?\r\n\r\n", "Hey @jdduke ,\r\n\r\n### Regarding the issue:\r\n\r\nThe issue was in my design - to speed up performance I'm passing a pointer to the Native environment.\r\nApparently on Android when encoding / decoding images (which may be passed through a `Bitmap` object) with the same `stride` and `size` parameters there's a default header shift.\r\nI think this issue is addressed in Android 11's [Image Decoder API](https://developer.android.com/ndk/guides/image-decoder) - I was using an older Android API.\r\nThis issue did not occur when using Tensorflow-Lite via the Java API.\r\n\r\nCurrently I can get the inference to run OK, but I have to copy the image (or patches) data, serially, 4 times (2 for the Native environment pointer and 2 for the TF buffer), which slows the whole thing down (which kinda defeats the purpose of running an inference in the Native environment). I'll have to solve the performance issue now.\r\n\r\nAlso - for the identity model I replaced `out = in` with `out = tensorflow.identity(in)`, which seem to be helpful.\r\n\r\n### Regarding your questions (for future reference mostly):\r\n\r\nThe input image (3 channels) object was cloned 4 times and \"stacked\" one on top of the other, in a way that mimics Numpy arrays behavior (since this is what I used on the Tensorflow model), so each \"pixel\" (that is, every 3 float values) was cloned 4 times - \r\nFor example: every 4 bytes represent a single float value `f`, and each float value `f` will represent a single value in a single channel, within one matrix (so for a gray-scale image `<f>` is the pixel value, and for an RGB image `<fa,fb,fc>` is the pixel value). If the first pixel is represented by `<f0,f1,f2>`, the second pixel is represented by `<f3,f4,f5>`, and so on - then the byte array of the input given to the inference will be bytes representing the data in the following order:\r\n`<f0,f1,f2>, <f0,f1,f2>, <f0,f1,f2>, <f0,f1,f2> | <f3,f4,f5>, <f3,f4,f5>, <f3,f4,f5>, <f3,f4,f5> | ...` and so on and so forth.\r\n\r\nReturn values from `TfLiteTensorCopyToBuffer` / `TfLiteTensorCopyFromBuffer` we're both `kTfLiteOk`.", "Thanks :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41825\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41825\">No</a>\n", "Thanks for the update!\r\n\r\nIn general, Java-based inference performance should be close to native inference performance when using ByteBuffer (or FloatBuffer) inputs and outputs. We are working on a (formal) way of allowing the user to inject their own CPU buffer in C++ to avoid copying from their source buffer to the Tensor buffer. There will be an update in the release notes for this.", "> Thanks for the update!\r\n> \r\n> In general, Java-based inference performance should be close to native inference performance when using ByteBuffer (or FloatBuffer) inputs and outputs. We are working on a (formal) way of allowing the user to inject their own CPU buffer in C++ to avoid copying from their source buffer to the Tensor buffer. There will be an update in the release notes for this.\r\n\r\nThat's great news! :+1:  I'm having issues with my algorithm's runtime, and the buffer-copies are the biggest part of that... Thanks for the info - I'll be sure to check the release notes :)"]}, {"number": 41824, "title": "S3 filesystem test part 1", "body": "@mihaimaruseac \r\nThis PR adds 2 test for `s3_filesystem`.", "comments": []}, {"number": 41823, "title": "Tensorflow Serving 2.2.0 error: output_shape has incorrect number of elements: 2 should be: 1", "body": "I have a SavedModel, which a am serving with Tensorflow Serving. When I load and run it in a separate script, it's working fine. But sending a request to the serving API throws the following error:\r\n\r\n    {\r\n        \"error\": \"[_Derived_]{{function_node __inference__wrapped_model_6314}} {{function_node __inference__wrapped_model_6314}} output_shape has incorrect number of elements: 2 should be: 1\\n\\t [[{{node model/feature_input/Code_trait._indicator/SparseToDense}}]]\\n\\t [[StatefulPartitionedCall_66/StatefulPartitionedCall]]\"\r\n    }\r\n\r\nThe normal output shape of the model is (1, 1).\r\n\r\nThe curl call I use is:\r\n\r\n    curl -d '{\"instances\":[{\"Sexe\":\"M\",\"MCD\":5,\"Code_diag.\":\"I33.5\",\"Code_trait.\":\"b66\",\"Med._traitant\":\"RickRich\",\"DRG\":\"x\",\"N_med.\":\"5324\",\"Traitement_principal\":[126,178,177,175,162,93,160,165,172,160,93,162,169,162,160,177],\"Operation\":[161,205,208,205,204,191,208,205,197,208,191,206,198,199,195,0],\"Diag._principal\":[144,179,172,188,52,23,6,7,106,26,106,176,182,191,190,35],\"Diagnostic\":[181,186,194,177,191,192,181,179,173,192,181,187,186,191,108,175]}]}' -X POST http://localhost:8501/v1/models/Excel_AI_Model:predict\r\n  \r\n\r\nThe versions used to create the SavedModel are the following: \r\n\r\nPython version:  3.6.9\r\n\r\nTensorflow version:  2.2.0\r\n\r\nThe Serving is the docker image tensorflow/serving:2.2.0.\r\n\r\nThe model metadata while served is this:\r\n\r\n        {\r\n    \"model_spec\":{\r\n     \"name\": \"Excel_AI_Model\",\r\n     \"signature_name\": \"\",\r\n     \"version\": \"4\"\r\n    }\r\n    ,\r\n    \"metadata\": {\"signature_def\": {\r\n     \"signature_def\": {\r\n      \"serving_default\": {\r\n       \"inputs\": {\r\n        \"Operation\": {\r\n         \"dtype\": \"DT_INT32\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"16\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_Operation:0\"\r\n        },\r\n        \"DRG\": {\r\n         \"dtype\": \"DT_STRING\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_DRG:0\"\r\n        },\r\n        \"Code_trait.\": {\r\n         \"dtype\": \"DT_STRING\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_Code_trait.:0\"\r\n        },\r\n        \"MCD\": {\r\n         \"dtype\": \"DT_INT32\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_MCD:0\"\r\n        },\r\n        \"Diagnostic\": {\r\n         \"dtype\": \"DT_INT32\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"16\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_Diagnostic:0\"\r\n        },\r\n        \"Traitement_principal\": {\r\n         \"dtype\": \"DT_INT32\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"16\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_Traitement_principal:0\"\r\n        },\r\n        \"N_med.\": {\r\n         \"dtype\": \"DT_STRING\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_N_med.:0\"\r\n        },\r\n        \"Code_diag.\": {\r\n         \"dtype\": \"DT_STRING\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_Code_diag.:0\"\r\n        },\r\n        \"Diag._principal\": {\r\n         \"dtype\": \"DT_INT32\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"16\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_Diag._principal:0\"\r\n        },\r\n        \"Sexe\": {\r\n         \"dtype\": \"DT_STRING\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_Sexe:0\"\r\n        },\r\n        \"Med._traitant\": {\r\n         \"dtype\": \"DT_STRING\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"serving_default_Med._traitant:0\"\r\n        }\r\n       },\r\n       \"outputs\": {\r\n        \"result_output\": {\r\n         \"dtype\": \"DT_FLOAT\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           },\r\n           {\r\n            \"size\": \"1\",\r\n            \"name\": \"\"\r\n           }\r\n          ],\r\n          \"unknown_rank\": false\r\n         },\r\n         \"name\": \"StatefulPartitionedCall_66:0\"\r\n        }\r\n       },\r\n       \"method_name\": \"tensorflow/serving/predict\"\r\n      },\r\n      \"__saved_model_init_op\": {\r\n       \"inputs\": {},\r\n       \"outputs\": {\r\n        \"__saved_model_init_op\": {\r\n         \"dtype\": \"DT_INVALID\",\r\n         \"tensor_shape\": {\r\n          \"dim\": [],\r\n          \"unknown_rank\": true\r\n         },\r\n         \"name\": \"NoOp\"\r\n        }\r\n       },\r\n       \"method_name\": \"\"\r\n      }\r\n     }\r\n    }\r\n    }\r\n    }\r\n\r\nAlready posted this on stackoverflow, but got no reply:\r\nhttps://stackoverflow.com/questions/63073490/tensorflow-serving-2-2-0-error-output-shape-has-incorrect-number-of-elements-2", "comments": ["@ansreng,\r\nTensorFlow Serving issues are handled in the Serving repo. Could you please create a new issue from [this link](https://github.com/tensorflow/serving/issues/new) and fill in the template, so that we can track the issue there. Thanks!", "Created new issue in the Tensorflow Serving repo:\r\nhttps://github.com/tensorflow/serving/issues/1703"]}, {"number": 41822, "title": "fix windows 10 build dependency error", "body": "fix for windows 10 with CUDA 11.0 build\r\n\r\n#41446 ", "comments": []}, {"number": 41821, "title": "Linking error: \"undefined reference to\" - using CMSIS NN", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): make with Linux Ubuntu 20.04, complied on Atmel Studio on Windows 10, Python 3.7.7\r\n- TensorFlow installed from (source or binary): downloaded from master\r\n- Tensorflow version (commit SHA if source): 2.3.0, e544dce3a3a43631811e0760db5c33fe0a7519ba\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Atmel SAMD51 - Atmel Studio\r\n\r\n**Describe the problem**\r\nI have made the projects as described on the tflite for microcontroller webpage specifying TAGS=cmsis-nn to use the optimized backend. I have then moved the files from magic wand project to my project on Atmel Studio, making sure to add all the directories in a correct manner and compiled the project. Compilation is succesfull but I get a bunch of errors when trying to link:\r\n```\r\nSeverity\tCode\tDescription\tProject\tFile\tLine\r\nError\t\trecipe for target 'TFLite_SAMD51.elf' failed\tTFLite_SAMD51\tDebug\\Makefile\t1314\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\reference\\conv.h\t69\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\reference\\conv.h\t149\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\reference\\integer_ops\\conv.h\t72\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\reference\\integer_ops\\pooling.h\t122\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\reference\\pooling.h\t118\r\nError\t\tundefined reference to `tflite::micro::GetEvalInput(TfLiteContext const*, TfLiteNode const*, int)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\types.h\t390\r\nError\t\tundefined reference to `tflite::micro::GetEvalOutput(TfLiteContext const*, TfLiteNode const*, int)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\types.h\t390\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\types.h\t390\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\types.h\t391\r\nError\t\tundefined reference to `tflite::micro::GetEvalInput(TfLiteContext const*, TfLiteNode const*, int)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\types.h\t391\r\nError\t\tundefined reference to `tflite::micro::GetEvalOutput(TfLiteContext const*, TfLiteNode const*, int)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\types.h\t391\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\types.h\t391\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\kernels\\internal\\types.h\t392\r\nError\t\tundefined reference to `tflite::micro::GetEvalInput(TfLiteContext const*, TfLiteNode const*, int)'\tTFLite_SAMD51\ttensorflow\\lite\\micro\\kernels\\conv.cpp\t308\r\nError\t\tundefined reference to `tflite::micro::GetEvalOutput(TfLiteContext const*, TfLiteNode const*, int)'\tTFLite_SAMD51\ttensorflow\\lite\\micro\\kernels\\conv.cpp\t308\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\micro\\kernels\\fully_connected.cpp\t178\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\micro\\kernels\\fully_connected.cpp\t178\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\micro\\kernels\\fully_connected.cpp\t202\r\nError\t\tundefined reference to `tflite::micro::GetEvalInput(TfLiteContext const*, TfLiteNode const*, int)'\tTFLite_SAMD51\ttensorflow\\lite\\micro\\kernels\\fully_connected.cpp\t228\r\nError\t\tundefined reference to `tflite::micro::GetEvalOutput(TfLiteContext const*, TfLiteNode const*, int)'\tTFLite_SAMD51\ttensorflow\\lite\\micro\\kernels\\fully_connected.cpp\t228\r\nError\t\tundefined reference to `tflite::micro::GetTensorShape(TfLiteEvalTensor const*)'\tTFLite_SAMD51\ttensorflow\\lite\\micro\\kernels\\fully_connected.cpp\t228\r\n```\r\nI have adapted the magic wand project made without the CMSIS-NN tag and it runs smoothly.\r\nThe working non-cmsis can be found here: https://github.com/Sixaxis9/TFLite-SAMD51\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn generate_projects`\r\ngo to `tensorflow/lite/micro/tools/make/gen/linux_x86_64/prj/magic_wand/tensorflow_lite/src` and copy the tensorflow and third_parties folders into an Atmel studio project\r\nAdded `./` to the directory in compiler settings\r\nUsed the same main as the non cmsis variant I have been using succesfully until now (derived anyway from the example).", "comments": ["@Sixaxis9 \r\nPlease provide with simple stand alone code for us to replicate the issue faced or a colab gist with the error.\r\n\r\n", "@Saduf2019 \r\nThank you for your reply. Eventually I succeded in building TFLite as a static library. Linking the .a instead of integrating the C code of tensorflow does not report any error.\r\nNow I get a runtime error: \"Only 1 subgraph is currently supported. Exiting with status 1\", for which I will open a new issue since (might be) unrelated to this problem. \r\nP.s. The model I tried has only one subgraph and works with the non-cmsis implementation of tflite."]}, {"number": 41820, "title": "windows build error(makedataset)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2019\r\n- CUDA/cuDNN version: 11.0/8.0.2\r\n- GPU model and memory: RTX2070 GDDR6 8GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nbuild error ( link error )\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n./configure\r\nbazel build --copt=-nvcc_options=disable-warnings --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\noptimize_dataset_op.lo.lib(optimize_dataset_op.obj) : error LNK2019: unresolved external symbol \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl tensorflow::port::JobName(void)\" (?JobName@port@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ) referenced in function \"protected: virtual void __cdecl tensorflow::data::OptimizeDatasetOp::MakeDataset(class tensorflow::OpKernelContext *,class tensorflow::data::DatasetBase *,class tensorflow::data::DatasetBase * *)\" (?MakeDataset@OptimizeDatasetOp@data@tensorflow@@MEAAXPEAVOpKernelContext@3@PEAVDatasetBase@23@PEAPEAV523@@Z)\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\python\\_pywrap_tensorflow_internal.so : fatal error LNK1120: 1 unresolved externals\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "comments": ["Looks like a missing symbol issue about tf.data ?\r\n", "This has been fixed by https://github.com/tensorflow/tensorflow/commit/a912a8ed6cc873e1b4ed5de0fb0524d2e499ea34", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41820\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41820\">No</a>\n", "Thank you @jsimsa "]}, {"number": 41819, "title": "[TFLite] 16x8 quantization: fixes for SLICE, TRANSPOSE operators", "body": "This PR adds fixes for operators SLICE, TRANSPOSE to make them work for a model quantized in 16x8 quantization mode.\r\nI haven't added versioning for these operators, because it is missing or inconsistent currently.\r\nThese changes have been done to make quantization/inference work on a real model. ", "comments": ["Hi @renjie-liu I added versioning for 16x8 case to these operators. Please take a look. Thanks!"]}, {"number": 41818, "title": "esp - No rule to make target person_detect_model_data.cc", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nmacOS Catalina - 10.15.5 (19F101)\r\n\r\n- TensorFlow installed from (source or binary):\r\n\r\nsource cloned from git - have tried r2.2, r2.3 and master\r\n\r\n- Tensorflow version (commit SHA if source):\r\n\r\nf295633406569f9a6ee71467a9bb34ef1cc6852b\r\n\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\nESP32\r\n\r\n**Describe the problem**\r\n\r\nTrying to generate all projects fails\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n```\r\n$make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_projects\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/ruy'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/ruy'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\nmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/person_detection/make/tensorflow/lite/micro/examples/person_detection/person_model_grayscale/person_detect_model_data.cc', needed by 'generate_person_detection_make_project'.  Stop.\r\n```\r\n", "comments": ["ok - I think this is just me not understanding how this should work - running:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp  generate_person_detection_esp_project\r\n```\r\n\r\nSeems to work - am I right that each project needs to be generated individually?", "That's correct. See example usage below;\r\nhttps://github.com/tensorflow/tensorflow/blob/a4725be4a74c9acae623231e80d5a8002a54e94a/tensorflow/lite/micro/tools/make/Makefile#L10\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}]