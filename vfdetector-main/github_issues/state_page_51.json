[{"number": 42594, "title": "Unexpected behavior , none persistent errors and, Loop execution was cancelled.", "body": "hello ,\r\n\r\nI having an issue with code piece of code I wrote to perform a customer operation in Tensor flow.  The code generally works fine but sometimes throws unexpected errors and exceptions even when the same input is used. \r\n\r\n\r\nSystem information\r\n-Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n-OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n-Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n-TensorFlow installed from (source or binary): Binary\r\n-TensorFlow version (use command below): '2.3.0'\r\n-Python version: 3.7\r\n-Bazel version (if compiling from source): NA\r\n-GCC/Compiler version (if compiling from source): NA\r\n-CUDA/cuDNN version: 10.1\r\n-GPU model and memory: RTX 2080 TI\r\nYou can collect some of this information using our environment capture\r\n\r\n**Describe the current behavior**\r\nI have written some tensorflow code to perform multiplication on  RaggedTensors.  My issue is that code sometimes works fine and other times throws unexpected errors even when the same input is used\r\n\r\n**Describe the expected behavior**\r\nI would expect tensor flow to be more persistent/consistent in errors. especially when the same input is used, for example the same input sometimes generates the correct output , other times I get an `[_Derived_]Loop execution was cancelled` and some times I also get `Expected size[0] in [0, 0], but got 3`\r\n\r\n**Standalone code to reproduce the issue**\r\nI have  tired to reduce the code to a smaller example unfortunately I could not since the error is being thrown at runtime and I'm not sure which part of my code is the source of the issue. , below is my code with comments : \r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.compat.v1.disable_eager_execution()\r\ntf.disable_v2_behavior()\r\n\r\n\r\nmyTensor_values = tf.placeholder(dtype=tf.float32)\r\nmyTensor_l2_splits = tf.placeholder(dtype=tf.int32)\r\nmyTensor_l1_splits = tf.placeholder(dtype=tf.int32)\r\n\r\n\r\ndef innerloop_processing(begin_index , end_index , input1) : \r\n    innerloop_counter = begin_index\r\n    ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )\r\n    def innerloop_body(counter , begin_index , end_index , input1 , ta) : \r\n        inner_being_index = input1[1][counter]\r\n        inner_end_index = input1[1][counter+1]\r\n        row = tf.slice(input1[0] , [inner_being_index] ,  [inner_end_index-inner_being_index])\r\n        ta = ta.write(counter-begin_index , row)\r\n        counter = counter + 1 \r\n        return counter , begin_index , end_index , input1 , ta\r\n    \r\n    \r\n    def innerloop_cond(counter , begin_index , end_index , input1 , ta ) : \r\n        return input1[1][counter] < input1[1][end_index] -1  #stop at the next pointer of the l2_splits \r\n \r\n    results = tf.while_loop(innerloop_cond , innerloop_body , [innerloop_counter , begin_index , end_index , input1 , ta] )\r\n    print_resutls = tf.print(\"this is the component result  :\" , results[4].stack())\r\n    return results[4].stack()\r\n\r\n\r\ndef generateL1Tensor_writeback(start_offest,step,num):\r\n    counter=tf.constant(0,tf.int32)\r\n    values = tf.TensorArray(tf.int32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )\r\n    def cond(values , start_offest , num ,counter) : \r\n        return counter*step <= num*step\r\n    def body(values , start_offest , num ,counter) : \r\n        values = values.write(counter,[(counter*step)+start_offest])\r\n        counter = counter+1\r\n        return  values , start_offest , num ,counter\r\n    \r\n    final_values , _ , _ , _  = tf.while_loop(cond,body,[values , start_offest , num , counter])\r\n    final = final_values.concat()\r\n    #print_line = tf.print(\" xxxxx This is the is the split : \" ,  final)\r\n    return final\r\n\r\ndef multiply2n_ragged(tensor1 , tensor2) : \r\n    #this  function multiplies two ragged tesnsors of rank 2 . the most outer ranks of the two tensros must be equal .\r\n    #setting variables and constats \r\n    outerloop_counter = tf.constant(0 , dtype=tf.int32)\r\n    carry_on = tf.constant(0 , dtype=tf.int32)\r\n    taValues = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )\r\n    taL2Splits = tf.TensorArray(tf.int32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )\r\n    taL1Splits = tf.TensorArray(tf.int32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )\r\n    taL1Splits = taL1Splits.write(0,[0]) ## required intialization for L1 split only\r\n    innerloop_processing_graphed = tf.function(innerloop_processing)\r\n    generateL1Tensor_writeback_graphed = tf.function(generateL1Tensor_writeback)\r\n    def outerloop_cond(counter,input1,input2 ,taValues  ,taL2Splits , taL1Splits , carry_on ) :\r\n        value = tf.shape(input1[2])[0]-1\r\n        return counter < value ## this is the length of the outermost dimision , stop of this \r\n    def outloop_body(counter,input1,input2, taValues  ,taL2Splits , taL1Splits , carry_on) : \r\n        l1_comp_begin = input1[2][counter]                  ## this is begin position of the current row in the outer split  ( ie. the ith value in the outer row split tensor ) \r\n        l1_comp_end = input1[2][counter+1]                  ## this is end position of the current row in the outer split   (ie. the ith + 1 value in the outer row split tensor)\r\n        l1_comp2_begin = input2[2][counter]                 ## we do the same for the second components \r\n        l1_comp2_end = input2[2][counter+1]                 ## we do the same for the second components\r\n        comp  = innerloop_processing_graphed(l1_comp_begin ,l1_comp_end ,input1  ) ## now retrive the data to be procesed for the selected rows from vector1\r\n        comp2  =innerloop_processing_graphed(l1_comp2_begin ,l1_comp2_end ,input2  ) ## do the same for vector 2 \r\n        \r\n        comp2 = tf.transpose(comp2) ### desired operation\r\n        multiply =tf.matmul(comp , comp2) #### This is the desired operation  \r\n\r\n        \r\n        myshape= tf.shape(multiply) ## calculate the shape of the result in order to prepare to write the result in a ragged tensor format. \r\n        offset = tf.cond( taValues.size() >0  ,lambda: tf.shape(taValues.concat())[0] , lambda : [0]) ### this is a hack, TensorArray.concat returns an error if the array is empty. Thus we check before calling this. \r\n        l2v = generateL1Tensor_writeback_graphed(offset,myshape[1],myshape[0])  # generate the inner row split of the result for the current element\r\n        taL2Splits=taL2Splits.write(counter,l2v) # write back the inner rowlplit to a TensorArray \r\n        taValues=taValues.write(counter,tf.reshape(multiply , [-1])) # wirte back the actual ragged tensor elemnts in a another TensorArray\r\n        carry_on=carry_on+myshape[0] ## required to calculate the outer row splite\r\n        taL1Splits=taL1Splits.write(counter+1,[carry_on]) ## This is the outmost row split. \r\n        counter = counter+1\r\n        return counter , input1,input2, taValues  ,taL2Splits , taL1Splits , carry_on\r\n    \r\n    outerloop_finalcounter , _ , _ , ta1,ta2,ta3,_ = tf.while_loop(outerloop_cond,outloop_body,[outerloop_counter , tensor1 , tensor2 ,taValues  ,taL2Splits , taL1Splits,carry_on])\r\n    uinquie_ta2 , _ = tf.unique(ta2.concat())  # this is required since some values might be duplicate in the row split itself \r\n    final_values = ta1.concat() , uinquie_ta2   ,ta3.concat()\r\n    return final_values\r\n\r\n\r\n\r\n\r\nt = myTensor_values , myTensor_l2_splits , myTensor_l1_splits\r\n\r\noo   =multiply2n_ragged(t,t)\r\nnew_oo = multiply2n_ragged(oo,oo)\r\n\r\n\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\r\nsess.run(tf.global_variables_initializer())\r\nvals =np.array([1.0, 2.2  , 1.1 , 4.0, 5.0 , 1.1 , 6.0, 7.0 , 1.1 , 8.0, 9.0 , 1.1 ,10.0, 11.0 , 1.1 ])\r\nl2_splits = np.array([0,3,6,9,12,15])\r\nl1_splits = np.array([0, 2, 5  ]) \r\nre       = sess.run([new_oo  ] , feed_dict={myTensor_values:vals ,myTensor_l1_splits:l1_splits ,myTensor_l2_splits:l2_splits  } )\r\nprint(re)\r\n\r\n```\r\n\r\n\r\n As I said the code works fine many times , however it some times generates the below errors for the same inputs . stack traces of the two different errors that I get : \r\n```\r\n\r\nthis is the component result  : [[1 2.2 1.1]\r\n [4 5 1.1]]\r\nthis is the component result  : [[6 7 1.1]\r\n [8 9 1.1]\r\n [10 11 1.1]]\r\nthis is the component result  : [[6 7 1.1]\r\n [8 9 1.1]\r\n [10 11 1.1]]\r\nthis is the component result  : [[1 2.2 1.1]\r\n [4 5 1.1]]\r\n---------------------------------------------------------------------------\r\nCancelledError                            Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n-> 1350                                       target_list, run_metadata)\r\n   1351 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1442                                             fetch_list, target_list,\r\n-> 1443                                             run_metadata)\r\n   1444 \r\n\r\nCancelledError: {{function_node __inference_innerloop_processing_11240}} {{function_node __inference_innerloop_processing_11240}} [_Derived_]Loop execution was cancelled.\r\n\t [[{{node while/LoopCond/_20}}]]\r\n\t [[while_27/StatefulPartitionedCall_1]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nCancelledError                            Traceback (most recent call last)\r\n<ipython-input-15-238a2ce9a03a> in <module>\r\n     94 l2_splits = np.array([0,3,6,9,12,15])\r\n     95 l1_splits = np.array([0, 2, 5  ])\r\n---> 96 re       = sess.run([new_oo  ] , feed_dict={myTensor_values:vals ,myTensor_l1_splits:l1_splits ,myTensor_l2_splits:l2_splits  } )\r\n     97 print(re)\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    956     try:\r\n    957       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 958                          run_metadata_ptr)\r\n    959       if run_metadata:\r\n    960         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1179     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1180       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1181                              feed_dict_tensor, options, run_metadata)\r\n   1182     else:\r\n   1183       results = []\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1357     if handle is None:\r\n   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1359                            run_metadata)\r\n   1360     else:\r\n   1361       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nCancelledError:   [_Derived_]Loop execution was cancelled.\r\n\t [[{{node while/LoopCond/_20}}]]\r\n\t [[while_27/StatefulPartitionedCall_1]]\r\n\r\n```\r\n\r\n\r\nAnother Error is : \r\n```\r\n\r\nthis is the component result  : [[1 2.2 1.1]\r\n [4 5 1.1]]\r\nthis is the component result  : [[1 2.2 1.1]\r\n [4 5 1.1]]\r\nthis is the component result  : [[6 7 1.1]\r\n [8 9 1.1]\r\n [10 11 1.1]]\r\nthis is the component result  : [[6 7 1.1]\r\n [8 9 1.1]\r\n [10 11 1.1]]\r\nthis is the component result  : [[7.05 16.21]\r\n [16.21 42.21]]\r\nthis is the component result  : [[7.05 16.21]\r\n [16.21 42.21]]\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n-> 1350                                       target_list, run_metadata)\r\n   1351 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1442                                             fetch_list, target_list,\r\n-> 1443                                             run_metadata)\r\n   1444 \r\n\r\nInvalidArgumentError: {{function_node __inference_innerloop_processing_13658}} {{function_node __inference_innerloop_processing_13658}} Expected size[0] in [0, 0], but got 3\r\n\t [[{{node while/body/_1/while/Slice}}]]\r\n\t [[while_33/StatefulPartitionedCall_1]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-18-238a2ce9a03a> in <module>\r\n     94 l2_splits = np.array([0,3,6,9,12,15])\r\n     95 l1_splits = np.array([0, 2, 5  ])\r\n---> 96 re       = sess.run([new_oo  ] , feed_dict={myTensor_values:vals ,myTensor_l1_splits:l1_splits ,myTensor_l2_splits:l2_splits  } )\r\n     97 print(re)\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    956     try:\r\n    957       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 958                          run_metadata_ptr)\r\n    959       if run_metadata:\r\n    960         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1179     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1180       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1181                              feed_dict_tensor, options, run_metadata)\r\n   1182     else:\r\n   1183       results = []\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1357     if handle is None:\r\n   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1359                            run_metadata)\r\n   1360     else:\r\n   1361       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\AutoEncoder\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nInvalidArgumentError:   Expected size[0] in [0, 0], but got 3\r\n\t [[{{node while/body/_1/while/Slice}}]]\r\n\t [[while_33/StatefulPartitionedCall_1]]\r\n\r\n```\r\n\r\n\r\n\r\nI would really  appreciate your help , Thanks in advance", "comments": ["I have an update , It seems like the issue is with the slice operation in the below line of code : \r\n`row = tf.slice(input1[0] , [inner_being_index] ,  [inner_end_index-inner_being_index])`\r\nApparently for some reason  and some runs TF is not evaluating the Tensor input1[0] ( which is a tensor of rank 1 )  correctly , where the tensor expected to look something like this `[7.05 16.21 16.21 ... 138.21 180.21 222.21]` but it is in some runs evaluated as `[0, 0]` , thus resulting in the error : `Expected size[0] in [0, 0], but got 3` . \r\n\r\nI have tried adding the following : \r\n```\r\n        with tf.control_dependencies([input1[0] ]):\r\n          row = tf.slice(input1[0] , [inner_being_index] ,  [inner_end_index-inner_being_index])\r\n\r\n```\r\nbut the above did not help. I hope someone can shed light on this. ", "hi @Saduf2019 , I do not want to nag but did you get a chance to look at this ?", "@jvishnuvardhan \r\nI ran the code that leads to session crashing.", "@malsulaimi I can reproduce the issue. [Here](https://colab.research.google.com/gist/jvishnuvardhan/355c20d0221f86809d03a102cd0bfe5c/untitled18.ipynb) is the gist for our reference. Thanks!", "thanks for your help @jvishnuvardhan  , any luck yet ? \r\n\r\nI have tried placing different control dependencies in different places in the function  but one way or another I end up with a similar error. \r\n\r\nAlso its worth mentioning that now I'm certain that the issue  is inside def innerloop_processing(begin_index , end_index , input1) . it also seems that if I use tf.print inside the function for some reason fix the issue , probably tf.print forces TF to resolves the dependencies correctly. in either ways this is not a solution but rather a guide to the right direction: below is the updated function after adding tf.print  and works fine all the time if tf.print is there : \r\n\r\n```\r\ndef innerloop_processing(begin_index , end_index , input1 ) : \r\n    innerloop_counter = begin_index\r\n    ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )\r\n    def innerloop_body(counter , begin_index , end_index , input1 , ta) : \r\n        tf.print(\"Slicing from index : \" , counter , \"to index : \" , counter+1 , \"index vector : \" , input1[1] , \"at counter : \" , counter)\r\n        tf.print(\"index value from   \" , input1[1][counter] , \"to value :\" , input1[1][counter+1] , input1[0] ,\"at counter : \" , counter )        \r\n        #with tf.control_dependencies([input1[0],input1[1] ,input1[2] , counter ]):\r\n        inner_being_index = input1[1][counter]\r\n        inner_end_index = input1[1][counter+1]\r\n        #tf.print(\"Slicing from index : \" , counter , \"to index : \" , counter+1 , \"index vector : \" , input1[1] , \"at counter : \" , counter)\r\n        #tf.print(\"index value from   \" , input1[1][counter] , \"to value :\" , input1[1][counter+1] , input1[0] ,\"at counter : \" , counter )\r\n        #with tf.control_dependencies([input1[0],input1[1] ,input1[2] , counter , inner_being_index ,inner_end_index]):\r\n        row = tf.slice(input1[0] , [inner_being_index] ,  [inner_end_index-inner_being_index] , name=\"row\")\r\n        ta = ta.write(counter-begin_index , row)\r\n        #with tf.control_dependencies([inner_being_index,inner_end_index,row,ta.concat() , input1[1] , input1[0],input1[2], counter]):\r\n        counter = counter + 1 \r\n        return counter , begin_index , end_index , input1 , ta\r\n    \r\n    \r\n    def innerloop_cond(counter , begin_index , end_index , input1 , ta ) : \r\n        return input1[1][counter] < input1[1][end_index] -1  #stop at the next pointer of the l2_splits \r\n    with tf.name_scope(\"innerbody\") : \r\n        results = tf.while_loop(innerloop_cond , innerloop_body , [innerloop_counter , begin_index , end_index , input1 , ta] )\r\n    #print_resutls = tf.print(\"this is the component result  :\" , results[4].stack())\r\n    return results[4].stack()\r\n```\r\n\r\n\r\n", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210526, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/4e0819ba715832fcab66dbaee5ddace1/42594.ipynb). Thanks!"]}, {"number": 42521, "title": "tf.argmin across longer axis significantly slower than tf.reduce_min", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI 2018.03\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0,2.1,2.2\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0, 10.1\r\n- GPU model and memory: V100 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen finding the argmin across axis=1 on a large, two dimensional tensor, the performance significantly degrades on GPU. We are seeing almost 10x slower on GPU rather than on CPU. Argmin is okay when taking the global argmin, however when specifying axis=1, is very slow. You can even get around it by using tf.map_fn then taking the global argmin, but that is obviously not optimal.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport time\r\nbegin = time.time()\r\nfor _ in range(1000):\r\n    a = tf.random.uniform(shape=(20, 150000), \r\n                                 minval=0, \r\n                                 maxval=1, \r\n                                 dtype=tf.float64)\r\n    c = tf.argmin(a, axis=1)\r\nend = time.time()\r\nend - begin\r\n# around 25 seconds\r\n```\r\n\r\nNow do not use axis=1\r\n\r\n```\r\nimport time\r\nbegin = time.time()\r\nfor _ in range(1000):\r\n    a = tf.random.uniform(shape=(20, 150000), \r\n                                 minval=0, \r\n                                 maxval=1, \r\n                                 dtype=tf.float64)\r\n    c = tf.argmin(a)\r\nend = time.time()\r\nend - begin\r\n# around 0.5 seconds\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ctargon \r\nI ran the code shared on tf-nightly please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f26fa4c336701d9e60c91e5bd0cd8b1e/untitled379.ipynb) and confirm if it replicates the issue reported.", "@Saduf2019 \r\nYes, just ran it. Please ensure you are using GPU run time. When doing so, taking the average of a few experiments, I get the code block where tf.argmin(a, axis=1) to run around 15 seconds and the code block where tf.argmin(a) to be around 0.2 seconds. so around a 75x **slow down**", "@ctargon \r\nI ran the shared code on tf 2.2 gpu and do not see much time been consumed, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/def6ebade137fc9e0950dbc3894dbbd1/untitled389.ipynb)", "I still think there is a performance issue with the argmin function.  When I run the code, you are right there does not seem to be as much a difference between axis=1 and the global argmin, both time in around 9 seconds. \r\n\r\nHowever, if you change the function call to reduce_min, then the operation runs in ~0.15 seconds. That is over 50x faster than the argmin function, which does not seem correct. How could argmin be that much slower, when all it needs is to track the indices for the min function? ", "Also note that i have been mistaken in saying \"global argmin\", rather it is the argmin for default axis=0. When the shape is (20,150000), the axis=0 means we are parallelizing over the longer axis. When axis=1, we are parallelizing over the shorter axis and thus each thread has to solve the argmin of a larger vector. This can explain why there is a difference between tf.argmin(u, axis=0) and tf.argmin(u, axis=1), but it does not explain why the reduce_min computation is over 50x faster, especially considering these are very similar ops.", "It looks like in order to get precise timings, we need to make sure that all the operations are actually fully evaluated. I suspect that TF2 is doing some kind of lazy/delayed evaluation, and the operations like tf.random.uniform or tf.argmin might not be actually performed immediately.\r\n\r\nI found that one way to force the evaluation is to print something that depends on tensor values. This is confirmed by tf.profiler: without that print statement (see below), there are fewer than 1000 GPU kernel calls to generate randoms, for example. I tried searching TF docs but couldn't find any mention of this behaviour there. Could someone confirm if my understanding about lazy evaluation is correct?\r\n\r\nHere is a new test case that makes sure that all operations have been fully executed before we stop the timer:\r\n```import time\r\nbegin = time.time()\r\nc = 1\r\nfor _ in range(1000):\r\n    a = tf.random.uniform(shape=(20, 150000), \r\n                                 minval=0, \r\n                                 maxval=1, \r\n                                 dtype=tf.float64)\r\n    # Uncomment one of the 4 lines below\r\n    #c = c + tf.math.reduce_min(a, axis=0)  # 0.26 seconds total time if this line is used\r\n    #c = c + tf.math.reduce_min(a, axis=1)  # 0.31 seconds\r\n    #c = c + tf.argmin(a, axis=0)  # 0.26 seconds\r\n    c = c + tf.argmin(a, axis=1)  # 17.6 seconds!!!\r\nprint(tf.math.reduce_sum(c))  # Force evaluation\r\nend = time.time()\r\nprint(end - begin)\r\n```\r\n\r\nI can see that tf.math.reduce_min is fast with either axis=0 or axis=1, but tf.argmin is very slow with axis=1.", "Hi @ctargon, can you clarify the performance issue here? As you point out the time difference in time between axis=0 and axis=1 is probably due to the difference in size of the two dimensions. And I'm not seeing much of a difference. Is the issue here about a difference in performance between argmin on CPU and GPU? Or argmin and reduce_min on GPU? Or something else?", "@nikitamaia If you run the example @zhezherun provides in Colab, you see that reduce_min, regardless of axis, runs in around 0.2 seconds. Similarly, you see argmin with axis=0 have the same performance. However, when argmin has axis = 1, the run time increases by almost 50x. I am not sure how you are not seeing much of a difference, as the comments in @zhezherun post regarding time appear to hold true on Colab for me as well.\r\n\r\nHow is it that two (seemingly) reduction algorithms, that should in theory have very similar performance, differ so widely? I would expect argmin across axis = 1 to have a very similar runtime compared to reduce_min across axis = 1. If argmin is implemented using reduction, then I would expect its performance to be much better.", "Thanks for clarifying. I've changed the title of the issue to reflect the comparison between argmin and reduce_min. I do see the performance difference between reduce_min and argmin. \r\n\r\nI don't know that argmin is necessarily implemented with reduction in the same way that reduce_min, reduce_all, reduce_any, etc are. I found a [similar post on stack overflow](https://stackoverflow.com/questions/17840661/is-there-a-way-to-make-numpy-argmin-as-fast-as-min) for the numpy equivalents that might be of interest.\r\nIs this difference a performance blocker for you? Have you tried utilizing @tf.function?", "It actually is a bit of a performance blocker right now, which is how we found it. We did try @tf.function, but the results are the same. We also tried different functional calls of argmin, such as tf.keras.backend.argmin, etc. but it appears they all point to the same backend function.", "any update?"]}, {"number": 42513, "title": "Discrepancy between available operation semantics documentation and documentation of `tf2xla.python.xla.conv`", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/xla/operation_semantics#conv_convolution\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5f2e159a58d1ef3414b2c34339266449574d8f94/tensorflow/compiler/tf2xla/python/xla.py#L239:L269\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\n [tf2xla.python.xla.conv](https://github.com/tensorflow/tensorflow/blob/5f2e159a58d1ef3414b2c34339266449574d8f94/tensorflow/compiler/tf2xla/python/xla.py#L239:L269) points to the operation semantics for `ConvWithGeneralPadding` but actually wraps the more general `ConvGeneralDilated`. It would make sense to actually have documentation about the operation semantics of this more general operation.\r\n", "comments": ["Can you please help me locate `tf2xla.python.xla.conv` on the webpage? Thanks!", "Hi,\r\n\r\nIt is not on the webpage, but [here](https://github.com/tensorflow/tensorflow/blob/5f2e159a58d1ef3414b2c34339266449574d8f94/tensorflow/compiler/tf2xla/python/xla.py#L239:L269). The [documentation these comments point to](https://www.tensorflow.org/xla/operation_semantics#conv_convolution) (which in turn points to [ConvWithGeneralPadding](https://www.tensorflow.org/xla/operation_semantics#convwithgeneralpadding_convolution)), but they refer to `ConvGeneralDilated` [which is defined here](https://github.com/tensorflow/tensorflow/blob/589587081e4ad3d61cbd8b5f700022922173b8fc/tensorflow/compiler/xla/client/xla_builder.h#L1032:L1039) and is more generic.\r\n\r\nI think that either the docstring should be modified, or the operation semantics should document the more general `ConvGeneralDilated` function.\r\n\r\nHope this is clearer!", "Hi @bchetioui,\r\n\r\nLooks like the [docstrings for conv](https://github.com/tensorflow/tensorflow/blob/5f2e159a58d1ef3414b2c34339266449574d8f94/tensorflow/compiler/tf2xla/python/xla.py#L239:L269) in `xla.py` are incorrect. The method that is wrapped seems to be the [`XLA Conv`](https://github.com/tensorflow/tensorflow/blob/1dd24b74c291218f097f0e2a086a0fbd847bbea2/tensorflow/compiler/xla/client/xla_builder.h#L529) operator, not the `XLA ConvGeneralDilated` operator.", "Hi @Harsh188,\r\n\r\nIt doesn't seem like this is the case to me; [Xla::Conv](https://github.com/tensorflow/tensorflow/blob/5f2e159a58d1ef3414b2c34339266449574d8f94/tensorflow/compiler/tf2xla/python/xla.py#L239:L269) has a different prototype than the [function](https://github.com/tensorflow/tensorflow/blob/5f2e159a58d1ef3414b2c34339266449574d8f94/tensorflow/compiler/tf2xla/python/xla.py#L273:L283) that is called in `xla.py`. That latter function is a specific call to [ConvGeneralDilated](https://github.com/tensorflow/tensorflow/blob/5f2e159a58d1ef3414b2c34339266449574d8f94/tensorflow/compiler/tf2xla/kernels/xla_conv_op.cc#L77:L81).", "@bchetioui you seem to be right. It also looks like it's just named [`XLAConv`](https://github.com/tensorflow/tensorflow/blob/5f2e159a58d1ef3414b2c34339266449574d8f94/tensorflow/compiler/tf2xla/kernels/xla_conv_op.cc#L91), but it seems to use `xla::ConvGeneralDilated`.\r\n"]}, {"number": 42491, "title": "TFLite - enable XNNPACK with dynamic shapes of tensors", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn 2.3.0, dynamic shapes  were fixed in TFLite (\"TFLite now properly supports dynamic shapes during conversion and inference.\"). We can now process tensors with varying batch size, everything works, great. \r\n\r\nHowever, running on Windows 10 64b, the processing is slower than base TF2. Using TFLite with XNNPACK and fixed batch size 1, the processing is about 2x faster than with base TF2. So the final difference is about 3x in latency. My request is to enable XNNPACK Delegate to process dynamic shaped tensors to enjoy fast processing with variable batch size.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone wishing to process batches of images of various size.\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 42487, "title": "Application crash when using SetNumThreads from tflite::impl::Interpreter", "body": "**The issue**\r\n\r\nWe handle worker threads with different TFLite sessions in our mobile application and we set mInterpreter->SetNumThreads(0).\r\nIn this case, the call of mInterpreter->Invoke() causes an application crash (see the log below) regularly on a 32-bit application version (armeabi-v7a), but works as expected on a 64-bit version.\r\n\r\n**System information**\r\n- OS Android 8, 9, 10:\r\n- Mobile device (Samsung Galaxy Notes 9, 10)\r\n- TensorFlow installed from TF2.0 branch:\r\n- TensorFlow version (use command below):\r\n- Python version: 3.7\r\n- Android NDK 19\r\n\r\n**Describe the current behavior**\r\n32-bit application crash\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n// some init and allocates\r\nmModel = tflite::FlatBufferModel::BuildFromBuffer((const char*)modelInfo.modelData, modelInfo.modelSize);\r\ntflite::InterpreterBuilder(*mModel, mResolver)(&mInterpreter);\r\nmInterpreter->UseNNAPI(false);\r\nmInterpreter->ResizeInputTensor(0, sizes);\r\nmInterpreter->AllocateTensors();\r\nint input = mInterpreter->inputs()[0];\r\nfloat* inputArr = mInterpreter->typed_tensor<float>(input);\r\n\r\n// fill in inputArr \r\ninputArr[array_iter++] = ...\r\n\r\n// crash here\r\nif(mInterpreter == NULL || mInterpreter->Invoke() != kTfLiteOk) {\r\n...\r\n}\r\n\r\n**Other info / logs** \r\n\r\ncase 1:\r\n#00 pc 00056e28  /system/lib/libc.so (tgkill+12)\r\n #02 pc 0010e411  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (_ZN14EigenForTFLite15TensorEvaluatorIKNS_19TensorContractionOp\r\n #03 pc 0010e1a9  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (_ZNK14EigenForTFLite15TensorEvaluatorIKNS_19TensorContractionO\r\n #04 pc 0010e047  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (_ZN14EigenForTFLite30TensorContractionEvaluatorBaseINS_15Tenso\r\n #05 pc 0010d78f  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (_ZN14EigenForTFLite8internal14TensorExecutorIKNS_14TensorAssig\r\n #06 pc 001076d9  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (_ZN14EigenForTFLite12TensorDeviceINS_9TensorMapINS_6TensorIfLi\r\n #07 pc 001075a1  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (tflite::multithreaded_ops::EigenTensorConvFunctor<float>::oper\r\n #08 pc 00106669  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (tflite::multithreaded_ops::Conv(EigenForTFLite::ThreadPoolDevi\r\n #09 pc 00121a5b  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (_ZN6tflite3ops7builtin4conv9EvalFloatILNS2_10KernelTypeE2EEEvP\r\n #10 pc 0010527f  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (_ZN6tflite3ops7builtin4conv4EvalILNS2_10KernelTypeE2EEE12TfLit\r\n #11 pc 000e4e25  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (tflite::Subgraph::Invoke()+248)\r\n #12 pc 000dcc7f  /data/app/com.android.app.notes-wVTuDe5vEFmIodYgMItUjQ==/lib/arm/libTFLite.so (tflite::Interpreter::Invoke()+14)\r\n\r\n", "comments": ["Additionally, the same problem occurs when we set mInterpreter->SetNumThreads(-1). It seems the -1 value should let the TFLite interpreter set the no of threads."]}, {"number": 42454, "title": "Keras 'Random' preprocessing layers do not work on cloud TPU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nSimple demo available in colab [here](https://colab.research.google.com/drive/1lZBE7bDvRxRoEijYkx8Zch3JU9xug6V3?usp=sharing).\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nGoogle Colab with TPU\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n\r\n- TensorFlow installed from (source or binary):\r\nGoogle Colab\r\n\r\n- TensorFlow version (use command below):\r\n2.3.0\r\n\r\n- Python version:\r\n3.6.9\r\n\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nCloud TPU\r\n\r\n** Current Behaviour **\r\n'Random' layers within tf.keras.layers.preprocessing.experimental (e.g. RandomRotation, RandomTranslation) fail with `Detected unsupported operations when trying to compile graph` ... `No registered 'ImageProjectiveTransformV2' OpKernel for XLA_TPU_JIT devices compatible with node` when using a cloud TPU, either in colab or from a VM\r\n\r\n** Expected Behaviour **\r\nNew layers should use ops that are implemented on TPUs, or at least carry a health warning that they are not (yet) available on TPU.\r\n\r\n**Standalone code to reproduce the issue**\r\nSee [this colab](https://colab.research.google.com/drive/1lZBE7bDvRxRoEijYkx8Zch3JU9xug6V3?usp=sharing). Or run the following code with a TPU:\r\n```\r\nimport tensorflow as tf\r\n\r\n# establish connection to TPU\r\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_cluster(tpu)\r\ntf.tpu.experimental.initialize_tpu_system(tpu)\r\nstrategy = tf.distribute.TPUStrategy(tpu)\r\nprint('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \r\n\r\nwith strategy.scope():\r\n  # define model layers\r\n  model_input = tf.keras.layers.Input(shape=(224, 224, 1))\r\n  x = tf.keras.layers.experimental.preprocessing.RandomRotation((-0.1, 0.1))(model_input)\r\n  x = tf.keras.layers.MaxPool2D((224, 224))(x)\r\n  x = tf.keras.layers.Flatten()(x)\r\n  x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n\r\n  # compile model\r\n  model = tf.keras.Model(inputs=model_input, outputs=x)\r\n  model.summary(line_length=120)\r\n  model.compile(optimizer='adam', loss='binary_crossentropy')\r\n\r\n  # generate some random data and fit the model\r\n  images = tf.random.uniform((10, 224, 224, 1))\r\n  labels = tf.zeros((10, 1))\r\n  model.fit(images, labels)\r\n\r\n  # predict on the data\r\n  print(model.predict(images))\r\n```\r\n\r\n**Other info / logs**\r\nI appreciate that these are 'experimental' features, so I am not sure if this constitutes a bug or a feature request. It would be helpful to indicate if TPU support for these new keras layers is in the pipeline - I have been looking for an image augmentation solution in tf2.0+ for some time, and was hopeful these new layers would be it. Without TPU support the feature has limited use at this time. It would also have been helpful to put a health warning in the docs that TPU support is not yet available, as I would not have wasted my time on this. Someone has recently [asked a question about this](https://stackoverflow.com/questions/63302446/colabtpu-not-supporting-tf-2-3-0-tf-keras-layers-experimental-preprocessing) on StackOverflow, but in the absence of an official reply there I chose to raise it here.\r\n\r\nNote that I tried using a VM with tf-nightly (2.4.0-dev20200817) installed, but I get the same error message. Since I can only create TPUs at version 2.3, this is not altogether surprising.\r\n\r\nThanks", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7bad14f22c5624458d66b3e6caf0bba8/untitled367.ipynb)\r\nThanks!", "In the meantime, as a workaround I created a standalone 'layer' and incorporated it into my input pipeline instead of in the model, e.g.:\r\n```\r\naugmentor = tf.keras.layers.experimental.preprocessing.RandomRotation((-0.1, 0.1))\r\nds = ds.map(lambda x, y: (augmentor.call(x), y))\r\n```\r\nThis is probably abuse of a keras layer, but it seems to work on a TPU.", "Hi @Lipod, glad you were able to find a workaround for now. I've changed the label from bug to a docs-feature. Sounds like you're proposing that the docs for keras layers include a warning if they don't work on TPUs. Additionally you're asking for some insight into what keras layers are planned to be supported in the near future. Is that correct?", "Hi @nikitamaia ,\r\n\r\nI disagree with your decision. Primarily, I would characterise this as a bug. As per my instructions to reproduce, I tried to use a keras feature on a cloud TPU and it failed. Since keras is primary high-level API, and TPUs are one of the key features of tensorflow, I think it is reasonable for me, as an end user, to expect new keras features to work on a TPU. I am asking for someone to fix the bug so that I can use these layers on a TPU.\r\n\r\nMy reference to updating the documentation was because I was anticipating that you might say that these layers are not supported on a TPU (I probably overthought this). To an end user like me reporting his first bug, you might already know it doesn't work on a TPU and either be planning to introduce it later or never support it. If either of those are true then you could have put that in the documentation and saved users like me the bother of trying to use an unsupported feature. As it is, from the comments of your colleagues above, it looks like this is a straight up bug or oversight. I guess your test coverage for keras does not include running on a TPU.\r\n\r\nRegarding my workaround, it is clear that these new layers are intended to be compiled into a keras model and not used in the input pipeline (hence my comment about abusing a keras layer), so it is not ideal.  If you can't make these layers work in a compiled model on a TPU then you might consider making equivalent functions available in the  `tf.image` namespace or similar so that pipeline use is officially supported.", "Ah okay, thanks for the clarification. I've changed the label back to bug. tf keras preprocessing layers were recently added to the TPU tests and currently RandomRotation and RandomTranslation fail. So this is a known issue, but a bug nonetheless.\r\n\r\nAnother workaround for the time being is to set `tf.config.set_soft_device_placement(True)` at the beginning of the program. It will run the RandomRotation op on CPU. Obviously not ideal, but it will allow your program to run.", "Hi @Lipod, providing an update after lots of discussions between the keras and TPU teams. While this originally seemed like a bug, TPU implementation for `ImageProjectiveTransform` (and `V2`) has been determined to be much less efficient than just using CPU. Recommendations here are to either move the preprocessing to your tf.data transformations if you want to use TPUs, or to just use CPU or GPU if you want to use these tf.keras image preprocessing layers. Main takeaway is that supporting `ImageProjectiveTransform` at the TPU level is discouraged due to the implementation being much less efficient than on CPU.", "Hi @nikitamaia, thanks for the update. In that case, as an end user, it would help if you could:\r\n\r\n1. Expose these augmentations in an API appropriate for use in the `tf.data` pipeline (such as `tf.image`). This would effectively make my workaround 'official', and would separate the ops from all of the scaffold that comes with a keras layer.\r\n\r\n2. Update the documentation in the `tf.keras.layers.experimental.preprocessing` namespace to warn users that they do not support a TPU, and suggest workarounds.\r\n\r\nThanks", "Agreed, we do need a way to make this more apparent to users. Will keep this issue open until that information is available somewhere other than just this thread.", "Was able to reproduce the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/cafccc4b64d87a241f318502ca1c5e26/untitled58.ipynb#scrollTo=weWesOVfS_R8)..Thanks !"]}, {"number": 42446, "title": "Should the custom loss function in Keras return a single loss value for the batch or an arrary of losses for every sample in the training batch?", "body": "I asked a question on [StackOverflow](https://stackoverflow.com/questions/63390725/should-the-custom-loss-function-in-keras-return-a-single-loss-value-for-the-batc) regarding as the return value of a custom loss funtion. But I didn't get a clear answer.\r\n\r\nIn this [guide](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses) on tensorflow website, I found an example of custom loss funciton:\r\n\r\n        def custom_mean_squared_error(y_true, y_pred):\r\n            return tf.math.reduce_mean(tf.square(y_true - y_pred))\r\n\r\nThe `reduce_mean` function in this custom loss function will return an scalar. But I think the custom loss function should return an array of losses for every example in a training batch, rather than a single loss value. \r\n\r\nAccording to the source code of [Model](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/training.py#L159-L2634) class, the custom loss function is used to constructed a `LossFunctionWrapper` object. I read the source code of the [loss](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/losses.py) module. I think it's `LossFunctionWrapper.__call()__` method that is responsible for getting the mean loss value for the training batch. `LossFunctionWrapper.__call()__` method first calls the `LossFunctionWrapper.call()` method to get an array of losses for every example in the training batch. It's in the  `LossFunctionWrapper.call()` method that our custom loss function is called.\r\n\r\nIn addition, in the souece code of [losses](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/losses.py) module, the `MeanAbsoluteError` class uses the `mean_squared_error` function to construct a `LossFunctionWrapper` class. We can see that the `mean_squared_error` function returns `K.mean(math_ops.squared_difference(y_pred, y_true), axis=-1)`, which is an array, not a single value. I think our custom loss function shoud just be like this.\r\n\r\nSo, why does the custom loss function in the [guide](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses) on tensorflow website return a scalar? Is it wrong to define a custom function like this?", "comments": ["@lambdaphy custom loss function in keras returns a single value as in the above example we are using `tf.math.reduce_mean`. Simple example of reduce_mean is as follows.\r\n\r\n```\r\nimport tensorflow as tf\r\na = tf.ones([1,100],tf.int32)\r\nreduce_m = tf.math.reduce_mean(a) \r\nprint(reduce_m)       # output tf.Tensor(1, shape=(), dtype=int32)\r\n\r\n```\r\n`reduce_mean` of the above array `a` is 1 as shown in the output. \r\n\r\nFor Keras models, we need to define those custom_loss functions to provide a scalar loss for each batch. For custom_training, you can define loss functions as you want to implement in your custom model. Thanks!\r\n\r\n\r\n", "I know that when training a model we need a scalar loss for each batch. But according to the source code, the custom loss function is not responsible for getting this scalar loss. Could you check the source codes of the `Model.compile()`, `Model.fit()` , `Mode.train_step()`,([source code](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/training.py#L159-L2634)) `Loss.__call()__` , `Loss.call()` and  `LossFunctionWrapper.call()` methods ([source code](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/losses.py)) to look the calculating process of loss value of a training batch? \r\n\r\nI read those codes, and I find the `Loss.__call__()` method calls the `Loss.call()` method (which is implemented in subclass, e.g. `LossFunctionWrapper`) to get an array of losses for every example in the training batch, then `Loss.__call__()` method calls `compute_weighted_loss()` function to get the (weighted) average of those losses.  How does the `LossFunctionWrapper.call()` method get an array of losses? Reading the source code, we can see that it uses `self.fn` to get those losses, and the `self.fn` is the loss funciton we provided to `Model.compile()` method.\r\n\r\nHere is the source code of `Loss.__call__()`:\r\n\r\n        def __call__(self, y_true, y_pred, sample_weight=None):\r\n           \"\"\"\r\n           .................(omitted)\r\n           Returns:\r\n                Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\r\n                shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1`\r\n                because all loss functions reduce by 1 dimension, usually axis=-1.)\r\n           .................(omitted)\r\n           \"\"\"\r\n            graph_ctx = tf_utils.graph_context_for_symbolic_tensors(\r\n                    y_true, y_pred, sample_weight)\r\n            with K.name_scope(self._name_scope), graph_ctx:\r\n            ag_call = autograph.tf_convert(self.call, ag_ctx.control_status_ctx())\r\n            losses = ag_call(y_true, y_pred)\r\n            return losses_utils.compute_weighted_loss(\r\n                          losses, sample_weight, reduction=self._get_reduction())\r\n\r\nHere is the source code of `Loss.call()`:\r\n\r\n        def call(self, y_true, y_pred):\r\n        \"\"\"Invokes the `Loss` instance.\r\n            Args:\r\n                y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\r\n                sparse loss functions such as sparse categorical crossentropy where\r\n                shape = `[batch_size, d0, .. dN-1]`\r\n                y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\r\n           Returns:\r\n               Loss values with the shape `[batch_size, d0, .. dN-1]`.\r\n        \"\"\"\r\n            NotImplementedError('Must be implemented in subclasses.')\r\n\r\nThe `Loss.call()` method is just an interface that a subclass of `Loss` must implement. But we can see that the return value of this method is **Loss values with the shape `[batch_size, d0, .. dN-1]`**.\r\n\r\nNow let's see `LossFunctionWrapper` class. `LossFunctionWrapper` is a subclass of `Loss`. In its constructor, we should provide a loss function, which is stored in `LossFunctionWrapper.fn`. Here is the source code of `LossFunctionWrapper.call()`, which implements `Loss.call()` method:\r\n\r\n        def call(self, y_true, y_pred):\r\n        \"\"\"Invokes the `LossFunctionWrapper` instance.\r\n            Args:\r\n                 y_true: Ground truth values.\r\n                 y_pred: The predicted values.\r\n            Returns:\r\n                Loss values per sample.\r\n        \"\"\"\r\n             if tensor_util.is_tensor(y_pred) and tensor_util.is_tensor(y_true):\r\n                 y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(y_pred, y_true)\r\n             ag_fn = autograph.tf_convert(self.fn, ag_ctx.control_status_ctx())\r\n             return ag_fn(y_true, y_pred, **self._fn_kwargs)\r\n\r\nSee? The loss function we provide is called here, and its task is to return **Loss values per sample**.\r\n\r\nIn addition, as an example, we can see how the `MeanSquaredError` class is defined. It is just a `LossFunctionWrapper` class using the `mean_squared_error` function as the loss function(i.e. `LossFunctionWrapper.fn=mean_squared_error`). The source code of `mean_squared_error` function is also defined in the `losses` module:\r\n\r\n    def mean_squared_error(y_true, y_pred):\r\n        \"\"\"Computes the mean squared error between labels and predictions.\r\n            After computing the squared distance between the inputs, the mean value over\r\n            the last dimension is returned.\r\n           `loss = mean(square(y_true - y_pred), axis=-1)`\r\n            Standalone usage:\r\n            >>> y_true = np.random.randint(0, 2, size=(2, 3))\r\n            >>> y_pred = np.random.random(size=(2, 3))\r\n            >>> loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\r\n            >>> assert loss.shape == (2,)\r\n            >>> assert np.array_equal(\r\n                     ...     loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))\r\n           Args:\r\n              y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\r\n              y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\r\n           Returns:\r\n             Mean squared error values. shape = `[batch_size, d0, .. dN-1]`.\r\n        \"\"\"\r\n        y_pred = ops.convert_to_tensor_v2(y_pred)\r\n        y_true = math_ops.cast(y_true, y_pred.dtype)\r\n        return K.mean(math_ops.squared_difference(y_pred, y_true), axis=-1)\r\n\r\nWe can see that it returns an array, not a scalar (`axis=-1` in the `K.mean()`), and the first dimension of its return value is `batch_size`. \r\n\r\nAccording the the source code of `Model.compile()` and `Model.fit()`, when we provide a custom loss function, this funtion is used to construct a `LossFunctionWrapper` object, just like the `MeanSquaredError` object uses `mean_squared_error` fucntion to construct a `LossFunctionWrapper` object. That's why I think that the custom loss function shoud return an array of losses, because obtaining a scalar loss value for the training batch is not the task of a loss funtion, the `Loss.__call()__` shoud do that job.", "Based on the above analysis, we can see that when we define our custom loss **CLASS**, we should implement the `call()` method, and this `call()` method SHOULD return an array, not a scalar. But in the [guide](https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses) on tensorflow, we can also see an example of custom loss class, just below the example of custom loss function:\r\n\r\n    class CustomMSE(keras.losses.Loss):\r\n        def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\r\n            super().__init__(name=name)\r\n            self.regularization_factor = regularization_factor\r\n\r\n        def call(self, y_true, y_pred):\r\n            mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\r\n            reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\r\n            return mse + reg * self.regularization_factor\r\n\r\n    model = get_uncompiled_model()\r\n    model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\r\n\r\n    y_train_one_hot = tf.one_hot(y_train, depth=10)\r\n    model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)\r\n\r\nWe can see that this implementation of `call()` method returns a scallar. I think this behavior is also wrong.", "@lambdaphy Thank you for the issue. Custom loss function is required to return one loss value per sample. The example will need to be updated to reflect this.", "If you are interested in making the change, please feel free to send me a PR.", "Thank you for your reply. I'm very happy to contribute, but I am not familiar with using github to open pull request. In addition I'm not a native English speaker, so maybe I'm not competent to edit the documentation.\r\n\r\nIf you can edit the documentation, just help change it. Thanks.", "Seems that we are still waiting the documentation to be changed.", "Most of guidances follow the tf doc which return single value for a batch of training data. I follow the source code to the `compute_weighted_loss` where `LossFunctionWrapper` do the mean across batch axis, and found that if custom loss func return a value instead of a array of value, we got two problems:\r\n\r\n1. `sample_weights` argument will be meaningless since wo do mean first.\r\n2. Loss are averaged based on a uncorrect denominator when do training with `tf.distribute.Strategy`.\r\n@lambdaphy what do you think?", "> Most of guidances follow the tf doc which return single value for a batch of training data. I follow the source code to the `compute_weighted_loss` where `LossFunctionWrapper` do the mean across batch axis, and found that if custom loss func return a value instead of a array of value, we got two problems:\r\n> \r\n> 1. `sample_weights` argument will be meaningless since wo do mean first.\r\n> 2. Loss are averaged based on a uncorrect denominator when do training with `tf.distribute.Strategy`.\r\n>    @lambdaphy what do you think?\r\n\r\nYes, you're right.  Loss function returning a single loss value for the whole batch will cause problems under some circumstances.\r\n\r\nThe documentation has yet to be corrected.\r\n\r\nThere are many ambiguities in the document."]}, {"number": 42425, "title": "Convert savedmodel to h5 Keras format - AttributeError: 'AutoTrackable' object has no attribute 'summary'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.1\r\n\r\n**Describe the current behavior**\r\nI used Google AI-Platform to train a model for object detection. The result is a model in the savedmodel format. I wan't to convert it to h5 Keras format so I can import into Matlab. \r\n\r\n**Standalone code to reproduce the issue**\r\nHere is the output folder directly from the bucket:\r\n\r\nhttps://drive.google.com/file/d/1mgVGG_WNGC9gxZSgn1G2_YdQTVkJJInM/view?usp=sharing\r\n\r\n\r\n**Other info / logs** \r\n\r\nI tried to do this:\r\n\r\n`New_Model = load_model('model')`, or this `New_Model = tf.keras.models.load_model('model')` or this `New_Model = tf.saved_model.load(export_dir='model')` and lastly this `New_Model.summary()` and I always get the same error: \r\n\r\n`AttributeError: 'AutoTrackable' object has no attribute 'summary'`\r\n\r\nIf the model would had loaded correctly I would try to do `tf.keras.models.save_model(New_Model, 'New_Model.h5')`\r\n\r\nWhich will end up throwing this error:\r\n\r\n`AttributeError: 'AutoTrackable' object has no attribute '_is_graph_network'`\r\n\r\nPlease help, as I said the model is saved in the savedmodel format, so I guess every piece needed is there\r\n\r\nComplete log for summary:\r\n\r\n```\r\n2020-08-17 01:48:46.011995: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-08-17 01:48:46.016769: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-08-17 01:48:53.754437: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n2020-08-17 01:48:53.758821: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-08-17 01:48:53.780897: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: EC2347417W3\r\n2020-08-17 01:48:53.785702: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: EC2347417W3\r\n2020-08-17 01:48:53.789520: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-17 01:48:53.820077: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16799c381d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-17 01:48:53.832027: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/moving_variance:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/moving_variance:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/moving_variance:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet50/batch_normalization/moving_variance:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nTraceback (most recent call last):\r\n  File \"savedmodel_to_h5.py\", line 15, in <module>\r\n    New_Model.summary()\r\nAttributeError: 'AutoTrackable' object has no attribute 'summary'\r\n\r\n```", "comments": ["@dfvr1994,\r\nI am not able to access the file as I do not have the required permissions. \r\n\r\nAlso, could you please provide the complete code you are using to convert the model. Thanks! ", "@dfvr1994,\r\nPlease take a look at [this](https://stackoverflow.com/a/58757328) similar StackOverflow issue and let us know if it helps. Thanks!", "@amahendrakar I have updated permissions on the file so it can be accessed by anyone with the link. \r\n\r\nI'm following this steps:\r\nhttps://stackoverflow.com/questions/59375679/tensorflow-pb-format-to-keras-h5\r\n\r\nThe second link you provided shows how to use the .pb model outside docker, I assume you pointed it because of what I mentioned about using the model on Matlab, which at the moment doesn't support .pb format but ONNX and h5. \r\n\r\nI need to transform the model into h5 format. ", "Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/76d535b330e9047270342f58aa20fb1c/42425.ipynb#scrollTo=Jy5_OTkVVjrf). Thanks!", "Same issue could be reproduce with TF v2.2.0.\r\n\r\n", "Hi, any update on this?", "I'm afraid not. @dfvr1994 ", "I am having the same error", "Did anyone manage to find a solution? Having the same issue here", "I am running into this issue too!", "Hi @jvishnuvardhan any update on this?", "any luck on converting to h5 format", "@dfvr1994 I have a question. Did you save the model using `tf.saved_model.save` or `model.save`? Thanks! If possible, please share model building code also. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "It's not the time to close issue. \r\n\r\nThere are many users faced the same problem crossing various versions.", "@DjangoPeng I removed `stalled` label, so it will not close. Thanks!", "@jvishnuvardhan I have been working on this issue for the past 2 months. I am working on Object detection models and trying to prune and compress it. This has been a major issue that I am facing in my work. I am happy to work on this problem with a little guidance. If you can help me out with understand a few concepts I will be able to pull this of. ", "Same issue here", "`tf.keras.models.load_model` only works with SavedModels saved from Keras. I'm guessing that the SavedModel is saved from another source that is not Keras.\r\n\r\nIf you want to convert a SavedModel to Keras, you could try something like:\r\n\r\n```\r\nloaded = tf.saved_model.load(path)\r\n\r\nclass LayerFromSavedModel(tf.keras.layers.Layer):\r\n  def __init__(self):\r\n    super(LayerFromSavedModel, self).__init__()\r\n    self.vars = loaded.variables\r\n  def call(self, inputs):\r\n    return loaded.signatures['serving_default'](inputs)\r\n\r\ninput = tf.keras.Input(...)\r\nmodel = tf.keras.Model(input, LayerFromSavedModel()(input))\r\nmodel.save('saved_model')\r\n```", "@dfvr1994 Would you like to try out @k-w-w 's solution? ", "@DjangoPeng sure, will post results shortly", "I'm getting this error: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ZX776FN\\Documents\\out\\savedmodel_to_h5.py\", line 18, in <module>\r\n    input = tf.keras.Input(...)\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\", line 311, in Input\r\n    input_layer = InputLayer(**input_layer_config)\r\n  File \"C:\\Program Files\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\", line 150, in __init__\r\n    batch_input_shape = (batch_size,) + tuple(input_shape)\r\nTypeError: 'ellipsis' object is not iterable\r\n\r\n```\r\n@k-w-w did you try your code with the model I uploaded at the top of the issue? ", "@dfvr1994 What was the input for `tf.keras.Input` in your code? Can you share a standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan I did run this: \r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.preprocessing import image\r\n\r\nloaded = tf.saved_model.load(r\"C:\\Users\\my_user\\Documents\\out\\2\\OUTPUT\\model\")\r\n\r\nclass LayerFromSavedModel(tf.keras.layers.Layer):\r\n  def __init__(self):\r\n    super(LayerFromSavedModel, self).__init__()\r\n    self.vars = loaded.variables\r\n  def call(self, inputs):\r\n    return loaded.signatures['serving_default'](inputs)\r\n\r\ninput = tf.keras.Input(...)\r\nmodel = tf.keras.Model(input, LayerFromSavedModel()(input))\r\nmodel.save('saved_model')\r\n\r\n```\r\n\r\nWhere \"C:\\Users\\my_user\\Documents\\out\\2\\OUTPUT\\model\" contains: \r\n\r\n![image](https://user-images.githubusercontent.com/45764991/99588470-9a0a0000-29b8-11eb-9ba8-851b30866564.png)\r\n\r\nOUTPUT contains: \r\n\r\n![image](https://user-images.githubusercontent.com/45764991/99588508-adb56680-29b8-11eb-84e6-cf4e83601ee5.png)\r\n", "Hi, any update on this?", "SavedModel does not have compressed format? this is very incombinient...\r\n\r\n", "Hey, any updates on this issue ?  I've been dealing with the same problem.", "Same problem, unable to convert pb to h5(keras), pb file was created from an onnx file.", "Was able to reproduce the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/dbe545f79728d97edfe0475bd194c170/42425.ipynb#scrollTo=LfnZxnIJU1Qi)..Thanks ! ", "Converting a Tensorflow model to Keras is no small feat. Some details can be found [here](https://stackoverflow.com/questions/44466066/how-can-i-convert-a-trained-tensorflow-model-to-keras).", "> `tf.keras.models.load_model`Keras\u304b\u3089\u4fdd\u5b58\u3055\u308c\u305fSavedModels\u3067\u306e\u307f\u6a5f\u80fd\u3057\u307e\u3059\u3002SavedModel\u306fKeras\u3067\u306f\u306a\u3044\u5225\u306e\u30bd\u30fc\u30b9\u304b\u3089\u4fdd\u5b58\u3055\u308c\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u3002\r\n> \r\n> SavedModel\u3092Keras\u306b\u5909\u63db\u3059\u308b\u5834\u5408\u306f\u3001\u6b21\u306e\u3088\u3046\u306a\u65b9\u6cd5\u3092\u8a66\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\r\n> \r\n> ```\r\n> loaded = tf.saved_model.load(path)\r\n> \r\n> class LayerFromSavedModel(tf.keras.layers.Layer):\r\n>   def __init__(self):\r\n>     super(LayerFromSavedModel, self).__init__()\r\n>     self.vars = loaded.variables\r\n>   def call(self, inputs):\r\n>     return loaded.signatures['serving_default'](inputs)\r\n> \r\n> input = tf.keras.Input(...)\r\n> model = tf.keras.Model(input, LayerFromSavedModel()(input))\r\n> model.save('saved_model')\r\n> ```\r\n\r\nIn my case, this sample was worked well.", "@k-w-w I converted the my savedmodel format by making \r\n```\r\ninput = tf.keras.Input(shape=(160,160,3), dtype=np.float32) \r\ninput = tf.keras.Input(\r\n    shape=(160,160,3), dtype=np.float32)\r\nmodel = tf.keras.Model(input, LayerFromSavedModel()(input))\r\nmodel.save('saved_model.h5')\r\n```\r\nin your proposed code. However, loading the saved_model.h5 using \r\n`model_h5 = tf.keras.models.load_model(path2h5model, compile=False) `\r\ngenerates the following error.\r\n\r\n> ValueError: Unknown layer: LayerFromSavedModel\r\n", "Same error, any solution!\r\n", "So the problem with this is due to the fact that the saved_model file is in bitstream format.  I have tried to decode it and failed multiple times. There is no known way to get layer information out of the bitstream. So if you want to recreate the model you have to use keras layers to recreate the model and then use the weights from saved_model to get the performance of the saved_model. Most common network architectures can be found in github. If you are trying to work on any specific model please let me know i can see if I can help you out. @aynesss", "@abhishekbalu  Thank you very much if you can help me!\r\nHere is the saved_model that I use : \"gs://simclr-checkpoints-tf2/simclrv1/pretrain/1x/saved_model/\"\r\nHere is a part of my code : \r\n```\r\npath = \"gs://simclr-checkpoints-tf2/simclrv1/pretrain/1x/saved_model/\"\r\ndef create_model():\r\n    baseModel =  tf.keras.models.load_model(path)\r\n    baseModel.summary()   \r\n    model_output = tf.keras.layers.Dense(3, name=\"head_supervised_new\")(baseModel)\r\n    model = tf.keras.Model(inputs=baseModel.input, outputs=model_output)\r\n    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n              loss=tf.keras.losses.CategoricalCrossentropy(),\r\n              metrics=[tf.keras.metrics.CategoricalAccuracy()])  \r\n    return model\r\nmodel = create_model()\r\nmodel.fit(train_set)\r\n```"]}, {"number": 42424, "title": "```tf.stack``` returns different shapes when run in eager and graph mode", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.3\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n```tf.stack(list(np.ones([2,0,3]))).shape```\r\nreturns EagerTensor with shape [2,0]\r\n**Describe the expected behavior**\r\n```tf.stack(list(np.ones([2,0,3]))).shape```\r\nreturns EagerTensor with shape [2,0,3] which same as graph mode\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nEager mode:\r\n``` python\r\nimport tensorflow as tf\r\nprint(tf.stack(list(np.ones([2,0,3]))).shape)\r\ntf.compat.v1.disable_eager_execution()\r\nprint(tf.stack(list(np.ones([2,0,3]))).shape)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@fsx950223 \r\nI ran your code on colab tf 2.3 and did not face the issue mentioned, please have a look at the [gist here](https://colab.research.google.com/gist/Saduf2019/80b8950b2b559c2a64057a6bd8f3c3a2/untitled375.ipynb).", "> @fsx950223\r\n> I ran your code on colab tf 2.3 and did not face the issue mentioned, please have a look at the [gist here](https://colab.research.google.com/gist/Saduf2019/80b8950b2b559c2a64057a6bd8f3c3a2/untitled375.ipynb).\r\n\r\nYou should run it in eager mode and graph mode", "You could test it via follow example:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(tf.stack(list(np.ones([2,0,3]))).shape)\r\n@tf.function\r\ndef test():\r\n  print(tf.stack(list(np.ones([2,0,3]))).shape)\r\n\r\ntest()\r\n```", "The issue is really due to `tf.convert_to_tensor` which does not handling shape correctly:\r\n```\r\n% python3\r\nPython 3.7.3 (default, Jun  2 2020, 19:48:59) \r\n[Clang 11.0.3 (clang-1103.0.32.62)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> import numpy as np\r\n>>> x = list(np.ones([2,0,3]))\r\n>>> x\r\n[array([], shape=(0, 3), dtype=float64), array([], shape=(0, 3), dtype=float64)]\r\n>>> y = tf.convert_to_tensor(x)\r\n>>> y\r\n<tf.Tensor: shape=(2, 0), dtype=float32, numpy=array([], shape=(2, 0), dtype=float32)>\r\n```\r\n\r\nIt looks like `tf.convert_to_tensor()` in eager mode (or more specifically `InitEagerTensor`), dropped the dimension of `3`.", "I would like to work on this issue. How can I remove this error?", "Was able to reproduce the issue in 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ee363c69aca3ac5bcd18e73007c2bd0b/untitled61.ipynb#scrollTo=LQl8zZLibQ4m)..Thanks !"]}, {"number": 42419, "title": "parallel_for: No converter defined for UniqueWithCounts, SegmentSum, Range, Where, Tile in tf.vectorized_map()", "body": "- **TensorFlow version tested:** _(all installed using binary)_\r\n    - TensorFlow 2.3.0\r\n    - TensorFlow 2.2.0\r\n    - TensorFlow 1.15.0\r\n\r\n- **Issue:** I'm trying to decrease the latency of `my_func()` (code below) by replacing `tf.map_fn()` with `tf.vectorized_map()`. But it seems like not all ops are ported in as of now. Any idea when above mentioned ops will be available for vectorized_map? Feel free to suggest any alternative.\r\n\r\n- **Code:** \r\n```python\r\ndef my_func(tensor_x):\r\n    label_tensor = tensor_x[:, 0]\r\n    score_tensor = tensor_x[:, 1]\r\n    unique_labels, unique_labels_idx, unique_labels_counts = tf.unique_with_counts(label_tensor)\r\n    score_sum_flatten = tf.math.segment_sum(score_tensor, unique_labels_idx)\r\n    repeated_label_tensor = tf.repeat(unique_labels, unique_labels_counts)\r\n    repeated_score_tensor = tf.repeat(score_sum_flatten, unique_labels_counts)\r\n    label_score_tensor = tf.stack((repeated_label_tensor, repeated_score_tensor), axis=1)\r\n    label_score_tensor = tf.reshape(label_score_tensor, shape=tensor_x.shape)\r\n    return label_score_tensor\r\n```\r\n- **Expected behavior with tf.map_fn():** \r\n```python\r\n>> input_tensor = tf.constant([[[2.0, 0.5], [2.0, 0.5], [3.0, 0.4000000059604645], [1.0, 0.10000000149011612]], [[4.0, 0.800000011920929], [2.0, 0.5], [1.0, 0.5], [1.0, 0.5]], [[4.0, 0.800000011920929], [3.0, 0.699999988079071], [3.0, 0.30000001192092896], [1.0, 0.10000000149011612]]])\r\n>> tf.map_fn(my_func, input_tensor)\r\n\r\n<tf.Tensor: shape=(3, 4, 2), dtype=float32, numpy=\r\narray([[[2. , 1. ],\r\n        [2. , 1. ],\r\n        [3. , 0.4],\r\n        [1. , 0.1]],\r\n\r\n       [[4. , 0.8],\r\n        [2. , 0.5],\r\n        [1. , 1. ],\r\n        [1. , 1. ]],\r\n\r\n       [[4. , 0.8],\r\n        [3. , 1. ],\r\n        [3. , 1. ],\r\n        [1. , 0.1]]], dtype=float32)>\r\n```\r\n\r\n- **TF 2.3.0 [WARNING -- no speedup]:** Using tf.vectorized_map() with TF 2.3.0 I get following warnings and I'm not getting required speedup.\r\n```python\r\n>> tf.vectorized_map(my_func, input_tensor)\r\n\r\nWARNING:tensorflow:Using a while_loop for converting UniqueWithCounts\r\nWARNING:tensorflow:Using a while_loop for converting SegmentSum\r\nWARNING:tensorflow:Using a while_loop for converting Range\r\nWARNING:tensorflow:Using a while_loop for converting Where\r\nWARNING:tensorflow:Using a while_loop for converting Tile\r\nWARNING:tensorflow:Using a while_loop for converting Range\r\nWARNING:tensorflow:Using a while_loop for converting Where\r\nWARNING:tensorflow:Using a while_loop for converting Tile\r\n\r\n<tf.Tensor: shape=(3, 4, 2), dtype=float32, numpy=\r\narray([[[2. , 1. ],\r\n        [2. , 1. ],\r\n        [3. , 0.4],\r\n        [1. , 0.1]],\r\n\r\n       [[4. , 0.8],\r\n        [2. , 0.5],\r\n        [1. , 1. ],\r\n        [1. , 1. ]],\r\n\r\n       [[4. , 0.8],\r\n        [3. , 1. ],\r\n        [3. , 1. ],\r\n        [1. , 0.1]]], dtype=float32)>\r\n```\r\n\r\n- **TF2.2.0/TF1.15.0 [ERROR -- not working]:** tf.vectorized_map() fails on TF 2.2.0 and TF 1.15.0 since there is no `fallback_to_while_loop=True` argument available on previous versions of TF.\r\n```python\r\n>> tf.vectorized_map(my_func, input_tensor)\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-11-9f5af9e23eb8> in <module>\r\n----> 1 tf.vectorized_map(sum_me, sorted_tensor)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in vectorized_map(fn, elems)\r\n    405   if batch_size is None:\r\n    406     batch_size = array_ops.shape(first_elem)[0]\r\n--> 407   return pfor(loop_fn, batch_size)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, parallel_iterations)\r\n    196       def_function.run_functions_eagerly(False)\r\n    197     f = def_function.function(f)\r\n--> 198   outputs = f()\r\n    199   if functions_run_eagerly is not None:\r\n    200     def_function.run_functions_eagerly(functions_run_eagerly)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    625       # This is the first call of __call__, so we have to initialize.\r\n    626       initializers = []\r\n--> 627       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    628     finally:\r\n    629       # At this point we know that the initialization is complete (or less\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    504     self._concrete_stateful_fn = (\r\n    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 506             *args, **kwds))\r\n    507 \r\n    508     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2444       args, kwargs = None, None\r\n   2445     with self._lock:\r\n-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2447     return graph_function\r\n   2448 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2775 \r\n   2776       self._function_cache.missed.add(call_context_key)\r\n-> 2777       graph_function = self._create_graph_function(args, kwargs)\r\n   2778       self._function_cache.primary[cache_key] = graph_function\r\n   2779       return graph_function, args, kwargs\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2665             arg_names=arg_names,\r\n   2666             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2667             capture_by_value=self._capture_by_value),\r\n   2668         self._function_attributes,\r\n   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    979         _, original_func = tf_decorator.unwrap(python_func)\r\n    980 \r\n--> 981       func_outputs = python_func(*func_args, **func_kwargs)\r\n    982 \r\n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    440         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    442     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    443 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nValueError: in user code:\r\n/Users/snehal/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:183 f  *\r\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n    /Users/snehal/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py:269 _pfor_impl  **\r\n        outputs.append(converter.convert(loop_fn_output))\r\n    /Users/snehal/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1284 convert\r\n        output = self._convert_helper(y)\r\n    /Users/snehal/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/parallel_for/pfor.py:1464 _convert_helper\r\n        (y_op.type, y_op, converted_inputs))\r\n\r\n    ValueError: No converter defined for UniqueWithCounts\r\n    name: \"loop_body/UniqueWithCounts\"\r\n    op: \"UniqueWithCounts\"\r\n    input: \"loop_body/strided_slice\"\r\n    attr {\r\n      key: \"T\"\r\n      value {\r\n        type: DT_FLOAT\r\n      }\r\n    }\r\n    attr {\r\n      key: \"out_idx\"\r\n      value {\r\n        type: DT_INT32\r\n      }\r\n    }\r\n    \r\n    inputs: [WrappedTensor(t=<tf.Tensor 'loop_body/strided_slice/pfor/StridedSlice:0' shape=(3, 4) dtype=float32>, is_stacked=True, is_sparse_stacked=False)]. \r\n    Either add a converter or set --op_conversion_fallback_to_while_loop=True, which may run slower\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/39615d5b036455789f8ac0fed969aa09/42419-tf-nightly.ipynb). Thanks!", "Any news on this? Running into a similar issue with `vectorized_map` selecting something with `where` before the loop and then using it in the loop I get the warnings and with certain data it crashes with \r\n```\r\nTensorflow.python.framework.errors_impl.InvalidArgumentError:  PartialTensorShape: Incompatible shapes during merge: [5,1] vs. [4,1]\r\n```\r\n\r\nEDIT: Using a for loop in the function given to vectorized, no side effects, the function does not share any state"]}, {"number": 42410, "title": "ConverterError: input resource[0] expected type resource != float, the type of while_model_embed_gather_resource_0[0]", "body": "**System information**\r\n- OS Platform and Distribution: Linux archlinux 5.8.1-arch1-1 x86_64 GNU/Linux (using Anaconda3)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): >=2.2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\n\r\nclass Embed(tf.keras.layers.Layer):\r\n    def __init__(self, vocab_size, embed_dim, **kwargs):\r\n        self.vocab_size = vocab_size\r\n        self.embed_dim = embed_dim\r\n        super().__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.embeddings = self.add_weight(\r\n            \"weight\",\r\n            shape=[self.vocab_size, self.embed_dim],\r\n            initializer=tf.keras.initializers.GlorotNormal(),\r\n        )\r\n\r\n    def call(self, inputs):\r\n        return tf.gather(self.embeddings, tf.cast(inputs, tf.int32))\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self, vocab_size, embed_dim, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.embed = Embed(vocab_size, embed_dim)\r\n        self.dense = tf.keras.layers.Dense(350)\r\n\r\n    def _build(self):\r\n        self(tf.keras.Input([None], dtype=tf.int32))\r\n\r\n    def call(self, inputs, training=False):\r\n        outputs = self.embed(inputs, training=training)\r\n        return self.dense(outputs, training=training)\r\n\r\n\r\nmodel = Model(29, 320)\r\nmodel._build()\r\nmodel.summary()\r\n\r\n\r\n@tf.function(\r\n    input_signature=[\r\n        tf.TensorSpec([1, None], dtype=tf.int32)\r\n    ]\r\n)\r\ndef func(inputs):\r\n    i = tf.constant(0, dtype=tf.int32)\r\n    T = tf.constant(100, dtype=tf.int32)\r\n\r\n    def _cond(i, T): return tf.less(i, T)\r\n\r\n    def _body(i, T):\r\n        _ = model(inputs)\r\n        return i + 1, T\r\n\r\n    _, _ = tf.while_loop(\r\n        _cond,\r\n        _body,\r\n        loop_vars=(i, T),\r\n        shape_invariants=(\r\n            tf.TensorShape([]),\r\n            tf.TensorShape([])\r\n        )\r\n    )\r\n\r\n    return inputs\r\n\r\n\r\nprint(func(tf.zeros([1, 100], tf.int32)))\r\n\r\nconcrete_func = func.get_concrete_function()\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5080482/log.txt)\r\n\r\nShort output:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 77, in <module>\r\n    tflite = converter.convert()\r\n  File \"/home/nlhuy/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"/home/nlhuy/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 899, in convert\r\n    return super(TFLiteFrozenGraphConverterV2,\r\n  File \"/home/nlhuy/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 629, in convert\r\n    result = _toco_convert_impl(\r\n  File \"/home/nlhuy/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 569, in toco_convert_impl\r\n    data = toco_convert_protos(\r\n  File \"/home/nlhuy/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 202, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: input resource[0] expected type resource != float, the type of while_model_embed_gather_resource_0[0]\r\n\tIn {{node while/model/embed/Gather}}\r\n```\r\n\r\n**Failure details**\r\nIf I don't use `tf.while_loop`, the conversion works. However, it doesn't if I use `tf.gather` inside `tf.while_loop`\r\n\r\n**Any other info / logs**\r\nThis bug also occurs in `tf.keras.layers.Embedding`", "comments": ["@usimarit expand last dimention in ur input so [1, None] -> [1, None, 1] then use `tf.gather_nd` rather than `tf.gather`. I just tested, it works fine but not convinced :))). @jaeyoo can you take a look :)). ", "@usimarit \r\nPlease update as per above comment.", "@Saduf2019 I've tested the solution provided by @dathudeptrai and it works. Thank you a lot :smile: However, the bug in `tf.keras.layers.Embedding` and `tf.gather` still needs to be fixed.", "@Saduf2019 even it woks it's still not convinced, we should fix the `tf.gather` problem :)).", "I am able to replicate this issue, please find [gist here](https://colab.research.google.com/gist/Saduf2019/29ff1ec42f9401833986c08086c586ff/untitled377.ipynb)", "Look forward to this fix. Since huggingface transformers uses tf.gather for embeddings lookup. And it is currently blocked by this issue for coreml conversion with while loop decoding.", "Meghna, can you please have a look\r\n\r\nThanks", "Was able to replicate the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/981de58c616763b39d1012eb55dbfec6/untitled64.ipynb#scrollTo=PdHnKroHlnYZ)..Thanks !", "I am also facing the same issue. \r\nMy methodology is as below->\r\n\r\n1 - I am trying to build a model using the TF-Hub's universal-sentence-encoder in TF2.0 to serve as the embeddings like below -\r\n```\r\nembedding = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \r\nhub_layer = hub.KerasLayer(embedding, input_shape=[],dtype=tf.string, trainable=True)\r\nmodel = tf.keras.Sequential()\r\nmodel.add(hub_layer)\r\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam, \r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n```\r\n              \r\n2 - Now I am trying to save the model definition into a single .pb file using the below approach \r\n```\r\n\r\n# Convert Keras model to ConcreteFunction\r\nfull_model = tf.function(lambda x: model(x))\r\nfull_model = full_model.get_concrete_function(\r\n    tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype, name=\"input\"))\r\n# Get frozen ConcreteFunction\r\nfrozen_func = convert_variables_to_constants_v2(full_model)\r\nfrozen_func.graph.as_graph_def()\r\nlayers = [op.name for op in frozen_func.graph.get_operations()]\r\nprint(\"-\" * 50)\r\nprint(\"Frozen model layers: \")\r\nfor layer in layers:\r\n    print(layer)\r\nprint(\"-\" * 50)\r\nprint(\"Frozen model inputs: \")\r\nprint(frozen_func.inputs)\r\nprint(\"Frozen model outputs: \")\r\nprint(frozen_func.outputs)\r\n# Save frozen graph from frozen ConcreteFunction to hard drive\r\ntf.io.write_graph(graph_or_graph_def=frozen_func.graph,\r\n                  logdir=\"./frozen_models\",\r\n                  name=\"frozen_graph_keraslayer.pb\",\r\n                  as_text=False)\r\n\r\n\r\n```\r\n\r\n3 - On running these codes, the warning can be seen. \r\n\r\n\r\nOS  - macOS Catalina intel i3 2020\r\nPython Version - 3.7.11\r\nTF Version - 2.7.0\r\nTF-Hub Version - 0.12.0\r\n (all installed via pip)\r\n\r\nTagging everyone to bump this topic up again. @usimarit, @MeghnaNatraj , @ymodak, @dathudeptrai , @karimnosseir \r\n", "I could replicate this issue in [2.8 ](https://colab.sandbox.google.com/gist/mohantym/9a837a46022ed575819e7afdc4bf09fa/github_42410.ipynb)version too."]}, {"number": 42399, "title": "Adding Conv3DLSTM and depth-wise separable Conv3D layers to tensorflow.keras API", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): Yes (but may not have the technical coding skills)\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThank you for creating this wonderful tool for researchers. It would be extremely helpful to have implementations of Conv3DLSTM and depth-wise separable Conv3D layers to the API for 3D medical image analysis.\r\n\r\n**Will this change the current api? How?**\r\nYes, these layers would extend the features currently available in 2D within the current API making it more useful to researchers working with 3D data\r\n\r\n**Who will benefit with this feature?**\r\nResearchers working with 3D imaging data, particularly in the medical field\r\n\r\n**Any Other info.**\r\nI would appreciate any available tips or advice on how I could go by extending these features myself or how I could contribute to adding those features to the API", "comments": ["The Conv3DLSTM feature would be very useful for my work also.", "@omarallouz, \r\n\r\n`ConvLSTM3D` layer is available in `tf.keras` API. For more information you can refer [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ConvLSTM3D). Thanks!"]}, {"number": 42393, "title": "Add Object Detection EfficientDet/Retinanet Model(s) to tf.keras.applications", "body": "**System information**\r\n- TensorFlow version (2.3): Although this request feels very TF 2.4.\r\n- Are you willing to contribute it: I could be involved in testing. \r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n`tf.keras.applications` is amazing. With all the model architectures and the pre-trained weights there is a lot\r\nof possibilities for transfer learning and feature extraction. However, there are not any object detection models.\r\n\r\nI would love to see some object detection models baked into \r\n`tf.keras.applications`  as well as pre-trained weights (trained on object detection tasks like coco for example) that would make it possible for training new object detection models, via transfer learning, on custom data. Very much like how the current API works for classification models for example i.e. (import resnet, remove top/head, add layer, freeze, train).\r\n\r\nSince we already have Resnet backbones and Efficient backbones (as of TF 2.3), would it be possible to add object detection architectures on top of these such as [Retinanet](https://arxiv.org/abs/1708.02002) or [EfficientDet](https://arxiv.org/abs/1911.09070). As well as have the pre-trained weights trained on coco for example for such models. Then having some sort of easy way to adjust these for training on custom object detection data. Much like with the current models we can remove the top/head and add on top for custom classifiers. \r\n\r\nIt's not straight forward to get started with object detection using tf.keras.  I am also aware of the newly designed TF Models Detection API but it still feels bloated and very disconnected from TF main library. It would be lovely to see something built into tf.keras.applications.\r\n\r\n**Will this change the current api? How?**\r\nJust by adding the new feature request.\r\n\r\n**Who will benefit with this feature?**\r\nAll users of tf.keras.applications. Anyone working in industry with current TF 2 in their tech stack looking to add object detection functionality.\r\n\r\n**Any Other info.**\r\nI think this would be amazing! \r\n\r\nThere is the project https://github.com/google/automl/tree/master/efficientdet which is Google and it seems like they are trying to use tf.keras there some. There is also https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md but it has so much else going on in it. How can we take these projects and pick out the pieces and build for example EfficientDet into tf.keras.applications. At the moment things are convoluted and lots of outside projects. Having this within tf.keras.applications main code would be nice and easier to build off in existing tech stacks that already use Tensorflow. ", "comments": ["Hello @ymodak, I would like to work on this issue. I am new to this but this **can help me learn alot**. \r\nCan you tell where I have to add this in your code base?", "@aavishkarmishra , love the enthusiasm but I think this is a good project for internal Google developers and core tf.keras team developers. It\u2019s a fairly big ask but one that I think would have tons of value. \r\n\r\n", "@pengchongjin @fchollet , think this will get any attention? Thanks for your hard work."]}, {"number": 42388, "title": "Image-Scaling Attacks: Discuss threat and mention secure scaling options to prevent attacks", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/resize\r\n\r\n## Description of issue:\r\nTensorFlow is vulnerable to image-scaling attacks if specific scaling algorithms and parameters are used. These attacks\r\nallow an adversary to arbitrarily change the output image of a downscaling operation. It is a threat for machine learning applications, similar to adversarial examples. \r\n\r\nThe resize documentation should therefore at least mention the risk of scaling attacks with specific resize settings, and motivate the usage of secure algorithms/parameters.\r\n\r\n### Clear description\r\nDownscaling is an important step in machine learning, as deep neural networks usually expect small fixed-sized inputs. An adversary can control the output of the downscaling step by slightly manipulating the input image. We see another image what the ML system uses for learning. This allows an attack, similar to adversarial examples. The attack is easy to deploy with a considerable impact on the security of machine learning. It is relevant whenever TF scales images that users did not create themselves.\r\n\r\nWe verified that TensorFlow provides vulnerable scaling algorithms, such as nearest, bilinear or bicubic scaling. In the default settings, many pixels are hardly or not at all included in the reduction. Attacks can thus only change those pixels that are relevant to the scaling. The remaining pixels are not changed, so that the attack is rather unnoticeable. A secure scaling algorithm should thus consider all pixels (equally).\r\n\r\nTo prevent the attack in TensorFlow, a user can either choose area scaling or set ```antialias = True``` with TensorFlow 2.x. This parameter was introduced in TF 2.x, but it is set to False by default (probably for compatibility to older version). Nearest neighbor scaling is still vulnerable if ```antialias = True``` (antialias is here ignored as documented). For older TensorFlow versions, only area scaling prevents the attack without changing the code base.\r\n\r\nOur project website ([https://scaling-attacks.net](https://scaling-attacks.net)) shows some examples and gives more information. In our research paper on scaling attacks [1], presented at the USENIX Security Symposium, we provide a detailed root-cause analysis and discuss possible defenses.\r\n\r\n_All in all, the resize documentation should raise the awareness for the risk of scaling attacks in machine learning. \r\nIn the long run, scaling algorithms with secure parameters should be used as default (e.g. ```antialias = True``` as default)._\r\n\r\n\r\n\r\n[1] Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning, Erwin Quiring et al., USENIX Security Symposium 2020. [Paper-Link](https://www.sec.cs.tu-bs.de/pubs/2020-sec.pdf)\r\n", "comments": []}, {"number": 42380, "title": "Embedding for Embedding Projector works on one computer, not on another.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1909\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): Version: 2.3.0 (same problem with 2.2.0)\r\n- Python version: 3.7, 3.8\r\nWorking computer\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX1080 8 GB\r\nNon-working computer 1 (Lenovo Y470P)\r\n- CUDA/cuDNN version 10.1\r\n- GPU model and memory: GTX950M, variable memory\r\nNon-working computer 2 (Dell XPS 8500)\r\n- CUDA/cuDNN version 10.1\r\n-GPU model and memory: GTX970, 4 GB (actually only 3.5 GB)\r\n\r\nProjections for use with Embedding Projector works on one computer, doesn't work on two others. On computer 1 I get the \"No checkpoint found.\" message; on computer 2 I get a blank screen in the Projecter area, nothing at all. Computer 1 used to be able to display Embedding Projector with Tensorflow 1.x.  I've never tried running Tensorboard on Computer 2 until now. All systems are 64-bit Windows 10, same version.\r\n\r\nExpected behavior was that Embedding projector would work on computers other than my main desktop.\r\n\r\nEmbedding folders can be downloaded at this link:\r\nhttps://drive.google.com/file/d/18Zojdf50CmwsS-YulwMMOBYFY-3_YORI/view?usp=sharing\r\n\r\nNo significant error or logging messages show up.\r\n", "comments": ["@leszekmp,\r\nCan you please share the below details:\r\n\r\n1. Browsers of all the 3 computers, in which you've tried opening Embedding Projector\r\n2. Operating Systems of all the 3 Computers\r\n3. Screenshots of both working and non-working Embedding Projectors.\r\n4. Steps you have followed to visualize the embeddings.\r\n\r\nThanks!", "Thanks for responding! Checked my original posting, and the info I gave got scrambled. I've tried it on three computers:\r\n\r\nA. My main computer (the one it's working on):\r\nCUDA/cuDNN version: 10.1\r\nGPU model and memory: GTX1080 8 GB\r\nSystem memory: 64 GB RAM\r\nIntel 6-core processor\r\n\r\nB. My laptop ( the first one it's not working on):\r\nLenovo Y470P, 16 GB RAM, 4-core Intel processor\r\nCUDA/cuDNN version 10.1\r\nGPU model and memory: GTX950M, variable memory\r\n\r\nC. My secondary desktop (the second one it's not working on):\r\nDell XPS 8500, 32 GB RAM, 4-core Intel processor\r\nCUDA/cuDNN version 10.1\r\n-GPU model and memory: GTX970, 4 GB (actually only 3.5 GB)\r\n\r\n\r\n1. I've tried Chrome and Edge on all three computers. Chrome and Edge both work on the \"working computer\", both don't work on the other two. All three systems run Tensorboard 2.2.2.\r\n2. All three systems run Windows 10 1909.\r\n3. I've attached screenshots from all three computers from Chrome at the bottom; Edge looks identical.\r\n4.  Here's the Python code that creates the embedding files for Tensorboard. (Frankensteined from online code I found). One of these days, I'll update it to TF 2. The sprite and metadata files are created elsewhere. \r\n\r\ndef register_embedding(embedding_tensor_name, meta_data_fname, sprite_name, log_dir):\r\n    image_dims=[224,224]\r\n    config = projector.ProjectorConfig()\r\n    embedding = config.embeddings.add()\r\n    embedding.tensor_name = embedding_tensor_name + \":0\"\r\n    embedding.metadata_path = meta_data_fname\r\n    embedding.sprite.image_path = sprite_name\r\n    embedding.sprite.single_image_dim.extend(image_dims)\r\n    projector.visualize_embeddings(log_dir, config)\r\n\r\nLOG_DIR = PATH+ '/' + test_id\r\nMETA_DATA_FNAME = test_id+ '.meta.tsv'  # Labels will be stored here\r\nEMBEDDINGS_TENSOR_NAME = 'features'\r\nEMBEDDINGS_FPATH = os.path.join(LOG_DIR, EMBEDDINGS_TENSOR_NAME + '.ckpt')\r\nSPRITE_NAME=test_id+'.jpg'\r\nSTEP=0\r\n\r\nregister_embedding(EMBEDDINGS_TENSOR_NAME, META_DATA_FNAME, SPRITE_NAME, LOG_DIR)\r\n\r\ntensor_embeddings = tf.Variable(feature_vectors, name=EMBEDDINGS_TENSOR_NAME)\r\nsaver = tf.compat.v1.train.Saver([tensor_embeddings])  # Must pass list or dict\r\nsaver.save(sess=None, global_step=STEP, save_path=EMBEDDINGS_FPATH)\r\n\r\nSame command used on all three for invoking Tensorboard: tensorboard --logdir (data_directory). I use the same batch file for all three computers to start up Tensorboard, but I've also hand-typed the commands as well, and they only work on my main computer.\r\n\r\n5. I tried switching over to the online projector (projector.tensorflow.org), but ran into a problem. PCA and UMAP look the same on both my main \"working computer\" and the online projector, but t-SNE looks completely different. Have checked the t-SNE settings multiple times, and they're the same on both the local and web projectors. No idea as to why t-SNE looks different, but the other two don't.\r\n\r\n![TB_working_computer](https://user-images.githubusercontent.com/8127516/90819433-d5772e80-e2e4-11ea-8464-a2ed12beb8e3.png)\r\n![TB_not_working_laptop](https://user-images.githubusercontent.com/8127516/90819430-d4de9800-e2e4-11ea-9d5b-742456504b69.png)\r\n![TB_not_working_desktop](https://user-images.githubusercontent.com/8127516/90819436-d60fc500-e2e4-11ea-8dc2-e0ad22b22b28.png)"]}, {"number": 42376, "title": "Exception when concatenating empty flattened layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, attached in colab.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab & local Ubuntu 20.04 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tried\r\n- TensorFlow installed from (source or binary): Colab: bundled, local: using pip install\r\n- TensorFlow version (use command below): Colab: v2.3.0-0-gb36436b087, local: 2.2.0\r\n- Python version: local: 3.8.2\r\n\r\n**Describe the current behavior**\r\nUsing keras bundled with tensorflow, from one input with shape (None, num > 0) and another input with shape (None, num2 > 0, 0), the second is flattened, obtaining shape (None, 0), and, when trying to concatenate both, it fails during training because it assumes the shape of the second has double number of rows. Interestingly, the summary description of the model after compiling is correct. A minimum code example is in this [colab](https://colab.research.google.com/drive/1oSFd_4MBvX0-e3-S3PEpnZeJyz_RNSAY?usp=sharing).\r\n\r\nThe exception in colab is,\r\n```\r\nInvalidArgumentError:  ConcatOp : Dimensions of inputs should match: shape[0] = [5,1] vs. shape[1] = [10,0]\r\n\t [[node functional_5/concatenate_2/concat (defined at <ipython-input-3-f5c5cd6d4c13>:29) ]] [Op:__inference_train_function_1468]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\nWhile locally, I get\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: All dimensions except 1 must match. Input 1 has shape [256 0] and doesn't match input 0 with shape [128 10].\r\n\t [[{{node training/Adam/gradients/gradients/concatenate_1/concat_grad/ConcatOffset}}]]\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe flattened version of the input with a zero dimension should have the same number of rows.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab example](https://colab.research.google.com/drive/1oSFd_4MBvX0-e3-S3PEpnZeJyz_RNSAY?usp=sharing).\r\n\r\n**Other info / logs**\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1096                 batch_size=batch_size):\r\n   1097               callbacks.on_train_batch_begin(step)\r\n-> 1098               tmp_logs = train_function(iterator)\r\n   1099               if data_handler.should_sync:\r\n   1100                 context.async_wait()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    838         # Lifting succeeded, so variables are initialized and we can run the\r\n    839         # stateless function.\r\n--> 840         return self._stateless_fn(*args, **kwds)\r\n    841     else:\r\n    842       canon_args, canon_kwds = \\\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2827     with self._lock:\r\n   2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2830 \r\n   2831   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)\r\n   1846                            resource_variable_ops.BaseResourceVariable))],\r\n   1847         captured_inputs=self.captured_inputs,\r\n-> 1848         cancellation_manager=cancellation_manager)\r\n   1849 \r\n   1850   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1922       # No tape is watching; skip to running the function.\r\n   1923       return self._build_call_outputs(self._inference_function.call(\r\n-> 1924           ctx, args, cancellation_manager=cancellation_manager))\r\n   1925     forward_backward = self._select_forward_and_backward_functions(\r\n   1926         args,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    548               inputs=args,\r\n    549               attrs=attrs,\r\n--> 550               ctx=ctx)\r\n    551         else:\r\n    552           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError:  ConcatOp : Dimensions of inputs should match: shape[0] = [5,1] vs. shape[1] = [10,0]\r\n\t [[node functional_5/concatenate_2/concat (defined at <ipython-input-3-f5c5cd6d4c13>:29) ]] [Op:__inference_train_f\r\n```\r\n", "comments": ["I noted that changing the `Flatten` layer to a `Reshape((x_extra_data.shape[1] * x_extra_data.shape[2],))` layers works as expected, although it is not ideal to know the shape of data in advance.", "Was able to reproduce the issue in TF 2.3 and Nightly version. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/c4e0e0cb96d88078d9a1ae22b40c38bf/issue.ipynb). Thanks!", "Adding the `contributions welcome` label to this issue for further investigation by the community. If you are interested in working on this issue, please leave a comment and I will assign it to you. Thanks! ", "I am able to reproduce the issue in TF 2.5. The error message is \r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError:  ConcatOp : Dimensions of inputs should match: shape[0] = [5,1] vs. shape[1] = [10,0]\r\n         [[node model/concatenate/concat (defined at reproduce.py:29) ]] [Op:__inference_train_function_464]`\r\n\r\nI am interested in this issue. Please assign to me if this issue is still available.", "@CyangXu Can you share your analysis/proposal to solve this before opening a PR?", "@bhack Based on my investigation, the output size inferred by Keras can be different from that calculated by C++ code. I did some work two weeks ago then got distracted by something else. I will have more things to show by end of this week.", "@CyangXu Ok thanks. Have you already compiled tensorflow in the case the change will involve C ++?", "Yes. I built TF from source code. I investigated the issue by printing log from the C++ side.", "Ok send us an update when you are ready.", "Take more time than my expectation. Hopefully I can have something to show before Friday.", "More progress! In the code to reproduce the issue, when calling `print(model.summary())`,  `extra_layer`'s shape is (None, 10). I believe this shape is produced from [array_ops.reshape()](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/keras/layers/core.py;l=693;drc=56f148ddeb6a571bc78f69fbf23229426ed176ba;bpv=1;bpt=0?q=layers%2Fcore.py). Then when hitting `model.fit(x=[x_data, x_extra_data], y=y_data, epochs=2)` in the reproduction code, we reshape the tensor by [ReshapeOp](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/reshape_op.h;l=104;drc=a41733ae0a093040cacf3aad010786fa6bed5f1b;bpv=1;bpt=0). The output `shape` is (10, 0). In other words, the output shape of \"Flatten()\" calculated by Keras is different from that calculated by C++ operation. The fixing should be straightforward: we have to unify the result from Keras and that from C++. \r\n\r\nI do have a question: it seems that when reshaping a tensor in Keras, we hit [here](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/ops/array_ops.py;l=61;drc=0b9ff2eb1a097602206c6b29823543768bfb34fe;bpv=1;bpt=0?q=array_ops.py). **Does anyone know where I can find the implementation for that Python function**?", "Array ops are in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/array_ops.cc", "Was able to replicate the issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/63e916ab451437a9bc2fda112ac70a94/untitled66.ipynb#scrollTo=b6akEO2SqORI)..Thanks !", "Very sorry that I am unable to continue working on this issue due to other things. If anyone is interested in this issue, feel free to take it. The bug's root cause is clear as demonstrated by the comment above. "]}, {"number": 42370, "title": "Changing hyper parameters during training based on scalars", "body": "I'm training a model and logging values to tensorboard. Based on these values, I would like to tune hyperparameters (in code, not by hand). For example, one could think about switching to a different loss function, or changing a coefficient in the loss function, as a function of the loss.\r\n\r\nIt is not hard to find _some_ way to do it: one way would be to simply:\r\n\r\n1. Work with eager execution\r\n2. Save the scalars I'm logging to tensorboard also to a python list\r\n3. Do computations on those lists in between batches\r\n4. Execute different eager tensorflow logic, or assign different hyperparameters, based on those computation\r\n\r\nIs there a way that is more idiomatic? For example, it would be nice to:\r\n\r\n- Not have to save all these scalars to python lists myself. Can they be retrieved from tensorboard in a performant way?\r\n- Use the tf.function api for better performance\r\n\r\nThis was originally a [Stack Overflow question](https://stackoverflow.com/questions/63403109/changing-hyper-parameters-during-training-based-on-scalars).\r\n", "comments": ["@gowthamkpr is there any more information I could provide, that would help you to respond to this issue?", "Are you using keras here? Can you please explain what api are you using here. Thanks!", "I'm also using keras, but for this question, only tensorflow seems relevant. I'm just computing scalars all the time, such as losses, and logging those. I want to access all these historical loss values in a performant way, e.g. to determine whether the loss has stabilized (enough) or not using a test.", "Are these features (and is this documentation) new? Is this feature somewhat performant?\r\n\r\nhttps://www.tensorflow.org/tensorboard/dataframe_api"]}, {"number": 42360, "title": "Inference for two or more neural networks on gpu", "body": "I am using tensorflow 1.15.0 and built a library from source with GPU gl delegates. I am using the native C ++ API. With Android NDK I am building an executable for Android.\r\n\r\nI want to make inference for two or more networks. To do this, I create an instance of the class, in which the interpreter is created during initialization, the GPU delegate is applied and the inference is executed. The first instance with a neural network is created and performs the inference perfectly. But when the second instance of the class is initialized, \r\n\r\n**Problem**: I get an error when calling the ModifyGraphWithDelegate method:\r\n\r\n`Assertion failed (TfLiteGpuDelegate Prepare: Shader compilation failed: 0:8: L0001: Typename expected, found 'unknown' : Node number 69 (TfLiteGpuDelegate) failed to prepare.`\r\n\r\nor\r\n\r\n`Assertion failed (TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'data' : Syntax error:  syntax error INTERNAL ERROR: no main() function! ERROR: 1 compilation errors.  No code generated. Node number 69 (TfLiteGpuDelegate) failed to prepare.`\r\n\r\nI have verified that for the second EGL thread the context is created normally and set as current. Devices supports OpenGL ES 3.2\r\n\r\n<details>\r\n<summary>Some code:</summary>\r\n\r\n<code><pre>\r\nclass Model\r\n{\r\npublic:\r\n\ttypedef std::shared_ptr<ModelInterpreter> Ptr;\r\n\r\n\tModel(const std::string tflite_model_path)\r\n\t{\r\n                auto model = FlatBufferModel::BuildFromFile(tflite_model_path);\r\n                if (!model) return false;\r\n\r\n                tflite::ops::builtin::BuiltinOpResolver resolver;\r\n\r\n                tflite::InterpreterBuilder interpreter_builder(\r\n                \t*model,\r\n                \tresolver);\r\n\r\n                const TfLiteStatus build_status = interpreter_builder(&interpreter, 1);\r\n\r\n                TfLiteGpuDelegateOptions delegate_options = TfLiteGpuDelegateOptionsDefault();\r\n                delegate_options.metadata = nullptr;\r\n                delegate_options.compile_options.precision_loss_allowed = true; // FP16\r\n                delegate_options.compile_options.preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST;\r\n                delegate_options.compile_options.dynamic_batch_enabled = 0;\r\n\r\n                gl_delegate = TfLiteGpuDelegateCreate(&delegate_options);\r\n\r\n                interpreter->ModifyGraphWithDelegate(gl_delegate);\r\n\r\n                interpreter->SetAllowFp16PrecisionForFp32(true);\r\n        }\r\n\r\n        ~Model()\r\n        {\r\n                interpreter = nullptr;\r\n                TfLiteGpuDelegateDelete(gl_delegate);\r\n        }\r\n\r\n        void predict(\r\n                const Size input_size,\r\n                float const* const input_data,\r\n                const Size output_size,\r\n                float* const output_data)\r\n        {\r\n               WriteToInputTensor(interpreter->typed_input_tensor<float>(0));\r\n               interpreter->Invoke();\r\n               ReadFromOutputTensor(interpreter->typed_output_tensor<float>(0));\r\n        }\r\n\r\n        private:\r\n                std::unique_ptr<tflite::Interpreter> interpreter;\r\n                TfLiteDelegate* gl_delegate;\r\n\r\n                Model(const Model&) = delete;\r\n                Model& operator=(const Model&) = delete;\r\n};\r\n\r\n</pre></code>\r\n\r\n</details>\r\n\r\nWhy does this error occur, tell me?", "comments": []}, {"number": 42349, "title": "tf.data.experimental.dense_to_ragged_batch fails with inputs from generator with unspecified shape in TF 2.3", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Both\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows + Linux (Colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary (PyPI)\r\n- TensorFlow version (use command below): 2.1, 2.2, 2.3\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: Titan RTX/P100\r\n\r\n**Describe the expected behavior**\r\nIn TF 2.1, 2.2, and 2.3 batching variable length elements work fine when generated from tensor slices:\r\n```python\r\nds = tf.data.Dataset.from_tensor_slices(tf.range(4))\r\n\r\n# Generate variable length elements via map.\r\n# First batch will have length 1. Subsequent batches will have length 2.\r\ndef f(x):\r\n  if x == 0:\r\n    return tf.ones([1,])\r\n  else:\r\n    return tf.ones([2,])\r\nds = ds.map(f)\r\n\r\n# Inspect individual elements.\r\nprint(\"Unbatched shapes:\")\r\nfor batch in ds:\r\n  print(batch.shape)\r\nprint()\r\n\r\n# Batch into ragged tensors.\r\nds = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=2))\r\n\r\n# Inspect batched elements.\r\nprint(\"Batched shapes:\")\r\nfor batch in ds:\r\n  print(batch.to_tensor().shape)\r\n```\r\nOutputs:\r\n```\r\nUnbatched shapes:\r\n(1,)\r\n(2,)\r\n(2,)\r\n(2,)\r\n\r\nBatched shapes:\r\n(2, 2)\r\n(2, 2)\r\n```\r\n\r\nNow, in TF 2.1 and 2.2, this also works when the dataset consumes elements from a generator:\r\n```python\r\n# Generate elements via a generator.\r\n# First batch will have length 1. Subsequent batches will have length 2.\r\ndef gen():\r\n  for i in range(4):\r\n    if i == 0:\r\n      yield tf.ones((1,))\r\n    else:\r\n      yield tf.ones((2,))\r\nds = tf.data.Dataset.from_generator(gen, output_types=tf.float32)\r\n\r\n# Inspect individual elements.\r\nprint(\"Unbatched shapes:\")\r\nfor batch in ds:\r\n  print(batch.shape)\r\nprint()\r\n\r\n# Batch into ragged tensors.\r\nds = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=2))\r\n\r\n# Inspect batched elements.\r\nprint(\"Batched shapes:\")\r\nfor batch in ds:\r\n  print(batch.to_tensor().shape)\r\n```\r\n\r\nOutputs:\r\n```\r\nUnbatched shapes:\r\n(1,)\r\n(2,)\r\n(2,)\r\n(2,)\r\n\r\nBatched shapes:\r\n(2, 2)\r\n(2, 2)\r\n```\r\nAs expected, we get identical outputs both before and after batching.\r\n\r\n\r\n**Describe the current behavior**\r\nIn TF 2.3, the generator version results in an error:\r\n```\r\n[...]\r\nInvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2]. [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-313ce2c9fab5> in <module>()\r\n     16 \r\n     17 print(\"Batched shapes:\")\r\n---> 18 for batch in ds:\r\n     19   print(batch.to_tensor().shape)\r\n[...]\r\nInvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2].\r\n```\r\n\r\nThe release notes for 2.3 mention:\r\n\r\n> - tf.data.experimental.dense_to_ragged_batch works correctly with tuples.\r\n> - tf.data.experimental.dense_to_ragged_batch to output variable ragged rank.\r\n\r\nPresumably this issue is related to these changes.\r\n\r\nHere's the relevant implementation for the actual batching in TF 2.2:\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/data/experimental/ops/batching.py#L371-L426\r\n\r\nAnd in TF 2.3:\r\nhttps://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/data/experimental/ops/batching.py#L380-L452\r\n\r\nAs suggested by the changes above, the behavior prior to 2.3 is achieved again when the output shape is specified, even if unknown:\r\n```python\r\n# Generate elements via a generator.\r\n# First batch will have length 1. Subsequent batches will have length 2.\r\ndef gen():\r\n  for i in range(4):\r\n    if i == 0:\r\n      yield tf.ones((1,))\r\n    else:\r\n      yield tf.ones((2,))\r\n\r\n# Creating the generator explicitly specifying the unknown shape.\r\nds = tf.data.Dataset.from_generator(\r\n    gen,\r\n    output_types=tf.float32,\r\n    output_shapes=tf.TensorShape([None])\r\n)\r\n\r\n# Inspect individual elements.\r\nprint(\"Unbatched shapes:\")\r\nfor batch in ds:\r\n  print(batch.shape)\r\nprint()\r\n\r\n# Batch into ragged tensors.\r\nds = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=2))\r\n\r\n# Inspect batched elements.\r\nprint(\"Batched shapes:\")\r\nfor batch in ds:\r\n  print(batch.to_tensor().shape)\r\n```\r\n\r\nOutputs:\r\n```\r\nUnbatched shapes:\r\n(1,)\r\n(2,)\r\n(2,)\r\n(2,)\r\n\r\nBatched shapes:\r\n(2, 2)\r\n(2, 2)\r\n```\r\n\r\nIt's definitely less convenient to have to specify the output shapes in the generator, requiring some refactoring when updating to 2.3 -- maybe the shapes could just default to unknown when batching if unspecified in the generator?\r\n\r\nI appreciate that this is an experimental function, which is noted in the [tf.data.experimental](https://www.tensorflow.org/api_docs/python/tf/data/experimental) docs:\r\n\r\n> Note that the tf.data.experimental API is not subject to the same backwards compatibility guarantees as tf.data, but **we will provide deprecation advice in advance of removing existing functionality**.\r\n\r\nIf this is intended behavior, then perhaps it could be documented somewhere as it does remove the \"functionality\" of being able to ragged batch elements from a generator without specified output shape :)\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n- [Colab: TF 2.1](https://colab.research.google.com/drive/1lm9aOqtGob7PD_LMflmCONuXrpA-M1Kg?usp=sharing) (Works)\r\n- [Colab: TF 2.2](https://colab.research.google.com/drive/1clcicwg22-DVYirFlR_IAlbWaAn-NfUf?usp=sharing) (Works)\r\n- [Colab: TF 2.3](https://colab.research.google.com/drive/1NK5SWwqUaMsisDBjsRe8E-gpc3kcLI5S?usp=sharing) (Fails)\r\n- [Colab: TF 2.3 with `output_shape` specified](https://colab.research.google.com/drive/1WKYYNm96xHIMfdBn_nNwAaN_BmurVuM1?usp=sharing) (Works)\r\n\r\n\r\n**Other info / logs**\r\nFull traceback of the error in TF 2.3:\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2101       ctx.executor = executor_new\r\n-> 2102       yield\r\n   2103     finally:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    757             output_types=self._flat_output_types,\r\n--> 758             output_shapes=self._flat_output_shapes)\r\n    759 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py in iterator_get_next(iterator, output_types, output_shapes, name)\r\n   2609     except _core._NotOkStatusException as e:\r\n-> 2610       _ops.raise_from_not_ok_status(e, name)\r\n   2611     except _core._FallbackException:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6842   # pylint: disable=protected-access\r\n-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6844   # pylint: enable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2]. [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-313ce2c9fab5> in <module>()\r\n     16 \r\n     17 print(\"Batched shapes:\")\r\n---> 18 for batch in ds:\r\n     19   print(batch.to_tensor().shape)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in __next__(self)\r\n    734 \r\n    735   def __next__(self):  # For Python 3 compatibility\r\n--> 736     return self.next()\r\n    737 \r\n    738   def _next_internal(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in next(self)\r\n    770   def next(self):\r\n    771     try:\r\n--> 772       return self._next_internal()\r\n    773     except errors.OutOfRangeError:\r\n    774       raise StopIteration\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    762         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    763       except AttributeError:\r\n--> 764         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    765 \r\n    766   @property\r\n\r\n/usr/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)\r\n     97                 value = type()\r\n     98             try:\r\n---> 99                 self.gen.throw(type, value, traceback)\r\n    100             except StopIteration as exc:\r\n    101                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2103     finally:\r\n   2104       ctx.executor = executor_old\r\n-> 2105       executor_new.wait()\r\n   2106 \r\n   2107 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)\r\n     65   def wait(self):\r\n     66     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     68 \r\n     69   def clear_error(self):\r\n\r\nInvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2].\r\n```\r\n", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/12ad7a2cbacdeed027d6a5dcde2e39f8/untitled371.ipynb).", "Was able to replicate this issue in TF 2.6.0-dev20210528,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/4600daa1fd55bb20059c0873f9a1133b/untitled67.ipynb#scrollTo=-bRdnwusr1Dz)..Thanks !"]}, {"number": 42348, "title": "Issue with iou_threshold in non_max_suppression", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: 2060\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWith TF 2.2.0, it looks like the threshold is weak (i.e. it excludes all the boxes if their iou is greater or equal than the threshold).\r\n\r\n**Describe the expected behavior**\r\n\r\nWith TF 1.15, this threshold is strong.\r\nIn general, it doesn't make a lot of difference, except in one particular use case.\r\nIf I do a NMS to keep only non overlapping boxes, I would use a threshold of 0.\r\nWith TF 1.15, the result is the list of all non-overlapping boxes as expected.\r\nWith TF 2.2, the ouput is a single box, the one with the highest score.\r\nThere is an easy workaround of adding a small epsilon, but it's not very intuitive.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n    boxes = tf.constant([[0.1, 0.1, 0.3, 0.3], [0.5, 0.5, 0.7, 0.7], [0.6, 0.6, 0.8, 0.8]], dtype=tf.float32)\r\n    scores = tf.constant([0.9, 0.8, 0.7], dtype=tf.float32)\r\n\r\n    print('Expected indices')\r\n    print(tf.image.non_max_suppression(boxes, scores, 100, iou_threshold=1e-5))\r\n\r\n    print('Actual indices')\r\n    print(tf.image.non_max_suppression(boxes, scores, 100, iou_threshold=0.))\r\n\r\n\r\nOutput\r\n\r\n```\r\nExpected indices\r\ntf.Tensor([0 1], shape=(2,), dtype=int32)\r\nActual indices\r\ntf.Tensor([0], shape=(1,), dtype=int32)\r\n```", "comments": ["I have tried in colab with TF version 2.3, nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/6a1118b74f9cf4fe3a71dc5c0f788b23/untitled251.ipynb).Thanks!", "Was able to reproduce this issue in TF v2.7.0,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/02ef1ba736dbd7d3c0d445eb78d96866/untitled68.ipynb)..Thanks !"]}, {"number": 42346, "title": "Different inference values tensorflow vs tflite model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra-10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not checked\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nModel built using tensorflow and later converter to tflite gives different results for same input. See code provided to reproduce the error.\r\n\r\n**Describe the expected behavior**\r\nMust produce same output. \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow.keras.models import Model, Sequential, load_model\r\nfrom tensorflow.keras import layers, Input\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport imutils\r\nimport tflite\r\nimport cv2\r\n\r\n\r\ndef get_laplacian_kernel(shape, dtype=None):\r\n    h, w, nr_channels, channel_multiplier  = shape\r\n    assert channel_multiplier == 1\r\n    assert h == w == 3\r\n    laplacian_mask = np.array([[ 0,  1, 0],\r\n                               [ 1, -4, 1],\r\n                               [ 0,  1, 0]])\r\n    mask = np.zeros(shape)\r\n    for ch in range(nr_channels):\r\n        mask[:, :, ch, 0] = laplacian_mask\r\n    laplacian_kernel = K.variable(mask, dtype=dtype)\r\n    return laplacian_kernel\r\n\r\n\r\ndef build_laplacian_model():\r\n    # allow input image to be of any size\r\n    input_shape = (1200, 1800, 3)\r\n    input_layer = Input(shape=input_shape, )\r\n    layer_conv_laplacian = layers.DepthwiseConv2D(kernel_size=3,\r\n                                                  depthwise_initializer=get_laplacian_kernel, \r\n                                                  depth_multiplier=1,\r\n                                                  bias_initializer='zeros',\r\n                                                  strides=1, padding='same',\r\n                                                  trainable=False\r\n                                                  )(input_layer)\r\n    layer_variance = layers.Lambda(lambda tensor: K.var(tensor), trainable=False)(layer_conv_laplacian)\r\n    laplacian_model = Model(inputs=input_layer, outputs=layer_variance)\r\n    return laplacian_model\r\n\r\nblur_model = build_laplacian_model()\r\nprint(blur_model.summary())\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(blur_model)\r\nfpath = 'model.tflite'\r\nwith open(fpath, 'wb') as fw:\r\n    fw.write(converter.convert())\r\nprint(f'tflite model fpath: {fpath}')\r\n\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=fpath)\r\n\r\ndef tflite_inference(model, imdata):\r\n    input_details = model.get_input_details()\r\n    output_details = model.get_output_details()\r\n    model.resize_tensor_input(input_details[0][\"index\"], imdata.shape)\r\n    model.allocate_tensors()\r\n    model.set_tensor(input_details[0]['index'], imdata)\r\n    model.invoke()\r\n    result = model.get_tensor(output_details[0]['index'])\r\n    return result\r\n\r\n\r\n# inference using both tensorflow and tflite models\r\nprint('\\n\\nInference using both models')\r\nurls = ['https://images.unsplash.com/photo-1470020337050-543c4e581988',\r\n        'https://images.unsplash.com/photo-1516811108838-030371f93644',\r\n        'https://zipbooks.com/wp-content/uploads/2017/05/royalty-free-images-free-of-charge.jpeg',\r\n       ]\r\n\r\nfor url in urls:\r\n    imdata = imutils.url_to_image(url)\r\n    imdata = cv2.resize(imdata, dsize=(1800, 1200))\r\n    imdata = np.array([imdata]).astype(np.float32)\r\n    pred0 = blur_model.predict(imdata)\r\n    pred1 = tflite_inference(interpreter, imdata)\r\n    print(f'tensorflow: {pred0} \\t tflite: {pred1}')\r\n```\r\n\r\n**Other info / logs** \r\nOutput\r\n```\r\nModel: \"functional_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 1200, 1800, 3)]   0         \r\n_________________________________________________________________\r\ndepthwise_conv2d (DepthwiseC (None, 1200, 1800, 3)     30        \r\n_________________________________________________________________\r\nlambda (Lambda)              ()                        0         \r\n=================================================================\r\nTotal params: 30\r\nTrainable params: 0\r\nNon-trainable params: 30\r\n_________________________________________________________________\r\nNone\r\nWARNING:tensorflow:From /Users/<...>/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /Users/<...>/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: /var/<...>/assets\r\ntflite model fpath: model.tflite\r\n\r\n\r\nInference using both models\r\ntensorflow: 981.5494995117188 \t tflite: 976.8895874023438\r\ntensorflow: 289.7860412597656 \t tflite: 285.5138854980469\r\ntensorflow: 57.03384017944336 \t tflite: 56.148902893066406\r\n```\r\n", "comments": ["@naolenikhil I am able to reproduce the issue. [Here](https://colab.research.google.com/gist/jvishnuvardhan/e0688bf5d2093db84478f70b1623fb29/untitled3.ipynb) is the gist for our reference. Thanks!", "@jvishnuvardhan - Thanks for creating the gist. \r\n1. Were you able to identify the root cause of this issue?\r\n2. Does a previous version of tensorflow or tflite solve this issue? \r\n3. Are you expecting any other information from me? ", "Wanted to check on progress of the ticket. I created [#42694 ](https://github.com/tensorflow/tensorflow/issues/42694) . I suspect variance calculation is the main culprit here and DepthwiseConv2D operations are fine. ", "Hey @thaink, could you take a look at this issue? See [this comment](https://github.com/tensorflow/tensorflow/issues/42694#issuecomment-685172606) for more details on the where our reduction ops might be causing loss of precision."]}, {"number": 42319, "title": "How to convert a image caption model to tensorflow Lite model?ValueError: Python inputs incompatible with input_signature:", "body": "**System information**\r\n- OS Platform and Distribution :CentOS Linux release 7.7.1908\r\n-TensorFlow version:2.3.0\r\n\r\n\r\nI am following this example:<https://www.tensorflow.org/tutorials/text/image_captioning?hl=en>\r\n\r\nIt is working as it should be and saving checkpoints and I want to now convert this to a TF Lite model.\r\n\r\nHere is the Link of full convert code:<https://colab.research.google.com/drive/1GJkGcwWvDAWMooTsECzuSRUSPbirADhb?usp=sharing>\r\n\r\nHere is the Link of full train code:\r\n<https://colab.research.google.com/drive/1X2d9WW1EMEzN8Rgva3rtjevP0T_jFccj?usp=sharing>\r\n\r\nI also following the [isssue#32999](https://github.com/tensorflow/tensorflow/issues/32999)\r\n\r\nHere is what I am running to save and them convert the inference graph:\r\n\r\n\r\n```\r\n@tf.function\r\ndef evaluate(image):\r\n    hidden = decoder.reset_states(batch_size=1)\r\n\r\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\r\n    img_tensor_val = image_features_extract_model(temp_input)\r\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\r\n\r\n    features = encoder(img_tensor_val)\r\n\r\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\r\n    result = []\r\n\r\n    for i in range(max_length):\r\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\r\n\r\n        predicted_id = tf.random.categorical(predictions, 1)[0][0]\r\n        # print(tokenizer.index_word)\r\n        print(predicted_id,predicted_id.dtype)\r\n\r\n        # for key,value in tokenizer.index_word.items():\r\n        #     key = tf.convert_to_tensor(key)\r\n        #     tf.dtypes.cast(key,tf.int64)\r\n        #     print(key)\r\n\r\n        # print(tokenizer.index_word)\r\n\r\n        result.append(predicted_id)\r\n\r\n        # if tokenizer.index_word[predicted_id] == '<end>':\r\n        #     return result\r\n\r\n        dec_input = tf.expand_dims([predicted_id], 0)\r\n\r\n    return result\r\n\r\nexport_dir = \"./\"\r\ntflite_enc_input = ''\r\nckpt.f = evaluate\r\nto_save = evaluate.get_concrete_function('')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])\r\ntflite_model = converter.convert()\r\n```\r\nbut I get this error\r\n```\r\nValueError: in user code:\r\n\r\n    convert2savedmodel.py:310 evaluate  *\r\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\r\n    /share/nishome/19930072_0/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__  **\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /share/nishome/19930072_0/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:780 __call__\r\n        result = self._call(*args, **kwds)\r\n    /share/nishome/19930072_0/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py:840 _call\r\n        return self._stateless_fn(*args, **kwds)\r\n    /share/nishome/19930072_0/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/python/eager/function.py:2828 __call__\r\n        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n    /share/nishome/19930072_0/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/python/eager/function.py:3171 _maybe_define_function\r\n        *args, **kwargs)\r\n    /share/nishome/19930072_0/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/python/eager/function.py:2622 canonicalize_function_inputs\r\n        self._flat_input_signature)\r\n    /share/nishome/19930072_0/miniconda3/envs/tf2.3/lib/python3.7/site-packages/tensorflow/python/eager/function.py:2713 _convert_inputs_to_signature\r\n        format_error_message(inputs, input_signature))\r\n\r\n    ValueError: Python inputs incompatible with input_signature:\r\n      inputs: (\r\n        Tensor(\"ExpandDims_1:0\", shape=(1, 1), dtype=int32),\r\n        Tensor(\"cnn__encoder/StatefulPartitionedCall:0\", shape=(1, 64, 256), dtype=float32),\r\n        Tensor(\"zeros:0\", shape=(1, 512), dtype=float32))\r\n      input_signature: (\r\n        TensorSpec(shape=(1, 1), dtype=tf.int64, name=None),\r\n        TensorSpec(shape=(1, 64, 256), dtype=tf.float32, name=None),\r\n        TensorSpec(shape=(1, 512), dtype=tf.float32, name=None))\r\n\r\n```\r\n\r\nEncoder Model:\r\n```\r\nclass CNN_Encoder(tf.keras.Model):\r\n    def __init__(self, embedding):\r\n        super(CNN_Encoder, self).__init__()\r\n        # shape after fc == (batch_size, 64, embedding_dim)\r\n        self.fc = tf.keras.layers.Dense(embedding_dim)\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(1, 64, features_shape),dtype=tf.dtypes.float32)])\r\n    def call(self, x):\r\n        x = self.fc(x)\r\n        x = tf.nn.relu(x)\r\n        return x\r\n```\r\nDecoder model:\r\n```\r\nclass RNN_Decoder(tf.keras.Model):\r\n    def __init__(self, embedding_dim, units, vocab_size):\r\n        super(RNN_Decoder, self).__init__()\r\n        self.units = units\r\n\r\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n        self.gru = tf.keras.layers.GRU(self.units,\r\n                                       return_sequences=True,\r\n                                       return_state=True,\r\n                                       recurrent_initializer='glorot_uniform',\r\n                                       unroll = True)\r\n        self.fc1 = tf.keras.layers.Dense(self.units)\r\n        self.fc2 = tf.keras.layers.Dense(vocab_size)\r\n\r\n        self.attention = BahdanauAttention(self.units)\r\n\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[1, 1], dtype=tf.int64),\r\n                                  tf.TensorSpec(shape=[1, 64, 256], dtype=tf.float32),\r\n                                  tf.TensorSpec(shape=[1, 512], dtype=tf.float32)])\r\n    def call(self, x , features, hidden):\r\n\r\n        context_vector, attention_weights = self.attention(features, hidden)\r\n\r\n        #x shape after passing through embedding == (batch_size, 1, embedding_dim)\r\n        x = self.embedding(x)\r\n\r\n        #x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\r\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\r\n\r\n\r\n        output, state = self.gru(x)\r\n\r\n        #shape == (batch_size, max_length, hidden_size)\r\n        x = self.fc1(output)\r\n\r\n        #x shape == (batch_size, max_length, hidden_size)\r\n        x = tf.reshape(x, (-1, x.shape[2]))\r\n\r\n        # output shape == (batch_size * max_length, vocab)\r\n        x = self.fc2(x)\r\n\r\n        return x, state, attention_weights\r\n\r\n    def reset_states(self, batch_size):\r\n        return tf.zeros((batch_size, self.units))\r\n```", "comments": ["@DavidInWuhanChina \r\nI ran into another error while i try to execute the code shared, please find [gist here](https://colab.research.google.com/gist/Saduf2019/e487ff83d175e357aa5c4859fbd4a8a2/untitled367.ipynb), please share all dependencies for us to replicate error faced.", "> @DavidInWuhanChina\r\n> I ran into another error while i try to execute the code shared, please find [gist here](https://colab.research.google.com/gist/Saduf2019/e487ff83d175e357aa5c4859fbd4a8a2/untitled367.ipynb), please share all dependencies for us to replicate error faced.\r\n\r\nThe coco2017 dateset is so big(13GB) that I can't put it in the colab.Please tell me How to do next?", "From the error message looks like your have he tf.function expecting first input as int64 while the input is int32. Can you try changing the tf.function to expect int32 instead.\r\n\r\nThanks", "> From the error message looks like your have he tf.function expecting first input as int64 while the input is int32. Can you try changing the tf.function to expect int32 instead.\r\n> \r\n> Thanks\r\n\r\nI just change the tf.function to int32 as below:\r\n`@tf.function(input_signature=[tf.TensorSpec(shape=[1, 1], dtype=tf.int32),\r\n                                  tf.TensorSpec(shape=[1, 64, 256], dtype=tf.float32),\r\n                                  tf.TensorSpec(shape=[1, 512], dtype=tf.float32)])`\r\nbut another error came:\r\n    ValueError: Python inputs incompatible with input_signature:\r\n      inputs: (\r\n        Tensor(\"ExpandDims_2:0\", shape=(1, 1), dtype=int64),\r\n        Tensor(\"cnn__encoder/StatefulPartitionedCall:0\", shape=(1, 64, 256), dtype=float32),\r\n        Tensor(\"rnn__decoder/StatefulPartitionedCall:1\", shape=(1, 512), dtype=float32))\r\n      input_signature: (\r\n        TensorSpec(shape=(1, 1), dtype=tf.int32, name=None),\r\n        TensorSpec(shape=(1, 64, 256), dtype=tf.float32, name=None),\r\n        TensorSpec(shape=(1, 512), dtype=tf.float32, name=None))\r\nWhy the dtypes of inputs change from int64 to int32?", "@karimnosseir to follow up comments again. cc @abattery "]}, {"number": 42294, "title": "Huge CPU, GPU output diff after updates to fully_connected GpuDelegate gl kernel.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nAndroid\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTested on Pixel2\r\n\r\n- TensorFlow installed from (source or binary):\r\nBuilt from source\r\n\r\n- TensorFlow version (use command below):\r\nv2.3.0\r\n\r\n- Python version:\r\n3.7\r\n\r\n- Bazel version (if compiling from source):\r\n3.1.0\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n7.5.0\r\n\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThere seems to be a bug in tensorflow/lite/delegates/gpu/gl/kernels/fully_connected.cc that was first updated in [this commit](https://github.com/tensorflow/tensorflow/commit/7c7d924821a8b1b20433c2f3f484bbd409873a84#diff-a1d24db3e0ad4103e389ffed433cf470R82).\r\n\r\nThe test itself in tensorflow/lite/delegates/gpu/gl/kernels/fully_connected_test.cc passes but when running\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff on a simple custom model, the avg_error between the CPU and the GPU delegate piles up and leads to completely different results.\r\n\r\n**Describe the expected behavior**\r\nThe avg_error between the CPU and the GPU delegate should stay the same even after the updates to fully_connected code, and it should not produce weird outputs.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n1. Prepare `run_eval` in v2.3.0\r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  //tensorflow/lite/tools/evaluation/tasks/inference_diff:run_eval\r\ncp bazel-bin/tensorflow/lite/tools/evaluation/tasks/inference_diff/run_eval ./run_eval_1\r\n```\r\n\r\n2. Change the fully_connected shader implementation to the one prior to commit `7c7d9248`\r\nThe simple implementation without the usage of shared variables, memoryBarrierShared, etc.\r\n\r\n3. Prepare `run_eval` after second step\r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  //tensorflow/lite/tools/evaluation/tasks/inference_diff:run_eval\r\ncp bazel-bin/tensorflow/lite/tools/evaluation/tasks/inference_diff/run_eval ./run_eval_2\r\n```\r\n\r\n4. Connect android device and push the tflite models below, run_eval_1 and run_eval_2 to device\r\n```\r\nadb push [] /data/local/tmp\r\n```\r\n[simple.zip](https://github.com/tensorflow/tensorflow/files/5066647/simple.zip)\r\nThe models in the zip file are submodels from a custom model that was sliced from the input layer to deeper layers accumulatively, so that running `run_eval` on them would show how the error changes in the subsequent layers.\r\n\r\n\r\n5. Run both eval scripts on the models and save the output\r\n```\r\n# For example\r\nadb shell find /data/local/tmp/simple -type f -name \"*.tflite\" | sort -V | xargs -L 1 -I{} sh -c \"echo {}; adb shell /data/local/tmp/run_eval_1 --model_file={} --delegate=gpu --gpu_precision_loss_allowed=false --num_runs=5\" > eval_1.txt\r\n```\r\n\r\n6. Compare output from both eval results\r\n```\r\n[Output from run_eval_1]\r\n\r\n/data/local/tmp/simple/0_layer.tflite (input)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=350.6(us), std_dev=182(us)\r\nTest run latency: avg=7788.33(us), std_dev=1361(us)\r\nOutputDiff[0]: avg_error=0, std_dev=0\r\n\r\n/data/local/tmp/simple/1_layer.tflite (conv2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=51505.2(us), std_dev=19231(us)\r\nTest run latency: avg=76189.3(us), std_dev=21566(us)\r\nOutputDiff[0]: avg_error=5.65735e-08, std_dev=1.23668e-10\r\n\r\n/data/local/tmp/simple/2_layer.tflite (conv2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=156689(us), std_dev=34476(us)\r\nTest run latency: avg=91240.9(us), std_dev=20015(us)\r\nOutputDiff[0]: avg_error=2.88166e-07, std_dev=4.7596e-10\r\n\r\n/data/local/tmp/simple/3_layer.tflite (average_pooling2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=154217(us), std_dev=49214(us)\r\nTest run latency: avg=63426.3(us), std_dev=3858(us)\r\nOutputDiff[0]: avg_error=2.06003e-07, std_dev=5.04424e-10\r\n\r\n/data/local/tmp/simple/4_layer.tflite (average_pooling2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=154598(us), std_dev=56655(us)\r\nTest run latency: avg=43006.3(us), std_dev=8692(us)\r\nOutputDiff[0]: avg_error=8.01869e-08, std_dev=8.87667e-09\r\n\r\n/data/local/tmp/simple/5_layer.tflite (dense)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=156916(us), std_dev=54132(us)\r\nTest run latency: avg=36966.8(us), std_dev=1220(us)\r\nOutputDiff[0]: avg_error=0, std_dev=0\r\n\r\n/data/local/tmp/simple/6_layer.tflite (dense)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=152091(us), std_dev=53248(us)\r\nTest run latency: avg=36698(us), std_dev=1180(us)\r\nOutputDiff[0]: avg_error=0.00405404, std_dev=8.56487e-06\r\n\r\n/data/local/tmp/simple/7_layer.tflite (reshape)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=157181(us), std_dev=52575(us)\r\nTest run latency: avg=39436.9(us), std_dev=2816(us)\r\nOutputDiff[0]: avg_error=0.00405404, std_dev=8.56487e-06\r\n\r\n/data/local/tmp/simple/8_layer.tflite (mul)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=151318(us), std_dev=51893(us)\r\nTest run latency: avg=47089.9(us), std_dev=3074(us)\r\nOutputDiff[0]: avg_error=0.00415553, std_dev=1.17143e-05\r\n\r\n/data/local/tmp/simple/9_layer.tflite (conv2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=176098(us), std_dev=49177(us)\r\nTest run latency: avg=51363.7(us), std_dev=933(us)\r\nOutputDiff[0]: avg_error=0.0141088, std_dev=9.8753e-05\r\n\r\n/data/local/tmp/simple/10_layer.tflite (conv2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=194176(us), std_dev=42305(us)\r\nTest run latency: avg=56783.7(us), std_dev=942(us)\r\nOutputDiff[0]: avg_error=0.116762, std_dev=0.00102424\r\n```\r\n\r\n```\r\n[Output from run_eval_2]\r\n\r\n/data/local/tmp/simple/0_layer.tflite (input)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=404.067(us), std_dev=291(us)\r\nTest run latency: avg=9060.07(us), std_dev=2928(us)\r\nOutputDiff[0]: avg_error=0, std_dev=0\r\n\r\n/data/local/tmp/simple/1_layer.tflite (conv2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=49690.5(us), std_dev=18876(us)\r\nTest run latency: avg=76622.3(us), std_dev=16760(us)\r\nOutputDiff[0]: avg_error=5.65735e-08, std_dev=1.23668e-10\r\n\r\n/data/local/tmp/simple/2_layer.tflite (conv2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=154928(us), std_dev=37358(us)\r\nTest run latency: avg=91402.5(us), std_dev=20801(us)\r\nOutputDiff[0]: avg_error=2.88166e-07, std_dev=4.7596e-10\r\n\r\n/data/local/tmp/simple/3_layer.tflite (average_pooling2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=156680(us), std_dev=51722(us)\r\nTest run latency: avg=63421.1(us), std_dev=5300(us)\r\nOutputDiff[0]: avg_error=2.06003e-07, std_dev=5.04424e-10\r\n\r\n/data/local/tmp/simple/4_layer.tflite (average_pooling2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=168827(us), std_dev=51702(us)\r\nTest run latency: avg=42583.5(us), std_dev=9370(us)\r\nOutputDiff[0]: avg_error=8.01869e-08, std_dev=8.87667e-09\r\n\r\n/data/local/tmp/simple/5_layer.tflite (dense)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=153892(us), std_dev=54096(us)\r\nTest run latency: avg=38346.1(us), std_dev=2488(us)\r\nOutputDiff[0]: avg_error=0, std_dev=0\r\n\r\n/data/local/tmp/simple/6_layer.tflite (dense)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=158208(us), std_dev=54326(us)\r\nTest run latency: avg=38795.7(us), std_dev=2783(us)\r\nOutputDiff[0]: avg_error=2.78233e-08, std_dev=0\r\n\r\n/data/local/tmp/simple/7_layer.tflite (reshape)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=159020(us), std_dev=56125(us)\r\nTest run latency: avg=36865.9(us), std_dev=1231(us)\r\nOutputDiff[0]: avg_error=2.78233e-08, std_dev=0\r\n\r\n/data/local/tmp/simple/8_layer.tflite (mul)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=155681(us), std_dev=51329(us)\r\nTest run latency: avg=46260.3(us), std_dev=2496(us)\r\nOutputDiff[0]: avg_error=3.92487e-08, std_dev=9.90499e-11\r\n\r\n/data/local/tmp/simple/9_layer.tflite (conv2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=192473(us), std_dev=49285(us)\r\nTest run latency: avg=68372(us), std_dev=8581(us)\r\nOutputDiff[0]: avg_error=1.127e-07, std_dev=2.97108e-10\r\n\r\n/data/local/tmp/simple/10_layer.tflite (conv2d)\r\nGPU delegate is created.\r\nNum evaluation runs: 5\r\nReference run latency: avg=184917(us), std_dev=50705(us)\r\nTest run latency: avg=57926.3(us), std_dev=4106(us)\r\nOutputDiff[0]: avg_error=6.8757e-07, std_dev=1.08467e-09\r\n```\r\n\r\nin `run_eval_1`, the `avg_error` rises high after the dense layers, when compared to `run_eval_2`.\r\n\r\nThis leads to a weird output in the custom model that I'm using in an Android app. When I rollback the changes in the `fully_connected` code and build the aar myself, it works as expected and I can benefit the performance enhancements on devices that support OpenCL.\r\n\r\nCould you please have a look at it?\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["> When I rollback the changes in the `fully_connected` code and build the aar myself, it works as expected and I can benefit the performance enhancements on devices that support OpenCL.\r\n\r\nCan you clarify which commit breaks it?  Should make it much easier if you could pinpoint the problematic commit.", "The commit was linked above, but let me paste it again here for convenience.\r\n\r\n[https://github.com/tensorflow/tensorflow/commit/7c7d924821a8b1b20433c2f3f484bbd409873a84](https://github.com/tensorflow/tensorflow/commit/7c7d924821a8b1b20433c2f3f484bbd409873a84)", "Must've missed that.  Thank you, Kevin!", "Wow, that's a commit from a year ago.  Either nobody is using FC, or you're hitting a weird corner case.  We'll look into that.", "I got in touch with the engineer who worked on that commit.  He told me that the implementation had a flaw but got fixed with commit `522aa755f9e4cdb37ab8d80ac252df60553c6439` from Sep 12, 2019.  Can you sync to that?", "The comparison was done between \r\n\r\nA. The code in tag v2.3.0, which includes the code changes you mentioned in `522aa755f9e4cdb37ab8d80ac252df60553c6439`\r\n\r\nand \r\n\r\nB. Just the `fully_connected.cc` replaced by its former implementation, prior to commit `7c7d924821a8b1b20433c2f3f484bbd409873a84`\r\n\r\nSo I don't think commit `522aa755f9e4cdb37ab8d80ac252df60553c6439` fixed the issue."]}, {"number": 42280, "title": "Predict on batch being able to do the prediction with inputs of different first shapes", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFor a recommender system usage we want to be able to have a keras model that takes inputs with different first dimensions, for example one input with N rows and another with one row.\r\nThat feature was also asked here https://github.com/keras-team/keras/issues/13646 for another use case.\r\n\r\nUntil https://github.com/tensorflow/tensorflow/commit/56a0ce87911236765633d2a873e706ebc6401ef9 predict on batch did not check the cardinality of tensors before creating the dataset so our usage fits in the Keras api.\r\nbut now there is an additional _check_data_cardinality(data):\r\nhttps://github.com/tensorflow/tensorflow/commit/56a0ce87911236765633d2a873e706ebc6401ef9#diff-f8dd40712ac721c1b363e1a1ec44c1a3R1523\r\nthat will break if you pass two inputs one with one row and one with N rows. \r\n\r\n**Will this change the current api? How?**\r\nProvide a way to disable that check, either making it a warning or removing it. \r\n\r\n**Who will benefit with this feature?**\r\nUsers with non static input shapes, recommenders systems where we want to take into advantage the fact that we do a prediction on 1 movie for all the users to use broadcasting instead of running N times the same embedding. \r\n**Any Other info.**\r\n", "comments": ["Hello,\r\n\r\nyou could just use the **call** method instead of the **predict** or **predict_batch** method.\r\nThis would make your code even shorter, because you can write:\r\n`output = model(input)` instead of `output = model.predict(input)`\r\n\r\nmaybe it will solve your issue?", "@tanguycdls As mentione in the above comment, did you try using the call method?", "hi thanks @bela127 @gowthamkpr yes it works but for performance reason i want to graph it. I could wrap it into a tf function but my input is a dict of ragged tensor and those are currently not supported well while in a dict.\r\nIn the mean time i found another trick: if you use a generator and predict instead the check does not happen as well.\r\n\r\nI just find that check odd at such high level of the api: for datasets the check makes sense since we might want to batch it so we would need to split the data in the first dim: But for predict_on_batch there is only one batch and if there is a shape issue I think Tensorflow in the graph will break with a clear error. ", "+1 to what @tanguycdls mentioned, in predict_on_batch we don't need such check, can we just raise an error if there is any issue following the \"let it crash\" philosophy?", "@tanguycdls  It looks like you are using an older Version of Tensorflow (2.3). Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version (2.4.1 or 2.5.0) and let us know if the issue still persists? Thanks!", "@sushreebarsa Yes the issue persists, the workaround is to create a custom tf dataset and then we no longer call the check_data_cardinality function. \r\nThis is why I believe the behavior is strange, if you create a dataset using predict directly it breaks, if you create beforehand the tf dataset it works. "]}, {"number": 42277, "title": "XNNPACK delegate performs much slower than default TFLite backend if multi-threading is configured according to documentation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04, Android 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Any Android smarthphone**\r\n- TensorFlow installed from (source or binary): **Source**\r\n- TensorFlow version (use command below): **2.3.0**\r\n- Python version: **-** \r\n- Bazel version (if compiling from source): **3.1.0**\r\n- GCC/Compiler version (if compiling from source): GCC 5.4.0 / Clang shipped with Android NDK 21\r\n- CUDA/cuDNN version: **-** \r\n- GPU model and memory: **-**\r\n\r\nWhen TFLite is built with XNNPACK, performance improvement is expected. However, seems like the code provided in `/tensorflow/lite/examples/minimal` with a minor change of interpreter settings leads to performance degradation comparing to the default build.\r\n\r\nSo here is the code taken from minimal example with my changes:\r\n\r\n```cpp\r\n  ...\r\n\r\n  // Load model\r\n  std::unique_ptr<tflite::FlatBufferModel> model =\r\n      tflite::FlatBufferModel::BuildFromFile(filename);\r\n  TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n  // Build the interpreter\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  InterpreterBuilder builder(*model, resolver);\r\n  std::unique_ptr<Interpreter> interpreter;\r\n  builder(&interpreter);\r\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n\r\n  // Allocate tensor buffers.\r\n  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n  printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  // Set number of threads (added by me)\r\n  interpreter->SetNumThreads(8);\r\n\r\n  // Run inference\r\n  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n  printf(\"\\n\\n=== Post-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  ...\r\n```\r\n\r\nThis code performs slower if executed with TFLite + XNNPACK build. I've tested it both on x64 desktop and arm64 Android using ResNet-34 FP32 TFLite model and observed the exact same performance degradation.\r\n\r\nI was able to fix the behavior and achieve 30% performance improvement only after I spent few hours with TFLite code and found out that **tflite::Interpreter::SetNumThreads is not applied to XNNPACK delegate (maybe to other delegates as well)**, because XNNPACK delegate is only initialized in `builder(&interpreter)` with number of threads passed to this invocation and then is not being updated on `interpreter->SetNumThreads(8)` call! In the  case is illustrated by the code above), XNNPACK work in single-threaded mode or so. So the fix is to initialize interpreter as the following:\r\n\r\n```cpp\r\n  builder(&interpreter, 8);\r\n```\r\n\r\nThen XNNPACK really introduces significant performance improvement.\r\n\r\nI'm OK with the solution that I found, but I was really confused with this issue and spent almost a day figuring out why can't I achieve claimed performance, because neither official documentation nor TFLite code commentary does not mention `InterpreterBuilder`'s `num_threads` argument as something necessary or mention it at least. Thus, following [\"Tweak the number of threads\"](https://www.tensorflow.org/lite/performance/best_practices#tweak_the_number_of_threads) documentation section in combination with using XNNPACK will lead anyone to this pitfall and will result with a very poor performance.\r\n\r\nIf it's need I can provide more standalone example, point to the parts of TFLite code, which are responsible for this behavior, and detailed measurements obtained on different devices.\r\n\r\n", "comments": ["@dev0x13 Sorry for tagging along, but could you please share how did you build TFLite library with xnnpack enabled? Is it as following?\r\n\r\n```\r\nbazel build -c opt --define tflite_with_xnnpack=true --config=android_arm64 //tensorflow/lite:libtensorflowlite.so\r\n```\r\n\r\nI tried this, but still no available symbols are found yet.", "@andreydung Yes, I am building the library this way. Here are the steps for clarity sake:\r\n\r\n1. Get sources from https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0.\r\n2. Run `./configure` with all parameters being default, except Android NDK setup (I am using NDK 21 btw).\r\n3. Run `bazel build -c opt --define tflite_with_xnnpack=true --config=android_arm64 //tensorflow/lite:libtensorflowlite.so`.\r\n\r\nWorks like a charm for me.", "@dev0x13 Thanks for your response, it's much appreciated.", "Hi @dev0x13 \r\nI followed your approach to confine number of threads to 1, by passing it as argument to tflite::InterpreterBuilder\r\nHowever, I get an error with destructor.\r\n\r\n```\r\n(gdb) bt\r\n#0  0x00007ffff5e314c0 in __pthread_timedjoin_ex () from /lib64/libpthread.so.0\r\n#1  0x00007ffff3686661 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#2  0x00007ffff3696137 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#3  0x00007fffeb67dd69 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#4  0x00007fffeb67e1b1 in ?? ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#5  0x00007fffeb640353 in tflite::flex::DelegateData::~DelegateData() ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#6  0x00007fffeb639134 in tflite::FlexDelegate::~FlexDelegate() ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#7  0x00007fffee762e3f in tflite::TfLiteDelegateFactory::DeleteSimpleDelegate(TfLiteDelegate*) ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#8  0x00007fffeb61799e in tflite::impl::Interpreter::~Interpreter() ()\r\n   from /home/smaniyar/Projects/test_project/target/linux-x86-64/bin/validation_package/11/lib/libtensorflowlite.so\r\n#9  0x00007ffff72bbdc7 in boost::checked_delete<tflite::impl::Interpreter> (x=0xe83960)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/checked_delete.hpp:34\r\n#10 0x00007ffff72bc998 in boost::detail::sp_counted_impl_p<tflite::impl::Interpreter>::dispose (this=0x129b190)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/sp_counted_impl.hpp:78\r\n#11 0x00007ffff70af8b6 in boost::detail::sp_counted_base::release (this=0x129b190)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/sp_counted_base_gcc_x86.hpp:146\r\n#12 0x00007ffff70af949 in boost::detail::shared_count::~shared_count (this=0x100bd88, __in_chrg=<optimized out>)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/detail/shared_count.hpp:371\r\n#13 0x00007ffff72b962c in boost::shared_ptr<tflite::impl::Interpreter>::~shared_ptr (this=0x100bd80, __in_chrg=<optimized out>)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost/boost/smart_ptr/shared_ptr.hpp:328\r\n#14 0x00007ffff72bca60 in TensorflowInferenceCore::ThreadInterpreter::~ThreadInterpreter (this=0x100bd80,\r\n    __in_chrg=<optimized out>)\r\n    at /home/smaniyar/Projects/test_project/src-main/deep_learning_inference/tensorflow_lite/TensorflowInferenceCore.h:17\r\n#15 0x00007ffff72bca86 in boost::thread_specific_ptr<TensorflowInferenceCore::ThreadInterpreter>::delete_data::operator() (\r\n    this=0x1009fc0, data=0x100bd80) at /home/smaniyar/Projects/test_project/vendor/boost_thread/boost/thread/tss.hpp:42\r\n#16 0x00007ffff72c3d7f in boost::detail::set_tss_data (key=0x6c5628, func=..., tss_data=0x0, cleanup_existing=true)\r\n    at /home/smaniyar/Projects/test_project/vendor/boost_thread/src/pthread/thread.cpp:716\r\n#17 0x00007ffff72b9760 in boost::thread_specific_ptr<TensorflowInferenceCore::ThreadInterpreter>::~thread_specific_ptr (\r\n    this=0x6c5628, __in_chrg=<optimized out>) at /home/smaniyar/Projects/test_project/vendor/boost_thread/boost/thread/tss.hpp:79\r\n#18 0x00007ffff72b8434 in TensorflowInferenceCore::~TensorflowInferenceCore (this=0x6c5618, __in_chrg=<optimized out>)\r\n    at /home/smaniyar/Projects/test_project/src-main/deep_learning_inference/tensorflow_lite/TensorflowInferenceCore.cpp:35\r\n\r\n```\r\n\r\nThreadinterpreter is just a struct defined as follows : \r\n```\r\nstruct ThreadInterpreter {\r\n\texplicit ThreadInterpreter() :\r\n\t\tinterpreter_(nullptr),\r\n\t\tinput_tensor(nullptr),\r\n\t\toutput_tensor(){}\r\n\r\n\tboost::shared_ptr<tflite::Interpreter> interpreter_;\r\n\tTfLiteTensor* input_tensor;\r\n\tstd::vector<TfLiteTensor*> output_tensor;\r\n};\t\r\n```\r\n\r\nI built tensorflowlite with xnnpack delegate dependency as follows : \r\n```\r\nbazel build -c opt //tensorflow/lite:tensorflowlite --config=monolithic --experimental_ui_max_stdouterr_bytes=1070048576 --define tflite_with_xnnpack=true --jobs=16\r\n```\r\n\r\nHow do I properly delete thread resources in the destructor?\r\n\r\nAny help on fixing this issue will be really appreciated.\r\n", "Hi @suraj-maniyar . I believe that your error has nothing to do with the threading settings, because setting number threads does not require any specific destruction logic.", "My program uses multiprocessing via forking and that is somehow causing mutex lock condition with Tensorflow's threading mechanism.", "@suraj-maniyar Oh, forking should not be used in multithreaded environment, [here ](https://www.linuxprogrammingblog.com/threads-and-fork-think-twice-before-using-them) is a good explanation of why it should be avoided. The simplest workaround is to use process spawning instead of forking.", "@dev0x13 \r\nI have a hard requirement of forking. One parent process can fork one or many child processes to do work, and the destructor gets called once for each child process + once for the parent process. I cannot avoid this workflow. This problem would be solved if I can just run TensorflowLite on single thread.\r\nAlso I am not sure if I am even using xnnpack. While the [documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md) says that xnnpack will run on single thread by default, I do not see any log messages about xnnpack being used during execution.\r\nAny suggestions on this?", "> @dev0x13\r\n> I have a hard requirement of forking. One parent process can fork one or many child processes to do work, and the destructor gets called once for each child process + once for the parent process. I cannot avoid this workflow. This problem would be solved if I can just run TensorflowLite on single thread.\r\n\r\nIn addition to setting the number of threads to \"1\" explicitly when creating the interpreter, could you try adding \"--define=tflite_with_ruy=true\" explicitly when building the library ? This flag will disable using Eigen library, which I'm not sure, whether it's the cause of the issue.\r\n\r\nBtw, is it possible to use std::unique_ptr instead of std::shared_ptr in your code? I just think that using std::unique_ptr makes the memory ownership of the TfLite interpreter object much more clear.\r\n\r\n> Also I am not sure if I am even using xnnpack. While the [documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md) says that xnnpack will run on single thread by default, I do not see any log messages about xnnpack being used during execution.\r\n> Any suggestions on this?\r\n\r\nIn production build (i.e. using -c opt), I think the message about whether the default xnnpack delegate is applied or not to the model has been suppressed. Using \"-c dbg\" could help debug the issue.\r\n\r\nAnother way to check whether the model could be delegated or not by xnnpack is to simply use the [benchmark model tool](https://www.tensorflow.org/lite/performance/measurement) ([pre-built nightly binaries](https://www.tensorflow.org/lite/performance/implementing_delegate#download_links_for_nightly_pre-built_tflite_tooling_binaries)). When invoking the binary, adding \"--use_xnnpack=true\" and the output should show sth. whether the model could be delegated or not like the following:\r\n\"\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nExplicitly applied XNNPACK delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.\r\n\"\r\n\r\n", "@dev0x13 @multiverse-tf \r\nThanks for your response. I was indeed not using xnnpack. But that was because I wanted to confine TFLite execution to single thread. Right now single threaded inference is being achieved by passing it as argument to InterpreterBuilder : \r\n```\r\ntflite::InterpreterBuilder(*flat_buffer_model, builtins)(&interpreter, 1);\r\n```\r\nI will try your approach of disabling eigen library and also work on my code to use unique_ptr.\r\nThanks.", "> **System information**\r\n> \r\n> * Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04, Android 10**\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Any Android smarthphone**\r\n> * TensorFlow installed from (source or binary): **Source**\r\n> * TensorFlow version (use command below): **2.3.0**\r\n> * Python version: **-**\r\n> * Bazel version (if compiling from source): **3.1.0**\r\n> * GCC/Compiler version (if compiling from source): GCC 5.4.0 / Clang shipped with Android NDK 21\r\n> * CUDA/cuDNN version: **-**\r\n> * GPU model and memory: **-**\r\n> \r\n> When TFLite is built with XNNPACK, performance improvement is expected. However, seems like the code provided in `/tensorflow/lite/examples/minimal` with a minor change of interpreter settings leads to performance degradation comparing to the default build.\r\n> \r\n> So here is the code taken from minimal example with my changes:\r\n> \r\n> ```c++\r\n>   ...\r\n> \r\n>   // Load model\r\n>   std::unique_ptr<tflite::FlatBufferModel> model =\r\n>       tflite::FlatBufferModel::BuildFromFile(filename);\r\n>   TFLITE_MINIMAL_CHECK(model != nullptr);\r\n> \r\n>   // Build the interpreter\r\n>   tflite::ops::builtin::BuiltinOpResolver resolver;\r\n>   InterpreterBuilder builder(*model, resolver);\r\n>   std::unique_ptr<Interpreter> interpreter;\r\n>   builder(&interpreter);\r\n>   TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n> \r\n>   // Allocate tensor buffers.\r\n>   TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n\r\nCurrently, if XNNPACK delegate is applied by default in TFLite, it will be applied at the beginning of AllocateTenosrs. Therefore, one has to call \"interpreter->SetNumThreads(8)\" before this function so that the number of threads used by XNNPACK delegate will be set accordingly.\r\n\r\n>   printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n>   tflite::PrintInterpreterState(interpreter.get());\r\n> \r\n>   // Set number of threads (added by me)\r\n>   interpreter->SetNumThreads(8);\r\n> \r\n>   // Run inference\r\n>   TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n>   printf(\"\\n\\n=== Post-invoke Interpreter State ===\\n\");\r\n>   tflite::PrintInterpreterState(interpreter.get());\r\n> \r\n>   ...\r\n> ```\r\n> \r\n> This code performs slower if executed with TFLite + XNNPACK build. I've tested it both on x64 desktop and arm64 Android using ResNet-34 FP32 TFLite model and observed the exact same performance degradation.\r\n> \r\n> I was able to fix the behavior and achieve 30% performance improvement only after I spent few hours with TFLite code and found out that **tflite::Interpreter::SetNumThreads is not applied to XNNPACK delegate (maybe to other delegates as well)**, because XNNPACK delegate is only initialized in `builder(&interpreter)` with number of threads passed to this invocation and then is not being updated on `interpreter->SetNumThreads(8)` call! In the case is illustrated by the code above), XNNPACK work in single-threaded mode or so. So the fix is to initialize interpreter as the following:\r\n> \r\n> ```c++\r\n>   builder(&interpreter, 8);\r\n> ```\r\n> \r\n> Then XNNPACK really introduces significant performance improvement.\r\n> \r\n> I'm OK with the solution that I found, but I was really confused with this issue and spent almost a day figuring out why can't I achieve claimed performance, because neither official documentation nor TFLite code commentary does not mention `InterpreterBuilder`'s `num_threads` argument as something necessary or mention it at least. Thus, following [\"Tweak the number of threads\"](https://www.tensorflow.org/lite/performance/best_practices#tweak_the_number_of_threads) documentation section in combination with using XNNPACK will lead anyone to this pitfall and will result with a very poor performance.\r\n> \r\n> If it's need I can provide more standalone example, point to the parts of TFLite code, which are responsible for this behavior, and detailed measurements obtained on different devices.\r\n\r\n", "@multiverse-tf Thank you for the clarification! However, I am concerned by [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/interpreter_builder.cc#L804) line. Yet XNNPACK delegate is being applied to the graph in AllocateTensors, it's still being created on the interpreter creation stage with no respect to SetNumThreads option. Taking into consideration the fact that XNNPACK delegate only initializes its thread pool once on the construction stage, the issue is still present in my perspective. Please correct me if I am wrong.", "> @multiverse-tf Thank you for the clarification! However, I am concerned by [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/interpreter_builder.cc#L804) line. Yet XNNPACK delegate is being applied to the graph in AllocateTensors, it's still being created on the interpreter creation stage with no respect to SetNumThreads option. Taking into consideration the fact that XNNPACK delegate only initializes its thread pool once on the construction stage, the issue is still present in my perspective. Please correct me if I am wrong.\r\n\r\nYes, you are right about this. I was confused w/ an earlier implementation of this feature where the number of threads was passed when creating the XNNPACK delegate.\r\n\r\nI think such a delicate situation is mainly caused by we trying to apply the XNNPACK delegate by default while honoring users' intention to explicitly use another TfLite delegate with C++ APIs. When it comes to using C APIs, I think this pitfall will be avoided as one has to provide the number of threads when creating the interpreter.\r\n", "@multiverse-tf @dev0x13 \r\n\r\nSo I get this error message on running my code: \r\n```\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n```\r\n\r\nBut I still see a speed-up in my inference time and the output does not change. \r\nDoes this mean that **some** operations from my model were not delegated and hence defaulted back to the original definition in TFLite?\r\nShould I be worried about this log message?\r\nAlso, is there a way to suppress this log message?\r\n\r\nThanks.", "> @multiverse-tf @dev0x13\r\n> \r\n> So I get this error message on running my code:\r\n> \r\n> ```\r\n> INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n> ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n> ```\r\n> \r\n> But I still see a speed-up in my inference time and the output does not change.\r\n> Does this mean that **some** operations from my model were not delegated and hence defaulted back to the original definition in TFLite?\r\n\r\nI think so. You could see what ops are delegated via our [benchmark tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) ([nightly pre-built binaries](https://www.tensorflow.org/lite/performance/implementing_delegate#download_links_for_nightly_pre-built_tflite_tooling_binaries)) with \"--print_preinvoke_state=true\" or \"--print_postinvoke_state=true\" cmdline flags. The output could be quite long, but look for sth. like the following:\r\n```\r\nNode  60 Operator Builtin Code   3 CONV_2D (delegated by node 66)\r\n  3 Input Tensors:[167,69,105] -> 0B (0.00MB)\r\n  1 Output Tensors:[168] -> 0B (0.00MB)\r\n  1 Temporary Tensors:[207] -> 0B (0.00MB)\r\n...\r\nNode  64 Operator Builtin Code  22 RESHAPE (delegated by node 66)\r\n  2 Input Tensors:[171,107] -> 0B (0.00MB)\r\n  1 Output Tensors:[172] -> 0B (0.00MB)\r\n...\r\nNode  66 Operator Custom Name TfLiteXNNPackDelegate \r\n  108 Input Tensors:[0-107] -> 14558508B (13.88MB)\r\n  1 Output Tensors:[173] -> 4004B (0.00MB)\r\n\r\nExecution plan as the list of 1 nodes invoked in-order: [66]\r\nAmong these nodes in the execution plan:\r\n  Node 66 is a TfLiteXNNPackDelegate node (0x152fbefabea0), which has delegated 66 nodes: [0-65]\r\n```\r\n\r\n> Should I be worried about this log message?\r\n\r\nNo, I don't think so.\r\n\r\n> Also, is there a way to suppress this log message?\r\n\r\nWe will always log this \" INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\" as it's generated [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc#L58).\r\nBut for the \"ERROR: Attempting to use...\" message, it's output via ReportError(...) ([code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/core/subgraph.cc#L1592-L1595)). By default, we use this [StderrReporter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/stderr_reporter.h)\r\n\r\n> \r\n> Thanks.\r\n\r\n", "Thanks a lot for this info.", "Have just posted a comment regarding this issue in https://github.com/tensorflow/tensorflow/issues/52076#issuecomment-928843253. In short, I think the issue in passing the correct \"num_threads\" to XNNPACK delegate creation should be fixed by commit https://github.com/tensorflow/tensorflow/commit/3d3c6db1ca2d50f6f07722cd800144f8f736167c.", "Note also that I recently committed a change to deprecate Interpreter::SetNumThreads in favour of InterpreterBuilder::SetNumThreads; see\r\nhttps://github.com/tensorflow/tensorflow/commit/44eb1aa889805b1c13c9488cea59fb1c86d431c3"]}, {"number": 42252, "title": "There is memory leak in Invoke() of gpu delegate (OpenCL) in Tensorflow lite v2.3.0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android armv8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): tag v2.3.0\r\n- Python version:\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):  android-ndk-r18b\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI use tflite with delegate creating by TfLiteGpuDelegateV2Create(&options), and call ModifyGraphWithDelegate and AllocateTensors, and when I call interpreter->Invoke() in a loop, I see the memory increasing continuously.\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nstd::unique_ptr<tflite::FlatBufferModel> model;\r\ntflite::ops::builtin::BuiltinOpResolver op_resolver;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\nTfLiteDelegate* delegate = nullptr;\r\n\r\n        struct : public tflite::ErrorReporter {\r\n            virtual int Report(const char *format, va_list args) {\r\n                return __android_log_vprint(ANDROID_LOG_INFO, \"native_jni\", format, args);\r\n            }\r\n        } er;\r\n        model = tflite::FlatBufferModel::BuildFromFile(\r\n                \"/storage/emulated/0/Android/data/com.example.myapplication2/files/model.tflite\",\r\n                &er);\r\n        if (!model) {\r\n            LOG(\"build model error\");\r\n            return;\r\n        } else {\r\n            LOG(\"build model success:%p\", model.get());\r\n        }\r\n        tflite::InterpreterBuilder(*model, op_resolver)(&interpreter);\r\n\r\n        TfLiteGpuDelegateOptionsV2 options = TfLiteGpuDelegateOptionsV2Default();\r\n        options.is_precision_loss_allowed = 1;\r\n        options.inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED;\r\n        options.inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY;\r\n        options.inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO;\r\n        options.inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO;\r\n        options.experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_CL_ONLY;\r\n        options.max_delegated_partitions = 1;\r\n        delegate = TfLiteGpuDelegateV2Create(&options);\r\n\r\n        if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {\r\n            __android_log_print(ANDROID_LOG_INFO, \"native_jni\", \"ModifyGraphWithDelegate error\");\r\n            return;\r\n        }\r\n        __android_log_print(ANDROID_LOG_INFO, \"native_jni\", \"ModifyGraphWithDelegate success\");\r\n        if (interpreter->AllocateTensors() == kTfLiteOk) {\r\n            __android_log_print(ANDROID_LOG_INFO, \"native_jni\", \"allocate success\");\r\n        } else {\r\n            __android_log_print(ANDROID_LOG_INFO, \"native_jni\", \"allocate error\");\r\n        }\r\n        while (true)\r\n            interpreter->Invoke();\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI suspect that the bug is something about using opencl. In  CLCommandQueue::DispatchImplicit (tensorflow/lite/delegates/gpu/cl/cl_command_queue.cc)\uff0cif I comment the code calling clEnqueueNDRangeKernel, the memory leak disappears.\r\n\r\n", "comments": ["@wanglei-source \r\n\r\nCan you add details of the mobile device that you're using?  We need more info such as manufacturer & model, e.g. Samsung Galaxy S12, and your GPU specs, e.g. Adreno 650.\r\n\r\nAlso, if you have more devices available at your hands, try running it on other phones to see whether it occurs on them too.", "@impjdi hi. i use tensorflow-lite on ios. I have memory leak on iphone 6, but not on iphone 7+"]}, {"number": 42242, "title": "tf.convert_to_tensor and tf.constant ignoring tf.device", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/device\r\n\r\n## Description of issue (what needs changing):\r\n\r\nAt least on 2.3.0, it seems to me that \r\n```\r\nimport numpy.random as npr\r\nimport tensorflow as tf\r\nwith tf.device(\"GPU\"):\r\n  A=tf.convert_to_tensor(npr.randn(500))\r\n```\r\nwill create an eager tensor `A` on the CPU device (it will not allocate ram on the gpu).  This is counter-intuitive to someone who has only read the doc as it is written.  My understanding is that this happens because tf.convert_to_tensor isn't an op, and tf.device only deals with ops.  \r\n\r\n### Clear description\r\n\r\nThe doc is pretty short now, and I don't think it would hurt to add a little remark, something like this:\r\n\r\n*Note* -- `tf.convert_to_tensor` does not create an op.  As such, it ignores the contexts created by tf.device.  To ensure a given tensor is assigned memory on a particular device, one can wrap convert_to_tensor inside a `tf.identity` op.\r\n\r\nThoughts?", "comments": ["@jacksonloper \r\n\r\nCan you please share a simple standalone code to reproduce the issue? Thanks!", "I don't think there's really an issue with the code per se, it's a doc issue.  But how about this:\r\n\r\n- Given current docs, a novice user might well expect: A.device (from post, above) will be GPU\r\n- Actually: A.device will be CPU\r\n- Solution: make it clear in the docs why this is the case\r\n\r\nDoes that help?  I also added the numpy.random import to the codeblock in the original post above in case that was causing any confusion.  Maybe it would be cleaner to use this as the example:\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nwith tf.device(\"GPU\"):\r\n  A=tf.convert_to_tensor(np.zeros(500))\r\n\r\n...which yields the same behavior (A.device is going to be CPU)", "@jacksonloper Currently there is some description [here](https://www.tensorflow.org/guide/gpu). You can set the device placement for any op. You can also check some useful info [here](https://www.tensorflow.org/guide/variable).\r\n\r\nSetting `set_log_device_placement` will place the ops on the specified device.\r\n```\r\nimport tensorflow as tf\r\ntf.debugging.set_log_device_placement(True)\r\nimport numpy.random as npr\r\nwith tf.device('/GPU:0'):\r\n  A=tf.convert_to_tensor(npr.randn(500))\r\n  B=tf.convert_to_tensor(npr.randn(500))\r\n\r\nC = tf.add(A, B)\r\nprint(C)\r\n```\r\n\r\nIf you set [`set_soft_device_placement`](https://www.tensorflow.org/api_docs/python/tf/config/set_soft_device_placement) along with `set_log_device_placement`, then if the specified device is not present, then the code silently place them in the available device. Thanks!\r\n\r\n> If enabled, an op will be placed on CPU if any of the following are true\r\n> \r\n> there's no GPU implementation for the OP\r\n> no GPU devices are known or registered\r\n> need to co-locate with reftype input(s) which are from CPU\r\n\r\nIf you plan to update any document, then please feel free to create a PR for updating the doc. Thanks!", "I'm not exactly sure why you're referring me to those docs -- I'm quite familiar with the docs you mention.  I'm well-aware that running tf.Variable or tf.constant inside a tf.device(\"GPU\") block will allocate it on the GPU.  The thing is that **running tf.convert_to_tensor inside a tf.device(\"GPU\") block does not allocate ram on the GPU**.  So, in the example you have,\r\n```import tensorflow as tf\r\ntf.debugging.set_log_device_placement(True)\r\nimport numpy.random as npr\r\nwith tf.device('/GPU:0'):\r\n  A=tf.convert_to_tensor(npr.randn(500))\r\n  B=tf.convert_to_tensor(npr.randn(500))\r\n  C = tf.add(A, B)\r\n```\r\nA and B will be on the CPU, though C will be on the GPU.  And that is surprising to the uninitiated. \r\n\r\nSo the proposal is to clarify this with a remark on the tf.device page.  \r\n\r\nDoes that make sense?  Or am I missing something here?  Happy to put in a PR, just want to make sure this is crystal clear.", "Yes. I see your point.\r\n\r\nEven with soft device placement turned off a constant in a `with tf.device(\"GPU\"):` still reports it's device as CPU.\r\n\r\nWhatever is happening here it's a little more subtle than \"`convert_to_tensor` does not create an op.\" since it creates `tf.constant` op.\r\n\r\nIf you capture the tf.Graph you see the \"const\" op with it's device set to GPU:\r\n\r\n```\r\nimport numpy.random as npr\r\nimport tensorflow as tf\r\ntf.config.set_soft_device_placement(False)\r\ntf.debugging.set_log_device_placement(True)\r\nwith tf.Graph().as_default() as g:\r\n  with tf.device(\"GPU\"):\r\n    Ag=tf.convert_to_tensor(npr.randn(2)) \r\n    Bg = Ag + 1\r\nprint(g.as_graph_def())\r\nprint()\r\n```\r\n\r\n```\r\nnode {\r\n  name: \"Const\"\r\n  op: \"Const\"\r\n  device: \"/device:GPU:*\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_DOUBLE\r\n    }\r\n  }\r\n...\r\n```\r\n\r\nAs you said identity does copy the tensor to the device:\r\n```\r\nwith tf.device(\"GPU\"):\r\n  A=tf.convert_to_tensor(npr.randn(2))\r\n  B = tf.identity(A)\r\n\r\nprint(A.device)\r\nprint(B.device)\r\nprint()\r\n```\r\n\r\n```\r\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\n```\r\n\r\nSo I do agree that this is surprising.", "Lol so much for me being \"well-aware\" that tf.constant respects tf.device.  I didn't even check, like a fool!\r\n\r\nWell that is kind of interesting.  I guess it's a bug then, not a doc issue?", "+@jaingaurav\r\n\r\nThis looks like a bug to me. Gaurav do you know more about what's going on here? Can you help triage?\r\n\r\n", "So is the concern that `tf.convert_to_tensor` doesn't create a GPU tensor even with soft placement turned off? I think this was intentional right?\r\n\r\n@ccrusius: Could you confirm that all the behaviors above are as intended? Perhaps we just need to clarify the docs a bit further.", "Yes, exactly. \r\n\r\nIt's fine if it's intentional, but we couldn't find any doc describing this behavior. It would be good to know for sure:\r\n\r\n1. That this is intentional.\r\n2. Why.", "Yes, the behaviors are as intended. The documentation for `tf.constant` mentions it always creates host tensors, and that `tf.identity` will create tensors in the specified devices.\r\n\r\nI agree the behavior can be confusing, and we need better documentation here. In particular, `convert_to_tensor` makes no mention of this behavior.", "Hi,\r\n\r\nI have gone through this thread and, somehow, I do not fully understand the rationale behind the design approach of always creating the tensor into host memory, when invoking `tf.convert_to_tensor()`.\r\n\r\nIf the remaining ops are being executed in device memory, why do not allocate the Tensor in device memory when a GPU is available.\r\n\r\nI faced this behaviour when creating a snippet code to showcase how to interact TensorFlow tensors with JAX and CuPy libraries. None of them accept DLPack objects hosted in host memory.\r\n\r\nThe following code fails:\r\n> src = tf.convert_to_tensor([[1, 2], [3, 4]])\r\n> dst = cp.fromDlpack(tf.experimental.dlpack.to_dlpack(src)) \r\n\r\nCould you please reconsider allocating those tensors in host/device memory depending if a GPU device is present?\r\n\r\nThanks!\r\n\r\nRegards,\r\nMiguel"]}, {"number": 42234, "title": "ValueError when using dictionary format for input of optimizer options in Tensorflow 2.2.0 Keras models", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (but very basic)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): I used pip\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: More than one machine\r\n\r\n**Describe the current behavior**\r\nI am using the dictionary format (see code below) to input options to an optimizer when compiling a Tensorflow Keras model. However, in Tensorflow 2.2.0 this throws an error. I fixed this issue for now by switching back to Tensorflow 2.1.0. Another workaround is to define the loss beforehand as tf.keras.losses.Nadam(**config). Neither of those is a final solution so I thought it best to report this bug. \r\n\r\n**Describe the expected behavior**\r\nThe dictionary format should work for easy input of non-default options of optimizers when compiling Keras models with Tensorflow 2.2.0.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python script\r\nimport tensorflow as tf\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(3, activation='sigmoid'))\r\ncompiler_params = {\r\n    \"optimizer\": {\r\n        'class_name': 'Nadam',\r\n        'config': {\r\n            'lr': 0.0001\r\n        }\r\n    }\r\n}\r\nmodel.compile(**compiler_params)\r\n```\r\n\r\n**Other info / logs** \r\nInclude any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```python script\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-187c8cc52bd6> in <module>\r\n     11     }\r\n     12 }\r\n---> 13 model.compile(**compiler_params)\r\n     14\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cinc3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, **kwargs)\r\n    326       self._run_eagerly = kwargs.pop('run_eagerly', None)\r\n    327\r\n--> 328       self.optimizer = self._get_optimizer(optimizer)\r\n    329       self.compiled_loss = compile_utils.LossesContainer(\r\n    330           loss, loss_weights, output_names=self.output_names)\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cinc3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _get_optimizer(self, optimizer)\r\n    348       return opt\r\n    349\r\n--> 350     return nest.map_structure(_get_single_optimizer, optimizer)\r\n    351\r\n    352   @trackable.no_automatic_dependency_tracking\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cinc3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py in map_structure(func, *structure, **kwargs)\r\n    615\r\n    616   return pack_sequence_as(\r\n--> 617       structure[0], [func(*x) for x in entries],\r\n    618       expand_composites=expand_composites)\r\n    619\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cinc3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py in <listcomp>(.0)\r\n    615\r\n    616   return pack_sequence_as(\r\n--> 617       structure[0], [func(*x) for x in entries],\r\n    618       expand_composites=expand_composites)\r\n    619\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cinc3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _get_single_optimizer(opt)\r\n    342\r\n    343     def _get_single_optimizer(opt):\r\n--> 344       opt = optimizers.get(opt)\r\n    345       if (self._dtype_policy.loss_scale is not None and\r\n    346           not isinstance(opt, lso.LossScaleOptimizer)):\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\cinc3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py in get(identifier)\r\n    900     return deserialize(config)\r\n    901   else:\r\n--> 902     raise ValueError('Could not interpret optimizer identifier:', identifier)\r\n\r\nValueError: ('Could not interpret optimizer identifier:', 0.0001)\r\n```", "comments": ["@hallabjorg5 \r\nIs there any particular reason for using a dictionary, I have tried the code with out dictionary and it works with out error.\r\nCan you please refer to [this link](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) and update, nowhere has a dict been used.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Apart from backwards compatibility reasons, it has been a very nice feature to easily feed parameters from JSON config files.", "Was able to reproduce the issue in TF v2.7.0 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/fbe069571ded5bcf915f5f766cf6c71b/untitled71.ipynb)..Thanks !", "Hi @hallabjorg5 ! According to this [document](https://keras.io/api/models/model_training_apis/) ,You can pass only a string value for optimizer  and dictionary/list for other parameters in model.compile.  \r\n\r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues) for further assistance.\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "Is it possible that you create an optimizer object and provide it to `Model.compile()` instead of a dict? "]}, {"number": 42215, "title": "Testing Operator \"raw_ops.ApplyAdam\" ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Conda install\r\n- TensorFlow version (use command below): r2.2\r\n- Python version:  python3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: Teslav100 / 32G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI'd like to test raw_ops.ApplyAdam in TensorFlow r2.2, yet  \r\n```c++\r\n\"TypeError: 'ApplyAdam' Op requires that input 'var' be a mutable tensor (e.g.: a tf.Variable) \" \r\n```\r\nraises even if I define var as a variable like \r\n```c++\r\n\"var = tf.Variable([[1.], [2.]])\"\r\n``` \r\nSo what is exactly a mutable tensor looks like? \r\n\r\n**Describe the expected behavior**\r\nI expected this operator could work correctly?\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\nsess= tf.compat.v1.Session(config = config)\r\n\r\nvar = tf.Variable([[1.], [2.]]) \r\nm = tf.Variable([[1.], [2.]]) \r\nv = tf.Variable([[1.], [2.]]) \r\nbeta1_power = tf.Variable(0.5)\r\nbeta2_power = tf.Variable(0.5)\r\nlr = tf.Variable(0.5)\r\nbeta1 = tf.Variable(0.5)\r\nbeta2 = tf.Variable(0.5)\r\nepsilon = tf.Variable(0.5)\r\ngrad =tf.Variable([[1.], [2.]]) \r\nuse_locking = False\r\nuse_nesterov = False\r\n\r\nsess.run(tf.compat.v1.global_variables_initializer())\r\n\r\nsess.run(tf.compat.v1.raw_ops.ApplyAdam(var=var, m=m, v=v, beta1_power=beta1_power, beta2_power=beta2_power, lr= lr, beta1 = beta1, beta2 = beta2, epsilon = epsilon, grad =grad,\r\n    use_locking=False, use_nesterov=False, name=None))\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/ec320daf62374d76962c67be9b5ebafe/42215.ipynb). Thanks!", "Was able to reproduce the issue in TF v2.7.0,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/8bbc388a5feaf3f19ee7c54ed5fd9e45/untitled72.ipynb)..Thanks !"]}, {"number": 42207, "title": "Reduction is average step time when disabling certain HLO passes", "body": "**System information**\r\n- Model: ResNet50 with Cifar-10\r\n- OS Platform and Distribution : Linux Ubuntu 18.04\r\n- TensorFlow: installed from source\r\n- TensorFlow version : v2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version : 2.0.0\r\n- GCC/Compiler version : 7.5.0\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: GeForce RTX 2080 / 7982MiB\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nHi all,\r\n\r\nI am currently trying to find way if a model execution time can be reduced further . For instance I saw 5 ms reduction in average step time (Tensorboard Profile Summary) at times when I disabled certain HLO passes like  gpu-conv-padding-legalization, transpose-folding while running ResNet50 with Cifar 10 . Now I understand that entire set of 51 HLO passes(Target Dependent and Target Independent) are there for a reason. But considering the reduction in step time , is this normal behavior if not can some of these passes be avoided depending on the model we execute ? \r\n\r\nI tried disabling some HLO passes \r\n- convolution_4d_expander\r\n- gpu-conv-padding-legalization\r\n- multi_output_fusion\r\n- reduction-degenerate-dim-remover\r\n- reduction-dimension-grouper\r\n- transpose-folding\r\n\r\n\r\n** plots with some results obtained **\r\nhttps://github.com/mmadala95/xla_analysis/tree/master/results\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nprobably execution time of model might not improve when disabling certain HLO passes (which might not be useful to a model).  \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/mmadala95/xla_analysis/blob/master/ResNet50_xla.py\r\n\r\n\r\n\r\n", "comments": ["> **System information**\r\n> \r\n> * Model: ResNet50 with Cifar-10\r\n> * OS Platform and Distribution : Linux Ubuntu 18.04\r\n> * TensorFlow: installed from source\r\n> * TensorFlow version : v2.2.0\r\n> * Python version: 3.6.9\r\n> * Bazel version : 2.0.0\r\n> * GCC/Compiler version : 7.5.0\r\n> * CUDA/cuDNN version: 10.1/7.6\r\n> * GPU model and memory: GeForce RTX 2080 / 7982MiB\r\n> \r\n> **Describe the current behavior**\r\n> \r\n> Hi all,\r\n> \r\n> I am currently trying to find way if a model execution time can be reduced further . For instance I saw 5 ms reduction in average step time (Tensorboard Profile Summary) at times when I disabled certain HLO passes like gpu-conv-padding-legalization, transpose-folding while running ResNet50 with Cifar 10 . Now I understand that entire set of 51 HLO passes(Target Dependent and Target Independent) are there for a reason. But considering the reduction in step time , is this normal behavior if not can some of these passes be avoided depending on the model we execute ?\r\n> \r\n> I tried disabling some HLO passes\r\n> \r\n> * convolution_4d_expander\r\n> * gpu-conv-padding-legalization\r\n> * multi_output_fusion\r\n> * reduction-degenerate-dim-remover\r\n> * reduction-dimension-grouper\r\n> * transpose-folding\r\n> \r\n> ** plots with some results obtained **\r\n> https://github.com/mmadala95/xla_analysis/tree/master/results\r\n> \r\n> **Describe the expected behavior**\r\n> \r\n> Ideally as per , execution time of model must not improve when disabling certain HLO passes which might not be useful to a model.\r\n> \r\n> **Standalone code to reproduce the issue**\r\n> https://github.com/mmadala95/xla_analysis/blob/master/ResNet50_xla.py\r\n\r\nAdding Sanjoy as per his suggestion\r\n\r\n@sanjoy ", "@mmadala95 \r\n\r\nI tried reproducing the issue in colab with TF version 2.2 and i am seeing the below error message.\r\n`KeyError: 'XLA_Current_Disabled'`\r\n\r\nPlease, find the gist [here](https://colab.research.google.com/gist/ravikyram/5d4393a5d52662370b3a7179793a7097/untitled236.ipynb).Thanks!", "@ravikyram \r\nsorry for that, please ignore that environment variable that was custom set to modify logs folder names.\r\n\r\nreplace line 167 with \r\n\r\nlogs = [logs directory path] + datetime.now().strftime(\"%Y%m%d\")\r\n\r\nbefore each run of this model , I used to set this flag with HLO passes I need to disable \r\nfor example:  export XLA_FLAGS=\"--xla_disable_hlo_passes=convolution_4d_expander,multi_output_fusion\"\r\n\r\nand obtained results by tensorboard profiling on logs generated in above path . ", "Tim, does this mean we need to adjust some of our heuristics in the passes listed?\r\n\r\nCC @thomasjoerg @akuegel ", "At least regarding convolution_4d_expander, I doubt that it has an impact on resnet average step time.\r\nAs far as I know, resnet doesn't have 4 spatial dimensions in its convolutions, so this pass will not trigger. Given that you seem to have measured some timing difference, this is most likely noise (or you are also measuring compile time, not only runtime). If you measured compile time, there is not much to optimize there: we just iterate once over all HLO instructions to see which ones do match (I expect none), and only on match do some replacements."]}]