[{"number": 17395, "title": "add AdaMax optimizer", "body": "Fix #17104. The PR implements the AdaMax optimizer: https://arxiv.org/pdf/1412.6980v8.pdf\r\n\r\nPlease pay attention to three points when reviewing:\r\n1. To avoid division by zero, we add `epsilon` for `v_t`, which is slightly different from original paper section 7.1. And we explained the change in the document.\r\n2. Contrast to the sparse implementation of [`tf.train.AdamOptimizer`](https://www.tensorflow.org/versions/master/api_docs/python/tf/train/AdamOptimizer), the AdaMaxOptimizer only updates variable slices and corresponding `m_t`, `v_t` terms when that part of the variable was used in the forward pass.\r\n3. Because AdaMax is quite similar with Adam, we refactor the Adam code in c++ side to reuse its route. Not sure whether the solution sounds good.\r\n\r\nThe PR is still very rudimentary implementation, any feedback/review would be appreciated.", "comments": ["Can you please state the reason for point 2? @facaiy ", "@dchatterjee172 It depends upon how we think the missing parts in sparse gradient. I prefer to regard them as missing gradient (NaN value), rather than zero gradient. Moreover, there is less overhead as compared to update the whole.", "@facaiy yes you got a point ", "@asimshankar Would you mind taking a look? Thanks.", "@frankchn Thanks. I have removed the unused import which cannot pass sanity check. Could you help restart all tests?", "APIBackwardsCompatibility failed:  'Check if tf_export decorator/call is missing for this symbol.')\r\n\r\nDoes it mean that we need use `tf.export` to annotate class `AdaMaxOptimizer`?\r\n\r\n", "I won't have the cycles to do a thorough review of this anytime soon. @alextp are you able to take a look?", "@alextp Hi, I have answered most of your comments. Sorry for the delay. ", "@alextp Hi, I have removed `use_nesterov` argument and SYCL codes. I think all your comments has been resolved, could you take a look?", "AdaMax Optimizer is under contrib module,  I'm a little confusing about why we need  use `tf.export`?\r\n\r\n```python\r\nFAIL: testNewAPIBackwardsCompatibility (__main__.ApiCompatibilityTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\", line 281, in testNewAPIBackwardsCompatibility\r\n    'Check if tf_export decorator/call is missing for this symbol.')\r\n  File \"/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\", line 196, in _AssertProtoDictEquals\r\n    self.fail('%d differences found between API and golden.' % diff_count)\r\nAssertionError: 1 differences found between API and golden.\r\n```", "@alextp Thanks for you approval. Could you give some suggestions about `tf.export` error?", "@annarev who understands these backward compatibility tests (I don't)", "@annarev Could you take a look?", "@alextp Hi, could you help me restart all tests? Thanks.", "Sorry, I somehow missed all the pings in this PR. You are right, symbols in contrib should not need a tf_export decorator.\r\nAre you still seeing the error? If yes, I can take a look.", "I looked through the changes and I think I know what the issue is.\r\nAll ops registered in core are exported by default in core TensorFlow. For e.g. ApplyAdaMax would be available as tf.apply_ada_max. This is causing a diff in api_compatibility_test.\r\nIf you don't want to make these ops available in core, then you can set visibility: HIDDEN in the api_def_*.pbtxt files.\r\n\r\nYou can add \"visibility: HIDDEN\" after graph_op_name, for e.g.:\r\nop {\r\n  graph_op_name: \"ApplyAdaMax\"\r\n  visibility: HIDDEN\r\n  ...", "@annarev many thanks! Yes, I'd like to hide `ApplyAdaMax` at first. By the way, where  can I find the document about api_def_*.pbtxt?\r\n\r\nIf we hide the op, so we should call it by `training_ops._apply_ada_max` on python side, right?", "@jhseu Hi, I have upgraded the codes with `eager.context`, and I also hide the `ApplyAdaMax` op. So could you help me restart all tests again? Thanks very much.", "seems irrelevant Timeout?\r\n\r\n```bash\r\n//tensorflow/core:common_runtime_ring_reducer_test                      TIMEOUT in 452.1s\r\n\r\n  /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/testlogs/tensorflow/core/common_runtime_ring_reducer_test/test.log\r\n```", "Yep, that timeout is unrelated", "Thank all for your help! Many thanks!"]}, {"number": 17394, "title": "add support for other cpu type(aarch64)  for tensorRT", "body": "add support for other cpu for tensorRT\r\n\r\nIn Linux 64bit is /usr/lib/x86_64-linux-gnu\r\nIn Nvidia Jetson is 'aarch64.' /usr/lib/aarch64-linux-gnu", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17393, "title": "import tensorflow failed, \"ImportError: DLL load failed\". Even after install visual studio 2015, Microsoft Visual C++ 2015 Redistributable Update 3.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win10 x64\r\n- **TensorFlow installed from (source or binary)**:  pip install --upgrade tensorflow\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI was trying to install tensorflow **cpu version** on windows10, but always got error when import tensorflow.\r\n\r\nI read the [common_installation_problems](https://www.tensorflow.org/install/install_windows#common_installation_problems) , tried many solution I found on github, stackoverflow, etc. I install visual studio 2015, visual studio 2017, [Microsoft Visual C++ 2015 Redistributable Update 3](https://www.microsoft.com/en-us/download/details.aspx?id=53587)(both 32 and 64),  msvcp140.dll can find in both System32 and SysWow64 folder. But still can't import tensorflow.\r\n\r\nIs there something I missed out?\r\n\r\n\r\n### Source code / logs\r\n>import tensorflow error info:\r\n\r\n```\r\nPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: \u52a8\u6001\u94fe\u63a5\u5e93(DLL)\u521d\u59cb\u5316\u4f8b\u7a0b\u5931\u8d25\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: \u52a8\u6001\u94fe\u63a5\u5e93(DLL)\u521d\u59cb\u5316\u4f8b\u7a0b\u5931\u8d25\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n\r\n>run tensorflow_self_check.py result:\r\n\r\n```\r\nERROR: Failed to import the TensorFlow module.\r\n\r\n- Python version is 3.6.\r\n\r\n- TensorFlow is installed at: C:\\Users\\sss\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\r\n\r\n- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Download and install CUDA 8.0 from\r\n  this URL: https://developer.nvidia.com/cuda-toolkit\r\n\r\n- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that\r\n  this DLL be installed in a directory that is named in your %PATH%\r\n  environment variable. Typically it is installed in 'C:\\Windows\\System32'.\r\n  If it is not present, ensure that you have a CUDA-capable GPU with the\r\n  correct driver installed.\r\n\r\n- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Note that installing cuDNN is a\r\n  separate step from installing CUDA, and it is often found in a\r\n  different directory from the CUDA DLLs. You may install the\r\n  necessary DLL by downloading cuDNN 5.1 from this URL:\r\n  https://developer.nvidia.com/cudnn\r\n\r\n- Could not find cuDNN.\r\n```\r\n", "comments": ["I'm also having the same issue. ANybody with a fix?", "Seems tensorflow 1.6 is the issue,\r\nI installed 1.5 and it works just fine\r\ntry \r\npip install tensorflow==1.5", "Closing, duplicate of #17386 ", "@thuojb, thanks. it fixed my problem ", "@thuojb thanks mate. This solution still works till this day running tensorflow 1.8. Life saver", "@thuojb Thanks man, it still works.\r\n\r\nBut can you tell me how can we switch to tensorflow 1.8 without any errors while running", "@thuojb It really works! Thank you~", "Have in mind that downgrading to tensorflow 1.5 might not be the best solution because for instance, you won't have access to some of the new features of tensorflow like the 'eager execution' feature for example.\r\n\r\nWhat i did in my case was to install tensorflow 1.8 with conda instead of pip. From my little experience with conda and pip, conda seems to perform better when it comes to placing required files in their proper location.\r\n\r\nIn summary, Instead of `pip3 install --upgrade tensorflow`, i used `conda install tensorflow `which will install the latest version properly.\r\n\r\nIn case you don't have Anaconda installed for conda commands. Download from [https://www.continuum.io/downloads](url)", "@Abdulr-intija conda install tensorflow worked for me in an identical situation.  Many thanks for ending my struggle!", "@thuojb,  thanks,  yes it works with version 1.5.0 \r\n@Abdulr-intija **conda install tensorflow** worked for me too.  able to install 1.8.0 successfully. thanks ", "i am getting the same issue with tensorflow 1.8.0 on windows 10.\r\ncan anybody help me with this issue?\r\nas mentioned @thuojb i degraded the version and then its working but i want to work on tensorflow 1.8.0 ", "Same issue here. It worked with version 1.5.\r\n\r\nThis problem could occur when the CPU lacks AVX instruction. [Using a wheel built without AVX also fix problem](https://github.com/tensorflow/tensorflow/issues/17386#issuecomment-370129452). \r\n", "thuojb, thank you so much! I am so happy I got it working.", "i have same problem, when i import show error like this\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nc:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nc:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nc:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nc:\\python\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\nc:\\python\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\nc:\\python\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     20 \r\n     21 # pylint: disable=g-bad-import-order\r\n---> 22 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     23 \r\n     24 try:\r\n\r\nc:\\python\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nc:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\python\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\python\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nmy sistem are:\r\n- windows 7 64 bit\r\n- python 3.5.2rc1\r\n- intel core 2 due e8400\r\n\r\n", "i got this message with tenserflow 1.10.0,\r\nsolution ?", "@Abdulr-intija   thank you,i finaly resolve it .thank you", "For reference, using ProcessMonitor is a good way to determine which DLL can't be loaded by tensorflow. Documented here: http://shitalshah.com/p/debugging-tensorflow-dll-importerror/  ", "Win 10 64bit. python 3.5.2, CUDA 8.0, cuDNN v5.1 still having DLL load failed on both tensorflow-gpu 1.10 and 1.12. Other setups tested including python 3.6.7, CUDA 9.0/10.0, cuDNN 7.4.1 for 10.0, 7.4.1/7.3.1 for 9.0.  Confirmed msvcp140.dll, added cuda/bin, include, lib paths, made a copy for both cudnn64_5.dll and  cudnn64_6.dll, reinstalled python with Tcl/Tk, etc. Nothing worked so far.\r\n\r\nI even tried ProcessMonitor @testlump posted above, and last two dlls were \"C:\\Windows\\System32\\nvcuda.dll\" and \"C:\\Windows\\System32\\setupapi.dll\" However, it didn't fail. Does anyone have a clue?", "You might find it's best to recompile from source. This resolved the issue for me (see below for more information).\r\n\r\nHere are some things I found compiling on Windows 10 with GPU support: \r\n\r\n- Check your CPU instruction set supports AVX with [CPU-Z](https://www.cpuid.com/softwares/cpu-z.html) or an equivalent tool. If this isn't present (E.g. you're running an older CPU), you'll likely need to recompile tensorflow from source if you want it to work (this will likely apply irrespective of the OS). \r\n\r\n- Check you've got the correct CUDA libraries installed. The release Python package version of tensorflow (1.12) is built against `CUDA 9.0` (I used package: `cuda_9.0.176_win10.exe`) and CuDNN 7.0 (I used package: `cudnn-9.0-windows10-x64-v7.4.1.5.zip`). If you have multiple CUDA versions installed side-by-side (v7.0, v9.0, v10.0) etc which you are not using, uninstall them. \r\n\r\n- Make sure your CUDA *System* environment variables are configured (E.g. for v9.0): \r\n                - Variable: `CUDA_PATH` Value: `C:\\Program Files\\NVIDIA CPU Computing Toolkit\\CUDA\\v9.0`\r\n                - Variable: `CUDA_PATH_V9_0` Value: `C:\\Program Files\\NVIDIA CPU Computing Toolkit\\CUDA\\v9.0`\r\n \r\n- The CuDNN libraries should be unzipped to the CUDA install path, on Windows by default this is `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0 for version 9.0 `\r\n\r\n**Building from source**\r\n\r\n- Install `BAZEL` and `MSYS2` to their default locations (recommended). E.g. `C:\\MSYS2\\` and `C:\\Bazel\\`\r\n\r\n- If you want to build the Python package from source follow these instructions: [https://www.tensorflow.org/install/source_windows](https://www.tensorflow.org/install/source_windows). Ensure you configure your `BAZEL` and `MSYS2` installations according to their respective web-pages (linked from tensorflow), pay particular attention to environment variables. If you've installed CUDA, CuDNN you can skip the section **Install GPU support (optional)**\r\n\r\n- Clone or download the source as per the instructions and expand (E.g. `C:\\temp\\tensorflow-master\\`)\r\n\r\n- Edit the source file `{sourcegit for tensorflow path}\\third_party\\gpus\\cuda_configure.bzl` and change the line `[\"grep\", \"--color=never\", \"-A1\", \"-E\", define,` to: `[\"C:/msys64/usr/bin/grep\", \"--color=never\", \"-A1\", \"-E\", define,`  (RESPECT THE INDENTATION!)\r\n\r\n- If you have a username with a space in it - for example `Peter Blakey` so you users folder is `C:\\users\\Peter Blakey\\`, create a symlink from `Peter` in `C:\\Users\\` - E.g. `C:\\users\\Peter -> C:\\users\\Peter Blakey\\` - The white-space causes path resolution problems during the build. E.g. from a command prompt in `C:\\users\\` type `mklink /d Peter \"C:\\Users\\Peter Blakey\"` - you should now be able to `cd` in `c:\\users\\Peter\\` and see the contents of `c:\\users\\Peter Blakey`\r\n\r\n- If you have a username with a space in it, edit your *User* environment variables to path to `Python.exe` at the symlinked directory. E.g. Edit `c:\\users\\Peter Blakey\\AppData\\Local\\Programs\\Python\\Python36\\` to `C:\\users\\Peter\\AppData\\Local\\Programs\\Python\\Python36\\` and  `c:\\users\\Peter Blakey\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\` to `c:\\users\\Peter\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\`\r\n\r\n- Create a empty folder for the build - E.g. `C:\\temp\\buildcache` - make sure you have at least 20GB free.\r\n\r\n\r\n- When you execute the prebuild step `python configure.py` from the location of the source you cloned from github: \r\n      - Check the python path or override *No whitespace should be in the pathname*.\r\n      - Check the python packages path or override *No whitespace should be in the pathname*.\r\n      - Choose N for XLA JIT support \r\n      - Choose N for ROCm support\r\n      - Choose Y for CUDA support\r\n      - Enter the version of CUDA you want to use or accept the default (9.0)\r\n      - Check the CUDA library path exists or if installed to another location, override.\r\n      - Enter the version of cuDNN you want to use or accept the default (7)\r\n      - Check the cuDNN path exists or if installed to another location, override\r\n      - When prompted for the compute capability you want to enable for your GPU, visit [https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus) and enter the correct value. E.g. for a `Geforce GTX 970` it was 5.2 \r\n       - Hit enter to accept the default optimization flags \r\n       - Choose `Yes` when asked to override eigen strong inline (Speeds up compilation time). \r\n\r\n**Build Step**\r\n\r\n- Your compilation command should be something like this: `bazel --output_user_root=C:/temp/buildcache build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n** or ** \r\n\r\n- If your CPU *does not* support AVX instruction set, when you build tensorflow omit the `--config=opt ` flag so you build with `bazel --output_user_root=C:/temp/buildcache build --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nOnce it's built (takes several hours), follow the instructions to build the pip package (E.g. execute `bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_pkg` and then install the resulting Python wheel package to your environment).\r\n\r\nFinally, test the installed tensorflow with: \r\n\r\n`python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"`\r\n\r\n**Other notes** \r\n\r\n- If the build fails, make sure you run `bazel clean` and delete the contents of the buildcache folder (E.g. `c:\\temp\\buildcache`) before commencing a new build\r\n\r\n- You can build against the latest version of CUDA (v10.0) and cuDNN (v7) if you want. You may wish to try building against the supported version first, once you've verified that you can uninstall CUDA / CuDNN, install the latest versions and rebuild tensorflow. ", "@testlump I appreciate the detailed help. I actually attempted to compile from source first because the original pip install didn't work. Then, I soon realized fixing pip install could be easier than setting up the environment to build from source. Now it's circling back to source compiling again... I didn't want to run this on Ubuntu because the package dependency is pretty messed up there already and it keeps crashing, but perhaps I should try on Ubuntu. I will try source compiling as the last resort if that doesn't work either. Thanks for the help anyways...", "Facing the same issue on my machine after some trials found a solution which worked for me.\r\nSteps to solve the issue:\r\n1. Re-Install Cuda(Till Date Tensorflow only supports Cuda V9.0 so install Cuda9.0 only)\r\n2. Add Cuda library, bin paths to system variables\r\n3. Add Cudnn(Any Version is Ohkey) library to Cuda9.0\r\n4. Restart PC\r\n5. Install \"pip install tensorflow-gpu==1.8\"(I tried with higher version also but not supported in my case but 1.8 easily got installed)\r\n\r\n", "--------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-10-491f9b70df53> in <module>\r\n      1 import pandas as pd\r\n----> 2 from keras.models import Sequential\r\n      3 from keras.layers import Dense\r\n      4 import numpy as np\r\n      5 from sklearn.model_selection import train_test_split\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py in <module>\r\n      1 from __future__ import absolute_import\r\n      2 \r\n----> 3 from . import utils\r\n      4 from . import activations\r\n      5 from . import applications\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py in <module>\r\n      4 from . import data_utils\r\n      5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n      7 \r\n      8 # Globally-importable utils.\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py in <module>\r\n      7 from six.moves import range\r\n      8 import numpy as np\r\n----> 9 from .. import backend as K\r\n     10 \r\n     11 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py in <module>\r\n     87 elif _BACKEND == 'tensorflow':\r\n     88     sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 89     from .tensorflow_backend import *\r\n     90 else:\r\n     91     # Try and load external backend.\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in <module>\r\n      3 from __future__ import print_function\r\n      4 \r\n----> 5 import tensorflow as tf\r\n      6 from tensorflow.python.framework import ops as tf_ops\r\n      7 from tensorflow.python.training import moving_averages\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\ndataframe = pd.read_csv(\"dia", "The problem is that I think there is an imcompatibility issue.\r\nWhen installing tensorflow-gpu directly, did not work. \r\nI solved the problem as. \r\n\r\nFirst, I created the conda environment as following:\r\n**_conda create -n myenv  python=3.6\r\nconda activate myenv_** \r\nThen I installed the keras-gpu=2.2.2 \r\n**_conda install keras-gpu=2.2.2_** #this command also installed the tensorflow  ", "Open ...\\Python37\\Lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd use [Dependency Walker](http://www.dependencywalker.com/), it will show you the DLL dependency tree, you will find which DLL cause the problem. TensorFlow always linked to the specific CUDA version.", "I fixed it.\r\nSpend days searching to fix them.\r\n\r\nInstalled tensorflow with pip install tensorflow-cpu\r\nUpdated visual c++ 2015-2019\r\n\r\nissue was because i didn't had nvidia graphics so that i should install with cpu version because i am using Raedon vega 8 graphics\r\n\r\nHope it helps.", "(base) PS C:\\Users\\Priyanshi> pip install tensorflow==1.5\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.5 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0rc0, 1.15.0rc1, 1.15.0rc2, 1.15.0rc3, 1.15.0, 1.15.2, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2, 2.0.0, 2.0.1, 2.1.0rc0, 2.1.0rc1, 2.1.0rc2, 2.1.0)\r\nERROR: No matching distribution found for tensorflow==1.5\r\ni found this kind of error please anyone help me?\r\n", "Simply downgrade your TF version!\r\n\r\npip uninstall tensorflow\r\npip install tensorflow==2.0.0\r\n\r\n(for me happened when I upgraded to 2.1.0 and I had problem with any pip version)", "If it works on 2.0.0 but not on 2.1.0 then you need to install the 2019 MSVC redistributable as [the requirements state](https://www.tensorflow.org/install/pip)\r\n\r\nIf it doesn't work on 2.0.0 either, then check if your Python interpreter is on 64 bits (it has to be) and if your CPU supports AVX (it has to). If Python interpreter is not right, please install the right one. If CPU does not support AVX, then you can compile from source.\r\n\r\nSince this is the solution, locking conversation to prevent bad advice and \"it works for me\" messages"]}, {"number": 17392, "title": "Fix MKL on mac", "body": "Fixes #10685\r\nBuild can be created with:\r\n```\r\nenv TF_MKL_ROOT=/Users/antonmatosov/Downloads/mklml_mac_2018.0.1.20171227 bazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```", "comments": ["After re-basing on top of latest master there are several new build incompatibilities. Working on them now", "I have sorted out all build issues. PR is ready for review.\r\nThis branch doesn't cover automatic download and bundling of the mkl for mac. I am working on this ATM. I will submit this in a separate PR.", "@anton-matosov This PR and 17396 break the MKL Linux builds. Can you test on Linux? Here's the error that we're getting:\r\n \r\n> bazel build --config=mkl --copt=-mavx2 --copt=-mfma --copt=-march=broadwell --copt=-O3 --copt=-L/opt/tensorflow/gcc6.3/lib64 -s -c opt //tensorflow/tools/pip_package:build_pip_package\r\n> ++ grep 'ERROR: ' ../bazel_build.log\r\n> ++ wc -l\r\n> + '[' 3 = 0 ']'\r\n> + RESULT=FAILURE\r\n> + ERROR=1\r\n> + grep 'ERROR: ' ../bazel_build.log\r\n> ERROR: missing input file '//third_party/mkl:MKL_LICENSE'\r\n> ERROR: /dataset/cje/AIPG_TF/workspace/private-tensorflow-pr-mkldnn/private-tensorflow/tensorflow/tools/pip_package/BUILD:133:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '//third_party/mkl:MKL_LICENSE'\r\n> ERROR: /dataset/cje/AIPG_TF/workspace/private-tensorflow-pr-mkldnn/private-tensorflow/tensorflow/tools/pip_package/BUILD:133:1 1 input file(s) do not exist\r\n> + tail -5 ../bazel_build.log\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> ERROR: /dataset/cje/AIPG_TF/workspace/private-tensorflow-pr-mkldnn/private-tensorflow/tensorflow/tools/pip_package/BUILD:133:1 1 input file(s) do not exist\r\n> INFO: Elapsed time: 16.226s, Critical Path: 0.27s\r\n> FAILED: Build did NOT complete successfully\r\n> + echo 'RESULT: FAILURE'\r\n> RESULT: FAILURE", "@anton-matosov Renaming of mkl:LICENSE to mkl:MKL_LICENSE in 17396 (https://github.com/tensorflow/tensorflow/pull/17396/files#diff-a36e13dd6539354205d68ce60ea57ea5) seems to have broken the build.", "@opencici2006 can you take a look?", "#17396 is marked as WIP am still sorting out some issues.\r\nI am now testing build on linux to make sure there are no regressions.\r\nAdditionally, it seems, that with existing bugs in bazel it will not be possible to create Mac build without installing MKL dylibs in the library search path. Details are in #17396", "@claynerobison @nhasabni I have verified MKL build on linux with latest #17396 and it passed without errors:\r\n```\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 1693.973s, Critical Path: 60.63s\r\nINFO: Build completed successfully, 4388 total actions\r\n```\r\nBuild command\r\n```\r\nbazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nAlso tried with the one from your log\r\n```\r\nbazel build --config=mkl --copt=-mavx2 --copt=-mfma --copt=-march=broadwell --copt=-O3 --copt=-L/opt/tensorflow/gcc6.3/lib64 -s -c opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nThe rename issues was fixed 4 days ago in https://github.com/tensorflow/tensorflow/pull/17396/commits/65e8bd064836156cc699507eb1797d3027435cdb", "Hi @anton-matosov ,\r\n\r\nWe have encountered problem when building TF with MKL on mac as is written in \r\nhttps://github.com/bazelbuild/bazel/issues/4794\r\n\r\n\r\nAfter communications in the same page, it is recommended to try the newest bazel non -  released, so we think about compiling bazel from source. This requires the installation of a released version, and problems still occur when installing the released bazel v 0.11.1 with a direct `bash ./compile.sh` . \r\n\r\nMaybe you would be interested to try building bazel from source code on mac, and generously share with us a built binary? \r\n", "Hi @opencici2006,\r\nThanks for trying this branch out. I believe you shared wrong link, the one you shared is about a broken URL in bazel's readme file.\r\n\r\nBuilding bazel is a bit cumbersome and will require tweaking build arguments later (e.g. manually built version doesn't infer cpu on mac for some reason, so you need to pass `--cpu=darwin` for build to pass). So I discourage this way.\r\n\r\nAs discussed in #17396 I have used rc1 build version made available by bazel team. So this is better approach than building yourself.\r\n\r\nAnd the simplest way to bypass bazel linker issues is to simply install MKL dylibs in your system to `/usr/local/lib`. You can find detailed instructions in my original guide on building MKL here https://github.com/anton-matosov/tensorflow-wheels/tree/master/Tensorflow-Wheels/MacOS/MKL", "This PR is superseded by #17396 ", "Hi @anton-matosov \r\nThanks for your reply, I have updated the issue link.\r\nhttps://github.com/bazelbuild/bazel/issues/4794\r\n\r\nYou could see our changes similiary as your PR. We will have a try following your instructions guide to bypass bazel linker issue.\r\n\r\nThanks a lot.\r\n\r\n\r\n\r\n", "You are very welcome. Bazel 0.12 is finally out, so no more manual workarounds needed."]}, {"number": 17391, "title": "Tensorrt improvements", "body": "This PR improves the TF-TensorRT integration and provides\r\n- FP16 and INT8 TensorRT engine support\r\n- Calibration mechanism for INT8 engines\r\n- Expanded op support\r\n- User configurable graph partition size threshold\r\n", "comments": ["Tagging @aaroey and @zheng-xq ", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "This PR breaks the internal build, I'll try to fix it before merging.", "@cliffwoolley It doesn't work internally as we have some script to replace `#include \"cuda/include/cuda_runtime_api.h\"` with an internal path, so if the directory is missing the match will fail."]}, {"number": 17390, "title": "Full-fledged Java api roadmap?", "body": "Are there any estimated plans for Java api development in terms of functionality ? The only absolute choice for Java language nowadays is Deeplearning4j, any plans to occupy a niche for Java language? \r\nBy the term \"full-fledged\" I mean: constructing a graph from scratch (including most common math function/operations, respectively), train CNN, test CNN, recognize the result, all of this using Java language only.\r\n[#14094](https://github.com/tensorflow/tensorflow/pull/14094) and [#16120](https://github.com/tensorflow/tensorflow/pull/16120) is still in work and latter frozen for a few days. Just want to know your vision of future. \r\n\r\nP.s. need to choose right framework for starting a project at the end of June.", "comments": ["@asimshankar Can you comment on this?", "any news ? ", "If we parse the protocol buffer byte array from `TensorFlow.opList()`, will this give us a full readout of the functions we can pass to `Graph.opBuilder`? My thought is just to get a list of all available functions with inputs and outputs from `opList`, then use these to parse valid/invalid values when try to build a graph. Based on how a pre-trained graph is loaded using this Java api, it seems like this approach should work for building a graph from scratch.\r\n\r\nI am in the same boat as you @up-to-you, and after looking through DL4J I think my preference is to stick with Tensorflow.", "@Nicholas-Schaub, thanks a lot, i will test this approach. Nice to have fellow in misfortune :) I hope the guys from Tensorflow will not let our boat drown in the pythonish quagmire :(\r\n\r\n**UPDATE**\r\n\r\n`TensorFlow.registeredOpList()` eventually delivers some operations description, but this is a painfull pain of it's translation to `opBuilder` , first of all, taking into account `Java API is not covered by the TensorFlow API stability guarantees.`\r\n\r\nI do not have the goal of pushing this problem to Tensorflow developers, i just need information - will their automation tool for Java ops generation be ready soon or not ?\r\n", "I misspoke. `TensorFlow.registeredOpList()` is the method I meant to say. I have already parsed the byte array out, and from every check I've made, it matches the Python implementation. I'm just going to make a function that builds a graph like it does in Python and checks the input values for each function against the values given from registeredOpList.", "@up-to-you @Nicholas-Schaub : Thanks for your interest. We haven't published a roadmap yet. So far we've been mostly interested in being able to \"deploy\" models typically architected in Python (often trained, but at least designed) into other languages such as Java.\r\n\r\nThe next step for the Java API is creating the op functions (#16120 is being worked on - thanks @karllessard @kbsriram ). However, we have not yet hashed out a timeline for building higher level APIs on top of that. That said, this is something that can be built on top of the lower level APIs that we do have, so if anyone wants to take a stab at it ... \ud83d\udc4d \r\n\r\nTo be perfectly honest, it is unlikely that we'll have a significant change in the state of the Java API by June, but hopefully we'll have more of a plan to share by then (CC @ewilderj )\r\n", "@asimshankar thanks for your information ! if I understand correctly - [#16120](https://github.com/tensorflow/tensorflow/pull/16120) will let us construct graph from scratch using Java ? If so, is there any barrier for constructing multilayered CNN ? \r\n\r\nThanks in advance.", "@up-to-you, there are a few more pull requests upcoming after #16120 to be able to construct a multilayered CNN from scratch in Java using a rich API. \r\n\r\nBut like @asimshankar said, even with that, there will still be some additional work to do to reach the maturity of the Python API as it contains more logic than what is exposed by the core library, shared by all the clients. For example, gradient backpropagation will still be more easier in Python than in Java (while still feasible). It would be nice if somebody could take a look at this part!", "Well...I'm glad I stumbled upon this. I thought that the logic for backpropagation would be in the core library, and the main hurtle was creating functions to use the core library. Looks like I'm building and training in Python and importing into Java.", "@Nicholas-Schaub : For now, yeah, building and training in Python and exporting to Java for inference is probably best.\r\n\r\nThough, see the [Java samples](https://github.com/tensorflow/models/tree/master/samples/languages/java), in particular the linked slides with speaker notes, and the one trivial example of building the model in Python but actually training in Java.", "@karllessard @asimshankar regarding the information you provided (thanks a lot !), i think its better to emphasize efforts on model training maturity, using Java API. In this case we have advantages of both worlds : \r\n1. declaring model using full-fledged, short, readable Python syntax\r\n2. training NN and inference with Java, using its performance, multithreading, big-data collaboration etc.\r\n\r\n@asimshankar can you please provide video / source link, from where does this slides come ?", "The slides are linked to in the README I pointed to, but [here they are anyway](https://docs.google.com/presentation/d/e/2PACX-1vQ6DzxNTBrJo7K5P8t5_rBRGnyJoPUPBVOJR4ooHCwi4TlBFnIriFmI719rDNpcQzojqsV58aUqmBBx/pub?start=false&loop=false&delayms=3000)\r\n\r\nThey were from a talk I gave in a Java Users Group meeting. I don't think there is a video link.", "Just to add something about this: other than the ubiquitous differences between the two languages, what is really interesting about having a Java graph building API compared to Python is the way it enforces tensor type checking at compile time (by using generics). So you know exactly what datatype you are dealing with at the output of an operation and what you can possibly do with it. So I think current efforts are not done in vain... or at least I hope ;)\r\n\r\nImplementing the core ops in Java is a big step forward for having a mature client library and yes @Nicholas-Schaub, it will be possible to do backpropagation using the [training ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/training_ops.cc), for example. But it will not be as easy as in Python until we figure out how to extract and expose the logic implemented in its client library, like its [gradient descent optimizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/gradient_descent.py).", "@karllessard If the issue is just figuring out how to expose the logic, isn't it just a matter of parsing the `registeredOpList' to get a list of functions and their inputs/outputs and writing a rudimentary translator to make sure that the appropriate inputs and outputs are being sent/received? The way it was discussed above, I thought that we would have to implement the gradient descent optimizer. My approach was just to parse the op list and write a generic graph building class that shows which operations are available and what inputs/attributes are needed. If the gradient descent functions are implemented in the core library, then it seems like it's just a matter of then building a simple class to cycle through layers for backpropagation. The hardest part for me seemed like it was going to be parsing the operations list, and then I could build a couple simple classes to build and train.", "My thought in implementing this wouldn't be something quite as nice as what Python currently has, but it would be enough to build a graph and train it. During the graph building phase, the individual elements would be added to a JSON string, and then the graph could be built based off the JSON string and checked for errors using the info we get from parsing the op list.", "@Nicholas-Schaub \r\n\r\n> isn't it just a matter of parsing the `registeredOpList`...\r\n\r\nIt is a bit more tricky than that. The `registeredOpList` does not provides all the required information to classify the ops properly (for instance, their \"group\"). This is why it has been decided to import them from a C++ process instead, that links to the individual libraries and generates Java classes for their ops at compile-time. PR #16120 and those upcoming are about this.\r\n\r\nJust if you are curious, you might find a snapshot of the ops I have generated (and never tested) a long time ago from a intermediate version of this generator [here](https://github.com/karllessard/tensorflow/tree/java-ops-samples/tensorflow/java/src/main/java/org/tensorflow/op).", "Does the grouping matter? I thought having the necessary information to build a function using `Graph.opBuilder` would have been enough. It seems like this is how the current state of the java side of things imports pretrained networks. Is there ambiguity in function handles between the different groupings? Why isn't having the names of all functions with corresponding inputs and attributes sufficient to use the opBuilder?\r\n\r\nI apologize for drawing out the conversation, but it's been incredibly helpful thus far. Thank you for the link.", "> Does the grouping matter? \r\n\r\nGrouping doesn't matter at the functional level but it does matter at the API level. By this I mean that if you want to list and create classes just parsing the `registeredOpList`, it will work but you'll endup with a single package (group) of 600 classes or so, which would not be usable/acceptable as an official API for Tensorflow (just think how the auto-complete feature of popular IDEs will react on this). But for personal use, yes that'll do.\r\n\r\n> I apologize for drawing out the conversation, but it's been incredibly helpful thus far.\r\n\r\nNo problem it's a real pleasure, we can continue that discussion on a more private channel if you are interested, by joining [this group](https://groups.google.com/forum/#!forum/tensorflow-java-dev-unofficial).", "@karllessard can you please shed some light on other pull requests planned after [#16120](https://github.com/tensorflow/tensorflow/pull/16120) which will allow to construct multilayered CNN ? I'm gonna make some performance benchmarks in comparison to python impl's, since training elapsed time depends not only on C++/CUDA implementation, but on fetching and treatment of data too (before feeding it to CNN). And I make big bets, that in this part of whole process, Java will surpass Pythonish approach.\r\n\r\nP.s. it will be more clearly for results if whole NN lifecycle will be build using Java. Just for some reflection on an abstract level [Python vs Java benchmarks](https://benchmarksgame.alioth.debian.org/u64q/python.html)", "@up-to-you : FYI: if you use the [`tf.data` APIs](https://www.tensorflow.org/programmers_guide/datasets) to feed data to your model, then all the data is read, processed, and fed by the TensorFlow runtime, so the frontend language is unlikely to be a bottleneck. Long story short,  it will generally be possible to get the same performance irrespective of frontend language.", "@up-to-you : very interesting, indeed. Asim brought an interesting point with the `tf.data` APIs but it would still be very nice to benchmark the performances of the Java client independently on this.\r\n\r\nTimewise, #16120 has been accepted and is just waiting to be merged. After this, the target is that I come up with another PR by the end of the month which will actually generates the Op classes in the `libtensorflow` jar. \r\n\r\nFrom there, you'll be able to build a graph in Java using those classes. More PR(s) will still come after to wrap those classes with a builder-like API but functionally, they should be ready to be used directly.", "BTW, it's already possible to use the full C and C++ APIs with the [JavaCPP Presets for TensorFlow](https://github.com/bytedeco/javacpp-presets/tree/master/tensorflow), and we can pretty much do anything we want with that including building graphs and accessing the ops, for example: \r\nhttps://tebesu.github.io/posts/Training-a-TensorFlow-graph-in-C++-API\r\nhttps://matrices.io/training-a-deep-neural-network-using-only-tensorflow-c/\r\n\r\nIf there is anything missing from the bindings though, it should be a simple matter to add, so please do let me know! I'm very interested in making C++ libraries usable from Java and in understanding what is lacking, the difficulties, etc. So any feedback is welcome! Thanks", "@saudet, First of all, its awesome, really!\r\nBut, as @karllessard  mentioned - there are a lot of logic, implemented at python level. For example i can't find api, that allows to write something like this:\r\n\r\n`l1 = tf.layers.dense(x, L1, activation = tf.nn.relu, use_bias = True )`\r\n`l2 = tf.layers.dense(l1, L2, activation = tf.nn.relu , use_bias = True )`\r\n`...`\r\n\r\nand, honestly, unfortunately, i don't know how to implement same logic, using existing Java api in Tensorflow project, nor in your bindings.", "Thanks! Yes, I am aware a lot is implemented only in Python, but at some\npoint it needs to be implemented in either C++ or in Java. Imagine it gets\nimplemented in a user-friendly API in C++, what would make you not use\nJavaCPP? What would still be missing? That's the kind of feedback I'm\nlooking for. :)\n", "I'm glad I've held off working on making up my own solution to this problem since it sounds like things are starting to come down the pipe.\r\n\r\n@saudet What would make me not use JavaCPP is that I need to do things in native Java. If it's included in a dll that's fine, otherwise it's a headache.", "@saudet, JavaCPP is a very interesting project indeed, but I think for tensorflow, Python client will always be a step ahead of both Java and C++ clients for training.\r\n\r\n@kbsriram already thought of doing the same thing but with a PythonToJava flavour, I don\u2019t know what would be the complexity of such work though.", "@Nicholas-Schaub IMO, we're far from getting everything in place, so please keep looking at things! What bothers me though is that I had mapped the C++ API of TensorFlow API way back in 2015, but apparently this was not a satisfactory solution for Google (and I am still looking for an explanation as to why that is) and their engineers (mainly @asimshankar) started writing JNI code manually and building a custom API on top of that. I can see this is still in progress, first to match the functionality of the C++ API for the ops, and then eventually of the Python API.\r\n\r\nSo, anyway, all that to say, I would welcome any feedback about JavaCPP and what's missing. I mean, I do receive feedback from users saying that the C++ API isn't user-friendly, which is fair enough, but then why not build a user-friendly API on top of automatically generated wrappers like they've been doing for the Python API with SWIG? Why is there a need to write everything manually down to all the JNI functions?\r\n\r\n@karllessard Yes, I've also been talking about that, and https://jyni.org/ is another project which could use our support and that could provide that. They've got NumPy running, adding TensorFlow would probably not be that hard, if we can actually get 1 or 2 engineers working on that full time that is. Again, it won't provide a Java-friendly API out of the box, but it would take care of generating all the nasty boilerplate code, so why not use, if not that exactly, something like that, at least?", "Wow, it seems like [#14094](https://github.com/tensorflow/tensorflow/pull/14094) merged, i'm staying tuned on next pull requests ! @karllessard @asimshankar thanks guys !", "@saudet: Regarding [\"I'm still looking for an explanation as to why that is\"](https://github.com/tensorflow/tensorflow/issues/17390#issuecomment-375837293), I didn't know that you had ever asked for one :). My apologies if I missed some previous comment asking about this.\r\n\r\nFirst some background:\r\n- TensorFlow has a runtime implemented in C++, but for \"frontends\" we provide a smaller C API surface in [c_api.h](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h). Unlike the C++ interfaces used in the runtime's implementation, we do provide API stability guarantees for the C API.\r\n- This stable C API surface makes it suitable for implementing most language bindings. The Python frontend connects to the runtime via this C API, as do the other languages like Go, Haskell, Rust, Ruby etc. (In Python's case, this C API is what is SWIGed)\r\n\r\nThe Java API implementation in this repository has a relatively small hand-written JNI wrapper over the C API for the lowest level constructs (Graph, Operation, Session). The work by @karllessard and others is not adding any additional JNI, but generating idiomatic pure-Java wrappers for all the TensorFlow operations by  composing the lower level Java constructs. This is again very similar to how the Python API wrappers (`tf.add`, `tf.gather` etc.) are generated to use the Python constructs built on top of the C API. The same for Go. These language-specific generation tools make it easier to generate code that follow idioms of the target language. For example, in Java, generics are used to indicate the element-type of `Tensor` objects. It's a small amount of JNI to build the lowest level constructs and then a generator of pure Java code on top of that. It's not that \"everything\" is being written in JNI manually.\r\n\r\nLong story short, there were a few options we had:\r\n- Write a small amount of JNI over a stable C API and generators for higher level APIs on top of it.\r\n- Use tools like JavaCPP or SWIG or  [CLIF](https://github.com/google/clif/blob/master/clif/python/faq.md#q-what-about-interfacing-c-with-languages-other-than-python-such-as-java-and-go) to generate a Java interface from the C++ API.\r\n- Use tools like jyni to generate from the Python API.\r\n\r\nEach of them is attractive and has pros and cons. It was useful for us to have a Java API, particularly for executing models. In our judgement - given needs, the resources we have, and the fact that the API in this repository is something we (the TensorFlow team) support and maintain, the simplicity of the first approach - with fewer layers of abstraction, a small set of core constructs (backed by native code) upon which richer idiomatic APIs can be generated or built (in pure Java)  made it appropriate.\r\n\r\nI appreciate that there is some subjectivity involved in such a choice and that reasonable people can differ. However, I hope this at least provides a sense of where we were coming from. Looking ahead, we're figuring out the best way to get more community involvement in Java development, I'll be sure to reach out once we've figured out an appropriate forum for that. At that point, we can have a more detailed discussion (and possibly revisit these choices).\r\n\r\nAlso, your point about the C++ API not being user-friendly enough is well taken. Making graph construction APIs in C++ easier to use/navigate would be great, but is not currently on the top of our priority list.\r\n", "@asimshankar Thank you for the thorough explanation and for clarifying the discussions that have been happening offline! My opinion though is that it is possible to design a C++ API that can be mapped automatically to other languages in a satisfactory manner. OpenCV is an example that is pretty close to that, and JavaCPP maps it pretty well. There are rough edges, but this is only because Java and Python are an afterthought of the C++ API. Given a bit more planning though, we could save many hours that are currently spent developing and maintaining essentially the same library in different languages. JavaCPP wraps only for Java, obviously, but there are no reasons why the same couldn't be done for other platforms. This is out of scope for TensorFlow, but it would be nice if we as a community could start developing more processes and tools like this to save time and effort. CLIF looks like an attempt at that, but I am unaware of anything significant actually using it...?\r\n\r\nIn any case, for now, I've bundled the official Java API in the JavaCPP Presets for TensorFlow:\r\nhttps://github.com/bytedeco/javacpp-presets/tree/master/tensorflow#documentation\r\nThis will make it easy for anyone to start with that API, but \"upgrade\" to the C++ API to access functionality only available there--without any build time or runtime conflict. If you ever need help integrating this into the Bazel build or anything, please let me know!", "@asimshankar I talked to rajat about this a few weeks ago and he actually encouraged collaboration.@saudet has done a ton of optimizations on the c++ side already (esp lower level pointer math with zero copy buffers rather than the manual copying that happens with the current java API) - we use those same optimizations in depth with our tensor library as well. If you don't mind I'm going to follow up directly again with him showing him this issue. We're trying to play ball here if we can. We are likely one of the few entities outside of google interested in making this work well. I'd like to see if we can open this up a bit.", "@agibsonccc : Certainly, please do follow up. Those optimizations sound great, and we'd love to leverage expertise here. The details of how to do so would be worth working out, and having some off thread conversations before settling on something and updating this issue sounds great to me.", "Perfect. Just wanted to start a dialog. Someone reached out already.  Will keep folks posted on concrete outcomes, thanks for the interest!", "Just FYI everyone, pull request #18192 has been created to generate the Java operators and it is a big step forward to have full-fledged support for TF graph building in Java. The PR is quite big and it might take some time before we get it merged but we'll do our best, thanks!", "@karllessard , I want to do the graphbuilding using static functions(until tf rolls out the generated APIs), similar to what you have in the LabelImage example. It would be a complete training example with ops for both objective and say gradient descent. Since Java does not implement the autodiff functionality that python has, can you suggest any simple way of doing this (or do you have any POCs like the Labelimage one?). ", "@asimshankar Great! Looking forward to the discussions.\r\n\r\nFWIW, TensorRT has a nice C++ API and JavaCPP maps it pretty well too:\r\nhttps://github.com/bytedeco/javacpp-presets/tree/master/tensorrt\r\nI'm not saying TensorFlow needs a C++ API though. I suppose tools for other platforms should catch up before it makes sense...", "@himsR , either with the upcoming Ops API wrappers or with current `OperationBuilder` & friends, the only thing that is supported in Java for building a graph is by using TensorFlow core operations. Anything else needs to be ported to the clients of each languages at some point (e.g. the C++ client has already started to port some of the Python functionalities). \r\n\r\nI guess supporting those features will be the next step for the Java client, now that the `Op` wrappers are almost there. Right now, its improvement mainly relies on contributions so I suggest that you go on with building your graph, identify what is missing in your case and share it with us... or even better, write it down and submit your code :)\r\n\r\n", "@karllessard thanks for the information. The op wrappers that you are writing , do they include the gradient ops as well? For example , something equivalent to the ops inside the math_grad.cc in the core ? ", "@himsR : are you talking about `math_grad.cc` of the C++ client? If so, most of those ops come from the `math_ops` library so yes, they will be wrapped. The current scope for the first draft of wrappers is to cover the following libraries (that could be found [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/ops))\r\n```\r\narray_ops\r\ncandidate_sampling_ops\r\ncontrol_flow_ops\r\ndata_flow_ops\r\nimage_ops\r\nio_ops\r\nlinalg_ops\r\nlogging_ops\r\nmath_ops\r\nnn_ops\r\nno_op\r\nparsing_ops\r\nrandom_ops\r\nsparse_ops\r\nstate_ops\r\nstring_ops\r\ntraining_ops\r\nuser_ops\r\n```", "@karllessard yes that's awesome. Since the above list does not include the gradient files, it would still be required to code all the gradients using ops from these apis.  So yeah this should give all the ops required to build the backpropagation logic into java. I asked particularly about the gradients because I think at present client languages like python impelement all the gradients themselves(by calling and combining the normal ops) instead of calling the c core. So at present in Java, although you can call all the ops using the 'OperationBuilder' but you can't get the gradients that way. Thanks.", "Hey, @karllessard, I have a question for you. I see from your pull requests #14094, #15928, #16120 and #18192 that you had to write almost everything in C++, but with JavaCPP, it would have been possible to do it all in Java. Had you been given the chance to do it in Java instead of C++, do you think it would have made things easier for you?", "In any case, my point is that if we could offer developers the option of contributing code in Java instead of having them write most of the plumbing in C++, TensorFlow would get a much wider adoption from the Java community. Many Python developers can also write code in C++, but this is not typically the case of Java developers.\r\n\r\nIt's also one of the big differences between SWIG and JavaCPP. With SWIG, developers are forced to code in both C++ and the \"scripting language\" (Python, Java, etc). Instead, JavaCPP maps the C++ API wholesale, offering a complete and usable Java interface, without any need to write or make adjustments to the C++ code. BTW, if anyone knows of an alternative to access the full C++ API of TensorFlow from Java (or from any other platform for that matter, in C#, Go, Rust, JavaScript, etc), please let me know: I would welcome competition! :)", "Hi @saudet, sorry for the delay. I feel that there might be a little bit of misunderstanding here, please let me explain more in details. Sorry for the novel ;)\r\n\r\n> Had you been given the chance to do it in Java instead of C++, do you think it would have made things easier for you\r\n\r\nDefinitely, a challenge I had in C++ was to output Java source code efficiently, which could have been provided out-of-the-box from Java frameworks, such as JavaPoet.\r\n\r\nThe main reason why I had to go with C++ the generator the operation wrappers, as I told @Nicholas-Schaub previously in this thread, is to benefit from operation classification which, at that time, could only be achieved by only linking one core op library at a time to the generator binary and use that library as a package delimiter.  Otherwise you'll end up having all 700+ ops in the same package, which is not convenient. That's the approach the C++ client was taking too. It was a bit hacky, in my opinion, but I'm not sure to see how JavaCPP would have help me doing otherwise here. \r\n\r\nOn this topic though, there have been some updates to this issue. Lately, the TF community has introduced a new layer of abstraction over the core ops definitions, called `Api defs`, which not only exposes classification information in proto buffers but also allows variation of the API per language for fine-tuning. With this new API framework, the wrapper generator could have been written in Java from the start. Now that there is only #18192 remaining to generator those wrappers, I guess we should continue in the current direction and maybe migrate that code to Java later.\r\n\r\n > if we could offer developers the option of contributing code in Java instead of having them write most of the plumbing in C++\r\n\r\nAnd that is actually already the case. Once the wrappers are available, any addition or \"plumbing\" to the Java client could be written directly in Java. There might be a bit of JNI code to do here and there I guess, but features I can think so far shouldn't require it.\r\n\r\nFrom what I understand, JavaCPP goal is to simplify access to native libraries without the need of writing JNI code by ourself or interfacing with proto buffers, is that right? So your idea is to have a fully-compliant C++ client that could be \"wrapped\" in Java as a replacement of the actual client or is it just to replace the JNI bridge? I have no doubt JavaCPP is more convenient that writing JNI code. Proto buffers, on the other hand, is easy to support. Also, how JavaCPP would it handle features that are unique to the Java, such as generics, Javadoc, etc.? IMHO, what really matters in the end is the quality of the interface exposed to the user and not how it was developed.", "@karllessard Thanks for the insights! Unless the C++ API is designed in a way to let us generate automatically nice high-level APIs, there will always be a need to develop something on top. I agree with that. \r\n\r\nI'm not familiar enough with TensorFlow to evaluate how much work there is still to be done with the C++ API to get the Java API to be as feature complete as the Python API, but looking at the amount of code that the SWIG interface files from TensorFlow generate (about 25000 lines), I'm guessing \"a lot\". Am I wrong?\r\n\r\nJavaCPP basically provides access to everything C++. JNI is completely abstracted away, so the fact that it uses JNI isn't really relevant. Any other approach that I am aware of forces developers to get their hands dirty with C++. They don't offer a way to just use it from another language such as Java--with full support for reflection--whether it's one big API, or a bunch of smaller ones in \"groups\" (of operations, etc), which I call \"presets\": http://bytedeco.org/.\r\n\r\nThere is a Java API for protocol buffers, so that should be used whenever possible, sure. I'm not saying that using wrappers around C++ generated code is a better idea. :) Anyway, the concept of using C++ classes and templates from another language isn't new, but it's a road paved with failures and very limited success that has made many casualties over the decades, so it's hard to get people to even acknowledge that it's actually possible...", "@karllessard can we expect tf.train.Example, tf.python_io.TFRecordWriter, tf.train.Feature and other tf.train.* types wrappers? This impl will blow up everyone, since it will shut the hole in Spark <-> Tensorflow integration. Until higher level api will be implemented in java.", "Hi @up-to-you , what is being ported right now are core ops that can be added to a neural network for training, i.e. most of the ops that are registered [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/ops).\r\n\r\nAFAIK, what you are referring to are utilities to store example data that happens outside the network, so it won't be part of the current work (though you'll be able to read data stored in a `TFRecord` from the network using the `TFRecordReader` core op).\r\n\r\nGood news is that nothing prevent a programmer to add `Example` & friends support right now in the Java client, I guess it is just a matter of creating a proto message from data provided in input. Some utilities though might require a pinch of JNI bindings (or what @saudet is currently proposing).", "@karllessard I'll try my best in this way, but can you please provide some food for thought or some direction where i can start. Thanks in advance!\r\nP.s. e.g. can't find any information about in-depth proto-structure of TFRecord file.\r\n", "@up-to-you : Well, I haven't look too deeply in the Python code but it looks like it's invoking the core utility [`RecordWriter`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/record_writer.h) to accomplish the task of writing data. We should probably do the same by writing a new [JNI binding](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/src/main/native) for it.\r\n\r\nThen, for supporting the `Example` format, I guess we could generate classes out of the [protocol messages](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto) using a [`java_proto_library` rule](https://github.com/google/protobuf/blob/master/examples/BUILD) and maybe wrap their usage in a `ExampleBuilder` object? ", "@up-to-you: `tf.train.Example` is available in the [`org.tensorflow:proto`](http://mvnrepository.com/artifact/org.tensorflow/proto) maven artifact (see [javadoc](http://static.javadoc.io/org.tensorflow/proto/1.7.0/org/tensorflow/example/Example.html)). That artifact contains all the TensorFlow protocol buffers for Java.\r\n\r\nIt would be great to work out a path to include full functionality by leveraging the best of @karllessard and @saudet 's work (and effectively utilizing JavaCPP). But we haven't gotten around to figuring out the details of that yet. \r\n\r\nPerhaps you're willing to pioneer a first exploration towards that by updating the JavaCPP presets for TensorFlow so that it can provide a Java interface to the C++ [`RecordWriter`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/record_writer.h) class?", "@asimshankar @karllessard Sounds like the perfect opportunity to try this out with JavaCPP! Thanks\r\n\r\n@up-to-you I've updated the presets with `RecordWriter` in commit https://github.com/bytedeco/javacpp-presets/commit/e64724b5d194affeebf4c2934c0c4c557f3111ea. New snapshots should be available in a few hours: http://bytedeco.org/builds/\r\nAnd a sample pom.xml file is available at:\r\nhttps://github.com/bytedeco/javacpp-presets/tree/master/tensorflow#sample-usage\r\nLet me know if you encounter any issues with that or have any questions. If this works well, I'm assuming we would next have to integrate JavaCPP into the Bazel build, and probably start small and/or somehow modularize the API, something I'm happy to help with!", "@up-to-you While we're at it, I've also included `feature.pb.h`, `example.pb.h`, `record_reader.h` in commit https://github.com/bytedeco/javacpp-presets/commit/fce6bfb26ceacb8d7a4cd4ed6651e0f1e1ffc8ac. FYI, while the Java API of protocol buffers are easier to use, serializing and deserializing with the C++ versions might be be faster, especially if the resulting objects are to be used with the C++ API anyway.", "Thanks @saudet, but AFAIK, that shouldn't be required in our case, the `Example` protos are just used to define a standard format to a binary file and are not used for transportation, the data is passed as a simple byte array to `TFRecordWriter`. \r\n\r\nAs of serializing/deserializing those messages in the Java client, I think we can opt for simplicity over performances here. The way I see it that we should use as much as possible vanilla-Java code in our solution and rely on JavaCPP only when required, what do you guys think?\r\n\r\n(Just FYI, I talked @ewilderj and there is a possibility for us to have a official community platform regarding Java development in TF for discussions like these, if you are interested!)", "You're referring to premature optimization? Yes, of course, I agree.\n", "Hi, it seems that https://github.com/tensorflow/tensorflow/pull/18192 is merged so can we expect a Maven repository and some tutorials to make graphs anytime soon?", "Hi @arnaghizadeh , even if #18192 has been commited and that you can build a graph out of it, the work is still in progress and only curious developers should make use of those wrappers right now. To have a fully-compliant graph building API, there is two more tasks to be completed:\r\n1. Generate the API classes that exposes those wrappers in a builder fashion. This is what will allow you to write clean code like\r\n```java\r\nops.add(ops.unique(s, Long.class).y(), ops.constant(4));\r\n```\r\nwhere right now, you'll need to do something more verbose like\r\n```java\r\nAdd.create(scope, Unique.create(scope, x, Long.class).y(), Constant.create(scope, 4));\r\n```\r\nI was working on that PR last week and it is now ready to be reviewed (#19309)\r\n\r\n2. Categorise the generated operations wrappers. Right now, all ~800 wrappers are output under the same package, which makes it hard to use, I find. I'll check with @asimshankar how they should be divided into groups but the end goal is to have something like:\r\n```java\r\nops.math().add(ops.array().unique(s, Long.class).y(), ops.constant(4));\r\n```\r\nWhen those two tasks will be completed, I'll write down an example on how to use the new API, thanks for your patience!", "@karllessard thank you can't wait to create graphs in Java. By the way, have you guys been in touch with Swift for Tensorflow group and specifically Chris Lattner? The cleanness of their code is simply out of this world. My understanding is that they are changing the language itself and that's why they can achieve that kind of cleanness and ease of use, so possibly this is not feasible in Java. But maybe there are still some hope and long term plans to have something similar in Java or even with Kotlin which is probably more open to change the core language?", "@arnaghizadeh No, I didn\u2019t look at the Swift implementation but I will, thanks for the tip!\r\n\r\nFor the Java part, I also believe that Kotlin is the way to go these days for a simplified version of the language, so I\u2019ll make sure we take all advantages of it in the API we are building. Please let me know if you have any specific idea in mind about this.", "@karllessard From my limited understanding, what they are doing is that they have integrated Tensorflow eager mode with the swift compiler so there is no need to create graphs, the graphs are created by compiler itself. The end result is a very natural integration of Tensorflow with Swift language which is much more pleasant to work with than what we have in python. Even with its eager mode, the Tensorflow of python does not seem to be a match with this design. From what I could tell, they also provided a better version of the Numpy (with Tensors) for the swift language in a natural way. If next to the API that you are working on, something like this can be implemented for e.g., Kotlin that will be a blast.", "@arnaghizadeh : Speaking on behalf of the TensorFlow maintainers: Yes TensorFlow for Swift seems like a very promising avenue which is why we are pursuing it. There is some discussion on the approach described there working for Java in [the whitepapers](https://github.com/tensorflow/swift/blob/master/docs/WhySwiftForTensorFlow.md#which-languages-fit-our-project-requirements).\r\n\r\nThat said, this is still a very early stage project with lots of technical details to work though and for now there are no plans to investigate this approach for Java/Kotlin (at least by the TensorFlow maintainers).", "Hi TensorFlow maintainers, Thanks for a great project!\r\n`TensorRT` has been integrated into TF since `r1.7`. How can we use this in java for inference phase?", "@asimshankar @karllessard For reference, with the new additions to the Java API, is it now possible to build a graph for training like this? \r\nhttps://github.com/bytedeco/javacpp-presets/blob/master/tensorflow/samples/src/main/java/org/bytedeco/javacpp/samples/tensorflow/CarPricePredictionExample.java", "@saudet: yes, with the addition of supporting features that are unique to Java\r\n\r\nFor instance, using generics, you can keep track of the datatype of your tensors as you build your graph and get notified at compile time (or a edit time within your IDE) of any operation that is not allowed. For example, using Kotlin:\r\n```kotlin\r\nval x = ops.placeholder(Float::class.java)\r\nval y = ops.placeholder(Float::class.java)\r\nval z = ops.placeholder(Long::class.java)\r\nops.matmul(x, y) // compiles\r\nops.matmul(x, z) // does not compile\r\n```\r\nI'm waiting for PR #19309 to be merged and for instructions from @asimshankar on how operations will be grouped into packages, then we'll be able to write some more examples.", "Yes, that's cool, but I was mainly wondering about the gradients for training. It sounds like that would require some JNI... I guess I'll wait for the examples :)", "Are there any ways to migrate missing functionality from @saudet CPP to @karllesard solution ?\r\n**P.s.** i think its more difficult to migrate vise versa", "@up-to-you The plan is to integrate JavaCPP in the Bazel build, but for now we can still use the Maven build. At this moment though, as far as I know (given that I didn't get any reply from @karllessard or @asimshankar  about this) I am assuming there is no integration with the Java API for the ops with C++ functions like `AddSymbolicGradients()`, so work other than on the build also needs to be done before we can use both APIs together. No need to do everything by yourself though. You could work on the integration, get that working, and someone else could work on the build in parallel, or vice versa. In any case, if you are interested in working on this, but are encounter issues, please post your questions so that we can help. Thanks!", "Hi guys, please let me reiterate my thoughts on this.\r\n\r\nI think JavaCPP can be a useful addition to the current Java client but I would keep its usage internal. It perfectly fits as a replacement to the actual JNI bridge, being more flexible and probably more performant. But I would stick to a thin layer of pure-Java for classes that are exposed publicly, so we can easily introduce Java-specific features, like generics, and have more control over the API in general.\r\n\r\nThat being said, I don't want to sound dramatic but I think that open-source development is all about collaboration and not competition. Competition is a complete no-fit. Our goal is to develop the best Java client for TensorFlow, no matter how, and we should merge all of our efforts towards this end only. @saudet, if you can't agree with this, then I don't think I can help you much here. BTW, you say that you get no reply from me but I can't recall or find any question or request from you.", "@karllessard Don't worry, I agree. I asked about how we could build a graph for training, but I did not get a reply, so I simply assumed it wasn't possible. If this is not the case, please say so! I'm merely trying to make it clear what is possible and what isn't, what needs to get done, etc. I am always trying to find ways to communicate in a more friendly way, but I admit it is not easy for me, as I simply get ignored if I don't \"push\" a little.", "@saudet : Regarding adding symbolic gradients in the Java API, we'd have to add a Java function corresponding to [`TF_AddGradients`](https://github.com/tensorflow/tensorflow/blob/65c05bc2ac19f51f7027e66350bc71652662125c/tensorflow/c/c_api.h#L1127). So, you're right it doesn't exist yet, but should be easy to add as a method on `Graph`.", "@asimshankar : actually writing down an Mnist example in Java, I already ended up with the need to implement the JNI binding for `TF_AddGradients` in `Graph`. Another PR ready to get in line for you ;)\r\n\r\n@saudet : Glad to hear that! Then, I think we can start to discuss on how you could help contributing. I really think we need another communication channel than this thread to synchronize everybody's efforts. I suggest we use this [unofficial group](https://groups.google.com/forum/#!forum/tensorflow-java-dev-unofficial) for now until Google can setup something for us (and I invite everybody in this thread to join it). And maybe we can have a separate branch to consolidate our work, ideally in TF repo? @ewilderj , can you help us with all of this?\r\n\r\nThanks", "@asimshankar @karllessard Well, it's not just about me contributing, it's also about changing _your_ frame of mind. You keep writing everything in C++ and JNI manually, why? You have to understand this doesn't look attractive to Java developers. Could we start by making a list of what's missing from JavaCPP? Is it just the Bazel build for now or is there something else that I should look at first?\r\n\r\n@karllessard I've applied to join your group. Thanks! As for a branch, if you keep your fork up to date, I think we can just all use that for now as well.", "@saudet : At a high level, I feel that the **primary focus** should be on creating the right API to **use** TensorFlow from Java. The details of how that API is _implemented_ (JavaCPP, manual JNI, C++ or Java code generator etc.) is secondary, and changeable. The \"right\" API is characterized by appropriate abstractions that are easy to understand for a Java developer, a thoughtful exposure of features, and performance.\r\n\r\nIf we agree on that (that the experience of the API user is paramount) then we can have a productive conversation on the implementation details. I don't believe we should be working the other way around (i.e., being driven by what's easy to implement). \r\n\r\nWith regards to what's missing in JavaCPP - I don't have a list handy, and maybe nothing is missing. Honestly, I haven't devoted enough time to thinking through the details of what switching to JavaCPP would entail. Which is where contributions can help. Since you're passionate about this, perhaps you could do that? Some things to think about in a concrete proposal would be:\r\n\r\n- What does the end-user Java API look like? Would it make sense to implement the current Java API (with generics etc.) using JavaCPP for the native-call redirection, or would it make sense to define a smaller C++ class and generate its interface? For example, compare the `Tensor` class in [current Java API](http://bytedeco.org/javacpp-presets/tensorflow/apidocs/org/tensorflow/Tensor.html) vs. [JavaCPP wrapped over the C++ Tensor class](http://bytedeco.org/javacpp-presets/tensorflow/apidocs/org/bytedeco/javacpp/tensorflow.Tensor.html). Along the same lines, since so far the primary interest in the Java API has been in deploying trained models in an application - the APIs for that should be succinct and easy to comprehend ([examples](https://github.com/tensorflow/models/tree/master/samples/languages/java)).\r\n\r\n- How would we scope the effort required and benefits of it? For example, assuming we stick with the existing `org.tensorflow.Tensor` API, what does it take to back it with JavaCPP? Can we quantify the benefits (e.g., improved performance in some cases, like perhaps creating large tensors on Android or something)? Or if the proposal is to have a different API for the Java user, how do we evaluate? For example, can we evaluate the change in APIs by writing examples including \"load a model and serve for inference\" and \"fine tune a previously trained model\" and \"construct a trained model\".\r\n\r\n- Once this is scoped out, who is going to drive the changes? \r\n\r\n- Is it possible to break this down into small sets of changes/projects? For example, the code generator that Karl is working on could just as well be done in Java (no JNI) and doing that may be a reasonable project (Karl has ideas on switching to that, but wanted to get his C++ version in first just because he has that almost working and again, most _users_ of the API won't care whether the Java code was generated one way or another).\r\n\r\nI'm not at all suggesting that I'm convinced that JavaCPP isn't the appropriate choice here. However, I haven't had the opportunity to think through all these details, and probably won't have much time to devote to that in the near future. So if someone wants to think this through and write it up and come up with concrete proposals (that can be [shared as an RFC](https://github.com/tensorflow/community) ([example](https://groups.google.com/a/tensorflow.org/d/msg/developers/6Irde5yjUBc/SRool5ghAgAJ))), we're open to that. Perhaps there is some precedence here? For example, I noticed that there are [JavaCPP presets of MXNet](https://github.com/bytedeco/javacpp-presets/tree/master/mxnet) as well, while MXNet also has [handwritten scala bindings](https://github.com/apache/incubator-mxnet/tree/master/scala-package/core/src/main/scala/org/apache/mxnet) instead of generating them from the C++ (though admittedly I know nothing about Scala or the implementation/design choices in MXNet). Perhaps you've been through similar thinking there as well?\r\n\r\nLong story short, I'd say that the barrier to \"use JavaCPP\" isn't anyone's \"mindset\", it's simply a person putting the effort to dig into the details of what that means. Or perhaps I'm misunderstanding you and there is a concrete proposal you're suggesting?", "My point isn't about what the end user API should look like, it's about how to get there. There is no need to change what's already been done for the API at all. But you've spend over 2 years hacking in C++ and forcing contributors like @karllessard to contribute in C++. Why not let them contribute to the effort by doing it in Java? We can use the C/C++ API of TensorFlow or of anything else like MXNet for that matter, from Java. Why does no one (other than Skymind) does it that way? I don't know, why should I know? I spend a lot of time arguing about that, but never got a satisfying answer. If no one does something, does it automatically make it wrong?\r\n\r\nTensorFlow could have a  policy to do as much as possible in C++, sure, I guess, but you should make that clear if that's the case. Is it the case?", "@asimshankar Maybe what we need is a separate repo like https://github.com/tensorflow/swift where contributors wouldn't be limited to contrived hybrid C++/Java solutions? Such a project would essentially become a consumer of the core of TensorFlow and evolve from there...", "@saudet that's similar to what I have mentioned earlier and it seems there is no plan for such integration yet. But even if they are willing to do such project I don't want this to hurt what is being done right now. It is already 2018 and we still do not have a proper implementation of TF for Java/Kotlin. This waiting is at the excruciating level (even c# has a TF implementation now). I personally don't want this to delay for more fancy features, they can be hopefully developed later on.", "@arnaghizadeh : I agree with you here. Just FYI, I think the major reason why it is taking so long is that a lot of time is spent in code reviews before features can get merged into the master branch. I am not blaming Asim for that, I think he's doing a fantastic job to keep the code neat and under control. But having a separate branch to experiment some features on our side before creating a PR in TensorFlow might help to develop things more dynamically and to get that full-fledged Java client out faster.\r\n\r\nIf TensorFlow is not able to provide us that branch, maybe we should simply start our own and start to do some cherry-picking after.", "@karllessard I am not sure if the current setup will let us do that in the Tensorflow repo, though I do think it's a good idea for velocity. @martinwicke is it possible to give a sub-team a branch, or is a cloned repo a better idea?", "Branches are complicated. I would create a fork, keep its master branch synched with tensorflow/tensorflow, and work on a branch off of that. Then, make occasional PRs from there to tensorflow/tensorflow master. Do you think that would work?", "@martinwicke :+1: I think that would work, yes, how the commits into that fork would be moderated? \r\n\r\nRight now, I'm doing all my PR from my personal repo, which is very convenient, but the fork would really help if other developers manifest a serious interest in contributing for the Java client also (which seems to be the case reading those previous comments).", "Oh, my suggestion was that you review the commits on your fork (or, you can also give others write access to let others review). \r\n\r\nIf there are more than a couple of regular contributors, we can also set up a special interest group for you, and talk about moving the repo to the tensorflow org. The process would be mainly the same though: you review contributions and you can move at high speed, and you make larger, but less frequent PRs back to TF. @asimshankar what do you think?\r\n\r\nWe could also talk about moving the Java bindings out of the TensorFlow repo altogether. We'd have to figure out how to package that, but that would remove the need to sync back to TF altogether, which could be quite attractive.\r\n\r\n@ewilderj FYI", "> Oh, my suggestion was that you review the commits on your fork (or, you can also give others write access to let others review).\r\n\r\nOn my end, I agree with that plan, let's do this for now and see if some other serious contributors would like to do it differently, I'll send a link to that new branch soon, thanks!", "@martinwicke yep, we're already having SIG conversations.... will post an update on that soon.", "Nagging Assignee @asimshankar: It has been 64 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 79 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This conversation seems to have run its course. Closing this issue since I don't know of any plans on the docs side to publish a separate Java roadmap (here's the [general TF roadmap](https://www.tensorflow.org/community/roadmap) that was recently updated).\r\n\r\nSounds like the mailing lists/SIGs are the best way to keep up with new developments.", "there is no mention of the Java API in the road-map though, it seems that Java is going to be ignored for TF 2.0?", "TF 2.0 is certainly focused on Python, as you'll see in the design reviews at https://github.com/tensorflow/community/pulls\r\n\r\nThe Java APIs will continue to be mainly focused on loading and serving models (to integrate into existing applications) and the low level API for building graphs.  Higher level APIs to build models aren't something that are on the roadmap of the TensorFlow maintainers.\r\n\r\nFor that we'd encourage community driven efforts. As mentioned above, perhaps a SIG (if there is enough interest in design and development) or just great projects like JavaCPP presets mentioned above. ", "FYI, since TF 1.10, operations wrappers have been added to build and train graphs in Java (please note that it is still at an experimental stage). \r\n\r\nA minimalist example of their usage can be found [here](https://github.com/karllessard/models/tree/java-examples/samples/languages/java/mnist).", "@karllessard @asimshankar - the dl4j team is committed to supporting the javacpp stack. We have the high level keras api already built (maintained by @maxpumperla ) - we are also seeing people building demos not even a few weeks after its release: https://github.com/tzolov/mtcnn-java - we also have an equivalent api that could easily compliment tf.data.  Rather than a bunch of 1 off repos, why don't we focus on this upcoming TF 2.0 and being able to do a proper java SIG where we can address this?", "It seems that closing this issue just ignited it ;)\r\n\r\n@agibsonccc , personally I'm all for having a SIG where all those matters could be discussed and efforts could be merged, a big \ud83d\udc4d for this. I'm not sure though where all this is going since the current topic is about the current state of the TF Java client but discussions always tend to be dragged in the directions of other projects. When you talk about focusing on TF2.0, you mean in Tensorflow or in DL4J? ", "@karllessard We're still interested in doing this as part of dl4j or TF we don't care.  It really depends on what the TF team is interested in doing with 2.0. The team had expressed interest in merging efforts somehow. We'd prefer not to be rewriting things your team is already doing. We're interested in providing a graph runner regardless since we have to interop with every stack anyways. We're going to be doing something similar with onnx as well.", "@karllessard Great work, thanks for the contribution! The JavaCPP Presets for TensorFlow now include your Java ops wrappers, with builds on Maven for Android, Linux, Mac, and Windows, compiled against and bundled with MKL-DNN, CUDA, and cuDNN (that is, no need to install them, all binaries get pulled from Maven), along with a more complex JNI loader than the default one in TensorFlow that works well with multiple class loaders and Windows applications including JavaFX ones:\r\n    https://github.com/bytedeco/javacpp-presets/tree/master/tensorflow\r\n\r\nThey seem to work fine, but let me know if you find any issues or would like to change something. With a bit more work, I'm sure we could get all this to interoperate with the rest of the C/C++ APIs that come with the presets. That would be nice, given that at that level, JavaCPP can share memory with zero-copy and manage with scopes the resources used by multiple APIs, not only from TensorFlow but from other native libraries as well, such as OpenCV, FFmpeg, or Tesseract:\r\n    http://bytedeco.org/news/2018/07/17/bytedeco-as-distribution/"]}, {"number": 17389, "title": "Documentation links for version 1.5 silently redirect to 1.6", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: NVIDIA Titan X\r\n- **Exact command to reproduce**: Go to https://www.tensorflow.org/versions/r1.5/api_docs/python\r\n\r\n### Describe the problem\r\nThe Python and C++ API Documentation links at https://www.tensorflow.org/versions/r1.5/api_docs/ silently redirect you to documentation for version 1.6. The links are (I assume) correct, but the URLs https://www.tensorflow.org/versions/r1.5/api_docs/python and https://www.tensorflow.org/versions/r1.5/api_docs/cc redirect to https://www.tensorflow.org/api_docs/python/ and https://www.tensorflow.org/api_docs/cc/ respectively.\r\n\r\nI spent a while trying to diagnose a problem based on the documentation for 1.6, without realizing I was looking at 1.6 rather than 1.5. Turns out the problem was caused by an API change between 1.5 and 1.6. \r\n\r\n### Source code / logs\r\nN/A", "comments": ["Thanks for the report, this is fixed now."]}, {"number": 17388, "title": "Inconsistent OMPI_SKIP_MPICXX define", "body": "Looks like we `#define OMPI_SKIP_MPICXX` before `#include third_party/mpi/mpi.h` in two places:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b79ce0029dce3264266ced739590bc238b17096c/tensorflow/contrib/mpi_collectives/kernels/ring.h#L32-L34\r\nhttps://github.com/tensorflow/tensorflow/blob/b79ce0029dce3264266ced739590bc238b17096c/tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc#L36-L37\r\n\r\nBut it is not defined here:\r\nhttps://github.com/tensorflow/tensorflow/blob/b79ce0029dce3264266ced739590bc238b17096c/tensorflow/contrib/mpi/mpi_utils.h#L27\r\n\r\nBecause the first two are defined without an `#ifndef`, I can't define it in a compiler flag. We should probably add it to the last one, or at least make the first two no-ops if it's already set.", "comments": ["This seems like a pretty small / low-impact change, so I'll mark it as contributions welcome. Please feel free to prepare a PR.", "Added a PR #17414 for the fix."]}, {"number": 17387, "title": "[feature request] add higher order function `scanr`", "body": "Have I written custom code: yes\r\nOS Platform and Distribution: OSX 10.11.6\r\nTensorFlow installed from: conda-forge\r\nTensorFlow version: 1.5\r\nBazel version: N.A\r\nCUDA/cuDNN version: N/A (CPU only)\r\nGPU model and memory (CPU only)\r\nExact command to reproduce: N/A\r\n\r\nDescription:\r\nTensorFlow has `foldl` which folds the array from first to last, and `foldr` which folds the array from last to first. It also has `scan` which returns the trace of `foldl`, but there is no `scanr` that returns the trace of `foldr`. \r\n\r\n`scanr` can be proven useful sometimes. Look at the following snippet that is sampled from my implementation of a [crf lstm sequence tagger](https://github.com/shengc/tf-lstm-crf-tagger/blob/master/lstm-crf-tagger.ipynb).\r\n\r\n```python\r\nreverse_path = tf.scan(step_path, tf.reverse(backpointers, axis=[0]), initializer=best_tag_id, back_prop=False)\r\nbest_path = tf.concat([tf.reverse(reverse_path, axis=[0])[1:], [best_tag_id]], axis=0)\r\n```\r\n\r\nI have to call `tf.reverse` twice, just because TensorFlow only provides `scan` from first to last.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler updated", "@tensorflowbutler why it has been assigned to no one for half a month? This should be a very simple implementation, just like what foldr looks like vs. foldl ", "@ebrevdo Hi, do you know who could reply the feature request?", "Added a PR #18410 for `scanr` support. ", "Nagging Assignee @shivaniag: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 75 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 90 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 105 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 120 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looks like scan with reverse=true will do that you want."]}, {"number": 17386, "title": "Tensorflow 1.6.0 cpu fails on import on Windows 10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, problem appears on import (import tensorflow as tf).\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **TensorFlow installed from (source or binary)**: binary (pip --no-cache-dir) install --upgrade tensorflow\r\n- **TensorFlow version (use command below)**: 1.6.0 (install says tensorflow-1.6.0-cp36-cp36m-win_amd64.whl)\r\n- **Python version**: 3.6.4 x64\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: import tensorflow as tf\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nWhen installing tensorflow 1.6.0 the import reports problems. See log below. Tried uninstalled r1.6 and reinstalled 1.5.0 to see if something else might be the problem. 1.5.0 works like a charm. Tried a clean install of 1.6.0 without any luck.\r\n\r\nRan the 'tensorflow_self_check.py' script. See log below.\r\n\r\n### Source code / logs\r\n\r\n```\r\nPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n------------------------\r\n\r\nWhen running 'tensorflow_self_check.py' this is what is reported. Supposed to have installed the CPU version so cuda is not installed.\r\n\r\nERROR: Failed to import the TensorFlow module.\r\n\r\n- Python version is 3.6.\r\n\r\n- TensorFlow is installed at: C:\\Users\\Jonas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\r\n\r\n- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Download and install CUDA 8.0 from\r\n  this URL: https://developer.nvidia.com/cuda-toolkit\r\n\r\n- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Note that installing cuDNN is a\r\n  separate step from installing CUDA, and it is often found in a\r\n  different directory from the CUDA DLLs. You may install the\r\n  necessary DLL by downloading cuDNN 5.1 from this URL:\r\n  https://developer.nvidia.com/cudnn\r\n\r\n- Could not find cuDNN.\r\n```", "comments": ["@gunan Can you take a look at this?", "This is interesting. Could you retry installing with \"--ignore_installed\" plus everything you had, just in case.", "I also have this problem, exact same errors and outputs (except obviously my username instead of OPs). Installing with --ignore-installed doesn't change anything", "No positive result on installing with --ignore-installed. And now there is another report on the same issue,  #17393.", "The CPU lack AVX instruction can cause this error.\r\n\r\nI have built tensorflow wheel without AVX.\r\n\r\nhttps://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.6.0/py36/CPU/sse2\r\n\r\nI wish this .whl may help you.", "@mrry any ideas?\r\nCPU incompatibility makes sense to me.\r\n@epivbg @bradezard131 what is the CPU make/model on your machine?", "AMD Phenom II x4 955BE for me, which does indeed lack AVX support. These unlisted requirements (AVX support, VC++2015, etc.) are rather frustrating...\r\n\r\n@fo40225's wheel does fix the issue though!", "CPU Identification utility v2.14                 (c) 1997-2017 Jan Steunebrink\r\n ------------------------------------------------------------------------------\r\n CPU Vendor and Model: Intel Core i5 Dual M i5-430M/520E/520M/540M C2-step\r\n Internal CPU speed  : 2260.9 MHz\r\n System CPU count    : 1 Physical CPU(s), 2 Core(s) per CPU, 4 Thread(s)\r\n CPU-ID Vendor string: GenuineIntel\r\n CPU-ID Name string  : Intel(R) Core(TM) i5 CPU       M 430  @ 2.27GHz\r\n CPU-ID Signature    : 020652\r\n CPU Features        : Floating-Point Unit on chip  : Yes\r\n                       Time Stamp Counter           : Yes\r\n                       Enhanced SpeedStep Technology: Yes\r\n                       Hyper-Threading Technology   : Yes\r\n                       Execute Disable protection   : Yes\r\n                       64-bit support               : Yes\r\n                       Virtualization Technology    : Yes\r\n Instr set extensions: MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\r\n Size of L1 cache    : 2 x 64 KB\r\n Integrated L2 cache : 2 x 256 KB\r\n Integrated L3 cache : 3072 KB", "So I guess it's about the (lack of) AVX support.\r\n\r\nThanks @fo40225, @gunan and @bradezard131.", "@fo40225's wheel does fix the issue for me too. Many thanks!\r\n\r\nI fully agree with  bradezard131: Why arent these requirements listed on the official project website?\r\n\r\nCPU Identification utility v2.14                 (c) 1997-2017 Jan Steunebrink\r\n ------------------------------------------------------------------------------\r\n CPU Vendor and Model: Intel -Unknown model-\r\n Internal CPU speed  : 1439.9 MHz\r\n System CPU count    : 1 Physical CPU(s), 4 Core(s) per CPU, 4 Thread(s)\r\n CPU-ID Vendor string: GenuineIntel\r\n CPU-ID Name string  : Intel(R) Atom(TM) x5-Z8550  CPU @ 1.44GHz\r\n CPU-ID Signature    : 0406C4\r\n CPU Features        : Floating-Point Unit on chip  : Yes\r\n                       Time Stamp Counter           : Yes\r\n                       Enhanced SpeedStep Technology: Yes\r\n                       Hyper-Threading Technology   : No\r\n                       Execute Disable protection   : Yes\r\n                       64-bit support               : Yes\r\n                       Virtualization Technology    : Yes\r\n Instr set extensions: MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2,\r\n                       AES\r\n Size of L1 cache    : 4 x 56 KB\r\n Integrated L2 cache : 4 x 1024 KB\r\n Integrated L3 cache : None", "I am having this issue too on Windows 10.  No luck installing the CPU-only version on 2 different laptops.  No clue as to what requirements I'm missing.  I've tried Python 3.5 and 3.6.  I have the VC runtime\r\n\r\n **CPU Identification utility v2.14                 (c) 1997-2017 Jan Steunebrink**\r\n ------------------------------------------------------------------------------\r\n CPU Vendor and Model: Intel Core i7 Quad i7-860S/860/870S/870/875K/880 B1-step\r\n Internal CPU speed  : 1728.9 MHz\r\n System CPU count    : 1 Physical CPU(s), 4 Core(s) per CPU, 8 Thread(s)\r\n CPU-ID Vendor string: GenuineIntel\r\n CPU-ID Name string  : Intel(R) Core(TM) i7 CPU       Q 740  @ 1.73GHz\r\n CPU-ID Signature    : 0106E5\r\n CPU Features        : Floating-Point Unit on chip  : Yes\r\n                       Time Stamp Counter           : Yes\r\n                       Enhanced SpeedStep Technology: Yes\r\n                       Hyper-Threading Technology   : Yes\r\n                       Execute Disable protection   : Yes\r\n                       64-bit support               : Yes\r\n                       Virtualization Technology    : Yes\r\n Instr set extensions: MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\r\n Size of L1 cache    : 4 x 64 KB\r\n Integrated L2 cache : 4 x 256 KB\r\n Integrated L3 cache : 6144 KB\r\n", "The .whl file provided by @fo40225 solved this issue for me. \r\n\r\nOS Name\tMicrosoft Windows 10 Pro\r\nProcessor\tIntel(R) Core(TM) i5-3570K CPU @ 3.40GHz, 3401 Mhz, 4 Core(s), 4 Logical Processor(s)\r\nSystem Type\tx64-based PC\r\n\r\nHere is a list of CPU's compatible with AVX instructions used in version 1.6 of tensorflow. \r\nhttps://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX \r\n\r\nEDIT: It does seem like my Intel Core i5-3570K has support for AVX which makes me a bit suspicious whether or not my issue actually is fixed. \r\n\r\nWhen I validate the installation everything works, so I guess that's it! :) \r\nAlthough I get a warning when doing the installation validation recommended by tensorflow. ( https://www.tensorflow.org/install/install_windows )\r\n`\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('hello, Tensorflow!')\r\n>>> sess = tf.Session()\r\n2018-03-04 13:32:15.912873: I C:\\Users\\User\\Source\\Repos\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX\r\n>>> print(sess.run(hello))\r\nb'hello, Tensorflow!'\r\n>>>\r\n\r\n\r\nEDIT2: Btw, I'm using CUDA 9.1 which apparently isn't recommended with tensorflow 1.6.`But I think the .whl file @fo40225 provided allows for the latest version of CUDA in constrast to the tensorflow pip package. \r\n", "I've tried installing that whl a couple of times with no luck - ( I'm a python newbie just trying to do the hello  world tensorflow example).  I'm using the --ignore-installed flag.  I will uninstall it and try again, give it few more goes and post the results", "ok, success!  I uninstalled and re-installed the .whl by @fo40225  using the --upgrade flag and it worked!  Thanks", "Thanks a lot  @fo40225 . Fixed it", "and yet another guy reported this **UNDOCUMENTED** problem\r\nAnd the whl-solution doe snot work : is not a supported wheel on this platform", "Thanks to @fo40225 !", "downgraded tensorflow to version 1.4 and everything works like a charm)", "I have the same issue with version 1.6. Downgrading to version 1.4 fixed the problem. I'm using the CPU version only.", "Had the same issue using the CPU version while testing on an older computer - downgrading to 1.4 worked for me. This should probably fixed or at least the error message could be improved.", ".whl fixed the issue finally! Thanks a lot @fo40225", "@fo40225 !\r\nThank you so much! I have been trying to figure out this for a couple of days now. Your solution solved all of my problems!", "@fo40225 \r\nThank you ! Your solution fixed my problems.", "@fo40225 Thanks for the wheel. But it seems like TFLite is missing from the contrib !", "So thanks @fo40225 ! It really work for me.", "Thanks @fo40225  Installing the tensorflow-1.6.0-cp36-cp36m-win_amd64.whl helped fixed the issue for me. I really appreciate.", "**System Specs -**\r\n**CPU - AMD Athlon X2 250**\r\nIs it not supported?\r\nAlso, how exactly I use the **.whl file** to install and what are the limitations of doing so.\r\nThanks! @fo40225 \r\nJust need a little help over here.", "Your 1.8 with AVX2 worked on my machine\r\nBut my compiled 1.9RC2 with AVX2 doesn't\r\nAny idea?", ".whl file fixed it for me.\r\n\r\n@Curious-Nikhil just run it using pip install <path-to-file/filename>", "hi i had same problem trying to install tensorflow cpu\r\nwhen i installed tensorflow it got the error\r\n\"no modile named pywrap_tensorflow\"\r\n\r\nso i downgraded to tensorflow 1.5 \r\nthis time it gave me another error \r\n\"dll load failed coulnt find procedure \" \r\ncan some one help me with this", "I used the .whl file that @fo40225 said and it works like a charm..\r\nThanks to @fo40225", "Building with Bazel works on Windows", "I used the .whl file(TF 1.10.0) downloaded from @fo40225's Github repository said and it works without any issue..\r\nThanks to @fo40225", "After almost one week trying everything I solve the problem.\r\nFirst I had to unistall tensorflow and than just run pip install!\r\n\r\n@fo40225 you are the one!", "Attempt restoring spyder to default settings before going down the rabbit hole. This may save you days of sifting through the thousand different solutions out there.", "i have the same problem,\r\ni use AVX like @fo40225 say, but that not working !", "I fixed this issue with the version of tensorflow-gpu==1.12.0, python 3.6.8: \r\n1. Download CUDA 10.1 (the newest)\r\n2. Download cuDNN 7.5.0 (the newest)\r\n3. Install CUDA\r\n4. Extract cuDNN and copy to installed folder of CUDA\r\n5. Enjoin:\r\n![image](https://user-images.githubusercontent.com/40728496/54088644-d63b2f80-4392-11e9-8d4c-6e8527167550.png)\r\n", "@hanhtd2 That has nothing to do with any of this. This entire issue stems from a time where AVX was required for the binary available on Pip, but was not listed in the system requirements. So if you used a CPU that did not support AVX (i.e. an older one) it would not work. Your output shows that your CPU supports AVX...", "I had a similar problem and this worked for me\r\n\r\n1. Install anaconda\r\n2. Open anaconda prompt\r\n3.  `conda create -n tfp python=3.6`\r\n4. `activate tfp`\r\n5. `conda install tensorflow`\r\n6. `python`\r\n7. `import tensorflow`\r\n\r\nIf it works and you are using PyCharm go to File -> Settings ->Project Interpreter\r\nClick on top right button ->Add (Project Interpreter) -> Conda Environment ->Existing environment\r\nChoose --path--\\Anaconda3\\envs\\tfp\\python.exe", "run this code \"import tensorflow as tf\" and it report below errors:\r\n\r\nImportError                               Traceback (most recent call last)\r\nD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     34         return _mod\r\n---> 35     _pywrap_tensorflow_internal = swig_import_helper()\r\n     36     del swig_import_helper\r\n\r\nD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     29         try:\r\n---> 30             _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     31         finally:\r\n\r\nD:\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\nD:\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-6-8c0c2a0e5106> in <module>\r\n----> 1 import tensorflow as tf\r\n      2 # https://www.lfd.uci.edu/~gohlke/pythonlibs/#xgboost\r\n\r\nD:\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     20 \r\n     21 # pylint: disable=g-bad-import-order\r\n---> 22 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     23 from . import app\r\n     24 from . import bitwise\r\n\r\nD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 35, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 30, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nmy python is Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\n\r\nmy computer is Intel(R) Core(TM) i3 CPU 64bit with win7 Service Pack 1\r\n\r\ni have tried many different solutions but they still won't work. Anybody has met the same problem?\r\n", "ERROR: tensorflow-1.6.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\r\nwhat should i do to solve this \r\n", "I have AMD Ryzen 1600 CPU which supports both AVX and AVX2, and I cannot use the CPU version of Tensorflow either.\r\n\r\nStacktrace:\r\n```\r\nUsing TensorFlow backend.\r\nUsage: python -m flask run [OPTIONS]\r\n\r\nError: While importing \"app\", an ImportError was raised:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\Angius\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\flask\\cli.py\", line 240, in locate_app\r\n    __import__(module_name)\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\app.py\", line 2, in <module>\r\n    from textgenrnn import textgenrnn\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\textgenrnn\\__init__.py\", line 1, in <module>\r\n    from .textgenrnn import textgenrnn\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\textgenrnn\\textgenrnn.py\", line 1, in <module>\r\n    from keras.callbacks import LearningRateScheduler, Callback\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\Angius\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Angius\\PycharmProjects\\LNNameGen\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\nCan't install the provided alternative wheel either, because\r\n```\r\ntensorflow-1.6.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\r\n```", "Same\r\n>Can't install the provided alternative wheel either, because\r\ntensorflow-1.6.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\r\n\r\nPlz help", "After the installation I am able to import tensorflow thanks @fo40225 \r\n@pgavda just try to download the wheel from [link](https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.6.0/py36/CPU/sse2/tensorflow-1.6.0-cp36-cp36m-win_amd64.whl) and then run \"pip install --user tensorflow\" \r\nThen run \" import tensorflow as tf \"  \r\nIt worked for me.", "I am using **Python 3.6.0** in Windows 7 with an i5-6300U (which should have **AVX2 support**, according to [intel](https://ark.intel.com/content/www/us/en/ark/products/88190/intel-core-i5-6300u-processor-3m-cache-up-to-3-00-ghz.html)). I have tried installing **TensorFlow 1.7, 1.6 and 1.4** but nothing worked. The **wheel file** did **not work** either. Any idea of what can be the cause of my issue? I get this message when trying to include the module: \r\n\r\n![image](https://user-images.githubusercontent.com/36297047/72887277-ab886a00-3d0b-11ea-9608-8900c37eb271.png)\r\n\r\n[EDIT] @pgavda the \"cp36\" means CPython3.6, so you need to be using Python 3.6", "> The CPU lack AVX instruction can cause this error.\r\n> \r\n> I have built tensorflow wheel without AVX.\r\n> \r\n> https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.6.0/py36/CPU/sse2\r\n> \r\n> I wish this .whl may help you\r\nI had installed tensorflow 2.2 and facing the same error. Can you please elaborate on what should I do (step by step procedure would be more helpful)\r\n\r\n"]}, {"number": 17385, "title": "MKL: Removing unnecessary check for reorder", "body": "Fixes failure in //tensorflow/python/keras:pooling_test\r\n\r\n\"F tensorflow/core/kernels/mkl_input_conversion_op.cc:450] Check failed: tf_input.CheckReorderToOpMem( memory::primitive_desc(output_mkl_md, cpu_engine), tensor_out, &net) == true (0 vs. 1)\"", "comments": []}, {"number": 17384, "title": "broken Qualcomm link on tensorflow.org", "body": "On tensorflow.org there is a logo with a broken link to https://qualcomm.com/ (ERR_TIMED_OUT)\r\nLooks like it should be https://www.qualcomm.com/\r\n\r\nSorry, if creating an issue was wrong, but I did not find any contact possibilities.\r\n", "comments": ["@MarkDaoust Can you take a look at this?", "Fixed."]}, {"number": 17383, "title": "Performance dropped in cifar10 in TF 1.6.0 compiled with MKL", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.7/3.6\r\n- **Bazel version (if compiling from source)**: Build label: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)\r\n- **CUDA/cuDNN version**: V9.0.176\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n./configure\r\n```\r\nI enabled Jemalloc, XLA JIT, Amazon S3 support during configuration.\r\n```\r\nbazel build --config=opt --config=mkl --config=monolithic --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --copt=\"-DEIGEN_USE_VML\" //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nsudo pip install /tmp/tensorflow_pkg/tensorflow-1.6.0-cp27-cp27mu-linux_x86_64.whl\r\n```\r\n```\r\nexport OMP_NUM_THREADS=72\r\nexport KMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\nexport KMP_BLOCKTIME=1\r\nexport KMP_SETTINGS=1\r\nexport OMP_PROC_BIND=true\r\n\r\ngit clone https://github.com/tensorflow/models.git\r\ncd models/tutorials/image/cifar10\r\npython cifar10_train.py\r\n```\r\n### Hardware information\r\n-**Model Name**: Intel(R) Xeon(R) Platinum 8124M CPU @ 3.00GHz\r\n-**CPU(s)**: 72\r\n-**Architecture**:x86_64\r\n\r\n### Describe the problem\r\nI work with an AWS C5.18xlarge instance. \r\nWith TensorFlow 1.5.0 compiled with MKL and setting MKL parameters shown above, I can get a ~2000 examples/sec speed when running cifar10_train.py. But for TensorFlow 1.6.0 with exactly same building steps, I can only get speed of ~300 examples/sec.\r\n\r\n\r\n", "comments": ["I see this regression also. Training is 5 times slower with 1.6 vs. 1.5 on my slightly modified copy of the CIFAR10 example from https://github.com/tensorflow/models. CPU usage is 20X higher for that 5X slower training run. This translates to a 100X increase in CPU usage per batch. Something is very wrong.", "We have reproduced this problem within Intel and are working on a fix.", "Great! Thank you for such a quick turnaround!", "The regression has been fixed in master. The fix will be included in TF1.7.", "Did this fix make it to 1.7? I am still seeing a slowdown there (see the ref issue above for a script reproducing the problem)", "Yes, the fix is in 1.7. After some changes inTF 1.6, cifar10 was about 8x slower than TF 1.5. After the fix, TF 1.7 is now about 12% slower. The fix in TF 1.7 was temporary, it will be improved in the future versions.\r\n", "Then it seems that the slowdown I encountered is due to a different reason, since my code is still slow on 1.7 (about 100 times slower than in 1.4)."]}, {"number": 17382, "title": "https://www.tensorflow.org/versions/r1.6/ Dead link on versions page", "body": "On https://www.tensorflow.org/versions, there's a link to \r\nhttps://www.tensorflow.org/versions/r1.6/\r\nThis link is 404", "comments": ["@MarkDaoust Can you take a look at this?", "FIxed: https://www.tensorflow.org/versions/"]}, {"number": 17381, "title": "Cannot feed value of shape (64, 25, 9) for Tensor 'Placeholder:0', which has shape '(?, 25, 25)'", "body": "i am trying to train my RNN-LSTM model in python 3.5, this is my code and my dataset is a 3D accelerometer dataset\r\n\r\n```\r\nX = tf.placeholder(tf.float32, [None, config.n_steps, config.n_inputs])\r\nY = tf.placeholder(tf.float32, [None, config.n_classes])\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n\r\n    for epoch in range(training_epochs):\r\n        cost_history = np.empty(shape=[0],dtype=float)\r\n        for b in range(total_batches):\r\n            offset = (b * config.batch_size) % (train_y.shape[0] - config.batch_size)\r\n            batch_x = train_x[offset:(offset + config.batch_size), :, :]\r\n            batch_y = train_y[offset:(offset + config.batch_size), :]\r\n\r\n            print (\"batch_x shape =\",batch_x.shape)\r\n            print (\"batch_y shape =\",batch_y.shape)\r\n\r\n            _, c = sess.run([optimizer, cost],feed_dict={X: batch_x, Y : batch_y})\r\n            cost_history = np.append(cost_history,c)\r\n        loss_over_time[epoch] = np.mean(cost_history)\r\n```\r\nbut it gives me the following error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hp\\Downloads\\Deep-Learning-for-Human-Activity-Recognition-master\\ModelCreation\\RNN\\FFLSTM\\fflstm.py\", line 250, in <module>\r\n    _, c = sess.run([optimizer, cost],feed_dict={X: batch_x, Y : batch_y})\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1100, in _run\r\n    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\r\nValueError: Cannot feed value of shape (64, 25, 9) for Tensor 'Placeholder:0', which has shape '(?, 25, 25)'\r\n\r\n```\r\nand this is the shapes of my dataset\r\n```\r\nn_inputs len(X_train[0][0]) 25\r\nbatch_x shape = (64, 25, 9)\r\nbatch_y shape = (64, 2)\r\nX <tf.Tensor 'Placeholder:0' shape=(?, 25, 25) dtype=float32>\r\nY <tf.Tensor 'Placeholder_1:0' shape=(?, 2) dtype=float32>\r\n```", "comments": ["Well its kinda self explanatory, the placeholder `X` expected you to feed it tensors of shape (n, 25, 25) where n is (I guess) 64 in your case, but you fed it tensors of shape (n, 25, 9). I'm guessing you did a mistake while creating the `train_x` variable.\r\n\r\nEdit: Btw, this isn't the place to post these kind of questions, [this is](https://stackoverflow.com/questions/tagged/tensorflow).", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 17380, "title": "Branch 187471483", "body": "", "comments": ["@rmlarsen I'll start a pull after this PR is merged.", "Thanks for trying to resolve the conflicts, @benoitsteiner . can you take a look at the test failure after your resolution commit? https://source.cloud.google.com/results/invocations/f418fb3a-6960-4db9-8345-78d5576ee32c/targets/%2F%2Ftensorflow%2Fcore%2Fgrappler%2Foptimizers:loop_optimizer_test/tests", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@caisq The good new is that the failure is not due to a merge commit, just the fact that the \"Enter\" nodes created for the LoopOptimizerTest.RemovePush_NoOp test are not annotated with a frame name. Is there any way to edit the test to fix it ?", "@benoitsteiner the test was not failing before the merge commit. So I think it has something to do with the new PR that got merged (https://github.com/tensorflow/tensorflow/commit/84fe908258550e1ce27e8725de1e2af279479c9d) or the merge commit. Do you have time to take a look at that test? If so, it might be more efficient for you to do it rather than me given you are more familiar with that code. Either way let me know. Thanks.", "@caisq The test was creating a technically invalid graph, but only started failing before https://github.com/tensorflow/tensorflow/commit/84fe908258550e1ce27e8725de1e2af279479c9d got merged because https://github.com/tensorflow/tensorflow/commit/84fe908258550e1ce27e8725de1e2af279479c9d calls the function that requires the \"Enter\" nodes to be correctly created (i.e. annotated with a frame name). @rmlarsen will fix the test to add the frame names.", "@benoitsteiner Thanks for the explanation. @rmlarsen, should I disable the test for now?", "@caisq I just had a quick glance of the commits but it looks like some commits are already in master?", "@terrytangyuan I think something got messed up in the previous push(es). I just tried again and the old commits still show up. Let us merge this PR, which hopefully will straighten things out."]}, {"number": 17379, "title": "`tf.case` is not allowing the computation of the `default` tensor when it falls in the default case", "body": "```\r\n== cat /etc/issue ===============================================\r\nLinux tf-service 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux tf-service 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.0)\r\nprotobuf (3.5.1)\r\ntensorflow (1.6.0rc1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.6.0-rc1\r\ntf.GIT_VERSION = v1.6.0-rc0-19-gecec1d8\r\ntf.COMPILER_VERSION = v1.6.0-rc0-19-gecec1d8\r\nSanity check: array([1], dtype=int32)\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n\r\n### Describe the problem\r\n\r\n`tf.case` does not seem to be computing the graph tensor related with the `default` parameter when it should fall in it. It seems to be computing all the case tensors correctly in all the other cases\r\n\r\n\r\n### Source code / logs\r\n\r\nThe script optimizes a piecewise rectilinear function defined with `tf.case` to match the square of the argument as closely as possible in the interval (-10, 10). If the argument is greater or equal than 10, the case should be handled by the `default` argument tensor, which in the example below is `O1`, which is also the case tensor when the argument is between [-5, -1). After the function has been optimized, the script outputs the value of the piecewise function in the range (-100, 100), and we see that in the interval [10, 100), O1 seems to have the constant value -1.0 that does not depend on the input, while in the range [-5, -1) it correctly depends on it\r\n\r\n\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\nimport numpy as np\r\n\r\ninitia = tf.random_normal_initializer(5e-3, 1e-4)\r\nbias_init = tf.random_normal_initializer(-1e-3, 5*1e-4)\r\ncdtype = tf.float32\r\nDEPTH_1 = 1\r\nOUT_DEPTH = 1\r\n\r\ndef act(inp, **kwargs):\r\n  return tf.nn.elu(inp, name = kwargs['name'])\r\n\r\nI = tf.placeholder(cdtype, shape=[None,1], name='I') # input\r\n\r\nW = tf.get_variable('W', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb = tf.get_variable('b', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO = act(tf.matmul(I, W) + b, name='O') # activation / output\r\n\r\nW1 = tf.get_variable('W1', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb1 = tf.get_variable('b1', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO1 = act(tf.matmul(I, W1) + b1, name='O1')\r\n\r\nW2 = tf.get_variable('W2', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb2 = tf.get_variable('b2', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO2 = act(tf.matmul(I, W2) + b2, name='O2')\r\n\r\nW3 = tf.get_variable('W3', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb3 = tf.get_variable('b3', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO3 = act(tf.matmul(I, W3) + b3, name='O3')\r\n\r\nW4 = tf.get_variable('W4', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb4 = tf.get_variable('b4', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO4 = act(tf.matmul(I, W4) + b4, name='O4')\r\n\r\nW5 = tf.get_variable('W5', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb5 = tf.get_variable('b5', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO5 = act(tf.matmul(I, W5) + b5, name='O5')\r\n\r\neval_inp = tf.gather_nd(I,[[0,0]])\r\n\r\nc1 = tf.gather_nd(tf.less(I , [[-5.0]]), [[0,0]])[0]\r\nc2 = tf.gather_nd(tf.less(I , [[-1.0]]), [[0,0]])[0]\r\nc3 = tf.gather_nd(tf.less(I , [[0.0]]), [[0,0]])[0]\r\nc4 = tf.gather_nd(tf.less(I , [[1.0]]), [[0,0]])[0]\r\nc5 = tf.gather_nd(tf.less(I , [[5.0]]), [[0,0]])[0]\r\nc6 = tf.gather_nd(tf.less(I , [[10.0]]), [[0,0]])[0]\r\n\r\nf1 = lambda: O\r\nf2 = lambda: O1\r\nf3 = lambda: O2\r\nf4 = lambda: O3\r\nf5 = lambda: O4\r\nf6 = lambda: O5\r\nfdef = lambda: O1\r\n\r\ncaseFun = tf.case([(c1 , f1), (c2 , f2), (c3 , f3), (c4, f4), (c5, f5), (c6,f6)], default=fdef)\r\n\r\ndistance = tf.reduce_mean( tf.square( caseFun - tf.square(eval_inp) ) )\r\n\r\ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(distance)\r\ninit_op = tf.global_variables_initializer()\r\n\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(init_op)\r\n\r\n  for i in range(10000):\r\n    s = sess.run([\r\n      train_op,\r\n      I, caseFun, distance\r\n      ], feed_dict={ I: 20.0*np.random.rand(1,1) - 10.0})\r\n    if i % 1000 == 0:\r\n      print s\r\n\r\n  print \"W, b, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5\"\r\n  print sess.run([W, b, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5], feed_dict={})\r\n  for i in np.arange(-100.0, 100.0):\r\n    print sess.run([I, caseFun, distance , O1\r\n      ], feed_dict={ I: [[i]]})\r\n```\r\n\r\nThe output looks like this:\r\n\r\n```\r\n....\r\n[array([[-10.]], dtype=float32), array([[43.378143]], dtype=float32), 3206.0347, array([[27.115139]], dtype=float32)]\r\n[array([[-9.]], dtype=float32), array([[39.456512]], dtype=float32), 1725.8613, array([[24.640066]], dtype=float32)]\r\n[array([[-8.]], dtype=float32), array([[35.53488]], dtype=float32), 810.26294, array([[22.164993]], dtype=float32)]\r\n[array([[-7.]], dtype=float32), array([[31.613249]], dtype=float32), 302.2991, array([[19.68992]], dtype=float32)]\r\n[array([[-6.]], dtype=float32), array([[27.691616]], dtype=float32), 69.02924, array([[17.214848]], dtype=float32)]\r\n[array([[-5.]], dtype=float32), array([[14.739774]], dtype=float32), 105.27224, array([[14.739774]], dtype=float32)]\r\n[array([[-4.]], dtype=float32), array([[12.264701]], dtype=float32), 13.952459, array([[12.264701]], dtype=float32)]\r\n[array([[-3.]], dtype=float32), array([[9.789628]], dtype=float32), 0.62351245, array([[9.789628]], dtype=float32)]\r\n[array([[-2.]], dtype=float32), array([[7.314555]], dtype=float32), 10.986276, array([[7.314555]], dtype=float32)]\r\n[array([[-1.]], dtype=float32), array([[0.6768117]], dtype=float32), 0.10445068, array([[4.8394823]], dtype=float32)]\r\n[array([[0.]], dtype=float32), array([[0.02995399]], dtype=float32), 0.00089724176, array([[2.3644097]], dtype=float32)]\r\n[array([[1.]], dtype=float32), array([[4.860016]], dtype=float32), 14.899722, array([[-0.10475975]], dtype=float32)]\r\n[array([[2.]], dtype=float32), array([[7.3534083]], dtype=float32), 11.245347, array([[-0.9246594]], dtype=float32)]\r\n[array([[3.]], dtype=float32), array([[9.846801]], dtype=float32), 0.7170716, array([[-0.99365956]], dtype=float32)]\r\n[array([[4.]], dtype=float32), array([[12.340193]], dtype=float32), 13.394189, array([[-0.9994664]], dtype=float32)]\r\n[array([[5.]], dtype=float32), array([[24.035427]], dtype=float32), 0.9304009, array([[-0.9999551]], dtype=float32)]\r\n[array([[6.]], dtype=float32), array([[28.001446]], dtype=float32), 63.97687, array([[-0.99999624]], dtype=float32)]\r\n[array([[7.]], dtype=float32), array([[31.96746]], dtype=float32), 290.1074, array([[-0.9999997]], dtype=float32)]\r\n[array([[8.]], dtype=float32), array([[35.93348]], dtype=float32), 787.72955, array([[-1.]], dtype=float32)]\r\n[array([[9.]], dtype=float32), array([[39.899498]], dtype=float32), 1689.2512, array([[-1.]], dtype=float32)]\r\n[array([[10.]], dtype=float32), array([[-1.]], dtype=float32), 10201.0, array([[-1.]], dtype=float32)]\r\n[array([[11.]], dtype=float32), array([[-1.]], dtype=float32), 14884.0, array([[-1.]], dtype=float32)]\r\n.....\r\n```", "comments": []}, {"number": 17378, "title": "Please explain what is going on in tf.nn.raw_rnn function?", "body": "I am trying to implement custom hidden state computation with a help of `tf.nn.raw_rnn` function. However, using the API example provided I am receiving a strange message:\r\n\r\n```\r\n<tensorflow.python.util.tf_should_use._add_should_use_warning.<locals>.TFShouldUseWarningWrapper at 0x7f80ac138dd8>\r\n```\r\n\r\n\r\n![2018-03-02-143635_1049x358_scrot](https://user-images.githubusercontent.com/7676160/36901389-45c88efc-1e27-11e8-93b1-f42c3018057e.png)\r\n\r\nWhat I want to do is the following:\r\n\r\n```\r\n                    |--   hidden recurrent layer   --|\r\n[10 x 32 x 100] --> LSTM cell [200] + Linear [200 x 1] --> [10 x 32 x 1]\r\n```\r\n\r\nHere is my implementation, well basically it is the example from official docs:\r\n\r\n```python\r\n   outputs, state = self._get_raw_rnn_graph(inputs, config, is_training)\r\n\r\n   def _get_raw_rnn_graph(self, inputs, config, is_training):\r\n        time = tf.constant(0, dtype=tf.int32)\r\n        # define placeholders\r\n        #_inputs = tf.placeholder(shape=(config.num_steps, config.batch_size, config.input_size),\r\n        #                         dtype=tf.float32)\r\n        #_batch_len = tf.placeholder(shape=(config.batch_size,), dtype=tf.int32)\r\n        _inputs_ta = tf.TensorArray(dtype=tf.float32, size=config.num_steps, name=\"TA\")\r\n        _inputs_ta = _inputs_ta.unstack(inputs)  # <-- throws a warning \r\n\r\n        # create simple LSTM cell\r\n        cell = tf.contrib.rnn.LSTMCell(config.hidden_size)\r\n\r\n        # create loop_fn for raw_rnn\r\n        def loop_fn(time, cell_output, cell_state, loop_state):\r\n            emit_output = cell_output  # == None if time = 0\r\n\r\n            if cell_output is None:  # time = 0\r\n                next_cell_state = cell.zero_state(config.batch_size, tf.float32)\r\n                self._initial_state = next_cell_state\r\n            else:\r\n                next_cell_state = cell_state\r\n\r\n            elements_finished = (time >= config.num_steps)\r\n            finished = tf.reduce_all(elements_finished)\r\n            next_input = tf.cond(finished,\r\n                                 lambda: tf.zeros([config.batch_size, config.input_size],\r\n                                                   dtype=tf.float32),\r\n                                 lambda: _inputs_ta.read(time))\r\n\r\n            # apply linear + sig transform here\r\n            print(\"before lin+sig\", next_input)\r\n            next_input = self._linear_transform(next_input)  # [32, 200] --> [32, 1]\r\n            print(\"after lin+sig\", next_input)\r\n\r\n            next_loop_state = None\r\n            return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\r\n\r\n        outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)\r\n        outputs = outputs_ta.stack()\r\n        return outputs, final_state\r\n```\r\n\r\nThe line `_inputs_ta = _inputs_ta.unstack(inputs)` causes the message above and I wonder if this is actually a bug or am I doing something wrong completely. The end result is that I am not getting the correct output shape because the `next_input` comes as `[32, 100]` into my linear transform function as if LSTM cell was never applied to it. Please clarify whether `raw_rnn` is usable under Tensorflow 1.5.0. \r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not clear if this is a bug or a usage error. There is also a larger community that reads questions there.\r\n\r\nIf you've determined that this certainly a bug or bad documentation, please file another issue. Thanks!"]}, {"number": 17377, "title": "Attempt failed to use tf.ConditionalAccumulator.take_grad() with GradientDescentOptimizer.apply_gradients", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Home 64-bit\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n3.6.3\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n8.0/6.0\r\n- **GPU model and memory**:\r\nNVIDIA 1050Ti 4Gb\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nI have implemented 3 convolutional neural network and I have used one of the `tf.ConditionalAccumulator` for each network to aggregate the partial gradients during the training for every network. More precisely, I have used `tf.GradientDescentOptimizer.compute_gradients(<a_network_loss>,<trainable_var_of_a_network>)` to take these partial gradients **feeding** the necessary placeholders to calculate the loss with partial values.\r\n\r\nAfter accumulating all the necessary gradients for all networks, I have used `tf.ConditionalAccumulator.take_grad()` to take the whole gradients for each `tf.ConditionalAccumulator`, then I have zipped all the gradients with variables name and finally I have used `tf.GradientDescentOptimizer.apply_gradients(<zip>)`.\r\nTensorflow returns error when get called the apply_gradients because it wants  that palceholders will be **feeded** again\r\n\r\n### Source code \r\nThis code is just an example to show the working of my code for one tf.ConditionalAccumulator:\r\n\r\n```python\r\noptimizer=tf.GradientDescentOptimizer(lr)\r\naccumulator=tf.ConditionalAccumulator(tf.float32,shape=[3,3,3,1,64])\r\ngrad_ph=tf.placeholder(tf.float32)        \r\naccumulator_add_gradient_op=accumulator.apply_grad(grad)\r\ngrads_vars_=self.optimizer.compute_gradients(first_net.loss, var_list=first_net.vars)\r\ngrads_list=[grad_var[0] for grad_var in grads_vars]\r\nvars_list=[grad_var[1] for grad_var in grads_vars]\r\n\r\ncount_ph=tf.placeholder(tf.int32,[])\r\ngrads_us=tf.unstack(accumulator.take_grad(count_ph))\r\n\r\nn=0\r\nwhile(...):\r\n    ...\r\n    grads=sess.run(grads_list,{<feeding_placeholders_with_partial_values>})\r\n    sess.run(accumulator_add_gradient_op,{grad_ph:grads}\r\n    n+=1\r\ngrads_and_vars=list(zip(sess.run(grads_us,{count_ph:n}),vars_list))\r\nsess.run(optimizer.apply_gradients(grads_and_vars))\r\n```\r\n\r\nI have used `list` in `grads_and_vars=list(zip(sess.run(grads_us,{count_ph:n}),vars_list))` because i need to use `grads_and_vars.extend(<other_grads_and_vars>)` for other tf.ConditionalAccumulator. I tried not used sess.run() in zip(), but have the same problem.\r\n\r\n### logs\r\n```\r\n2018-03-02 11:52:16.233461: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.233946: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.234279: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.234580: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.235302: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.235649: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.235928: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.236210: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.236497: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.236802: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.237108: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.237419: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.237746: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.238071: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.238404: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.238748: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.240460: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.240933: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.241344: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.241868: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.242252: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.242531: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.242811: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.243095: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.243858: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.244213: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.244493: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.244773: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.245057: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.245343: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.246162: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.246591: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.246872: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.247152: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.247492: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.248138: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.248582: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.249021: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.249406: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.249970: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.250396: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.250990: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.251452: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.252026: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.252518: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.253006: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.253455: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.253760: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.254429: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.254957: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.255390: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.255867: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-03-02 11:52:16.256557: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\nTraceback (most recent call last):\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\t [[Node: first_CCNN/conv4/weights/read/_1163 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_958_first_CCNN/conv4/weights/read\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\User\\Desktop\\LiClipse Workspace\\CCNN+\\src\\main.py\", line 44, in <module>\r\n    s.train(args)\r\n  File \"D:\\User\\Desktop\\LiClipse Workspace\\CCNN+\\src\\temp.py\", line 46, in train\r\n    self.train_all(args)    \r\n  File \"D:\\User\\Desktop\\LiClipse Workspace\\CCNN+\\src\\temp.py\", line 416, in train_all\r\n    _,total_loss,summary=self.sess.run([self.optimizer.apply_gradients(grads_and_vars),self.loss,self.summary_op],{self.learning_rate:lr})\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\t [[Node: first_CCNN/conv4/weights/read/_1163 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_958_first_CCNN/conv4/weights/read\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'gt1', defined at:\r\n  File \"D:\\User\\Desktop\\LiClipse Workspace\\CCNN+\\src\\main.py\", line 42, in <module>\r\n    load_first_model=args.load_first_model)\r\n  File \"D:\\User\\Desktop\\LiClipse Workspace\\CCNN+\\src\\temp.py\", line 31, in __init__\r\n    self.first_CCNN=CCNN(sess,load_model=self.load_first_model,patch_size=self.patch_size,batch_size=self.batch_size,isTraining=self.train_first_model,version=self.first_CCNN_version,model_name=first_CCNN_model_path)\r\n  File \"D:\\User\\Desktop\\LiClipse Workspace\\CCNN+\\src\\model.py\", line 36, in __init__\r\n    self.build()\r\n  File \"D:\\User\\Desktop\\LiClipse Workspace\\CCNN+\\src\\model.py\", line 46, in build\r\n    self.CCNN_build()\r\n  File \"D:\\User\\Desktop\\LiClipse Workspace\\CCNN+\\src\\model.py\", line 127, in CCNN_build\r\n    self.gt = tf.placeholder(tf.float32, [None,1,1, 1], name='gt1')\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1599, in placeholder\r\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3090, in _placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"D:\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'gt1' with dtype float and shape [?,1,1,1]\r\n\t [[Node: gt1 = Placeholder[dtype=DT_FLOAT, shape=[?,1,1,1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\t [[Node: first_CCNN/conv4/weights/read/_1163 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_958_first_CCNN/conv4/weights/read\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17376, "title": "Fix nsync build issue", "body": "", "comments": []}, {"number": 17375, "title": "tensorflow lite cannot use my own model, crashed without error log.", "body": "**Version Info:** \r\ntensorflow r1.5 \r\nubuntu 14.04 \r\narmv7 platform \r\nandroid 6.0.1 and pc\r\nndk version: r14 \r\nandroid studio 2.3.1 \r\n\r\n**Describe the problem**\r\ni write the code in c++, it can get results when using the mobilenet model from the tutorails(link: https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip)\r\nbut when i use my own model converted by toco, it crashed without any information. set input and load model seemed correct, but invoke failed.\r\n\r\n**code**\r\n\r\n**network define**\r\nnetwork is defined by tf.slim framework:\r\n```\r\n def lite_v1(images, num_classes=10, is_training=False, dropout_keep_prob=0.5, prediction_fn=slim.softmax, scope='lite_v1'):\r\n    end_points = {}\r\n      with tf.variable_scope(scope, 'lite_v1', [images, num_classes]):\r\n        images_input = tf.placeholder_with_default(images, shape=[None, 39, 39, 1], name='InputPlaceholder')\r\n        net = slim.conv2d(images_input, 16, [4, 4], padding='VALID', scope='conv1')\r\n        end_points['conv1'] = net\r\n        feature = slim.flatten(net)\r\n    with tf.variable_scope('Logits'):\r\n      net = slim.fully_connected(feature, 100, scope='fc3')\r\n      logits = slim.fully_connected(net, num_classes,\r\n                              biases_initializer=tf.zeros_initializer(),\r\n                              weights_regularizer=None,\r\n                              activation_fn=None,\r\n                              scope='logits')\r\n      end_points['Logits'] = logits\r\n    output = tf.multiply(logits, 1, name=\"Output\")\r\n    end_points['Output'] = output\r\n  return logits, end_points\r\n```\r\n**model convert**\r\ni trained it and use the freeze_graph.py and optimize_for_inference.py scripe in tensorflow/python/tools/ and get a frozen .pb file. then i convert it to a .tflite file by following the tutorials in github: https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/lite just like:\r\n```\r\nbazel build tensorflow/contrib/lite/toco:toco\r\nbazel-bin/tensorflow/contrib/lite/toco/toco -- \\\r\n  --input_file=$(pwd)/mobilenet_v1_1.0_224/frozen_graph.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n  --output_file=/tmp/mobilenet_v1_1.0_224.lite --inference_type=FLOAT \\\r\n  --input_type=FLOAT --input_arrays=input \\\r\n  --output_arrays=MobilenetV1/Predictions/Reshape_1 --input_shapes=1,224,224,3\r\n```\r\n\r\n**c++ test code**\r\nafter get a .tflite file. and then i wrote the c++ code to read the model and image from sdcard:\r\n```\r\nvoid LoadImageFromFile(std::string file_name, std::vector<uint8_t>& output, int& out_width, int& out_height, int& out_channels)\r\n{\r\ncv::Mat image = cv::imread(file_name);\r\ncv::Mat gray;\r\ncv::cvtColor(image, gray, cv::COLOR_BGR2GRAY);\r\n\r\nout_width = gray.cols;\r\nout_height = gray.rows;\r\nout_channels = gray.channels();\r\nfor(int nrow =0; nrow < out_height; nrow++)\r\n    for(int ncol = 0; ncol < out_width; ncol++)\r\n        output.push_back(gray.at<unsigned char>(nrow,ncol));\r\n}\r\nsize_t writeByteBuffer(uint8_t* in, char** dst, int dst_size) {\r\n  char* buf = (char*)in;\r\n  if (!buf) {\r\n    return 0;\r\n  }\r\n  *dst = buf;\r\n  return dst_size;\r\n}\r\n\r\nint RunInferenceOnImage()\r\n{\r\nconst int num_threads = 1;\r\nstd::string input_layer_type = \"float\";\r\nstd::vector<int> sizes = {1, 39, 39, 1};\r\n//    std::vector<int> sizes = {1, 224, 224, 3};\r\nstd::string graph_path = \"/storage/emulated/0/DCIM/fastnetv2.tflite\"; //mobilenet_quant_v1_224.tflite\r\nstd::unique_ptr<tflite::FlatBufferModel> model(tflite::FlatBufferModel::BuildFromFile(graph_path.c_str()));\r\nif (!model)\r\n{\r\n    LOGD(\"bkzero jni: , Failed to mmap model %s\", graph_path.c_str());\r\n}\r\nmodel->error_reporter();\r\n\r\n#ifdef TFLITE_CUSTOM_OPS_HEADER\r\n  tflite::MutableOpResolver resolver;\r\n  RegisterSelectedOps(&resolver);\r\n#else\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n#endif\r\n\r\nLOGD(\"bkzero jni: , %s\", \"resolved reporter end\");\r\n\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\nif (!interpreter)\r\n{\r\n    LOGD(\"bkzero jni: , %s\", \"Failed to construct interpreter\");\r\n}\r\nif (num_threads != -1) {\r\n  interpreter->SetNumThreads(num_threads);\r\n}\r\nint input = interpreter->inputs()[0];\r\nif (input_layer_type != \"string\") {\r\n  interpreter->ResizeInputTensor(input, sizes);\r\n}\r\n\r\nif (interpreter->AllocateTensors() != kTfLiteOk)\r\n{\r\n    LOGD(\"bkzero jni: , %s\", \"Failed to allocate tensors!\");\r\n}\r\nstd::string image_path = \"/storage/emulated/0/DCIM/test2.jpg\";\r\nint image_width;\r\nint image_height;\r\nint image_channels;\r\nstd::vector<uint8_t> image_data;\r\nLoadImageFromFile(image_path, image_data, image_width, image_height, image_channels);\r\n\r\n//    const int wanted_width = 224;\r\n//    const int wanted_height = 224;\r\n//    const int wanted_channels = 3;\r\n//    const float input_mean = 127.5f;\r\n//    const float input_std = 127.5f;\r\n\r\nconst int wanted_width = 39;\r\nconst int wanted_height = 39;\r\nconst int wanted_channels = 1;\r\nconst float input_mean = 128.0f;\r\nconst float input_std = 64.0f;\r\nassert(image_channels >= wanted_channels);\r\nuint8_t* in = image_data.data();\r\nfloat* out = interpreter->typed_tensor<float>(input);\r\nint input_idx = interpreter->inputs()[0];\r\nTfLiteTensor* target = interpreter->tensor(input_idx);\r\nint num_bytes = sizes[0]*sizes[1]*sizes[2]*sizes[3];\r\nwriteByteBuffer(in, &(target->data.raw), static_cast<int>(num_bytes));\r\n    if (interpreter->Invoke() != kTfLiteOk)\r\n    {\r\n        LOGD(\"bkzero jni: , %s\", \"Failed to invoke!\");\r\n    }\r\n    const std::vector<int>& results = interpreter->outputs();\r\n    if (results.empty()) {\r\n      LOGD(\"bkzero jni: , %s\", \"results.empty()\");\r\n    }\r\n    long outputs[results.size()];\r\n    size_t size = results.size();\r\n    for (int i = 0; i < size; ++i) {\r\n      TfLiteTensor* source = interpreter->tensor(results[i]);\r\n      outputs[i] = reinterpret_cast<long>(source);\r\n    }\r\n    uint8_t* final = (uint8_t*)outputs[0];\r\n    for(int i = 0; i < 10; i++)\r\n        LOGD(\"bkzero jni: results, %f\", (final[i]/255.0));\r\n    LOGD(\"bkzero jni: , %s\", \"interpreter outputs read finished\");\r\n}\r\n```\r\n\r\nthis code can read and ouput result correct when run the example mobilenet file, mobilenet_quant_v1_224.tflite. but crash when i run my own model and give the error: Fatal signal 7 (SIGBUS), code 1, fault addr 0xdd3dd008 in tid 10528\r\n\r\nand on PC, it also crashes, without more information.", "comments": ["i am sorry i am new to github and not familar to the issue editor, so the code block was mass. \r\nnow it is clear to read the code.", "the inference type of mobilenet_quant_v1_224.tflite is quantized uint8, but the inference type of your model is float32 `...--inference_type=FLOAT...`. You may wanna take a look at [label_image for TFLite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/label_image/label_image.cc) on dealing with difference inference type", "Thanks @freedomtan I'll close this one. @BKZero either use a quantized version of mobilenet, or change your code to read data from interpreter->typed_tensor<float>(results[i]). You can also check  the type of the input tensor when you get interpreter->tensor(results[i]) to make sure you can access it properly.", "In my experience the your input index is wrong.\r\nIt should be: `float* out = interpreter->typed_tensor<float>(0);`\r\ninstead of `float* out = interpreter->typed_tensor<float>(input);`\r\n", "@BKZero Did you resolve this problem ? \r\nI'm also facing same issue.\r\n\r\nOther tflite model like DeepLab, MobileNet are running but when I try to load my own model app crashes.. "]}, {"number": 17374, "title": "Potential resource leaks caused by unclear Java examples", "body": "### System information\r\n\r\nJava examples at https://www.tensorflow.org/ for tensorflow 1.6.0.\r\n\r\n### Describe the problem\r\n\r\n`org.tensorflow.Session.Runner.run()` returns list of closables, Javadoc clearly states that the caller is responsible to free all of them. None of the Java examples I found at https://www.tensorflow.org/ highlights that, I realized it by happy accident during in-depth reading implementation in Session.java quite long time after I wrote my code that uses TensorFlow.\r\n\r\n```\r\n    /**\r\n     * Execute the graph fragments necessary to compute all requested fetches.\r\n     *\r\n     * <p><b>WARNING:</b> The caller assumes ownership of all returned {@link Tensor}s, i.e., the\r\n     * caller must call {@link Tensor#close()} on all elements of the returned list to free up\r\n     * resources.\r\n     *\r\n     * ...\r\n     */\r\n    public List<Tensor<?>> run() {\r\n      return runHelper(false).outputs;\r\n    }\r\n```\r\n\r\nI'm not sure if the examples them-self contain any resource leak or not, they free only the first element of the list, but there may be more of them (in general). I would expect an explicit loop properly freeing all the returned resources there.\r\n\r\nSuch examples for beginners should be as explicit as possible, 100% clear and understandable for anyone. A lot of people (like me) base core of their code on them which may easily introduce significant resource leak bugs to their applications.\r\n\r\n- https://www.tensorflow.org/install/install_java, HelloTF example\r\n- https://www.tensorflow.org/install/install_java, referenced advanced example LabelImage\r\n- https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java\r\n\r\n### Source code / logs\r\n\r\nNone.\r\n", "comments": ["@asimshankar Can you take a look at this?", "@mixalturek : Thanks for the note and yes we'd like to make documentation and examples as clear as possible.\r\n\r\nThe examples referenced only fetch a single tensor, hence the list returned necessarily has a single element (and since that is referenced in the try-with-resources blocks, there is no leak). When the example is explicitly requesting a single tensor, the loop seems unnecessary.\r\n\r\nHappy to hear any suggestions for improvement though.", "Yes, I guessed there is only a single one so the code is correct. So what about to add a comment with an explicit info.\r\n\r\n```\r\n      // Execute the \"MyConst\" operation in a Session.\r\n      try (Session s = new Session(g);\r\n           // Generally, there may be multiple output tensors, all of them must be closed to prevent resource leaks.\r\n           Tensor output = s.runner().fetch(\"MyConst\").run().get(0)) {\r\n        System.out.println(new String(output.bytesValue(), \"UTF-8\"));\r\n      }\r\n```", "Sure. A PR would be welcome :)", "The fix PR is merged, so I think we can close this."]}, {"number": 17373, "title": "Illegal instruction on import tensorflow", "body": "I had installed a non-optimized version of tensorflow (1.6.0) on debian (Debian GNU/Linux 8) for use within python 3.6.4.\r\nI was told that SSE4.1 and SSE4.2 instructions were supported by my CPU but that the tensorflow I was using was not compiled to support it, so I decided to use one of the pre-compiled whl's.\r\nThis resulted in a \"illegal instruction\" error which exits python immediately.\r\nI tried uninstalling and reinstalling the original version but the error message keeps coming back...\r\nI cleared the pip cache as well, yet still nothing seems to help.\r\n\r\nAny idea on how to resolve this? \r\nIt almost appears like uninstalling doesn't remove everything?\r\n(I know some may suggest that I work with docker, yet that still leaves me with a broken environment...)\r\n\r\nOS Platform and Distribution: Debian GNU/Linux 8\r\nTensorflow installed from pip: pip install --no-cache-dir tensorflow (after uninstalling everything TF related)\r\nTF version: 1.6.0\r\nBazel: not installed\r\nCUDA/cuDNN: not installed\r\nGPU model: not available\r\nExact command to reproduce: import tensorflow as tf", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I updated my system and software specifications. Thank you!", "I'm getting the same error on `import tensorflow as tf`. The program says \"Illegal Command\" and exits. No further info. \r\n\r\nI'm running on a ARMv6 raspberry pi, installed from `http://ci.tensorflow.org/view/Nightly/job/nightly-pi-zero/lastSuccessfulBuild/artifact/output-artifacts/tensorflow-1.7.0-cp27-none-any.whl` using the `--no-cache-dir` flag. The install went off without a hitch, but I get this runtime error. \r\n\r\nwith GDB (which I'm not super familiar with)\r\n```\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/arm-linux-gnueabihf/libthread_db.so.1\".\r\n\r\nProgram received signal SIGILL, Illegal instruction.\r\n0xb4c21b54 in tensorflow::TensorShapeProto_Dim::SharedCtor() ()\r\n   from /home/pi/[my project]/.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```\r\n", "Nagging Assignee @cy89: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Djoels  and @jattenberg can you post more exhaustive error logging, please, including the command that triggered the errors? \r\n\r\nIt seems unlikely to me that this is the same problem, given that one of you is on x86 and one is on ARM. But they could both be build/configuration issues. ", "Hi @Cliff, the problem was eventually solved by manually compiling a\nversion that had only the right GPU compile flags set.\nWhat I can't understand is why uninstalling the GPU enabled version and\nreinstalling the CPU only version didn't work. It kept failing at import.\nCould it be that the uninstall is not exhaustive? Sadly we can't reproduce\nthis error at this point.\n\nJulien\n\nOn Wed, May 9, 2018 at 8:03 PM, Cliff Young <notifications@github.com>\nwrote:\n\n> @Djoels <https://github.com/Djoels> and @jattenberg\n> <https://github.com/jattenberg> can you post more exhaustive error\n> logging, please, including the command that triggered the errors?\n>\n> It seems unlikely to me that this is the same problem, given that one of\n> you is on x86 and one is on ARM. But they could both be build/configuration\n> issues.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17373#issuecomment-387823784>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AITf4tKAULx5uigcLYEmyK6rq4pt5apyks5twy9jgaJpZM4SZiSs>\n> .\n>\n", "Nagging Assignee @cy89: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Djoels I think we're stuck without reproducibility, alas. It does seem reasonable that uninstall isn't perfect. "]}, {"number": 17372, "title": "tf.svd and matlab svd return different results with the same input. Why?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 6.0\r\n- **GPU model and memory**: Nvidia GTX 750 Ti, 2G\r\n\r\n### Describe the problem\r\ntf.svd and matlab svd return different results with the same input. Why?\r\n\r\n### Source code / outputs\r\n**TensorFlow tf.svd**\r\nTensorFlow demo code:\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.set_random_seed(1)\r\n''' \r\noriginal matrix \r\nX =\r\n [-0.67086124  0.22357143  0.79727304\r\n   0.09617059  0.72314787  0.33812162\r\n  -0.73006123  0.91153419 -0.50938189\r\n   0.97486407 -2.13402033  0.60229129\r\n  -0.31214711 -0.85811871  1.78664649]\r\n''' \r\nX = tf.random_normal([5, 3], mean=0.0, stddev=1.0, dtype=tf.float32)\r\n\r\ns, u, v = tf.svd(X)\r\n\r\n\r\nwith tf.Session() as sess:\r\n    print('X=\\n', sess.run(X))\r\n    print('s=\\n', sess.run(s))\r\n    print('u=\\n', sess.run(u))\r\n    print('v=\\n', sess.run(v))\r\n\r\n```\r\nMatlab output:\r\n```\r\ns=\r\n [ 3.6135273   2.42769122  0.81916636]\r\nu=\r\n [[ 0.06457954  0.5192402  -0.50217324]\r\n [-0.52760589 -0.70443964 -0.21893539]\r\n [ 0.7611897  -0.46179301 -0.45036247]\r\n [ 0.32708997 -0.12951432  0.65888679]\r\n [-0.17625111  0.06424804 -0.25086099]]\r\nv=\r\n [[ 0.18638727  0.92181212 -0.33988595]\r\n [ 0.95351493 -0.2531015  -0.16355196]\r\n [ 0.23678979  0.29360226  0.92613626]]\r\n```\r\n**Matlab svd**\r\nMatlab demo code:\r\n```\r\nX = ....\r\n [-0.67086124  0.22357143  0.79727304\r\n   0.09617059  0.72314787  0.33812162\r\n  -0.73006123  0.91153419 -0.50938189\r\n   0.97486407 -2.13402033  0.60229129\r\n  -0.31214711 -0.85811871  1.78664649]\r\n[u, s, v] = svd(X)\r\n```\r\nMatlab output:\r\n```\r\nu =\r\n    0.0060    0.5579    0.2153    0.4231   -0.6807\r\n   -0.1317    0.2258   -0.8417    0.4338    0.1873\r\n   -0.3964    0.1323    0.4871    0.5077    0.5747\r\n    0.7643   -0.3502    0.0765    0.5309    0.0739\r\n    0.4913    0.7054    0.0457   -0.3051    0.4072\r\ns =\r\n    3.0475         0         0\r\n         0    1.8932         0\r\n         0         0    0.6537\r\n         0         0         0\r\n         0         0         0\r\nv =\r\n    0.2837   -0.5339   -0.7965\r\n   -0.8229    0.2909   -0.4880\r\n    0.4923    0.7939   -0.3568\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17371, "title": "minor spelling tweaks for contrib/verbs docs", "body": "", "comments": []}, {"number": 17370, "title": "Image retraining script memory problem and issue", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n      Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6-gpu/nightly-gpu\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0/7.04\r\n- **GPU model and memory**: Tesla K80 / 11441MiB\r\n- **Exact command to reproduce**:\r\npython retrain_quantize.py w/ certain parameters.\r\n\r\n\r\n### Describe the problem\r\nI was trying the new retrain script on my own model. (In order to fully convert quantized model to tflite)\r\n1. Different memory usage in different version.\r\nI opened allow_growth parameter in order to trace memory usage during training.\r\nIn tf-gpu-1.6.0 :\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                                      GPU Memory |\r\n|  GPU       PID   Type   Process name                                                    Usage      |\r\n|======================================================== |\r\n|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |\r\n|    0     15440      C   python                                                                     302MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nBut in tf-nightly-gpu:\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                                      GPU Memory |\r\n|  GPU       PID   Type   Process name                                                    Usage      |\r\n|======================================================== |\r\n|    0      1673      G   /usr/lib/xorg/Xorg                                                        15MiB |\r\n|    0     15747      C   python                                                                    4152MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\nI was wondering what causes the tremendous difference in these two versions?\r\n\r\n2. Per the traceback below, my retraining process could not be done in the last step. \r\n    Due to the feed_dict issue. I saw my process restart a session after 100 steps, could the restart \r\n    cause the loss of DecodeJPGInput tensor?\r\n\r\n```\r\nAfter last-1 steps, system shows:\r\n2018-03-02 04:49:23.569386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-03-02 04:49:23.569450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-03-02 04:49:23.569460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0\r\n2018-03-02 04:49:23.569464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N\r\n2018-03-02 04:49:23.569688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10750 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n\r\n```\r\nThanks in advance!\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1080, in _run\r\n    subfeed, allow_tensor=True, allow_operation=False)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3458, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3537, in _as_graph_element_locked\r\n    raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\r\nValueError: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"retrain_quantize.py\", line 355, in create_bottleneck_file\r\n    resized_input_tensor, bottleneck_tensor)\r\n  File \"retrain_quantize.py\", line 290, in run_bottleneck_on_image\r\n    {image_data_tensor: image_data})\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1083, in _run\r\n    'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\r\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"retrain_quantize.py\", line 1411, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"retrain_quantize.py\", line 1219, in main\r\n    export_model(model_info, class_count, FLAGS.saved_model_dir)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1558, in __exit__\r\n    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5059, in get_controller\r\n    yield g\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 77, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 4869, in get_controller\r\n    yield default\r\n  File \"/home/cheyu.lin/tf-night/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 5059, in get_controller\r\n    yield g\r\n  File \"retrain_quantize.py\", line 1211, in main\r\n    bottleneck_tensor)\r\n  File \"retrain_quantize.py\", line 813, in run_final_eval\r\n    bottleneck_tensor, FLAGS.architecture))\r\n  File \"retrain_quantize.py\", line 505, in get_random_cached_bottlenecks\r\n    resized_input_tensor, bottleneck_tensor, architecture)\r\n  File \"retrain_quantize.py\", line 400, in get_or_create_bottleneck\r\n    bottleneck_tensor)\r\n  File \"retrain_quantize.py\", line 358, in create_bottleneck_file\r\n    str(e)))\r\nRuntimeError: Error during processing file /home/cheyu.lin/master_dataset/dog/source_dog_wash_011682.jpg (Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"DecodeJPGInput:0\", dtype=string) is not an element of this graph.)\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-lite) since it is not a  bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you can concretely identify a bug that is causing this, please file another issue. \r\n\r\nThanks!", "Getting the same issue. Did you find any solution?", "@angersson  Thanks for your reply!\r\n\r\n@Avtarsingh127  Sorry, I had no luck to solve this problem. Waiting for a more stable version.", "@Avtarsingh127  I think I found some solution to avoid the error.\r\n\r\nIn def run_final_eval function\r\nchange \r\n`(sess, bottleneck_input, ground_truth_input, evaluation_step,\r\n   prediction) = build_eval_session(model_info, class_count)`\r\ninto\r\n`(eval_sess, bottleneck_input, ground_truth_input, evaluation_step,\r\n   prediction) = build_eval_session(model_info, class_count)`\r\n\r\nand\r\n`test_accuracy, predictions = sess.run(\r\n      [evaluation_step, prediction],\r\n      feed_dict={\r\n          bottleneck_input: test_bottlenecks,\r\n          ground_truth_input: test_ground_truth\r\n      })`\r\ninto\r\n`test_accuracy, predictions = eval_sess.run(\r\n      [evaluation_step, prediction],\r\n      feed_dict={\r\n          bottleneck_input: test_bottlenecks,\r\n          ground_truth_input: test_ground_truth\r\n      })`\r\n\r\nI think the problem is caused by the newly built eval sess is not included the DecodeJPGInput Tensor.\r\nIt's just a guess.", "@Icycoding, thanks for sharing the solution.\r\n\r\nI downloaded the earlier version of tensorflow (1.5), compiled and used it. It worked without any issue and without changing anything in the code. \r\n", "Looks like there was a faulty commit, see here: https://github.com/tensorflow/tensorflow/issues/17423"]}, {"number": 17369, "title": "TF 1.6 cannot be installed on MacOS because of failure in installing grpcio", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.13.3\r\n- **TensorFlow installed from (source or binary)**: both\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: 0.11\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: - \r\n- **GPU model and memory**: - \r\n- **Exact command to reproduce**:\r\n\r\nOn MacOS, type:\r\n`sudo pip3 install tensorflow`\r\npip3 will start installing grpcio, but an error during the installation of grpcio (see below) will prevent the complete installation of tensorflow.\r\n\r\n(This does not happen on TF 1.5, which doesn't seem to use grpcio)\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n`MacBook-Pro:grpc feranick$ sudo pip3 install --upgrade tensorflow\r\nThe directory '/Users/feranick/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nThe directory '/Users/feranick/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nCollecting tensorflow\r\n  Downloading tensorflow-1.6.0-cp36-cp36m-macosx_10_11_x86_64.whl (43.2MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 43.2MB 37kB/s \r\nRequirement already up-to-date: absl-py>=0.1.6 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already up-to-date: wheel>=0.26 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already up-to-date: termcolor>=1.1.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already up-to-date: astor>=0.6.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already up-to-date: protobuf>=3.4.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already up-to-date: six>=1.10.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already up-to-date: tensorboard<1.7.0,>=1.6.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already up-to-date: gast>=0.2.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)\r\nRequirement already up-to-date: numpy>=1.13.3 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow)\r\nCollecting grpcio>=1.8.6 (from tensorflow)\r\n  Downloading grpcio-1.10.0.tar.gz (14.0MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.0MB 110kB/s \r\nRequirement already up-to-date: setuptools in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow)\r\nRequirement already up-to-date: html5lib==0.9999999 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\r\nRequirement already up-to-date: bleach==1.5.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\r\nRequirement already up-to-date: werkzeug>=0.11.10 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\r\nRequirement already up-to-date: markdown>=2.6.8 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\r\nInstalling collected packages: grpcio, tensorflow\r\n  Running setup.py install for grpcio ... error\r\n    Complete output from command /opt/local/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c \"import setuptools, tokenize;__file__='/private/tmp/pip-build-da0d116h/grpcio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-m90opqn_-record/install-record.txt --single-version-externally-managed --compile:\r\n    Found cython-generated files...\r\n    running install\r\n    running build\r\n    running build_py\r\n    running build_project_metadata\r\n    creating python_build\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc\r\n    copying src/python/grpcio/grpc/_channel.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc\r\n    copying src/python/grpcio/grpc/_common.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc\r\n    copying src/python/grpcio/grpc/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc\r\n    copying src/python/grpcio/grpc/_utilities.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc\r\n    copying src/python/grpcio/grpc/_plugin_wrapping.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc\r\n    copying src/python/grpcio/grpc/_interceptor.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc\r\n    copying src/python/grpcio/grpc/_grpcio_metadata.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc\r\n    copying src/python/grpcio/grpc/_server.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc\r\n    copying src/python/grpcio/grpc/_auth.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta\r\n    copying src/python/grpcio/grpc/beta/_server_adaptations.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta\r\n    copying src/python/grpcio/grpc/beta/interfaces.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta\r\n    copying src/python/grpcio/grpc/beta/_metadata.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta\r\n    copying src/python/grpcio/grpc/beta/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta\r\n    copying src/python/grpcio/grpc/beta/utilities.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta\r\n    copying src/python/grpcio/grpc/beta/implementations.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta\r\n    copying src/python/grpcio/grpc/beta/_client_adaptations.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/beta\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework\r\n    copying src/python/grpcio/grpc/framework/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython\r\n    copying src/python/grpcio/grpc/_cython/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation\r\n    copying src/python/grpcio/grpc/framework/foundation/callable_util.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation\r\n    copying src/python/grpcio/grpc/framework/foundation/abandonment.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation\r\n    copying src/python/grpcio/grpc/framework/foundation/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation\r\n    copying src/python/grpcio/grpc/framework/foundation/stream.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation\r\n    copying src/python/grpcio/grpc/framework/foundation/stream_util.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation\r\n    copying src/python/grpcio/grpc/framework/foundation/future.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation\r\n    copying src/python/grpcio/grpc/framework/foundation/logging_pool.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/foundation\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/common\r\n    copying src/python/grpcio/grpc/framework/common/style.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/common\r\n    copying src/python/grpcio/grpc/framework/common/cardinality.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/common\r\n    copying src/python/grpcio/grpc/framework/common/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/common\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces\r\n    copying src/python/grpcio/grpc/framework/interfaces/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/face\r\n    copying src/python/grpcio/grpc/framework/interfaces/face/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/face\r\n    copying src/python/grpcio/grpc/framework/interfaces/face/utilities.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/face\r\n    copying src/python/grpcio/grpc/framework/interfaces/face/face.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/face\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/base\r\n    copying src/python/grpcio/grpc/framework/interfaces/base/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/base\r\n    copying src/python/grpcio/grpc/framework/interfaces/base/utilities.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/base\r\n    copying src/python/grpcio/grpc/framework/interfaces/base/base.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/framework/interfaces/base\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython/_cygrpc\r\n    copying src/python/grpcio/grpc/_cython/_cygrpc/__init__.py -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython/_cygrpc\r\n    creating python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython/_credentials\r\n    copying src/python/grpcio/grpc/_cython/_credentials/roots.pem -> python_build/lib.macosx-10.13-x86_64-3.6/grpc/_cython/_credentials\r\n    running build_ext\r\n    b'[C]       Compiling third_party/cares/cares/ares__close_sockets.c\\n[C]       Compiling third_party/cares/cares/ares__get_hostent.c\\n[C]       Compiling third_party/cares/cares/ares__read_line.c\\n[C]       Compiling third_party/cares/cares/ares__timeval.c\\n[C]       Compiling third_party/cares/cares/ares_cancel.c\\n[C]       Compiling third_party/cares/cares/ares_create_query.c\\n[C]       Compiling third_party/cares/cares/ares_data.c\\n[C]       Compiling third_party/cares/cares/ares_destroy.c\\n[C]       Compiling third_party/cares/cares/ares_expand_name.c\\n[C]       Compiling third_party/cares/cares/ares_expand_string.c\\n[C]       Compiling third_party/cares/cares/ares_fds.c\\n[C]       Compiling third_party/cares/cares/ares_free_hostent.c\\n[C]       Compiling third_party/cares/cares/ares_free_string.c\\n[C]       Compiling third_party/cares/cares/ares_getenv.c\\n[C]       Compiling third_party/cares/cares/ares_gethostbyaddr.c\\n[C]       Compiling third_party/cares/cares/ares_gethostbyname.c\\n[C]       Compiling third_party/cares/cares/ares_getnameinfo.c\\n[C]       Compiling third_party/cares/cares/ares_getopt.c\\n[C]       Compiling third_party/cares/cares/ares_getsock.c\\n[C]       Compiling third_party/cares/cares/ares_init.c\\n[C]       Compiling third_party/cares/cares/ares_library_init.c\\n[C]       Compiling third_party/cares/cares/ares_llist.c\\n[C]       Compiling third_party/cares/cares/ares_mkquery.c\\n[C]       Compiling third_party/cares/cares/ares_nowarn.c\\n[C]       Compiling third_party/cares/cares/ares_options.c\\n[C]       Compiling third_party/cares/cares/ares_parse_a_reply.c\\n[C]       Compiling third_party/cares/cares/ares_parse_aaaa_reply.c\\n[C]       Compiling third_party/cares/cares/ares_parse_mx_reply.c\\n[C]       Compiling third_party/cares/cares/ares_parse_naptr_reply.c\\n[C]       Compiling third_party/cares/cares/ares_parse_ns_reply.c\\n[C]       Compiling third_party/cares/cares/ares_parse_ptr_reply.c\\n[C]       Compiling third_party/cares/cares/ares_parse_soa_reply.c\\n[C]       Compiling third_party/cares/cares/ares_parse_srv_reply.c\\n[C]       Compiling third_party/cares/cares/ares_parse_txt_reply.c\\n[C]       Compiling third_party/cares/cares/ares_platform.c\\n[C]       Compiling third_party/cares/cares/ares_process.c\\n[C]       Compiling third_party/cares/cares/ares_query.c\\n[C]       Compiling third_party/cares/cares/ares_search.c\\n[C]       Compiling third_party/cares/cares/ares_send.c\\n[C]       Compiling third_party/cares/cares/ares_strcasecmp.c\\n[C]       Compiling third_party/cares/cares/ares_strdup.c\\n[C]       Compiling third_party/cares/cares/ares_strerror.c\\n[C]       Compiling third_party/cares/cares/ares_timeout.c\\n[C]       Compiling third_party/cares/cares/ares_version.c\\n[C]       Compiling third_party/cares/cares/ares_writev.c\\n[C]       Compiling third_party/cares/cares/bitncmp.c\\n[C]       Compiling third_party/cares/cares/inet_net_pton.c\\n[C]       Compiling third_party/cares/cares/inet_ntop.c\\n[C]       Compiling third_party/cares/cares/windows_port.c\\n[AR]      Creating /private/tmp/pip-build-da0d116h/grpcio/libs/opt/libares.a\\n[C]       Compiling src/boringssl/err_data.c\\n'\r\n    b\"In file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:115:\\n/opt/local/include/openssl/e_os2.h:276:11: error: 'OPENSSL_EXPORT' macro redefined [-Werror,-Wmacro-redefined]\\n#  define OPENSSL_EXPORT extern\\n          ^\\nthird_party/boringssl/include/openssl/base.h:178:9: note: previous definition is here\\n#define OPENSSL_EXPORT\\n        ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:83:31: error: redefinition of typedef 'ASN1_INTEGER' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_INTEGER;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:253:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_INTEGER;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:84:31: error: redefinition of typedef 'ASN1_ENUMERATED' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_ENUMERATED;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:249:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_ENUMERATED;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:85:31: error: redefinition of typedef 'ASN1_BIT_STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_BIT_STRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:247:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_BIT_STRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:86:31: error: redefinition of typedef 'ASN1_OCTET_STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_OCTET_STRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:254:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_OCTET_STRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:87:31: error: redefinition of typedef 'ASN1_PRINTABLESTRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_PRINTABLESTRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:255:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_PRINTABLESTRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:88:31: error: redefinition of typedef 'ASN1_T61STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_T61STRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:257:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_T61STRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:89:31: error: redefinition of typedef 'ASN1_IA5STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_IA5STRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:252:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_IA5STRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:90:31: error: redefinition of typedef 'ASN1_GENERALSTRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_GENERALSTRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:251:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_GENERALSTRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:91:31: error: redefinition of typedef 'ASN1_UNIVERSALSTRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_UNIVERSALSTRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:259:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_UNIVERSALSTRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:92:31: error: redefinition of typedef 'ASN1_BMPSTRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_BMPSTRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:248:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_BMPSTRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:93:31: error: redefinition of typedef 'ASN1_UTCTIME' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_UTCTIME;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:260:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_UTCTIME;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:94:31: error: redefinition of typedef 'ASN1_TIME' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_TIME;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:258:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_TIME;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:95:31: error: redefinition of typedef 'ASN1_GENERALIZEDTIME' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_GENERALIZEDTIME;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:250:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_GENERALIZEDTIME;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:96:31: error: redefinition of typedef 'ASN1_VISIBLESTRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_VISIBLESTRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:262:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_VISIBLESTRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:97:31: error: redefinition of typedef 'ASN1_UTF8STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_UTF8STRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:261:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_UTF8STRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:98:31: error: redefinition of typedef 'ASN1_STRING' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef struct asn1_string_st ASN1_STRING;\\n                              ^\\nthird_party/boringssl/include/openssl/base.h:256:31: note: previous definition is here\\ntypedef struct asn1_string_st ASN1_STRING;\\n                              ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:99:13: error: redefinition of typedef 'ASN1_BOOLEAN' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef int ASN1_BOOLEAN;\\n            ^\\nthird_party/boringssl/include/openssl/base.h:242:13: note: previous definition is here\\ntypedef int ASN1_BOOLEAN;\\n            ^\\nIn file included from src/boringssl/err_data.c:18:\\nIn file included from /opt/local/include/openssl/err.h:122:\\n/opt/local/include/openssl/ossl_typ.h:100:13: error: redefinition of typedef 'ASN1_NULL' is a C11 feature [-Werror,-Wtypedef-redefinition]\\ntypedef int ASN1_NULL;\\n            ^\\nthird_party/boringssl/include/openssl/base.h:243:13: note: previous definition is here\\ntypedef int ASN1_NULL;\\n            ^\\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\\n20 errors generated.\\nmake: *** [/private/tmp/pip-build-da0d116h/grpcio/objs/opt/src/boringssl/err_data.o] Error 1\\n\"\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/private/tmp/pip-build-da0d116h/grpcio/setup.py\", line 311, in <module>\r\n        cmdclass=COMMAND_CLASS,\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/setuptools/__init__.py\", line 129, in setup\r\n        return distutils.core.setup(**attrs)\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/core.py\", line 148, in setup\r\n        dist.run_commands()\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/dist.py\", line 955, in run_commands\r\n        self.run_command(cmd)\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n        cmd_obj.run()\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/setuptools/command/install.py\", line 61, in run\r\n        return orig.install.run(self)\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/command/install.py\", line 545, in run\r\n        self.run_command('build')\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n        self.distribution.run_command(command)\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n        cmd_obj.run()\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/command/build.py\", line 135, in run\r\n        self.run_command(cmd_name)\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/cmd.py\", line 313, in run_command\r\n        self.distribution.run_command(command)\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/dist.py\", line 974, in run_command\r\n        cmd_obj.run()\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/setuptools/command/build_ext.py\", line 78, in run\r\n        _build_ext.run(self)\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py\", line 186, in run\r\n        _build_ext.build_ext.run(self)\r\n      File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/distutils/command/build_ext.py\", line 339, in run\r\n        self.build_extensions()\r\n      File \"/private/tmp/pip-build-da0d116h/grpcio/src/python/grpcio/commands.py\", line 278, in build_extensions\r\n        raise Exception(\"make command failed!\")\r\n    Exception: make command failed!\r\n    \r\n    ----------------------------------------\r\nCommand \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 -u -c \"import setuptools, tokenize;__file__='/private/tmp/pip-build-da0d116h/grpcio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-m90opqn_-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/tmp/pip-build-da0d116h/grpcio/`", "comments": ["me too. failed to install v1.6 on OSX 10.12.6 because of the same issue.", "Same here, I'm also on OSX 10.12.6 with the same issue.", "Failed for me too. I am on OSX 10.13.3.", "+1 on OSX 10.12.6", "+1 on OSX 10.12.6 as well, Python 2.7", "me too", "me as well... (on macOS 10.13.3 MBP 2015) python 3.6", "same issue on OSX 10.13.3, python3.6", "macOS 10.13.3 Python 3.6 same problem.", "The issue is related to `grpcio` v1.10.0 (https://github.com/grpc/grpc/releases/tag/v1.10.0) not being able to build.\r\n\r\nIf you install *don't*  do `pip install -U tensorflow` (which will try to also update grpcio). Instead: `pip install tensorflow==1.6.0` and you should be fine.", "Thanks @sbl for discovering the problem\r\n```\r\n$ pip3 install grpcio==1.9.1 tensorflow\r\n```\r\nworks", "Try install grpcio 1.09\r\n```\r\n$ pip3 install grpcio==1.09\r\n$ pip3 install tensorflow\r\n```", "+1 on OSX 10.13.3 Python 3.6\r\n\r\nfixed the problem by\r\n`pip3 install grpcio==1.9.1 tensorflow`", "+1 on OSX 10.11.6 Python 3.6.1\r\n\r\n`pip3 install grpcio==1.9.1 tensorflow`", "+1\r\n\r\nIf you are installing it freshly for the first time\r\n`install grpcio==1.9.1 tensorflow`\r\nwill work\r\n\r\nIf you are trying to upgrade tensorflow from existing version,\r\n`pip install tensorflow==1.6.0`\r\nwill work\r\n\r\n", "Thank you.\r\n\r\npip install grpcio==1.9.1 tensorflow worked for me.\r\n\r\n", "Just a heads up that a MacOS wheel for grpcio 1.10.0 is now on pypi as of today so the default TF dependencies (grpcio = \"*\") once again work fine. Backed out the `grpcio = \"1.9.1\"` here and `pipenv` no longer fails to install TF.\r\n\r\nSee https://github.com/grpc/grpc/issues/14573 and https://pypi.python.org/pypi/grpcio/1.10.0\r\n\r\nThanks to @mehrdada for the quick work on resolving this!", "I still have an error: \r\n\r\nCould not find a version that satisfies the requirement tensorflow==1.6.0 (from versions: )\r\nNo matching distribution found for tensorflow==1.6.0\r\n\r\nTo install TensorFlow Python Package I had to use the URL (invoked pip version<8.1), which was this one: \r\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py2-none-any.whl\r\nSo for me the normal command doesn't work anyway.", "(tensorflow) AlexandrasMBP3:tensorFlow Alexandra$ pip3 install grpcio==1.9.1 tensorflow\r\nCollecting grpcio==1.9.1\r\n  Using cached grpcio-1.9.1-cp36-cp36m-macosx_10_7_intel.whl\r\nCollecting tensorflow\r\n  Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\r\n(tensorflow) AlexandrasMBP3:tensorFlow Alexandra$ \r\n\r\n\r\nSTILL GETTING THIS ERROR! :( please help\r\n\r\n\r\n", "@simothejudge @duboyal : try \r\n\r\nFor MAC\r\n\r\n       For python 2.X\r\n       pip install grpcio==1.9.1 https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py2-none-any.whl\r\n\r\n       For python 3.X\r\n       pip install grpcio==1.9.1 https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py3-none-any.whl\r\n\r\n\r\nThis worked for me", "Closing because the issue seems understood, but please reopen if you think there's something we should change.", "I'm sorry for reopening this but I'm still getting the same error! even with\r\n   pip install grpcio==1.9.1 https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py3-none-any.whl\r\nAnybody can help? Thanks a lot.", "@djcerbero you can try `pip install --no-cache-dir --only-binary :all: grpcio==1.10.0rc1` before `pip install --upgrade tfBinaryURL` It worked for me", "@djcerbero I'm having the same issue now, too.\r\n@rrodriguezperez \"No matching distribution found for grpcio==1.10.0rc1\" after running the first piece of that. Yes I have the latest version of pip installed. This is inside the virtual python environment.\r\nHelp?", "@Speuce I gave up and installed anaconda, everything went smooth. ", "python 3.7.0 \r\n\r\ninsallation on MacOS\r\n`sudo pip3 install grpcio==1.9.1 --user --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.8.0-py3-none-any.whl`\r\n\r\nStill produces\r\n`Command \"/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -u -c \"import setuptools, tokenize;__file__='/private/tmp/pip-install-l8x9q309/grpcio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/tmp/pip-record-g5bcv1gx/install-record.txt --single-version-externally-managed --compile --user --prefix=\" failed with error code 1 in /private/tmp/pip-install-l8x9q309/grpcio/`", "@GitHubMurt : Can you once try\r\n         \r\n        pip install grpcio==1.9.1 https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py3-none-any.whl\r\n\r\nAlso why are you using sudo ?", "I use it preventively. I tired this but pip is incompatible with given http address py3\r\n\r\n> tensorflow-1.6.0-py3-none-any.whl is not a supported wheel on this platform.", "I have the same problem", "same problem", "gRPC Python 3.7 64-bit binary wheels are uploaded to PyPI for macOS and Windows (as well as Linux which was previously available).  Install should work even if you don't have the build dependencies for `grpcio`.", "mac:\r\n\r\n<download python3 package from python.org and install with package installer>\r\n\r\ncd src; mkdir tensorflow; cd tensorflow; python3 -m venv tf-test; cd tf-test\r\n\r\nsource bin/activate\r\n\r\ncurl https://bootstrap.pypa.io/get-pip.py | python\r\n\r\npip install --trusted-host pypi.org --trusted-host files.pythonhosted.org grpcio\r\n\r\npip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.9.0-py3-none-any.whl\r\n", "@mazz Any reason you are forcing `grpcio` to 1.9.1? The current version 1.13.0 seems to install just fine.", "@mehrdada based upon an old post .. updated", "solved installing `pkg-config` on Mac\r\n`brew install pkg-config`"]}, {"number": 17368, "title": "The best way to pass the LSTM state between batches", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n### System information\r\nThe problem is independent of the system information.\r\n\r\nI am using Tensorflow 1.5\r\n\r\n### Describe the problem\r\nI think I will be insane. I am trying to find the best way to pass the LSTM state between batches. I have searched everything but I could not find a solution for the current implementation. Imagine I have something like:\r\n\r\n```\r\ncells = [rnn.LSTMCell(size) for size in [256,256]\r\ncells = rnn.MultiRNNCell(cells, state_is_tuple=True)\r\ninit_state = cells.zero_state(tf.shape(x_hot)[0], dtype=tf.float32)\r\nnet, new_state = tf.nn.dynamic_rnn(cells, x_hot, initial_state=init_state ,dtype=tf.float32)\r\n```\r\n\r\nNow I would like to pass the `new_state` in each batch efficiently, so without storing it back to memory and then re-feed to tf using `feed_dict`. To be more precise, all the solutions I found use `sess.run` to evaluate `new_state` and  `feed-dict` to pass it into `init_state`. Is there any way to do so without having the bottleneck of using `feed-dict`?\r\n\r\nI think I should use `tf.assign` in some way but the doc is incomplete and I could not find any workaround.\r\n\r\nI am writing this problem here since It is unsolved everywhere else and I think this should be in the doc since it is a common case scenario. \r\n\r\nI want to thank everybody that will ask in advance.\r\n\r\nCheers,\r\n\r\nFrancesco Saverio\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there. \r\n\r\nIf you have figured out something that you think should be in the docs, please file an issue or PR after you determine what it is you think needs to be included.\r\n\r\nThanks!"]}, {"number": 17367, "title": "Enable CUDNN_TENSOR_OP_MATH for fp16 RNNs", "body": "- Speeds up CUDNN RNNs with fp16 input/output when possible on supported\r\n  GPUs. Computations will fall back to pseudo-fp16 if tensor op math is\r\n  not supported.\r\n- Enabled by default, but can be disabled by setting the environment\r\n  variable TF_DISABLE_CUDNN_RNN_TENSOR_OP_MATH=1.", "comments": ["Notifying @zheng-xq @tfboyd."]}, {"number": 17366, "title": "Link error on compile_android_protobuf.sh", "body": "TensorFlow version (use command below): r1.6 \r\n\r\nHi, \r\nI'm trying to use Makefile to compile android library(following this guideline - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) , but having an issue with compile_android_protobuf.sh. \r\n\r\nI tried with both NDKr16b and NDKr15c, but getting this error. \r\n\r\n>> ~/DL/tensorflow/tensorflow$ tensorflow/contrib/makefile/compile_android_protobuf.sh -c -a x86_64\r\n...\r\n...\r\n/DL/tensorflow/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/src/.libs/libprotobuf.a(common.o):common.cc:function google::protobuf::internal::DefaultLogHandler(google::protobuf::LogLevel, char const*, int, std::string const&): error: undefined reference to 'stderr'\r\n/DL/tensorflow/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/src/.libs/libprotobuf.a(common.o):common.cc:function google::protobuf::internal::DefaultLogHandler(google::protobuf::LogLevel, char const*, int, std::string const&): error: undefined reference to 'stderr'\r\ncollect2: error: ld returned 1 exit status\r\n\r\nI also modified sh to add ${SYSROOT}/usr/include as include path, but still getting same error. \r\n", "comments": ["@smilejimin Please provide all of the platform information requested in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n@petewarden Can you take a look at this?", "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source \r\n- **TensorFlow version (use command below)**: r1.6\r\n- **Python version**:  3.4.3\r\n- **Bazel version (if compiling from source)**: 0.6.1\r\n- **GCC/Compiler version (if compiling from source)**:  4.8.5\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: tensorflow/contrib/makefile/compile_android_protobuf.sh -c -a x86_64\r\n", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I was able to build successfully with ndk14b.  "]}]