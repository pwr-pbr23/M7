[{"number": 13562, "title": "Fix for a regression in graph rewrite pass  (MKL)", "body": "Changed CPU device string from \"cpu\" to \"CPU\" to re enable MKL graph rewrite pass that was disabled by a prior commit.", "comments": ["Can one of the admins verify this patch?", "@rmlarsen Did you get a chance to look at this?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@gunan @caisq  what does it take to get a successful ubuntu CC test? All the other tests passed yesterday, so that is the only blocker to merge this.", "I think adding the \"kokoro:run\" tag is the magic word :) Just added it. The test is running now.\r\n\r\ncc @yifeif ", "Jenkins, test this please.", "@rmlarsen and @gunan, this fix is critical for Intel MKL functionality. Although this seems to be merged soon into master, we see that r1.4 has been released without this fix, that means r1.4 won't be able to to use MKL. Is there anyway to add this to r1.4?", "@mahmoud-abuzaina @agramesh1 Merged. Thanks for the fix. We had some internal test infrastructure overload that delayed testing. Thank you for your patience.", "@wicke @rajatmonga It seems critical that we cherrypick this into R1.4.", "@av8ramit @case540 Please cherrypick this PR into the release, then we can prepare rc1.", "@av8ramit could you take a look, please?", "@gunan didn't see your comment. Thanks!", "We will cherry-pick this into 1.4. Thanks everyone!"]}, {"number": 13561, "title": "Improve shape inference for `tf.slice`", "body": "This fix is an effort to address the issue raised by #4590 where improvement of shape inference for `tf.slice` is needed.\r\n\r\nWhen one of the size element is unknwon, the output shape is completely unknwon (with right rank):\r\n```python\r\n>>> import tensorflow as tf\r\n>>> z = tf.zeros((1, 2, 3))\r\n>>> z.get_shape().as_list()\r\n[1, 2, 3]\r\n>>> m = tf.slice(z, [0, 0, 0], [tf.constant(1) + 0, 2, -1])\r\n>>> m.get_shape().as_list()\r\n[None, None, None]\r\n```\r\n\r\nThis fix improves the shape inference so that as long as the size element is not unknown or `-1`, the right shape will shown up:\r\n```python\r\n>>> import tensorflow as tf\r\n>>> z = tf.zeros((1, 2, 3))\r\n>>> z.get_shape().as_list()\r\n[1, 2, 3]\r\n>>> m = tf.slice(z, [0, 0, 0], [tf.constant(1) + 0, 2, -1])\r\n>>> m.get_shape().as_list()\r\n[None, 2, None]\r\n```\r\n\r\nNote: this fix does not handle the case where one of the size element is `-1` and one of the size element is unknown. However, it is an improvement nevertheless.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@cwhipkey Thanks a lot for the review. The PR has been updated. Please take a look and let me know if there are any issues.", "@tensorflow-jenkins test this please\r\n", "@yongtang This test failure seems to be related to this PR:\r\n\r\n======================================================================\r\nERROR: testShapeInference (__main__.ResizeImageWithCropOrPadTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/image_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/image_ops_test.py\", line 2305, in testShapeInference\r\n    self._assertShapeInference([None, 69, 3], 55, 66, [55, 66, 3])\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/image_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/image_ops_test.py\", line 2155, in _assertShapeInference\r\n    y = image_ops.resize_image_with_crop_or_pad(image, height, width)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/image_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/image_ops_impl.py\", line 659, in resize_image_with_crop_or_pad\r\n    min_(target_width, width))\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/image_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/image_ops_impl.py\", line 565, in crop_to_bounding_box\r\n    cropped.set_shape(cropped_shape)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/image_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 419, in set_shape\r\n    self._shape = self._shape.merge_with(shape)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/image_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/tensor_shape.py\", line 582, in merge_with\r\n    raise ValueError(\"Shapes %s and %s are not compatible\" % (self, other))\r\nValueError: Shapes (?, ?, 65, ?) and (1, ?, 66, 3) are not compatible\r\n\r\n----------------------------------------------------------------------\r\nRan 27 tests in 8.282s\r\n\r\nFAILED (errors=1)\r\n================================================================================", "Thanks @caisq. The issue should have been fixed and the PR has been updated.", "@tensorflow-jenkins test this please"]}, {"number": 13560, "title": "The document of tf.nn.dynamic_rnn needs to be re-formatted", "body": "Hi,\r\n\r\nSomeone take a look at the document page of [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn). I think there are some formatting bugs in the second half of the web page. ", "comments": ["@dr4b @wolffg : Seems like the empty newlines in https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/rnn.py#L483 are confusing the website generator?\r\n\r\nWould either of you have some cycles to investigate?", "Hy, I was trying to solve this issue, but when I forked the project and download it my source code file is different of the file in the web, it doesn't have the empty newlines that @asimshankar comments. Can anyone tell me why this is happening?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@tensorflowbutler this bug has been fixed. "]}, {"number": 13559, "title": "Add execute permission to import_pb_to_tensorboard.py", "body": "I used `chmod +x` to add the execute permission.\r\n\r\nTESTING\r\n\r\nBefore this change, you get a `Permission denied` error when trying to run import_pb_to_tensorboard.py:\r\n```\r\n$ ./tensorflow/python/tools/import_pb_to_tensorboard.py\r\n-bash: ./tensorflow/python/tools/import_pb_to_tensorboard.py: Permission denied\r\n```\r\nAfter the change, the script runs without needing to use `chmod`.\r\n\r\nTested on Mac OSX Sierra 10.12.6.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13558, "title": "segfaults in GPU tf.matrix_inverse", "body": "I'm running into segfaults in tf.matrix_inverse\r\nI'm adding identity*0.001 so matrices should be invertible, and same procedure works fine in numpy and in TensorFlow CPU version.\r\n\r\nhttps://github.com/yaroslavvb/stuff/blob/master/inverse_segfault.py\r\n`python inverse_segfault.py`\r\n\r\nThis non-deterministically crashes after 1-2 seconds with various backtraces.\r\n\r\nIE\r\n\r\n```\r\n#0  0x0000000000000001 in ?? ()\r\n#1  0x00007fe90ed9c652 in tensorflow::Tensor::TotalBytes() const ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#2  0x00007fe90ed9c7d6 in tensorflow::Tensor::tensor_data() const ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007fe9137adda3 in bool tensorflow::internal::TransposeUsingTile<unsigned int>(Eigen::GpuDevice const&, tensorflow::Tensor const&, tensorflow::gtl::ArraySlice<int>, tensorflow::Tensor*) ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fe9137a696c in tensorflow::Status tensorflow::DoTranspose<Eigen::GpuDevice>(Eigen::GpuDevice const&, tensorflow::Tensor const&, tensorflow::gtl::ArraySlice<int>, tensorflow::Tensor*) ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fe911ada0fd in tensorflow::SvdOpGpu<float>::PerformSVD_MgeqN(tensorflow::OpKernelContext*, std::function<void ()>, long long, long long, long long, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor const&, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*) ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fe911ade897 in tensorflow::SvdOpGpu<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>) ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fe90f20790b in tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>) ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#8  0x00007fe90f23cf37 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)\r\n    ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n```\r\n\r\nor this\r\n\r\n```\r\n#0  0x00007fa89090686a in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#1  0x00007fa89091b074 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#2  0x00007fa890826e2c in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#3  0x00007fa890978880 in cuLaunchKernel ()\r\n   from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#4  0x00007fa891bf1dc1 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0\r\n#5  0x00007fa891c0f9cd in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0\r\n#6  0x00007fa891aa1132 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0\r\n#7  0x00007fa891aa2b72 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0\r\n#8  0x00007fa891aa32e3 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0\r\n#9  0x00007fa891aa36fa in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0\r\n#10 0x00007fa89190f5f3 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0\r\n#11 0x00007fa891912375 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0\r\n#12 0x00007fa89aa82c50 in tensorflow::Status tensorflow::CudaSolver::Getrf<float>(int, int, float*, int, int*, int*) ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007fa89a55f5d6 in tensorflow::MatrixInverseOpGpu<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>) ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007fa897dce90b in tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>) ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#15 0x00007fa897e03f37 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)\r\n    ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n#16 0x00007fa897df1ec5 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) ()\r\n   from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n\r\n```\r\n\r\nTensorFlow commit: https://github.com/tensorflow/tensorflow/commit/22a886b\r\nNVIDIA-SMI 381.09\r\nlibcudart.so.8.0.44\r\nlibcudnn.so.6.0.21\r\nNvidia GTX 1080\r\n", "comments": ["@rmlarsen : Would you have some cycles to look into this?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "From the comment on the PR, it looks like the mutex was added."]}, {"number": 13557, "title": "Fix the gradient computation of dynamic stitch.", "body": "Currently the gradient of tf.dynamic_stitch is not correct for duplicated indices. This is\r\nissue #7397. @drasmuss submitted a pull request for this issue, but it was ultimately not merged\r\ndue to a performance drop.\r\n\r\nWhile working on this, I realised that the problem is more complicated than what follows\r\nfrom the discussion in #7487. In fact, the solution proposed in #7487 is not correct, because\r\none can have duplicated indices in the same input tensor. The following example shows this:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.zeros((3,))\r\ny = tf.dynamic_stitch([[0, 1, 0]], [x])\r\n\r\nwith tf.Session() as sess:\r\n    print(\"y\")\r\n    print(sess.run(y))\r\n\r\n    analytic, numeric = tf.test.compute_gradient(x, (3,), y, (2,))\r\n    print(\"analytic\")\r\n    print(analytic)\r\n    print(\"numeric\")\r\n    print(numeric)\r\n```\r\n\r\nThe output (even with #7487) is\r\n\r\n```\r\ny\r\n[ 0.  0.]\r\nanalytic\r\n[[ 1.  0.]\r\n [ 0.  1.]\r\n [ 1.  0.]]\r\nnumeric\r\n[[ 0.          0.        ]\r\n [ 0.          0.99998707]\r\n [ 0.99998707  0.        ]]\r\n```\r\n\r\nMy fix is to add a new kernel op `GatherDisjoint`, that can handle all the inputs together.\r\nThis replaces the `n` calls to `gather` in `data_flow_grad._DynamicStitchGrads` with a\r\nsingle `gather_disjoint`.\r\n\r\nThe implementation is very similar to `gather`, I just zero out duplicated slices at the end.\r\nI've put this op in core/, if you would like I can move it to contrib/, just let me know where.\r\n\r\nI've also added tests for the op and new tests for the gradient of dynamic stitch. Here\r\nI inspired myself from @drasmuss pull request.\r\n\r\nI've run the script by @drasmuss again:\r\n\r\n```\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nstitch_size = 1000\r\nn_inputs = 10\r\ninput_shape = (100, 50, 50)\r\nreps = 10\r\n\r\nwith tf.device(\"cpu:0\"):\r\n    idxs = [tf.constant(np.random.randint(stitch_size, size=input_shape[0]), dtype=tf.int32)\r\n            for _ in range(n_inputs)]\r\n    vals = [tf.constant(np.random.uniform(-1, 1, size=input_shape), dtype=tf.float32)\r\n            for _ in range(n_inputs)]\r\n    y = tf.dynamic_stitch(idxs, vals)\r\n    grad = tf.gradients(y, vals)\r\n\r\ntotal_time = 0.0\r\nfor _ in range(reps):\r\n    with tf.Session() as sess:\r\n        start = time.time()\r\n        sess.run(grad)\r\n        total_time += time.time() - start\r\n\r\nprint(total_time / reps)\r\n```\r\n\r\nThe results are as follows:\r\nwith my fix:\r\nCPU - 0.09455678462982178\r\nGPU - 0.09739606380462647\r\nwithout my fix:\r\nCPU - 0.11728813648223876\r\nGPU - 0.11805038452148438\r\n\r\nSo with the fix it runs faster. On GPU the slowdown is due to the memory copies from host to device\r\nand back, for input and output (I've checked with nvprof), and not because of my fix.\r\n\r\nI've run this script using local builds of the same master branch, with and without my patch.\r\n\r\nLet me know if the op should be moved somewhere else, or it should be made hidden.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "The reasons for failure are:\r\n- dynamic stitch was modified since I started working on this and my commit does not work properly with the new modifications\r\n- the API changed and the goldens should be updated (as I've said, I can move the op to contrib)\r\n- for the GPU, XLA and Windows there is a strange failure involving a hudson plugin that I don't understand\r\n\r\nI will fetch the latest version of tensorflow and commit an update.", "ping @aselle ", "Would you mind testing the PR? I think all tests should pass.", "Jenkins, test this please.", "Hi @martinwicke ,\r\n\r\nI fixed the failure, would you mind testing again?", "ping @aselle for review.", "@codrut3 wondering if this PR valid , if yes can you please resolve conflicts.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "    Hi Rajeshwar,\n\n   I am sorry for taking so long to answer. I think the PR is still valid\nbut needs to be updated in view of recent changes. I'll have a look and\nreopen it if that's the case.\n\n   Best,\n     Codrut\n\nOn Fri, Apr 26, 2019 at 3:54 AM Rajeshwar Reddy T <notifications@github.com>\nwrote:\n\n> Closed #13557 <https://github.com/tensorflow/tensorflow/pull/13557>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13557#event-2301870254>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACSJ5ZNEEQLGQMAXEV5RUVLPSJOF5ANCNFSM4D6F6ZYA>\n> .\n>\n"]}, {"number": 13556, "title": "Problem importing Tensorflow in Windows 10 64bits", "body": "Hello everybody. I'm new in Tensorflow. So, forgive me for any mistakes that a make. \r\n\r\nI'm importing Tensorfow-gpu in Windows but I'm getting the message bellow. \r\n\r\n**### System information**\r\n- **OS Platform and Distribution:** Windows 10 64 bits\r\n- **TensorFlow installed from:** pip\r\n- **TensorFlow version:** 1.3\r\n- **Python version:** 3.6.2 - v3.6.2:5fd33b5, Jul  8 2017, 04:57:36\r\n- **CUDA:** 9.0.176\r\n- **cuDNN version:** I've tried with version 5, 6 and 7\r\n- **GPU model and memory**: 1050 4GB\r\n- **Exact command to reproduce**: import tensorflow\r\n\r\n**PS:** all the path settings have been set. \r\n\r\n**### The error message**\r\nC:\\Windows\\system32>python\r\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\python\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: N\u00e3o foi poss\u00edvel encontrar o m\u00f3dulo especificado.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\python\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\python\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: N\u00e3o foi poss\u00edvel encontrar o m\u00f3dulo especificado.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\python\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>", "comments": ["The current release build requires CUDA 8. Can you either downgrade your CUDA version or try building from source with CUDA 9?", "Thanks a lot @mrry. I've just downgraded my installations and Tensorflow worked just fine. "]}, {"number": 13555, "title": "Fix broken link in performance guide", "body": "This fix fixes broken link in performance guide as models repo moved `slim` to `models/research/slim`\r\n`https://github.com/tensorflow/models/tree/master/slim#Data`\r\n->\r\n`https://github.com/tensorflow/models/tree/master/research/slim#Data`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13554, "title": "Update installation instructions for conda install to include pip", "body": "This branch updates the installation instructions for conda install to include pip as well, in order to prevent the usage of the pip installed in the root conda environment.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13553, "title": "Add `<` operator for IndicesRowIterator", "body": "This fix adds the `<` operator for IndicesRowIterator to address C2678 error in VS Debug mode, as was specified in #12000.\r\n\r\nThis fix fixes #12000.", "comments": ["Can one of the admins verify this patch?", "ping @ThomasColthurst ", "/cc @gunan as this PR is a fix for issue #12000. Please take a look as well.", "@ThomasColthurst Could you review the change?\r\nJenkins, test this please.", "It looks like another PR #14404 is making the same change. As #14404 has already been merged, will close this PR. Thanks all for the review!"]}, {"number": 13552, "title": "how to restore the certain variable_scope Variables into another certain variable_scope?", "body": "Now I have trained a model A and I need two model A instances because one of them just is fixed and untrainable for outputting and another is trainable for next network. I design two variable_scope **A_train** and **A_untrain**, I pre-trained A model in variable_scope **A_untrain** and restore the model also in this scope, code like:\r\n```python\r\nsaver_untrain = tf.train.Saver(tf.get_collection(\r\n                                   tf.GraphKeys.GLOBAL_VARIABLES,\r\n                                   'A_untrain'))\r\nsaver_path = '~/models/model.ckpt'\r\n# here pre-train model A\r\nsaver_untrain.save(sess, saver_path)\r\n```\r\nNow I need to restore the same model A parameters into the same model in scope **A_train**, but I cannot follow the previous code because the ckpt files restore the params like `A_untrain/input_w1` instread of `A_train/input_w1`. I want to know if there is a solution to my problem OR a better solution to make two instances which one is trainable and another is untrainable. Thanks a lot.\r\n\r\n**EDIT_1**: I know I can realize my need use code like:\r\n\r\nsaver_train = tf.train.Saver({'A_untrain/input_w1': A_train.input_w1})\r\nbut it will be unpractical when my variables amount is large, so I need to use the variable_scope to restore instead of the specific variables' names.", "comments": ["this is better for stackoverflow (this list is for bugs/issues in TensorFlow itself)", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Has anybody solved this problem???"]}, {"number": 13551, "title": "Added missing `` in train_and_evaluate doc", "body": "", "comments": []}, {"number": 13550, "title": "https://www.tensorflow.org/ extremly sucks: cockie madness", "body": "https://www.tensorflow.org/* totally sucks because of its cookie popup dialog, which jumps into one's face __each__ time one goes to a different page on this site, NO matter whether one _has already clicked clicked_ OK or something else. No way to do any research on such annoying site ...", "comments": ["www.tensorflow.org should not be causing any popups. Perhaps there are some settings/plugins in your browser/environment that are making this happen.", "On 3 different, independent workstations, on Solaris as well as under Linux?\r\nSorry for not using Windows ...", "BTW: Added rules to block all *.js from tensorflow.org and google related sites, which fixes the problem. I'll give our students the advice to do the same.\r\nSo far to your \"professional\" analysis. Good job @asimshankar ...", "The issue is real.\r\nThe cookie popup is for Europe users. The popup says:\r\n\"We serve cookies on this site to analyse traffic, remember your preferences, and optimise your experience.\"\r\n", "Thanks for adding the detail @fidlej \r\nOut of curiosity, have you explicitly disabled cookies in your browser? (Wondering if this is happening by default or if conflicting with some user settings)\r\n\r\nCC @wolffg : Who might have some insight.\r\n", "I do not have cookies disabled.\r\nThe popup keep appearing even if using a clean incognito chrome window.", "@dr4b @wolffg @jugglerix PTAL", "We've turned the permission off for now; you should be OK. ", "Thanks. The popup is not appearing anymore."]}, {"number": 13549, "title": "const_op.h missing from C++ API documentation", "body": "I could not find the documentation for tensorflow::ops::Const starting from the C++ API documentation. I figured out it is declared in ops/const_op.h, but there is no link from the main C++ docs:\r\nhttps://www.tensorflow.org/api_docs/cc/\r\n\r\nA search returns this, but it looks orphaned:\r\nhttps://www.tensorflow.org/api_docs/cc/group/const-op\r\n\r\nSo I guess there is something wrong in the docs.", "comments": ["@skye - Is `const_op.h` somehow excluded when generating the API docs?", "I'm not sure what's going on. @dr4b do you have any ideas? I wonder if this somehow has to do with Const being in the namespace ops, and all the other op methods being in nested namespaces inside ops?", "Another strange problem I noticed is that tensorflow::Input::Initializer appears twice in the menu on the left:\r\nhttps://www.tensorflow.org/api_docs/cc/struct/tensorflow/input/initializer", "hey, so I looked into the const_op thing and basically it's not showing up because that module/file has no classes in it (you might notice the index pages are all based around listening classes/structs in each part of the API).  We'll have to think of a way to deal with this, I guess.\r\n\r\n(Initializer is a separate bug that I was aware of a while ago but didn't realize was still there)", "Assigning to @dr4b, since it seems like this is a bug we should fix.  Feel free to re-assign as appropriate.", "I've already looked into this but I'm still not sure what the right solution is given that seriously const_op had no classes/structs in it.  Unless that's changed?\r\n\r\nThe duplicate inner struct bug that Remi mentioned, however, is out against me in our internal tools and I should look into that at some point, it's true.", "I no longer work on Tensorflow and won't be able to address this any time soon (as I said, the issue is that there's nothing inside the file that Doxygen is going to pick up.)  Reassigning if Wolff cares to deal with it.", "Nagging Assignees @wolffg, @MarkDaoust: It has been 112 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "There's no simple way to fix this, and the C++ api is relatively low priority.\r\n\r\nSorry."]}, {"number": 13548, "title": "Fix a small typo", "body": "This PR fixes a small typo : categort -> category.\r\nThanks for the awesome project!", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13547, "title": "Fix typos", "body": "This PR fixes some typos: `initalizers`, `fileds`, `mutli`, `beacuse`, and `summmary`.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13546, "title": "Feature request: RMSProp without momentum variables", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.3.0-24-g658866597 1.3.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: 0.6.1\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nThe current `RMSPropOptimizer` always allocates momentum variables, even though the default (and probably 99% of people) never make use of it. In fact, if one wishes to combine momentum and adaptive gradient descent, he/she will most likely instantiate an `AdamOptimizer` instead. The extra variables waste precious GPU/CPU memory as well as disk space (when saved as checkpoints) while providing minimal utility. \r\n\r\nSuggestion: introduce a new version of `ApplyRMSProp` operations that doesn't use momentum variables at all, and dynamically choose which implementation to use in `RMSPropOptimizer` constructor depending on whether the `momentum` argument is constant zero.\r\n\r\nAlternative solution: change the current `ApplyRMSProp` operations so that it doesn't use momentum variables, and direct the minority users who currently need momentum with RMSProp to Adam instead.\r\n\r\n", "comments": ["If I understand you correctly, do you mean that intermediate variable `mom` is wasteful when `momentum = 0`?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/624bcfe409601910951789325f0b97f520c0b1ee/tensorflow/python/training/rmsprop.py#L32-L36\r\n\r\nSounds reasonable, at least for me. I prefer to the first solution, which seems harmless and feasible: \r\n+ stop allocating resources if `momentum = 0`:\r\n   https://github.com/tensorflow/tensorflow/blob/624bcfe409601910951789325f0b97f520c0b1ee/tensorflow/python/training/rmsprop.py#L103\r\n+ adjust interface and c++ kernel implementations.\r\n   https://github.com/tensorflow/tensorflow/blob/624bcfe409601910951789325f0b97f520c0b1ee/tensorflow/core/kernels/training_ops.cc#L352-L353\r\n\r\nThe work seems hard but not difficult :-)", "@facaiy Yes that is exactly what I meant.", "@jhseu Do you have ideas on who might be able to evaluate / work on this?", "I'd favor not changing it. Is the extra memory and disk usage a big issue in practice? The additional memory usage would be minimal compared to activations. Users who care about memory/disk usage could copy the optimizer and remove the momentum part.", "@jhseu \r\n\r\nIt makes a big difference in practice. I don't know what you mean by \"minimal compared to activations\", but I know about the checkpoint size. Without the momentum, one checkpoint consists of network parameters and the moving average of squared gradients, so double the size of parameters. With the momentum, triple. Hence the increase is 50%. That is 50% more traffic when transferring checkpoints over the Internet, and 50% more disk usage when storing them. In fact, when I worked as an intern at Google, our group frequently had Colossus quota alerts resulting from saving many checkpoints, and this dramatic increase makes the matter much worse.\r\n\r\nCopying the optimizer and removing the momentum part does not work, because `ApplyRMSProp` family of operations requires the existence of the extra variables. And if it would work, it would be better for user experience if Tensorflow does it automatically, since, as I have stressed multiple times, zero momentum is the *dominate* use case, and therefore worth optimizing for.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "While I'm not opposed to this in general if there's an easy way of doing it, I am somewhat opposed to adding a new Op to save a slot unless there's a lot of demand for it. Closing this out for now."]}, {"number": 13545, "title": "Pin TensorBoard 0.4 to tf-nightly", "body": "See also https://github.com/tensorflow/tensorflow/pull/13544 which pins 0.4 to TF 1.4.", "comments": []}, {"number": 13544, "title": "Pin TensorBoard 0.4 to TensorFlow 1.4", "body": "See also:\r\n- https://pypi.python.org/pypi/tensorflow-tensorboard\r\n- https://github.com/tensorflow/tensorboard/releases/tag/0.4.0-rc1", "comments": ["@jart the failing test fails due to this:\r\nNo matching distribution found for tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.0rc0)\r\n\r\nIs this expected?", "The tests shouldn't be failing. https://pypi.python.org/pypi/tensorflow-tensorboard/0.4.0rc1", "test this please"]}, {"number": 13543, "title": "how to condition encoder final hidden state on the inputs of RNN dynamic decoder with ScheduledOutputTrainingHelper?", "body": "Hi, I'm trying to use tensorflow to code RDD encoder and decoder and with different length sequence inputs, so hope both encoder and decoder can be dynamic. Additionally, a decoder inputs is conditioned by the encoder final hidden states (context vector), which is similar to the [Related Paper](https://arxiv.org/pdf/1702.05538.pdf) see picture a in page 3. The decoder is trying to fully inference during training with feeding previous outputs and context vector as inputs at each step.\r\n\r\n`class RNNEncoder_Decoder(object):\r\n    def __init__(self,input_dim,\r\n                 context_dim,output_dim,hidden_dim,\r\n                 layers_stacked_count,learning_rate):\r\n        \r\n        self.graph = tf.get_default_graph()\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        self.context_dim = context_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.layers_stacked_count = layers_stacked_count\r\n        self.learning_rate = learning_rate\r\n        self.sampling_probability = tf.constant(dtype=tf.float32,value=1.0)\r\n        \r\n        # [batch_size,sequence_length,input_dimension]\r\n        self.enc_inp = tf.placeholder(tf.float32, [None,None,self.input_dim], name='encoder_inputs')\r\n        self.expected_out = tf.placeholder(tf.float32, [None,None,self.output_dim], name='expected_outs')\r\n        # fullly inference during trianing\r\n        self.dec_inp = tf.zeros_like(self.expected_out,dtype=tf.float32,name='decoder_inputs')\r\n                \r\n        seq_length = tf.reduce_sum(tf.sign(tf.reduce_max(tf.abs(self.enc_inp), 2)), 1)\r\n        self.seq_length = tf.cast(seq_length, tf.int32)\r\n        \r\n        with tf.variable_scope('RNNEncoderDecoder'):\r\n            with tf.variable_scope(\"Enocder\") as encoder_varscope:\r\n                # create encoder LSTM cell\r\n                encoder_cells = []\r\n                for i in range(self.layers_stacked_count):\r\n                    with tf.variable_scope('EncoderCell_{}'.format(i)):\r\n                        encoder_cells.append(tf.nn.rnn_cell.LSTMCell(self.hidden_dim,\r\n                                                             use_peepholes=True))\r\n                self.encoder_cell = tf.nn.rnn_cell.MultiRNNCell(encoder_cells)\r\n\r\n                # ruuning dynamic rnn encoder                \r\n                _, enc_state = tf.nn.dynamic_rnn(cell = self.encoder_cell,\r\n                                                 initial_state=None,\r\n                                                 dtype=tf.float32,\r\n                                                 inputs = self.enc_inp,\r\n                                                 sequence_length = self.seq_length\r\n                                                )\r\n \r\n                # extract top layer hidden state as feature representation\r\n                self.context_vector = enc_state[-1].h\r\n                \r\n                cell_state0 = tf.zeros_like(enc_state[0].c,dtype=tf.float32)\r\n                hidden_state0 = tf.zeros_like(enc_state[0].h,dtype=tf.float32)\r\n\r\n                dec_init_state = (enc_state[1], # pass the top layer state of enocder to the bottom layer of decoder\r\n                                  tf.nn.rnn_cell.LSTMStateTuple(cell_state0, hidden_state0))\r\n                \r\n                # condition extracted features on decoder inputs\r\n                # with a shape that matches decoder inputs in all but (potentially) the final dimension. \r\n                # tile context vector from [batch_size,context_dim] to [batch_size,decoder_sequence_length,context_dim]\r\n                context_vector_shape = tf.shape(self.context_vector)\r\n                context_vector_reshaped = tf.reshape(self.context_vector, \r\n                                                     [context_vector_shape[0], 1, context_vector_shape[1]]\r\n                                                    )\r\n                enc_inp_shape = tf.shape(self.enc_inp)\r\n                self.auxiliary_inputs = tf.tile(context_vector_reshaped,\r\n                                           multiples=[1,enc_inp_shape[1],1]\r\n                                          )\r\n                \r\n            with tf.variable_scope(\"Deocder\") as decoder_varscope:\r\n                # create decoder LSTM cell\r\n                decoder_cells = []\r\n                for i in range(self.layers_stacked_count):\r\n                    with tf.variable_scope('DecoderCell_{}'.format(i)):\r\n                        decoder_cells.append(tf.nn.rnn_cell.LSTMCell(self.hidden_dim,\r\n                                                             use_peepholes=True))\r\n                self.decoder_cell = tf.nn.rnn_cell.MultiRNNCell(decoder_cells)\r\n\r\n                dec_out_dense = Dense(units = self.output_dim,\r\n                                      activation = None,\r\n                                      use_bias = False,\r\n                                      kernel_initializer = tf.truncated_normal_initializer(\r\n                                          dtype=tf.float32,\r\n                                          stddev = 1.0 / math.sqrt(float(self.hidden_dim))\r\n                                      ),\r\n                                      name = 'dec_outp_linear_projection'\r\n                                     )\r\n                \r\n                training_helper = tf.contrib.seq2seq.ScheduledOutputTrainingHelper(\r\n                    inputs = self.dec_inp,\r\n                    sequence_length = self.seq_length,\r\n                    auxiliary_inputs = self.auxiliary_inputs, # condtional on inputs\r\n                    sampling_probability = 1.0, # for fullly inference\r\n                    name = 'feeding_conditional_input'\r\n                )\r\n                \r\n                decoder = tf.contrib.seq2seq.BasicDecoder(\r\n                    cell = self.decoder_cell,\r\n                    helper = training_helper,\r\n                    initial_state = dec_init_state,\r\n                    output_layer = dec_out_dense\r\n                )\r\n                \r\n                outputs, _ , final_seq_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,\r\n                                                                                   impute_finished = True\r\n                                                                                  )\r\n            self.outputs = outputs\r\n            \r\n    ### optimize loss part\r\n    \r\n    def get_decoder_prediction(self,X,session):\r\n        feed_dict = {\r\n            self.enc_inp:X\r\n        }\r\n        feed_dict.update({self.expected_out:X})\r\n        run = [self.outputs]\r\n        return session.run(run,feed_dict=feed_dict)\r\nRNN_test = RNNEncoder_Decoder(input_dim=1,context_dim=32,output_dim=1,hidden_dim=32,layers_stacked_count=2,learning_rate=0.01)`\r\n\r\n\r\nWithout \"auxiliary_inputs = self.auxiliary_inputs\", it running successfully,\r\nBut with auxiliary_inputs = self.auxiliary_inputs I got following error:\r\n\r\n`---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-02522a01f0d8> in <module>()\r\n      9                           hidden_dim=hidden_dim,\r\n     10                           layers_stacked_count=layers_stacked_count,\r\n---> 11                           learning_rate=learning_rate\r\n     12                          )\r\n\r\n<ipython-input-2-86494b8d99fa> in __init__(self, input_dim, context_dim, output_dim, hidden_dim, layers_stacked_count, learning_rate)\r\n     98 \r\n     99                 outputs, _ , final_seq_lengths = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,\r\n--> 100                                                                                    impute_finished = True\r\n    101                                                                                   )\r\n    102             self.outputs = outputs\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)\r\n    284         ],\r\n    285         parallel_iterations=parallel_iterations,\r\n--> 286         swap_memory=swap_memory)\r\n    287 \r\n    288     final_outputs_ta = res[1]\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2773     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)\r\n   2774     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)\r\n-> 2775     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2776     return result\r\n   2777 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2602       self.Enter()\r\n   2603       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2604           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2605     finally:\r\n   2606       self.Exit()\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2552         structure=original_loop_vars,\r\n   2553         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2554     body_result = body(*packed_vars_for_body)\r\n   2555     if not nest.is_sequence(body_result):\r\n   2556       body_result = [body_result]\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)\r\n    232       \"\"\"\r\n    233       (next_outputs, decoder_state, next_inputs,\r\n--> 234        decoder_finished) = decoder.step(time, inputs, state)\r\n    235       next_finished = math_ops.logical_or(decoder_finished, finished)\r\n    236       if maximum_iterations is not None:\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py in step(self, time, inputs, state, name)\r\n    137     \"\"\"\r\n    138     with ops.name_scope(name, \"BasicDecoderStep\", (time, inputs, state)):\r\n--> 139       cell_outputs, cell_state = self._cell(inputs, state)\r\n    140       if self._output_layer is not None:\r\n    141         cell_outputs = self._output_layer(cell_outputs)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    178       with vs.variable_scope(vs.get_variable_scope(),\r\n    179                              custom_getter=self._rnn_get_variable):\r\n--> 180         return super(RNNCell, self).__call__(inputs, state)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    448         # Check input assumptions set after layer building, e.g. input shape.\r\n    449         self._assert_input_compatibility(inputs)\r\n--> 450         outputs = self.call(inputs, *args, **kwargs)\r\n    451 \r\n    452         # Apply activity regularization.\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\r\n    936                                       [-1, cell.state_size])\r\n    937           cur_state_pos += cell.state_size\r\n--> 938         cur_inp, new_state = cell(cur_inp, cur_state)\r\n    939         new_states.append(new_state)\r\n    940 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    178       with vs.variable_scope(vs.get_variable_scope(),\r\n    179                              custom_getter=self._rnn_get_variable):\r\n--> 180         return super(RNNCell, self).__call__(inputs, state)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    448         # Check input assumptions set after layer building, e.g. input shape.\r\n    449         self._assert_input_compatibility(inputs)\r\n--> 450         outputs = self.call(inputs, *args, **kwargs)\r\n    451 \r\n    452         # Apply activity regularization.\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\r\n    554     input_size = inputs.get_shape().with_rank(2)[1]\r\n    555     if input_size.value is None:\r\n--> 556       raise ValueError(\"Could not infer input size from inputs.get_shape()[-1]\")\r\n    557     scope = vs.get_variable_scope()\r\n    558     with vs.variable_scope(scope, initializer=self._initializer) as unit_scope:\r\n\r\nValueError: Could not infer input size from inputs.get_shape()[-1]`\r\n\r\nI'm just getting start to use tensforflow, so could anyone help me with:\r\nIs this a correct way to condition the last hidden state of encoder on the inputs of decoder?\r\nand why the inputs of decoder become None after I feed the auxiliary_inputs as the error?\r\n@ebrevdo \r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@cy89 \r\nOk,I will post there. \r\nBy the way, is using auxiliary_inputs of ScheduledOutputTrainingHelper a correct way to condition encoder final state on the decoder inputs at each step? "]}, {"number": 13542, "title": "Update version strings for TensorFlow 1.4-rc0", "body": "Strings were updated by tensorflow/tools/ci_build/update_version.py script.\r\nHand-edited several pom.xml files missed by the script.", "comments": []}, {"number": 13541, "title": "Update README.md with tf-nightly-gpu", "body": "", "comments": []}, {"number": 13540, "title": "Update release notes for TensorFlow 1.4", "body": "", "comments": ["Just a note, the contributing authors section will be created later. In theory it's possible a cherrypick could come externally. Just in case anyone was wondering."]}, {"number": 13539, "title": "Branch 171343275", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13538, "title": "more pythonic", "body": "remove semicolons", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Jenkins, test this please.", "@rtshanks can you take care of CLA so we can merge this PR?", "Closing pull request. Please respond after taking care of the CLA so we can reopen this pull request."]}, {"number": 13537, "title": "Feature Request: tf.assign() support tuples", "body": "I have recently updated to V1.3 of Tensorflow.  I have some code that I use for dynamic_rnn which copies the STATE of the cell so it persists to the next .run(), I can also INIT that value as well.  Since the update, I am getting a \"WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f278c196940>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\".  I have tried to enable state_is_tuple but then the assign() commands fail as they don't support the tuple structures.\r\n\r\nI have an open StackOverflow question with the details:\r\nhttps://stackoverflow.com/questions/46576194/how-do-i-assign-a-lstmstatetuple-using-tf-assign\r\n\r\nSince it seems like the RNN core is moving in the direction of the tuple for the state, it would be nice if the .assign() can handle this transparently.\r\n", "comments": ["@ebrevdo do we expect to add tuple support soon?", "you can perform an assign on a tuple by using\r\n\r\n```python\r\nvariables = (var1, var2)  # or whatever; you can use map_structure with tf.get_variable here as well\r\nupdates = tf.contrib.framework.nest.map_structure(\r\n  lambda state, var: tf.assign(var, state),\r\n  final_lstm_state,\r\n  variable_tuple,\r\n  check_types=False)\r\n```", "@Mazecreator does @ebrevdo 's suggestion get you back in business?", "@cy89 this might do it but it doesn't seem like \"tf.contrib.framework.nest.map_structure()\" exists.  I am using TF1.3 but checked the HEAD and didn't see that \"nest\" folder.  I tried to find if it moved to another location but couldn't find it.\r\n\r\nI am sure this is a good short term solution within the active LSTM graph but will greatly complicate copying variables from one Net to another as the transparency of the .assign() would be appreciated.\r\n\r\nOnce we have a short-term solution I will post it on Stack Overflow in response to my open issue.\r\n\r\nHere was my interpretation of the implementation for the LSTM State:\r\n\r\n            output, self.new_state = tf.nn.dynamic_rnn(self.cell_L1,hidden_input,time_major=True,\\\r\n                    initial_state=self.state,  dtype=tf.float32, swap_memory=True)\r\n\r\n            self.state = tf.contrib.framework.nest.map_structure(\r\n                    lambda state, var: tf.assign(var, state),\r\n                    self.new_state,\r\n                    #variable_tuple,\r\n                    check_types=False)\r\n\r\n            with tf.control_dependencies([self.state]):\r\n                outputs = tf.identity(output)       ", "It might not be the version expected but I found a \"**map_structure**\" here:\r\nfrom tensorflow.python.util.nest import *\r\n\r\nRunning the above code failed with this error so I must have missed something:\r\n\r\n    File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py\", line 325, in map_structure\r\n      structure[0], [func(*x) for x in entries])\r\n    File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py\", line 325, in <listcomp>\r\n      structure[0], [func(*x) for x in entries])\r\n    TypeError: <lambda>() missing 1 required positional argument: 'var'", "I have been working on this for a while now.  I was not certain what needs to replace \"#variable_tuple,\" in the map_structure but have a better guess:\r\n\r\n        output, self.new_state = tf.nn.dynamic_rnn(self.cell_L1,hidden_input,time_major=True,\\\r\n                initial_state=self.state,  dtype=tf.float32, swap_memory=True)\r\n\r\n        update = tf.contrib.framework.nest.map_structure(\r\n                lambda state, var: tf.assign(var, state),\r\n                self.new_state,\r\n                self.state,\r\n                check_types=False)\r\n\r\n        with tf.control_dependencies([update]):\r\n            outputs = tf.identity(output) \r\n\r\nThe problem now is I get a different error which doesn't really make sense to me as Tensors should allow an assign() should it?\r\n\r\n    File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py\", line 325, in map_structure\r\n    structure[0], [func(*x) for x in entries])\r\n    File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py\", line 325, in <listcomp>\r\n    structure[0], [func(*x) for x in entries])\r\n    File \"/home/greg/DFP_trading_sequence/test6/Network_sequence_new1.py\", line 143, in <lambda>\r\n    lambda state, var: tf.assign(var,state) ,\r\n    File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 272, in assign\r\n    return ref.assign(value)\r\n    AttributeError: 'Tensor' object has no attribute 'assign'", "You can only assign something to a Variable object.  So if you want an\ninitial state it has to be a tuple of Variables (and cell.zero_state()\ndoesn't return Variables - only Tensors).  You can then assign to those in\nyour update.\n\nOn Tue, Oct 10, 2017 at 11:54 AM, Greg Peatfield <notifications@github.com>\nwrote:\n\n> I have been working on this for a while now. I was not certain what needs\n> to replace \"#variable_tuple,\" in the map_structure but have a better guess:\n>\n>     output, self.new_state = tf.nn.dynamic_rnn(self.cell_L1,hidden_input,time_major=True,\\\n>             initial_state=self.state,  dtype=tf.float32, swap_memory=True)\n>\n>     update = tf.contrib.framework.nest.map_structure(\n>             lambda state, var: tf.assign(var, state),\n>             self.new_state,\n>             self.state,\n>             check_types=False)\n>\n>     with tf.control_dependencies([update]):\n>         outputs = tf.identity(output)\n>\n> The problem now is I get a different error which doesn't really make sense\n> to me as Tensors should allow an assign() should it?\n>\n> File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py\", line 325, in map_structure\n> structure[0], [func(*x) for x in entries])\n> File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/nest.py\", line 325, in <listcomp>\n> structure[0], [func(*x) for x in entries])\n> File \"/home/greg/DFP_trading_sequence/test6/Network_sequence_new1.py\", line 143, in <lambda>\n> lambda state, var: tf.assign(var,state) ,\n> File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 272, in assign\n> return ref.assign(value)\n> AttributeError: 'Tensor' object has no attribute 'assign'\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13537#issuecomment-335571606>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzk-JUX2tqhVVWOTG-cVWMkblG8Dks5sq71bgaJpZM4PxCmq>\n> .\n>\n", "@ebrevdo @Mazecreator is this issue resolved? (I don't know anything about the RNN API).", "I think I understand the approach although I have yet to code the solution.  It seems like the more an more I dig into this approach the more complexity and issue the rise.  I may need to wait for the actual support for assign() with Tuples.  I ran into another issue as I was trying to prototype this and it was initializing a variable with a Tuple only returns a single combined tensor.  Hopefully if Assign() supports tuples the \"variable()\" will support Tuples as well to initialize.  I find the biggest problem is the complexity of other libraries by manually copying like this.  Also, I need to figure out how to screen for a tuple so when I \"assign()\" the variables I don't collapse the tuple into a single tensor variable as this was tough to find even in a short segment of code.\r\n\r\nSo I think I might be able to do a short-term solution if needed based upon the comments from @ebrevdo , but I really think if the RNN training is going to force the state to be a tuple then the rest of TensorFlow should help support this new requirement.  Another option is to remove the warning statement and just document that Tuples are handled faster and support both formats moving forward.  I am sure this is an easier solution for now, not sure about the long term support issues as I am sure the team will need to think through.\r\n\r\nHere is what I am talking about when the RNN cells are \"state_is_tuple=True\"\r\n\r\n            self.init_state = self.cell_L1.zero_state(batch_size, tf.float32)\r\n            self.state = tf.Variable(self.init_state, trainable=False)\r\n\r\nIn the code above, this silently functions as you would expect.  The self.state is initialized with the Tuple value from self.init_state, but what you get for self.state is actually a single Tensor variable with the Tuple collapsed.  This causes the RNN functions to fail later as they are expecting a Tuple for the state.\r\n", "I'm rather surprised that `tf.Variable(self.init_state)` even works!  Sounds\r\nlike a bug.  That should have failed in python-land right away! @lukaszkaiser \r\n\r\nYou need to do something like:\r\n\r\n```\r\ncounter = [0]\r\ndef create_variable(init):\r\n  r = tf.get_variable(initializer=init, name=\"rnn_state_%d\" % counter[0])\r\n  counter[0] += 1\r\n  return r\r\nself.state = tf.contrib.framework.nest.map_structure(create_variable,\r\nself.init_state)\r\n```\r\n", "What you saw looks like a bug in tf.convert_to_tensor.  It treats complex\ndata structures, like LSTMStateTuple and nested LSTMStateTuples, as nested\ntuples and converts them to initializers.  We'll look at fixing that bug\nseparately.\n\nOn Thu, Oct 12, 2017 at 7:57 PM, ebrevdo <notifications@github.com> wrote:\n\n> I'm rather surprised that tf.Variable(self.init_state) even works! Sounds\n> like a bug. That should have failed in python-land right away! +lukasz\n>\n> You need to do something like:\n>\n>\n> counter = [0]\n> def create_variable(init):\n> r = tf.get_variable(initializer=init, name=\"rnn_state_%d\" % counter[0])\n> counter[0] += 1\n> return r\n> self.state = tf.contrib.framework.nest.map_structure(create_variable,\n> self.init_state)\n>\n>\n> On Thu, Oct 12, 2017 at 5:32 PM, Greg Peatfield <notifications@github.com>\n> wrote:\n>\n> > I think I understand the approach although I have yet to code the\n> > solution. It seems like the more an more I dig into this approach the\n> more\n> > complexity and issue the rise. I may need to wait for the actual support\n> > for assign() with Tuples. I ran into another issue as I was trying to\n> > prototype this and it was initializing a variable with a Tuple only\n> returns\n> > a single combined tensor. Hopefully if Assign() supports tuples the\n> > \"variable()\" will support Tuples as well to initialize. I find the\n> biggest\n> > problem is the complexity of other libraries by manually copying like\n> this.\n> > Also, I need to figure out how to screen for a tuple so when I \"assign()\"\n> > the variables I don't collapse the tuple into a single tensor variable as\n> > this was tough to find even in a short segment of code.\n> >\n> > So I think I might be able to do a short-term solution if needed based\n> > upon the comments from @ebrevdo <https://github.com/ebrevdo> , but I\n> > really think if the RNN training is going to force the state to be a\n> tuple\n> > then the rest of TensorFlow should help support this new requirement.\n> > Another option is to remove the warning statement and just document that\n> > Tuples are handled faster and support both formats moving forward. I am\n> > sure this is an easier solution for now, not sure about the long term\n> > support issues as I am sure the team will need to think through.\n> >\n> > Here is what I am talking about when the RNN cells are\n> > \"state_is_tuple=True\"\n> >\n> > self.init_state = self.cell_L1.zero_state(batch_size, tf.float32)\n> > self.state = tf.Variable(self.init_state, trainable=False)\n> >\n> > In the code above, this silently functions as you would expect. The\n> > self.state is initialized with the Tuple value from self.init_state, but\n> > what you get for self.state is actually a single Tensor variable with the\n> > Tuple collapsed. This causes the RNN functions to fail later as they are\n> > expecting a Tuple for the state.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 13537#issuecomment-336317687>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> ABtim8Y8cn7LRauhYWTefrUrFpZTFfDUks5srq-IgaJpZM4PxCmq>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13537#issuecomment-336336527>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim39WZJ3MfxbtGMMDHyHUTf-ZkBSyks5srtGUgaJpZM4PxCmq>\n> .\n>\n", "I would like to still request either tf.assign() be updated to support Tuples to drastically reduce the complexity of updating the LSTM Tuple (as well as any future changes) or that the current merged tensor remain as supported.  \r\n\r\nFor cases where the network is copied for parallel Reinforcement Learning threads where the global network is copied, the complexity is going to increase due to the unique nature or handling Tuple Named Tensors.", "The current merged tensor behavior is broken since even though you can\nassign a merged tensor to a variable, but then you have to slice it back up\nwhen you want to read the states back out again; and in the meantime you've\nlost the structure and shape of the intermediate states.\n\nHaving tf.assign support tuples may make sense; we'll look into it.  In the\nmeantime, you should be able to get pretty far using nest:\n\nupdated_variables = tf.contrib.framework.nest.map_structure(lambda v, s:\nv.assign(s), variables, states)\n\nwhere variables are created via map_structure using the rnn_cell's\nstate_size property or by looping over the zero_state tensors.\n\nOn Wed, Oct 18, 2017 at 10:06 AM, Greg Peatfield <notifications@github.com>\nwrote:\n\n> I would like to still request either tf.assign() be updated to support\n> Tuples to drastically reduce the complexity of updating the LSTM Tuple (as\n> well as any future changes) or that the current merged tensor remain as\n> supported.\n>\n> For cases where the network is copied for parallel Reinforcement Learning\n> threads where the global network is copied, the complexity is going to\n> increase due to the unique nature or handling Tuple Named Tensors.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13537#issuecomment-337659615>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7gQ0A7m_4sjIO5tMMTtd9SjExDfks5stjAIgaJpZM4PxCmq>\n> .\n>\n", "Thanks, I understand the concern.\r\n\r\nThe introduction of the Tuple ultimately creates yet another class of Variable which requires special handling, it would be nice to get this new internal structure to work like the other variables.  Maybe the nest.map_structure() can be include within the Assign() if there is some test to detect the input/output are tuples()?", "Yes this is a possibility.  Are you interested in sending a PR?", "nest.is_sequence can be used to test the variables", "I am very tight on time at the moment and think this might need some more holistic thinking.  My thought is it is going to be more than just an Assign update to better integrate the Tuple into TensorFlow \"seamlessly\".  There is likely to be more issues as well which I am  not sure I will be able to handle as I am not familiar with Named Tuples so it is more than just moving the Tensor as the Name needs to go with it.\r\n\r\nMy thought right now is a few things need to change to integrate the Tuple():\r\n- Assign() (_add() and _subtract() should be considered as well, but don't think that is urgent)\r\n- Variable()       - Would be nice to Init a Tuple variable with another Tuple Tensor (Initial State for example)\r\n- get_variable() - Not sure if the Tuple would be be supported here\r\n- Not sure if Run() will be impacted and if Placeholder needs a face-lift as well...\r\n- How with other Ops handle a Tuple if it is passed in place of a Tensor?  Is this a good way to concatenate different inputs\r\n\r\nI am tied up until about January/Feb of next year, so if the basic Assign() needs to wait until then I can hopefully tackle it at that time (I may need to come back for hints).  I think the more high level integration would be best within the development team as they would have better visibility of issues that maybe be caused or ways to handle the Tuple Tensor.\r\n\r\nFor now I need to use the Concatenated State until I have time to try to convert all the code to Tuples() as described above.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Let's move this discussion to discuss@tensorflow.org, which is more appropriate for design docs and proposals."]}, {"number": 13536, "title": "BeamSearchDecoder incorrectly truncates results when used with dynamic_decode", "body": "### System information (irrelevant for this bug)\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04/Any\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: Python 3.5.2 :: Continuum Analytics, Inc.\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: irrelevant\r\n- **GPU model and memory**: irrelevant\r\n- **Exact command to reproduce**: irrelevant\r\n\r\n### Describe the problem\r\ntf.contrib.seq2seq.BeamSearchDecoder incorrectly truncates some of the results because the same index was previously used for a beam member that ended at a earlier step.\r\n\r\nThe root of the problem is that the while_loop body in dynamic_decode assumes that sequences are independent and will finish only once. In the same time BeamSearchDecoder creates a tree-like structure where a beam index can be reused in a later step for a state that originates from a different parent index.  This causes the decoding loop to sometimes record the wrong sequence length for a beam member. Then this wrong sequence length is passed to BeamSearchDecoder.finalize which returns a truncated sequence.\r\n\r\n\r\n### Source code / logs\r\nI use the following code to workaround the problem. This causes the right sequence to be returned but still the length returned by dynamic_decode is wrong.\r\n```python\r\nclass FixedBeamSearchDecoder(seq2seq.BeamSearchDecoder):\r\n    def finalize(self, outputs, final_state, sequence_lengths):\r\n        # BeamSearchDecoder does not follow the correct semantics of the the finished flag\r\n        # which results in taking wrong length here and getting wrong decoded string.\r\n        # We substitute the sequence length recorded by dynamic_decoder (which is wrong because\r\n        # of the wrong finished flag returned by BeamSearchDecoder.step) with the length\r\n        # recorded in BeamSearchState which is correct.\r\n        return super().finalize(outputs, final_state, final_state.lengths)\r\n``` \r\n", "comments": ["@ebrevdo Can you take a look? I see that you wrote the seq2seq library. I wanted to submit a fix but I don't see how to correct this problem without changing some of the library's public inteface.", "Seems ok to update the BeamSearchDecoder.finalize to use final_state.lengths instead of sequence_lengths -- looks like this fixes a couple of other open issues.\r\n\r\nWe could consider having finalize return new updated sequence lengths to decode_dynamic as well.", "Thanks for catching this!  Could you send a PR with the fix and a unit test that catches it?", "Will look into submitting a fix.", "Sorry, I've been meaning to make a PR last week but never got to it.", "No problem. We're evaluating your change internally.\n\nOn Sat, Oct 14, 2017, 7:17 PM bdaskalov <notifications@github.com> wrote:\n\n> Sorry, I've been meaning to make a PR last week but never got to it.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13536#issuecomment-336681065>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxRoy93pElky6ZtzF-ctLOu6Khocks5ssWs8gaJpZM4PxBUM>\n> .\n>\n", "The problem is deeper and the solution requires some additional changes.  I'll try to submit something in the next couple days.", "Could anyone tell me when this bug was fixed. I couldn't find it in the release notes. Thank you! @ebrevdo ", "It was first released in TensorFlow 1.5.", "@guillaumekln Thank you for the info!"]}, {"number": 13535, "title": "<DO NOT MERGE> infra experiment", "body": "", "comments": ["@yifeif  I'll close this PR for now as there seems to have been no activity for the past 9 days.", "Thanks @caisq!"]}, {"number": 13534, "title": "Fix unevaluated link in \"Reading data\" docs", "body": "In the pull-out in https://www.tensorflow.org/api_guides/python/reading_data#Reading_from_files\r\nthere's a link that's unevaluated due to beginning with a `$` instead of a `@`.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 13533, "title": "KeyError: \"Couldn't find enum google.protobuf.MethodOptions.IdempotencyLevel\"", "body": "Hello! I am using macOS sierra 10.2.6, with tensorflow=1.3.0. TF was working fine until sometime ago, but as of today I get the error below when trying to import. Has anyone had the same problem?\r\n\r\nHere are the logs.\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-1-a649b509054f> in <module>()\r\n----> 1 import tensorflow\r\n\r\n/Users/antoniocampello/anaconda/lib/python3.5/site-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n/Users/antoniocampello/anaconda/lib/python3.5/site-packages/tensorflow/python/__init__.py in <module>()\r\n     52 \r\n     53 # Protocol buffers\r\n---> 54 from tensorflow.core.framework.graph_pb2 import *\r\n     55 from tensorflow.core.framework.node_def_pb2 import *\r\n     56 from tensorflow.core.framework.summary_pb2 import *\r\n\r\n/Users/antoniocampello/anaconda/lib/python3.5/site-packages/tensorflow/core/framework/graph_pb2.py in <module>()\r\n      8 from google.protobuf import reflection as _reflection\r\n      9 from google.protobuf import symbol_database as _symbol_database\r\n---> 10 from google.protobuf import descriptor_pb2\r\n     11 # @@protoc_insertion_point(imports)\r\n     12 \r\n\r\n/Users/antoniocampello/anaconda/lib/python3.5/site-packages/google/protobuf/descriptor_pb2.py in <module>()\r\n    237   options=None,\r\n    238   serialized_start=4644,\r\n--> 239   serialized_end=4724,\r\n    240 )\r\n    241 _sym_db.RegisterEnumDescriptor(_METHODOPTIONS_IDEMPOTENCYLEVEL)\r\n\r\n/Users/antoniocampello/anaconda/lib/python3.5/site-packages/google/protobuf/descriptor.py in __new__(cls, name, full_name, filename, values, containing_type, options, file, serialized_start, serialized_end)\r\n    597                 serialized_start=None, serialized_end=None):\r\n    598       _message.Message._CheckCalledFromGeneratedFile()\r\n--> 599       return _message.default_pool.FindEnumTypeByName(full_name)\r\n    600 \r\n    601   def __init__(self, name, full_name, filename, values,\r\n\r\nKeyError: \"Couldn't find enum google.protobuf.MethodOptions.IdempotencyLevel\"\r\n\r\n\r\n", "comments": ["Sounds like version mismatch between protobuf you have and what TF expects. Maybe some other package upgraded your protobuf?\r\n\r\nUniversal solution is to do reinstall in clean environment. ", "@gunan do you know what the issue is?", "Thanks! The \"universal solution\" worked. After a fresh install everything seems to be working fine. Now only god will know what caused the problem...\r\n\r\nI can close the issue or wait a bit if you guys feel like trying to understand the problem for future reference.", "Looks like a leftover old protobuf installation lingering on the system.\r\nIt would be nice to understand it fully, but in reality with the volume of issues, we wont have time to dig deeper into this.\r\nAlso, for future reference, anyone seeing this problem should just purge their TF and protobuf installations and reinstall."]}]