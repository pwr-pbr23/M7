[{"number": 44671, "title": "Problem building from source with cuda 11.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):Source \r\n- TensorFlow version:2.3.0\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11.1,8.0\r\n- GPU model and memory:RTX 3070\r\n\r\n\r\n\r\n**Describe the problem**\r\nI I have brought a RTX 3070. To use it with tensorflow I need to use cuda 11.1.So I built it from source. the problem is that the compiler is asking for files that do not exist in CUDA 11.1. You can see does file in the log below\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed all the commands from the https://www.tensorflow.org/install/source_windows guide and at it worked fine.\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n.\\bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Output user root \"C:/Users/pc/_bazel_my new pc\" contains a space. This will probably break the build. You should set a different --output_user_root.\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=210\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=F:/anaconda/envs/tf/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=F:/anaconda/python.exe --action_env PYTHON_LIB_PATH=F:/anaconda/lib/site-packages --python_path=F:/anaconda/python.exe --config=xla --action_env TF_CUDA_VERSION=11.1 --action_env TF_CUDNN_VERSION=8 --action_env TF_CUDA_PATHS=C:Program FilesNVIDIA GPU Computing ToolkitCUDAv11.1,C:cuda --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file c:\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:opt in file c:\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file c:\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Repository local_config_cuda instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule cuda_configure defined at:\r\n  C:/tensorflow/third_party/gpus/cuda_configure.bzl:1399:18: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 955, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx, <1 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 657, in _get_cuda_config\r\n                find_cuda_config(repository_ctx, <2 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 635, in find_cuda_config\r\n                _exec_find_cuda_config(<3 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 629, in _exec_find_cuda_config\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nCould not find any cublas_api.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\nINFO: Repository rules_cc instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule tf_http_archive defined at:\r\n  C:/tensorflow/third_party/repo.bzl:134:19: in <toplevel>\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  C:/users/pc/_bazel_my new pc/xv6zejqw/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 955, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx, <1 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 657, in _get_cuda_config\r\n                find_cuda_config(repository_ctx, <2 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 635, in find_cuda_config\r\n                _exec_find_cuda_config(<3 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 629, in _exec_find_cuda_config\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nCould not find any cublas_api.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 955, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx, <1 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 657, in _get_cuda_config\r\n                find_cuda_config(repository_ctx, <2 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 635, in find_cuda_config\r\n                _exec_find_cuda_config(<3 more arguments>)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 629, in _exec_find_cuda_config\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nCould not find any cublas_api.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\nINFO: Elapsed time: 1.046s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package", "comments": ["You can also suggest other ways to install tensorflow on rtx 3070", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44671\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44671\">No</a>\n"]}, {"number": 44669, "title": "_pywrap_tensorflow_internal not found", "body": "System information:\r\n- OS: windows 10\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: tensorflow-cpu 2.3.0\r\n- Python version: 3.8.3\r\n- Installed using  pip from https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow_cpu-2.3.0-cp38-cp38-win_amd64.whl\r\n\r\nLog:\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@aRandomBlock,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "The issue has been resolved after installing Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44669\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44669\">No</a>\n"]}, {"number": 44668, "title": "Unable to \"import tensorflow\" using 2.4.0rc0 from within conda environment", "body": "\r\n**System information**\r\n- OS: Ubuntu 20.04\r\n- TensorFlow binary installed using pip\r\n- TensorFlow version 2.4.0rc0\r\n- Python version 3.8.6\r\n- CUDA/cuDNN version: 11.0/8.0.4\r\n- GPU model: GTX 1070\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nAttempting to import tensorflow produces an \"Illegal instruction\" error.\r\n\r\n**Describe the expected behavior**\r\n\r\nImport tensorflow without error.\r\n\r\n**Standalone code to reproduce the issue**\r\nI created new conda environment and then installed tensorflow using pip from within the new environment:\r\n\r\n```\r\nconda create -n tf24 -c conda-forge python=3.8 pip\r\nconda activate tf24\r\npip install tensorflow==2.4.0rc0\r\n```\r\nThe following command exits with an \"Illegal instruction\" error:\r\n```\r\npython -c \"import tensorflow as tf\"\r\n```\r\nNote that this is not an issue with the last development version of 2.4.0 (2.4.0.dev20201023) from tf-nightly, installed with:\r\n\r\n```\r\npip install tf-nightly==2.4.0.dev20201023\r\n```\r\n", "comments": ["@psalveson \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nPlease refer to these resolved issues with same error : #44204, #40978, #43229.\r\n ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @psalveson\r\n> We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n> \r\n> Please refer to these resolved issues with same error : #44204, #40978, #43229.\r\n\r\n~~Whilst #44204, #40978, #43229 report the same error, they seem to all discuss the transition to AVX compiled binaries and suggest to use older version than 1.6, i.e. `tensorflow<=1.5`~~\r\n\r\n~~I am experiencing a similar situation to @psalveson where I was previously able to import and run tensorflow up to version `2.3.1` in my case, but the latest `tf-nightly` and it seems all other builds now throw the `Illegal instruction` error.~~\r\n\r\n~~Further discussion would bee greatly appreciated.~~\r\n\r\nIt turns out I was indeed testing on different chips, albeit both Intel, the one I was testing on seems to have AVX instructions", "@psalveson \r\nIs this still an issue", "@Saduf2019\r\nI just installed 2.4.0rc2 and I am still receiving an \"Illegal instruction\" error. My processor supports the AVX instruction set, so I don't think  #44204, #40978, #43229 apply (please correct me if I'm wrong). As mentioned previously, 2.4.0 from tf-nightly worked fine, it is only the  release candidates that are causing the error.\r\n\r\n", "@psalveson \r\nCould you please update us on which os are you trying to install, and is there a tf version already there on it.", "@Saduf2019 \r\nPlease see my original post for details. I am attempting to install tf into a conda environment. There is no other tf version currently installed. I am using Ubuntu 20.04.", "@psalveson \r\nPlease refer to this issue and let us know if it helps: #41315\r\n\r\nThis issue is more suitable on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues) repo since its related to TF installation with Anaconda.\r\nPlease post it on Continuum [Anaconda.](https://github.com/ContinuumIO/anaconda-issues/issues)\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44668\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44668\">No</a>\n", "@Saduf2019 \r\n\r\nAs stated in my original ticket, this is a typical installation using pip. It is not installed from an anaconda repository. I am installing into a conda environment, however. I have just attempted to install the most recent RC (rc4), and the problem persists. What changed between the final version of 2.4 on tf-nightly (which worked) and rc0 on tensorflow (which doesn't work)? That seems like a more useful area of inquiry. "]}, {"number": 44667, "title": "2.4.0rc1 cherry-pick request: Reduce frequency of logging for untraced functions while loading and saving from SavedModel ", "body": "For certain SavedModel consumers such as tf.Keras and tf Hub, the content and frequency of the existing level of warnings in 2.4.0rc0 is alarming and poses a usability issue, e.g. flooding notebooks with a wall of logs. Without this change, the publishing of tf hub tutorials and blogposts could get delayed (?).", "comments": ["cc @goldiegadde "]}, {"number": 44666, "title": "2.4.0rc1 cherry-pick request: Reduce frequency of logging for untraced functions while loading and saving from SavedModel", "body": "For certain SavedModel consumers such as tf.Keras and tf Hub, the content and frequency of the existing level of warnings in 2.4.0rc0 is alarming and poses a usability issue, e.g. flooding notebooks with a wall of logs. Without this change, the publishing of tf hub tutorials and blogposts could get delayed (?). ", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44666) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 44665, "title": "[INTEL MKL] Bug fix to duplicate kernel registration of BatchMatMulV2.", "body": "The PR fixes duplicate kernel registration of BatchMatMulV2 on CPU device with bfloat16 data type.", "comments": []}, {"number": 44664, "title": "Register Custom Operator Prototypes in TFLite C API", "body": "**System information**\r\n- TensorFlow version (you are using): v2.4.0-rc0\r\n  - TFLite C API on android arm/arm64 JNI\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI am specifically interested in the RandomStandardNormal operator. It was added as a custom op prototype in v2.4.0-rc0: https://github.com/tensorflow/tensorflow/commit/11823c6179a5e2ca6ac4f8480aa00c42c772885e\r\n\r\nI am looking for a way to register this operator prototype in the TFLite C API. I am able to call the function `TfLiteInterpreterOptionsAddCustomOp` from `tensorflow/lite/c/c_api_experimental.h`, but I am unable to call `tflite::ops::custom::Register_RANDOM_STANDARD_NORMAL()` to pass a registration pointer. \r\n\r\n**Will this change the current api? How?**\r\n\r\nPotentially if we add a extern \"C\" wrapper to `tensorflow/lite/kernels/custom_ops_register.h` it will expose more functions in the C API.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople who want to use the currently implemented custom operator prototypes via the C API. Also, people who want to learn more about how they can develop their own custom operators and use them in the C API.\r\n\r\n**Any Other info.**\r\n\r\nTFLite mailing list discussion with @abattery https://groups.google.com/a/tensorflow.org/g/tflite/c/1zfZtRtVfpY\r\n\r\n## My attempt at getting this to work\r\n### My Code (android JNI project, I'm calling this `load_model` function from C)\r\n#### load-model.h\r\n\r\n```C++\r\n#ifndef SYNC_LOAD_MODEL_H\r\n#define SYNC_LOAD_MODEL_H\r\n\r\n#include <tensorflow/lite/c/common.h>\r\n#include <tensorflow/lite/c/c_api.h>\r\n#include <tensorflow/lite/c/c_api_experimental.h>\r\n\r\n#ifdef __cplusplus\r\nextern \"C\" {\r\n#endif  // __cplusplus\r\n    TfLiteInterpreter* load_model(const char* model_path);\r\n#ifdef __cplusplus\r\n};\r\n#endif  // __cplusplus\r\n\r\n#endif //SYNC_LOAD_MODEL_H\r\n```\r\n\r\n#### load-model.cc\r\n```C++\r\n#include <load-model.h>\r\n\r\n#include <tensorflow/lite/kernels/custom_ops_register.h>\r\n\r\n// load the model and return pointer to the interpreter\r\nextern \"C\" {\r\n    TfLiteInterpreter* load_model(const char* model_path)\r\n    {\r\n        TfLiteModel *model = TfLiteModelCreateFromFile(model_path);\r\n        TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n\r\n        // register custom ops\r\n        TfLiteInterpreterOptionsAddCustomOp(options, \"RandomStandardNormal\", tflite::ops::custom::Register_RANDOM_STANDARD_NORMAL(), 1, 1);\r\n\r\n        // create the interpreter\r\n        TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n        TfLiteInterpreterAllocateTensors(interpreter);\r\n\r\n        return interpreter;\r\n    }\r\n}\r\n```\r\n#### Android.mk\r\n```\r\nAPP_ABI         := arm64-v8a\r\n\r\nLOCAL_PATH := $(call my-dir)\r\n\r\ninclude $(CLEAR_VARS)\r\n\r\nLOCAL_MODULE    := streaming\r\nLOCAL_SRC_FILES := stream-server.c stream-client.c stream-denoise.c\r\nLOCAL_SHARED_LIBRARIES := load_model gstreamer_android tensorflowlite_c\r\nLOCAL_LDLIBS := -llog\r\ninclude $(BUILD_SHARED_LIBRARY)\r\n\r\ninclude $(CLEAR_VARS)\r\n\r\nLOCAL_CPP_EXTENSION := .cc\r\nLOCAL_MODULE    := load_model\r\nLOCAL_SRC_FILES := load-model.cc\r\nLOCAL_SHARED_LIBRARIES := tensorflowlite_c\r\nLOCAL_CPPFLAGS := -std=c++14\r\ninclude $(BUILD_SHARED_LIBRARY)\r\n\r\nifndef GSTREAMER_ROOT_ANDROID\r\n$(error GSTREAMER_ROOT_ANDROID is not defined!)\r\nendif\r\n\r\nifeq ($(TARGET_ARCH_ABI),armeabi)\r\nGSTREAMER_ROOT        := $(GSTREAMER_ROOT_ANDROID)/arm\r\nelse ifeq ($(TARGET_ARCH_ABI),armeabi-v7a)\r\nGSTREAMER_ROOT        := $(GSTREAMER_ROOT_ANDROID)/armv7\r\nelse ifeq ($(TARGET_ARCH_ABI),arm64-v8a)\r\nGSTREAMER_ROOT        := $(GSTREAMER_ROOT_ANDROID)/arm64\r\nelse ifeq ($(TARGET_ARCH_ABI),x86)\r\nGSTREAMER_ROOT        := $(GSTREAMER_ROOT_ANDROID)/x86\r\nelse ifeq ($(TARGET_ARCH_ABI),x86_64)\r\nGSTREAMER_ROOT        := $(GSTREAMER_ROOT_ANDROID)/x86_64\r\nelse\r\n$(error Target arch ABI not supported: $(TARGET_ARCH_ABI))\r\nendif\r\n\r\nGSTREAMER_NDK_BUILD_PATH  := $(GSTREAMER_ROOT)/share/gst-android/ndk-build/\r\nGSTREAMER_PLUGINS         := coreelements audiotestsrc audioconvert audioresample audiobuffersplit udp rtp rtpmanager opensles opus nice dtls srtp webrtc playback\r\nGSTREAMER_EXTRA_DEPS      := gstreamer-net-1.0 gstreamer-sdp-1.0 gstreamer-webrtc-1.0\r\nGSTREAMER_EXTRA_LIBS      := -liconv\r\ninclude $(GSTREAMER_NDK_BUILD_PATH)/gstreamer-1.0.mk\r\n\r\ninclude $(CLEAR_VARS)\r\n\r\nLOCAL_MODULE    := tensorflowlite_c\r\nifeq ($(TARGET_ARCH_ABI),armeabi-v7a)\r\nLOCAL_SRC_FILES := $(LOCAL_PATH)/libtensorflowlite_c/armv7/libtensorflowlite_c.so\r\nelse ifeq ($(TARGET_ARCH_ABI),arm64-v8a)\r\nLOCAL_SRC_FILES := $(LOCAL_PATH)/libtensorflowlite_c/arm64/libtensorflowlite_c.so\r\nelse ifeq ($(TARGET_ARCH_ABI),x86)\r\nLOCAL_SRC_FILES := $(LOCAL_PATH)/libtensorflowlite_c/x86/libtensorflowlite_c.so\r\nelse ifeq ($(TARGET_ARCH_ABI),x86_64)\r\nLOCAL_SRC_FILES := $(LOCAL_PATH)/libtensorflowlite_c/x86_64/libtensorflowlite_c.so\r\nelse\r\n$(error Target arch ABI not supported: $(TARGET_ARCH_ABI))\r\nendif\r\n\r\nLOCAL_EXPORT_C_INCLUDES := $(LOCAL_PATH)/tensorflow/tensorflow/lite/\r\nLOCAL_CFLAGS := -I$(LOCAL_PATH)/tensorflow/tensorflow/lite/c/ -I$(LOCAL_PATH)/tensorflow/tensorflow/lite/kernels/\r\ninclude $(PREBUILT_SHARED_LIBRARY)\r\n```\r\n### Tensorflow modifications for `libtensorflowlite_c.so`\r\n\r\n#### tensorflow/lite/c/BUILD\r\n```bazel\r\ntflite_cc_shared_object(\r\n    name = \"tensorflowlite_c\",\r\n    linkopts = select({\r\n        \"//tensorflow:ios\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:macos\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-Wl,--version-script,$(location //tensorflow/lite/c:version_script.lds)\",\r\n        ],\r\n    }),\r\n    per_os_targets = True,\r\n    deps = [\r\n        \":c_api\",\r\n        \":c_api_experimental\",\r\n        \":exported_symbols.lds\",\r\n        \":version_script.lds\",\r\n        \"//tensorflow/lite/kernels:custom_ops\", # added this\r\n    ],\r\n)\r\n```\r\n\r\n#### tensorflow/lite/c/version_scripts.lds\r\n```\r\nVERS_1.0 {\r\n  # try to make everything global for now\r\n  global:\r\n    *;\r\n};\r\n```\r\n#### tensorflow/lite/c/exported_symbols.lds\r\n```\r\n# try to make everything global for now\r\n*\r\n```\r\n#### tensorflow/lite/kernels/custom_ops_register.h\r\n```C++\r\n#ifndef TENSORFLOW_LITE_KERNELS_CUSTOM_OPS_REGISTER_H_\r\n#define TENSORFLOW_LITE_KERNELS_CUSTOM_OPS_REGISTER_H_\r\n\r\n#include \"tensorflow/lite/c/common.h\"\r\n\r\nnamespace tflite {\r\nnamespace ops {\r\nnamespace custom {\r\n\r\nTfLiteRegistration* Register_HASHTABLE();\r\nTfLiteRegistration* Register_HASHTABLE_FIND();\r\nTfLiteRegistration* Register_HASHTABLE_IMPORT();\r\nTfLiteRegistration* Register_HASHTABLE_SIZE();\r\nTfLiteRegistration* Register_IMAG();\r\nTfLiteRegistration* Register_MULTINOMIAL();\r\nextern \"C\" TfLiteRegistration* Register_RANDOM_STANDARD_NORMAL(); # added `extern \"C\"`\r\nTfLiteRegistration* Register_REAL();\r\nTfLiteRegistration* Register_RFFT2D();\r\n\r\n}  // namespace custom\r\n}  // namespace ops\r\n}  // namespace tflite\r\n\r\n#endif  // TENSORFLOW_LITE_KERNELS_CUSTOM_OPS_REGISTER_H_\r\n```\r\n#### bazel build command\r\n```\r\n$ bazel build --verbose_failures --config=monolithic --config android_arm64 -c opt //tensorflow/lite/c:libtensorflowlite_c.so\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=289\r\nINFO: Reading rc options for 'build' from /Users/corey/Desktop/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/corey/Desktop/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /Users/corey/Desktop/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/corey/.pyenv/versions/3.7.3/bin/python3 --action_env PYTHON_LIB_PATH=/Users/corey/.pyenv/versions/3.7.3/lib/python3.7/site-packages --python_path=/Users/corey/.pyenv/versions/3.7.3/bin/python3 --config=xla --action_env ANDROID_NDK_HOME=/Users/corey/library/Android/sdk/ndk-bundle --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=29.0.3 --action_env ANDROID_SDK_API_LEVEL=29 --action_env ANDROID_SDK_HOME=/Users/corey/library/Android/sdk --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /Users/corey/Desktop/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/corey/Desktop/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/corey/Desktop/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:monolithic in file /Users/corey/Desktop/tensorflow/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:android_arm64 in file /Users/corey/Desktop/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nINFO: Found applicable config definition build:android in file /Users/corey/Desktop/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nINFO: Analyzed target //tensorflow/lite/c:libtensorflowlite_c.so (1 packages loaded, 108 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/c:libtensorflowlite_c.so up-to-date:\r\n  bazel-bin/tensorflow/lite/c/libtensorflowlite_c.so\r\nINFO: Elapsed time: 1.380s, Critical Path: 0.93s\r\nINFO: 1 process: 1 local.\r\nINFO: Build completed successfully, 2 total actions\r\n```\r\n## Error\r\n```\r\n[arm64-v8a] Prebuilt       : libtensorflowlite_c.so <= ../app/src/main/jni/libtensorflowlite_c/arm64/\r\n[arm64-v8a] SharedLibrary  : libload_model.so\r\n\r\n../app/build/intermediates/ndkBuild/debug/obj/local/arm64-v8a/objs-debug/load_model/load-model.o: In function `load_model':\r\n../app/src/main/jni/load-model.cc:13: undefined reference to `Register_RANDOM_STANDARD_NORMAL'\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\n", "comments": ["On the TFLite mailing list, @jdduke suggested a temporary workaround:\r\n- add a `extern \"C\"` wrapper head to `custom_ops_register.h`\r\n```C\r\nextern \"C\" {\r\nTFL_CAPI_EXPORT TfLiteRegistration* TfLiteRegisterRandomStandardNormal();\r\n}\r\n```\r\n- add a `extern \"C\"` wrapper implementation to `random_standard_normal.cc`\r\n```C++\r\nextern \"C\" {\r\nTFL_CAPI_EXPORT TfLiteRegistration* TfLiteRegisterRandomStandardNormal() {\r\n  return tflite::ops::custom::Register_RANDOM_STANDARD_NORMAL();\r\n}\r\n}\r\n```\r\n- ensure `//tensorflow/lite/kernels:custom_ops` is included as a dependency in `tensorflow/lite/c/BUILD`\r\n```bazel\r\ntflite_cc_shared_object(\r\n    name = \"tensorflowlite_c\",\r\n    linkopts = select({\r\n        \"//tensorflow:ios\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:macos\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-Wl,--version-script,$(location //tensorflow/lite/c:version_script.lds)\",\r\n        ],\r\n    }),\r\n    per_os_targets = True,\r\n    deps = [\r\n        \":c_api\",\r\n        \":c_api_experimental\",\r\n        \":exported_symbols.lds\",\r\n        \":version_script.lds\",\r\n        \"//tensorflow/lite/kernels:custom_ops\", # here\r\n    ],\r\n)\r\n```\r\n- modify my C++ code to call this new wrapper function\r\n```C++\r\nTfLiteInterpreterOptionsAddCustomOp(options, \"RandomStandardNormal\", TfLiteRegisterRandomStandardNormal(), 1, 1);\r\n```\r\n\r\nAnd it worked! Thank your Jared, my tensors have finally allocated on android :)", "Glad to hear it! Like I said, we're hoping to make this a builtin op soon, it'll be in the release notes when that happens. Cheers.", "@CoreyCole Starting TF 2.8, we support RandomUniform, RandomStandardNormal and Multinomial as built-in ops in TFLite. Let us know if you can convert models without using custom ops. "]}, {"number": 44663, "title": "Updated Estimator Version to 2.4.0-rc0", "body": "", "comments": []}, {"number": 44662, "title": "Update version numbers for TensorFlow 2.4.0-rc1", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 4 -> 4\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.4.0-rc0\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.4.0rc0\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 44661, "title": "[INTEL MKL] Fixed unit test failures of matmul_op_test, c_api_test an\u2026", "body": "\u2026d tensordot_op_test in MKL optimized tensorflow caused by recent matmul and batch_matmul op merge", "comments": []}, {"number": 44660, "title": "PSv2: Only instantiate monitoring.ExponentialBuckets if enable_metric\u2026", "body": "\u2026s is True.\r\n\r\nThis is to avoid a memory leak that's occurring in ExponentialBuckets.\r\n\r\nFixes #44192.\r\n\r\nPiperOrigin-RevId: 340920893\r\nChange-Id: I25acd8984c8df5bc63bf03208582b28ca1f64a35", "comments": []}, {"number": 44657, "title": "InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.", "body": "**System information**\r\n\r\n- Linux Ubuntu 16.04\r\n- TensorFlow 2.2\r\n- Python 3\r\n\r\n**Describe the current behavior**\r\n```\r\n\r\nuser_input = Input( shape=(None, 1), dtype=tf.int32 )\r\n# same error with: user_input = Input( shape=(1,), dtype=tf.int32 )\r\n\r\nMF_Embedding_User = tf.Variable(tf.random.normal([8176, 20], stddev=0.05), dtype=tf.float32, name='user_embedding')\r\n\r\nuser_embedding = tf.nn.embedding_lookup(MF_Embedding_User, user_input)\r\n#same error with: tf.gather\r\n\r\nuser_input = [0,1,2,3,4,5]\r\nmodel = Model( user_input, user_embedding )\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI would like to define an Input vector of indices that I can use whether in a tf.ebedding_lookup or a tf.gather\r\nIt used to work in TF1 with placeholders and not anymore.\r\n\r\nI don't know how i can Input my index vectors in my model\r\nI know it is possible to define a @tf.function but idk how to do this and still be abble to train my model\r\nI am really stuck.\r\n\r\nThank you for your help :)", "comments": ["@MaxHeuillet \r\nPlease, refer similar issue #37441 and see if it helps you.\r\nIf the issue still persists, request you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster.Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44657\">No</a>\n"]}, {"number": 44656, "title": "ERROR: Could not find a version that satisfies the requirement tensorflow==2.3 (from versions: none)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["![image](https://user-images.githubusercontent.com/42645864/98389553-38719980-207a-11eb-9106-f28b68a41531.png)\r\n\r\nGetting error on installing TENSORFLOW", "Having an identical issue with installing Tensorflow on a clean Python reinstallation using pip. 64 bit Win10 Pro, CPU only. Looking for a workaround if an immediate solution is not possible.\r\n![Screenshot 2020-11-07 154507](https://user-images.githubusercontent.com/30355572/98451231-d5334600-2111-11eb-9654-ba77c1d0ea16.png)\r\n\r\n\r\n", "Workaround found: current version of Tensorflow is not compatible with Python 3.9; I was able to get it to install by first installing Python 3.8.", "> Getting error on installing TENSORFLOW\r\n\r\n@Harshit-Kesar,\r\nFrom the error log, it seems like you have installed 32-bit Python. TensorFlow is tested and supported on 64-bit systems. For more information, please take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements). \r\n\r\nCould you please install 64-bit Python and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44656\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44656\">No</a>\n"]}, {"number": 44655, "title": "[Go] package go-gettable, update wrappers, fix tests", "body": "Hi,\r\n\r\nfrom several months, as documented by these issues:\r\n\r\n- https://github.com/tensorflow/tensorflow/issues/39307\r\n- https://github.com/tensorflow/tensorflow/issues/41808\r\n- https://github.com/tensorflow/tensorflow/issues/35133\r\n\r\nThe simple process of `go get github.com/tensorflow/tensorflow/tensorflow/go` was not possible without errors.\r\n\r\nThis pull request solves this issue by committing the compiled .proto files to the expected location `tensorflow/go/core/framework/for_core_protos_go_proto/` in order to make the repo go-gettable again.\r\n\r\nMoreover, I added a simple `.gitignore` in the `go` folder that ignores some common output/debugging files/folder.\r\n\r\nI also fixed a failing test ` TestOperationAttrs` because the type assertion to `*tf.Tensor` was not always possible, and thus I had to move the `reflect.DeepEqual(out, want)` control.\r\n\r\n**note** because of this bug of the Go compiler (https://github.com/golang/go/issues/42032) it's highly recommended to set the requirement for the Go language to be != 1.15.3 - otherwise the `(*Graph).Operations()` method fails, as I documented here: https://twitter.com/paolo_galeone/status/1324087418086215683\r\n\r\n\r\nMoreover, I also committed the update generated wrappers (file op/wrappers.go) that contains the updated wrappers, in this way the package is ready to be go-gettable\r\n\r\n", "comments": ["> Unfortunately, in general we don't allow generated protobuf files to be checked-in to the repository as the protos can change and this would be a maintenance issue. Do you know if there is an alternative to this?\r\n\r\nI guess it's not possible - or better, there is the option to clone the TensorFlow repository and do the protoc invocation manually - but this is not handy and more importantly, it doesn't solve the problem of go-getting the tensorflow package without errors.\r\n\r\n\r\nHowever, the TensorFlow/Go package already suffers from this maintenance issue. For example, the op/wrappers.go file, is a generated file that must be manually updated and committed in order to have the correct (updated) wrappers for the C API.\r\n\r\nSo I guess since the Go package already works in this way, having the generated proto files committed in the repo doesn't change the maintenance issues, but at least gives to the users the possibility of go-getting the package", "Hey @jhseu would like your thoughts on this.", "Hmm, maybe in another repository. Definitely not in this repo. The main concern from the Go team was the conflict between the bazel version and the go get version, but we don't expose the bazel version anyway.\r\n\r\nI wonder if we should make tensorflow/go and make it the long-term home of the go bindings, and move the generated protos there?\r\n\r\nAlso, since this kind of change is difficult to review because of the generated code, I'd also rather we generate them here.", "> Hmm, maybe in another repository. Definitely not in this repo. The main concern from the Go team was the conflict between the bazel version and the go get version, but we don't expose the bazel version anyway.\r\n> \r\n> I wonder if we should make tensorflow/go and make it the long-term home of the go bindings, and move the generated protos there?\r\n> \r\n> Also, since this kind of change is difficult to review because of the generated code, I'd also rather we generate them here.\r\n\r\nI really like the idea of creating tensorflow/go and make it the home for the Go bindings :+1: \r\n\r\nSeparating the go stuff from all the other TensorFlow stuff (the whole huge main repo, and so on) will make the Go part easier to maintain (and finally we could go-get the package without downloading several GB of repo :S) - but I do understand the concerns about the review of the generated code (or better, the possibility of misalignments between the tensorflow/go and tensorflow/tensorflow).\r\n\r\nP.S: I don't know if you know my project tfgo (https://github.com/galeone/tfgo) but my project, and other projects that depend on the Go bindings, will benefit from having all the proto generated Go files in the same location together with the Go wrappers [if you look at the prerequisite section of the README.md of tfgo you'll understand why - I have to depend on a fork I have to maintain of TensorFlow, that has all the proto files compiled and available in the go folder)", "@jhseu Can you please take a look on the above comment from @galeone. Thanks!", "Hey @gbaned and @jhseu, I was wondering if there was any progress in planning/thinking about moving generated go files to tensorflow/go; or some other officially-supported ways to make it go gettable without errors? Thanks!", "> Hey @gbaned and @jhseu, I was wondering if there was any progress in planning/thinking about moving generated go files to tensorflow/go; or some other officially-supported ways to make it go gettable without errors? Thanks!\r\n\r\nI maintain a fork: https://github.com/galeone/tensorflow\r\n\r\n```\r\ngo get github.com/galeone/tensorflow/tensorflow/go@r2.4-go\r\n```\r\n\r\nYou can also use [tfgo](https://github.com/galeone/tfgo) that depends on the fork and it allows a simplified usage of the Go bindings:\r\n\r\n```\r\ngo get github.com/galeone/tfgo\r\n```", "@jhseu Any update on this PR? Please. Thanks!", "> @jhseu Any update on this PR? Please. Thanks!\r\n\r\nYou can refer to my last comment to use a working solution :) https://github.com/tensorflow/tensorflow/pull/44655#issuecomment-779763454", "@galeone you're awesome for doing this and I trust you, but it would still be ideal if the `go get` command that was specified in the README of this repo _actually worked_.\r\n\r\nSpeaking of which, do you have 2.4.1 available?", "> @galeone you're awesome for doing this and I trust you, but it would still be ideal if the `go get` command that was specified in the README of this repo _actually worked_.\r\n> \r\n> Speaking of which, do you have 2.4.1 available?\r\n\r\nHi, and thank you for the kind words :smile: \r\n\r\nThe go get command in the README of [galeone/tfgo](https://github.com/galeone/tfgo) works if you run it inside a new module (e.g. `go mod init`), followed by `go get github.com/galeone/tensorflow/tensorflow/go@r2.4-go`.\r\n\r\nOtherwise, if you want it installed in your system, you have to go get the package (without the `@r2.4-go` since it's not possible to specify the version if you're not in a module) - it will fail, then you can move inside the repository, do a `git checkout r2.4-go` and it will work :)\r\n\r\nThe currently supported version *is* 2.4.0 - the only one you can download from the [Install TensoFlow for C](https://www.tensorflow.org/install/lang_c) page.", "> > @galeone you're awesome for doing this and I trust you, but it would still be ideal if the `go get` command that was specified in the README of this repo _actually worked_.\r\n> > Speaking of which, do you have 2.4.1 available?\r\n> \r\n> Hi, and thank you for the kind words \ud83d\ude04\r\n> \r\n> The go get command in the README of [galeone/tfgo](https://github.com/galeone/tfgo) works if you run it inside a new module (e.g. `go mod init`), followed by `go get github.com/galeone/tensorflow/tensorflow/go@r2.4-go`.\r\n> \r\n> Otherwise, if you want it installed in your system, you have to go get the package (without the `@r2.4-go` since it's not possible to specify the version if you're not in a module) - it will fail, then you can move inside the repository, do a `git checkout r2.4-go` and it will work :)\r\n> \r\n> The currently supported version _is_ 2.4.0 - the only one you can download from the [Install TensoFlow for C](https://www.tensorflow.org/install/lang_c) page.\r\n\r\nThe version is rather old. It doesn't contain some fixes from TensorFlow go team. e.g., https://github.com/tensorflow/tensorflow/commit/b6d5fa8ab6735880cbd1f501720450680345be98\r\nAs GitHub doesn't support a fork on fork, it only depends on the owner of the repo to import the updates to this repo.\r\n\r\nA workaround is to git clone the whole tensorflow in local dir.\r\nRun tensorflow/tensorflow/go/genop/generate.sh and then copy the pb files to their dir.\r\nThen use the replace rule in go.mod to point own dir for gitHub.com/tensorflow/tensorflow\r\nreplace github.com/tensorflow/tensorflow/tensorflow/go => ../tensorflow/tensorflow/go\r\nAfter this, you can build and run other code using tensorflow go package as usual.\r\n", "> > > @galeone you're awesome for doing this and I trust you, but it would still be ideal if the `go get` command that was specified in the README of this repo _actually worked_.\r\n> > > Speaking of which, do you have 2.4.1 available?\r\n> > \r\n> > \r\n> > Hi, and thank you for the kind words smile\r\n> > The go get command in the README of [galeone/tfgo](https://github.com/galeone/tfgo) works if you run it inside a new module (e.g. `go mod init`), followed by `go get github.com/galeone/tensorflow/tensorflow/go@r2.4-go`.\r\n> > Otherwise, if you want it installed in your system, you have to go get the package (without the `@r2.4-go` since it's not possible to specify the version if you're not in a module) - it will fail, then you can move inside the repository, do a `git checkout r2.4-go` and it will work :)\r\n> > The currently supported version _is_ 2.4.0 - the only one you can download from the [Install TensoFlow for C](https://www.tensorflow.org/install/lang_c) page.\r\n> \r\n> The version is rather old. It doesn't contain some fixes from TensorFlow go team. e.g., [b6d5fa8](https://github.com/tensorflow/tensorflow/commit/b6d5fa8ab6735880cbd1f501720450680345be98)\r\n> As GitHub doesn't support a fork on fork, it only depends on the owner of the repo to import the updates to this repo.\r\n> \r\n> A workaround is to git clone the whole tensorflow in local dir.\r\n> Run tensorflow/tensorflow/go/genop/generate.sh and then copy the pb files to their dir.\r\n> Then use the replace rule in go.mod to point own dir for gitHub.com/tensorflow/tensorflow\r\n> replace github.com/tensorflow/tensorflow/tensorflow/go => ../tensorflow/tensorflow/go\r\n> After this, you can build and run other code using tensorflow go package as usual.\r\n\r\nI'm going to update in the next few days the fork and the library - with the support for TensorFlow 2.5 since it's now been released", "@galeone  Any update on this PR? Please. Thanks!", "> @galeone Any update on this PR? Please. Thanks!\r\n\r\nI guess I can close this pull request, it makes no sense now.\r\n\r\nI maintain a fork (galeone/tensorflow) that works with a simple `go get` and it's automatically downloaded when you use [tfgo](https://github.com/galeone/tfgo).\r\nI recommend you to use this approach for using TensorFlow in Go - it's way easier.", "I would love to see any update on this one.\r\nPretty please :)", "> I would love to see any update on this one. Pretty please :)\r\n\r\nThe pull request is closed, I have a fork of TensorFlow that is go-gettable and I recommend to use it instead of the official one. Moreover, you can also use tfgo that's a wrapper I built on top of it to simplify the usage of the Tensorflow's Go bindings: [tfgo](https://github.com/galeone/tfgo)"]}, {"number": 44654, "title": "Version requirements for Python dependencies too strict", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nUp until 2.3.1 the version requirements in the setup.py file where of the form `foo >=x.y` or `foo >=x.y,<z.q`, i.e. there was a minimum and a potential maximum version specified. This allows for TensorFlow to be installed alongside other Python packages with own requirements.\r\nHowever https://github.com/tensorflow/tensorflow/commit/aafe25d3f97d645bca91b34a9476eb770d1abf90 changed that significantly and IMO to the worse: It basically pins a specific major and minor version.\r\n\r\nI think this is overly eager. For example TF works fine with protobuf 3.9.2 or 3.10 but now requires 3.13.x. Similar the wheel version doesn't really matter much (a 0.26 is already enough) but it now requires a 0.35.x\r\n\r\nThis is really bad for forward and backward compatibility. For example the system Python (e.g. on a HPC cluster, which can't be changed) could include a newer wheel (say 0.36.0) for e.g. security reasons. Now TF cannot be installed in that environment any longer even though there is no actual reason but that arbitrary limitation to a version which might have been the most recent at some point in time. Also an already installed wheel 0.33.0 can't be used although everything would work.\r\n\r\nAnd also cross compatibility is affected. Let's say a user needs to install a software which needs a feature from wheel 0.36, this can now not be installed because TF limited the version to 0.35\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone using TF in existing environments and/or with other packages\r\n\r\n**Any Other info.**\r\n\r\nMy suggestion is to revert the version list back to what it was using min and max versions and be quite liberal there too (e.g. numpy >=1.16 works just fine and even just limiting to either pre 1.19 or post 1.19 would exclude whole classes of other software). The `~=` notation can be used where it makes sense, e.g. for `tensorboard` whose major and minor version must match the TF version.", "comments": ["Based on my reading, `wheel ~= 0.35` means any `wheel` version from 0.35, 0.36, ... up to 1.0 (exclusive) can be installed, as the resolver picks the latest version from cache that still satisfies all the constraints.\r\n\r\n`~x.y` means every version after `x.y` but before `x+1.0`.\r\n\r\nWe should not be using `>= x.y` because that harms releasing patch releases. We've had issues when a dependency released a new version and suddenly tests started failing.", "This is true, wheel is currently set at >=0.36 < 1.0. However that is needlessly to high. E.g. on our cluster we have wheel 0.33.6 installed (such stuff usually gets installed once and updated once a year when a new toolchain generation gets installed which changes pretty much everything) and that version would well be enough. There are no features used which are not present in 0.33 so the requirement is too strict and was likely simply generated by simply using whatever was the latest version at that time. IMO the better approach would be to translate the old version requirements (in this case: `'wheel >= 0.26'`) to the new feature which would be `wheel ~= 0.26`.\r\nFor `six` it is even worse: Before: `'six >= 1.12.0'`, Now: `six~=1.15.0`, so this is restricted to the 1.15.x versions and 1.12 or 1.16 or not accepted which I see no reasoning for.\r\n\r\nSo my point is not that it is wrong to restrict the major version, but that indistinctly restricting the minor version and raising the minimum version without a reason should be avoided to not break peoples environments.\r\nTo me it looks like an oversight in a search&replace action not an intentional change.", "That makes sense. I will try expanding the bounds.\r\n\r\nThe change was to go ahead of the new pip resolver, to not cause errors. That's why we picked restrictive bounds for now and will slowly expand, based on this feedback.", "> That makes sense. I will try expanding the bounds.\r\n> \r\n> The change was to go ahead of the new pip resolver, to not cause errors. That's why we picked restrictive bounds for now and will slowly expand, based on this feedback.\r\n\r\nDo you have any specific testsuite to run tensorflow with changes in requirements? It might be a computationally intensive task to check all the TF parts so having a testsuite specific for this case might be nice.", "There is no simpler test-suite. We are building the pip and testing that it can be installed with the newer pip resolver.\r\n\r\nAlso, linking #44467 to this one since it is mostly caused by overly-large bounds.", "It seems we will need to increase upper bounds of dependencies to support python3.9 without overcomplicating `setup.py`", "There is also https://github.com/tensorflow/tensorflow/issues/35027#issuecomment-734791293 that suggests another issue with too lax dependencies.", "> There is also #35027 (comment) that suggests another issue with too lax dependencies.\r\n\r\nNot really. The users reported an incompatibility with absl-py < 0.9 but TF has required >= 0.9 before the change mentioned here, so that isn't a recent issue at least", "Agree, it's just a datapoint to not go below for absl.", "Another datapoint (for when we can finally tackle this): numpy should be restricted to be either above 1.20 (and drop py36 support) or below 1.19.\r\n\r\nBreaking change: https://numpy.org/doc/stable/release/1.20.0-notes.html#the-pyarray-descrcheck-macro-is-modified", "> Breaking change: https://numpy.org/doc/stable/release/1.20.0-notes.html#the-pyarray-descrcheck-macro-is-modified\r\n\r\nFWIW: This macro is not used in TF and a portable solution exists (use the explicit function)", "Most builds were failing yesterday because of numpy 1.20. We are currently limiting all dependencies/numpy so that we don't get numpy 1.20 in the CI.", "> Most builds were failing yesterday because of numpy 1.20. We are currently limiting all dependencies/numpy so that we don't get numpy 1.20 in the CI.\r\n\r\nAlso ran into 1.20 not being supported. Compiled tf 2.4.1 and the tf wheel wants to remove my numpy 1.20 and replace it with 1.19.5. In addition I have to rebuild at least `pandas` - my pandas was built w/ numpy 1.20 installed, and trying to `import pandas` after numpy change 1.20->1.19.5 now says `ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject`.", "> Another datapoint (for when we can finally tackle this): numpy should be restricted to be either above 1.20 (and drop py36 support) or below 1.19.\r\n\r\n@mihaimaruseac If I read this correctly, should TensorFlow=2.4.x use numpy>1.20 for Python>=3.7 and numpy<1.19 for Python==3.6?", "@fridex sure, but this will have diverging behavior (also, 1.19.5 is good)\r\n@edrozenberg that's why we're limiting numpy to ~1.19.2 to get all the patches on 1.19 series.\r\n\r\nOnce we can drop support for python 3.6 (or numpy adds it back), we'll invest into moving to `> 1.20`)", "> @fridex sure, but this will have diverging behavior (also, 1.19.5 is good)\r\n\r\n@mihaimaruseac , thanks for the clarification. I'm not sure if https://github.com/tensorflow/tensorflow/issues/47117 is somehow associated as there were issues with numpy==1.19.5. Is it OK to always run numpy==1.19.5 with TensorFlow 2.4 or is there a specific issue with numpy=1.19.5? Sorry, it's not directly clear to me from the issue and logs, especially https://github.com/tensorflow/tensorflow/issues/47117 confuses me. Thanks for your time.", "1.19.5 is good. We have no successful 1.20 build (we did not try to get one)", "\r\n>  numpy >=1.16 works just fine and even just limiting to either pre 1.19 or post 1.19 would exclude whole classes of other software). T\r\n\r\nThis is exactly our issue.", "Just ran into another problem caused by TensorFlow using `~=` instead of `>=` for its dependencies:\r\n\r\n```\r\nERROR: Cannot install -r requirements.txt (line 11), -r requirements.txt (line 17) and -r requirements.txt (line 33) because these package versions have conflicting dependencies.\r\nThe conflict is caused by:\r\n    mypy 0.910 depends on typing-extensions>=3.7.4\r\n    pylint 2.11.1 depends on typing-extensions>=3.10.0; python_version < \"3.10\"\r\n    tensorflow 2.6.0 depends on typing-extensions~=3.7.4\r\n```\r\n\r\nSo basically, we can't update `pylint` because of that. :neutral_face:\r\n\r\nI think it would be *much* better if TensorFlow would require `typing-extensions>=3.7.4` instead of `typing-extensions~=3.7.4`.", "This should be fixed now, I think?", "> This should be fixed now, I think?\r\n\r\nJust tried this minimal `Dockerfile` with the latest versions of `pylint` and `tensorflow`:\r\n\r\n```Dockerfile\r\nFROM python:3.9\r\nRUN pip install pylint==2.11.1 tensorflow==2.6.0\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nERROR: Cannot install pylint==2.11.1 and tensorflow==2.6.0 because these package versions have conflicting dependencies.\r\n\r\nThe conflict is caused by:\r\n    pylint 2.11.1 depends on typing-extensions>=3.10.0; python_version < \"3.10\"\r\n    tensorflow 2.6.0 depends on typing-extensions~=3.7.4\r\n```\r\n\r\nSo, no, it's not fixed.", "Oh, it's fixed in nightly, not in the released TF.", "Oh, sorry. And thanks! :slightly_smiling_face:\r\n\r\nThe following works nicely:\r\n\r\n```Dockerfile\r\nFROM python:3.9\r\nRUN pip install pylint==2.11.1 tf-nightly==2.8.0.dev20211005\r\n```\r\n\r\n:tada:", "We'll cherry-pick this on the 2.7 branch and from RC1 (RC0 is too late as the builds started) this should be the default", "This should be fully fixed now.", "why this issue is closed ? even working with 2.6 version still requires typing extensions ~3.7.4 which many libraries cannot work with.  why all other libraries can be more loose about the version ?\r\n```\r\nThere are incompatible versions in the resolved dependencies:\r\n  typing-extensions~=3.7.4 (from tensorflow==2.6.0->-r requirements.in (line 8))\r\n  typing-extensions>=4.1.0 (from mypy-boto3-redshift-data==1.21.34->apache-airflow-providers-amazon==1!3.2.0->-r requirements.in (line 21))\r\n  typing_extensions (from cattrs==1.10.0->apache-airflow==1!2.2.5+astro.2->-r requirements.in (line 30))\r\n  typing-extensions>=3.7.4.3 (from gitpython==3.1.27->mlflow==1.24.0->-r requirements.in (line 4))\r\n  typing-extensions>=3.6.4 (from importlib-metadata==4.11.3->moto==3.1.4->-r requirements.in (line 52))\r\n  typing-extensions>=3.7.4 (from apache-airflow==1!2.2.5+astro.2->-r requirements.in (line 30))\r\n  typing-extensions<5.0,>=4.0.0 (from rich==12.2.0->apache-airflow==1!2.2.5+astro.2->-r requirements.in (line 30))\r\n  typing-extensions>=4.1.0 (from mypy-boto3-rds==1.21.34->apache-airflow-providers-amazon==1!3.2.0->-r requirements.in (line 21))\r\n  typing-extensions (from kiwisolver==1.4.2->matplotlib==3.5.1->-r requirements.in (line 19))\r\n\r\n```\r\n", "Because this is fixed form TF 2.7 onwards", "> Because this is fixed form TF 2.7 onwards\r\n\r\n:+1: "]}, {"number": 44653, "title": "Go: restore the possibility of go getting tensorflow and fix tests", "body": "Hi,\r\n\r\nfrom several months, as documented by these issues:\r\n\r\n- https://github.com/tensorflow/tensorflow/issues/39307\r\n- https://github.com/tensorflow/tensorflow/issues/41808\r\n- https://github.com/tensorflow/tensorflow/issues/35133\r\n\r\nThe simple process of `go get github.com/tensorflow/tensorflow/tensorflow/go` was not possible without errors.\r\n\r\nThis pull request solves this issue by committing the compiled .proto files to the expected location `tensorflow/go/core/framework/for_core_protos_go_proto/` in order to make the repo go-gettable again.\r\n\r\nMoreover, I added a simple `.gitignore` in the `go` folder that ignores some common output/debugging files/folder.\r\n\r\nI also fixed a failing test ` TestOperationAttrs` because the type assertion to `*tf.Tensor` was not always possible, and thus I had to move the `reflect.DeepEqual(out, want)` control.\r\n\r\n**note** because of this bug of the Go compiler (https://github.com/golang/go/issues/42032) it's highly recommended to set the requirement for the Go language to be != 1.15.3 - otherwise the `(*Graph).Operations()` method fails, as I documented here: https://twitter.com/paolo_galeone/status/1324087418086215683\r\n\r\n\r\n", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44653) for more info**.\n\n<!-- need_author_consent -->", " @googlebot I consent", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44653) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44653) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 44652, "title": "Go: make ReadTensor return error for shapes with negative dimention", "body": "This PR fixes `SIGABRT: abort` when shape with negative dimension is passed to `ReadTensor` function\r\n", "comments": []}, {"number": 44659, "title": "Fatal Exception: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found", "body": "**System information**\r\n\r\n- Mobile device (vivo 1904, Pixel 2, etc),\r\n- tenserflow lite\r\n- build.gradle\r\n\r\n```\r\n'lite'    : 'org.tensorflow:tensorflow-lite:2.0.0',\r\n'liteGpu': 'org.tensorflow:tensorflow-lite-gpu:2.0.0'\r\n```\r\n\r\n**Describe the current behavior**\r\nException is thrown on invoking `Interpreter.runForMultipleInputsOutputs()` on the model collected from https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite\r\n\r\n**Describe the expected behavior**\r\n\r\nSuccessful recognition of posture.\r\n\r\n**Standalone code to reproduce the issue**\r\nI am getting this on crash analysis tool. Since this is not reproducible on my devices I am not able to produce a standalone code.\r\n\r\n\r\n**Other info / logs** \r\n\r\n```\r\nFatal Exception: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: OpenCL library not loaded - dlopen failed: library \"libOpenCL-pixel.so\" not found\r\nFalling back to OpenGL\r\nTfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer.\r\nNode number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n\r\n       at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java)\r\n       at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:171)\r\n       at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n       at a.b.c.e.Posenet.estimateSinglePose(Posenet.java:296)\r\n       at a.b.c.d.ImageAnalyser.processImage(ImageAnalyser.java:176)\r\n       at a.b.c.d.ImageAnalyser.access$processImage(ImageAnalyser.java:26)\r\n       at a.b.c.d.ImageAnalyser$processImageForAnalysis$2.invokeSuspend(ImageAnalyser.java:141)\r\n       at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(BaseContinuationImpl.java:33)\r\n       at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.java:56)\r\n       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n       at java.lang.Thread.run(Thread.java:764)\r\n```\r\n\r\nI am following this sample https://github.com/tensorflow/examples/blob/master/lite/examples/posenet/android/posenet/src/main/java/org/tensorflow/lite/examples/posenet/lib/Posenet.kt\r\n", "comments": ["Hello @amahendrakar \r\nPlease let me know if there are any other info I can help you with.", "Hi,\r\n\r\nAs the log mentions the following, it means the missing OpenCL lib isn't a issue here, and OpenGL is used instead for the GPU delegate. However, there's some memory related issue. \r\n\"\r\nFalling back to OpenGL\r\nTfLiteGpuDelegate Invoke: Write to buffer failed. Source data is larger than buffer.\r\n\",\r\n\r\nConsidering you are running the example w/ tflite 2.0 and tflite-gpu-delegate 2.0, I'm wondering does this same issue show up with more recent builds (such as tflite 2.3.0, tflite 2.4.0rc1, or tflite nightly)? We have fixed several memory related bugs in our gpu delegate code base since then.", "Thank you for your response @multiverse-tf \r\n\r\nI will update to `2.3.0` and revert on this.", "I am getting same issue in realme and lenovo devices, I am using nightly build\r\n\r\n```\r\nFatal Exception: java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Following operations are not supported by GPU delegate:\r\nSPLIT: Operation is not supported.\r\nSPLIT_V: Operation is not supported.\r\n94 operations will run on the GPU, and the remaining 58 operations will run on the CPU.\r\nCan not open OpenCL library on this device - dlopen failed: library \"libOpenCL.so\" not found\r\nFalling back to OpenGL\r\nTfLiteGpuDelegate Init: Add does not support HWC constant tensor\r\nTfLiteGpuDelegate Prepare: delegate is not initialized\r\n```", "Here is log from other device (\"lenovo\")\r\n\r\n\r\n   ```\r\n SPLIT: Operation is not supported.\r\n    SPLIT_V: Operation is not supported.\r\n    94 operations will run on the GPU, and the remaining 58 operations will run on the CPU.\r\n    Can not open OpenCL library on this device - dlopen failed: library \"libOpenCL.so\" not found\r\n    Falling back to OpenGL\r\n    TfLiteGpuDelegate Init: Add does not support HWC constant tensor\r\n    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n    Node number 152\r\n```", "Hello!\r\n\r\nAfter update to `2.3.0` there is no \"source data larger than the buffer\" issues. But I am getting these two with the same stacktrace.\r\n\r\n```\r\nFatal Exception: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Can not open OpenCL library on this device - dlopen failed: library \"libOpenCL.so\" not found\r\nFalling back to OpenGL\r\nTfLiteGpuDelegate Invoke: GpuDelegate must run on the same thread where it was initialized.\r\nNode number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n```\r\n\r\n```\r\nFatal Exception: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: TfLiteGpuDelegate Invoke: Failed to read data from GPU (clEnqueueReadBuffer) - Execution status error for events in wait list\r\nNode number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n\r\n```\r\n\r\nThe first one seems to imply there is some threading issues however I am running the estimation on a single threaded executor.\r\nNot sure of the second one. \r\n\r\nPlease let me know if you would like to have a separate ticket for these though.\r\n\r\nThanks!", "> Here is log from other device (\"lenovo\")\r\n> \r\n> ```\r\n> SPLIT: Operation is not supported.\r\n>  SPLIT_V: Operation is not supported.\r\n>  94 operations will run on the GPU, and the remaining 58 operations will run on the CPU.\r\n>  Can not open OpenCL library on this device - dlopen failed: library \"libOpenCL.so\" not found\r\n>  Falling back to OpenGL\r\n>  TfLiteGpuDelegate Init: Add does not support HWC constant tensor\r\n>  TfLiteGpuDelegate Prepare: delegate is not initialized\r\nSuch error messages indicate that the model isn't well supported by the OpenGL backend of the TFLite gpu delegate yet.\r\n\r\n>  Node number 152\r\n> ```\r\n\r\n", "> Hello!\r\n> \r\n> After update to `2.3.0` there is no \"source data larger than the buffer\" issues. But I am getting these two with the same stacktrace.\r\n\r\nGood to know such issues are gone now.\r\n\r\n> \r\n> ```\r\n> Fatal Exception: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Can not open OpenCL library on this device - dlopen failed: library \"libOpenCL.so\" not found\r\n> Falling back to OpenGL\r\n> TfLiteGpuDelegate Invoke: GpuDelegate must run on the same thread where it was initialized.\r\nAs the error msg indicates, pls call invoke on the same thread where the GPU delegate is created as noted here (https://www.tensorflow.org/lite/performance/gpu#android). \r\n\r\nFor more discussions about this requirement, pls check out https://github.com/tensorflow/tensorflow/issues/25657#issuecomment-462498449 and follow-ups.\r\n\r\n> Node number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n> ```\r\n> \r\n> ```\r\n> Fatal Exception: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: TfLiteGpuDelegate Invoke: Failed to read data from GPU (clEnqueueReadBuffer) - Execution status error for events in wait list\r\n> Node number 31 (TfLiteGpuDelegateV2) failed to invoke.\r\n> ```\r\n> \r\n> The first one seems to imply there is some threading issues however I am running the estimation on a single threaded executor.\r\n> Not sure of the second one.\r\n> \r\n> Please let me know if you would like to have a separate ticket for these though.\r\n> \r\n> Thanks!\r\n\r\n", "As the issue has turned to others based on https://github.com/tensorflow/tensorflow/issues/44659#issuecomment-727843592 and https://github.com/tensorflow/tensorflow/issues/44659#issuecomment-727848537, I'll close this issue then. Feel free to file separate tickets to track these new issues.", "i have same error:\r\n    TfLiteGpuDelegate Prepare: Add does not support HWC constant tensor\r\nbut there is not add ops for 3 dims.\r\n![image](https://user-images.githubusercontent.com/26035529/131635786-9269f24c-929e-47a9-94fb-e26815125f26.png)\r\n\r\n", "@WillLiGitHub you're posting a completely irrelevant issue to a closed bug...\r\n\r\nAnyway, your ADD operation below LOGISTIC has only 1 input, which means the other input is a constant.  And I assume that's a 3D tensor.", "@impjdi \r\nthanks for you help , i will check it out"]}, {"number": 44651, "title": "Tensorflow lite Makefile: add support of gpu ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using):  2.3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently we cannot use GPU delegate when compiling tensorflow lite using the Makefile\r\n\r\n**Will this change the current api? How?** No\r\n\r\n**Who will benefit with this feature?** \r\n\r\n**Any Other info.**\r\n", "comments": ["@justeph \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "I am currently trying to build a yocto recipe for tensorflow-lite using Makefile, and I need the GPU delegate. I am trying to modify the Makefile right now, and I will open a merge request when this will be ready", "You can use OpenCL delegate with CMake.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#available_options_to_build_tensorflow_lite"]}, {"number": 44650, "title": "A listing of API symbols exported and supported in a machine readable form", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nI would like to know if there is a listing of API symbols available and exported in a release (public API), I'm interested in Python code specifically. I see docs on the main page, but I would like to access symbols exported per release in a machine-readable form (for example in a JSON format). I couldn't find any related documentation, sorry if there are already such files. Do you plan to provide/support such a listing to the community?\r\n\r\nI hope the issue is addressed correctly in this repo and thanks for any advice in advance.\r\n", "comments": ["When you install the pip package, the public API is under the `api/` directory of the unpacked wheel.", "Thanks, this will work."]}, {"number": 44649, "title": "ValueError: All inputs to `ConcreteFunction`s must be Tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip \r\n- TensorFlow version (use command below): 2.1.2\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.4\r\n- GPU model and memory: gtx1080Ti\r\n\r\n**Describe the current behavior**\r\nTf.train_on_batch cant take numpy array as input, asks for tensor and even using tf.convert_to_tensor doesn't help. It works in Tf2.3, but Tf2.1 is a requirement for compatibility with another library.\r\n\r\n**Describe the expected behavior**\r\nShould train without issues, tested on Tf2.3 and it works, but Tf2.1 is a requirement.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    from sklearn.model_selection import train_test_split\r\n    from tensorflow.keras.layers import (Conv2D,Input)\r\n    from tensorflow.keras.models import Model\r\n    from tensorflow.keras.optimizers import Adam\r\n    import tensorflow_addons as tfa #version 0.9.1\r\n\r\n    opt = Adam(lr=1e-4)\r\n    tf.config.experimental_run_functions_eagerly(True)\r\n\r\n\r\n    def custom_mean_squared_error(y_true, y_pred):\r\n        return(tf.reduce_mean(tf.math.squared_difference(y_true, y_pred)))\r\n\r\n\r\n    def firstStream(ip):\r\n            layer = Conv2D(64, kernel_size=(5, 5), padding='same')(ip)\r\n            op = Conv2D(3, (5, 5), activation='sigmoid', padding='same')(layer)\r\n            return op\r\n        \r\n    def secStream(ip):\r\n            layer = Conv2D(8, kernel_size=(5, 5), padding='same')(ip)\r\n            flow = Conv2D(2, (5, 5), activation='sigmoid', padding='same')(layer)\r\n            return flow\r\n\r\n\r\n    def main():\r\n\r\n        ip = Input(shape=(None, None, 3))\r\n        op = firstStream(ip)\r\n        flow = secStream(ip)\r\n        warped = tf.expand_dims(tfa.image.dense_image_warp(op,flow,name='warp'),axis=1)\r\n        model = Model(ip,warped)\r\n        model.compile(optimizer=opt, loss=custom_mean_squared_error)\r\n            \r\n        numEpochs = 1\r\n \r\n        for epochNo in range(numEpochs):\r\n\r\n                print(\"Running epoch : %d\" % epochNo)\r\n                batch_ip=np.ones((1,64,64,3))\r\n                      \r\n                curr_loss = model.train_on_batch(batch_ip, batch_ip)\r\n                print(curr_loss)\r\n\r\n\r\n    main()\r\n\r\n\r\n\r\n\r\n**Other info / logs** \r\n\r\nmain()\r\n  File \"/home/mohana/work/simple.py\", line 46, in main\r\n    curr_loss = model.train_on_batch(batch_ip, batch_ip)\r\n  File \"/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 1078, in train_on_batch\r\n    standalone=True)\r\n  File \"/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 312, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 269, in _process_single_batch\r\n    grads = tape.gradient(scaled_total_loss, trainable_weights)\r\n  File \"/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\", line 1029, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\", line 77, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1256, in _backward_function_wrapper\r\n    processed_args, remapped_captures)\r\n  File \"/home/mohana/virtualenvs/tfcomp/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1684, in _call_flat\r\n    \"Tensor.\" % (self._func_graph.name, i, str(arg)))\r\nValueError: All inputs to `ConcreteFunction`s must be Tensors; on invocation of __backward__defun_call_1331, the 0-th input (IndexedSlices(indices=tf.Tensor([  65   65   66 ... 4028 4029 4030], shape=(16384,), dtype=int32), values=tf.Tensor(\r\n[[-0.0000000e+00 -0.0000000e+00 -0.0000000e+00]\r\n [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00]\r\n [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00]\r\n ...\r\n [-2.7218828e-05 -2.6730117e-05 -2.4183297e-05]\r\n [-2.7333013e-05 -2.7423557e-05 -2.2896325e-05]\r\n [-2.1508897e-05 -2.1834361e-05 -1.8788463e-05]], shape=(16384, 3), dtype=float32), dense_shape=tf.Tensor([4096    3], shape=(2,), dtype=int32))) was not a Tensor.\r\n\r\n\r\n", "comments": ["@moha23 \r\nThe issue exist in tf 2.1 and it has been fixed in the later versions, as i have tried on nightly and this bug is fixed, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/eef3c2923cd0807f4afbdf56e72ff27e/untitled458.ipynb), can you please upgrade your tf version.", "@Saduf2019 hi, as I mentioned I also tried in tf2.3 and it works without error, but tf2.1 is a requirement due to another libraries dependency. Is there a patch I can apply to the source codes may be?", "@moha23 Please check the TF code base to find which patch works best for you. \r\n\r\nGitHub is mainly for bugs/performance related issues. Please post any support kind of questions in Stackoverflow where there is big community to support and learn. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44649\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44649\">No</a>\n", "Solution here : https://github.com/tensorflow/addons/issues/2232#issuecomment-734057727"]}, {"number": 44648, "title": "[CherryPick 2.4] Fix invalid fusion of Matmul and Mul", "body": "This PR cherrypicks #41790 onto the 2.4 release branch. This bug has been blocking me since TF 2.2 so it would be amazing if the fix could make it into TF 2.4.\r\n\r\n/cc @mihaimaruseac @abattery\r\n\r\nPiperOrigin-RevId: 341026586\r\nChange-Id: Idb578a4cf48c82abaad2f811612c4df4c2f2752c", "comments": []}, {"number": 44647, "title": "Python/Django", "body": "I just wanted to make a new application on Django. There were no problems during installation. But when I decided to create a new project, I received an unknown error. I didn\u2019t find a solution in google. Here is what I wrote: \r\n\r\n\r\n_django-admin startproject mysite_\r\n\r\nThis is what it gave me:\r\n\r\n\r\n _Traceback (most recent call last):\r\n  File \"c:\\users\\danii\\appdata\\local\\programs\\python\\python38-32\\lib\\runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"c:\\users\\danii\\appdata\\local\\programs\\python\\python38-32\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\danii\\AppData\\Local\\Programs\\Python\\Python38-32\\Scripts\\django-admin.exe\\__main__.py\", line 4, in <module>\r\n  File \"c:\\users\\danii\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\django\\core\\management\\__init__.py\", line 12, in <module>\r\n    from django.conf import settings\r\n  File \"c:\\users\\danii\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\django\\conf\\__init__.py\", line 19, in <module>\r\n    from django.core.validators import URLValidator\r\n  File \"c:\\users\\danii\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\django\\core\\validators.py\", line 8, in <module>\r\n    from django.utils.encoding import punycode\r\n  File \"c:\\users\\danii\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\django\\utils\\encoding.py\", line 8, in <module>\r\n    from django.utils.deprecation import RemovedInDjango40Warning\r\n  File \"c:\\users\\danii\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\django\\utils\\deprecation.py\", line 1, in <module>\r\n    import asyncio\r\n  File \"c:\\users\\danii\\appdata\\local\\programs\\python\\python38-32\\lib\\asyncio\\__init__.py\", line 8, in <module>\r\n    from .base_events import *\r\n  File \"c:\\users\\danii\\appdata\\local\\programs\\python\\python38-32\\lib\\asyncio\\base_events.py\", line 40, in <module>\r\n    from . import events\r\n  File \"c:\\users\\danii\\appdata\\local\\programs\\python\\python38-32\\lib\\asyncio\\events.py\", line 783, in <module>\r\n    from _asyncio import (_get_running_loop, _set_running_loop,\r\nAttributeError: module 'traceback' has no attribute 'extract_stack'_\r\n\r\nPython 3.9\r\nDjango  3.1.3\r\nThis error can be very stupid, but I started doing this recently.If you have additional questions, I'm ready to answer\r\nThanks", "comments": ["@ImJustError \r\n\r\nIs this issue related to Tensorflow?.\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request related to Tensorflow. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44647\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44647\">No</a>\n"]}, {"number": 44646, "title": "Bug when a custom tf.keras.models.Model has multiple class inheritance", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows and Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: conda env with Python 3.7.9\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce RTX 2080 Super with Max-Q Design 8GB\r\n\r\n\r\n**Describe the current behavior**\r\nCreating a custom model that inherit of at least one other class than `tf.keras.models.Model`, the following exception is raised:\r\n```\r\nFile \"C:\\Users\\snake\\miniconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\snake\\miniconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 255, in __init__\r\n    inject_functional_model_class(self.__class__)\r\n  File \"C:\\Users\\snake\\miniconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 144, in inject_functional_model_class\r\n    cls.__bases__ = tuple(inject_functional_model_class(base)\r\n  File \"C:\\Users\\snake\\miniconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 144, in <genexpr>\r\n    cls.__bases__ = tuple(inject_functional_model_class(base)\r\n  File \"C:\\Users\\snake\\miniconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 144, in inject_functional_model_class\r\n    cls.__bases__ = tuple(inject_functional_model_class(base)\r\n  File \"C:\\Users\\snake\\miniconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 144, in <genexpr>\r\n    cls.__bases__ = tuple(inject_functional_model_class(base)\r\n  File \"C:\\Users\\snake\\miniconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 144, in inject_functional_model_class\r\n    cls.__bases__ = tuple(inject_functional_model_class(base)\r\n  File \"C:\\Users\\snake\\miniconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 144, in <genexpr>\r\n    cls.__bases__ = tuple(inject_functional_model_class(base)\r\n  File \"C:\\Users\\snake\\miniconda3\\envs\\transformers\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 144, in inject_functional_model_class\r\n    cls.__bases__ = tuple(inject_functional_model_class(base)\r\nTypeError: can't set attributes of built-in/extension type 'object'\r\n```\r\n\r\n**Describe the expected behavior**\r\nBeing able to create a custom model with different mixins.\r\n\r\n**Standalone code to reproduce the issue**\r\nHere a simple piece of code to reproduce the issue:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass PrintMixin:\r\n    def custom_print(self):\r\n        print(\"Hello world\")\r\n\r\nclass CustomModel(tf.keras.models.Model, PrintMixin):\r\n    def __init__(self, *args, **kwargs):\r\n        my_input = tf.keras.layers.Input(shape=(16,))\r\n        dense = tf.keras.layers.Dense(32, activation='relu')\r\n        output = dense(my_input)\r\n        outputs = {\"output\": output}\r\n\r\n        super().__init__(inputs=[my_input], outputs=outputs, *args, **kwargs)\r\n\r\n\r\nmy_model = CustomModel()\r\n```\r\n\r\n**Other info / logs**\r\nApparently when giving the `inputs` and `outputs` parameters, TensorFlow tries to inject an attribute to all the classes and super classes until reaching `tf.keras.models.Model`. Here the piece of code from the file `training.py` line 136:\r\n```\r\ndef inject_functional_model_class(cls):\r\n  \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"\r\n  from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\r\n  from tensorflow.python.keras.engine import training_v1  # pylint: disable=g-import-not-at-top\r\n  if cls == Model or cls == training_v1.Model:\r\n    return functional.Functional\r\n\r\n  cls.__bases__ = tuple(inject_functional_model_class(base)\r\n                        for base in cls.__bases__)\r\n  # Trigger any `__new__` class swapping that needed to happen on `Functional`\r\n  # but did not because functional was not in the class hierarchy.\r\n  cls.__new__(cls)\r\n\r\n  return cls\r\n```\r\n\r\nBut when it tries to check the superclass of my mixin class, which is `object` an error is raised saying that we cannot add an attribute to the `object` type. For me the following update of the method fix the issue:\r\n\r\n```\r\ndef inject_functional_model_class(cls):\r\n  \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"\r\n  from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\r\n  from tensorflow.python.keras.engine import training_v1  # pylint: disable=g-import-not-at-top\r\n  if cls == Model or cls == training_v1.Model:\r\n    return functional.Functional\r\n  if cls == 'object':\r\n    return cls\r\n\r\n  cls.__bases__ = tuple(inject_functional_model_class(base)\r\n                        for base in cls.__bases__)\r\n  # Trigger any `__new__` class swapping that needed to happen on `Functional`\r\n  # but did not because functional was not in the class hierarchy.\r\n  cls.__new__(cls)\r\n\r\n  return cls\r\n```\r\nHere we return the `object` class as it is. But I don't know if it is a proper fix that won't bring another error elsewhere.\r\n\r\nFirst, I wanted to know if it is really a bug?\r\nIf not, how I could do a proper custom model with mixin classes and my inputs/outputs.\r\nIf, yes, is the fix I proposed ok and if needed I can open a PR with it.\r\n\r\nThanks!", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3b58866c9c9e0ac935296d26b903de22/44646-2-3.ipynb). Thanks!", "Thanks for reporting the issue. Let me take a close look.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44646\">No</a>\n", "Has anyone found solution this so far?\r\n\r\nIt seems the issue persists in version 2.4 as well but is gone in 2.5."]}, {"number": 44645, "title": "AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'swapaxes'", "body": "I am building a CNN model. model compile fine but when i fit the model it does not work: The code fine in my jupyttor but not in kaggle\r\n#metric\r\n\r\n```\r\ndef iou_metric_batch(y_true_in, y_pred_in):\r\n    batch_size = y_true_in.shape[0]\r\n    metric = []\r\n    for batch in range(batch_size):\r\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\r\n        metric.append(value)\r\n    return np.array(np.mean(metric), dtype=np.float32)\r\n\r\ndef my_iou_metric(label, pred):\r\n    metric_value = tf.py_function(iou_metric_batch, [label, pred], tf.float32)\r\n    return metric_value\r\n\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[my_iou_metric])\r\ntf.compat.v1.disable_eager_execution()\r\nmodel_path = \"nuclei_finder_unet_1.h5\"\r\n\r\ncheckpoint = ModelCheckpoint(model_path,\r\n                             monitor=\"val_loss\",\r\n                             mode=\"min\",\r\n                             save_best_only = True,\r\n                             verbose=1)\r\n\r\nearlystop = EarlyStopping(monitor = 'val_loss', \r\n                          min_delta = 0, \r\n                          patience = 5,\r\n                          verbose = 1,\r\n                          restore_best_weights = True)\r\n\r\n# Fit our model \r\nresults = model.fit(X_train, Y_train, validation_split=0.1,\r\n                    batch_size=16, epochs=10,callbacks=[earlystop, checkpoint])\r\n\r\n\r\n\r\n```Erroe:\r\n```\r\nnknownError: AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'swapaxes'\r\nTraceback (most recent call last):\r\n\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 242, in __call__\r\n    stateful=None,\r\n\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 131, in __call__\r\n    \"\"\"\r\n\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\r\n\r\n  File \"<ipython-input-99-1bbcb510438c>\", line 58, in iou_metric_batch\r\n    value = iou_metric(y_true_in[batch], y_pred_in[batch])\r\n\r\n  File \"<ipython-input-99-1bbcb510438c>\", line 2, in iou_metric\r\n    labels = label(y_true_in > 0.5)\r\n\r\n  File \"/opt/conda/lib/python3.7/site-packages/skimage/measure/_label.py\", line 93, in label\r\n    return clabel(input, neighbors, background, return_num, connectivity)\r\n\r\n  File \"skimage/measure/_ccomp.pyx\", line 348, in skimage.measure._ccomp.label_cython\r\n\r\n  File \"skimage/measure/_ccomp.pyx\", line 322, in skimage.measure._ccomp.reshape_array\r\n\r\n  File \"skimage/measure/_ccomp.pyx\", line 299, in skimage.measure._ccomp._apply_swaps\r\n\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'swapaxes'\r\n\r\n\r\n\t [[{{node metrics_10/my_iou_metric/EagerPyFunc}}]]\r\n```", "comments": ["@NaghmehShahverdi \r\nPlease share the tf version used, i ran the code shared and face a [different issue](https://colab.research.google.com/gist/Saduf2019/d6341c5809b9c1e823e6cccdd5bbb2e0/untitled458.ipynb).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44645\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44645\">No</a>\n"]}, {"number": 44644, "title": "tf.keras.models.load_model fails if tf.linalg.band_part is used.", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Arch Linux (5.9.4-arch1-1)\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.8.6\r\n- CUDA/cuDNN version: 11.1/8.0.4\r\n- GPU model and memory: NVIDIA GeForce GTX 1080 Ti/10.91GiB\r\n\r\n**Describe the current behavior**\r\n`tf.keras.models.load_model` fails when `tf.linalg.band_part` is used.\r\n~~~\r\nValueError: Inconsistent values for attr 'Tindex' DT_INT32 vs. DT_INT64 while building NodeDef 'tf_op_layer_MatrixBandPart/MatrixBandPart' using Op<name=MatrixBandPart; signature=input:T, num_lower:Tindex, num_upper:Tindex -> band:T; attr=T:type; attr=Tindex:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>\r\n~~~\r\n\r\n**Describe the expected behavior**\r\nModel is loaded without error.\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~\r\nimport tensorflow as tf\r\n\r\nx = tf.keras.Input(shape=(3, 3))\r\ny = tf.linalg.band_part(x, -1, 0)  # getting lower triangle part. I need band_part() for making look-ahead-mask on Transformer.\r\n# y = tf.reshape(x, (-1, 9))\r\n\r\nmodel = tf.keras.Model(x, y)\r\nprint(model.predict([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\r\n                     [[9, 8, 7], [6, 5, 4], [3, 2, 1]]]))\r\nmodel.save('model')\r\n\r\ntf.keras.models.load_model('model')\r\n~~~\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n~~~\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent values for attr 'Tindex' DT_INT32 vs. DT_INT64 while building NodeDef 'tf_op_layer_MatrixBandPart/MatrixBandPart' using Op<name=MatrixBandPart; signature=input:T, num_lower:Tindex, num_upper:Tindex -> band:T; attr=T:type; attr=Tindex:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test_band_part.py\", line 11, in <module>\r\n    tf.keras.models.load_model('model')\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\", line 187, in load_model\r\n    return saved_model_load.load(filepath, compile, options)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 120, in load\r\n    model = tf_load.load_internal(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 632, in load_internal\r\n    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 194, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 130, in __init__\r\n    self._load_all()\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 221, in _load_all\r\n    self._finalize_objects()\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 530, in _finalize_objects\r\n    self._reconstruct_all_models()\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 548, in _reconstruct_all_models\r\n    self._reconstruct_model(model_id, model, layers)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 588, in _reconstruct_model\r\n    created_layers) = functional_lib.reconstruct_from_config(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\", line 1214, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\", line 1162, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 925, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1117, in _functional_construction_call\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 3099, in call\r\n    return self._make_op(inputs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 3121, in _make_op\r\n    c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1815, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Inconsistent values for attr 'Tindex' DT_INT32 vs. DT_INT64 while building NodeDef 'tf_op_layer_MatrixBandPart/MatrixBandPart' using Op<name=MatrixBandPart; signature=input:T, num_lower:Tindex, num_upper:Tindex -> band:T; attr=T:type; attr=Tindex:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>\r\n~~~\r\n\r\nSame as #42301. I just wrote standalone code.", "comments": ["The issue is resolved in TF 2.4-rc0 version.I have tried in colab with TF 2.4-rc0 and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/895d7ea9e27adcce336e891a746964ff/untitled491.ipynb).Please, verify once and close the issue. Thanks!", "Thanks. And sorry I forgot to check latest version...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44644\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44644\">No</a>\n"]}, {"number": 44643, "title": "let huber loss return the same dtype as inputs", "body": "I suggest changing https://github.com/tensorflow/tensorflow/blob/b3c4b50fa70868a2da709cb7868822e5361a50e4/tensorflow/python/keras/losses.py#L1529 to y_true.dtype too.\r\nIMO:\r\n1. It's annoyed when I run ```tf.keras.losses.categorical_crossentropy(tf.constant([[0, 1, 0], [0, 0, 1]], tf.float16), tf.constant([[0.05, 0.95, 0], [0.1, 0.8, 0.1]], tf.float16), label_smoothing=0.1)```, then I got dtype mismatch bug.\r\nThe result's dtype should depend on inputs.\r\n2. Is it tf.keras.backend.set_xxx is going to be abandoned?", "comments": ["Thanks for the PR.\r\n\r\n> Is it tf.keras.backend.set_xxx is going to be abandoned?\r\n\r\nNo, it is a current API.\r\n\r\nAll layers and losses generally cast their inputs to the global compute dtype when called. This mechanism enables mixed precision training, etc.", "> Thanks for the PR.\r\n> \r\n> > Is it tf.keras.backend.set_xxx is going to be abandoned?\r\n> \r\n> No, it is a current API.\r\n> \r\n> All layers and losses generally cast their inputs to the global compute dtype when called. This mechanism enables mixed precision training, etc.\r\n\r\nI met the bug in custom train_step model and I reproduce it in follow example:\r\n```python\r\nimport tensorflow as tf\r\ntf.keras.mixed_precision.set_global_policy('float16')\r\nprint(tf.keras.backend.floatx()) #return 'float32'\r\nprint(tf.keras.losses.huber([1],[2],0.)) #return float32 tensor\r\n```\r\nIs the behavior correct? @fchollet\r\nNo such note in https://www.tensorflow.org/guide/mixed_precision tells users to ```set_floatx```."]}, {"number": 44642, "title": "test with random fail on graph mode", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nwindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nv1.12.1-44421-g5e5730ba9d 2.4.0-rc0\r\n- Python version:\r\n3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n11.1\r\n- GPU model and memory:\r\n960m cuda\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nrunning the followin:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import test_util\r\n\r\n\r\nclass WeirdTest(tf.test.TestCase):\r\n\r\n  @test_util.run_in_graph_and_eager_modes\r\n  def test_test(self):\r\n    with self.cached_session():\r\n      x = tf.random.normal([])\r\n      self.assertAllClose(x, x)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.random.set_seed(42)\r\n  tf.test.main()\r\n```\r\n\r\nfails with assertion error (the x is not equal to itself!)\r\n(eager mode works fine)\r\n\r\n**Describe the expected behavior**\r\nthe test should pass\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nattached above (not sure how to run tests on colab)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nFailure\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\unittest\\case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"C:\\Program Files\\Python37\\lib\\unittest\\case.py\", line 628, in run\r\n    testMethod()\r\n  File \"D:\\python3_venvs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 1196, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"C:\\Users\\amitp\\dev\\github\\amitport\\federated-learning-research\\federated_learning_research\\v2\\encoders\\sign_encoder_test.py\", line 11, in test_encode\r\n    self.assertAllClose(x, x)\r\n  File \"D:\\python3_venvs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 1236, in decorated\r\n    return f(*args, **kwds)\r\n  File \"D:\\python3_venvs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 2711, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"D:\\python3_venvs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 2671, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"D:\\python3_venvs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 2606, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"D:\\python3_venvs\\tf_2_4\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 1528, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"D:\\python3_venvs\\tf_2_4\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 840, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-06, atol=1e-06\r\nMismatched value: a is different from b. \r\nnot close lhs = 0.4722212255001068\r\nnot close rhs = -1.5251041650772095\r\nnot close dif = 1.9973254203796387\r\nnot close tol = 2.525104165077209e-06\r\ndtype = float32, shape = ()\r\nMismatched elements: 1 / 1 (100%)\r\nMax absolute difference: 1.9973254\r\nMax relative difference: 1.3096322\r\n x: array(0.472221, dtype=float32)\r\n y: array(-1.525104, dtype=float32)\r\n```\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/905b70e15ffb5cfc8ea2d81d35f0cad9/44642-2-3.ipynb#scrollTo=NpVjmosnL7hg), [TF v2.4.0rc0](https://colab.research.google.com/gist/amahendrakar/e72ce48377783ee0f30debcb5a0b4525/44642-2-4.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/d080f341e52044024ea8c9bfe65d3f6e/44642-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@amahendrakar thanks for reproducing. I have to ask, why was the `type:bug` label removed? Should this be done differently?", "@amitport  I believe this is not a bug. In graph mode, `x = tf.random.normal([])` is returned as an operation, and `self.assertAllClose` will evaluate both input arguments and get the real value (if they are operations). Therefore, `x` is evaluated twice and returned different values. To fix it, you can try to use `self.evaluate` to get the exact value before asserting them.\r\n\r\nhttps://colab.research.google.com/drive/19iN2UNQ4vJ0erH64gskuVkvdZZQO8F8D?usp=sharing", "@WindQAQ thanks. I got to say this is very unintuitive and does not seem correct to me. I would expect assertion methods to evaluate the inputs' shared graph together. (DK if I l'm looking at bug or feature request, I just wonder if it could behave differently)\r\n\r\nIs there a use case to evaluate the shared graph part twice?", "> Is there a use case to evaluate the shared graph part twice?\r\n\r\n@amitport I would say that in most cases, we are evaluating a (shared) graph multiple times. For example, the ideal training pipeline in both eager and graph model is\r\n\r\n```python\r\n# Here is how the (shared) graph is built\r\nmodel = build_model()\r\n\r\n# Here is how operations in the graph are evaluated\r\n# including what kind of inputs are feeding and what kind of outputs are fetched\r\ndo_something(model)\r\n```\r\n\r\nLet me roughly expand the internal implementation of the example in this PR. What we are doing in graph mode is \r\n\r\n```python\r\nx = tf.random.normal([])\r\n\r\nwith tf.Session() as sess:\r\n    first = sess.run(x)\r\n    second = sess.run(x)\r\n    np.testing.assert_allclose(first, second)\r\n```\r\n\r\nMaybe it's more intuitive for users. If `x` is always evaluted as the same values in runtime, how can we get the different numbers without constructing new graphs?\r\n\r\nConsider another common example that sometimes users add \"noise\" into the input.\r\n\r\n```python\r\ninput = tf.placeholder(foo)\r\nnoise = tf.random.normal([])\r\nnoisy_output = input + noise\r\n\r\n# do something\r\n\r\nwith tf.Session() as sess:\r\n    for i in range(10):\r\n        y = sess.run(noisy_output, feed_dict=bar)\r\n```\r\n\r\nI think users would expect `y` to be different during iterations as they want noisy outputs even the graph is shared or unchanged.\r\n\r\nLet me know if this answers your question. Thank you.", "@WindQAQ Thank you for the example. I still think that these implementation details should not affect the test writer and assert methods could be smarter. That is, I realize it is implemented like this but it seems wrong for test case usage. (just look again at the first example in the issue, why would anyone expect this behavior?)", "@amitport ade73c6af0957e7cbe420ae515c6a5d9acc6ecb3 updated assertAllEqual and similar functions to evaluate `a` and `b` in the same graph.  Closing this as fixed, but please feel free to re-open if you find that this change doesn't address your problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44642\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44642\">No</a>\n"]}, {"number": 44641, "title": "Tensorflow 2+ framework class exception", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: custom code\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro 64 bit\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: N/A\r\n-   **TensorFlow installed from (source or binary)**: Binary\r\n-   **TensorFlow version (use command below)**: 2.1.0\r\n-   **Python version**:3.7.9\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: 11.1\r\n-   **GPU model and memory**: GeForce MX130, 2048 MiB\r\n-   **Exact command to reproduce**: python main.py in the tree/master/src dir after downloading the master from https://github.com/githubharald/SimpleHTR/ (I had another error that I rectified by replacing tf.placeholder with tf.Variable in several places.)\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nThe src/Model.py line 34 --> tf.Variable(tf.bool, name='is_train') starts the problem-chain culminating in \"Attempt to convert a value (tf.bool) with an unsupported type (<class 'tensorflow.python.framework.dtypes.DType'>) to a Tensor.\" Is there something amiss by replacing ..placeholder by ..Variable, as mentioned above?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nPS D:\\github\\word_beam_search_related\\SimpleHTR\\src>  & 'C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\python.exe' 'c:\\Users\\Xxxx\\.vscode\\extensions\\ms-python.python-2020.10.332292344\\pythonFiles\\lib\\python\\debugpy\\launcher' '57165' '--' 'c:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py'\r\n2020-11-06 10:16:00.520456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  \r\nPS D:\\github\\word_beam_search_related\\SimpleHTR\\src> conda activate tf-gpu\r\nPS D:\\github\\word_beam_search_related\\SimpleHTR\\src> python main.py\r\n2020-11-06 10:16:54.911411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nValidation character error rate of saved model: 10.624916%\r\n2020-11-06 10:16:56.574477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-11-06 10:16:57.531951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-11-06 10:16:57.540214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-11-06 10:16:57.546025: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll   \r\n2020-11-06 10:16:57.552043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-06 10:16:57.568518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-11-06 10:16:57.590344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-06 10:16:57.624576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-11-06 10:16:57.638909: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-06 10:16:57.647300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-11-06 10:16:57.662287: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-11-06 10:16:57.680211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-11-06 10:16:57.698051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  \r\n2020-11-06 10:16:57.700475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll   \r\n2020-11-06 10:16:57.711009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-06 10:16:57.715823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll   \r\n2020-11-06 10:16:57.729503: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-06 10:16:57.734204: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll \r\n2020-11-06 10:16:57.744820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-06 10:16:57.749826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-11-06 10:17:00.001152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-06 10:17:00.007966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-11-06 10:17:00.011319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-11-06 10:17:00.012945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1377 MB memory) -> physical GPU (device: 0, name: GeForce MX130, pci bus id: 0000:02:00.0, compute capability: 5.0)\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 145, in <module>\r\n    main()\r\n  File \"main.py\", line 140, in main\r\n    model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args.dump)\r\n  File \"D:\\github\\word_beam_search_related\\SimpleHTR\\src\\Model.py\", line 34, in __init__\r\n    self.is_train = tf.Variable(tf.bool, name='is_train')\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 260, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 254, in _variable_v2_call        \r\n    shape=shape)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 235, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 2645, in default_variable_creator_v2\r\n    shape=shape)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1411, in __init__    \r\n    distribute_strategy=distribute_strategy)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1543, in _init_from_a    name=\"initial_value\", dtype=dtype)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1314, in convert_to_tensor       \r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 317, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 258, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 266, in _constant_impl   \r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Attempt to convert a value (tf.bool) with an unsupported type (<class 'tensorflow.python.framework.dtypes.DType'>) to a Tensor.       \r\nPS D:\\github\\word_beam_search_related\\SimpleHTR\\src> python main.py\r\n2020-11-06 10:17:31.484639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nValidation character error rate of saved model: 10.624916%\r\n2020-11-06 10:17:33.172380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-11-06 10:17:34.133112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-11-06 10:17:34.144513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  \r\n2020-11-06 10:17:34.151353: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-11-06 10:17:34.168700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-06 10:17:34.182794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-11-06 10:17:34.199075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-06 10:17:34.214206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll \r\n2020-11-06 10:17:34.237372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-06 10:17:34.244143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-11-06 10:17:34.249031: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-11-06 10:17:34.264222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-11-06 10:17:34.279950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  \r\n2020-11-06 10:17:34.282239: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll   \r\n2020-11-06 10:17:34.292704: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-06 10:17:34.296967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll   \r\n2020-11-06 10:17:34.299030: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll \r\n2020-11-06 10:17:34.310039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-11-06 10:17:34.314767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll     \r\n2020-11-06 10:17:34.326801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-11-06 10:17:35.667289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-06 10:17:35.671078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-11-06 10:17:35.672623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-11-06 10:17:35.674178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1377 MB memory) -> physical GPU (device: 0, name: GeForce MX130, pci bus id: 0000:02:00.0, compute capability: 5.0)\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 145, in <module>\r\n    main()\r\n  File \"main.py\", line 140, in main\r\n    model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args.dump)\r\n  File \"D:\\github\\word_beam_search_related\\SimpleHTR\\src\\Model.py\", line 34, in __init__\r\n    self.is_train = tf.Variable(tf.bool, name='is_train')\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 260, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 254, in _variable_v2_call        \r\n    shape=shape)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 235, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 2645, in default_variable_creator_v2\r\n    shape=shape)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1411, in __init__    \r\n    distribute_strategy=distribute_strategy)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1543, in _init_from_args\r\n    name=\"initial_value\", dtype=dtype)\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 317, in _constant_tensor_conversion_function\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 258, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 266, in _constant_impl   \r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Attempt to convert a value (tf.bool) with an unsupported type (<class 'tensorflow.python.framework.dtypes.DType'>) to a Tensor.       \r\nPS D:\\github\\word_beam_search_related\\SimpleHTR\\src>  cd 'd:\\github\\word_beam_search_related\\SimpleHTR\\src'; & 'C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\python.exe' 'c:\\Users\\Xxxx\\.vscode\\extensions\\ms-python.python-2020.10.332292344\\pythonFiles\\lib\\python\\debugpy\\launcher' '57192' '--' 'c:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py' \r\n2020-11-06 10:18:08.558201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  \r\nPS D:\\github\\word_beam_search_related\\SimpleHTR\\src>  cd 'd:\\github\\word_beam_search_related\\SimpleHTR\\src'; & 'C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\python.exe' 'c:\\Users\\Xxxx\\.vscode\\extensions\\ms-python.python-2020.10.332292344\\pythonFiles\\lib\\python\\debugpy\\launcher' '57226' '--' 'd:\\github\\word_beam_search_related\\SimpleHTR\\src\\main.py' \r\n2020-11-06 10:20:06.521317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nValidation character error rate of saved model: 10.624916%\r\n2020-11-06 10:20:08.993191: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-11-06 10:20:09.962429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-11-06 10:20:09.969303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  \r\n2020-11-06 10:20:09.975677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-11-06 10:20:09.989720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-06 10:20:10.004558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll   \r\n2020-11-06 10:20:10.012081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-06 10:20:10.027702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-11-06 10:20:10.061178: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-06 10:20:10.071325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-11-06 10:20:10.087199: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-11-06 10:20:10.105630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-11-06 10:20:10.121203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll  \r\n2020-11-06 10:20:10.123716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll   \r\n2020-11-06 10:20:10.135764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-06 10:20:10.141182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll   \r\n2020-11-06 10:20:10.152498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-06 10:20:10.158452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll \r\n2020-11-06 10:20:10.168864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll     \r\n2020-11-06 10:20:10.173096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-11-06 10:20:11.486600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-06 10:20:11.492343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-11-06 10:20:11.502868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-11-06 10:20:11.507228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1377 MB memory) -> physical GPU (device: 0, name: GeForce MX130, pci bus id: 0000:02:00.0, compute capability: 5.0)\r\nWe've got an error while stopping in unhandled exception: <class 'ValueError'>.\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\Xxxx\\.vscode\\extensions\\ms-python.python-2020.10.332292344\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 1994, in do_stop_on_unhandled_exception\r\n    self.do_wait_suspend(thread, frame, 'exception', arg, EXCEPTION_TYPE_UNHANDLED)\r\n  File \"c:\\Users\\Xxxx\\.vscode\\extensions\\ms-python.python-2020.10.332292344\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 1855, in do_wait_suspend\r\n    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\r\n  File \"c:\\Users\\Xxxx\\.vscode\\extensions\\ms-python.python-2020.10.332292344\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 1890, in _do_wait_suspend\r\n    time.sleep(0.01)\r\nKeyboardInterrupt\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"c:\\Users\\Xxxx\\.vscode\\extensions\\ms-python.python-2020.10.332292344\\pythonFiles\\lib\\python\\debugpy\\__main__.py\", line 45, in <module> \r\n    cli.main()\r\n  File \"c:\\Users\\Xxxx\\.vscode\\extensions\\ms-python.python-2020.10.332292344\\pythonFiles\\lib\\python\\debugpy/..\\debugpy\\server\\cli.py\", line 430, in main\r\n    run()\r\n  File \"c:\\Users\\Xxxx\\.vscode\\extensions\\ms-python.python-2020.10.332292344\\pythonFiles\\lib\\python\\debugpy/..\\debugpy\\server\\cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"d:\\github\\word_beam_search_related\\SimpleHTR\\src\\main.py\", line 145, in <module>\r\n    main()\r\n  File \"d:\\github\\word_beam_search_related\\SimpleHTR\\src\\main.py\", line 140, in main\r\n    model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args.dump)\r\n  File \"d:\\github\\word_beam_search_related\\SimpleHTR\\src\\Model.py\", line 34, in __init__\r\n    self.is_train = tf.Variable(tf.bool, name='is_train')\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 260, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 254, in _variable_v2_call        \r\n    shape=shape)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 235, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 2645, in default_variable_creator_v2\r\n    shape=shape)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1411, in __init__    \r\n    distribute_strategy=distribute_strategy)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1543, in _init_from_a    name=\"initial_value\", dtype=dtype)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1314, in convert_to_tensor       \r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 317, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 258, in constant\r\n    allow_broadcast=True)\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\Xxxx\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_t    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Attempt to convert a value (tf.bool) with an unsupported type (<class 'tensorflow.python.framework.dtypes.DType'>) to a Tensor.     ", "comments": ["@daspranab \r\nWe see that you rae using cuda 11, If you want CUDA 11 you can use nightly version, and let us know if the issue exist.\r\nAlso please refer to these links with similar error and let us know if it helps:\r\n[link](https://github.com/tensorflow/tensorflow/issues/30120), [link1](https://stackoverflow.com/questions/55161550/valuerror-attemp-to-convert-a-value-type-model-to-a-tensor)", "1) Did you mean cuda_11.1.1_456.81_win10.exe? I will try that and report.\r\n2) I didn't find a way to use the information given in the two links. Please elaborate. Apparently, following the first link I need to downgrade Tensorflow--is that what you mean? Following the second link, I may have to change the codebase: please confirm.", "@daspranab \r\nCan you please check if you are facing the issue on nightly version.", "Sorry for the delay; there's was an unavoidable circumstance. Will post latest later today.", "Exactly the same error repeated after installing the Cuda version mentioned by me before. Please let me know if any other version was to be used. I have no time; hence, will appreciate a considered response early.", "@daspranab \r\nPlease share a simple stand alone code to replicate the issue faced or if possible share a colab gist with the issue reported.", "Kindly see my first submission. It's the same. I didn't make any change.", "> \r\n> \r\n> @daspranab\r\n> Please share a simple stand alone code to replicate the issue faced or if possible share a colab gist with the issue reported.\r\n\r\nColab is not my environment and using colab is not preconditioned in tensorflow-gpu. If you try on any laptop with a NVIDIA GPU, the same problem will repro. I tried it on another laptop with GeForce MX250 with the same result. If this is not something you are expert in, why not someone with knowledge of the concerned library check this. Else, let's stop this circular interaction.\r\n\r\nThe framework has hastily changed placeholder to Variable without any backward compatibility and that caused all the problems as will be clear to one who understands.", "@daspranab\r\nI ran the code shared and was unable to replicate due to the issue in the [gist here](https://colab.research.google.com/gist/Saduf2019/907bfbd82694dd2b70e3df6321de33d1/untitled458.ipynb).", "You're ignoring my instruction *not* to use Colab to repro. I provided complete set of instructions in my initial report. You're clearly trying to solve some other problem and not one reported.", "@daspranab I am sorry to hear that you are having problems. I see that your gpu config has cuda compute capability of 5.0\r\n> pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\r\n\r\nUnfortunately it is not supported CUDA\u00ae architectures for TF.\r\nSee https://www.tensorflow.org/install/gpu#hardware_requirements\r\nAs a sanity check you can execute your code on cpu by force and check if the issue still persists. Thanks!", "I think your support is worth 0: you can't even track an exception in the\nframework! I'm sorry that I thought your group is worth posting my problem!!\n\nOn Fri, Nov 13, 2020, 11:16 PM Yasir Modak <notifications@github.com> wrote:\n\n> @daspranab <https://github.com/daspranab> I am sorry to hear that you are\n> having problems. I see that your gpu config has cuda compute capability of\n> 5.0\n>\n> pciBusID: 0000:02:00.0 name: GeForce MX130 computeCapability: 5.0\n>\n> Unfortunately it is not supported CUDA\u00ae architectures for TF.\n> See https://www.tensorflow.org/install/gpu#hardware_requirements\n> As a sanity check you can execute your code on cpu by force and check if\n> the issue still persists. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44641#issuecomment-726912193>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABSFJYFGDPQHWIFIWYCKZDTSPVWHZANCNFSM4TMHSRKA>\n> .\n>\n", "Is this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44641\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44641\">No</a>\n"]}, {"number": 44640, "title": "tf.keras.experimental.WideDeepModel  can not accept two  tf.keras.Model  as input?", "body": "tf.keras.experimental.WideDeepModel claims:\r\n\r\n\r\n`tf.keras.experimental.WideDeepModel(linear_model, dnn_model, activation=None, **kwargs)`\r\nArgs:\r\nlinear_mode | a premade LinearModel, its output must match the output of the dnn model.\r\ndnn_model   | a\u00a0tf.keras.Model, its output must match the output of the linear model.\r\nactivation     | Activation function. Set it to None to maintain a linear activation.\r\n**kwargs      | The keyword arguments that are passed on to BaseLayer.init. Allowed keyword arguments include\u00a0name.\r\n\r\nI viewd the source code and can not find the difference between linear model and deep model.\r\nWhen i used this WideDeepModel with two ' tf.keras.Model' , i got some problem.\r\n\r\n```\r\n# input_dict key is the feature name,  value is keras Input Layer\r\ndeep_embedding_output=layers_utils.get_embed_output(input_dict, embedding_size=16)\r\ndeep_embedding_flat_inp = layers.Flatten(deep_embedding_output)\r\ndeep_mlp_out= layers_utils.mlp(units=[128,64,16])(deep_embedding_flat_inp)\r\ndeep_out=layers.Dense(1)(deep_mlp_out)\r\ndeep_channel = Model(inputs=input_dict,output=[deep_out])\r\n\r\nwide_embedding_output=layers_utils.get_embed_output(input_dict, embedding_size=1)\r\nwide_embedding_flat_inp = layers.Flatten(wide_embedding_output)\r\nwide_out=layers.Dense(1)(wide_embedding_flat_inp)\r\ndeep_channel = Model(inputs=input_dict,output=[wide_out])\r\n# label feature name is label\r\nmodel = WideDeepModel(wide_channel, deep_channel, act=\"sigmoid\", name=\u2018label\u2019)\r\n\r\nmodel.compile(optimizer=['ftrl', 'adam'], 'binary_cross_entropy', ['auc'])\r\n# `tf.data.Dataset` that contains a single tensor\r\ncombined_model.fit(dataset, epochs)\r\n#dataset = tf.data.Dataset.from_tensors((features, label))\r\n```\r\nI met the problem:\r\nlinear model expected 94 ,but given 95;\r\n94 is the input feature size, it seems like that the target label was not recognized?\r\n\r\n\r\n\r\n", "comments": ["when i used this tf.data.Dataset as follows:\r\n```\r\ndeep_embedding_output=layers_utils.get_embed_output(input_dict, embedding_size=16)\r\ndeep_embedding_flat_inp = layers.Flatten(deep_embedding_output)\r\ndeep_mlp_out= layers_utils.mlp(units=[128,64,16])(deep_embedding_flat_inp)\r\ndeep_out=layers.Dense(1)(deep_mlp_out)\r\n\r\nwide_embedding_output=layers_utils.get_embed_output(input_dict, embedding_size=1)\r\nwide_embedding_flat_inp = layers.Flatten(wide_embedding_output)\r\nwide_out=layers.Dense(1)(wide_embedding_flat_inp)\r\n\r\nwd =  layers.Add()([wide_out,deep_out])\r\nwd_out = layers.Dense(1,act='sigmoid',name='label')(wd)\r\nmodel =  Model(inputs=input_dict,output=[wd_out])\r\nmodel.compile(optimizer='adam', 'binary_cross_entropy', ['auc'])\r\ncombined_model.fit(dataset, epochs)\r\n```\r\nThis model can work.\r\n\r\nIf i tried to remove `wd_out = layers.Dense(1,act='sigmoid',name='label')(wd)` the name argument,  it did not work.\r\nSo i guss that my problem may caused by 'label' tag.", "@tanzhenyu Thx ", "It sounds like this is a user code issue and we're safe to close it?", "@jiaqinglin \r\n\r\nIs this still an issue?Please, close this thread if your issue was resolved. Thanks!"]}]