[{"number": 12857, "title": "Unhelpful exceptions from tf.truncated_normal with dtype=tf.int32 ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04 LTS \r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow \r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: Python 2.7.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A (CPU)\r\n- **GPU model and memory**: N/A \r\n- **Exact command to reproduce**:\r\n\r\npython -c \"import tensorflow as tf ; tf.truncated_normal([1], dtype=tf.int32)\"\r\n\r\n### Describe the problem\r\nWhen attempting to initialize a tf.Variable of type tf.int32 using ```tf.truncated_normal()```, invocation with simple args raise a TypeError, but the error message is ambiguous and unhelpful for debugging: \r\n\r\n```\r\npython -c \"import tensorflow as tf ; tf.truncated_normal([1], dtype=tf.int32)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 168, in truncated_normal\r\n    mean_tensor = ops.convert_to_tensor(mean, dtype=dtype, name=\"mean\")\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 611, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 676, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 121, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 102, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 376, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got 0.0 of type 'float' instead.\r\n\r\n```\r\n\r\nProviding more arguments to ```tf.truncated_normal``` correctly indicates that tf.int32 isn't a supported type: \r\n\r\n```\r\npython -c \"import tensorflow as tf ; tf.truncated_normal([1], mean=0, stddev=1, dtype=tf.int32)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 172, in truncated_normal\r\n    shape_tensor, dtype, seed=seed1, seed2=seed2)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 316, in _truncated_normal\r\n    seed=seed, seed2=seed2, name=name)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 700, in apply_op\r\n    attr_value.type = _MakeType(value, attr_def)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 179, in _MakeType\r\n    _SatisfiesTypeConstraint(i, attr_def, param_name=attr_def.name)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'dtype' has DataType int32 not in list of allowed values: float16, float32, float64\r\n```\r\n\r\nContrasting this with usage of complex64, which provides the correct error message even with simple args: \r\n\r\n```\r\npython -c \"import tensorflow as tf ; tf.truncated_normal([1], dtype=tf.complex64)\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 172, in truncated_normal\r\n    shape_tensor, dtype, seed=seed1, seed2=seed2)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 316, in _truncated_normal\r\n    seed=seed, seed2=seed2, name=name)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 700, in apply_op\r\n    attr_value.type = _MakeType(value, attr_def)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 179, in _MakeType\r\n    _SatisfiesTypeConstraint(i, attr_def, param_name=attr_def.name)\r\n  File \"/usr/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'dtype' has DataType complex64 not in list of allowed values: float16, float32, float64\r\n```\r\n\r\ndtype argument checking should happen up front, earlier in the codepath of ```truncated_normal()```\r\n\r\n\r\n### Source code / logs\r\n\r\nSee above\r\n", "comments": ["@slacy For reference, I am putting source code which is generating error.\r\n\r\n`snippet-1` generated via `framework/tensor_util.py`.\r\n\r\n```\r\ndef _AssertCompatible(values, dtype):\r\n  fn_list = _TF_TO_IS_OK.get(dtype, [_FilterNotTensor])\r\n  mismatch = _FirstNotNone([fn(values) for fn in fn_list])\r\n  if mismatch is not None:\r\n    if dtype is None:\r\n      raise TypeError(\"List of Tensors when single Tensor expected\")\r\n    else:\r\n      raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\r\n                      (dtype.name, repr(mismatch), type(mismatch).__name__))\r\n```\r\n\r\n`snippet-2 and snippet-3` generated via `framework/op_def_library.py`\r\n\r\n```\r\ndef _SatisfiesTypeConstraint(dtype, attr_def, param_name):\r\n  if attr_def.HasField(\"allowed_values\"):\r\n    allowed_list = attr_def.allowed_values.list.type\r\n    if dtype not in allowed_list:\r\n      raise TypeError(\r\n          \"Value passed to parameter '%s' has DataType %s not in list of \"\r\n          \"allowed values: %s\" %\r\n          (param_name, dtypes.as_dtype(dtype).name,\r\n           \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n```", "OK, I think it makes sense to send a PR to give the same error as with the complex type.", "@drpngx do you have any suggestions? I tried to change flow but not passing 1 test case. Would you advise something else to make it informative?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12856, "title": "C++ gradients: data_flow ops", "body": "This set has just two ops to port - `DynamicStitch` and `DynamicPartition`. Anyone already working on it? Otherwise, I'll sign up for this.\r\n\r\n/cc @suharshs ", "comments": []}, {"number": 12855, "title": "[Feature Request]Read the last batch in Dataset", "body": "## The programmer guide provides an example of work flow like this\r\n\r\n\r\n`filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\r\ndataset = tf.contrib.data.TFRecordDataset(filenames)\r\ndataset = dataset.map(...)\r\ndataset = dataset.batch(32)\r\niterator = dataset.make_initializable_iterator()\r\nnext_element = iterator.get_next()\r\n\r\n### Compute for 100 epochs.\r\nfor _ in range(100):\r\n  sess.run(iterator.initializer)\r\n  while True:\r\n    try:\r\n      sess.run(next_element)\r\n    except tf.errors.OutOfRangeError:\r\n      break`\r\n\r\n## if we have 33 examples in dataset then we will miss the last example, Is there an API to adjust the batch size automatically(e.g. in this case 1) in order to feed all the examples into model? ", "comments": ["`Dataset.batch()` always returns the last batch, even if it turns out to be smaller than the batch size.\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> iterator = tf.contrib.data.Dataset.range(33).batch(32).make_one_shot_iterator()\r\n>>> next_element = iterator.get_next()\r\n>>> sess = tf.Session()\r\n...\r\n>>> sess.run(next_element)\r\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\r\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\r\n>>> sess.run(next_element)\r\narray([32])\r\n>>> sess.run(next_element)\r\nTraceback (most recent call last):\r\n...\r\nOutOfRangeError (see above for traceback): End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](OneShotIterator)]]\r\n>>> \r\n```", "@mrry Thanks for your quick reply."]}, {"number": 12854, "title": "tf.qint8: Quantized 8-bit signed integer (Definition, code piece, examples)", "body": "What does quantized mean in this scenario?  Could you please explain with an example and a descriptive definition along with the code used for this?\r\n\r\ntf.qint8: Quantized 8-bit signed integer.\r\nWhat does quantized mean in this scenario?\r\n\r\ntf.quint8: Quantized 8-bit unsigned integer.\r\nWhat does quantized mean in this scenario?\r\n\r\ntf.qint16: Quantized 16-bit signed integer.\r\nWhat does quantized mean in this scenario?\r\n\r\ntf.quint16: Quantized 16-bit unsigned integer.\r\nWhat does quantized mean in this scenario?\r\n\r\ntf.qint32: Quantized 32-bit signed integer.\r\nWhat does quantized mean in this scenario?\r\n\r\ntf.resource: Handle to a mutable resource.\r\nWhat does mutable resource mean?", "comments": ["I guess this [link](https://www.tensorflow.org/performance/quantization) might help you to understand Quantization!", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12853, "title": "Sub-gradient for self_adjoin_eig when eigen values are equal ", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, I have written custom code \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.2.1\r\n- **Python version**: \r\nPython 2.7\r\n- **Bazel version (if compiling from source)**:\r\n\r\n- **CUDA/cuDNN version**:\r\n8.0.61\r\n- **GPU model and memory**:\r\nTesla K40c\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nThis is a feature request. When any two eigen values are equal, the tf.gradients( tf.self_adjoint_eig(matrix), matrix) returns NaN.\r\nWhile the gradient is not well defined, it would be useful if some valid sub-gradient is returned (which could be used in the optimization).\r\n \r\nIn particular, I am trying to optimize a function involving max eigen value of a matrix. \r\nEven when two eigen values are equal, a valid sub-gradient would be v_1 * v_1^T, \r\nwhere v_1 is the eigen vector corresponding to a max eigen value. \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["That sounds reasonable.\r\n@rmlarsen what do you think? Should we encourage a contribution?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@RAditi \r\nCould you please try on the latest tf version and let us know if this si still an issue.", "As for now, Lorentz broadening https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L712 is used both for SVD and ED gradients. I think that means you won't get neither nan nor valid sub-gradient.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "But I replied?", "@Randl \r\nPlease move this to closed status as it is not an issue anymore.", "Not sure why this is non-issue? \r\nClose or equal eigenvalues are currently resulting in (very large) invalid value which is non-infinite just to prevent nans from appearing. This is worse than returning valid (sub-)gradient."]}, {"number": 12852, "title": "Spatial pyramid pooling layer implementation.", "body": "PR Inspired from the work of @luizgh and @RikHeijdens.\r\nReferences:\r\n1. He, Kaiming et al (2015): Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. http://arxiv.org/pdf/1406.4729.pdf.\r\n2. [Implement SpatialPyramidPooling](https://github.com/Lasagne/Lasagne/pull/799)\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@jart Can you please review the PR. It's been here for a while and not yet reviewed.", "Hi @yardstick17 , Is this feature available in tf-nightly? I looked for it in tf.layers and tf.contrib.layers so far. I'm pretty new to TF. Can you please tell me where to find it? \r\nThanks!\r\n", "@dnnavn It looks like this layer isn't available yet. There is a related open pull request, https://github.com/tensorflow/tensorflow/pull/13259 .", "@sfikas  Thank you. Will follow that thread."]}, {"number": 12850, "title": "Removed unused imports in layers modules", "body": "", "comments": ["Can one of the admins verify this patch?", "@yardstick17, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @fchollet and @zhangyaobit to be potential reviewers.", "@tensorflow-jenkins test this please", "py3 rerun at http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/6319/", "gpu rerun at https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/6681/"]}, {"number": 12849, "title": "Unable to compile TF 1.3 from source using full MKL", "body": "Steps to reproduce:\r\n\r\n    git clone https://github.com/tensorflow/tensorflow.git test\r\n    cd test\r\n    git checkout r1.3\r\n    yes \"\" | TF_NEED_CUDA=0 TF_NEED_MKL=1 TF_DOWNLOAD_MKL=0 MKL_INSTALL_PATH=<path>/l_mkl_2017.3.196/inst/mkl ./configure\r\n    bazel build --config=mkl -c opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\nOS version: Ubuntu Linux 14.04\r\nBazel version: 0.5.3\r\n\r\nError message:\r\n\r\n    ERROR: missing input file '//third_party/mkl:libmklml_intel.so'\r\n    ERROR: <path>/tensorflow/test/third_party/mkl/BUILD:16:1: //third_party/mkl:intel_binary_blob: missing input file '//third_party/mkl:libmklml_intel.so'\r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n    ERROR: <path>/tensorflow/test/third_party/mkl/BUILD:16:1 1 input file(s) do not exist\r\n\r\nThe `configure` script is creating symlinks in `third_party/mkl/` for `libmkl_rt.so` ([see here](https://github.com/tensorflow/tensorflow/blob/r1.3/configure#L260)), which is fine, but not for `libmklml_intel.so` ([see here](https://github.com/tensorflow/tensorflow/blob/r1.3/configure#L264)), which doesn't exist in the full MKL distribution. However [`third_party/mkl/BUILD` references `libmklml_intel.so`](https://github.com/tensorflow/tensorflow/blob/r1.3/third_party/mkl/BUILD#L20). Is this a bug or is use of the full MKL library not supported in TensorFlow 1.3?", "comments": ["@gunan do you happen to know on the top of your head?", "AFAICT, my changes are not in 1.3 branch.\r\nAt master, you should only need to configure, then run `bazel build` with `--config=mkl`, and that should be it.\r\nOn 1.3, we still use the way contributed by intel. I believe this was not a very well tested build path, so it is likely this was broken all along.\r\nI would recommend setting `TF_DOWNLOAD_MKL` to 1 and see if it works when configure downloads MKL itself.\r\n", "Yes, it works with `TF_DOWNLOAD_MKL=1` but this downloads MKL-DNN, not the full MKL. If one wants to build MKL support into Eigen then, I believe, one needs to use the full MKL but maybe MKL support in TensorFlow is limited to MKL-DNN? Comments in the r1.3 `configure` script seem to imply that it should support the full MKL which is why I tried it.\r\n\r\nBy the way, the [instructions on the Intel site](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture) for building TensorFlow with MKL (which have also been communicated on various TensorFlow support channels) are, I believe, incorrect. It says one should build with the command `bazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" -c opt //tensorflow/tools/pip_package:build_pip_package` but this has two potential errors:\r\n\r\n1. One should include `--config=opt` in addition to `--config=mkl` to enable all the regular (non-MKL) CPU optimizations in addition to the MKL features.\r\n2. The flag to compile MKL vector operations into Eigen is `EIGEN_USE_MKL_VML`, not `EIGEN_USE_VML` ([as explained here](https://eigen.tuxfamily.org/dox/TopicUsingIntelMKL.html)).\r\n\r\nCorrecting the second of these two potential errors yields a new issue in TensorFlow: it fails to compile with a bazel error (something about MKL header files getting included without them being declared in a BUILD file). I don't have the details of that error to hand but it should probably be logged as separate issue anyway and if the MKL build options are changing in the next release maybe it's best to just wait.\r\n\r\nI hope the next version of TensorFlow will make clear which of the nine potential configurations of Eigen and TF each built with one of (no MKL, MKL-DNN, full MKL) is supported. It's also not clear whether the optional use of CUDA is an orthogonal consideration or could interact with the use (or not) of MKL/MKL-DNN.\r\n", "@tobyboyd for more information on this.\r\n\r\nMKL support is at the moment provided by Intel, not officially by us yet.\r\nThat is why installation guides are not on our webpage, but on intel's page.\r\nI recommend reaching out to them for support with the above questions.", "Thanks. I've started corresponding with Intel here: https://software.intel.com/en-us/comment/1911911", "A workaround has been provided for using the full MKL with TF r1.3; details at the Intel forums link above. In my opinion, this issue can be closed.\r\n\r\nAdditional information about building TF with the full MKL, including Eigen-MKL integration, can be found in the comments attached to this document: https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture"]}, {"number": 12848, "title": "RenderScript support", "body": "Is there a reason for not supporting RenderScript? It's there an ETA for this?", "comments": ["@aselle, would you please respond or redirect? ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "We have since released tflite, so closing for now. Let us know if that works for you or if you want us to clarify how it is positioned with respected to RenderScript.", "@drpngx it would be great if you could clarify how it is positioned with respect to RenderScript"]}, {"number": 12847, "title": "Compilation issue with AVX option", "body": "### System information\r\n- **OS Platform and Distribution**: Debian Buster\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: commit https://github.com/tensorflow/tensorflow/commit/12a628a623a5dae81b8fc699792eaf414e6ace41\r\n- **Python version**: 3.5.4\r\n- **Bazel version**: 0.5.4\r\n- **CUDA/cuDNN version**: CUDA 8/CuDNN 6\r\n- **GPU model and memory**: 2xTesla K80 with 12GB each\r\n- **CPU model**: Intel Xeon E5-2683 v4\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --copt=-msse4.1 --config=mkl --config=cuda --verbose_failures -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nOr\r\n\r\n```\r\nbazel build -c opt --copt=-march=native --copt=-mfpmath=both --config=mkl --config=cuda --verbose_failures -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n### Describe the problem\r\nI'm trying to compile a Tensorflow package specifically optimized for my machine. When I run the compilation with one of the command lines described above, I get some compilation errors. Doesn't matter if I let GCC deciding which optimization to make or if I force them. The kind of errors are always the same:\r\n\r\n```\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9220): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9231): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9244): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9255): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9268): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9279): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9292): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9303): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9316): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9327): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9340): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9352): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9365): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9376): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9389): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9401): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9410): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9419): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9428): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9437): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9445): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9454): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9463): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9472): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9481): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9490): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9499): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9508): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9517): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9526): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9535): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512fintrin.h(9544): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(55): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(63): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(73): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(81): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(91): error: argument of type \"void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(100): error: argument of type \"void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(109): error: argument of type \"void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(117): error: argument of type \"void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(127): error: argument of type \"void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(136): error: argument of type \"void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(145): error: argument of type \"void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512pfintrin.h(153): error: argument of type \"void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10799): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10811): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10823): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10835): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10847): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10859): error: argument of type \"const void *\" is incompatible with parameter of type \"const float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10871): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10883): error: argument of type \"const void *\" is incompatible with parameter of type \"const double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10895): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10907): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10919): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10931): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10943): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10955): error: argument of type \"const void *\" is incompatible with parameter of type \"const int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10967): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10979): error: argument of type \"const void *\" is incompatible with parameter of type \"const long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(10989): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11000): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11009): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11020): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11029): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11040): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11049): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11060): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11069): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11080): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11089): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11100): error: argument of type \"void *\" is incompatible with parameter of type \"float *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11109): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11120): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11129): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11140): error: argument of type \"void *\" is incompatible with parameter of type \"double *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11149): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11160): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11169): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11180): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11189): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11200): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11209): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11220): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11229): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11240): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11249): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11260): error: argument of type \"void *\" is incompatible with parameter of type \"int *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11269): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11280): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11289): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/5/include/avx512vlintrin.h(11300): error: argument of type \"void *\" is incompatible with parameter of type \"long long *\"\r\n\r\n92 errors detected in the compilation of \"/tmp/tmpxft_00007482_00000000-7_zero_initializer_op_gpu.cu.cpp1.ii\".\r\nERROR: /opt/tensorflow/tensorflow/contrib/framework/BUILD:88:1: output 'tensorflow/contrib/framework/_objs/python/ops/_variable_ops_gpu/tensorflow/contrib/framework/kernels/zero_initializer_op_gpu.cu.pic.o' was not created\r\nERROR: /opt/tensorflow/tensorflow/contrib/framework/BUILD:88:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 331.599s, Critical Path: 63.79s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nAm I using a wrong command line or is it a bug in the compilation process?\r\n\r\nThanks in advance for any help.", "comments": ["Duplicate from #10220"]}, {"number": 12846, "title": "Unable to install tensorflow=1.0.0", "body": "I have been trying to install tensorflow 1.0.0 using pip in Anaconda environment. I found the below error while installation. I have seen previous posts in github and stackover flow but I couldn't find the solution.\r\n\r\n(C:\\Users\\naresh.kumar\\AppData\\Local\\Continuum\\Anaconda3) C:\\Users\\naresh.kumar>**pip install --ignore-installed --upgrade  https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.0.0-cp35-cp35m-win_x86_64.whl**\r\n\r\npip = 9.0.1\r\npython = 3.6.1\r\nAnaconda=4.3.21\r\nPlease help me out how to solve this issue.\r\n\r\n\r\n", "comments": ["Have you tried\r\n`conda install tensorflow`?\r\nThis is one of the best things about Anaconda - it simplifies package installation hugely.", "Yes I have tried. But it installs tensorflow=1.3.\n\nOn Sep 7, 2017 1:16 AM, \"Aabir Abubaker\" <notifications@github.com> wrote:\n\n> Have you tried\n> conda install tensorflow?\n> This is one of the best things about Anaconda - it simplifies package\n> installation hugely.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12846#issuecomment-327592548>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJuFy3Z-aNuMz5HSq2ksaUh8nvxu8WYXks5sfvaAgaJpZM4PONML>\n> .\n>\n", "`conda install tensorflow=1.0 \r\n`\r\nworks on my system. This question should be asked in Stackoverflow!", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12845, "title": "Make saver_test pass on Windows Bazel Build", "body": "This is a workaround for https://github.com/tensorflow/tensorflow/issues/12844", "comments": ["Testing at http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/41/console", "Awaiting @yifeif to merge"]}, {"number": 12844, "title": "//tensorflow/python:saver_test is failing in Windows Bazel build", "body": "http://ci.tensorflow.org/job/tf-master-win-bzl/1515/console\r\n\r\n```\r\n17:24:51 ERROR: testSaveRestore (__main__.LenientNamesTest)\r\n17:24:51 ----------------------------------------------------------------------\r\n17:24:51 Traceback (most recent call last):\r\n17:24:51   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_call\r\n17:24:51     return fn(*args)\r\n17:24:51   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1300, in _run_fn\r\n17:24:51     status, run_metadata)\r\n17:24:51   File \"C:\\Program Files\\Anaconda3\\lib\\contextlib.py\", line 66, in __exit__\r\n17:24:51     next(self.gen)\r\n17:24:51   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n17:24:51     c_api.TF_GetCode(status))\r\n17:24:51 tensorflow.python.framework.errors_impl.NotFoundError: Key v0 not found in checkpoint\r\n17:24:51 \t [[Node: save_1/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]\r\n```\r\nI found out that `TF_SAVER_LENIENT_NAMES` environment variable set at\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver_test.py#L2474 is not propagated to the [C++ module](https://github.com/tensorflow/tensorflow/blob/bf6df5e2330dff8383869999840578fa5128e794/tensorflow/core/util/tensor_bundle/tensor_bundle.cc#L597).\r\n\r\nThe test passes when running with `--test_env=TF_SAVER_LENIENT_NAMES=True`.\r\n\r\nThis is only happening in the Bazel build, but not in the CMake build. We need to investigate the reason.\r\n\r\n", "comments": ["Looks like the whole build went down again, cannot verify if the issue is really resolved.", "@gunan No worries, sent #12900 to fix the build.", "@gunan And also, for this issue. I think it's because in Bazel build we use /MT option to statically link into C runtime, but in CMake, we use /MD to dynamically link into C runtime.\r\nThat's causing Python code and C++ not sharing the same C runtime in Bazel build, so the environment variable is not propagated from Python to C++.\r\nhttps://msdn.microsoft.com/en-us/library/2kzt1wy3.aspx", "Do we set that in TF build files and skylark rules, or does that come from bazel.\r\nI think eventually we want c runtime to be only dynamically linked everywhere.", "Also, we are now seeing this:\r\n```\r\n00:09:14.146 c:\\tmp\\_bazel_system\\424zmya1\\execroot\\org_tensorflow\\bazel-out\\msvc_x64-py3-opt\\genfiles\\external\\local_config_python\\numpy_include\\numpy\\numpyconfig.h(4): fatal error C1083: Cannot open include file: '_numpyconfig.h': No such file or directory\r\n```\r\nAny idea what causes this?", "Hi @gunan , \r\nhttps://github.com/bazelbuild/bazel/issues/2675\r\n\r\nAnd the last few comments in #11735\r\n", "@gunan Statically linking to C Run Time is currently the default option in Bazel, but it can be reconfigured to dynamic linking easily, I will send a change for this.\r\n\r\nFor the error, that's a flaky failure caused by https://github.com/bazelbuild/bazel/issues/2675, but unfortunately, we haven't found the root cause for this.", "The static linking problem is fixed in Bazel 0.7.0 by https://github.com/bazelbuild/bazel/commit/1edcd0f32d85686c5616a997b988479d3891d525 \r\nand the `Cannot open include file: '_numpyconfig.h': No such file or directory` error will be fixed Bazel 0.8.0 by https://github.com/bazelbuild/bazel/commit/6a4247b10f5cf040c1a7176498bef69c75b1b286.\r\nClosing this issue."]}, {"number": 12843, "title": "Error message of scatter_update is misleading ", "body": "When you call scatter_update with a wrong input vector, the error message will tell you that the rank is \r\nwrong, which is not very helpful.\r\nAn example case in the documentation (provided) would be great.\r\n\r\nExample:\r\n```\r\n      testVar = tf.Variable(tf.zeros([5,1]))\r\n      ind = tf.constant([0,3])\r\n      data = tf.constant([5,7], dtype=tf.float32)\r\n      up = tf.scatter_update(testVar,  ind,  data  )\r\n```\r\nError Message: \r\n**ValueError: Shapes must be equal rank, but are 1 and 2 for 'ScatterUpdate' (op: 'ScatterUpdate') with input shapes: [5,1], [2], [2].**\r\nThat's not that helpful because you don't know which of the three input shapes are wrong, but mostly fine.\r\nChanging the Code to:\r\n```\r\n      testVar = tf.Variable(tf.zeros([5,1]))\r\n      ind = tf.expand_dims(tf.constant([0,3]),0)\r\n      data = tf.expand_dims(tf.constant([5,7], dtype=tf.float32),0)\r\n      up = tf.scatter_update(testVar,  ind,  data  )\r\n```\r\n**ValueError: Shapes must be equal rank, but are 2 and 3 for 'ScatterUpdate' (op: 'ScatterUpdate') with input shapes: [5,1], [1,2], [1,2].**\r\nIs clearly wrong, because non of the inputs has rank 3!\r\n\r\nIn addition, it would be really awesome if you could add the following code as an example to the scatter_update documentation (https://www.tensorflow.org/api_docs/python/tf/scatter_update). \r\n(I couldn't find it in the repo(?), otherwise I would habe created a pull request)\r\n\r\nWorking Example:\r\n```\r\n      testVar = tf.Variable(tf.zeros([5,1]))\r\n      ind = tf.constant([0,3])\r\n      data = tf.expand_dims(tf.constant([5,7], dtype=tf.float32),1)\r\n      up = tf.scatter_update(testVar,  ind,  data  )\r\n```\r\n", "comments": ["Thanks for reporting! Do you want to send a PR to fix the message?", "@drpngx \r\nI don't know - could you point me to the place where the actual scatter_update implementation and documentation is? Then I may be able to create a PR.", "I found the part by myself:\r\nhttps://github.com/tensorflow/tensorflow/blob/2cfbd3347b943a389bb688125c2a90095b7735b5/tensorflow/core/ops/state_ops.cc\r\ncontains the doc and \r\nhttps://github.com/tensorflow/tensorflow/blob/584d4921014db921a8f4722749adff09737826a8/tensorflow/core/kernels/scatter_op.cc\r\nseems to be the actual implementation.\r\nWill provide a PR soon", "scatter_update has been changed to tensor_scatter_nd_update in Tensorflow 2 and the error message has been modified as well. Below is the code and the output.\r\n```\r\ntestVar = tf.Variable(tf.zeros([5,1]))\r\nind = tf.constant([0,3])\r\ndata = tf.constant([5,7], dtype=tf.float32)\r\nup = tf.tensor_scatter_nd_update(testVar,  ind,  data  )\r\n\r\nInvalidArgumentError: Inner dimensions of output shape must match inner dimensions of updates shape. Output: [5,1] updates: [2] [Op:TensorScatterUpdate]\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 12842, "title": "R1.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 12841, "title": "Tensorflow broken on importing tensorflow.contrib.tensorboard module", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Archlinux\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.3.0\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **CUDA/cuDNN version**:8.0.61\r\n- **GPU model and memory**: GTX 1070 7.92GiB\r\n- **Exact command to reproduce**:\r\n```\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nwhen I import input_data module from tensorflow.examples.tutorials.mnist, python throws the following errors.And I cannot find the files related to tensorboard under tensorflow/contrib directory. \r\nI dont know if the issue is a tensorboard issue.\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"tf_mnist.py\", line 1, in <module>\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/examples/tutorials/mnist/__init__.py\", line 21, in <modul\r\ne>\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 29, in <mod\r\nule>\r\n    from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 38, in <module>\r\n    from tensorflow.contrib import keras\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/__init__.py\", line 26, in <module>\r\n    from tensorflow.contrib.keras.api.keras import *\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/api/keras/__init__.py\", line 25, in <module\r\n>\r\n    from tensorflow.contrib.keras.api.keras import activations\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/api/keras/activations/__init__.py\", line 22\r\n, in <module>\r\n    from tensorflow.contrib.keras.python.keras.activations import elu\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/__init__.py\", line 21, in <mod\r\nule>\r\n    from tensorflow.contrib.keras.python.keras import activations\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/activations.py\", line 24, in <\r\nmodule>\r\n    from tensorflow.contrib.keras.python.keras.engine import Layer\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/engine/__init__.py\", line 26,\r\nin <module>\r\n    from tensorflow.contrib.keras.python.keras.engine.training import Model\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/engine/training.py\", line 28,\r\nin <module>\r\n    from tensorflow.contrib.keras.python.keras import callbacks as cbks\r\n  File \"/usr/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/callbacks.py\", line 34, in <mo\r\ndule>\r\n    from tensorflow.contrib.tensorboard.plugins import projector\r\nModuleNotFoundError: No module named 'tensorflow.contrib.tensorboard'\r\n```\r\nAnd I have installed the tensorboard package.But the problem remains.\r\nCould anyone help me?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Are you using an Arch based Linux distro, by any chance? Such as Arch Linux, Antergos or Manjaro?", "I am using Arch Linux, encountered the same issue\r\nI have reported to:\r\n[https://bugs.archlinux.org/task/55477](https://bugs.archlinux.org/task/55477)\r\nwaiting it to be fixed :)", "Well, this issue is not related with the repo itself ATM. I am the issuer of the ticket that have been mentioned.\r\n\r\nI have opened the ticket as soon as I got the error. First, they have closed it saying that it was a duplicate, it was not. Then they have reopened it but set it to \"Low\" priority, yeah a system breaking bug is not important as it seems. Now, the repo maintainer haven't fixed the issue for days.\r\n\r\nI have also made a pull request here, fixing the problem but I'm waiting for it to get a review. Even after they accept it, binaries of the new version might not be here for weeks.\r\n\r\nThis issue probably will be closed due to its unrelated-ness but you can go and change the occurrences of \"tensorflow.contrib.tensorboard\" with the new separate tensorflow, you are all good for now. ", "@goktay can you link to the PR? Thanks.", "PR: https://github.com/tensorflow/tensorflow/pull/12782\r\n\r\nHere you go. It is nothing big, just deleted the occurrences of \"contrib.tensorboard\" and put a few checks in keras/callbacks.py. \r\n\r\nI have looked for all of the instances of tensorboard with `grep -rwn \"tensorboard\"` and so far I think I have covered everywhere (grep has a recursion limit however I think I didn't hit it).\r\n\r\nMaybe you guys have a reason to keep a part of the tensorboard inside contrib but this seemed more logical.", "> I have opened the ticket as soon as I got the error. First, they have closed it saying that it was a duplicate, it was not. Then they have reopened it but set it to \"Low\" priority, yeah a system breaking bug is not important as it seems. Now, the repo maintainer haven't fixed the issue for days.\r\n\r\n1) mistakes happen\r\n2) https://wiki.archlinux.org/index.php/Reporting_bug_guidelines#Severity\r\nI do not read this issue as something that renders the entire package completely unusable. It absolutely, positively, cannot be described as \"system breaking\", something which you acknowledged when you opted for reporting it as high-severity rather than critical...\r\n\r\n\r\nYour aggressive attitude on the bugtracker and continued polemic here is not helping. Severities exist for a reason, and marking a bug as a lower priority is not a personal attack against you. Though if you absolutely insist and it would make you feel better, we can probably arrange for one...", "The latest tensorflow-cuda package has fixed the bug.Closed."]}, {"number": 12839, "title": "BLD: precheck patch command before invoked", "body": "The PR is opened to fix #12821.\r\n\r\n### How to test\r\n\r\n+ [ ] I have no idea how to test.", "comments": ["@facaiy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @tensorflower-gardener and @kirilg to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 12838, "title": "Feature Request: ADAG", "body": "It seems difficult to add [ADAG](http://joerihermans.com/ramblings/distributed-deep-learning-part-1-an-introduction/)  as an optimizer, because by default with tf.train.replica_device_setter(), all variables get assigned to a parameter server (ps).  Thus, it is difficult to update variables locally because the optimizer first pushes updates to the ps, allowing all other workers to see its updates.  If anyone know how to create local copies of variables, I can provide an example implementation.\r\n", "comments": ["@mrry, can you help?", "@tmulc18 don't use replica_device_setter, instead write your own setter which does all the fancy logic", "@yacoder thanks that tip helped!  I was able to implement ADAG but only with the worker updates using SDG.  When using any optimizer that creates variables, I'm having issues because the variables are by default global variables but they are assigned to the local device.  I'm able to get training to work by saying that each worker is the chief worker, which is a terrible hack. Is there a way to simply remove variables from collections, or have a collection scope similar to variable scope?", "I posted a hacky solution [here](https://github.com/tmulc18/Distributed-TensorFlow-Guide/blob/master/ADAG/ADAG.py).  It would be nice to have an actually function call from within TensorFlow before closing this issue, though.", "Closing as this has been resolved."]}, {"number": 12837, "title": "Add SONAME for Makefile-built libtensorflow_inference.so", "body": "Currently if we build our own shared library that depends on Makefile-built Tensorflow shared library, our built library is linking the Tensorflow library with **absolute path**. So added a `SONAME` to Makefile-built Tensorflow shared library to resolve this problem.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 12836, "title": "memory mapped file", "body": "This uses a FileChannel to map the model file for slightly faster IO. Because of the difficulty with assets in android, the files must be copied outside of the apk first and then read in.  I did not include that part, deciding it would be the onus of the parent application.   ctx.getFileStreamPath(model) is called in a static factory alongside what is already there for backward compatibility.  This means files must have been copied to the internal storage per ctx.getFilesDir().   Also it passed in the Application for context, taking care to recreate it as needed for reclaimed memory.\r\nThis is a preliminary example of what could be changed.  I am disappointed that the mmap implementation in Java is very limited.  The byte[] of array() is not written to work with \r\nMappedByteBuffer oddly, and asking for loaded state with load()  can kick off additional IO calls.\r\nIt is working in my library but I will also continue to improve on it. I  also plan to add a native jni version as well which makes use of the mmap util to see which runs faster.\r\nI backed out my static changes and will see if that works in another PR.", "comments": ["Can one of the admins verify this patch?", "@cloudbank, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar, @andrewharp and @tensorflower-gardener to be potential reviewers.", "@cloudbank thanks for the PR. Do you mind resolving the conflicts?", "it is merged\r\n\r\n\r\n\r\n> @cloudbank <https://github.com/cloudbank> thanks for the PR. Do you mind\r\n> resolving the conflicts?\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/pull/12836#issuecomment-327572225>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/ACLqpgSF9K9qxG6rStaRoUoV0AXmrXYCks5sfuRTgaJpZM4PNwsd>\r\n> .\r\n>\r\n", "@yifeif  I overloaded the constructor and loadGraph method instead of introducing something new. "]}, {"number": 12835, "title": "mnist", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?"]}, {"number": 12834, "title": "Update NCCL and Eigen sources for CUDA9", "body": "Also fix presumably outdated manifest_proto_py_pb2 and pandas dependencies.", "comments": ["Can one of the admins verify this patch?", "That seems to update eigen.\r\n\r\n@lukeiwanski @benoitsteiner @rmlarsen what do you think?", "I am pretty sure that this PR will hit the same issue as we did in https://github.com/tensorflow/tensorflow/pull/12010#issuecomment-320093539 .\r\n\r\nIn order to avoid that Eigen needs to get bumped to at least: https://bitbucket.org/eigen/eigen/commits/da11a2b1da3f93a3f1e5328368731f0ee8cdbab2?at=default", "Updated eigen changeset to top of default branch.\r\n\r\nAlso, what action is required for the bazel mirror 'http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz' to get created?", "For the Eigen changes, assign the reviewer to Benoit", "Jenkins, test this please."]}, {"number": 12833, "title": "Clarified difference between FIFOQueue and StagingArea", "body": "Clarified that StagingArea doesn't guarantee ordered delivery, contrasting tf.FIFOQueue", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I believe that StagingArea does behave in a FIFO manner. Internally, StagingArea uses a C++ [dequeue](https://github.com/tensorflow/tensorflow/blob/235192d47cfb375c0cc93c1deefb9e440715bf35/tensorflow/core/kernels/stage_op.cc#L46). \r\n\r\n[Put](https://github.com/tensorflow/tensorflow/blob/235192d47cfb375c0cc93c1deefb9e440715bf35/tensorflow/core/kernels/stage_op.cc#L115) pushes data onto the back of the deque, while [Get](https://github.com/tensorflow/tensorflow/blob/235192d47cfb375c0cc93c1deefb9e440715bf35/tensorflow/core/kernels/stage_op.cc#L134) pops data off the front. ", "I've observed different behaviour, and the documentation says that ordering isn't guaranteed. ", "The [test cases](https://github.com/tensorflow/tensorflow/blob/235192d47cfb375c0cc93c1deefb9e440715bf35/tensorflow/python/kernel_tests/stage_op_test.py#L240-L275) assume FIFO behaviour.\r\n\r\nAre you extracting values in different threads?", "Yes I have two filling the StagingAreas. One for training set and one for validation set, and also a thread running a validation pass every five seconds. ", "@yifeif good to go?\r\n\r\nJenkins, test this please.", "@yifeif ping ?", "Hi, I have the following code to be feed into the graph.\r\n\r\n    data_iter = get_train_dataflow(batch_size=cfg.TRAIN.BATCH_SIZE_PER_GPU)\r\n    ds = tf.data.Dataset()\r\n    ds = ds.from_generator(lambda: map(lambda x: tuple(x.values()), data_iter),\r\n                           (tf.float32, tf.float32, tf.int64, tf.int32),\r\n                           (tf.TensorShape([None, None, None, 3]),  # images\r\n                            tf.TensorShape([None, None, 4]),  # gt_boxes\r\n                            tf.TensorShape([None, None]),  # gt_labels\r\n                            tf.TensorShape([None, ])))  # orig_gt_counts\r\n    ds = ds.prefetch(buffer_size=256)\r\n    ds = ds.make_one_shot_iterator()\r\n    net_inputs = ds.get_next()\r\n    \r\n    then, the inputs are feed into the `StagingArea`,\r\n\r\n    inputs_list = []\r\n    for i in range(num_gpus):\r\n        inputs_list.append(net_inputs)\r\n    put_op_list = []\r\n    get_op_list = []\r\n    for i in range(num_gpus):\r\n        with tf.device(\"/GPU:{}\".format(i)):\r\n            area = StagingArea(dtypes=[tf.float32, tf.float32, tf.int64, tf.int32],\r\n                               shapes=(tf.TensorShape([None, None, None, 3]),\r\n                                       tf.TensorShape([None, None, 4]),\r\n                                       tf.TensorShape([None, None]),\r\n                                       tf.TensorShape([None, ])),\r\n                               capacity=64)\r\n            put_op_list.append(area.put(inputs_list[i]))\r\n            get_op_list.append(area.get())\r\n    \r\nit seems the code is wrong, do I need to generate a `tf.data.DataSet()` for each GPU stage area? The `tf.FIFOQueue` seems not to support the `None` shape of data but fixed known shape of data. But for object detection, the image size are not fixed. Could you please give some advice? Thanks."]}, {"number": 12832, "title": "Float16 (half or Eigen::half) for conv3d ops", "body": "Registrations of conv3d operations with fp16,  fp16 for batch_norms in tf.layers and tf.contrib.layers.  Related issue: https://github.com/tensorflow/tensorflow/issues/11341\r\n\r\nI don't understand how fp16 operations must be implemented in low-level with CUDA and I didn't make any additional optimizations.\r\nThere is no implementation for fused batch_norm yet.\r\n\r\nI copied part of code for dtypes from conv2d test to conv3d test and seems like all works. \r\nWith fp16 I get `inf` and large absolute errors with large numbers. I suppose that it's fine. At least CPU and GPU  implementations return almost similar values. I skip such cases in test.\r\nExample of such case:\r\n\r\n> \r\n> use_gpu: False\r\n> dtype: <dtype: 'float32'>\r\n> data_format: NDHWC\r\n> expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\r\n> actual =  [[[[[  36564.   38022.   39480.]\r\n>     [  37824.   39354.   40884.]\r\n>     [  39084.   40686.   42288.]]\r\n> \r\n>    [[  46644.   48678.   50712.]\r\n>     [  47904.   50010.   52116.]\r\n>     [  49164.   51342.   53520.]]]\r\n> \r\n> \r\n>   [[[ 107124.  112614.  118104.]\r\n>     [ 108384.  113946.  119508.]\r\n>     [ 109644.  115278.  120912.]]\r\n> \r\n>    [[ 117204.  123270.  129336.]\r\n>     [ 118464.  124602.  130740.]\r\n>     [ 119724.  125934.  132144.]]]]]\r\n> \r\n> use_gpu: False\r\n> dtype: <dtype: 'float16'>\r\n> data_format: NDHWC\r\n> expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\r\n> actual =  [[[[[ 36544.  38016.  39488.]\r\n>     [ 37824.  39360.  40896.]\r\n>     [ 39072.  40704.  42304.]]\r\n> \r\n>    [[ 46656.  48672.  50688.]\r\n>     [ 47936.  50016.  52128.]\r\n>     [ 49184.  51328.  53536.]]]\r\n> \r\n> \r\n>   [[[    inf     inf     inf]\r\n>     [    inf     inf     inf]\r\n>     [    inf     inf     inf]]\r\n> \r\n>    [[    inf     inf     inf]\r\n>     [    inf     inf     inf]\r\n>     [    inf     inf     inf]]]]]\r\n> fp16 using may result in inf values and large absolute errors when used with large numbers, skipping\r\n> \r\n> use_gpu: True\r\n> dtype: <dtype: 'float32'>\r\n> data_format: NDHWC\r\n> expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\r\n> actual =  [[[[[  36564.   38022.   39480.]\r\n>     [  37824.   39354.   40884.]\r\n>     [  39084.   40686.   42288.]]\r\n> \r\n>    [[  46644.   48678.   50712.]\r\n>     [  47904.   50010.   52116.]\r\n>     [  49164.   51342.   53520.]]]\r\n> \r\n> \r\n>   [[[ 107124.  112614.  118104.]\r\n>     [ 108384.  113946.  119508.]\r\n>     [ 109644.  115278.  120912.]]\r\n> \r\n>    [[ 117204.  123270.  129336.]\r\n>     [ 118464.  124602.  130740.]\r\n>     [ 119724.  125934.  132144.]]]]]\r\n> \r\n> use_gpu: True\r\n> dtype: <dtype: 'float16'>\r\n> data_format: NDHWC\r\n> expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\r\n> actual =  [[[[[ 36576.  38016.  39488.]\r\n>     [ 37824.  39360.  40896.]\r\n>     [ 39072.  40672.  42304.]]\r\n> \r\n>    [[ 46656.  48672.  50720.]\r\n>     [ 47904.  50016.  52128.]\r\n>     [ 49152.  51328.  53504.]]]\r\n> \r\n> \r\n>   [[[    inf     inf     inf]\r\n>     [    inf     inf     inf]\r\n>     [    inf     inf     inf]]\r\n> \r\n>    [[    inf     inf     inf]\r\n>     [    inf     inf     inf]\r\n>     [    inf     inf     inf]]]]]\r\n> fp16 using may result in inf values and large absolute errors when used with large numbers, skipping\r\n\r\n\r\nfp16 is not fully covered by tests because I not sure how to do it.", "comments": ["Can one of the admins verify this patch?", "@opensourcemattress, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @tensorflower-gardener and @keveman to be potential reviewers.", "@yzhwang do you mind taking a look? Thanks!", "@yzhwang any cycles for this?\r\n\r\nJenkins, test this please.", "@yzhwang ping", "Sorry I haven't been looking at this yet. Will finish it this week.", "@reedwm, what should I do next?", "Jenkins, test this please.", "Jenkins, test this please.", "Thanks for review!", "Jenkins, test this please."]}, {"number": 12831, "title": "Branch 167632896", "body": "", "comments": []}, {"number": 12830, "title": "Custom op linking flags", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Clean master (b5214cab6151fc9c0471829a05bab4872e2e3bc4)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.3\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: not relevant\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 7 cudnn 6\r\n- **GPU model and memory**: 1080 Ti\r\n- **Exact command to reproduce**: not relevant\r\n\r\n### Describe the problem\r\nIt is a feature request :)\r\n\r\nI'm compiling TF from source (for optimal performance). And I need custom op so I'm following (https://www.tensorflow.org/extend/adding_an_op#compile_the_op_using_bazel_tensorflow_source_installation) by adding a bazel BUILD file in user_ops directory.\r\n\r\nBut at the same time my custom op needs some other libraries (like opencv). I could easily install opencv using my system's package manager and add a \"-lopencv_xxx\" flag during linking. Currently I hacked definition of `tf_custom_op_library` to add the extra linking flags. But I hope that (since I think this scenario is pretty common), `tf_custom_op_library` could expose something like `extra_link_flags`.\r\n\r\nIt should just be a few extra lines of code upstream. I could submit a PR if you think it's mergable\r\n", "comments": ["@jart, this seems like a Bazel-related feature request (and offer to write a PR). Do you think it'd be mergeable?", "Apologies for the latency.\r\n\r\nIt should be possible to do this using the `linkopts` attribute of `tf_custom_op_library`.\r\n\r\nIt also surprises me that this build rule doesn't have a `copts` attr. That would be a trivial addition to make and we'd happily accept a PR adding it, if anyone needs it. Please note a workaround likely exists, which is defining a `cc_library` and adding that to the `deps` list of `tf_custom_op_library`."]}, {"number": 12829, "title": "Fix issue in tf.image.extract_glimpse", "body": "This fix tries to fix the issue raised in #2134 where `tf.image.extract_glimpse` does not work as expected when `centered=False` and `normalized=False`\r\n\r\nThis fix fixes #2134.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n\r\n@benoitsteiner what do you think?", "Hello, I am using tf.image.extract_glimpse. And I realised that the bug has not been in fixed in the code. How can I use this solution @yongtang ?", "@patriciacs1994 I think the PR has been merged long time ago. If the issue still persists, can you open a new issue with the extract version of TF you are using, and the minimal code snippet to reproduce the issue?", "Thank you, I will"]}, {"number": 12828, "title": "Skeleton code for annotation processor.", "body": "Add build changes and skeleton code (with test harness) for\r\nthe Operator annotation processor. This change focuses on build\r\nand test-related changes, and generates an empty Ops class.\r\n\r\nPlease see #7149 for the master tracking issue.", "comments": ["Can one of the admins verify this patch?", "@kbsriram, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar, @karllessard and @jart to be potential reviewers.", "@asimshankar what do you think?\r\n\r\nJenkins, test this please.", "@asimshankar ping", "@tensorflow-jenkins test this please", "Jenkins, test this please", "Could a kindly member look at/restart the testing? Looks like the \"Ubuntu CC\" test didn't run, or stalled. Thanks!", "Jenkins, test this please."]}, {"number": 12827, "title": "adding missing dep on jdk for gen_ops.bzl", "body": "sandboxed builds for this target are broken with the following error:\r\n+ external/local_jdk/bin/jar cMf bazel-out/linux_gnu_x86-opt/genfiles/tensorflow/java/ops/java_op_gen_sources.srcjar -C bazel-out/linux_gnu_x86-opt/genfiles/tensorflow/java/ops .\r\nexternal/local_jdk/bin/jar: error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory\r\n\r\nI found a similar issue in bazelbuild (https://github.com/bazelbuild/bazel/issues/444), seems to be the case that this error is produced when a (e.g, the genrule in line 476) is missing the JDK as an input to the action.", "comments": ["@nlopezgi, thanks for your PR! By analyzing the history of the files in this pull request, we identified @karllessard to be a potential reviewer.", "Can one of the admins verify this patch?", "Jenkins, test this please.\r\n\r\n@yifeif this may be a good candidate to test with.", "Thanks for the note @martinwicke. Gonna merge this now to unblock the build failure @nlopezgi saw. "]}, {"number": 12826, "title": "Branch 167604306", "body": "", "comments": []}]