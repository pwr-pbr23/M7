[{"number": 36652, "title": "Minor fix to logging message", "body": "There seems to be a `[` missing in the LogMessage log generation. For example I get\r\n`2020-02-10 11:20:35.474149: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: ...`\r\nImo this should be:\r\n`[2020-02-10 11:20:35.474149: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: ...`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36652) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36652) for more info**.\n\n<!-- ok -->", "It is a good PR, however, it will cause hours of CI for just one character. Can you offload this on a different PR please?", "@gf712 Can you please check mihaimaruseac's comments and keep us posted? Thanks!", "@mihaimaruseac Hi, do you mean add some more stuff to this PR? I can have a look at current issues and merge this PR with some other fix. ", "Yes please, @gf712 Thank you", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n", "@gbaned hey, sorry, I have been quite busy. I still have to look for some other issue in tf that I could integrate in this PR... Otherwise, someone could just cherrypick (I am not fussed about taking credit from contributions)? I am quite busy right now and will probably not be able to add more things within the next couple of weeks, sorry!"]}, {"number": 36651, "title": "VALUE error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nValueError Traceback (most recent call last)\r\nin\r\n----> 1 classifier.fit(X_train, y_train, batch_size=10, epochs=100)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n950 sample_weight=sample_weight,\r\n951 class_weight=class_weight,\r\n--> 952 batch_size=batch_size)\r\n953 # Prepare validation data.\r\n954 do_validation = False\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\r\n787 feed_output_shapes,\r\n788 check_batch_axis=False, # Don't enforce the batch size.\r\n--> 789 exception_prefix='target')\r\n790\r\n791 # Generate sample-wise weight values given the sample_weight and\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n136 ': expected ' + names[i] + ' to have shape ' +\r\n137 str(shape) + ' but got array with shape ' +\r\n--> 138 str(data_shape))\r\n139 return data\r\n140\r\n\r\nValueError: Error when checking target: expected dense_3 to have shape (6,) but got array with shape (1,)\r\n\r\n\r\n", "comments": ["@Jyothif  Please provide us with complete executable code for us to replicate it in our environment and help you resolve the issue.", "@Jyothif Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 36650, "title": "Custom layer weights all have the same name by default", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 \r\n- TensorFlow installed from (source or binary): no\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.6.7\r\n- GPU model and memory: no GPU\r\n\r\n**Describe the current behavior**\r\nI am writing a custom layer and stumbled upon the following error: https://github.com/tensorflow/tensorflow/issues/27688. The comment [here](https://github.com/tensorflow/tensorflow/issues/27688#issuecomment-582829374) made me realize that this was probably due to the custom layer weights I was using and realized that in my custom layer the weights all have by default the same name.\r\nBut it's not only my custom layer, if I use the code example from the [documentation](https://www.tensorflow.org/guide/keras/custom_layers_and_models#best_practice_deferring_weight_creation_until_the_shape_of_the_inputs_is_known), I also notice that the weights all have the same name (this is why I answered no to the question of whether I had written custom code, because it fails also in the code provided).\r\n\r\n**Describe the expected behavior**\r\n\r\nTo me it's more of a bug than a documentation issue because we expect the weights to have different names by default (increasing ids I mean). However, naming the weights indeed solves the problem.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Linear(tf.keras.layers.Layer):\r\n\r\n  def __init__(self, units=32):\r\n    super(Linear, self).__init__()\r\n    self.units = units\r\n\r\n  def build(self, input_shape):\r\n    self.w = self.add_weight(shape=(input_shape[-1], self.units),\r\n                             initializer='random_normal',\r\n                             trainable=True)\r\n    self.b = self.add_weight(shape=(self.units,),\r\n                             initializer='random_normal',\r\n                             trainable=True)\r\n\r\n  def call(self, inputs):\r\n    return tf.matmul(inputs, self.w) + self.b\r\n\r\nl = Linear(32)\r\nl.build((128,))\r\nprint([w.name for w in l.weights])\r\n```\r\n\r\n**Other info / logs**\r\nThe code above gives:\r\n\r\n```\r\n['Variable:0', 'Variable:0']\r\n```\r\n\r\nThis causes a problem when saving the model for example with the  `ModelCheckpoint` callback.\r\n", "comments": ["Could able to replicate the reported issue with Tf2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/a7bd20a199660b361c9eef47430f7e2a/untitled387.ipynb). Thanks!", "Thanks for reporting this! This is one of the effects of moving into TF 2.X -- variable names are no longer uniqueified. This should be fixed in one of two places in Keras: either generate unique names when the variables are created, or when the variables are saved out to HDF5. \r\n\r\n(This should only effect the HDF5 saving format. The `tf` format should still work)", "Passing the `name` argument in the `add_weight` should help override this behavior.", "> Thanks for reporting this! This is one of the effects of moving into TF 2.X -- variable names are no longer uniqueified. This should be fixed in one of two places in Keras: either generate unique names when the variables are created, or when the variables are saved out to HDF5.\r\n> \r\n> (This should only effect the HDF5 saving format. The `tf` format should still work)\r\n\r\nMay I ask the rationale behind allowing repeated names for variables in TF 2.X?", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524 , please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/d42ea3780c5a3889f919092c3d4d404f/35650.ipynb).Thanks!", "You can solve it using `tf.name_scope` in your build for each weight initialization like mentioned below.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass Linear(tf.keras.layers.Layer):\r\n\r\n  def __init__(self, units=32):\r\n    super(Linear, self).__init__()\r\n    self.units = units\r\n\r\n  def build(self, input_shape):\r\n    with tf.name_scope(\"first\"):\r\n      self.w = self.add_weight(shape=(input_shape[-1], self.units),\r\n                             initializer='random_normal',\r\n                             trainable=True)\r\n    with tf.name_scope(\"second\"):\r\n      self.b = self.add_weight(shape=(self.units,),\r\n                             initializer='random_normal',\r\n                             trainable=True)\r\n\r\n  def call(self, inputs):\r\n    return tf.matmul(inputs, self.w) + self.b\r\n\r\nl = Linear(32)\r\nl.build((128,))\r\nprint([w.name for w in l.weights])\r\n```\r\n\r\nOutput:\r\n`['first/Variable:0', 'second/Variable:0']`\r\n", "@sachinprasadhs yes indeed, you can also solve it by setting the name in `add_weight`. But I just think that the default behaviour should be to increase the index in `'Variable:0'`, i.e. by default not authorize same name.", "Issue still persists in `2.7.0-dev20210923`. Please take a look at the [gist here](https://colab.research.google.com/gist/sanatmpa1/f04bc96b4e7970ac200989132aeeb6d1/35650.ipynb). Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36650\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36650\">No</a>\n"]}, {"number": 36649, "title": "IndexError: list index out of range when using tf.dataset and keras for autoencoder", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, anaconda, python 3.7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow 2.0.0 (using tensorflow gpu)\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cudnn -7.6.5 /  cuda - 10.0\r\n- GPU model and memory: GeForce GTX 1050Ti, 4Gb of memory (anaconda sees 2996MB)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nunknown 2.0.0\r\n**Describe the current behavior**\r\nI'm trying to build an autoencoder using tensorflow keras using a sample of png files.   I've built a pipeline using the tensor flow dataset following closely to the tutorial provided here - https://www.tensorflow.org/guide/data#decoding_image_data_and_resizing_it .   I can see a tensor properly generated with the expected batch and shape.   The model also summarizes and compiles successfully, but when I do a model.fit to begin training, I get an error: IndexError: list index out of range. (full stack track below).  \r\n\r\n\r\n**Describe the expected behavior**\r\nModel should begin training.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n`import tensorflow as tf\r\nPNG_ROOT = 'g:/data/video/'\r\ndata_dir = PNG_ROOT + \"*/*\"\r\nlist_ds = tf.data.Dataset.list_files(data_dir)\r\n\r\ndef decode_img(img):\r\n  # convert the compressed string to a 3D uint8 tensor\r\n  IMG_WIDTH = 360\r\n  IMG_HEIGHT = 360\r\n  img = tf.io.decode_png(img, channels=3)\r\n  img = tf.image.resize_with_crop_or_pad(img, target_height=IMG_HEIGHT, target_width=IMG_WIDTH)  \r\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\r\n  img = tf.image.convert_image_dtype(img, tf.float32) \r\n  return img\r\n\r\ndef process_path(file_path):\r\n  img = tf.io.read_file(file_path)\r\n  image_tensor = decode_img(img)  \r\n  image_tensor = tf.reshape(image_tensor,(360,360,3))\r\n  #return img, label\r\n  return image_tensor\r\n\r\nbatch_size = 8\r\nlabeled_ds = list_ds.map(process_path)\r\nlabeled_ds = labeled_ds.batch(batch_size)\r\nds_train= labeled_ds.take(1000)\r\nds_validate = labeled_ds.take(50)\r\n\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Conv2D, Input, MaxPooling2D, UpSampling2D\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n\r\n\r\n#Build Model\r\ninput_feature = Input(batch_shape=(8,360,360,3))\r\n\r\nx = Conv2D(512, (3,3), activation='relu', input_shape=(8,360,360,3), padding='same')(input_feature)\r\nx = Conv2D(256, (3,3), activation='relu', padding='same')(x)\r\nx = MaxPooling2D(pool_size=2, padding='same')(x)\r\nx = Conv2D(128, (3,3), activation='relu', padding='same')(x)\r\nx = Conv2D(64, (3,3), activation='relu', padding='same')(x)\r\nencoded = MaxPooling2D(pool_size=2, padding='same')(x)\r\n\r\nx = Conv2D(64, (3,3), activation='relu', padding='same')(encoded)\r\nx = Conv2D(128, (3,3), activation='relu', padding='same')(x)\r\nx = UpSampling2D((2))(x)\r\nx = Conv2D(256, (3,3), activation='relu', padding='same')(x)\r\nx = Conv2D(512, (3,3), activation='relu', padding='same')(x)\r\nx = UpSampling2D((2))(x)\r\ndecoded = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\r\n\r\nautoencoder = Model(inputs=input_feature, outputs=decoded)\r\noptimizer = tf.keras.optimizers.Adam()\r\nautoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy'])\r\nautoencoder.summary()\r\n\r\ncheckpoint = ModelCheckpoint(\"best_conv2d_png_v1.hdf5\", monitor=\"val_loss\", verbose = 1, save_best_only=True, mode='auto', save_freq=100)\r\nhistory = autoencoder.fit(ds_train, epochs=5, callbacks=[checkpoint])\r\n`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nEpoch 1/5\r\n      1/Unknown - 0s 143ms/step\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-150-e74e2479945a> in <module>\r\n      1 #early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=5, mode='auto')\r\n      2 checkpoint = ModelCheckpoint(\"best_conv2d_video_v1.hdf5\", monitor=\"val_loss\", verbose = 1, save_best_only=True, mode='auto', save_freq=100)\r\n----> 3 history = autoencoder.fit(ds_train, epochs=5, callbacks=[checkpoint])\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    501       # This is the first call of __call__, so we have to initialize.\r\n    502       initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    504     finally:\r\n    505       # At this point we know that the initialization is complete (or less\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in distributed_function(input_iterator)\r\n     71     strategy = distribution_strategy_context.get_strategy()\r\n     72     outputs = strategy.experimental_run_v2(\r\n---> 73         per_replica_function, args=(model, x, y, sample_weights))\r\n     74     # Out of PerReplica outputs reduce or pick values to return.\r\n     75     all_outputs = dist_utils.unwrap_output_dict(\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in experimental_run_v2(self, fn, args, kwargs)\r\n    758       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\r\n    759                                 convert_by_default=False)\r\n--> 760       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    761 \r\n    762   def reduce(self, reduce_op, value, axis):\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   1785       kwargs = {}\r\n   1786     with self._container_strategy().scope():\r\n-> 1787       return self._call_for_each_replica(fn, args, kwargs)\r\n   1788 \r\n   1789   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n   2130         self._container_strategy(),\r\n   2131         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n-> 2132       return fn(*args, **kwargs)\r\n   2133 \r\n   2134   def _reduce_to(self, reduce_op, value, destinations):\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in train_on_batch(model, x, y, sample_weight, class_weight, reset_metrics)\r\n    262       y,\r\n    263       sample_weights=sample_weights,\r\n--> 264       output_loss_metrics=model._output_loss_metrics)\r\n    265 \r\n    266   if reset_metrics:\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py in train_on_batch(model, inputs, targets, sample_weights, output_loss_metrics)\r\n    309           sample_weights=sample_weights,\r\n    310           training=True,\r\n--> 311           output_loss_metrics=output_loss_metrics))\r\n    312   if not isinstance(outs, list):\r\n    313     outs = [outs]\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py in _process_single_batch(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n    250               output_loss_metrics=output_loss_metrics,\r\n    251               sample_weights=sample_weights,\r\n--> 252               training=training))\r\n    253       if total_loss is None:\r\n    254         raise ValueError('The model cannot be run '\r\n\r\nZ:\\anaconda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py in _model_loss(model, inputs, targets, output_loss_metrics, sample_weights, training)\r\n    164 \r\n    165         if hasattr(loss_fn, 'reduction'):\r\n--> 166           per_sample_losses = loss_fn.call(targets[i], outs[i])\r\n    167           weighted_losses = losses_utils.compute_weighted_loss(\r\n    168               per_sample_losses,\r\n\r\nIndexError: list index out of `range`\r\n", "comments": ["@jdchancellor \r\n\r\nCan you please provide colab link or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Thanks for the response.  I'll set up a colab and shoot over a link this\nafternoon once I get back to my workstation.\n\nJon\n\nOn Tue, Feb 11, 2020, 3:44 AM ravikyram <notifications@github.com> wrote:\n\n> @jdchancellor <https://github.com/jdchancellor>\n>\n> Can you please provide colab link or simple standalone code with proper\n> indentation and supporting files to reproduce the issue in our environment.\n> It helps us in localizing the issue faster. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36649?email_source=notifications&email_token=ANSXOLITDWW7W27XEY653WLRCKFQFA5CNFSM4KS3NHDKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELMD2KY#issuecomment-584596779>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANSXOLIF5KSSKHGN3C2YY6DRCKFQFANCNFSM4KS3NHDA>\n> .\n>\n", "https://colab.research.google.com/drive/1PvEO3waUT1CN_b9MqusrG6CKfKVCd324#scrollTo=BjLA9JsMElSF\n\n\nHas the stripped down code.   I enabled eager execution since colab is\nrunning TF 1.15 vs 2.0\n\nOn Tue, Feb 11, 2020 at 7:37 AM Jon Chancellor <jdchancellor@gmail.com>\nwrote:\n\n> Thanks for the response.  I'll set up a colab and shoot over a link this\n> afternoon once I get back to my workstation.\n>\n> Jon\n>\n> On Tue, Feb 11, 2020, 3:44 AM ravikyram <notifications@github.com> wrote:\n>\n>> @jdchancellor <https://github.com/jdchancellor>\n>>\n>> Can you please provide colab link or simple standalone code with proper\n>> indentation and supporting files to reproduce the issue in our environment.\n>> It helps us in localizing the issue faster. Thanks!\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/36649?email_source=notifications&email_token=ANSXOLITDWW7W27XEY653WLRCKFQFA5CNFSM4KS3NHDKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELMD2KY#issuecomment-584596779>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ANSXOLIF5KSSKHGN3C2YY6DRCKFQFANCNFSM4KS3NHDA>\n>> .\n>>\n>\n", "@jdchancellor  I think you are only specifying inputs not the model outputs. Please take a look this [comment](https://github.com/tensorflow/tensorflow/issues/21894#issuecomment-487746983).", "I tried this previously.  It appears that tensorflow knows it's an\nautoencoder and a tfds dataset as duplicating the input for Y gives the\nfollowing error. (or is there some way I need to extract the x values from\nthe dataset at run time?)  Interestingly, I switched over to using\nkeras's ImageDataGenerator with flow_from_directory and was able to get the\nmodel to start training (albeit less efficiently).  I got a similar error\nthere until I set the class_mode to 'input' and then everything worked\nnormally.  Does tfds have a similar class_mode that isn't apparent in the\ndocumentation?\n\nYou passed a dataset or dataset iterator (<DatasetV1Adapter shapes: (?,\n360, 360, 3), types: tf.float32>) as input `x` to your model. In that case,\nyou should not specify a target (`y`) argument, since the dataset or\ndataset iterator generates both input data and target data. Received:\n<DatasetV1Adapter shapes: (?, 360, 360, 3), types: tf.float32>\n\nOn Wed, Feb 12, 2020 at 2:14 PM gowthamkpr <notifications@github.com> wrote:\n\n> @jdchancellor <https://github.com/jdchancellor> I think you are only\n> specifying inputs not the model outputs. Please take a look this comment\n> <https://github.com/tensorflow/tensorflow/issues/21894#issuecomment-487746983>\n> .\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36649?email_source=notifications&email_token=ANSXOLLA4ZVEBPXTDR6O5X3RCRYFVA5CNFSM4KS3NHDKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELSS4HA#issuecomment-585444892>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANSXOLJUZLE3GSQMF7WCRPDRCRYFVANCNFSM4KS3NHDA>\n> .\n>\n", "@jdchancellor Sorry for the late response. Is this still an issue for you? I tried running your colab but don't have access to your data. Can you please update the colab with any public data and share it? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36649\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36649\">No</a>\n"]}, {"number": 36648, "title": "`tf.global_variables_initializer()` doesn't work sometimes", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.8\r\n\r\n`tf.global_variables_initializer()` doesn't work sometimes.\r\n\r\n```\r\nimport tensorflow as tf\r\ninit = tf.global_variables_initializer()\r\na = tf.Variable([1,2,3])\r\nwith tf.Session() as s:\r\n    s.run(init)\r\n    s.run(a)\r\n```\r\n\r\nThe above code throws an error of uninitialized variable.\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value Variable\r\n\t [[{{node _retval_Variable_0_0}}]]\r\n```\r\n\r\nWhile the below code works fine:\r\n\r\n```\r\na = tf.Variable([1,2,3])\r\nwith tf.Session() as s:\r\n    s.run(tf.global_variables_initializer())\r\n    s.run(a)\r\n```\r\n\r\nOutput:\r\n`array([1, 2, 3], dtype=int32)`\r\n", "comments": ["```python\r\na = tf.Variable([1,2,3])\r\ninit = tf.global_variables_initializer()\r\n```\r\n\r\nCall `tf.global_variables_initializer()` after all variables you want to init.", "@LoSealL\r\n\r\nYour solution works. Thank you..\r\n\r\nBut why is this necessary to call it after? I mean, it too just creates a node to execute and is independent of any other node in the graph, right?", "@ParikhKadam,\r\n[This](https://stackoverflow.com/a/45047930) StackOverflow comment might answer your query.", "Thank you @amahendrakar \r\n\r\nIt is a lot helpful.."]}, {"number": 36647, "title": "[INTEL MKL] DNNL 1.0 op support for Softmax, Identity_op, and Lrn ops.", "body": "Added changes to the softmax, identity_op and Lrn op kernels for DNNL 1.0 support.", "comments": []}, {"number": 36646, "title": "keras.callbacks.Tensorboard cannot reduce loss and produces error", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: No GPU\r\n\r\n**Describe the current behavior**\r\nWhen using keras.callbacks.Tensorboard as a callback for fitting a keras model, this error is raised:\r\n\r\n`ValueError: can only convert an array of size 1 to a Python scalar`\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.Input(shape=(), batch_size=5))\r\nmodel.add(keras.layers.Activation('sigmoid'))\r\nmodel.compile(loss=keras.losses.BinaryCrossentropy())\r\nxs = np.random.normal(size=200)\r\nys = np.random.randint(0, 2, 200)\r\ncb = keras.callbacks.TensorBoard('tmp')\r\nmodel.fit(xs, ys, callbacks=[cb], batch_size=5)\r\n```\r\nThe code above produces error. If TF 2.0 is used, OR if batch_size=1, the error is gone.\r\n", "comments": ["@Feri73, Its working without any error, Please take a look at the colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/294b786c8b8e0de16fe4d91e95ad9278/untitled388.ipynb). Thanks!", "The gist uses tf 2.0!\r\n\r\ngadagashwini <notifications@github.com> schrieb am Di., 11. Feb. 2020,\r\n22:40:\r\n\r\n> @Feri73 <https://github.com/Feri73>, Its working without any error,\r\n> Please take a look at the colab gist here\r\n> <https://colab.sandbox.google.com/gist/gadagashwini/294b786c8b8e0de16fe4d91e95ad9278/untitled388.ipynb>.\r\n> Thanks!\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/36646?email_source=notifications&email_token=AB6NILZIXCKSXHQ4ZZCRHYDRCOKWBA5CNFSM4KS3C66KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELPTSMY#issuecomment-585054515>,\r\n> or unsubscribe\r\n> .\r\n>\r\n", "Was able to replicate the issue with Tf 1.15.\r\nPlease find the [gist](https://colab.sandbox.google.com/gist/gadagashwini/47dbfb8fc51b92a690ec2b5645c61c70/untitled390.ipynb). Thanks!", "@Feri73,\r\nAs the `Tensorflow 1.x` is not very active in production and as `2.x` version of `Tensorflow` is predominantly used and is recommended, can you please confirm if we can close this issue? Thanks! ", "It's up to you if you want to ignore an existing bug :) I don't use tf 1.15 any more though.", "@Feri73,\r\nClosing the issue in line with [this comment](https://github.com/tensorflow/tensorflow/issues/36646#issuecomment-730396618). Thanks!  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36646\">No</a>\n"]}, {"number": 36645, "title": "Tensorflow lite NnApiDelegate crashes on Pixel 3a XL", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nAndroid\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nArch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nPixel 3A XL running on Android 10\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n2.1.0\r\n- Python version:\r\n3.7.0\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nRun tensorflow lite and specify the intepreter options with NnApiDelegete()\r\n```\r\n            val opts = Interpreter.Options()\r\n            opts.setNumThreads(NUM_LITE_THREADS)\r\n            opts.addDelegate(NnApiDelegate())\r\n            return@use Interpreter(modelBuffer, opts)\r\n```\r\n**Describe the expected behavior**\r\nThe code runs fine on emulator\r\n\r\n**Code to reproduce the issue**\r\nAdd NnApiDelegete and the code crashes on Pixel 3a XL\r\n\r\n**Other info / logs**\r\n```\r\n020-02-10 21:38:54.173 18949-18999/org.liberty.android.nlplib_demo E/ExecutionBuilder: ANeuralNetworksExecution_setInputFromMemory: Setting with operand type that is not fully specified\r\n    \r\n    --------- beginning of crash\r\n2020-02-10 21:38:54.182 18949-18949/org.liberty.android.nlplib_demo E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.liberty.android.nlplib_demo, PID: 18949\r\n    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: NN API returned error ANEURALNETWORKS_BAD_DATA at line 3126 while associating NNAPI execution input with a memory object.\r\n    \r\n    Node number 2428 (TfLiteNnapiDelegate) failed to invoke.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:154)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:311)\r\n        at org.liberty.android.nlplib.gpt2.GPT2Client$generate$1.invokeSuspend(GPT2Client.kt:70)\r\n        at org.liberty.android.nlplib.gpt2.GPT2Client$generate$1.invoke(Unknown Source:10)\r\n        at kotlinx.coroutines.flow.SafeFlow.collect(Builders.kt:53)\r\n        at org.liberty.android.nlplib_demo.MainActivity$onCreate$4$1$1.invokeSuspend(MainActivity.kt:65)\r\n        at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n        at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241)\r\n        at kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely(CoroutineScheduler.kt:594)\r\n        at kotlinx.coroutines.scheduling.CoroutineScheduler.access$runSafely(CoroutineScheduler.kt:60)\r\n        at kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run(CoroutineScheduler.kt:740)\r\n```", "comments": ["Thanks for the report, any chance you could attach the model, and/or a stripped version of the model? Feel free to PM me a link directly if there is any sensitivity, using my alias at google dot com.", "The model is this:\r\nhttps://s3.amazonaws.com/models.huggingface.co/bert/gpt2-64-fp16.tflite\r\n\r\nThe sample code is here: (Need to add `opts.addDelegate(NnApiDelegate())`\r\nhttps://github.com/huggingface/tflite-android-transformers/blob/master/gpt2/src/main/java/co/huggingface/android_transformers/gpt2/ml/GPT2Client.kt#L136", "Thanks for the report. Out of curiosity, since it looks like you used the fp16 compression feature, does the original full float model work properly with NNAPI? Or do you have a link to the fp32 model?", "I tried the original FP32 version https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-64.tflite.\r\nIt was converted from this model: https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-tf_model.h5\r\n\r\nIt still crashes with fp32 model.\r\n```\r\n2020-02-11 14:20:21.400 23855-23921/org.liberty.android.nlplib_demo E/Utils: The given inputs and outputs for operation MEAN are only supported in HAL version 1.1 and later (validating using HAL version 1.0)\r\n2020-02-11 14:20:21.400 23855-23921/org.liberty.android.nlplib_demo E/ExecutionBuilder: NN_RET_CHECK failed (frameworks/ml/nn/common/OperationsUtils.cpp:61): The given inputs and outputs are only supported in HAL version 1.2 and later (validating using HAL version 1.0)\r\n2020-02-11 14:20:21.400 23855-23921/org.liberty.android.nlplib_demo E/Utils: Validation failed for operation RSQRT\r\n2020-02-11 14:20:21.400 23855-23921/org.liberty.android.nlplib_demo E/ExecutionBuilder: NN_RET_CHECK failed (frameworks/ml/nn/common/OperationsUtils.cpp:61): The given inputs and outputs are only supported in HAL version 1.1 and later (validating using HAL version 1.0)\r\n2020-02-11 14:20:21.400 23855-23921/org.liberty.android.nlplib_demo E/ExecutionBuilder: NN_RET_CHECK failed (frameworks/ml/nn/common/operations/Broadcast.cpp:358): validateHalVersion(context, std::max(HalVersion::V1_0, opIntroducedAt)) \r\n2020-02-11 14:20:21.400 23855-23921/org.liberty.android.nlplib_demo E/Utils: Validation failed for operation SUB\r\n2020-02-11 14:20:21.680 23855-23855/org.liberty.android.nlplib_demo E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.liberty.android.nlplib_demo, PID: 23855\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: NN API returned error ANEURALNETWORKS_BAD_DATA at line 1068 while setting new operand value.\r\n   \r\n    NN API returned error ANEURALNETWORKS_BAD_DATA at line 1068 while setting new operand value.\r\n    \r\n    NN API returned error ANEURALNETWORKS_BAD_DATA at line 1068 while setting new operand value.\r\n    \r\n    NN API returned error ANEURALNETWORKS_BAD_DATA at line 1068 while setting new operand value.\r\n    \r\n    NN API returned error ANEURALNETWORKS_BAD_DATA at line 1068 while setting new operand value.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:336)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:82)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:234)\r\n        at org.liberty.android.nlplib.gpt2.GPT2Client.loadModel(GPT2Client.kt:110)\r\n        at org.liberty.android.nlplib.gpt2.GPT2Client.access$loadModel(GPT2Client.kt:31)\r\n        at org.liberty.android.nlplib.gpt2.GPT2Client$tflite$2.invoke(GPT2Client.kt:41)\r\n        at org.liberty.android.nlplib.gpt2.GPT2Client$tflite$2.invoke(GPT2Client.kt:31)\r\n        at kotlin.SynchronizedLazyImpl.getValue(LazyJVM.kt:74)\r\n        at org.liberty.android.nlplib.gpt2.GPT2Client.getTflite(Unknown Source:7)\r\n        at org.liberty.android.nlplib.gpt2.GPT2Client.access$getTflite$p(GPT2Client.kt:31)\r\n        at org.liberty.android.nlplib.gpt2.GPT2Client$generate$1.invokeSuspend(GPT2Client.kt:71)\r\n        at org.liberty.android.nlplib.gpt2.GPT2Client$generate$1.invoke(Unknown Source:10)\r\n        at kotlinx.coroutines.flow.SafeFlow.collect(Builders.kt:53)\r\n        at org.liberty.android.nlplib_demo.MainActivity$onCreate$4$1$1.invokeSuspend(MainActivity.kt:72)\r\n        at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n        at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241)\r\n        at kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely(CoroutineScheduler.kt:594)\r\n        at kotlinx.coroutines.scheduling.CoroutineScheduler.access$runSafely(CoroutineScheduler.kt:60)\r\n        at kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run(CoroutineScheduler.kt:740)\r\n```", "Could you post the logcat with \"adb shell setprop debug.nn.vlog 1\"? The verbose logging would helpful figuring out the root cause.", "[log-with-debug-nn-vlog-1.log](https://github.com/tensorflow/tensorflow/files/4189969/log-with-debug-nn-vlog-1.log)\r\n\r\nI have added the log file with debug on", "@miaowang14 and @jdduke: in case you don't have time to take a look at it yet. I dug a bit into it. The\r\n```\r\nNN API returned error ANEURALNETWORKS_BAD_DATA at line 1068 while setting new operand value.\r\n```\r\nmessages were caused by a scalar as the second operands of the Pow op. That is, broadcasting is supposed to work, but not. \r\n\r\ntensors dumped with in Python:\r\n```\r\n...\r\n{'name': 'tfgp_t2lm_head_model/transformer/h_._0/mlp/Pow', 'index': 360, 'shape': array([   1,   64, 3072], dtype=int32), 'shape_signature': array([   1,   64, 3072], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n{'name': 'tfgp_t2lm_head_model/transformer/h_._0/mlp/Pow/y', 'index': 361, 'shape': array([], dtype=int32), 'shape_signature': array([], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n...\r\n```\r\ntensors dumped with `label_image -v 1 ...`\r\n```\r\n...\r\n360: tfgp_t2lm_head_model/transformer/h_._0/mlp/Pow, 786432, 1, 0, 0                        \r\n361: tfgp_t2lm_head_model/transformer/h_._0/mlp/Pow/y, 4, 1, 0, 0\r\n...\r\n```\r\n\r\nI ran into similar problem when trying to pass [ALBERT](https://github.com/google-research/ALBERT) squad_v1 to NNAPI delegate.\r\n\r\nI sent a single line PR (https://github.com/tensorflow/tensorflow/pull/37383) to address this issue.", "@helloworld1 with my one-line patch, you should be able to delegate some ops of the model to NNAPI, but don't expect performance improvement yet. On Pixel 4, when I ran [TFLite Model Benchmark Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) `./benchmark_model --graph=gpt2-64.tflite --use_nnapi=1 --allow_fp16=1`, I got\r\n```\r\nNumber of nodes executed: 111\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t     TfLiteNnapiDelegate\t       50\t  3119.304\t    99.837%\t    99.837%\t     0.000\t       50\r\n\t      SQUARED_DIFFERENCE\t       25\t     4.177\t     0.134%\t    99.970%\t     0.000\t       25\r\n\t                    PACK\t       36\t     0.925\t     0.030%\t   100.000%\t     0.000\t       36\r\n\r\n```\r\n\r\nAs you can see `SQUARED_DIFFERENCE` and `PACK` ops are not supported in NNAPI, the model/graph is divided into > 100 nodes and performance doesn't look good.\r\n", "Thanks a lot @freedomtan I am going to try your patch out ", "Thanks a lot @freedomtan .\r\n\r\nAlso we are working on limiting the number of partitions when using NNAPI delegate, as it is never a good recipe for performance if we partition a graph into many small subgraphs.\r\n\r\nAlso, we are considering supporting SQUARED_DIFFERENCE with the ops already supported by NNAPI, e.g. MUL and SUB. Hopefully that could also help with models like this.", "Yes, the patch works and prevent the crash. But the performance regressed by more than 50%.\r\n", "@miaowang14 Glad to hear that. Yes, back and forth between CPU and accelerator because of tiny subgraphs doesn't sound right. Any plan for PACK? Converting PACK to RESHAPE + CONCATENATION should work.", "I am still facing this issue with TF 2.4\r\nPlease let me know if any fix is planned.\r\n", "@helloworld1 : Were you able to find some workaround for it ?", "Hi @helloworld1! Is this issue still valid in 2.8 version too? I see it has been resolved  from this [comment](https://github.com/tensorflow/tensorflow/issues/36645#issuecomment-595953641). Could you create a new issue to address the performance ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36645\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36645\">No</a>\n"]}, {"number": 36644, "title": "Node has inputs from different frames ('while_context', K.rnn)", "body": "I need a simple counter in LSTMCell's `call()`; how can this be accomplished? My attempts below.\r\n\r\n<hr>\r\n\r\n**Minimal example w/ approach 1**: I'm using TF 1.14.0 in Graph mode. Add the following to [`LSTMCell`](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L2027):\r\n\r\n```python\r\n# in call()\r\nself.add_update(self.step.assign_add(1))\r\n\r\n# in __init__()\r\nself._step = None\r\n\r\n@property\r\ndef step(self):\r\n    \"\"\"Variable. The current layer time step.\"\"\"\r\n    if self._step is None:\r\n        self._step = self.add_weight(\r\n            \"step\",\r\n            shape=[],\r\n            dtype=dtypes.int64,\r\n            trainable=False,\r\n            aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\r\n    return self._step\r\n```\r\n\r\n**Approach 2**:\r\n```python\r\n# instead of self._step and @property def step, add below to __init__\r\nself.step = K.variable(0, dtype='int64', name='step')\r\n```\r\n\r\n<hr>\r\n\r\n**Error traces**:\r\n\r\n```python\r\n## APPROACH 1\r\nFile \"C:\\dev_rbn\\main.py\", line 42, in <module>\r\n  model.train_on_batch(x, y)\r\nFile \"D:\\Anaconda\\envs\\s4_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1175, in train_on_batch\r\n  outputs = self.train_function(ins)  # pylint: disable=not-callable\r\nFile \"D:\\Anaconda\\envs\\s4_env\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3289, in __call__\r\n  self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)\r\nFile \"D:\\Anaconda\\envs\\s4_env\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3222, in _make_callable\r\n  callable_fn = session._make_callable_from_options(callable_opts)\r\nFile \"D:\\Anaconda\\envs\\s4_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1489, in _make_callable_from_options\r\n  return BaseSession._Callable(self, callable_options)\r\nFile \"D:\\Anaconda\\envs\\s4_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1446, in __init__\r\n  session._session, options_ptr)\r\n\r\nInvalidArgumentError: {{node training_1/group_deps}} has inputs from different frames. \r\nThe input {{node lstm/while/AssignAddVariableOp}} is in frame 'lstm/while/while_context'. \r\nThe input {{node Adam/Adam/AssignAddVariableOp}} is in frame ''.\r\n```\r\n```python\r\n## APPROACH 2\r\nFile \"C:\\dev_rbn\\main.py\", line 42, in <module>\r\n  model.train_on_batch(x, y)\r\nFile \"D:\\Anaconda\\envs\\s4_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1174, in train_on_batch\r\n  self._make_train_function()\r\nFile \"D:\\Anaconda\\envs\\s4_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 2219, in _make_train_function\r\n  params=self._collected_trainable_weights, loss=self.total_loss)\r\nFile \"D:\\Anaconda\\envs\\s4_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 492, in get_updates\r\n  grads = self.get_gradients(loss, params)\r\nFile \"D:\\Anaconda\\envs\\s4_env\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 399, in get_gradients\r\n  \"K.argmax, K.round, K.eval.\".format(param))\r\n  \r\nValueError: Variable <tf.Variable 'step:0' shape=() dtype=int64> has `None` for gradient. \r\nPlease make sure that all of your ops have a gradient defined (i.e. are differentiable). \r\nCommon ops without gradient: K.argmax, K.round, K.eval.\r\n```\r\n\r\n<hr>\r\n\r\n**Observations**:\r\n \r\n - `self.add_update(self.step.assign_add(1))` inserted right [after `K.rnn`](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/layers/recurrent.py#L749) in `RNN` does work w/ approach 1, but `self.step` must be accessible _during_ `K.rnn`'s [`control_flow_ops.while_loop()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4236), and be incremented by `1` at each loop iteration\r\n\r\n<hr>\r\n\r\n**Note**: I'm aware of workarounds via re-implementations of `RNN` or `K.rnn`, but that's too 'intrusive'; a simple op like this shouldn't require a dedicated base class and backend function.", "comments": ["Figured it out; passed `step` as a state. Didn't fix the error though, just worked around it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36644\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36644\">No</a>\n", "> Figured it out; passed `step` as a state. Didn't fix the error though, just worked around it.\r\n\r\nHow did you pass the step through hidden state? ", "@zahraatashgahi You can have a look at my attempted implementation of recurrent batchnorm for LSTM, which I've abandoned per [problems](https://github.com/tensorflow/tensorflow/issues/36797); need to override `self.state_size`, and `get_initial_state` (along possibly others).\r\n\r\n<details>\r\n  <summary><b>Code</b></summary>\r\n\r\n```python\r\n# -*- coding: utf-8 -*-\r\nimport tensorflow as tf\r\nfrom tensorflow.python.distribute import distribution_strategy_context\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.python.framework import tensor_shape, ops\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.keras import activations\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow.python.keras import constraints\r\nfrom tensorflow.python.keras import initializers\r\nfrom tensorflow.python.keras import regularizers\r\nfrom tensorflow.python.keras.engine.base_layer import Layer\r\nfrom tensorflow.python.keras.engine.input_spec import InputSpec\r\nfrom tensorflow.python.keras.utils import generic_utils\r\nfrom tensorflow.python.keras.utils import tf_utils\r\nfrom tensorflow.python.ops import array_ops, state_ops, math_ops\r\nfrom tensorflow.python.ops import nn\r\nfrom tensorflow.python.ops import variables as tf_variables\r\nfrom tensorflow.python.ops.logging_ops import print_v2 as tf_print\r\nfrom tensorflow.python.ops import control_flow_util\r\nfrom tensorflow.python.platform import tf_logging as logging\r\nfrom tensorflow.python.training.tracking import base as trackable\r\nfrom tensorflow.python.training.tracking import data_structures\r\nfrom tensorflow.python.util import nest\r\nfrom tensorflow.python.util.tf_export import keras_export\r\nfrom tensorflow.keras.layers import RNN\r\nfrom tensorflow.python.keras.layers.recurrent import DropoutRNNCellMixin\r\n\r\nimport warnings\r\nimport collections\r\nimport numpy as np\r\nimport sys\r\n\r\n\r\nRECURRENT_DROPOUT_WARNING_MSG = (\r\n    'RNN `implementation=2` is not supported when `recurrent_dropout` is set. '\r\n    'Using `implementation=1`.')\r\n\r\nRECURRENT_BATCHNORM_WARNING_MSG = (\r\n    'RNN `implementation=2` is not supported when `rbn_configs` '\r\n    'is set. Using `implementation=1`.')\r\n\r\n\r\nclass LSTMCell(DropoutRNNCellMixin, Layer):\r\n    def __init__(self,\r\n                 units,\r\n                 activation='tanh',\r\n                 recurrent_activation='hard_sigmoid',\r\n                 use_bias=True,\r\n                 kernel_initializer='glorot_uniform',\r\n                 recurrent_initializer='orthogonal',\r\n                 bias_initializer='zeros',\r\n                 unit_forget_bias=True,\r\n                 kernel_regularizer=None,\r\n                 recurrent_regularizer=None,\r\n                 bias_regularizer=None,\r\n                 kernel_constraint=None,\r\n                 recurrent_constraint=None,\r\n                 bias_constraint=None,\r\n                 dropout=0.,\r\n                 recurrent_dropout=0.,\r\n                 implementation=1,\r\n                  rbn_configs={},\r\n                 **kwargs):\r\n        super(LSTMCell, self).__init__(**kwargs)\r\n        self.units = units\r\n        self.activation = activations.get(activation)\r\n        self.recurrent_activation = activations.get(recurrent_activation)\r\n        self.use_bias = use_bias\r\n    \r\n        self.kernel_initializer = initializers.get(kernel_initializer)\r\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\r\n        self.bias_initializer = initializers.get(bias_initializer)\r\n        self.unit_forget_bias = unit_forget_bias\r\n    \r\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\r\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\r\n        self.bias_regularizer = regularizers.get(bias_regularizer)\r\n    \r\n        self.kernel_constraint = constraints.get(kernel_constraint)\r\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\r\n        self.bias_constraint = constraints.get(bias_constraint)\r\n    \r\n        self.dropout = min(1., max(0., dropout))\r\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\r\n        if self.recurrent_dropout != 0 and implementation != 1:\r\n            logging.debug(RECURRENT_DROPOUT_WARNING_MSG)\r\n            self.implementation = 1\r\n        elif rbn_configs and implementation != 1:\r\n            logging.debug(RECURRENT_BATCHNORM_WARNING_MSG)\r\n            self.implementation = 1\r\n        else:\r\n            self.implementation = implementation\r\n        # tuple(_ListWrapper) was silently dropping list content in at least \r\n        # 2.7.10, and fixed after 2.7.16. Converting the state_size to wrapper\r\n        # around NoDependency(), so that the base_layer.__setattr__ will not\r\n        # convert it to ListWrapper. Down the stream, self.states will be a list\r\n        # since it is generated from nest.map_structure with list, and \r\n        # tuple(list) will work properly.\r\n        self.state_size = data_structures.NoDependency([self.units, \r\n                                                        self.units, 1])\r\n        self.output_size = self.units\r\n\r\n        #######################################################################\r\n        self.recurrent_batchnorm = bool(rbn_configs != {})\r\n        if self.recurrent_batchnorm:\r\n            self.__init__recurrent_batchnorm(rbn_configs)\r\n\r\n    @tf_utils.shape_type_conversion\r\n    def build(self, input_shape):\r\n        default_caching_device = _caching_device(self)\r\n        input_dim = input_shape[-1]\r\n        self.kernel = self.add_weight(\r\n            shape=(input_dim, self.units * 4),\r\n            name='kernel',\r\n            initializer=self.kernel_initializer,\r\n            regularizer=self.kernel_regularizer,\r\n            constraint=self.kernel_constraint,\r\n            caching_device=default_caching_device)\r\n        self.recurrent_kernel = self.add_weight(\r\n            shape=(self.units, self.units * 4),\r\n            name='recurrent_kernel',\r\n            initializer=self.recurrent_initializer,\r\n            regularizer=self.recurrent_regularizer,\r\n            constraint=self.recurrent_constraint,\r\n            caching_device=default_caching_device)            \r\n    \r\n        if self.use_bias:\r\n          if self.unit_forget_bias:\r\n    \r\n            def bias_initializer(_, *args, **kwargs):\r\n              return K.concatenate([\r\n                  self.bias_initializer((self.units,), *args, **kwargs),\r\n                  initializers.Ones()((self.units,), *args, **kwargs),\r\n                  self.bias_initializer((self.units * 2,), *args, **kwargs),\r\n              ])\r\n          else:\r\n            bias_initializer = self.bias_initializer\r\n          self.bias = self.add_weight(\r\n              shape=(self.units * 4,),\r\n              name='bias',\r\n              initializer=bias_initializer,\r\n              regularizer=self.bias_regularizer,\r\n              constraint=self.bias_constraint,\r\n              caching_device=default_caching_device)              \r\n        else:\r\n          self.bias = None\r\n          \r\n        if self.recurrent_batchnorm:\r\n            self.build_recurrent_bn(input_shape)\r\n        self.built = True       \r\n\r\n    def _compute_carry_and_output(self, x, h_tm1, c_tm1, step, training):\r\n        \"\"\"Computes carry and output using split kernels.\"\"\"\r\n        x_i, x_f, x_c, x_o = x\r\n        h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\r\n        \r\n        z0 = K.dot(h_tm1_i, self.recurrent_kernel[:, :self.units])\r\n        z1 = K.dot(h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2])\r\n        z2 = K.dot(h_tm1_c, self.recurrent_kernel[:, self.units * 2:\r\n                                                  self.units * 3])\r\n        z3 = K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:])\r\n        if self.recurrent_batchnorm:\r\n            z0 = self.recurrent_bn(z0, 'recurrent_i', step, training)\r\n            z1 = self.recurrent_bn(z1, 'recurrent_f', step, training)\r\n            z2 = self.recurrent_bn(z2, 'recurrent_c', step, training)\r\n            z3 = self.recurrent_bn(z3, 'recurrent_o', step, training)\r\n        \r\n        i = self.recurrent_activation(x_i + z0)\r\n        f = self.recurrent_activation(x_f + z1)\r\n        c = f * c_tm1 + i * self.activation(x_c + z2)\r\n        o = self.recurrent_activation(x_o + z3)\r\n        if self.recurrent_batchnorm:\r\n            c = self.recurrent_bn(c, 'cell', step, training)\r\n        return c, o\r\n\r\n    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\r\n        if batch_size is None or dtype is None:\r\n            raise ValueError(\r\n                (\"'batch_size and dtype cannot be None while constructing \"\r\n                  \"initial state: 'batch_size={}, dtype={}'\").format(\r\n                    batch_size, dtype))\r\n\t    \r\n        def create_zeros(unnested_state_size):\r\n            flat_dims = tensor_shape.as_shape(unnested_state_size).as_list()\r\n            if len(flat_dims) == 1 and flat_dims[0] == 1:\r\n                # return array_ops.zeros(flat_dims, dtype='int64')\r\n                return tf.constant(0, dtype='int64')\r\n            else:\r\n                return array_ops.zeros([batch_size] + flat_dims, \r\n                                        dtype=dtype)\r\n        return nest.map_structure(create_zeros, self.state_size)\r\n        \r\n        \r\n    def call(self, inputs, states, training=None):\r\n        h_tm1 = states[0]  # previous memory state\r\n        c_tm1 = states[1]  # previous carry state\r\n        step  = states[2]\r\n        # step  = math_ops.cast(states[2], 'int64')\r\n        # t=tf.timestamp()\r\n\t    \r\n        dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\r\n        rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(\r\n            h_tm1, training, count=4)\r\n\t    \r\n        if 0 < self.dropout < 1.:\r\n            inputs_i = inputs * dp_mask[0]\r\n            inputs_f = inputs * dp_mask[1]\r\n            inputs_c = inputs * dp_mask[2]\r\n            inputs_o = inputs * dp_mask[3]\r\n        else:\r\n            inputs_i = inputs\r\n            inputs_f = inputs\r\n            inputs_c = inputs\r\n            inputs_o = inputs\r\n        k_i, k_f, k_c, k_o = array_ops.split(\r\n            self.kernel, num_or_size_splits=4, axis=1)\r\n        \r\n        x_i = K.dot(inputs_i, k_i)\r\n        x_f = K.dot(inputs_f, k_f)\r\n        x_c = K.dot(inputs_c, k_c)\r\n        x_o = K.dot(inputs_o, k_o)\r\n        if self.recurrent_batchnorm:\r\n            x_i = self.recurrent_bn(x_i, 'kernel_i', step, training)\r\n            x_f = self.recurrent_bn(x_f, 'kernel_f', step, training)\r\n            x_c = self.recurrent_bn(x_c, 'kernel_c', step, training)\r\n            x_o = self.recurrent_bn(x_o, 'kernel_o', step, training)\r\n        if self.use_bias:\r\n            b_i, b_f, b_c, b_o = array_ops.split(\r\n                self.bias, num_or_size_splits=4, axis=0)\r\n            x_i = K.bias_add(x_i, b_i)\r\n            x_f = K.bias_add(x_f, b_f)\r\n            x_c = K.bias_add(x_c, b_c)\r\n            x_o = K.bias_add(x_o, b_o)\r\n\t  \r\n        if 0 < self.recurrent_dropout < 1.:\r\n            h_tm1_i = h_tm1 * rec_dp_mask[0]\r\n            h_tm1_f = h_tm1 * rec_dp_mask[1]\r\n            h_tm1_c = h_tm1 * rec_dp_mask[2]\r\n            h_tm1_o = h_tm1 * rec_dp_mask[3]\r\n        else:\r\n            h_tm1_i = h_tm1\r\n            h_tm1_f = h_tm1\r\n            h_tm1_c = h_tm1\r\n            h_tm1_o = h_tm1\r\n            \r\n        x = (x_i, x_f, x_c, x_o)\r\n        h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\r\n        c, o = self._compute_carry_and_output(x, h_tm1, c_tm1, step, training)\r\n        h = o * self.activation(c)\r\n        \r\n#        @tf.function\r\n#        def notify(a, b):\r\n#            if tf.math.greater(a, b):\r\n#                tf.print(tf.timestamp() - self.t)\r\n#        notify(self.max_inference_step, math_ops.cast(step, 'int64'))\r\n#        self.t = tf.timestamp()\r\n        return h, [h, c, step + 1]\r\n\r\n    # def _get_bn_vars(self, op_name):\r\n    #     gate_name = ''\r\n    #     if 'cell' not in op_name:\r\n    #         gate_name = op_name[-2:]\r\n    #         op_name = op_name[:-2]\r\n    #     return [getattr(self, op_name + '_' + var_name + gate_name) \r\n    #             for var_name in \r\n    #             ('beta', 'gamma', 'moving_mean', 'moving_variance')]\r\n\r\n    def _get_bn_vars(self, op_name):\r\n        return [getattr(self, op_name + '_' + var_name) for var_name in\r\n                ('gamma', 'beta', 'moving_mean', 'moving_variance')]\r\n\r\n    def recurrent_bn(self, inputs_t, op_name, step, training=None):\r\n        training = self._get_training_value(training)\r\n        beta, gamma, moving_mean, moving_variance = self._get_bn_vars(op_name)\r\n\r\n        @tf.function\r\n        def outs(inputs_t, step, beta, gamma, moving_mean, moving_variance,\r\n                 training):\r\n            if tf.math.less(step, self.max_inference_step) and (\r\n                    training != False):\r\n                mean_t, variance_t = self._get_stats_and_maybe_update(\r\n                    inputs_t, step, moving_mean, moving_variance, training)\r\n                # tf.print(step, end='x')\r\n            else:\r\n                # tf.print(step, end=\"_\")\r\n                mean_t = K.stop_gradient(moving_mean[-1])\r\n                variance_t = K.stop_gradient(moving_variance[-1])\r\n                beta = K.stop_gradient(beta)\r\n                gamma = K.stop_gradient(gamma)\r\n\r\n            return nn.batch_normalization(inputs_t, mean_t, variance_t,\r\n                                          beta, gamma, self.bn_epsilon)\r\n    \r\n        outputs = outs(inputs_t, step, beta, gamma, moving_mean,\r\n                       moving_variance, training)\r\n        outputs.set_shape(inputs_t.shape)\r\n        return outputs\r\n                # mean_t, variance_t = (moving_mean[-1], moving_variance[-1])\r\n        # return K.stop_gradient(outputs)\r\n\r\n            # mean_t, variance_t = tf_utils.smart_cond(\r\n            #     tf.math.less(step, self.max_inference_step),\r\n            #     lambda: self._get_stats_and_maybe_update(\r\n            #         inputs_t, step, moving_mean, moving_variance, training),\r\n            #     lambda: (moving_mean[-1], moving_variance[-1]),\r\n            #     )\r\n            \r\n        # train_step_exceeds_max_inference = tf.math.logical_and(\r\n        #     training, tf.math.greater(step, self.max_inference_step))\r\n        # train_step_exceeds_max_inference = tf.math.greater(\r\n        #     step, self.max_inference_step)\r\n\r\n        # mean_t, variance_t = tf.cond(\r\n        #     train_step_exceeds_max_inference,\r\n        #     lambda: (moving_mean[-1], moving_variance[-1]),\r\n        #     lambda: self._get_stats_and_maybe_update(\r\n        #         inputs_t, step, moving_mean, moving_variance, training)\r\n        #     )\r\n\r\n    def _get_stats_and_maybe_update(self, inputs_t, step, \r\n                                    moving_mean, moving_variance, training):\r\n        moving_mean_t = moving_mean[step:step+1] \r\n        moving_variance_t = moving_variance[step:step+1]\r\n        \r\n        return tf_utils.smart_cond(training,\r\n                       lambda: self._update_moving_avgs_and_get_batch_stats(\r\n                           inputs_t, moving_mean_t, moving_variance_t, training),\r\n                       lambda: (moving_mean_t, moving_variance_t))\r\n\r\n        # mean_t = tf_utils.smart_cond(\r\n        #     training,\r\n        #     lambda: mean_t,\r\n        #     lambda: ops.convert_to_tensor_v2(moving_mean_t))\r\n        # variance_t = tf_utils.smart_cond(\r\n        #       training,\r\n        #       lambda: variance_t,\r\n        #       lambda: ops.convert_to_tensor_v2(moving_variance_t))\r\n\r\n    def _update_moving_avgs_and_get_batch_stats(self, inputs_t, moving_mean_t,\r\n                                                moving_variance_t, training):\r\n        mean_t, variance_t = nn.moments(inputs_t, axes=[0], keep_dims=False)\r\n        self._assign_moving_average(moving_mean_t, mean_t)\r\n        self._assign_moving_average(moving_variance_t, variance_t)\r\n        return mean_t, variance_t\r\n\r\n        # new_mean_t, new_variance_t = mean_t, variance_t\r\n\r\n        # if self._support_zero_size_input():\r\n        #     inputs_size = array_ops.size(inputs_t)\r\n        # else:\r\n        #     inputs_size = None\r\n\r\n        # def _do_update(var, value):\r\n        #     \"\"\"Compute the updates for mean and variance.\"\"\"\r\n        #     return self._assign_moving_average(var, value, self.bn_momentum,\r\n        #                                        inputs_size)\r\n        # def mean_update():\r\n        #     return _do_update(moving_mean_t, new_mean_t)\r\n    \r\n        # def variance_update():\r\n        #     return _do_update(moving_variance_t, new_variance_t)\r\n\r\n        # self.add_update(mean_update)\r\n        # self.add_update(variance_update)\r\n\r\n            # true_branch = lambda: _do_update(moving_mean_t, new_mean_t)\r\n            # false_branch = lambda: moving_mean_t\r\n            # return tf_utils.smart_cond(training, true_branch, false_branch)\r\n\r\n            # true_branch = lambda: _do_update(moving_variance_t, new_variance_t)\r\n            # false_branch = lambda: moving_variance_t\r\n            # return tf_utils.smart_cond(training, true_branch, false_branch)\r\n\r\n    def _get_training_value(self, training=None):\r\n        if training is None:\r\n            training = K.learning_phase()\r\n        return tf.cast(training, tf.bool)\r\n        # return K.stop_gradient(tf.cast(training, tf.bool))\r\n\r\n    def _assign_moving_average(self, variable, value):#, momentum, inputs_size):\r\n        with K.name_scope('AssignMovingAvg') as scope:\r\n            with ops.colocate_with(variable):\r\n                update_delta = (variable - math_ops.cast(value, variable.dtype)\r\n                                ) * self._decay\r\n                variable.assign(math_ops.sub(variable, update_delta), name=scope)\r\n\r\n                # if decay.dtype != variable.dtype.base_dtype:\r\n                #     decay = math_ops.cast(decay, variable.dtype.base_dtype)\r\n\r\n                # return state_ops.assign_sub(variable, update_delta, name=scope)\r\n\r\n                # if inputs_size is not None:\r\n                #     update_delta = array_ops.where(inputs_size > 0, update_delta,\r\n                #                                    K.zeros_like(update_delta))\r\n\r\n    # def _moments(self, inputs, reduction_axes, keep_dims):\r\n    #     mean, variance = nn.moments(inputs, reduction_axes, keep_dims=keep_dims)\r\n    #     if self._support_zero_size_input():\r\n    #         inputs_size = array_ops.size(inputs)\r\n    #         mean = array_ops.where(inputs_size > 0, mean, K.zeros_like(mean))\r\n    #         variance = array_ops.where(inputs_size > 0, variance,\r\n    #                                    K.zeros_like(variance))\r\n    #     return mean, variance\r\n        # return K.stop_gradient(mean), K.stop_gradient(variance)\r\n\r\n    def build_recurrent_bn(self, input_shape):\r\n        def _build_steps_variable(var_fullname, fullname):\r\n            return self.add_weight(\r\n                shape=(self.max_inference_steps, self.units),\r\n                name=fullname,\r\n                initializer=getattr(self, var_fullname + '_initializer'),\r\n                trainable=False,\r\n                caching_device=default_caching_device)\r\n\r\n        def _build_variable(var_fullname, fullname):\r\n            return self.add_weight(\r\n                        shape=(1, self.units),\r\n                        name=fullname,\r\n                        initializer=getattr(self, var_fullname + '_initializer'),\r\n                        regularizer=getattr(self, var_fullname + '_regularizer'),\r\n                        constraint=getattr(self, var_fullname + '_constraint'),\r\n                        trainable=True,\r\n                        caching_device=default_caching_device)\r\n        \r\n        default_caching_device = _caching_device(self)\r\n        \r\n        for weight_name in ('kernel', 'recurrent', 'cell'):    \r\n            gate_names = ('_i', '_f', '_c', '_o'\r\n                          ) if weight_name != 'cell' else ('',)\r\n            for gate_name in gate_names:\r\n                op_name = weight_name + gate_name\r\n                for var_name in ('gamma', 'beta', 'moving_mean', \r\n                                 'moving_variance'):\r\n                    fullname = op_name + '_' + var_name\r\n                    var_fullname = weight_name + '_' + var_name\r\n                    args = (var_fullname, fullname)\r\n    \r\n                    if 'moving' in var_name:\r\n                        setattr(self, fullname, _build_steps_variable(*args))\r\n                    else:\r\n                        setattr(self, fullname, _build_variable(*args))\r\n\r\n    \r\n    def __init__recurrent_batchnorm(self, configs):\r\n        def _validate_configs(configs, default_configs):\r\n            for key in configs:\r\n                if key not in default_configs:\r\n                    raise ValueError(\"unknown kwarg: `%s`\" % key)\r\n            for key in ('max_inference_steps_frac', 'bn_epsilon'):\r\n                if getattr(self, key) < 0:\r\n                    raise ValueError(\"%s cannot be negative\" % key)\r\n            if self.steps_in is None:\r\n                raise ValueError(\"must set `steps_in`; use K.int_shape(x)[1], \"\r\n                                  \"where `x` is input to layer\")\r\n\r\n        def _init_initializers(default_configs):\r\n            for key in default_configs:\r\n                if 'initializer' in key:\r\n                    val = getattr(self, key)\r\n                    if isinstance(val, float):\r\n                        setattr(self, key, initializers.Constant(val))\r\n                    else:\r\n                        setattr(self, key, initializers.get(val))\r\n\r\n        default_configs = dict(\r\n            steps_in=None,\r\n            max_inference_steps_frac=1,\r\n            bn_epsilon=1e-3,  # keras BN default\r\n            bn_momentum=.99,\r\n            kernel_gamma_initializer=0.1,  ## TODO: check 0.1\r\n            kernel_gamma_regularizer=None,\r\n            kernel_gamma_constraint=None,\r\n            kernel_beta_initializer='zeros',\r\n            kernel_beta_regularizer=None,\r\n            kernel_beta_constraint=None,\r\n            kernel_moving_mean_initializer='zeros',\r\n            kernel_moving_variance_initializer=0.1,\r\n            recurrent_gamma_initializer=0.1,  ## TODO: check 0.1\r\n            recurrent_gamma_regularizer=None,\r\n            recurrent_gamma_constraint=None,\r\n            recurrent_beta_initializer='zeros',\r\n            recurrent_beta_regularizer=None,\r\n            recurrent_beta_constraint=None,\r\n            recurrent_moving_mean_initializer='zeros',\r\n            recurrent_moving_variance_initializer=0.1,\r\n            cell_gamma_initializer=0.1,\r\n            cell_gamma_regularizer=None,\r\n            cell_gamma_constraint=None,\r\n            cell_beta_initializer='zeros',\r\n            cell_beta_regularizer=None,\r\n            cell_beta_constraint=None,\r\n            cell_moving_mean_initializer='zeros',\r\n            cell_moving_variance_initializer=0.1,\r\n              )\r\n        for key, val in default_configs.items():\r\n            setattr(self, key, configs.get(key, val))\r\n\r\n        _validate_configs(configs, default_configs)\r\n        _init_initializers(default_configs)\r\n        self.max_inference_steps = int(self.max_inference_steps_frac * \r\n                                        self.steps_in)\r\n        self.max_inference_step = K.constant(self.max_inference_steps - 1,\r\n                                             dtype='int64', \r\n                                             name='max_inference_step')\r\n        self._decay = K.constant(1.0 - self.bn_momentum, \r\n                                 dtype='float32', name='decay')\r\n                \r\n    def _support_zero_size_input(self):\r\n        return distribution_strategy_context.has_strategy() and getattr(\r\n            distribution_strategy_context.get_strategy().extended,\r\n            'experimental_enable_get_next_as_optional', False)\r\n\r\n    def get_config(self):  ##TODO\r\n        config = {\r\n            'units':\r\n                self.units,\r\n            'activation':\r\n                activations.serialize(self.activation),\r\n            'recurrent_activation':\r\n                activations.serialize(self.recurrent_activation),\r\n            'use_bias':\r\n                self.use_bias,\r\n            'kernel_initializer':\r\n                initializers.serialize(self.kernel_initializer),\r\n            'recurrent_initializer':\r\n                initializers.serialize(self.recurrent_initializer),\r\n            'bias_initializer':\r\n                initializers.serialize(self.bias_initializer),\r\n            'unit_forget_bias':\r\n                self.unit_forget_bias,\r\n            'kernel_regularizer':\r\n                regularizers.serialize(self.kernel_regularizer),\r\n            'recurrent_regularizer':\r\n                regularizers.serialize(self.recurrent_regularizer),\r\n            'bias_regularizer':\r\n                regularizers.serialize(self.bias_regularizer),\r\n            'kernel_constraint':\r\n                constraints.serialize(self.kernel_constraint),\r\n            'recurrent_constraint':\r\n                constraints.serialize(self.recurrent_constraint),\r\n            'bias_constraint':\r\n                constraints.serialize(self.bias_constraint),\r\n            'dropout':\r\n                self.dropout,\r\n            'recurrent_dropout':\r\n                self.recurrent_dropout,\r\n            'implementation':\r\n                self.implementation\r\n        }\r\n        base_config = super(LSTMCell, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n\r\n@keras_export(v1=['keras.layers.LSTM'])\r\nclass LSTM(RNN):\r\n    def __init__(self,\r\n                 units,\r\n                 activation='tanh',\r\n                 recurrent_activation='hard_sigmoid',\r\n                 use_bias=True,\r\n                 kernel_initializer='glorot_uniform',\r\n                 recurrent_initializer='orthogonal',\r\n                 bias_initializer='zeros',\r\n                 unit_forget_bias=True,\r\n                 kernel_regularizer=None,\r\n                 recurrent_regularizer=None,\r\n                 bias_regularizer=None,\r\n                 activity_regularizer=None,\r\n                 kernel_constraint=None,\r\n                 recurrent_constraint=None,\r\n                 bias_constraint=None,\r\n                 dropout=0.,\r\n                 recurrent_dropout=0.,\r\n                 implementation=1,\r\n                 return_sequences=False,\r\n                 return_state=False,\r\n                 go_backwards=False,\r\n                 stateful=False,\r\n                 unroll=False,\r\n                 **kwargs):\r\n        if implementation == 0:\r\n            logging.warning('`implementation=0` has been deprecated, '\r\n                            'and now defaults to `implementation=1`.'\r\n                            'Please update your layer call.')\r\n        cell = LSTMCell(\r\n            units,\r\n            activation=activation,\r\n            recurrent_activation=recurrent_activation,\r\n            use_bias=use_bias,\r\n            kernel_initializer=kernel_initializer,\r\n            recurrent_initializer=recurrent_initializer,\r\n            unit_forget_bias=unit_forget_bias,\r\n            bias_initializer=bias_initializer,\r\n            kernel_regularizer=kernel_regularizer,\r\n            recurrent_regularizer=recurrent_regularizer,\r\n            bias_regularizer=bias_regularizer,\r\n            kernel_constraint=kernel_constraint,\r\n            recurrent_constraint=recurrent_constraint,\r\n            bias_constraint=bias_constraint,\r\n            dropout=dropout,\r\n            recurrent_dropout=recurrent_dropout,\r\n            implementation=implementation,\r\n            rbn_configs=kwargs.pop('rbn_configs', {})\r\n            )\r\n        super(LSTM, self).__init__(\r\n            cell,\r\n            return_sequences=return_sequences,\r\n            return_state=return_state,\r\n            go_backwards=go_backwards,\r\n            stateful=stateful,\r\n            unroll=unroll,\r\n            **kwargs)\r\n        self.activity_regularizer = regularizers.get(activity_regularizer)\r\n        self.input_spec = [InputSpec(ndim=3)]\r\n\t    \r\n    def call(self, inputs, mask=None, training=None, initial_state=None):\r\n        self.cell.reset_dropout_mask()\r\n        self.cell.reset_recurrent_dropout_mask()\r\n        return super(LSTM, self).call(\r\n            inputs, mask=mask, training=training, initial_state=initial_state)\r\n\t    \r\n    def _validate_state_spec(self, cell_state_sizes, init_state_specs):\r\n        \"\"\"Validate the state spec between the initial_state and the state_size.\r\n\t    \r\n        Args:\r\n          cell_state_sizes: list, the `state_size` attribute from the cell.\r\n          init_state_specs: list, the `state_spec` from the initial_state that is\r\n            passed in `call()`.\r\n\t    \r\n        Raises:\r\n          ValueError: When initial state spec is not compatible with the state size.\r\n        \"\"\"\r\n        validation_error = ValueError(\r\n            'An `initial_state` was passed that is not compatible with '\r\n            '`cell.state_size`. Received `state_spec`={}; '\r\n            'however `cell.state_size` is '\r\n            '{}'.format(init_state_specs, cell_state_sizes))\r\n        flat_cell_state_size = nest.flatten(cell_state_sizes)\r\n        flat_state_spec = nest.flatten(init_state_specs)\r\n\t    \r\n        if len(flat_cell_state_size) != len(flat_state_spec):\r\n            raise validation_error\r\n        for i in range(len(flat_cell_state_size)):\r\n            state_spec_shape = flat_state_spec[i].shape\r\n            if len(state_spec_shape) == 1:\r\n                if not (tensor_shape.TensorShape(\r\n                    # Check scalar case first\r\n                    state_spec_shape[0]).is_compatible_with(\r\n                        tensor_shape.TensorShape(flat_cell_state_size[i]))):\r\n                    raise validation_error\r\n                continue\r\n            if not tensor_shape.TensorShape(\r\n                # Ignore the first axis for init_state which is for batch\r\n                state_spec_shape[1:]).is_compatible_with(\r\n                    tensor_shape.TensorShape(flat_cell_state_size[i])):\r\n                raise validation_error\r\n\r\n    @property\r\n    def units(self):\r\n        return self.cell.units\r\n\t    \r\n    @property\r\n    def activation(self):\r\n        return self.cell.activation\r\n\t    \r\n    @property\r\n    def recurrent_activation(self):\r\n        return self.cell.recurrent_activation\r\n\t    \r\n    @property\r\n    def use_bias(self):\r\n        return self.cell.use_bias\r\n\t    \r\n    @property\r\n    def kernel_initializer(self):\r\n        return self.cell.kernel_initializer\r\n\t    \r\n    @property\r\n    def recurrent_initializer(self):\r\n        return self.cell.recurrent_initializer\r\n\t    \r\n    @property\r\n    def bias_initializer(self):\r\n        return self.cell.bias_initializer\r\n\t    \r\n    @property\r\n    def unit_forget_bias(self):\r\n        return self.cell.unit_forget_bias\r\n\t    \r\n    @property\r\n    def kernel_regularizer(self):\r\n        return self.cell.kernel_regularizer\r\n\t    \r\n    @property\r\n    def recurrent_regularizer(self):\r\n        return self.cell.recurrent_regularizer\r\n\t    \r\n    @property\r\n    def bias_regularizer(self):\r\n        return self.cell.bias_regularizer\r\n\t    \r\n    @property\r\n    def kernel_constraint(self):\r\n        return self.cell.kernel_constraint\r\n\t    \r\n    @property\r\n    def recurrent_constraint(self):\r\n        return self.cell.recurrent_constraint\r\n\t    \r\n    @property\r\n    def bias_constraint(self):\r\n        return self.cell.bias_constraint\r\n\t    \r\n    @property\r\n    def dropout(self):\r\n        return self.cell.dropout\r\n\t    \r\n    @property\r\n    def recurrent_dropout(self):\r\n        return self.cell.recurrent_dropout\r\n\t    \r\n    @property\r\n    def implementation(self):\r\n        return self.cell.implementation\r\n\t    \r\n    def get_config(self):  ## TODO\r\n        config = {\r\n            'units':\r\n                self.units,\r\n            'activation':\r\n                activations.serialize(self.activation),\r\n            'recurrent_activation':\r\n                activations.serialize(self.recurrent_activation),\r\n            'use_bias':\r\n                self.use_bias,\r\n            'kernel_initializer':\r\n                initializers.serialize(self.kernel_initializer),\r\n            'recurrent_initializer':\r\n                initializers.serialize(self.recurrent_initializer),\r\n            'bias_initializer':\r\n                initializers.serialize(self.bias_initializer),\r\n            'unit_forget_bias':\r\n                self.unit_forget_bias,\r\n            'kernel_regularizer':\r\n                regularizers.serialize(self.kernel_regularizer),\r\n            'recurrent_regularizer':\r\n                regularizers.serialize(self.recurrent_regularizer),\r\n            'bias_regularizer':\r\n                regularizers.serialize(self.bias_regularizer),\r\n            'activity_regularizer':\r\n                regularizers.serialize(self.activity_regularizer),\r\n            'kernel_constraint':\r\n                constraints.serialize(self.kernel_constraint),\r\n            'recurrent_constraint':\r\n                constraints.serialize(self.recurrent_constraint),\r\n            'bias_constraint':\r\n                constraints.serialize(self.bias_constraint),\r\n            'dropout':\r\n                self.dropout,\r\n            'recurrent_dropout':\r\n                self.recurrent_dropout,\r\n            'implementation':\r\n                self.implementation\r\n        }\r\n        base_config = super(LSTM, self).get_config()\r\n        del base_config['cell']\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\t    \r\n    @classmethod\r\n    def from_config(cls, config):\r\n        if 'implementation' in config and config['implementation'] == 0:\r\n          config['implementation'] = 1\r\n        return cls(**config)\r\n    \r\ndef _generate_dropout_mask(ones, rate, training=None, count=1):\r\n  def dropped_inputs():\r\n    return K.dropout(ones, rate)\r\n\r\n  if count > 1:\r\n    return [\r\n        K.in_train_phase(dropped_inputs, ones, training=training)\r\n        for _ in range(count)\r\n    ]\r\n  return K.in_train_phase(dropped_inputs, ones, training=training)\r\n\r\n\r\ndef _standardize_args(inputs, initial_state, constants, num_constants):\r\n  \"\"\"Standardizes `__call__` to a single list of tensor inputs.\r\n\r\n  When running a model loaded from a file, the input tensors\r\n  `initial_state` and `constants` can be passed to `RNN.__call__()` as part\r\n  of `inputs` instead of by the dedicated keyword arguments. This method\r\n  makes sure the arguments are separated and that `initial_state` and\r\n  `constants` are lists of tensors (or None).\r\n\r\n  Arguments:\r\n    inputs: Tensor or list/tuple of tensors. which may include constants\r\n      and initial states. In that case `num_constant` must be specified.\r\n    initial_state: Tensor or list of tensors or None, initial states.\r\n    constants: Tensor or list of tensors or None, constant tensors.\r\n    num_constants: Expected number of constants (if constants are passed as\r\n      part of the `inputs` list.\r\n\r\n  Returns:\r\n    inputs: Single tensor or tuple of tensors.\r\n    initial_state: List of tensors or None.\r\n    constants: List of tensors or None.\r\n  \"\"\"\r\n  if isinstance(inputs, list):\r\n    # There are several situations here:\r\n    # In the graph mode, __call__ will be only called once. The initial_state\r\n    # and constants could be in inputs (from file loading).\r\n    # In the eager mode, __call__ will be called twice, once during\r\n    # rnn_layer(inputs=input_t, constants=c_t, ...), and second time will be\r\n    # model.fit/train_on_batch/predict with real np data. In the second case,\r\n    # the inputs will contain initial_state and constants as eager tensor.\r\n    #\r\n    # For either case, the real input is the first item in the list, which\r\n    # could be a nested structure itself. Then followed by initial_states, which\r\n    # could be a list of items, or list of list if the initial_state is complex\r\n    # structure, and finally followed by constants which is a flat list.\r\n    assert initial_state is None and constants is None\r\n    if num_constants:\r\n      constants = inputs[-num_constants:]\r\n      inputs = inputs[:-num_constants]\r\n    if len(inputs) > 1:\r\n      initial_state = inputs[1:]\r\n      inputs = inputs[:1]\r\n\r\n    if len(inputs) > 1:\r\n      inputs = tuple(inputs)\r\n    else:\r\n      inputs = inputs[0]\r\n\r\n  def to_list_or_none(x):\r\n    if x is None or isinstance(x, list):\r\n      return x\r\n    if isinstance(x, tuple):\r\n      return list(x)\r\n    return [x]\r\n\r\n  initial_state = to_list_or_none(initial_state)\r\n  constants = to_list_or_none(constants)\r\n\r\n  return inputs, initial_state, constants\r\n\r\n\r\ndef _is_multiple_state(state_size):\r\n  \"\"\"Check whether the state_size contains multiple states.\"\"\"\r\n  return (hasattr(state_size, '__len__') and\r\n          not isinstance(state_size, tensor_shape.TensorShape))\r\n\r\n\r\ndef _generate_zero_filled_state_for_cell(cell, inputs, batch_size, dtype):\r\n  if inputs is not None:\r\n    batch_size = array_ops.shape(inputs)[0]\r\n    dtype = inputs.dtype\r\n  return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\r\n\r\n\r\ndef _generate_zero_filled_state(batch_size_tensor, state_size, dtype):\r\n  \"\"\"Generate a zero filled tensor with shape [batch_size, state_size].\"\"\"\r\n  if batch_size_tensor is None or dtype is None:\r\n    raise ValueError(\r\n        'batch_size and dtype cannot be None while constructing initial state: '\r\n        'batch_size={}, dtype={}'.format(batch_size_tensor, dtype))\r\n\r\n  def create_zeros(unnested_state_size):\r\n    flat_dims = tensor_shape.as_shape(unnested_state_size).as_list()\r\n    init_state_size = [batch_size_tensor] + flat_dims\r\n    return array_ops.zeros(init_state_size, dtype=dtype)\r\n\r\n  if nest.is_sequence(state_size):\r\n    return nest.map_structure(create_zeros, state_size)\r\n  else:\r\n    return create_zeros(state_size)\r\n\r\nif 1:\r\n    # def normalize_inference():  ## TODO - check consistency w/ paper\r\n    #     return K.batch_normalization(\r\n    #         inputs_t,\r\n    #         moving_mean_t,\r\n    #         moving_variance_t,\r\n    #         beta,\r\n    #         gamma,\r\n    #         axis=-1,\r\n    #         epsilon=self.bn_epsilon)\r\n\r\n    # normed_training, mean_t, variance_t = K.normalize_batch_in_training(\r\n    #     inputs_t, gamma, beta, reduction_axes, epsilon=self.bn_epsilon)\r\n\r\n    # if K.backend() != 'cntk':\r\n    #     variance_t *= batch_size / (batch_size - (1 + self.bn_epsilon))\r\n    \r\n    # ## TODO: check if zero_debias is OK w/ recurrent bn\r\n    # self.add_update([K.moving_average_update(moving_mean_t, mean_t, \r\n    #                               self.bn_momentum),\r\n    #                  K.moving_average_update(moving_variance_t, variance_t, \r\n    #                               self.bn_momentum)],\r\n    #                 inputs_t)\r\n\r\n    # # Pick the normalized form corresponding to the training phase.\r\n    # return K.in_train_phase(normed_training,\r\n    #                         normalize_inference,\r\n    #                         training=training)\r\n    pass\r\n\r\ndef _caching_device(rnn_cell):\r\n  \"\"\"Returns the caching device for the RNN variable.\r\n\r\n  This is useful for distributed training, when variable is not located as same\r\n  device as the training worker. By enabling the device cache, this allows\r\n  worker to read the variable once and cache locally, rather than read it every\r\n  time step from remote when it is needed.\r\n\r\n  Note that this is assuming the variable that cell needs for each time step is\r\n  having the same value in the forward path, and only gets updated in the\r\n  backprop. It is true for all the default cells (SimpleRNN, GRU, LSTM). If the\r\n  cell body relies on any variable that gets updated every time step, then\r\n  caching device will cause it to read the stall value.\r\n\r\n  Args:\r\n    rnn_cell: the rnn cell instance.\r\n  \"\"\"\r\n  if context.executing_eagerly():\r\n    # caching_device is not supported in eager mode.\r\n    return None\r\n  if not getattr(rnn_cell, '_enable_caching_device', False):\r\n    return None\r\n  # Don't set a caching device when running in a loop, since it is possible that\r\n  # train steps could be wrapped in a tf.while_loop. In that scenario caching\r\n  # prevents forward computations in loop iterations from re-reading the\r\n  # updated weights.\r\n  if control_flow_util.IsInWhileLoop(ops.get_default_graph()):\r\n    logging.warn('Variable read device caching has been disabled because the '\r\n                'RNN is in tf.while_loop loop context, which will cause '\r\n                'reading stalled value in forward path. This could slow down '\r\n                'the training due to duplicated variable reads. Please '\r\n                'consider updating your code to remove tf.while_loop if '\r\n                'possible.')\r\n    return None\r\n  if rnn_cell._dtype_policy.should_cast_variables:\r\n    logging.warn('Variable read device caching has been disabled since it '\r\n                'doesn\\'t work with the mixed precision API. This is '\r\n                'likely to cause a slowdown for RNN training due to '\r\n                'duplicated read of variable for each timestep, which '\r\n                'will be significant in a multi remote worker setting. '\r\n                'Please consider disabling mixed precision API if '\r\n                'the performance has been affected.')\r\n    return None\r\n  # Cache the value on the device that access the variable.\r\n  return lambda op: op.device\r\n```\r\n</details>", "> @zahraatashgahi You can have a look at my attempted implementation of recurrent batchnorm for LSTM, which I've abandoned per [problems](https://github.com/tensorflow/tensorflow/issues/36797); need to override `self.state_size`, and `get_initial_state` (along possibly others).\r\n> \r\n> Code\r\n\r\nThank you very much for the solution.  It solved my problem."]}, {"number": 36643, "title": "Error Thrown When Decoration Used: Suggests Using Decoration", "body": "- I am going through the DeepDream tutorial for Tensorflow 2.0 located here:\r\nhttps://www.tensorflow.org/tutorials/generative/deepdream\r\n\r\n- OS Platform and Distribution: Clear Linux OS, VERSION_ID=32270\r\n- TensorFlow installed from (source or binary): clear linux machine-learning-tensorflow bundle\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.8.1\r\n- CPU model and memory: Intel(R) Xeon(R) Gold 6136 CPU @ 3.00GHz\r\n\r\n**Describe the current behavior**\r\n\r\nThis works:\r\n\r\n```\r\nclass DeepDream(tf.Module):\r\n  def __init__(self, model):\r\n    self.model = model\r\n\r\n# !!!!! NOTICE THAT THESE LINES ARE COMMENTED OUT !!!!!\r\n# :\r\n#   @tf.function(\r\n#       input_signature=(\r\n#         tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\r\n#         tf.TensorSpec(shape=[], dtype=tf.int32),\r\n#         tf.TensorSpec(shape=[], dtype=tf.float32),)\r\n#   )\r\n  def __call__(self, img, steps, step_size):\r\n      print(\"Tracing\")\r\n      loss = tf.constant(0.0)\r\n      for n in tf.range(steps):\r\n        with tf.GradientTape() as tape:\r\n          # This needs gradients relative to `img`\r\n          # `GradientTape` only watches `tf.Variable`s by default\r\n          tape.watch(img)\r\n          loss = calc_loss(img, self.model)\r\n\r\n        # Calculate the gradient of the loss with respect to the pixels of the input image.\r\n        gradients = tape.gradient(loss, img)\r\n\r\n        # Normalize the gradients.\r\n        gradients /= tf.math.reduce_std(gradients) + 1e-8 \r\n\r\n        # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\r\n        # You can update the image by directly adding the gradients (because they're the same shape!)\r\n        img = img + gradients*step_size\r\n        img = tf.clip_by_value(img, -1, 1)\r\n\r\n      return loss, img\r\n```\r\n\r\nThis (as is given in the tutorial) does not:\r\n\r\n```\r\nclass DeepDream(tf.Module):\r\n  def __init__(self, model):\r\n    self.model = model\r\n\r\n  @tf.function(\r\n      input_signature=(\r\n        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\r\n        tf.TensorSpec(shape=[], dtype=tf.int32),\r\n        tf.TensorSpec(shape=[], dtype=tf.float32),)\r\n  )\r\n  def __call__(self, img, steps, step_size):\r\n      print(\"Tracing\")\r\n      loss = tf.constant(0.0)\r\n      for n in tf.range(steps):\r\n        with tf.GradientTape() as tape:\r\n          # This needs gradients relative to `img`\r\n          # `GradientTape` only watches `tf.Variable`s by default\r\n          tape.watch(img)\r\n          loss = calc_loss(img, self.model)\r\n\r\n        # Calculate the gradient of the loss with respect to the pixels of the input image.\r\n        gradients = tape.gradient(loss, img)\r\n\r\n        # Normalize the gradients.\r\n        gradients /= tf.math.reduce_std(gradients) + 1e-8 \r\n\r\n        # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\r\n        # You can update the image by directly adding the gradients (because they're the same shape!)\r\n        img = img + gradients*step_size\r\n        img = tf.clip_by_value(img, -1, 1)\r\n\r\n      return loss, img\r\n```\r\nI've put the error that's thrown in the sections below.  \r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect it to work with the decoration tag and (perhaps) not work with the decoration tag commented out. However when I run it with the decoration it throws an error that seems to suggest adding the decoration that appears to already be there, and when I comment out the decoration tag it works.  When I use the decoration tag the final error is:\r\n\r\n`OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.`\r\n\r\n(more detailed log below)\r\n\r\nHowever when I remove the decoration by commenting it out, that is when it works.\r\n\r\nNOTE:  I have Tensorflow 2.0.0 setup on google collab as well with python 3.6.9 and it does not display the same behavior.  It works either with or without the decoration.\r\n\r\nAll of this was installed on a fresh Clear Linux install tonight with no tinkering.  I had another installation earlier where I tried a fix seen elsewhere which included downgrading gast to 0.2.2.  I did that and that did not work.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n# [CURRENT DEEP DREAM TUTORIAL AT: https://www.tensorflow.org/tutorials/generative/deepdream\r\n\r\ndef run_deep_dream_simple(img, steps=100, step_size=0.01):\r\n  # Convert from uint8 to the range expected by the model.\r\n  img = tf.keras.applications.inception_v3.preprocess_input(img)\r\n  img = tf.convert_to_tensor(img)\r\n  step_size = tf.convert_to_tensor(step_size)\r\n  steps_remaining = steps\r\n  step = 0\r\n  while steps_remaining:\r\n    if steps_remaining>100:\r\n      run_steps = tf.constant(100)\r\n    else:\r\n      run_steps = tf.constant(steps_remaining)\r\n    steps_remaining -= run_steps\r\n    step += run_steps\r\n\r\n    loss, img = deepdream(img, run_steps, tf.constant(step_size))\r\n    \r\n    display.clear_output(wait=True)\r\n    show(deprocess(img))\r\n    print (\"Step {}, loss {}\".format(step, loss))\r\n\r\n\r\n  result = deprocess(img)\r\n  display.clear_output(wait=True)\r\n  show(result)\r\n\r\n  return result\r\n```\r\n```\r\nfrom datetime import datetime\r\nstartTime = datetime.now()\r\n\r\ndream_img = run_deep_dream_simple(img=original_img, \r\n                                  steps=100, step_size=0.01)\r\n\r\nprint(datetime.now() - startTime)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\nWARNING:tensorflow:Entity <bound method DeepDream.__call__ of <tensorflow.python.eager.function.TfMethodTarget object at 0x7faea2d46d30>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\nWARNING: Entity <bound method DeepDream.__call__ of <tensorflow.python.eager.function.TfMethodTarget object at 0x7faea2d46d30>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\nTracing\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py in converted_call(f, options, args, kwargs, caller_fn_scope)\r\n    505         options=options, autograph_module=tf_inspect.getmodule(converted_call))\r\n--> 506     converted_f = conversion.convert(target_entity, program_ctx)\r\n    507 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py in convert(entity, program_ctx)\r\n    320 \r\n--> 321   converted_entity_info = _convert_with_cache(entity, program_ctx,\r\n    322                                               free_nonglobal_var_names)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py in _convert_with_cache(entity, program_ctx, free_nonglobal_var_names)\r\n    238 \r\n--> 239     nodes, converted_name, entity_info = convert_entity_to_ast(\r\n    240         entity, program_ctx)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py in convert_entity_to_ast(o, program_ctx)\r\n    470   elif tf_inspect.ismethod(o):\r\n--> 471     nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n    472   elif hasattr(o, '__class__'):\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py in convert_func_to_ast(f, program_ctx, do_rename)\r\n    668   context = converter.EntityContext(namer, entity_info, program_ctx, new_name)\r\n--> 669   node = node_to_graph(node, context)\r\n    670 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/conversion.py in node_to_graph(node, context)\r\n    697 \r\n--> 698   node = converter.standard_analysis(node, context, is_initial=True)\r\n    699   node = converter.apply_(node, context, function_scopes)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/core/converter.py in standard_analysis(node, context, is_initial)\r\n    383   node = qual_names.resolve(node)\r\n--> 384   node = activity.resolve(node, context, None)\r\n    385   node = reaching_definitions.resolve(node, context, graphs, AnnotatedDef)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py in resolve(node, context, parent_scope)\r\n    497 def resolve(node, context, parent_scope=None):\r\n--> 498   return ActivityAnalyzer(context, parent_scope).visit(node)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit(self, node)\r\n    479     if not anno.hasanno(node, anno.Basic.SKIP_PROCESSING):\r\n--> 480       result = super(Base, self).visit(node)\r\n    481     self.ctx.current_origin = parent_origin\r\n\r\n/usr/lib/python3.8/ast.py in visit(self, node)\r\n    359         visitor = getattr(self, method, self.generic_visit)\r\n--> 360         return visitor(node)\r\n    361 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py in visit_FunctionDef(self, node)\r\n    441     self._enter_scope(False)\r\n--> 442     node.body = self.visit_block(node.body)\r\n    443     anno.setanno(node, NodeAnno.BODY_SCOPE, self.scope)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit_block(self, nodes, before_visit, after_visit)\r\n    370 \r\n--> 371       replacement = self.visit(node)\r\n    372 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit(self, node)\r\n    479     if not anno.hasanno(node, anno.Basic.SKIP_PROCESSING):\r\n--> 480       result = super(Base, self).visit(node)\r\n    481     self.ctx.current_origin = parent_origin\r\n\r\n/usr/lib/python3.8/ast.py in visit(self, node)\r\n    359         visitor = getattr(self, method, self.generic_visit)\r\n--> 360         return visitor(node)\r\n    361 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py in visit_Expr(self, node)\r\n    265   def visit_Expr(self, node):\r\n--> 266     return self._process_statement(node)\r\n    267 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py in _process_statement(self, node)\r\n    259     self._enter_scope(False)\r\n--> 260     node = self.generic_visit(node)\r\n    261     anno.setanno(node, anno.Static.SCOPE, self.scope)\r\n\r\n/usr/lib/python3.8/ast.py in generic_visit(self, node)\r\n    444             elif isinstance(old_value, AST):\r\n--> 445                 new_node = self.visit(old_value)\r\n    446                 if new_node is None:\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit(self, node)\r\n    479     if not anno.hasanno(node, anno.Basic.SKIP_PROCESSING):\r\n--> 480       result = super(Base, self).visit(node)\r\n    481     self.ctx.current_origin = parent_origin\r\n\r\n/usr/lib/python3.8/ast.py in visit(self, node)\r\n    359         visitor = getattr(self, method, self.generic_visit)\r\n--> 360         return visitor(node)\r\n    361 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/static_analysis/activity.py in visit_Call(self, node)\r\n    327     self._enter_scope(False)\r\n--> 328     node.args = self.visit_block(node.args)\r\n    329     node.keywords = self.visit_block(node.keywords)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit_block(self, nodes, before_visit, after_visit)\r\n    370 \r\n--> 371       replacement = self.visit(node)\r\n    372 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/pyct/transformer.py in visit(self, node)\r\n    457                  type(node))\r\n--> 458       raise ValueError(msg)\r\n    459 \r\n\r\nValueError: invalid value for \"node\": expected \"ast.AST\", got \"<class 'NoneType'>\"; to visit lists of nodes, use \"visit_block\" instead\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nOperatorNotAllowedInGraphError            Traceback (most recent call last)\r\n<ipython-input-9-43912bcedaa9> in <module>\r\n      2 startTime = datetime.now()\r\n      3 \r\n----> 4 dream_img = run_deep_dream_simple(img=original_img, \r\n      5                                   steps=100, step_size=0.01)\r\n      6 \r\n\r\n<ipython-input-8-193f882b4182> in run_deep_dream_simple(img, steps, step_size)\r\n     14     step += run_steps\r\n     15 \r\n---> 16     loss, img = deepdream(img, run_steps, tf.constant(step_size))\r\n     17 \r\n     18     display.clear_output(wait=True)\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    501       # This is the first call of __call__, so we have to initialize.\r\n    502       initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 503       self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    504     finally:\r\n    505       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    405     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\r\n    406     self._concrete_stateful_fn = (\r\n--> 407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    408             *args, **kwds))\r\n    409 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2029     arg_names = base_arg_names + missing_arg_names\r\n   2030     graph_function = ConcreteFunction(\r\n-> 2031         func_graph_module.func_graph_from_py_func(\r\n   2032             self._name,\r\n   2033             self._python_function,\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py in bound_method_wrapper(*args, **kwargs)\r\n   2656     # However, the replacer is still responsible for attaching self properly.\r\n   2657     # TODO(mdan): Is it possible to do it here instead?\r\n-> 2658     return wrapped_fn(*args, **kwargs)\r\n   2659   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)\r\n   2660 \r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    894           # TODO(mdan): Push this block higher in tf.function's call stack.\r\n    895           try:\r\n--> 896             return autograph.converted_call(\r\n    897                 original_func,\r\n    898                 autograph.ConversionOptions(\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py in converted_call(f, options, args, kwargs, caller_fn_scope)\r\n    532         ' the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and'\r\n    533         ' attach the full output. Cause: %s', target_entity, e)\r\n--> 534     return _call_unconverted(f, args, kwargs, options)\r\n    535 \r\n    536   with StackTraceMapper(converted_f), tf_stack.CurrentModuleFilter():\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)\r\n    324 \r\n    325   if inspect_utils.istfmethodtarget(f):\r\n--> 326     return f.__self__.call(args, kwargs)\r\n    327 \r\n    328   try:\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/eager/function.py in call(self, args, kwargs)\r\n   2618     if tf_inspect.ismethod(wrapped_fn):\r\n   2619       wrapped_fn = six.get_unbound_function(wrapped_fn)\r\n-> 2620     return wrapped_fn(self.weakrefself_target__(), *args, **kwargs)\r\n   2621 \r\n   2622 \r\n\r\n<ipython-input-6-3ac13775042b> in __call__(self, img, steps, step_size)\r\n     12       print(\"Tracing\")\r\n     13       loss = tf.constant(0.0)\r\n---> 14       for n in tf.range(steps):\r\n     15         with tf.GradientTape() as tape:\r\n     16           # This needs gradients relative to `img`\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py in __iter__(self)\r\n    545   def __iter__(self):\r\n    546     if not context.executing_eagerly():\r\n--> 547       self._disallow_iteration()\r\n    548 \r\n    549     shape = self._shape_tuple()\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py in _disallow_iteration(self)\r\n    538       self._disallow_when_autograph_disabled(\"iterating over `tf.Tensor`\")\r\n    539     elif ag_ctx.control_status_ctx().status == ag_ctx.Status.ENABLED:\r\n--> 540       self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n    541     else:\r\n    542       # Default: V1-style Graph execution.\r\n\r\n/usr/lib/python3.8/site-packages/tensorflow_core/python/framework/ops.py in _disallow_when_autograph_enabled(self, task)\r\n    514 \r\n    515   def _disallow_when_autograph_enabled(self, task):\r\n--> 516     raise errors.OperatorNotAllowedInGraphError(\r\n    517         \"{} is not allowed: AutoGraph did not convert this function. Try\"\r\n    518         \" decorating it directly with @tf.function.\".format(task))\r\n\r\nOperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n```\r\n\r\nThank you!\r\n", "comments": ["@1-willAll-0 I cannot reproduce the issue. I am using most recent `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/8e144b09ad3992b36b798db1ec6e4c46/untitled818.ipynb). Thanks!", "Thanks for the response, jvishnuvardhan.  Perhaps bugs are only relevant to post if I'm using the most updated version?  If so I won't post next time.  I did already mention I myself couldn't reproduce the issue in collab, so maybe that's a non-starter.  \r\n\r\nI don't want to start building now and disrupt the continuity of the managed bundles in Clear Linux, otherwise I'd be using 2.1.0.  As of now I'm on 2.0.0 and I have a workaround which may or may not pan out as I proceed.  I suppose this is because I'm running Python 3.8?", "@1-willAll-0 \r\n\r\n> Thanks for the response, jvishnuvardhan. Perhaps bugs are only relevant to post if I'm using the most updated version? If so I won't post next time. I did already mention I myself couldn't reproduce the issue in collab, so maybe that's a non-starter.\r\n\r\nNo. we encourage you to post issues with older versions also. I just checked with latest version to see whether the issue is still there in the latest version or not. I also checked with `TF2.0` and `TF2.1` and I cannot reproduce the issue as well. I noticed your note about colab.\r\n\r\nLet me check with my laptop and respond to you soon. Thanks! ", "Ah, good to know!  \r\n\r\nThanks for looking into it!", "@1-willAll-0 On My MacBook Pro, I have install 2.0 and ran your code in the gist and I could not see any error with the code. Not sure why you are facing that issue. May be try uninstalling and reinstalling the TF and then check. Thanks!", "I am closing this as this was resolved. Feel free to reopen the issue if this was not resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36643\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36643\">No</a>\n"]}, {"number": 36642, "title": "Throw an error when connecting inputs to weight-shared-embedding-layer more than 5 times.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory with Ubuntu 18.04.3 LTS (Bionic Beaver)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-0-ge5bf8de410 2.1.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen inputs are connected to weight-shared-embedding-layer more than 5 times, tensorflow throws an error while training.\r\n\r\nThis behavior is not seen with below condition.\r\n- connections to weight-shared-embedding-layer are less than 4\r\n- using tensorflow version 1.15\r\n\r\n**Describe the expected behavior**\r\n\r\nSuccessfully train a model using weight-shared-embedding-layer connecting more than 5 inputs.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\n\r\n# changing this to 4 make train success\r\nn_inputs = 5\r\ndim_embedding = 10\r\n\r\ninputs = []\r\nfor i in range(n_inputs):\r\n  inputs.append(tf.keras.Input(shape=(1,), dtype='int32'))\r\n\r\nembeds = []\r\nshared_embed = tf.keras.layers.Embedding(n_input, dim_embedding)\r\nfor i in range(n_inputs):\r\n  embed = shared_embed(inputs[i])\r\n  flatten = tf.keras.layers.Flatten()(embed)\r\n  embeds.append(flatten)\r\n\r\nconcat = tf.keras.layers.Concatenate(axis=1)(embeds)\r\noutput = tf.keras.layers.Dense(1)(concat)\r\nmodel = tf.keras.Model(inputs=[inputs], outputs=[output])\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam', loss='mse')\r\n\r\n# run\r\ndummy_input = []\r\nfor i in range(n_shared_layers):\r\n  dummy_input.append(np.random.randint(0, n_input, size=(1000, )))\r\n\r\ndummy = np.arange(1000).reshape(1000, 1, 1)\r\n\r\nmodel.fit(x=dummy_input, y=dummy, batch_size=32, epochs=1, verbose=1)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nhttps://colab.research.google.com/drive/1NV_KXlKRv148H50SExVYdHkVCh9Acnp_?hl=ja#scrollTo=13C17fSiwhAC\r\n", "comments": ["@tsuzukit Can you please provide a standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan \r\n\r\nAs in original comment, I meant below as minimum standalone code to reproduce the issue.\r\nIf you run below in colab, you should be able to reproduce the issue. \r\nThis is the colab I used. https://colab.research.google.com/drive/1NV_KXlKRv148H50SExVYdHkVCh9Acnp_?hl=ja#scrollTo=13C17fSiwhAC\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\n\r\n# changing this to 4 make train success\r\nn_inputs = 5\r\ndim_embedding = 10\r\n\r\ninputs = []\r\nfor i in range(n_inputs):\r\n  inputs.append(tf.keras.Input(shape=(1,), dtype='int32'))\r\n\r\nembeds = []\r\nshared_embed = tf.keras.layers.Embedding(n_input, dim_embedding)\r\nfor i in range(n_inputs):\r\n  embed = shared_embed(inputs[i])\r\n  flatten = tf.keras.layers.Flatten()(embed)\r\n  embeds.append(flatten)\r\n\r\nconcat = tf.keras.layers.Concatenate(axis=1)(embeds)\r\noutput = tf.keras.layers.Dense(1)(concat)\r\nmodel = tf.keras.Model(inputs=[inputs], outputs=[output])\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam', loss='mse')\r\n\r\n# run\r\ndummy_input = []\r\nfor i in range(n_shared_layers):\r\n  dummy_input.append(np.random.randint(0, n_input, size=(1000, )))\r\n\r\ndummy = np.arange(1000).reshape(1000, 1, 1)\r\n\r\nmodel.fit(x=dummy_input, y=dummy, batch_size=32, epochs=1, verbose=1)\r\n```\r\n\r\nPls let me know if you need any other info!", "@tsuzukit It is throwing the following error. May be it was lost during translation as I had clicked google translation. Thanks!\r\n```\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-6-6b24491b8648> in <module>()\r\n      7 \r\n      8 embeds = []\r\n----> 9 shared_embed = tf.keras.layers.Embedding(n_input, dim_embedding)\r\n     10 for i in range(n_inputs):\r\n     11   embed = shared_embed(inputs[i])\r\n\r\nNameError: name 'n_input' is not defined\r\n```", "@jvishnuvardhan \r\n\r\nAh, sorry there were typos in the code.\r\nAgain, below is complete runnable code along with Colaboratory link that I used.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\n\r\n# changing this to 4 make train success\r\nn_inputs = 5\r\nn_items = 1000\r\ndim_embedding = 10\r\n\r\ninputs = []\r\nfor i in range(n_inputs):\r\n  inputs.append(tf.keras.Input(shape=(1,), dtype='int32'))\r\n\r\nembeds = []\r\nshared_embed = tf.keras.layers.Embedding(n_items, dim_embedding)\r\nfor i in range(n_inputs):\r\n  embed = shared_embed(inputs[i])\r\n  flatten = tf.keras.layers.Flatten()(embed)\r\n  embeds.append(flatten)\r\n\r\nconcat = tf.keras.layers.Concatenate(axis=1)(embeds)\r\noutput = tf.keras.layers.Dense(1)(concat)\r\nmodel = tf.keras.Model(inputs=[inputs], outputs=[output])\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam', loss='mse')\r\n\r\n# run\r\ndummy_inputs = []\r\nfor i in range(n_inputs):\r\n  dummy_inputs.append(np.random.randint(0, n_inputs, size=(n_items, )))\r\n\r\ndummy = np.arange(n_items).reshape(n_items, 1, 1)\r\n\r\nmodel.fit(x=dummy_inputs, y=dummy, batch_size=32, epochs=1, verbose=1)\r\n```\r\n\r\nhttps://colab.research.google.com/drive/1NV_KXlKRv148H50SExVYdHkVCh9Acnp_?hl=ja#scrollTo=JuJ1C645yW3x\r\n\r\nthanks!", "Here is the complete error trace for our reference.\r\n\r\n```\r\nTrain on 1000 samples\r\n  32/1000 [..............................] - ETA: 5s\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/backprop.py in _num_elements(grad)\r\n    626   if isinstance(grad, ops.IndexedSlices):\r\n--> 627     return functools.reduce(operator.mul, grad.values._shape_tuple(), 1)  # pylint: disable=protected-access\r\n    628   raise ValueError(\"`grad` not a Tensor or IndexedSlices.\")\r\n\r\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nSystemError                               Traceback (most recent call last)\r\n23 frames\r\nSystemError: <built-in function len> returned a result with an error set\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nSystemError                               Traceback (most recent call last)\r\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     75       output_gradients,\r\n     76       sources_raw,\r\n---> 77       compat.as_str(unconnected_gradients.value))\r\n\r\nSystemError: PyEval_EvalFrameEx returned a result with an error set\r\n```", "To add some confusion, but maybe some debugging assistance: \r\nI'm also getting this error on v2.1.0-rc2-17-ge5bf8de410 in my personal code. But, I adapted some example code as an attempt to reproduce it and was unable to reproduce the error.\r\n\r\n[Here](https://colab.research.google.com/gist/JoshEZiegler/317daa870ba923512887f5db9c9706de/functional.ipynb) is working embedding sharing code.\r\n\r\n[Here](https://colab.research.google.com/drive/18qodVaGRJHlV0dXvIVGUQm6J5t35VeX5) is slightly changed code from above that doesn't work. \r\n\r\nI can't see a difference between these two, but perhaps the comparison could be helpful?", "After some more debugging it looks like this code will run if `dataset_size` modulo `batch_size` is zero. Hopefully that's helpful! (Source: tried, in the above code, `dataset_size=1280`, `batch_size=32`, and `dataset_size=1000`, `batch_size=10,100` which all worked)", "@tsuzukit Looks like this was resolved in recent `tf-nightly`. Can you please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d399e888688f963a5ce850103e8ef98d/tf_shared_layer.ipynb?hl=ja).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Hi, I've got this error with this nightly build\r\n`  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1094, in fit\r\n    epoch_logs = copy.copy(logs)\r\nUnboundLocalError: local variable 'logs' referenced before assignment`\r\nwhen I was trying to reproduce the issue", "Hi @jvishnuvardhan \r\n\r\nThanks for letting me know.\r\n\r\nI have tested [same code](https://colab.research.google.com/drive/1Ie1HgUz5jxsK5Nfbq8hU9YGykqWnRFN4?hl=ja#scrollTo=apd3gVJzyail) with `v2.2.0-0-g2b96f3662b 2.2.0` build.\r\n\r\n It worked just fine!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36642\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36642\">No</a>\n"]}, {"number": 36641, "title": "[ROCm] CUDA/ROCm shared interface", "body": "This PR adds several macros and typedefs to assist with writing kernels that seamlessly work with both CUDA and ROCm.", "comments": ["gentle ping", "gentle ping", "@ekuznetsov139 Can you please check @cheshire's comments and keep us posted. Thanks!"]}, {"number": 36640, "title": "[ROCm] Create a wrapper header for rocprim and cub", "body": "This PR creates a header that includes either rocprim or cub, depending on the selected build architecture.", "comments": ["@nvining-work ", "gentle ping", "@ekuznetsov139 Can you please resolve conflicts? Thanks!", "Done", "gentle ping"]}, {"number": 36639, "title": "[ROCm]  Grappler unit tests and _FusedConv2D", "body": "This PR:\r\nEnables several grappler unit tests for ROCm\r\nPrevents grappler from fusing convolution with other ops on ROCm (since ROCm does not support fused convolution at this time)", "comments": ["gentle ping", "Sorry for long delay, I approved it long time ago internally, but then lost track of it. It failed to submit because of compilation errors:\r\n\r\n```tensorflow/core/grappler/utils_test.cc:426:20: error: extra tokens at end of #ifdef directive [-Werror,-Wextra-tokens]\r\n#ifdef GOOGLE_CUDA || TENSORFLOW_USE_ROCM```\\\r\n\r\nThis should be `#if defined(...)`", "Actually it should be `#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM` (even before the patch, it should have been `#if GOOGLE_CUDA`: these mistakes are present in a few other places in the code as well.)"]}, {"number": 36638, "title": "Update and rename 00-bug-performance-issue.md to 00-bug-issue.md", "body": "Update and rename 00-bug-performance-issue.md to 00-bug-issue.md", "comments": []}, {"number": 36637, "title": "expm grad can fail on TF2 when eager execution is disabled", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\nEvaluating the gradient of a `tf.linalg.expm` operation can fail with the following exception when eager execution is disabled:\r\n```\r\nNode 'gradients_1/matrix_exponential_1/while_grad/matrix_exponential_1/while_grad': Connecting to invalid output 6 of source node matrix_exponential_1/while which has 6 outputs.\r\n```\r\n\r\nCalling `tf.compat.v1.experimental.output_all_intermediates(True)` at the start of the session fixes the issue.\r\n\r\nThe issue only seems to happen if the gradient op gets added after the `Session` has already evaluated something.\r\n\r\n**Describe the expected behavior**\r\nNo exception should be thrown.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nsession = tf.compat.v1.Session()\r\nx = tf.constant([[1., 0.], [0., -1.]], dtype=tf.float32)\r\ny = tf.linalg.expm(x)\r\nsession.run(y)\r\nsession.run(tf.gradients(y, x))\r\n```\r\n\r\nAlso see gist here: https://colab.research.google.com/drive/1N5KeUBXJAmpict_lofiREHyE1QUcSDUl", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/b2adc534c82c405ca62b378842e531c3/36637.ipynb). Thanks!", "@charmasaur As mentioned in the error, if you make necessary changes this should work.\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\ntf.compat.v1.experimental.output_all_intermediates(True)\r\nsession = tf.compat.v1.Session()\r\nx = tf.constant([[1., 0.], [0., -1.]], dtype=tf.float32)\r\ny = tf.linalg.expm(x)\r\nsession.run(y)\r\nsession.run(tf.gradients(y, x))\r\n```\r\n", "Yup, that fixes the issue for me too.", "Great. I am gonna close this issue then. To understand the cause of this error please read the following [doc](https://www.tensorflow.org/api_docs/python/tf/compat/v1/experimental/output_all_intermediates). Thanks. \r\n\r\nIf you have any more questions please post them on stack overflow. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36637\">No</a>\n", "OK, but presumably the fact that `tf.compat.v1.experimental.output_all_intermediates(True)` is necessary means that there's still something to be fixed here. Surely using that function isn't the intended long-term solution for people who want to calculate gradients of `expm`?\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/experimental/output_all_intermediates also says that if we see this effect we should file an issue, which again suggests to me that there's some underlying issue to fix here.\r\n\r\nedit: I should add that this isn't a problem for me (it was easy enough simply to rework the code to build the entire graph before using the `Session`), but I filed in the issue because I assumed that the error indicated some underlying issue that needs to be fixed."]}, {"number": 36636, "title": "Created new template for performance related issues", "body": "", "comments": ["Just FYI, it looks like this might have broken the issue creation options. See #36721 \r\n"]}, {"number": 36635, "title": "Make callbacks serializable", "body": "Following the suggestion of @fchollet I follow up an old PR of the [keras](https://github.com/keras-team/keras/pull/13001) repository.\r\n\r\n### Summary\r\n\r\nAdd get_config method to callback classes:\r\n\r\nCallbacks as well as layers and optimizers have lots of arguments. The get_config method in the callbacks returns a dictionary with the configuration of the callback allowing us to inspect the parameters of the instance. This allow us to understand better the callbacks and even change them if it is necessary easily.\r\n\r\n### Objectives \r\n\r\n- This method allows to easily obtain the configuration of a callback without knowing the arguments of the callback. \r\n- For the sake of completeness classes such as the `layers`, and `optimizers` have the `get_config` method, thus I think that it will be useful to add this method to callback objects.\r\nAdditionally:\r\n- It is known that keras object are not pickleable, adding a get_config method to optimizers may enable to share different optimizers configuration without the need of sharing the object themselves.\r\n", "comments": ["I recently tried to make sklearn wrappers serializable. I ended up inspecting the constructors instead of hardcoding the parameter names. I'm wondering if a similar approach would work here. This is as much a suggestion as it is input on the robustness of the approach, I wonder if I am missing any edge cases:\r\n\r\n```python3\r\nimport inspect\r\n\r\n\r\nARGS_KWARGS_IDENTIFIERS = (\r\n    inspect.Parameter.VAR_KEYWORD,\r\n    inspect.Parameter.VAR_POSITIONAL,\r\n)\r\n\r\nclass Callback(object):\r\n\r\n    renamed_params = {\r\n        'chief_worker_only': '_chief_worker_only',\r\n        'param1': '_param1',\r\n    }\r\n\r\n    def __init__(self, param1):\r\n        self._param1 = param1\r\n    \r\n    def get_config(self):\r\n        config = dict()\r\n        for class_ in reversed(self.__class__.__mro__):  # invert order so that child class __init__ superseeds parent's\r\n            for p in inspect.signature(class_.__init__).parameters.values():\r\n                if p.kind not in ARGS_KWARGS_IDENTIFIERS and p.name != 'self':\r\n                    attr_name = class_.renamed_params.get(p.name, p.name)\r\n                    config[p.name] = getattr(self, attr_name)\r\n\r\n        # a quick check\r\n        assert None not in config.values()\r\n\r\n        return config\r\n\r\n\r\nclass Callback2(Callback):\r\n\r\n    renamed_params = {\r\n        'param3': '_param3',\r\n    }\r\n\r\n    def __init__(self, param2, param3, **kwargs):\r\n        self.param2 = param2\r\n        self._param3 = param3\r\n        super().__init__(**kwargs)\r\n\r\n\r\nclass CallbackFails(Callback):\r\n    \r\n    def __init__(self, unset_param, **kwargs):\r\n        super().__init__(**kwargs)\r\n\r\n# works\r\nc = Callback2(param1=1, param2=2, param3=3)\r\nprint(c.get_config())\r\n\r\n# fails as expected\r\nc = CallbackFails(param1=1, unset_param=3)\r\nprint(c.get_config())\r\n```\r\n\r\nIn theory, inherited classes just need to edit `renamed_params`.", "@adriangb I don't see any edge cases at first glance, and actually my first attempt of the the PR was implementing something like this.\r\n\r\nHowever after looking how the `get_config` method was implemented in other classes in layers or losses.py I decided to implemented in this way. I don't think that because in the other classes it is implemented like this I must implemented in the same way, but I was sure that after the refactor the functionality was the desired one and I am not missing any edge cases.\r\n\r\nI think that your approach together with a bunch of tests could definitely work in this and the other classes, but it sounds to me more a global refactor and I was looking for a simple (and maybe more robust) implementation to have the functionality.", "That makes sense. Maybe it could be used as a mixin with just the `get_config` and `set_config` methods. Would you mind pointing me in the direction of some of the other implementations of `get_config` you are referencing so I can take a look and see if something like this would work more broadly?", "> That makes sense. Maybe it could be used as a mixin with just the `get_config` and `set_config` methods. Would you mind pointing me in the direction of some of the other implementations of `get_config` you are referencing so I can take a look and see if something like this would work more broadly?\r\n\r\nAbsolutely, regarding the Layers checkout the `Conv` class in `/tensorflow/python/keras/layers/convolutional.py` or any of the classes in `/tensorflow/python/keras/layers/core.py` .\r\nAnd regarding the losses checkout `/tensorflow/python/keras/losses.py`. In this second case it is not that hardcoded.", "@ivanlen Can you please resolve conflicts? Thanks!", "> @ivanlen Can you please resolve conflicts? Thanks!\r\n\r\n@gbaned \r\ndone!", "@ivanlen Can you please resolve conflicts? Thanks!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36635) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36635) for more info**.\n\n<!-- need_author_consent -->", "@gbaned done!", "@ivanlen  Can you please resolve conflicts? Thanks!", "@gbaned, the conflicts are solved.", "@ivanlen Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@gbaned , conflicts solved.", "@ivanlen Can you please resolve conflicts? Thanks!", "@gbaned, done\r\n;)", "This looks good to me, it would be great if keras maintainers could take a look at this.", "This should be moved to keras repo.\r\n\r\nCC @qlzh727 ", "Thanks for the PR. Keras code has been moved to keras-team/keras repository, and the old code in tensorflow/python/keras will be removed soon. Please send your PR to the new repo and we will take it from there. Thanks.", "It's quite unfortunate that this PR sat without review for 1.5 years, and now it is not viable anymore because of changes in repo structures.", "@adriangb it's funny that it is a super simple PR, but it seems that maintainers have other priorities. My original PR was made more than two years ago on [keras repo](https://github.com/keras-team/keras/pull/13001), LOL.\r\nI guess that I will have to make the PR again... and again, and again.", "Very sorry for the bad experience you have had so far. Clearly we need to better to track and review our PR. I can mirror the content from this PR to the keras-team/keras if you don't have any free cycle to do this. Let me know if you are ok with that, or you still would like to contribute.  ", "Thank you for the reply @qlzh727, I will try to upload a new PR to the new repo. If I do not have that time I'll let you know.\r\nCheers."]}, {"number": 36634, "title": "How to create new cuda stream in custom op", "body": "I've written a custom op for the GPU that takes X with shape ```(n,p)``` and Y with shape ```(m,p)``` and returns Z with shape ```(n,m)```. \r\n\r\nIn the backward pass, dX and dY are independent computations so I'd like to have launch two separate CUDA kernels and have them run in separate streams. I know the cuDNN RNN is capable of doing kernels in parallel but I cannot find the source code for how to do this. It seems the only usable stream is provided by the OpKernelContext. \r\n\r\nMatrix multiplication must have a similar parallelism when doing the backward pass but I have not found anywhere in the source making use of multiple streams. This is essentially a reopening of [this issue](https://github.com/tensorflow/tensorflow/issues/6675) as there doesn't appear to a public answer\r\n", "comments": ["@chaaland, Please take a look at [this doc](https://www.tensorflow.org/guide/create_op#gpu_kernels). Thanks! ", "Thanks for the link. I've referenced this doc many times while developing the op but I don't see where it specifies how to generate two CUDA streams. The call to ```d.stream()``` just returns a stream that has already been created rather than creating a new stream (which is what I think I'd need).\r\n\r\nI'm open to any other way to get the dX and dY computations running in parallel as well. Currently I have something like the below which appears to be running in serial\r\n\r\n```c++\r\nconst cudaStream_t& cu_stream = GetCudaStream(ctx);\r\ndx_bprop<<<griddim, blockdim, 0, cu_stream>>>(...);\r\ndy_bprop<<<gridim, blockdim, 0, cu_stream>>>(...);\r\n```", "@gowthamkpr Any updates on this?", "@chaaland Sorry for the delayed response. Can you please take a look at this [doc](https://github.com/tensorflow/custom-op#build-and-test-gpu-op) and let me know if it helps. Thanks!", "@gowthamkpr yes I'm familiar with the custom op repo. My issue is not with being able to compile a GPU operation as I've managed to get that working already. I'd like to know how (or if it's even possible at all) to launch two CUDA kernels in 2 different streams. \r\n\r\nIn CUDA this can be done with a simple malloc of a new stream and passing the string as the 4th argument to the kernel itself. TensorFlow seems to have its own management of memory and streams which I didn't want to interfere with by calling a CUDA malloc directly. The example given in the `time_two_kernels.cu.cc` shows just using a single stream `d.stream()` which is the case I have working currently. I notice there are other streams created for copying host to device, device to device and so on but I don't see the ability to have multiple streams for compute.\r\n\r\nIs this just a fundamental limitation of TensorFlow? I can't find any examples of using multiple compute streams yet I can't think of a reason this shouldn't be possible given CUDA permits it", "@gowthamkpr bump on this issue", "@chaaland  afaik, currently TensorFlow runtime only creates a single compute stream per GPU device. Thought this might change in the future runtime, but I don't think anytime soon. cc: @iganichev ", "@yifeif is right. TensorFlow design today is fairly tied to the \"there is a single compute stream\" assumption. This is usually not a problem because CUDA kernels tend to operate on large enough pieces of data and utilize the whole GPU. Your use case will benefit from parallelism only if your data is too small. In this case, if this op is not a major part of the computation (e.g. done in a tight loop), you might end up saving just a few microseconds.\r\n\r\nIf you still believe that running in parallel will be beneficial enough, there are a couple of things you can try doing:\r\n\r\n- You can try putting both computations into the same kernel. A possible approach is to split your CUDA threads into two logical groups and make threads in group 1 do computation for dX and threads in group 2 do computation for dY.\r\n- A much more involved and likely slower approach is to create another stream in your kernel (you probably want to cache it somewhere), launch your work on both streams, synchronize the new stream to the one used by TensorFlow to make sure that the TensorFlow's stream will not proceed until all work on your stream is done.\r\n\r\nAlso, as @yifeif mentioned, this limitation is gone in the new TensorFlow runtime (https://www.youtube.com/watch?v=15tiQoPpuZ8) but it will be a while before the new runtime is the default one.", "@iganichev  Sounds like fusing the Op's backprop functor is the most straightforward approach. I was under the impression this would provide a perf benefit since as the thread blocks computing dX finish, threadblocks for dY could be scheduled. In my current implementation, _all_ of the thread blocks for dX must finish before any work on dY can be done. Will just have to test and see if it makes any difference. Thanks for your help!"]}, {"number": 36633, "title": "Fix concatenation_test", "body": "Commit a2aa5e3f045a5916b20a63b58f824ed59710845a broke `ROCm` CI build. The member variable should be initialized in a constructor.\r\n\r\n/cc @whchung @deven-amd ", "comments": ["@jerryyin \r\n\r\nshould have given you a heads up on this one.\r\n\r\nI filed a upstream PR last week for this issue : https://github.com/tensorflow/tensorflow/pull/36558\r\n\r\nif only the TF guys were prompt in their PR handling, it would have never made it downstream to us!", "Closing the PR as a duplicate of #36558\r\n"]}, {"number": 36632, "title": "Able to disable auto-insert quantize de-quantize node to quantized model.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.15.2, 2.1.0\r\n\r\n\r\nTFLiteConverter will automatically insert quantize and dequantize node to quantized model, so that input and output of the quantized model are float32. (check the code, and model example graph below)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef gen_calibration_dataset():\r\n    for _ in range(10):\r\n        yield [np.random.rand(1,2,2,2).astype(np.float32)]\r\n\r\ndef get_keras_model_conv():\r\n    input_0 = tf.keras.layers.Input(shape=[2, 2, 2])\r\n    conv_0 = tf.keras.layers.Conv2D(filters=2, kernel_size=(2, 2),\r\n                                    activation=tf.nn.relu)(input_0)\r\n    model = tf.keras.models.Model(inputs=[input_0], outputs=[conv_0])\r\n    model.summary()\r\n    return model\r\n\r\ndef gen_model():\r\n\r\n    keras_model = get_keras_model_conv()\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.representative_dataset = gen_calibration_dataset\r\n    tflite_quant_model = converter.convert()\r\n    open('test.tflite', 'wb').write(tflite_quant_model)\r\n\r\nprint(\"TF version: {}\".format(tf.__version__))\r\ngen_model()\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/55463253/74179096-ac3e5d00-4c34-11ea-8f92-6eb1015a1f16.png)\r\n\r\nIs there any convenient way (through python api) to force the model input,output remain int8 (not auto insert quantize, dequantize node)? I didn't find official doc about this.\r\nIf not, is it reasonable to add such feature to TFLiteConverter?", "comments": ["TfLiteConverterV1 had this option (using inference input/output type attributes) but these seem to have been removed in TfLiteConverterV2?", "we find on TFLiteConverter V1, we can still got fully quantized model.\r\ncheck code and model graph below:\r\n```python \r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef gen_calibration_dataset():\r\n    for _ in range(10):\r\n        yield [np.random.rand(1,2,2,2).astype(np.float32)]\r\ndef get_keras_model_conv():\r\n    input_0 = tf.keras.layers.Input(shape=[2, 2, 2])\r\n    conv_0 = tf.keras.layers.Conv2D(filters=2, kernel_size=(2, 2),\r\n                                    activation=tf.nn.relu)(input_0)\r\n    model = tf.keras.models.Model(inputs=[input_0], outputs=[conv_0])\r\n    model.summary()\r\n    return model\r\ndef gen_model():\r\n    keras_model = get_keras_model_conv()\r\n    keras_model.save('test_model.h5')\r\n    converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file('test_model.h5')\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n    converter.representative_dataset = gen_calibration_dataset\r\n    tflite_quant_model = converter.convert()\r\n    open('test.tflite', 'wb').write(tflite_quant_model)\r\nprint(\"TF version: {}\".format(tf.__version__))\r\ngen_model()\r\n```\r\n![image](https://user-images.githubusercontent.com/55463253/74254238-149a4680-4ce8-11ea-8c7b-55526495596d.png)\r\n", "@suharshs, is this the intended behaviour of the TF2.x API? It seems it would be useful to retain the option of not having the model wrapped into Quantize/Dequantize nodes.", "+1 on adding the `inference_input_type` and `inference_type` back in the v2 converter. ", "With latest TensorFlow, this works for INT8/UINT8 models:\r\n`bazel run //tensorflow/lite/tools/optimize:modify_model_interface_main /path-to-semi-quantized-model/model.tflite /path-to-save-fully-quantized-model/model.tflite int8 int8`", "With latest TensorFlow, it works for INT8/UINT8 model:\r\n```\r\n...\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\n```\r\n\r\ncheck this:\r\nhttps://www.tensorflow.org/lite/performance/post_training_quantization#integer_only"]}, {"number": 36631, "title": "Bug in LSTMBlockCell gpu implementation", "body": "Fixed bug in cuda kernel of LSTMBlockCell, also implemented test which checks LSTMCell and LSTMBlockCell compatibility on cpu and gpu.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36631) for more info**.\n\n<!-- need_author_consent -->", "This is not a backwards-compatible change. Also we're not releasing new 1.15 versions, just patch releases with security fixes, so please rebase this PR onto master."]}, {"number": 36630, "title": "Intel MKL DNN: Addng support of Conv backward for DNN 1.0", "body": "Adding support of Conv Backward for DNN 1.0", "comments": ["I have added my resolved comments. Please let me know if they are ok, then I will add the TODO comment in the corresponding piece of code."]}, {"number": 36629, "title": "Support deeplab v3 model on dsp delegate", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.15\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs tflite can support deeplabv3 on GPU with good performance as shown in:\r\nhttps://www.tensorflow.org/lite/models/segmentation/overview\r\n\r\nBut when ran the deeplab v3 model on dsp delegate, found it's very slow and the logs showed the upsupported ops on dsp delegate. @karimnosseir , do you think it's possible to fully support deeplab v3 on dsp delegate:\r\nModel file: https://github.com/tensorflow/tensorflow/files/4182220/deep_lab_v3_mv2.tflite.zip\r\nConverted and quantized from http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainaug_2018_10_01.tar.gz\r\n\r\nHere are the logs:\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for Hexagon.\r\nINFO: Hexagon delegate: unsupported node index: 25.\r\n\r\nINFO: Hexagon delegate: unsupported node index: 27.\r\n\r\nINFO: Hexagon delegate: unsupported node index: 33.\r\n\r\nINFO: Hexagon delegate: unsupported node index: 35.\r\n\r\nINFO: Hexagon delegate: 32 nodes delegated out of 111 nodes.\r\n\r\nINFO: Replacing 32 node(s) with delegate (TfLiteHexagonDelegate) node.\r\n\r\n\r\n**Will this change the current api? How?** No.\r\n\r\n**Who will benefit with this feature?** Developer who is using deeplab v3.\r\n\r\n**Any Other info.**\r\n", "comments": ["https://developer.qualcomm.com/project/image-segmentation-using-deeplabv3", "Thanks for the request.\r\nI think the main thing missing are SpacetoBatchND and BatchToSpaceND. We will look in adding them.\r\n\r\nI am not sure what are your expectations, but just to let you know the model you attached when i ran on \r\n Samsung S9 i see ~368ms when run on CPU (4 threads)", "@karimnosseir , Thanks for the support, this model is an example, is it reasonable to expect 3X+ faster on DSP than CPU? Looking forward to trying it out once you can add the support of two missing Ops. Thanks.", "@karimnosseir Are you sure? Check https://github.com/tensorflow/tensorflow/issues/29509", "@bhack The model attached by @dlaiup has the SpaceToBatch/BatchToSpace, and my reply is about it, i am assuming the conversion is done not using the new converter.\r\n@dlaiup can you reconvert the model using the new converter and attach the model (set experimental_new_converter=True)\r\n\r\nThanks", "@karimnosseir Sure, this is the new model with experimental_new_converter, please check if that's what you want?\r\nhttps://github.com/tensorflow/tensorflow/files/4189351/deep_lab_v3_mv2_new.zip", "@dlaiup I think you set some config incorrectly, the model you are sending has all ops expecting float not uint8.\r\n\r\nCan you double check please.\r\n\r\nThanks", "@karimnosseir , Add the two tfllite deeplab_v3 models converted with and without experimental_new_converter:\r\n1. Without experimental_new_converter:\r\nhttps://github.com/tensorflow/tensorflow/files/4194569/deeplab_v3_quant.zip\r\n2. With experimental_new_converter:\r\nhttps://github.com/tensorflow/tensorflow/files/4194570/deeplab_v3_quant_new.zip\r\n\r\nPlease check if they are what you expected. Thanks.", "@dialup Are you using the nightly version? See https://github.com/tensorflow/tensorflow/issues/29509#issuecomment-580451630", "@bhack , I am using tensorflow 2.1.0 version.", "@karimnosseir , using the correct convert configuration (up to ResizeBilinear_1 layer), the deeplab v3 model is running ok now. Will close this issue. Thanks. ", "@bhack Yes, the SpaceToBatchND-Conv2D-BatchToSpaceND can be converted to a single Conv2D, thanks.", "Glad it is working now.\r\nDo you mind sharing the newly converted model\r\n\r\nThanks", "@dlaiup There was some changes recently in resize bilinear handling. Check https://github.com/tensorflow/tensorflow/issues/33691.\r\n", "@karimnosseir , Sure, here is the converted model. Thanks.\r\nhttps://github.com/tensorflow/tensorflow/files/4199909/deeplab_v3_quant_new.zip", "@bhack  Thanks for pointing out. I think that change is mainly for CPU kernels.", "Thanks for sharing. The new is faster on CPU and can also be fully delegated on Hexagon DSP.\r\nJust tried it ~2x faster than CPU when run on hexagon", "@karimnosseir Yes, it can be fully deployed on DSP. Thanks for the support."]}, {"number": 36628, "title": "Updated an example for reduce_euclidean_norm", "body": "Updated an incorrect example as reported in https://github.com/tensorflow/tensorflow/issues/36554", "comments": ["This should first land on `master` and then be cherry-picked into the branch. At the moment, master has the wrong https://github.com/tensorflow/tensorflow/blob/74ffcb5a07d281d2adb7fec1974d888e783ec5af/tensorflow/python/ops/math_ops.py#L1770-L1777", "@mihaimaruseac Thanks. I thought I opened against `master` but I was wrong. Closing this issue and opening new PR against `master` https://github.com/tensorflow/tensorflow/pull/36670. Thanks!"]}, {"number": 36627, "title": "GPU memory not released until Java process terminates", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04.4 LTS, Kernel 4.15.0-76-generic\r\n\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n\r\n- TensorFlow version (use command below):\r\n1.15.0\r\n\r\n- Python version:\r\nPython 3.6.9\r\n\r\n- CUDA/cuDNN version:\r\n10.0.130\r\n\r\n- GPU model and memory:\r\nGeForce GTX 1080 Ti, 11177MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAfter closing all Tensors, Graphs and Sessions in our Java programm, the Java process still holds the previously used GPU memory until the Java process terminates.\r\n\r\n**Describe the expected behavior**\r\nAfter closing all Tensors, Graphs and Sessions, the Java process should release all allocated GPU memory.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```java\r\npackage de.tensorflowtest;\r\n\r\nimport static org.apache.commons.io.IOUtils.toByteArray;\r\nimport java.io.Console;\r\nimport java.io.IOException;\r\nimport java.io.InputStream;\r\nimport org.tensorflow.Graph;\r\nimport org.tensorflow.Session;\r\nimport org.tensorflow.framework.ConfigProto;\r\nimport de.tensorflowtest.Constants;\r\n\r\npublic class GpuLeakDebug {\r\n    public static void main(String[] args) throws IOException, InterruptedException {\r\n\r\n        waitWithMessage(\"Create Session\");\r\n\r\n        Session session = null;\r\n        Graph graph = null;\r\n        ConfigProto sessionConfig;\r\n        try {\r\n            byte[] graphdef = null;\r\n            try (InputStream graphStream = Constants.class.getResourceAsStream(\"/tensorflow/inception_v3.pb\")) {\r\n                graphdef = toByteArray(graphStream);\r\n            } catch (IOException e) {\r\n                System.exit(1);\r\n            }\r\n            graph = new Graph();\r\n            graph.importGraphDef(graphdef);\r\n            sessionConfig = ConfigProto.newBuilder().build();\r\n            session = new Session(graph, sessionConfig.toByteArray());\r\n\r\n        } catch (UnsatisfiedLinkError e) {\r\n\r\n            throw e;\r\n        } finally {\r\n            waitWithMessage(\"Close Session\");\r\n            session.close();\r\n            graph.close();\r\n        }\r\n        session = null;\r\n        graph = null;\r\n        sessionConfig = null;\r\n\r\n        waitWithMessage(\"Terminate\");\r\n    }\r\n\r\n    private static void waitWithMessage(String message, Object... args) {\r\n        Console c = System.console();\r\n        if (c != null) {\r\n            // printf-like arguments\r\n            if (message != null)\r\n                c.format(message, args);\r\n            c.format(\" Press ENTER to proceed.\\n\");\r\n            c.readLine();\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n**Other info / logs**\r\nThis is the output of `nvidia-smi` **before the session is created** (\"Create Session\") and **after the JVM terminates** (\"Terminate\").\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |\r\n| 25%   57C    P5    30W / 250W |      0MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nThis is the output of `nvidia-smi` **after the session has been created** and **after the session has been closed**.\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |\r\n| 19%   53C    P5    24W / 250W |    145MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     20221      C   /usr/bin/java                                135MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\n", "comments": ["@TheSentry By default TensorFlow allocates GPU memory for the lifetime of the process, not the lifetime of the session object. More details at: https://www.tensorflow.org/programmers_guide/using_gpu#allowing_gpu_memory_growth\r\n\r\nThus, if you want memory to be freed, you'll have to exit the Java interpreter, not just close the session.\r\n\r\nFor more info, you can refer to the following [issue](https://github.com/tensorflow/tensorflow/issues/17048). ", "@gowthamkpr Thank you for your response.\r\n\r\n> @TheSentry By default TensorFlow allocates GPU memory for the lifetime of the process, not the lifetime of the session object. More details at: https://www.tensorflow.org/programmers_guide/using_gpu#allowing_gpu_memory_growth\r\n\r\nUnfortunately, the text in this link doesn't state this as clearly as you did just now. I've interpreted \"Note we do not release memory, since it can lead to memory fragmentation.\" as session-bound, not process-bound.\r\n\r\n> Thus, if you want memory to be freed, you'll have to exit the Java interpreter, not just close the session.\r\n\r\nThis is unfortunate as our process is a Tomcat webserver, which is long-running.\r\n\r\n> For more info, you can refer to the following [issue](https://github.com/tensorflow/tensorflow/issues/17048).\r\n\r\nThank you for the link to that issue. I had hoped that this was an old issue and had changed by now, but this [other issue](https://github.com/tensorflow/tensorflow/issues/19731) also points to the conclusion that there is still no way to release GPU memory except terminating the process.\r\n\r\nIs there a way to turn this into a feature request, if it doesn't already exist?", "Yes. I can leave it open. As mentioned [here](https://github.com/tensorflow/tensorflow/issues/19731#issuecomment-556215327) pytorch has a way to clear it and is  being requested by many users.\r\n\r\n@sanjoy Can you PTAL?", "> Is there a way to turn this into a feature request, if it doesn't already exist?\r\n\r\nThis GH issue can serve a feature request.  We don't have anyone working on this in Q1 though.", "> @gowthamkpr Thank you for your response.\r\n> \r\n> > @TheSentry By default TensorFlow allocates GPU memory for the lifetime of the process, not the lifetime of the session object. More details at: https://www.tensorflow.org/programmers_guide/using_gpu#allowing_gpu_memory_growth\r\n> \r\n> Unfortunately, the text in this link doesn't state this as clearly as you did just now. I've interpreted \"Note we do not release memory, since it can lead to memory fragmentation.\" as session-bound, not process-bound.\r\n> \r\n> > Thus, if you want memory to be freed, you'll have to exit the Java interpreter, not just close the session.\r\n> \r\n> This is unfortunate as our process is a Tomcat webserver, which is long-running.\r\n> \r\n> > For more info, you can refer to the following [issue](https://github.com/tensorflow/tensorflow/issues/17048).\r\n> \r\n> Thank you for the link to that issue. I had hoped that this was an old issue and had changed by now, but this [other issue](https://github.com/tensorflow/tensorflow/issues/19731) also points to the conclusion that there is still no way to release GPU memory except terminating the process.\r\n> \r\n> Is there a way to turn this into a feature request, if it doesn't already exist?\r\n\r\n@TheSentry How about creating another sub-process to run Tensorflow instead of using the same process where Tomcat is running. The Tf-heap memory will be released back to CUDA as soon as the sub-process is killed. ", "> @TheSentry How about creating another sub-process to run Tensorflow instead of using the same process where Tomcat is running. The Tf-heap memory will be released back to CUDA as soon as the sub-process is killed.\r\n\r\n@akshayrana30 The is not feasible for us. It would require too much work to extract the TF part of our code and establish the proper inter-process communication, not to mention the impact on processing speed, which is very important in our system.\r\n\r\nRight now we mitigated this problem by having no other process on this machine that needs CUDA", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 36625, "title": "[ROCm] Fix for a test failure on the ROCm platform - 200210 - 1", "body": "The test `//tensorflow/python/distribute:mirrored_strategy_test_gpu ` currently fails on the ROCm platform.\r\nIt has one failing subtest ( `testFuctionPreservesAutoGraph (__main__.FunctionTest)` ), which fails with the following error\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/mirrored_strategy_test_gpu.runfiles/org_tensorflow/tensorflow/python/distribute/mirrored_strategy_test.py\", line 1369, in testFuctionPreservesAutoGraph\r\n    [context.LogicalDeviceConfiguration()] * 2)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/mirrored_strategy_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/config.py\", line 609, in set_logical_device_configuration\r\n    context.context().set_logical_device_configuration(device, logical_devices)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/mirrored_strategy_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/context.py\", line 1326, in set_logical_device_configuration\r\n    \"Virtual devices cannot be modified after being initialized\")\r\nRuntimeError: Virtual devices cannot be modified after being initialized\r\n```\r\n\r\nThe failure does not seem to ROCm specific, and the error seems legit, so am not sure how to go about \"fixing\" it.\r\n\r\nSkipping the subtest on the ROCm platform for now.\r\n\r\n-------------------\r\n\r\n/cc @whchung @chsigg ", "comments": ["@deven-amd Can you please resolve conflicts? Thanks!", "closing out this PR, as the commit that introduces the merge-conflict ( https://github.com/tensorflow/tensorflow/commit/b5083df1727a91179008d294263f1334158bbe8b ) also fixes the underlying issue being worked-around by this PR "]}, {"number": 36624, "title": "LSTM return_state=True fail with tf.keras.Sequencial model", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\nThe `call` method of a `tf.keras.Sequential` object fails and throws an error when one layer is an instance of the `tf.keras.layers.LSTM` class constructed with `return_state=True`. Given the error message, I believe it is because the output of the `call` method of such `LSTM` layer is a `list` instead of a `Tensor`, and the `call` method of `Sequential` does not know what to do with a `list`.\r\n\r\n**Describe the expected behavior**\r\n\r\nI think that the `call` method of `Sequential` should know that the `Tensor` output of `LSTM` is the first element of the `list` when `return_state=True`.\r\n\r\n**Code to reproduce the issue**\r\nSetting : \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\nbatch_size = 3\r\nts = 9\r\ninput_dim = 2\r\nnump = np.arange(examples*batch_size*ts*input_dim, dtype=np.float32).reshape(batch_size, ts, input_dim)\r\ndataset = tf.data.Dataset.from_tensor_slices(nump).batch(batch_size)\r\nfor x in dataset:\r\n    print(x.shape)\r\nreturn_state = True\r\n```\r\nOutput:\r\n```\r\nUsing Tensorflow version 2.1.0 (git version v2.1.0-rc2-17-ge5bf8de410)\r\n(3, 9, 2)\r\n```\r\n\r\nError with `Sequential`:\r\n```\r\nmodel_seq = tf.keras.Sequential([tf.keras.layers.LSTM(3, return_state=return_state)])\r\nfor x in dataset:\r\n    print(model_seq(x))\r\n```\r\nOutput:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-57-5500870ab2fc> in <module>\r\n      1 model_seq = tf.keras.Sequential([tf.keras.layers.LSTM(3, return_state=return_state)])\r\n      2 for x in dataset:\r\n----> 3     print(model_seq(x))\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    820           with base_layer_utils.autocast_context_manager(\r\n    821               self._compute_dtype):\r\n--> 822             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    823           self._handle_activity_regularization(inputs, outputs)\r\n    824           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py in call(self, inputs, training, mask)\r\n    283       # `outputs` will be the inputs to the next layer.\r\n    284       inputs = outputs\r\n--> 285       mask = outputs._keras_mask\r\n    286 \r\n    287     return outputs\r\n\r\nAttributeError: 'list' object has no attribute '_keras_mask'\r\n```\r\n\r\nIt works when constructing the model with the Functional API:\r\n```\r\ndef lstm_model(return_state, ts, input_dim):\r\n    inp = tf.keras.Input(shape=(ts, input_dim))\r\n    out = tf.keras.layers.LSTM(3, return_state=return_state)(inp)\r\n    return tf.keras.Model(inputs=inp, outputs=out)\r\n    \r\nmodel_func = lstm_model(return_state, ts, input_dim)\r\n\r\nfor x in dataset:\r\n    print(model_func(x))\r\n```\r\nOutput:\r\n```\r\n[<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\r\narray([[-8.8475537e-01,  2.9517543e-03, -9.9753261e-01],\r\n       [-9.7553629e-01,  9.5521700e-06, -9.9959475e-01],\r\n       [-9.9497062e-01,  3.0903845e-08, -9.9979442e-01]], dtype=float32)>, <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\r\narray([[-8.8475537e-01,  2.9517543e-03, -9.9753261e-01],\r\n       [-9.7553629e-01,  9.5521700e-06, -9.9959475e-01],\r\n       [-9.9497062e-01,  3.0903845e-08, -9.9979442e-01]], dtype=float32)>, <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\r\narray([[-7.6066346e+00,  2.9581292e-03, -3.3488092e+00],\r\n       [-8.9999275e+00,  9.5521846e-06, -4.2520967e+00],\r\n       [-9.0000000e+00,  3.0903848e-08, -4.5915442e+00]], dtype=float32)>]\r\n```\r\n\r\n**Related question**\r\nIn my Functional API example, `lstm_model`fails if I use `inp = tf.keras.Input(shape=(ts, None))` instead of providing the explicit input dimension. The error message I get is:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-64-9b042ffca48d> in <module>\r\n      4     return tf.keras.Model(inputs=inp, outputs=out)\r\n      5 \r\n----> 6 model_func = lstm_model(return_state, ts, input_dim)\r\n      7 \r\n      8 for x in dataset:\r\n\r\n<ipython-input-64-9b042ffca48d> in lstm_model(return_state, ts, input_dim)\r\n      1 def lstm_model(return_state, ts, input_dim):\r\n      2     inp = tf.keras.Input(shape=(ts, None))\r\n----> 3     out = tf.keras.layers.LSTM(3, return_state=return_state)(inp)\r\n      4     return tf.keras.Model(inputs=inp, outputs=out)\r\n      5 \r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)\r\n    642 \r\n    643     if initial_state is None and constants is None:\r\n--> 644       return super(RNN, self).__call__(inputs, **kwargs)\r\n    645 \r\n    646     # If any of `initial_state` or `constants` are specified and are Keras\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    746           # Build layer if applicable (if the `build` method has been\r\n    747           # overridden).\r\n--> 748           self._maybe_build(inputs)\r\n    749           cast_inputs = self._maybe_cast_inputs(inputs)\r\n    750 \r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n   2114         # operations.\r\n   2115         with tf_utils.maybe_init_scope(self):\r\n-> 2116           self.build(input_shapes)\r\n   2117       # We must set self.built since user defined build functions are not\r\n   2118       # constrained to set self.built.\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py in build(self, input_shape)\r\n    562     if isinstance(self.cell, Layer):\r\n    563       if not self.cell.built:\r\n--> 564         self.cell.build(step_input_shape)\r\n    565 \r\n    566     # set or validate state_spec\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in wrapper(instance, input_shape)\r\n    304     if input_shape is not None:\r\n    305       input_shape = convert_shapes(input_shape, to_tuples=True)\r\n--> 306     output_shape = fn(instance, input_shape)\r\n    307     # Return shapes from `fn` as TensorShapes.\r\n    308     if output_shape is not None:\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py in build(self, input_shape)\r\n   2299         regularizer=self.kernel_regularizer,\r\n   2300         constraint=self.kernel_constraint,\r\n-> 2301         caching_device=default_caching_device)\r\n   2302     self.recurrent_kernel = self.add_weight(\r\n   2303         shape=(self.units, self.units * 4),\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\r\n    444         synchronization=synchronization,\r\n    445         aggregation=aggregation,\r\n--> 446         caching_device=caching_device)\r\n    447     backend.track_variable(variable)\r\n    448 \r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\r\n    742         dtype=dtype,\r\n    743         initializer=initializer,\r\n--> 744         **kwargs_for_getter)\r\n    745 \r\n    746     # If we set an initializer and the variable processed it, tracking will not\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\r\n    140       synchronization=synchronization,\r\n    141       aggregation=aggregation,\r\n--> 142       shape=variable_shape if variable_shape else None)\r\n    143 \r\n    144 \r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    256   def __call__(cls, *args, **kwargs):\r\n    257     if cls is VariableV1:\r\n--> 258       return cls._variable_v1_call(*args, **kwargs)\r\n    259     elif cls is Variable:\r\n    260       return cls._variable_v2_call(*args, **kwargs)\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/ops/variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\r\n    217         synchronization=synchronization,\r\n    218         aggregation=aggregation,\r\n--> 219         shape=shape)\r\n    220 \r\n    221   def _variable_v2_call(cls,\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/ops/variables.py in <lambda>(**kwargs)\r\n    195                         shape=None):\r\n    196     \"\"\"Call on Variable class. Useful to force the signature.\"\"\"\r\n--> 197     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n    198     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n    199       previous_getter = _make_getter(getter, previous_getter)\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)\r\n   2594         synchronization=synchronization,\r\n   2595         aggregation=aggregation,\r\n-> 2596         shape=shape)\r\n   2597   else:\r\n   2598     return variables.RefVariable(\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    260       return cls._variable_v2_call(*args, **kwargs)\r\n    261     else:\r\n--> 262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    263 \r\n    264 \r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\r\n   1409           aggregation=aggregation,\r\n   1410           shape=shape,\r\n-> 1411           distribute_strategy=distribute_strategy)\r\n   1412 \r\n   1413   def _init_from_args(self,\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\r\n   1540           with ops.name_scope(\"Initializer\"), device_context_manager(None):\r\n   1541             initial_value = ops.convert_to_tensor(\r\n-> 1542                 initial_value() if init_from_fn else initial_value,\r\n   1543                 name=\"initial_value\", dtype=dtype)\r\n   1544           if shape is not None:\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in <lambda>()\r\n    120           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\r\n    121         initializer = initializer()\r\n--> 122       init_val = lambda: initializer(shape, dtype=dtype)\r\n    123       variable_dtype = dtype.base_dtype\r\n    124   if use_resource is None:\r\n\r\n~/path/to/python3.6/site-packages/tensorflow_core/python/ops/init_ops_v2.py in __call__(self, shape, dtype)\r\n    413       scale /= max(1., fan_out)\r\n    414     else:\r\n--> 415       scale /= max(1., (fan_in + fan_out) / 2.)\r\n    416     if self.distribution == \"truncated_normal\":\r\n    417       # constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\r\n\r\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\r\n```\r\nIs it normal? If so, why is that?\r\n", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/895c54df8a066fec5cfd7fdf27fc431a/36624.ipynb). Thanks!", "So for Sequential model, we expect the layer within it only have one input and one output. The LSTM layer with return_states=True will cause it to return more than 1 output, which violate this rule.\r\n\r\nI think the sequential model code need to be updated to show more explicit error for this case. We already show it if your model has the input_shape (which trigger model build under the hood), but we missed it in the deferred build case (input_shape is not provided by layers, but inferred when actual input is provided).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36624\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36624\">No</a>\n"]}, {"number": 36623, "title": "Unimplemented: Cast string to float is not supported", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1909\r\n- TensorFlow version (use command below): 2.1.0-gpu-version\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1.243/7.6.5\r\n- GPU model and memory: GTX 1660 ti 6GB and 32GB (available 28GB)\r\n\r\n**Describe the current behavior**\r\nI am working on nlp-getting-started compitetion from kaggle and written code but when executing I'm getting this issue and it doesn't seems to go furthuer or it is not even showing error and exiting it is just showing this and waiting...\r\nHere is the snap of the error\r\n![1](https://user-images.githubusercontent.com/44919399/74157450-4eaf0e00-4c3e-11ea-8a26-7ea1947a5d5b.jpg)\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nimport re\r\nimport nltk\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.preprocessing.text import Tokenizer\r\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\n\r\nvocab_size = 10000\r\nbatch_size = 32\r\nepochs = 4\r\nmax_len = 25\r\n\r\ntrain_df = pd.read_csv('train.csv')\r\ntest_df = pd.read_csv('test.csv')\r\n\r\ndef clean_text(text):\r\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\r\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\r\n    text = text.lower()\r\n    text = text.split()\r\n    text = [word for word in text if not word in set(nltk.corpus.stopwords.words('english'))]\r\n    text = ' '.join(text)\r\n    return text\r\n\r\ntrain_df['text'] = train_df['text'].apply(lambda x: clean_text(x))\r\nx_train, y_train = train_df.iloc[:, 3].values, train_df.iloc[:, 4].values\r\n\r\ntokenizer = Tokenizer(num_words=vocab_size)\r\n\r\ntokenizer.fit_on_texts(x_train)\r\nsequences = tokenizer.texts_to_sequences(x_train)\r\nx_train_new = pad_sequences(sequences, maxlen=max_len)\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(vocab_size, 128))\r\nmodel.add(LSTM(128, dropout=0.2))\r\nmodel.add(Dense(32, activation='relu'))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy', optimizer = 'RMSprop', metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose = 1)\r\n\r\nAnd also when I press ctrl+c then it gets this message\r\n![2](https://user-images.githubusercontent.com/44919399/74158187-a13cfa00-4c3f-11ea-810b-694e53158c47.jpg)\r\n\r\nThanks in advance. Please help me find the error and correct it.\r\n", "comments": ["Sorry this is the dumbest question I ever posted. The solution is this:\r\n\r\nmodel.fit(x_train_new, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\r\n\r\nhave you observed it. I didn't used padded sequence as input. I must use it and padded input is x_train_new not x_train. Ah! I hate Myself for this.", "> Sorry this is the dumbest question I ever posted. The solution is this:\r\n> \r\n> model.fit(x_train_new, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\r\n> \r\n> have you observed it. I didn't used padded sequence as input. I must use it and padded input is x_train_new not x_train. Ah! I hate Myself for this.\r\n\r\nIm also getting similar error. Is it like the training data sequence and the label sequence length in Y_train should be same?\r\nUnimplementedError: 2 root error(s) found.\r\n  (0) Unimplemented:  Cast string to float is not supported\r\n\t [[node CTC-MODEL/Cast (defined at <ipython-input-17-1bb769e8dc7d>:2) ]]\r\n  (1) Cancelled:  Function was cancelled before it was started", "For me, Y_train is not a sequence. If you have Y_train as a sequence, then also why would you get this error. Just apply tokenizer and pad_sequences to Y_train data too. That's should solve it. If it is not sequenced data, then you don't need to but carefully check or send me the required information to reproduce your code error. ", "My input is Sign Video and the output is labels corresponding to the signs in the video. Eg.  label sequence could be COMING HOME FRIEND MY.  What Im trying to do is put all the required frames to a folder and read N (20) frames and the corresponding tokenized label sequence (max length is 12, so all are padded to make it 12) written to a txt file, at a time. My doubt is the input video has 20 frames. But the padded label sequence length is just 12. How this difference is handled in the code?? Do v need to do some extra formatting with the labels?? I have shared the colab file and dataset with you, check gmail, to reproduce the error. \r\n", "okay, I sorry I saw it late. But I will get to it. Maybe it takes some time, so be patient I will contact you if done."]}, {"number": 36622, "title": "tensorflow-gpu 2.0.0 error | Non-OK-status: GpuLaunchKernel | status: Internal: invalid device function", "body": "```Using TensorFlow backend.\r\n2020-02-10 18:56:03.997155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll<br>\r\n2020-02-10 18:56:06.902771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll<br>\r\n2020-02-10 18:56:07.849352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:<br>\r\nname: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56\r\npciBusID: 0000:01:00.0<br>\r\n2020-02-10 18:56:07.855482: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-02-10 18:56:07.860526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0<br>\r\n2020-02-10 18:56:07.863861: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-02-10 18:56:07.880373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:<br>\r\nname: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56<br>\r\npciBusID: 0000:01:00.0<br>\r\n2020-02-10 18:56:07.886459: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-02-10 18:56:07.891278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0<br>\r\n2020-02-10 18:56:08.001331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:<br>\r\n2020-02-10 18:56:08.005924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0<br>\r\n2020-02-10 18:56:08.008184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N<br>\r\n2020-02-10 18:56:08.011837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3023 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)<br>\r\n2020-02-10 18:56:08.154262: F .\\tensorflow/core/kernels/random_op_gpu.h:227] <b>Non-OK-status: <br>GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: invalid device function</b>\r\n```\r\nI am getting this error while running keras LSTM code. The code is correct. <br>\r\nBelieve me I have all possible configurations to install tensorflow-gpu but it is not working.\r\ni installed this one by: <br>\r\nmaking new env<br>\r\n`conda install -c conda-forge tensorflow-gpu` <br>\r\n\r\ni have also tried by installing Cudnn and CUDA toolkit seperately but same thing appears. <br>\r\nCPU : i7 9th gen<br>\r\nGPU : GTX 1650<br>\r\nLAPTOP : ASUS G STRIX g531GT 2019<br>\r\n\r\nSomeone please help me, i have wasted 5-10 hours trying every possible configuration.", "comments": ["@himanshugullaiya \r\nCan you please provide the sequence of steps or a minimal standalone code to reproduce this issue and expedite the process. Thanks!", "Thanks ravikyram for the Response !\r\nResolved the issue by following installation steps using this tutorial.\r\nhttps://www.youtube.com/watch?v=xQVOaTUm9lM&t=390s"]}]