[{"number": 49389, "title": "Show search results of old path underneath 404 sign for dead links", "body": "A lot of old links to the Tensorflow documentation are now 404'ed. My suggestion would be that old link shows the 404 and also search results on tensorflow.org for the path. For example [https://www.tensorflow.org/deploy/distributed](https://www.tensorflow.org/deploy/distributed) should show the search results for [deploy distributed](https://www.tensorflow.org/s/results/?q=deploy%20distributed) underneath the 404 image.", "comments": ["@berendjan \r\nThank you for reporting the issue, we will fix this at the earliest.", "404 page WAI, as far as I can tell."]}, {"number": 49388, "title": "Fix one division by zero", "body": "PiperOrigin-RevId: 369474832\nChange-Id: I1082858ed78d9b2e4738ce30b231955973d49e1e", "comments": []}, {"number": 49387, "title": "Fix one FPE and remove two CHECK-fails.", "body": "PiperOrigin-RevId: 369349640\nChange-Id: I1fedbfc2b5bab635c5cb51f103d7c9176f79831a", "comments": []}, {"number": 49386, "title": "Fix one FPE and remove two CHECK-fails.", "body": "PiperOrigin-RevId: 369349640\nChange-Id: I1fedbfc2b5bab635c5cb51f103d7c9176f79831a", "comments": []}, {"number": 49385, "title": "Fix one FPE and remove two CHECK-fails.", "body": "PiperOrigin-RevId: 369349640\nChange-Id: I1fedbfc2b5bab635c5cb51f103d7c9176f79831a", "comments": []}, {"number": 49384, "title": "Fix one FPE and remove two CHECK-fails.", "body": "PiperOrigin-RevId: 369349640\nChange-Id: I1fedbfc2b5bab635c5cb51f103d7c9176f79831a", "comments": []}, {"number": 49383, "title": "Resolves coredump caused by `tf.data.experimental.save` with prefetch", "body": "Repeat and prefetch in combination cause the snapshot reader Initialize function to be invoked multiple times.\r\nHowever, there is nothing to prefetch on the very last iteration. This results in Prefetch issuing a CancelThreads call while the snapshot thread is trying to initialize. See https://github.com/tensorflow/tensorflow/blob/6446dda92eaadf11d22377e2354307642d739d73/tensorflow/core/kernels/data/prefetch_dataset_op.cc#L151\r\n\r\nCurrently the dataset reference counting is done asymmetrically. The reference increment happens at the end of initialization, where as the reference decrement\r\nhappens in a destructor. When prefetch cancels the snapshot thread, it errors out of the initialization function. And stops calling the reference increment. However, the reference decrement happens regardless, as it is in the destructor which always is invoked during cleanup. This results in an attempt to decrement the null dataset pointer, and therefore a segmentation fault.\r\nThis is different from all other dataset ops, where the dataset reference increment happens in the constructor and the decrement happens in the destructor, which are symmetric.\r\n\r\nThe solution to this is to ensure that the dataset reference is always initialized to nullptr, and to check for null when decrementing the dataset reference.", "comments": ["@yangustc07 @mihaimaruseac this is a cherry pick of https://github.com/tensorflow/tensorflow/pull/49166", "Thanks Abin. I added the release owner of r2.5.", "Since this is in 25 it will have to wait until we patch 2.5 again."]}, {"number": 49382, "title": "Remove unused constants", "body": "This PR removes a few unused constants to remove a few `-Wunused-variable` compile time warnings.", "comments": []}, {"number": 49381, "title": "TFLite NCHW data format issue and Open Questions", "body": "TFLite Version used - 2.4.1\r\nDoes TFlite Models can be created using NCHW format ?\r\n\r\nWhen I tried to create , I was having issues ?\r\nBut the error i was getting was incorrect, Is this expected ?\r\n\r\nIn documentation it is not mentioned that NCHW is not supported ? When is this expected to support ?\r\n\r\nFew open questions i found regarding this :- \r\nhttps://stackoverflow.com/questions/66671358/which-format-is-preferable-for-tflite-model-nchw-or-nhwc\r\n\r\nIs it possible to create tflite nchw models ? When TFLite NCHW format is supported ? Facing issues in the attached TFLite script \r\n\r\n\r\nOpen Questions :- \r\n\r\n**1.  Does TFLite supports NCHW ? \r\n2.  What is the default format of TFLite Models ? It is always N,H,W,C ?\r\n3.  What is the default format of TFLite Models LSTM models ? It is always N,S,D ? Does TFLite LSTM support S,N,D ?\r\n4.  What are the input formats other than  N,H,W, C  (or) N,S,D ?\r\n5.  How is ideally LSTM's and CNN's differentiated in TFLite ?\r\n6.  When will be NCHW supported for TFLite if its to be supported ?**\r\n\r\n\r\nFacing issues in the below code :- \r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\ninput_shape = (1, 3, 3, 227)\r\nx = tf.random.normal(input_shape)\r\n\r\n\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.InputLayer(input_shape=( 1, 3,227,227)),\r\n  tf.keras.layers.Conv2D(96, 11, activation='relu', strides=(4,4), dilation_rate=(1,1), groups=1, data_format = 'channels_first')])\r\nmodel.save('my_conv_model') \r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith open('convmodel.tflite', 'wb') as f:\r\nf.write(tflite_model)\r\n```", "comments": ["I am unable to generate the TF model. Is it possible to fix this code? For questions about TensorFlow post a [TensorFlow bug](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md) issue instead. We can then help you debug the conversion part.\r\n\r\nCode:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.InputLayer(input_shape=( 1, 3,227,227)),\r\n  tf.keras.layers.Conv2D(96, 11, activation='relu', strides=(4,4), dilation_rate=(1,1), groups=1, data_format = 'channels_first')])\r\nmodel.save('my_conv_model') \r\n```\r\n\r\nError:\r\n```\r\n10 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in set_shape(self, shape)\r\n    775     except errors.InvalidArgumentError as e:\r\n    776       # Convert to ValueError for backwards compatibility.\r\n--> 777       raise ValueError(str(e))\r\n    778 \r\n    779   @property\r\n\r\nValueError: Dimension 4 in both shapes must be equal, but are 217 and 55. Shapes are [?,1,96,55,217] and [?,1,96,55,55].\r\n```\r\n", "Could you provide the solution to resolve this issue .\r\nPlease provide me the answers for the questions i asked.", "1. Does TFLite supports NCHW ?\r\n> No.\r\n2. What is the default format of TFLite Models ? It is always N,H,W,C ?\r\n> NHWC. Yes.\r\n3. What is the default format of TFLite Models LSTM models ? It is always N,S,D ? Does TFLite LSTM support S,N,D ?\r\n> Yes it is always NSD. Refer to [this comment](https://github.com/tensorflow/tensorflow/issues/49381#issuecomment-854122927)\r\n4. What are the input formats other than N,H,W, C (or) N,S,D ?\r\n> Only NHWC is supported for CNN models and NSD for LSTMs.\r\n5. How is ideally LSTM's and CNN's differentiated in TFLite ?\r\n> It is identified based on the model specification (layers and ops defined)\r\n6. When will be NCHW supported for TFLite if its to be supported ?\r\n> This is not in our roadmap yet as there is no requirement. ", "I cannot provide a solution until I can first create a TensorFlow model. If you can provide a Tensorflow model along with the code, I can help with the conversion.", "Yes i will point an example of LSTM which takes N(Batch Size), S(Sequence length) and D(features ) as input. As in previous reply, You are saying only N,H,W, C are supported , But for LSTM's it takes N,S,D. Is there any API to tell what kind of format input is ?", "The below is the example of TFLite LSTM model \r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nmodel = keras.Sequential()\r\n\r\nmodel.add(layers.LSTM(input_shape=(None,20),units= 128, return_sequences = True))\r\nmodel.add(layers.Dense(10))\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy']);\r\nx_test = np.zeros([3000,4,20], dtype=np.float32);\r\nx_train = np.zeros([3000,4,20], dtype=np.float32);\r\ny_test =  np.zeros([3000, 4,], dtype=np.float32);\r\ny_train = np.zeros([3000, 4,], dtype=np.float32);\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test, y_test)\r\n\r\nrun_model = tf.function(lambda x: model(x))\r\n\r\nBATCH_SIZE = 1\r\nSTEPS = 2\r\nINPUT_SIZE = 20\r\n\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\r\n\r\n\r\nMODEL_DIR = \"keras_lstm_new\"\r\nmodel.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\ntflite_model = converter.convert()\r\n\r\nwith open('lstmmodelnew.tflite', 'wb') as f:\r\n  f.write(tflite_model) \r\n```\r\n", "From the documentation [here](https://www.tensorflow.org/lite/convert/rnn) and [example](https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb) [BATCH_SIZE, STEPS or TIME, INPUT_SIZE or UNITS] (NSD as per your definition) is the default format for LSTMs. Based on [API](https://github.com/tensorflow/tensorflow/blob/35a3ab91b42503776f428bda574b74b9a99cd110/tensorflow/python/keras/layers/recurrent_v2.py#L1238)  it looks like only NSD is supported as the input/output tensors are in NSD  (see the Args section.. all tensors use the same format).\r\n\r\nI was referring to NHWC wrt CNN/image models - I have corrected the previous comment.", "Created issue in stack overflow - https://stackoverflow.com/questions/67857868/unable-to-create-tflite-model-with-time-major-format\r\n\r\nNot sure, If we can create TFLite Models in [S,N,D] (time_major) format.\r\nCould you confirm creation of TFLite models in time major format @MeghnaNatraj ", "Any update ?", "@renjie-liu \r\n", "Yes, you can create time-majored lstm model.", "Please provide an example/python script  to create TFLite LSTM model time major ,", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49381\">No</a>\n"]}, {"number": 49380, "title": "[MLIR] TF Graphdef to MLIR python binding: input/output options", "body": "Update the Python binding from Graphdef to TF-MLIR with input/output\r\noptions - add non-trivial test cases.", "comments": []}, {"number": 49379, "title": "Distinction between categorical_crossentropy and sparse_categorical_crossentropy is not clear in the documentation", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThere is documentation for these 2 losses but there is no clear explanation distinguishing each of them and when exactly a specific Loss should be used.", "comments": ["A CL has been raised to fix it.", "A closer look at the usage example and their args documentation should help to spot the difference between the two functions.\r\n[`y_true` arg takes `one hot encoded` values vs `ground truth` values for sparse]\r\nThanks!\r\n", "@ymodak,\r\nInstead of taking a closer look at the code, a line of description highlighting the difference would make it more obvious and clear. ", "IMHO I disagree, reason being that there are widespread areas like these where you can argue that functions description is limited but they can be easily understood digging into usage examples and related documentation. \r\nThanks!", "@ymodak,\r\nIt's very harsh to close an issue without a proper justification. Just that you disagree, it doesn't mean it should be closed abruptly. \r\nI am not happy or not satisfied. ", "I think my justification is clear, the docs for those functions are good enough for any user to understand thier usages.\r\nYou don't need a distinction between different functions to justify further since the topics are independently and coherently discussed on their corresponding documentation page.\r\nIf your query was of a kind where api docs are limited that would have been a different case to deal with, here you are randomly comparing two functions and adding distinction between them which is not the design for any other docs anywhere on the website.\r\nTherefore the issue was closed. Hope this makes sense. Thanks!"]}, {"number": 49377, "title": "Tensorflow 2.3 GPU build failed | NCCL no such package | RHEL 7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): rhel 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.6.12\r\n- Installed using virtualenv? pip? conda?: local\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.3.1\r\n- CUDA/cuDNN version: 11.0 / 8.0\r\n- Clang Version: 5.1\r\n- GPU model and memory: 8 X Tesla V100 32GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n  After successfull configuration of tensorflow build we executed bazel build command , then it failed with below error ------\r\n-----------------------------------------------------------------------------------------------------------------\r\n\r\nERROR: /home/NTTA/vposaniadmin/tensorflow/tensorflow/core/kernels/BUILD:4689:1: C++ compilation of rule '//tensorflow/core/kernels:sparse_split_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /home/NTTA/vposaniadmin/.cache/bazel/_bazel_vposaniadmin/fbb878dda2a560b32c968c38a9e718c5/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.0 \\\r\n    GCC_HOST_COMPILER_PATH=/opt/rh/devtoolset-7/root/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=/opt/rh/rh-python36/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/usr/local/lib \\\r\n    PATH=/usr/local/cuda/bin:/usr/local/cuda/bin:/opt/rh/rh-python36/root/usr/bin:/home/NTTA/vposaniadmin/anaconda3/condabin:/usr/local/cuda/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/local/cuda/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/local/ssl/bin:/var/lib/snapd/snap/bin:/home/NTTA/vposaniadmin/.local/bin:/home/NTTA/vposaniadmin/bin:/usr/local/ssl/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/opt/rh/rh-python36/root/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/opt/rh/rh-python36/root/usr/lib64/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=7.0,7.0,7.0,7.0,7.0,7.0,7.0,7.0 \\\r\n    TF_NEED_CUDA=1 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/sparse_split_op/sparse_split_op.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/sparse_split_op/sparse_split_op.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' '-DTENSORFLOW_USE_XLA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_XLA=1' -c tensorflow/core/kernels/sparse_split_op.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/sparse_split_op/sparse_split_op.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from ./tensorflow/core/framework/op_kernel.h:35:0,\r\n                 from tensorflow/core/kernels/sparse_split_op.cc:19:\r\ntensorflow/core/kernels/sparse_split_op.cc: In member function 'void tensorflow::SparseSplitOp<T>::Compute(tensorflow::OpKernelContext*)':\r\ntensorflow/core/kernels/sparse_split_op.cc:71:34: error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus'\r\n                      dense_shape.AddDimWithStatus(input_shape_flat(i)));\r\n                                  ^\r\n./tensorflow/core/framework/op_requires.h:52:29: note: in definition of macro 'OP_REQUIRES_OK'\r\n     ::tensorflow::Status _s(__VA_ARGS__);                    \\\r\n                             ^~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n\r\n***** Full log attached below *********\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure\r\n\r\nEnabled CUDA support \r\nand rest of them are default\r\n\r\nbazel build  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "comments": ["@harishsureit \r\nThis has already been reported and worked upon, you may refer to these issues and pr on similar error. [#47815, #45575, #48393]\r\n\r\nI also came across a comment that says upgrading to 2.4.1, resolves the issue, please refer to the [comment here](https://github.com/tensorflow/tensorflow/issues/47832#issuecomment-800163367), and let us know by upgrading.\r\n\r\n", "@Saduf2019 \r\nCan we build tensorflow 2.4.1 offline , we are facing some network issues , is there any documentation or link for offline installation.", "@Saduf2019 \r\nAs you said We have upgraded tensorflow to 2.4.1 and cuda - 11.3, cudnn 8.2 , clang 3.4.2 , then we tried build and we got the error,\r\n/home/NTTA/vposaniadmin/.cache/bazel/_bazel_vposaniadmin/fbb878dda2a560b32c968c38a9e718c5/external/com_google_protobuf/BUILD:795:1: C++ compilation of rule '@com_google_protobuf//:python/google/protobuf/internal/_api_implementation.so' failed (Exit 1)\r\nexternal/com_google_protobuf/python/google/protobuf/internal/api_implementation.cc:31:10: fatal error: Python.h: No such file or directory\r\n#include <Python.h>\r\n^~~~~~~~~~\r\ncompilation terminated.\r\n\r\nPlease help us.", "@harishsureit \r\nCould you please try with the correct [Cuda,cudnn](https://www.tensorflow.org/install/source#gpu) version, 2.4 is tested with 11.0/8.0, 11.3 is not even updated on the site.", "@Saduf2019 \r\ndowngraded the CUDA to 11.0 and Cudnn to 8.0 , python 3.6.12, clang 5.1.0\r\nand we got error again , Full error message updated in the top comment,\r\n\r\n/home/NTTA/vposaniadmin/tensorflow/tensorflow/core/kernels/BUILD:4689:1: C++ compilation of rule '//tensorflow/core/kernels:sparse_split_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n\r\ntensorflow/core/kernels/sparse_split_op.cc:71:34: error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus'\r\n                      dense_shape.AddDimWithStatus(input_shape_flat(i)));\r\n\r\n", "@harishsureit \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/49150#issuecomment-845195683), which is work in progress.", "@Saduf2019 \r\nIm not using tensorrt , and im not even enabled in configuration, ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49377\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49377\">No</a>\n"]}, {"number": 49376, "title": "Added batch as formatting parameter during ModelCheckpoint callback", "body": "Fixes #38668 . It is to complete the work done in two of my previous stale PRs #38669 and [#1702](https://github.com/tensorflow/addons/pull/1702).\r\n\r\ncc @mihaimaruseac , please review.", "comments": ["@gbaned , Can you please check why ROCm build is failing? I suppose it is not related to the changes in this PR. It is failing because of `C++ compilation error`.\r\n\r\nFull error stack:\r\n```\r\ntensorflow/stream_executor/rocm/rocm_blas.cc:1830:14: error: 'CUDABlas' has not been declared\r\n port::Status CUDABlas::DoBlasGemmStridedBatchedWithAlgorithm(\r\n              ^~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\u001b[31m\u001b[1mERROR: \u001b[0m/workspace/tensorflow/lite/toco/python/BUILD:89:10 C++ compilation of rule '//tensorflow/stream_executor/rocm:rocblas_plugin' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n```", "cc @gbaned , @mihaimaruseac , @rchao, Any updates on this PR?", "@mihaimaruseac , @gbaned - Why is it failing internally? Previously also the PR for same issue I closed because of this. There was no bazel error but it failed internally. \n\nCan you please paste the stack trace or error if available?", "Here are the internal errors, @ashutosh1919 can you please verify ?  Thanks!\r\n\r\nTraceback (most recent call last):\r\n  File \"/py/absl/testing/parameterized.py\", line 316, in bound_param_test\r\n    return test_method(self, *testcase_params)\r\n  File \"/tensorflow/python/keras/keras_parameterized.py\", line 285, in decorated\r\n    _test_functional_model_type(f, self, *args, **kwargs)\r\n  File \"/tensorflow/python/keras/keras_parameterized.py\", line 299, in _test_functional_model_type\r\n    f(test_or_class, *args, **kwargs)\r\n  File \"/tensorflow/python/keras/callbacks_test.py\", line 720, in test_ModelCheckpoint\r\n    verbose=1)\r\n  File \"/tensorflow/python/keras/engine/training_v1.py\", line 814, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/tensorflow/python/keras/engine/training_arrays_v1.py\", line 661, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/tensorflow/python/keras/engine/training_arrays_v1.py\", line 391, in model_iteration\r\n    callbacks._call_batch_hook(mode, 'end', batch_index, batch_logs)\r\n  File \"/tensorflow/python/keras/callbacks.py\", line 316, in _call_batch_hook\r\n    self._call_batch_end_hook(mode, batch, logs)\r\n  File \"/tensorflow/python/keras/callbacks.py\", line 336, in _call_batch_end_hook\r\n    self._call_batch_hook_helper(hook_name, batch, logs)\r\n  File \"/tensorflow/python/keras/callbacks.py\", line 374, in _call_batch_hook_helper\r\n    hook(batch, logs)\r\n  File \"/tensorflow/python/keras/callbacks.py\", line 1383, in on_train_batch_end\r\n    self._save_model(epoch=self._current_epoch, batch=batch, logs=logs)\r\n  File \"/tensorflow/python/keras/callbacks.py\", line 1427, in _save_model\r\n    filepath = self._get_file_path(epoch, batch, logs)\r\n  File \"/tensorflow/python/keras/callbacks.py\", line 1485, in _get_file_path\r\n    epoch=epoch + 1, batch=batch + 1, **logs)\r\nTypeError: format() got multiple values for keyword argument 'batch'", "@mihaimaruseac , Can you please review the fix? I resolved the error.", "@qlzh727 should we move this to Keras?", "The PR is merged to keras-team/keras as https://github.com/keras-team/keras/commit/c567184255468a1d0ca0b6b284e20cdeb7561574. Closing this now."]}, {"number": 49375, "title": "Setting fixed seed in PreprocessingLayer leads to different results ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: Driver Version: 450.102.04   CUDA Version: 11.0\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\nI'm solving a task of image to image translation and would like to apply an identical augmentation to both the input and output images. For example, I'd like to flip both images, or rotate to the same degree. \r\nI can use transform from `tf.image.stateless_random_` and provide same seed value to get identical results, but there is no RandomTranslation or RandomRotation transforms. So I'm trying to use layers from `tf.keras.layers.experimental.preprocessing` with fixed seed.\r\nBut 2 layers initialised with same random seed are generating different transform (see example below). Is that expected?\r\n\r\n**Describe the expected behavior**\r\nInitialized from same seed 2 layers should do identical transformations.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nfrom tensorflow.keras import layers\r\nimport tensorflow as tf\r\n\r\ntransform_x = layers.experimental.preprocessing.RandomTranslation(height_factor=(-0.2, 0.3), width_factor=(-0.2, 0.3), seed=2)\r\ntransform_y = layers.experimental.preprocessing.RandomTranslation(height_factor=(-0.2, 0.3), width_factor=(-0.2, 0.3), seed=2)\r\n\r\ndef is_equal(x, y):\r\n    return tf.reduce_sum(tf.cast(tf.math.equal(x, y), tf.float32)).numpy() == 0\r\n\r\n\r\nx = tf.random.uniform((1, 32, 32, 3)) * 255.\r\n\r\nfor _ in range(15): \r\n    x_t = transform_x(x, training=True)\r\n    y_t = transform_y(x, training=True)\r\n    print(is_equal(x_t, y_t))\r\n```\r\n\r\nRunning this code leads to the following result:\r\n```\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\nFalse\r\n```\r\n\r\n", "comments": ["Closing due to finding bug in my code. Everything works as expected.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49375\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49375\">No</a>\n"]}, {"number": 49374, "title": "Segmentation fault (core dumped) - TFLite", "body": "@tensorflow/micro  x   @tensorflow/lite\r\n\r\nHi !\r\n\r\n**Describe the problem**\r\nTo read a model from official [TensorFlow source (COCO SSD MobileNet v1)](https://www.tensorflow.org/lite/examples/object_detection/overview#get_started) and perform inference with [minimal.cc](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/lite/examples/minimal/minimal.cc), we get the error below.\r\n\r\n**System information**\r\n- Host OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): From source (branch r1.12)\r\n- Target platform: iMX.6 (Arm v7)\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n```\r\nroot@analytics:~# ./minimal ssd_mobilenet_v1_1_metadata_1.tflite\r\nminimal: /usr/src/debug/tensorflow-lite/1.0-r0/git/tensorflow/contrib/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/util/\r\nMaxSizeVector.h:84: T& EigenForTFLite::MaxSizeVector<T>::operator[](std::size_t) [with T = EigenForTFLite::RunQueue\r\n<EigenForTFLite::StlThreadEnvironment::Task, 1024u>*; std::size_t = unsigned int]: \r\nAssertion `i < size_' failed. \r\nSegmentation fault (core dumped)\r\n```\r\nDo you have any idea or suggestion about why we are facing this error (because of versions, TFlite binaries or the model etc.) ?\r\nThank you in advance.", "comments": ["@ardtrkc,\r\nYou are using very older version of **`Tensorflow, 1.12`**. Can you please upgrade to the latest version of **`Tensorflow (2.5)`** and  see if the issue persists? Thanks!", "Thank you ! @rmothukuru ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49374\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49374\">No</a>\n"]}, {"number": 49372, "title": "Relu fake quant after Quantification Aware Training ", "body": "hi\r\n\r\nwhen  do  Quantification Aware Training with tensorflow-model-optimization, for Relu op, \r\nafter QAT, and found the min value of fake quant node is not zero ,and a constant value.\r\n\r\nso what reason cause that , thanks.", "comments": ["TF version is TF2.3.0.", "@xiaomsky \r\nYour request is not clear, can you explain in brief the issue faced, we see you haven filled the issue template as well.", "@Saduf2019\r\nnow I use quantize_model or quantize_annotate_layer to do QAT for test model , \r\nbut I found after do quantization ,  the min value of Relu op fake_quant node is not zero, and is it right ?\r\nalso every relu op fake_quant node min value is the same constant value.\r\nwill upload simple code followed.\r\nplease help to check. thanks.\r\n\r\n//////////////////////////////////////////\r\nimport tensorflow as tf\r\nimport numpy as np\r\nprint(f\"tf verion = {tf.__version__}\")\r\n\r\nimport tensorflow_model_optimization as tfmot\r\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Dropout,ReLU\r\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\r\n# load data\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nx_train = x_train[..., tf.newaxis]\r\nx_test = x_test[..., tf.newaxis]\r\n\r\nt_img =x_test[0]\r\nt_img = np.expand_dims(t_img, axis=0)\r\nprint('t_img shape:',t_img.shape)\r\nprint('t_img dtype:',t_img.dtype)\r\n\r\n\r\nmodel = tf.keras.models.Sequential([\r\n        Conv2D(filters=6,kernel_size=5,strides=(1,1),padding='same',use_bias=False,input_shape=(28,28,1)),\r\n        ReLU(max_value=6,negative_slope=0,threshold=0),\r\n        MaxPool2D(pool_size=(3,3),strides=2,padding=\"same\"),\r\n        Conv2D(filters=16,kernel_size=5,strides=(1,1),padding='same',use_bias=False),\r\n        MaxPool2D(pool_size=(3,3),strides=2,padding=\"same\"),\r\n        Flatten(input_shape=(7, 7)),\r\n        Dense(120),\r\n        Dense(84),\r\n        Dropout(0.2),\r\n        Dense(10, activation='softmax')\r\n    ])\r\n\r\ndef model_run():\r\n    '''\r\n        float32 model\r\n    '''\r\n    print(\"float32 model:\")\r\n    model.compile(optimizer='adam',\r\n                    loss='sparse_categorical_crossentropy',\r\n                    metrics=['accuracy'])\r\n    print(\"==> training\")\r\n    model.fit(x_train, y_train, epochs=1)\r\n    print(\"==> evaluate\")\r\n    model.evaluate(x_test,  y_test, verbose=2)\r\n\r\ndef quant_model_run():\r\n    '''\r\n        quantized model\r\n    '''\r\n    print(\"quantized model:\")\r\n    quant_model = tfmot.quantization.keras.quantize_model(model)\r\n\r\n    quant_model.compile(optimizer='adam',\r\n                    loss='sparse_categorical_crossentropy',\r\n                    metrics=['accuracy'])\r\n    print(\"==> training\")\r\n    quant_model.fit(x_train, y_train, epochs=1)\r\n    print(\"==> evaluate\")\r\n    quant_model.evaluate(x_test,  y_test, verbose=2)\r\n    \r\n    if 1:     \r\n        #save models\r\n        full_model = tf.function(lambda Input: quant_model(Input))\r\n        full_model = full_model.get_concrete_function(tf.TensorSpec(t_img.shape, t_img.dtype))\r\n        frozen_func = convert_variables_to_constants_v2(full_model)\r\n        frozen_func.graph.as_graph_def()\r\n        layers = [op.name for op in frozen_func.graph.get_operations()]\r\n        # Save frozen graph from frozen ConcreteFunction to hard drive\r\n        tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\r\n                          logdir=\"./\",\r\n                          name=\"test_quant.pb\",\r\n                          as_text=False)\r\n        \r\n    \r\n    \r\n\r\nif __name__=='__main__':\r\n    model_run()\r\n    quant_model_run()\r\n\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49372\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49372\">No</a>\n"]}, {"number": 49371, "title": "How to solve the INFO information: INFO:absl:resolver HttpCompressedFileResolver does not support the provided handle. INFO:absl:resolver GcsCompressedFileResolver does not support the provided handle. INFO:absl:resolver HttpUncompressedFileResolver does not support the provided handle.", "body": "> INFO:absl:resolver HttpCompressedFileResolver does not support the provided handle.\r\n> INFO:absl:resolver GcsCompressedFileResolver does not support the provided handle.\r\n> INFO:absl:resolver HttpUncompressedFileResolver does not support the provided handle.\r\n\r\nWhen I train my text classification model, there always comes the info above. I do not know where they come from ?\r\nAny solutions to ban these ?", "comments": ["Please share a very minimal example to reproduce this and fill the issue template with all the required info.", "@yananchen1989 \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\n@bhack \r\nThank you for triaging the issue but we follow a process and a template for the same, kindly let us stick to the process, once information is shared by user your insights will be helpful.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49371\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49371\">No</a>\n"]}, {"number": 49370, "title": "tflite_runtime interpreter produces strange results when warmup is done with empty images", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): YOCTO Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI have an application that runs on an embedded system. The application acquires images from a camera and performs predictions using a custom quantized neural network and the tflite_runtime interpreter. The applications and the model works fine with good accuracy, but the first inferences are very slow because of the warmup time needed by the interpreter. For this reason I tried to perform the warmup before the application starts, by feeding the interpreter some empty images (numpy.zeros). However, this causes subsequent inferences with real images to produce almost constant (and wrong) predictions.\r\n\r\n**Describe the expected behavior**\r\nI expect the interpreter to perform well even if the warmup is done with empty images\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): no - Briefly describe your candidate solution\r\n(if contributing): \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tflite_runtime.interpreter as tflite\r\nimport numpy as np\r\nimport cv2\r\n\r\ndef classify(interpreter, input_details, output_details, img):\r\n\tinput_shape = input_details[0]['shape']\r\n\timg = cv2.resize(img, (input_shape[2], input_shape[1]))\r\n\timg = (img/255.0).astype(np.float32)\r\n\timg = np.expand_dims(img, 0)\r\n\tinterpreter.set_tensor(input_details[0]['index'], img)\t\r\n\tinterpreter.invoke()\t\t\t\r\n\toutput = interpreter.get_tensor(output_details[0]['index'])\r\n\toutput = np.squeeze(output)\r\n        return output\r\n\r\n\r\ninterpreter = tflite.Interpreter('mymodel.tflite')\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninput_shape = input_details[0]['shape']\r\n\r\nwarmup_img = np.zeros(input_shape)\r\n_ = classify(interpreter, input_details, output_details, warmup_img)\r\n\r\ncap = cv2.VideoCapture(0)\r\n\r\nwhile cap.isOpened():\r\n      _, frame = cap.read()\r\n      print(classify(interpreter, input_details, output_details, frame))\r\n```\r\n\r\n\t\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Could you provide the TFLite model if possible or a minimal and reproducible step as a gist?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49370\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49370\">No</a>\n"]}, {"number": 49369, "title": "Need clarity on metrics reported in extras field on using run_op_benchmark", "body": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/test/Benchmark\r\n\r\n## Description of issue (what needs changing):\r\n\r\nI ran a simple tf.linalg.matvec operation using the benchmark framework and got different metrics in the extras field. I could not understand the difference between the keys in the extra field - allocator_max_num_bytes_GPU_0_bfc and allocator_max_num_bytes_gpu_host_bfc.\r\n\r\n\r\nMore  description of these metrics would help users understand what metrics are actually being  reported.", "comments": ["@aditisaluja5 ,\r\n\r\nWe see that the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue[tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced].\r\n\r\nLooks like the issue is with gpu memory allocation.Please try limiting [GPU memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) using any of the methods listed here and check if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49368, "title": "How about adding a note of `_get_optimizer` in \"Customize what happens in Model.fit\" guide?", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#wrapping_up_an_end-to-end_gan_example\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nWhen using Keras mixed-precision API and overriding `tf.keras.Model.compile` function like the above guide, training loss cannot converge easily, because the optimizers are not wrapped in `LossScaleOptimizer`. I found it can be fixed using `tf.keras.Model._get_optimizer` function like below.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/engine/training.py#L479-L586\r\n\r\nI think it would be much better to add a note like ``it is recommended to wrap optimizer objects using `self._get_optimizer` when using Keras mixed-precision API`` in that guide.\r\n", "comments": ["A pr has been created once the [pr](https://github.com/keras-team/keras-io/pull/524) is merged this issue will be moved to closed status.", "@jeongukjae \r\nMoving this issue to closed status as per update on pr."]}, {"number": 49367, "title": "benchmark_model performance", "body": "I have a tflite quantizd model(qat). \r\n\r\nTo benchmark it's performance with device rk3399(arm), I build a binary benchmark_model form  source, command is : ./tensorflow/lite/tools/make/build_aarch64_lib.sh.\r\n\r\nI also download a pre-build binary(linux_aarch64) here: https://www.tensorflow.org/lite/performance/measurement. \r\n\r\nThen I have two excutable binaries,  but the tflite models'performance  has a large difference. \r\n\r\nWith the binary I build from source, The performance is 74ms, but with the downloaded binary, the performance is 55ms.\r\n\r\nI am confused what's wrong with it. Does the command I build form source wrong?\r\n\r\nTensorflow commtid d99c821817e", "comments": ["@multiverse-tf could you take a look?", "> I have a tflite quantizd model(qat).\r\n> \r\n> To benchmark it's performance with device rk3399(arm), I build a binary benchmark_model form source, command is : ./tensorflow/lite/tools/make/build_aarch64_lib.sh.\r\n> \r\n> I also download a pre-build binary(linux_aarch64) here: https://www.tensorflow.org/lite/performance/measurement.\r\n> \r\n> Then I have two excutable binaries, but the tflite models'performance has a large difference.\r\n> \r\n> With the binary I build from source, The performance is 74ms, but with the downloaded binary, the performance is 55ms.\r\n> \r\n> I am confused what's wrong with it. Does the command I build form source wrong?\r\n> \r\n> Tensorflow commtid [d99c821](https://github.com/tensorflow/tensorflow/commit/d99c821817e711972f78b0d01d24a57b45b831b6)\r\n\r\nI guess the difference might come from two sources:\r\n1. the RUY library isn't enabled for the make-based build. \r\n2. the compiler flags might be different. As for the pre-built one, we used \"bazel --define=tflite_with_ruy=true --config=elinux_aarch64 --define tensorflow_mkldnn_contraction_kernel=0\" to compile the binary. \r\n\r\nSo, could you try the \"bazel\" to build your binary? Or try \"cmake\" to build the binary as shown [here](https://www.tensorflow.org/lite/guide/build_cmake_arm#build_for_aarch64_arm64)?\r\n", "Thanks very much. Does it mean ruy has a better performance than gemmlowp on arm platform?", "> Thanks very much. Does it mean ruy has a better performance than gemmlowp on arm platform?\r\n\r\nI think so as RUY is supposed to replace gemmlowp. @talumbau, could you shed some light on this?\r\n\r\nNote RUY is automatically enabled when building tflite library on Android.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49366, "title": "Undefined symbol zungqr_", "body": "Traceback (most recent call last):\r\n  File \"real_time_object_detection.py\", line 2, in <module>\r\n    from imutils.video import VideoStream\r\n  File \"/var/rootdirs/home/root/tflite/tflite-env/lib/python3.8/site-packages/imutils/__init__.py\", line 8, in <module>\r\n    from .convenience import translate\r\n  File \"/var/rootdirs/home/root/tflite/tflite-env/lib/python3.8/site-packages/imutils/convenience.py\", line 5, in <module>\r\n    import numpy as np\r\n  File \"/var/rootdirs/home/root/tflite/tflite-env/lib/python3.8/site-packages/numpy/__init__.py\", line 148, in <module>\r\n    from . import lib\r\n  File \"/var/rootdirs/home/root/tflite/tflite-env/lib/python3.8/site-packages/numpy/lib/__init__.py\", line 25, in <module>\r\n    from .index_tricks import *\r\n  File \"/var/rootdirs/home/root/tflite/tflite-env/lib/python3.8/site-packages/numpy/lib/index_tricks.py\", line 12, in <module>\r\n    import numpy.matrixlib as matrixlib\r\n  File \"/var/rootdirs/home/root/tflite/tflite-env/lib/python3.8/site-packages/numpy/matrixlib/__init__.py\", line 4, in <module>\r\n    from .defmatrix import *\r\n  File \"/var/rootdirs/home/root/tflite/tflite-env/lib/python3.8/site-packages/numpy/matrixlib/defmatrix.py\", line 11, in <module>\r\n    from numpy.linalg import matrix_power\r\n  File \"/var/rootdirs/home/root/tflite/tflite-env/lib/python3.8/site-packages/numpy/linalg/__init__.py\", line 73, in <module>\r\n    from .linalg import *\r\n  File \"/var/rootdirs/home/root/tflite/tflite-env/lib/python3.8/site-packages/numpy/linalg/linalg.py\", line 33, in <module>\r\n    from numpy.linalg import lapack_lite, _umath_linalg\r\nImportError: /var/rootdirs/home/root/tflite/tflite-env/lib/python3.8/site-packages/numpy/linalg/lapack_lite.cpython-38-arm-linux-gnueabi.so: undefined symbol: zungqr_\r\n", "comments": ["@Nytish,\r\n\r\nWe see that the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or if possible share a colab gist with the issue reported.\r\n\r\nThanks!", "Steps\r\n1.Installed lapack on system \r\n2. Try to run my Python code\r\n3. Got this error as shown above\r\n\r\ntf version 1.0.0 and numpy=1.17.4 \r\n\r\n", "@Nytish,\r\n\r\nWe see that you are using tf version 1.0, 1.x is not actively supported, please update to 2.x and let us know if you are using same issue.\r\n\r\nAlso looks like your numpy installation is broken. Numpy appears installed by linked incorrectly to lapack.\r\nThanks!", "How to upgrade tf to 2.x\n\nNumpy version 1.17.4 i installed\n\nOn Fri, May 21, 2021, 2:58 PM tilakrayal ***@***.***> wrote:\n\n> @Nytish <https://github.com/Nytish>,\n>\n> We see that you are using tf version 1.0, 1.x is not actively supported,\n> please update to 2.x and let us know if you are using same issue.\n>\n> Also looks like your numpy installation is broken. Numpy appears installed\n> by linked incorrectly to lapack.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/49366#issuecomment-845818198>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ARVZUDHYPT2DQBH42OGIPZDTOYRT5ANCNFSM45F6VVUQ>\n> .\n>\n", "@Nytish ,\r\n\r\nCan you please refer to this [link](https://www.tensorflow.org/install) for installing tensorflow latest version.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49366\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49366\">No</a>\n"]}, {"number": 49365, "title": "How to configure tensorflow2.x in Qt creator?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:no\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:18.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:source\r\n-   **TensorFlow version (use command below)**:tensorflow2.4.1\r\n-   **Python version**:python3.6\r\n-   **Bazel version (if compiling from source)**:3.1.0\r\n-   **GCC/Compiler version (if compiling from source)**:7.5.0\r\n-   **CUDA/cuDNN version**:no\r\n-   **GPU model and memory**:no\r\n-   **Exact command to reproduce**:N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This should be better asked on https://discuss.tensorflow.org/ (or Stack Overflow), but the community forum will be faster, I think.\r\n\r\nIt is not a feature request / bug report, so it should not be attached to the repository.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49365\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49365\">No</a>\n"]}, {"number": 49364, "title": "Prevent another division by zero.", "body": "PiperOrigin-RevId: 369338598\nChange-Id: I55471d363e401fdcf8d259670ad4eef672b731e2", "comments": []}, {"number": 49363, "title": "Prevent another division by zero.", "body": "PiperOrigin-RevId: 369338598\nChange-Id: I55471d363e401fdcf8d259670ad4eef672b731e2", "comments": []}, {"number": 49362, "title": "Prevent another division by zero.", "body": "PiperOrigin-RevId: 369338598\nChange-Id: I55471d363e401fdcf8d259670ad4eef672b731e2", "comments": []}, {"number": 49361, "title": "Prevent another division by zero.", "body": "PiperOrigin-RevId: 369338598\nChange-Id: I55471d363e401fdcf8d259670ad4eef672b731e2", "comments": []}, {"number": 49360, "title": "Fix overflow CHECK issue with `tf.raw_ops.AddManySparseToTensorsMap`.", "body": "PiperOrigin-RevId: 369492969\nChange-Id: I1d70d6c0c92e3d7a25bc3b3aa2a0c0ac9688bf81", "comments": []}, {"number": 49359, "title": "Fix overflow CHECK issue with `tf.raw_ops.AddManySparseToTensorsMap`.", "body": "PiperOrigin-RevId: 369492969\nChange-Id: I1d70d6c0c92e3d7a25bc3b3aa2a0c0ac9688bf81", "comments": []}, {"number": 49358, "title": "Fix overflow CHECK issue with `tf.raw_ops.AddManySparseToTensorsMap`.", "body": "PiperOrigin-RevId: 369492969\nChange-Id: I1d70d6c0c92e3d7a25bc3b3aa2a0c0ac9688bf81", "comments": []}]