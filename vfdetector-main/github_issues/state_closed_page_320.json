[{"number": 44605, "title": "Compiling tensorflow for 12 hours is ok?", "body": "\r\n\r\n**System information**\r\n- Windows 10 \r\n- I download tensorflow by GIT day before yesterday\r\n- TensorFlow version: last from GIT\r\n- Python version: 3.7.9\r\n- Im use pip\r\n- Bazel version 3.7.0\r\n- VS2019\r\n- CUDA/cuDNN version: 11.1/8\r\n- GPU model and memory:GeForce 3090, 24GB\r\nCPU i7 10700 RAM 32GB, ssd\r\n\r\n\r\n\r\nI following instructions for installation from sourses from website tensorflow.org (for windows 10). Now pass 12 hours from start time building the tensorflow. During this time I have 100% usage the CPU, and using 95% memory from 32GB. Its normally? Dont sure...((\r\nThanx!\r\n![compile](https://user-images.githubusercontent.com/29018860/98205473-47a4fa00-1f49-11eb-9527-560ab1e18c01.jpg)\r\n\r\n", "comments": ["Via 16 hours: [16,775 / 17,282] 16 actions running\r\n    Compiling tensorflow/core/kernels/matmul_op_real.cc; 48878s local\r\n    Compiling tensorflow/core/kernels/linalg/einsum_op_impl_bfloat16.cc; 48710s local\r\n    Compiling tensorflow/core/kernels/fused_batch_norm_op.cc; 36409s local\r\n    Compiling tensorflow/core/kernels/cwise_op_xlogy.cc; 26835s local\r\n    Compiling tensorflow/core/kernels/cwise_op_xlog1py.cc; 26833s local\r\n    Compiling tensorflow/core/kernels/cwise_op_sub.cc; 26749s local\r\n    Compiling tensorflow/core/kernels/cwise_op_squared_difference.cc; 26703s local\r\n    Compiling tensorflow/core/kernels/cwise_op_div.cc; 26624s local ...\r\n", "https://github.com/tensorflow/tensorflow/issues/44605", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44605\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44605\">No</a>\n"]}, {"number": 44604, "title": "keras epsilon doesn't change ", "body": "Windows, TF 2.3.1\r\n\r\nI'm trying to change the epsilon so my MAPE doesn't blow up. When I do:\r\n\r\n$tensorflow.keras.backend.set_epsilon = 1e-3\r\nand then:\r\n$tensorflow.keras.backend.epsilon()\r\nI get:\r\n>>>> 1e-07\r\n\r\n", "comments": ["@je-santos \r\n\r\nI have tried in colab with TF 2.3.1 version and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/3dd9f318dd195976dbdda087e7f4d2b9/untitled486.ipynb).Please, verify once and close the issue. Thanks!", "Hi @ravikyram . I re-installed my TF and now it works. Thanks for the quick response to the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44604\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44604\">No</a>\n"]}, {"number": 44603, "title": "tf.estimator.BoostedTreesClassifier - AttributeError: 'NoneType' object has no attribute 'is_compatible_with'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\n\r\n\r\n**System information**\r\n\r\n- Running my code on Google Colab notebook\r\n\r\nAm using the tf nightly build as:\r\n\r\n`!pip install tf-nightly`  \r\n\r\noutput of pip install :\r\n\r\nBuilding wheel for tensorflow-docs (setup.py) ... done\r\nRequirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.5.0.dev20201104)\r\nRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.19.4)\r\nRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\r\nRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.35.1)\r\nRequirement already satisfied: tb-nightly~=2.4.0.a in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0a20201104)\r\nRequirement already satisfied: protobuf~=3.13.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.13.0)\r\nRequirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\r\nRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\r\nRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\r\nRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\r\nRequirement already satisfied: tf-estimator-nightly~=2.4.0.dev in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0.dev2020102301)\r\nRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\r\nRequirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.32.0)\r\nRequirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\r\nRequirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\r\nRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\r\nRequirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\r\nRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\r\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (1.7.0)\r\nRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (1.0.1)\r\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (2.23.0)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (0.4.2)\r\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (3.3.3)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (1.17.2)\r\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.4.0.a->tf-nightly) (50.3.2)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly) (1.24.3)\r\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly) (2.10)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly) (2020.6.20)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.4.0.a->tf-nightly) (1.3.0)\r\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.4.0.a->tf-nightly) (2.0.0)\r\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly) (4.6)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly) (0.2.8)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly) (4.1.1)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.4.0.a->tf-nightly) (3.1.0)\r\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.4.0.a->tf-nightly) (3.4.0)\r\nRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly) (0.4.8)\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am using the tf.estimator.BoostedTreeClassifier\r\n\r\n```\r\nest = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\r\nest.train(train_input_fn, max_steps=100)\r\n\r\n```\r\n\r\nI switched to the tf-nightly build to use the fix for this issue: https://github.com/tensorflow/tensorflow/issues/40063\r\ngetting for following error since Nov 4 with the tf nightly build:\r\n\r\n`est.train(train_input_fn, max_steps=100)`\r\n\r\nis failing as follows:\r\n\r\n```\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp1dthkfom\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp1dthkfom', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Calling model_fn.\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-33-961616a6c72a> in <module>()\r\n     14 \r\n     15 # n_batches_per_layer=10, n_trees=n_trees, n_classes=9)\r\n---> 16 est.train(train_input_fn, max_steps=100)\r\n     17 \r\n     18 print(est.evaluate(eval_input_fn))\r\n\r\n13 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parallel_for/gradients.py in batch_jacobian(output, inp, use_pfor, parallel_iterations)\r\n    111   \"\"\"\r\n    112   output_shape = output.shape\r\n--> 113   if not output_shape[0].is_compatible_with(inp.shape[0]):\r\n    114     raise ValueError(\"Need first dimension of output shape (%s) and inp shape \"\r\n    115                      \"(%s) to match.\" % (output.shape, inp.shape))\r\n\r\nAttributeError: 'NoneType' object has no attribute 'is_compatible_with'\r\n\r\n```\r\n\r\nI switched back to stable version of tf=2.3.0 , dont get the error above , but it fails on issue #40063\r\nso it seems to be problem with the nightly build. I did not hit this error with earlier to Nov 3th nightly build\r\n\r\n**Describe the expected behavior**\r\n\r\nexpect \r\n\r\n```\r\nest = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\r\nest.train(train_input_fn, max_steps=100)\r\n```\r\nthis to run \r\n\r\nI did not have this issues till the Nov 3 nightly build, \r\nOne was to address it and move forward for now would be to use a pre Nov 3 tf nightly build. is there a way to specify the version of the nightly build to pip install. that would solve my problem for now.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nparams = {\r\n  'n_trees': 100,\r\n  'max_depth': 10,\r\n  'n_batches_per_layer': 1,\r\n  'n_classes': 9,\r\n}\r\nest = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\r\nest.train(train_input_fn, max_steps=100)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ln0v \r\n\r\nI have tried in colab with TF nightly version(`2.5.0-dev20201104`) and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/029e1dd9c39f33156c494abff35b3ee5/untitled484.ipynb).Please, verify once and close the issue.Thanks!", "@ravikyram  thanks for your efforts, tried the nightly build again and it works fine.  Seems like colab notebook needed a reset and a fresh start on my side. Closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44603\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44603\">No</a>\n"]}, {"number": 44602, "title": "fatal error: tensorflow/core/framework/types.pb.h: No such file or directory", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 18.04.5 LTS\r\n- TensorFlow installed from source\r\n- TensorFlow version : r2.2\r\n- Python version : py3.6\r\n- Bazel version : 2.0.0\r\n- GCC/Compiler version : 7.5.0\r\n\r\n \r\nsteps to produce error:\r\n\r\n- sudo apt install python3-dev python3-pip\r\n- pip install -U --user pip numpy wheel\r\n- pip install -U --user keras_preprocessing --no-deps\r\n- git clone https://github.com/tensorflow/tensorflow.git\r\n- cd tensorflow\r\n- git checkout r2.2\r\n- ./configure\r\n\r\nYou have bazel 2.0.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\n\r\nFound possible Python library paths:\r\n/usr/lib/python3/dist-packages\r\n/usr/local/lib/python3.6/dist-packages\r\nPlease input the desired Python library path to use. Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n--config=mkl # Build with MKL support.\r\n--config=monolithic # Config for mostly static monolithic build.\r\n--config=ngraph # Build with Intel nGraph support.\r\n--config=numa # Build with NUMA support.\r\n--config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.\r\n--config=v2 # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n--config=noaws # Disable AWS S3 filesystem support.\r\n--config=nogcp # Disable GCP support.\r\n--config=nohdfs # Disable HDFS support.\r\n--config=nonccl # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n-> bazel build -c opt tensorflow:libtensorflow_cc.so\r\n-> then i created CMakeLists.txt file in tensorflow/tensorflow/examples/speech_commands/ \r\n-> cd /tensorflow/examples/speech_commands/\r\n-> mkdir build\r\n-> cd build\r\n-> cmake ..\r\n-> make\r\nThen i am facing this issue.\r\nERROR:\r\n[ 50%] Building CXX object CMakeFiles/speech_recog.dir/label_wav.cc.o\r\nIn file included from /home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/core/framework/tensor.h:23:0,\r\nfrom /home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/examples/speech_commands/label_wav.cc:19:\r\n/home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/core/framework/tensor_shape.h:22:10: fatal error: tensorflow/core/framework/types.pb.h: No such file or directory\r\n#include \"tensorflow/core/framework/types.pb.h\"\r\n^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nCMakeFiles/speech_recog.dir/build.make:62: recipe for target 'CMakeFiles/speech_recog.dir/label_wav.cc.o' failed\r\nmake[2]: *** [CMakeFiles/speech_recog.dir/label_wav.cc.o] Error 1\r\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/speech_recog.dir/all' failed\r\nmake[1]: *** [CMakeFiles/speech_recog.dir/all] Error 2\r\nMakefile:83: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n\r\n\r\nThis is the cmake file\r\n[CMakeLists.txt](https://github.com/tensorflow/tensorflow/files/5491688/CMakeLists.txt)\r\n", "comments": ["@sairmreddy,\r\nCould you please specify if you're trying to build the [Python TensorFlow package](https://www.tensorflow.org/install/source) or the [TensorFlow C API](https://www.tensorflow.org/install/lang_c)?\r\n\r\nPlease check the linked guides for the respective instructions and let us know if you are still facing the same error. Thanks!", "@amahendrakar \r\nFor Tensorflow C++ API", "@sairmreddy,\r\nFollowing the instructions mentioned in the [guide](https://www.tensorflow.org/install/lang_c), I was able to build the API without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5d60664ec79b5fd06130dad057ed1b23/44602.ipynb).\r\n\r\nCould you please try building the API using TF v2.3 (instead of TF v2.2) and let us know if you are facing the same issue. Thanks!", "@amahendrakar,\r\ncan you please compile this\r\n[label_wav (copy).txt](https://github.com/tensorflow/tensorflow/files/5587239/label_wav.copy.txt)\r\n.cc file same as above.", "@sairmreddy,\r\nOn running the `label_wav.cc` file, I am facing an error stating `fatal error: tensorflow/core/framework/tensor.h: No such file or directory`. \r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/87a86001d2e147286dc2d15f2659379b/44602.ipynb#scrollTo=Gpr_pve3rzNG). Thanks!", "Then how can i compile this file, if any one tried this  please help. Thanks.", "Hi @rmothukuru \r\n\r\nnow i am getting these errors\r\n\r\nCMakeFiles/speech_recog.dir/label_wav.cc.o: In function `tensorflow::Status tensorflow::errors::NotFound<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*)':\r\nlabel_wav.cc:(.text._ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_]+0xe5): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::string_view, std::vector<tensorflow::StackFrame, std::allocator<tensorflow::StackFrame> >&&)'\r\nCMakeFiles/speech_recog.dir/label_wav.cc.o: In function `main':\r\nlabel_wav.cc:(.text.startup+0x1219): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(absl::Span<long long const>)'\r\nlabel_wav.cc:(.text.startup+0x1f55): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(absl::Span<long long const>)'\r\nlabel_wav.cc:(.text.startup+0x1fa8): undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(absl::Span<long long const>)'\r\ncollect2: error: ld returned 1 exit status\r\nCMakeFiles/speech_recog.dir/build.make:114: recipe for target 'speech_recog' failed\r\nmake[2]: *** [speech_recog] Error 1\r\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/speech_recog.dir/all' failed\r\nmake[1]: *** [CMakeFiles/speech_recog.dir/all] Error 2\r\nMakefile:83: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n\r\nThis is the CmakeLists.txt\r\n[CMakeLists.txt](https://github.com/tensorflow/tensorflow/files/5616823/CMakeLists.txt)\r\n", "sbojja@sbojja-Latitude-E5450:~/TF-speech/tensorflow$ git checkout r2.3\r\nM\ttensorflow/tools/ci_build/install/install_proto3.sh\r\nM\ttensorflow/tools/ci_build/protobuf/protobuf_optimized_pip.sh\r\nSwitched to branch 'r2.3'\r\nYour branch is up-to-date with 'origin/r2.3'.\r\nsbojja@sbojja-Latitude-E5450:~/TF-speech/tensorflow$ ./configure \r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: \r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: \r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\nsbojja@sbojja-Latitude-E5450:~/TF-speech/tensorflow$ history | grep bazel\r\n  436  bazel build tensorflow/examples/label_image/...\r\n  440  bazel build tensorflow/examples/label_image/...\r\n  816  history | grep bazel build\r\n  817  history | grep \"bazel build\"\r\n  819  bazel build -c opt --copt=-w --output_filter=DONT_MATCH_ANYTHING tensorflow:libtensorflow_cc.so\r\n  827  bazel build -c opt --copt=-w --output_filter=DONT_MATCH_ANYTHING tensorflow:libtensorflow_cc.so\r\n 1318  history | grep bazel\r\nsbojja@sbojja-Latitude-E5450:~/TF-speech/tensorflow$ bazel build -c opt --copt=-w --output_filter=DONT_MATCH_ANYTHING tensorflow:libtensorflow_cc.so\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=143\r\nINFO: Reading rc options for 'build' from /home/sbojja/TF-speech/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/sbojja/TF-speech/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /home/sbojja/TF-speech/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /home/sbojja/TF-speech/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/sbojja/TF-speech/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /home/sbojja/TF-speech/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/sbojja/TF-speech/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Build options --action_env, --define, and --host_copt have changed, discarding analysis cache.\r\nINFO: Analyzed target //tensorflow:libtensorflow_cc.so (196 packages loaded, 19181 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow:libtensorflow_cc.so up-to-date:\r\n  bazel-bin/tensorflow/libtensorflow_cc.so\r\nINFO: Elapsed time: 12399.091s, Critical Path: 316.60s\r\nINFO: 8489 processes: 8489 local.\r\nINFO: Build completed successfully, 8759 total actions\r\n\r\nand moved .so to /usr/local/lib/\r\n\r\nthen \r\nmkdir build\r\ncd build\r\ncmake ..\r\nmake\r\nthen i am getting above error", "same issue with my ubuntu 18.04 with tensorflow 2.3.1. The tensorflow build was successful. though the same error happened:\r\n: tensorflow/core/framework/types.pb.h: No such file or directory\r\n #include \"tensorflow/core/framework/types.pb.h\"", "@tensorflowbutler @rmothukuru , \r\nAny updates? I am getting the same errors on my Ubuntu 18.04 with tensorflow 2.3.0 install? ", "@shashank2710 Did you manage to solve the error?", "@aswath8 refer #46837. ", "@sairmreddy \r\nIs this still an issue can you please try with latest stable version and let us know.", "@sairmreddy,\r\n\r\nCan you try to build latest version of TF `2.6.0` using this [guide](https://www.tensorflow.org/install/lang_c)? You can take a look the similar issues #46837,#43307 and #40538 for your reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44602\">No</a>\n"]}, {"number": 44601, "title": "Why is the memory(no gpu) used so different between windows and Ubuntu\uff1f", "body": "PS: memory is not GPU-memory, is PC-memory\r\nThere are 6 small models, inference with 6 multiprocessing in python3.7 and tensorflow1.13.1.  \r\nIn windows, use 4G memory, but in Ubuntu use 12G memory, don't know why.  \r\nHow to reduce memory in ubuntu?", "comments": ["@nistarlwc,\r\nTensorFlow 1.x is not actively supported. Could you please upgrade TensorFlow to v2.3 and check if you are facing the same issue? Thanks!", "update tensorflow2.3, but it use keras\r\n`model=tf.keras.models.load_model(\"xx.h5)  \r\noutput = model.predict(image)`\r\nuse 12G memory in ubuntu", "@nistarlwc,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and the dataset you are using.\r\n\r\nAlso, please take a look at the [memory profile tool](https://www.tensorflow.org/guide/profiler#memory_profile_summary) and compare the usage on both the operating systems. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44600, "title": "cstdio: No such file or directory", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arduino Create - online IDE\r\n- TensorFlow installed from (source or binary): ARDUINO_TENSORFLOWLITE default library\r\n- Tensorflow version (commit SHA if source): 2.1.0 alpha-precompiled\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano\r\n\r\n**Describe the problem**\r\nUsing the Arduino Create online IDE, example hello_world from the above library, verification step produces the following error:\r\nIn file included from /home/builder/opt/libraries/latest/arduino_tensorflowlite_2_1_0_alpha_precompiled/src/tensorflow/lite/micro/all_ops_resolver.h:16:0,\r\n\r\nfrom /tmp/347074429/hello_world/hello_world.ino:20:\r\n\r\n/home/builder/opt/libraries/latest/arduino_tensorflowlite_2_1_0_alpha_precompiled/src/tensorflow/lite/micro/micro_mutable_op_resolver.h:18:10: fatal error: cstdio: No such file or directory\r\n\r\n#include cstdio\r\n\r\n^~~~~~~~\r\n\r\ncompilation terminated.\r\n\r\nexit status 1", "comments": ["@wwoofbum \r\n\r\nPlease, refer this[ SO link](https://stackoverflow.com/questions/18655168/couldnt-found-cstdio-h),[ link1](https://stackoverflow.com/questions/1023638/correct-order-for-including-both-cstdio-and-stdio-h/1026367#1026367) and see if it helps you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44599, "title": "esp32", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@happychriss can you please elaborate the issue in more detail? \r\n\r\nDo you face any issues running TFLu on ESP32 platforms?", "@happychriss,\r\nIn order to expedite the trouble-shooting process, please fill in the issue template and provide the complete code to reproduce the issue you are facing. Thanks!", "added the tag to early by mistake, this ticket can be closed."]}, {"number": 44598, "title": "AssertionError t.graph == cond_graph using tf.GradientTape", "body": "Hi, \r\n\r\nI posted a [similar question](https://stackoverflow.com/questions/64661822/what-is-the-meaning-of-assertion-error-assert-t-graph-cond-grap-using-tf-gra) on SO, but I am not sure anymore if the problem is on my side or somewhere in TF (hence no MRE).  Similar problem [here ](https://stackoverflow.com/questions/62911595/assertionerror-assert-t-graph-cond-graph-when-generating-heatmap-for-locally). I have been succesfully using `keras.backend.gradients` as for now but I have recently made a switch to `tensorflow.keras`  and since `tf.gradient` is not allowed in eager execution mode, I had to use `tf.GradientTape()`. \r\n\r\nSo basically I changed all calls from this:\r\n\r\n```python\r\ngrads = keras.backend.gradients(loss=target_tensor, variables=[var1, var2, var3])\r\n```\r\nto:\r\n```python\r\nwith tf.GradientTape(persistent=True) as tape:\r\n   # any calulations involving target_tensor, and var1, var2, ...\r\ngrads = tape.gradient(target=target_tensor, sources=[var1, var2, var3])\r\n``` \r\n\r\nand I am getting this on any call (even if target and source is the same):\r\n\r\n```console\r\nc:\\users\\tomas\\miniconda3\\envs\\tftest\\lib\\site-packages\\tensorflow\\python\\ops\\cond_v2.py in _resolve_grad_inputs(cond_graph, grad_graph)\r\n    411     # `cond_graph`.\r\n    412     if t.graph != grad_graph.outer_graph:\r\n--> 413       assert t.graph == cond_graph\r\n    414       # `internal_captures` are not treated as intermediates and hence not added\r\n    415       # to If op outputs. So we get the outer tensor corresponding to those\r\n\r\nAssertionError: \r\n```\r\n\r\nI tried to dig into the TF code but I cannot really tell what is it that I am doing wrong.  There are no problems with toy-examples from docs but I cannot calculate any gradient in my graph. What does this assertion check exactly? How I can resolve this? \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- TensorFlow installed from (source or binary): pip \r\n- TensorFlow version (use command below): 2.3.1, but also 2.2.0\r\n- Python version: 3.8\r\n", "comments": ["@prokotg,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44597, "title": "TensorFlow serving on GPUs using Docker 19.03 needs gpus flag", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tfx/serving/building_with_docker#running_a_container\r\n\r\n\r\n## Description of issue (what needs changing):\r\nSince Docker 19.03, you need to install nvidia-container-toolkit package and then use the --gpus all flag. The docker command to run the container with GPUs needs --gpus=all parameter so that the GPU devices are visible within the container. The command should instead be (if we want to assign all available GPUs to docker)\r\n\r\nsudo docker run -it -p 8500:8500 --gpus all tensorflow/serving:latest-devel-gpu\r\n\r\nIf you want to assign specific device then change it to --gpus device=0\r\n\r\n### Clear description\r\n\r\nIf we don't make this change then users will run into a (cryptic) error, and won't really know how to handle it. \r\n\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-11-02 21:06:24.739827: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@puneith \r\n\r\nThis issue is more suitable for TensorFlow Serving repo. Please post it on Serving repo from [here.](https://github.com/tensorflow/serving/issues) Thanks!", "@ravikyram I've filed the issue here https://github.com/tensorflow/serving/issues/1768"]}, {"number": 44596, "title": "tensorflow.python.framework.errors_impl.UnknownError: Access is denied.", "body": "Hi again!\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary):  binary i think\r\n- TensorFlow version: 2.5.0-dev20201028\r\n- Python version: Python 3.8.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n**Describe the problem**\r\nI can't train the model to recognize custom object.\r\n\r\n```\r\nSo, my structure folder is like these:\r\n-SistemiDigitali\r\n    - ImageCollector\r\n        - images\r\n             - test\r\n                    -images with xml file\r\n             - train\r\n                    -images with xml file\r\n             - test.record\r\n             - test_labels.csv\r\n             - train.record\r\n             - train_labels.csv\r\n    - model (the folder where tensorflow model is located)\r\n    - models-master (folder from git cloning the models repo)\r\n\r\n```\r\n\r\nSo i have changed the pipeline inside model like this:\r\n```\r\n\r\nmodel {\r\n  ssd {\r\n    num_classes: 1\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 320\r\n        width: 320\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: \"ssd_mobilenet_v2_fpn_keras\"\r\n      depth_multiplier: 1.0\r\n      min_depth: 16\r\n      conv_hyperparams {\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 3.9999998989515007e-05\r\n          }\r\n        }\r\n        initializer {\r\n          random_normal_initializer {\r\n            mean: 0.0\r\n            stddev: 0.009999999776482582\r\n          }\r\n        }\r\n        activation: RELU_6\r\n        batch_norm {\r\n          decay: 0.996999979019165\r\n          scale: true\r\n          epsilon: 0.0010000000474974513\r\n        }\r\n      }\r\n      use_depthwise: true\r\n      override_base_feature_extractor_hyperparams: true\r\n      fpn {\r\n        min_level: 3\r\n        max_level: 7\r\n        additional_layer_depth: 128\r\n      }\r\n    }\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    box_predictor {\r\n      weight_shared_convolutional_box_predictor {\r\n        conv_hyperparams {\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 3.9999998989515007e-05\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              mean: 0.0\r\n              stddev: 0.009999999776482582\r\n            }\r\n          }\r\n          activation: RELU_6\r\n          batch_norm {\r\n            decay: 0.996999979019165\r\n            scale: true\r\n            epsilon: 0.0010000000474974513\r\n          }\r\n        }\r\n        depth: 128\r\n        num_layers_before_predictor: 4\r\n        kernel_size: 3\r\n        class_prediction_bias_init: -4.599999904632568\r\n        share_prediction_tower: true\r\n        use_depthwise: true\r\n      }\r\n    }\r\n    anchor_generator {\r\n      multiscale_anchor_generator {\r\n        min_level: 3\r\n        max_level: 7\r\n        anchor_scale: 4.0\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        scales_per_octave: 2\r\n      }\r\n    }\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 9.99999993922529e-09\r\n        iou_threshold: 0.6000000238418579\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n        use_static_shapes: false\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    loss {\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          gamma: 2.0\r\n          alpha: 0.25\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    encode_background_as_zeros: true\r\n    normalize_loc_loss_by_codesize: true\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: false\r\n  }\r\n}\r\ntrain_config {\r\n  batch_size: 128\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    random_crop_image {\r\n      min_object_covered: 0.0\r\n      min_aspect_ratio: 0.75\r\n      max_aspect_ratio: 3.0\r\n      min_area: 0.75\r\n      max_area: 1.0\r\n      overlap_thresh: 0.0\r\n    }\r\n  }\r\n  sync_replicas: true\r\n  optimizer {\r\n    momentum_optimizer {\r\n      learning_rate {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.07999999821186066\r\n          total_steps: 50000\r\n          warmup_learning_rate: 0.026666000485420227\r\n          warmup_steps: 1000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.8999999761581421\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  fine_tune_checkpoint: \"D:/Davide/Uni/SistemiDigitali/model/checkpoint/ckpt-0\"\r\n  num_steps: 50000\r\n  startup_delay_steps: 0.0\r\n  replicas_to_aggregate: 8\r\n  max_number_of_boxes: 100\r\n  unpad_groundtruth_tensors: false\r\n  fine_tune_checkpoint_type: \"detection\"\r\n  fine_tune_checkpoint_version: V2\r\n}\r\ntrain_input_reader {\r\n  label_map_path: \"D:/Davide/Uni/SistemiDigitali/ImageCollector/label_map.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"D:/Davide/Uni/SistemiDigitali/ImageCollector/images/train.record\"\r\n  }\r\n}\r\neval_config {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  use_moving_averages: false\r\n}\r\neval_input_reader {\r\n  label_map_path: \"D:/Davide/Uni/SistemiDigitali/ImageCollector/label_map.pbtxt\"\r\n  shuffle: false\r\n  num_epochs: 1\r\n  tf_record_input_reader {\r\n    input_path: \"D:/Davide/Uni/SistemiDigitali/ImageCollector/images/test.record\"\r\n  }\r\n}\r\n```\r\n\r\n\r\nBut when i run this command:\r\n\r\n`python models-master/research/object_detection/model_main_tf2.py --pipeline_config_path=model/pipeline.config --model_dir=model --alsologtostderr`\r\n\r\nI get this exception\r\n\r\n```\r\nWARNING:tensorflow:model\\checkpoint: Checkpoint ignored\r\nW1104 21:34:18.945756  6844 checkpoint_management.py:296] model\\checkpoint: Checkpoint ignored\r\nTraceback (most recent call last):\r\n  File \"models-master/research/object_detection/model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"C:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"models-master/research/object_detection/model_main_tf2.py\", line 104, in main\r\n    model_lib_v2.train_loop(\r\n  File \"C:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\object_detection-0.1-py3.8.egg\\object_detection\\model_lib_v2.py\", line 630, in train_loop\r\n    manager.save()\r\n  File \"C:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 824, in save\r\n    self._record_state()\r\n  File \"C:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 727, in _record_state\r\n    update_checkpoint_state_internal(\r\n  File \"C:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 247, in update_checkpoint_state_internal\r\n    file_io.atomic_write_string_to_file(coord_checkpoint_filename,\r\n  File \"C:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 573, in atomic_write_string_to_file\r\n    rename(temp_pathname, filename, overwrite)\r\n  File \"C:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 532, in rename\r\n    rename_v2(oldname, newname, overwrite)\r\n  File \"C:\\Users\\Davide\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 548, in rename_v2\r\n    _pywrap_file_io.RenameFile(\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to rename: model\\checkpoint.tmp2ddac8459ee545f28c333f066a5292cc to: model\\checkpoint : Access is denied.\r\n; Input/output error\r\n```", "comments": ["I'm sorry about that. I have actually solved the problem.\r\n\r\nNow i get this output:\r\n\r\n```\r\nINFO:tensorflow:Step 100 per-step time 2.474s loss=0.629\r\nI1105 00:47:38.029456 12864 model_lib_v2.py:649] Step 100 per-step time 2.474s loss=0.629\r\n```\r\nSo i suppose everything is fine. The error was that i didn't understand how everything worked. I had to copy pipeline.config inside a new folder, and this is where the new model will be created. Instead of doing so i edited the pipeline.config of the model i have downloaded from internet and i gave that pipeline.config and folder to the training script.\r\n", "@fanto88,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> I'm sorry about that. I have actually solved the problem.\r\n> \r\n> Now i get this output:\r\n> \r\n> ```\r\n> INFO:tensorflow:Step 100 per-step time 2.474s loss=0.629\r\n> I1105 00:47:38.029456 12864 model_lib_v2.py:649] Step 100 per-step time 2.474s loss=0.629\r\n> ```\r\n> \r\n> So i suppose everything is fine. The error was that i didn't understand how everything worked. I had to copy pipeline.config inside a new folder, and this is where the new model will be created. Instead of doing so i edited the pipeline.config of the model i have downloaded from internet and i gave that pipeline.config and folder to the training script.\r\n\r\nI have this issue and can't figure it out can you please explain how you solved it, as i am confused by your file structure "]}, {"number": 44595, "title": "How to implement Bidirectional GRU in Encoder", "body": "I am currently trying to implement a bidirectional GRU within an Encoder class:\r\n\r\n```\r\ndef gru_cell(enc_units):\r\n  return tf.keras.layers.GRUCell(enc_units, recurrent_initializer='glorot_uniform')\r\n\r\nclass Encoder(tf.keras.Model):\r\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\r\n    super(Encoder, self).__init__()\r\n    self.batch_sz = batch_sz\r\n    self.enc_units = enc_units\r\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n    self.bid_gru = tf.keras.layers.Bidirectional(gru_cell(self.enc_units))\r\n\r\n   \r\n\r\n  def call(self, x, hidden):\r\n    x = self.embedding(x)\r\n    concatenated, forward, backward = self.bid_gru(x, initial_state=[hidden, hidden])\r\n    return concatenated, forward, backward\r\n\r\n  def initialize_hidden_state(self):\r\n    return tf.zeros((self.batch_sz, self.enc_units))\r\n```\r\nBut I am getting the following Key Error: 'go_backwards'\r\n![image](https://user-images.githubusercontent.com/43485111/98159495-ad788e00-1f17-11eb-94e9-3e11c792e065.png)\r\nHow can I resolve this?", "comments": ["@arunraja-hub,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44594, "title": "2.4-rc1 cherry-pick request: Improve mixed precision docstrings", "body": "This change only affects docstrings and greatly improves the mixed precision documentation, so it should be cherrypicked.", "comments": []}, {"number": 44593, "title": "Specialize sources for target and optimized kernels independently.", "body": "This change:\r\n * adds a new command line option to the makefile called OPTIMIZED_KERNEL_DIR which should be used instead of TAGS\r\n\r\n * TAGS is currently supported for backwards compatibility and a gradual roll-out but will be dropped (for optimized kernels) in follow-up changes.\r\n\r\n * Makes the necessary change to use OPTIMIZED_KERNEL_DIR for xtensa_hifimini.\r\n\r\nFirst step towards #44566\r\nFixes internal bug: http://b/160795217\r\nAddresses internal bug: http://b/170501366\r\n\r\nTested the keyword_benchmark with different combinations command line options.\r\n\r\nWe used this command\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa_hifimini  XTENSA_CORE=<core> test_keyword_benchmark\r\n```\r\nalong with these additional options:\r\n|   options                | InitializeKeywordRunner | KeywordRunNIerations(1) | KeywordRunNIerations(10) |\r\n|        ------------------ | ---------------------- | ------------------------- | ------------------------- |\r\n(on master) <br> `TAGS=xtensa_hifimini`        | 1389000 | 89318 | 892739 |\r\n`TAGS=xtensa_hifimini`                                    | 1389056 | 89318 | 892739 |\r\n`OPTIMIZED_KERNEL_DIR=xtensa_hifimini` | 1389056 | 89318 | 892739 |\r\n`OPTIMIZED_KERNEL_DIR=none`                 |  226514| 1010031 | 10088115 |\r\n\r\nThe table is showing that:\r\n * we support `TAGS=xtensa_hifimini` as well as `OPTIMIZED_KERNEL_DIR=xtensa_hifimini` to pull in the optimized implementations.\r\n * `OPTIMIZED_KERNEL_DIR=none` uses the reference kernels.\r\n * There is a difference of 56 cycles in the initialization with and without this change (first two lines of the table). This can be chalked up to some small difference in how a potentially difference sequence of object files is handled.\r\n\r\nWe also see a small difference in the binary size:\r\n```\r\n xt-size tensorflow/lite/micro/tools/make/gen/xtensa_hifimini_xtensa_hifimini/bin/keyword_benchmark\r\n```\r\nOn master:\r\n```\r\n   text\t   data\t    bss\t    dec\r\n  54864\t  48680\t  25032\t 128576\r\n```\r\n\r\nWith this change (and either `TAGS=xtensa_hifimini` or `OPTIMIZED_KERNEL_DIR=xtensa_hifimini`):\r\n```\r\n   text\t   data\t    bss\t    dec\r\n  54864\t  48664\t  25032\t 128560\t\r\n```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44592, "title": "Make timeseries_dataset_from_array() more intuitive", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n``timeseries_dataset_from_array()`` was introduced to replace ``tf.keras.preprocessing.sequence.TimeseriesGenerator``. But it is not a drop in replacement. In fact, as I will show below, it takes a bit of work to make ``timeseries_dataset_from_array()`` behave like ``TimeseriesGenerator``. Issue #40343 was created regarding this. But the resolution and suggested work around are problematic. This is why I am opening this issue.\r\n\r\nLet's explain the issues with an example.\r\n\r\n```python\r\nfeatures = np.arange(1, 10)\r\nlabels = features * -1\r\n\r\nprint(\"Features:\", features)\r\nprint(\"Labels:\", labels)\r\n```\r\n\r\nThis prints:\r\n\r\n```\r\nFeatures: [1 2 3 4 5 6 7 8 9]\r\nLabels: [-1 -2 -3 -4 -5 -6 -7 -8 -9]\r\n```\r\n\r\nWe can use ``TimeseriesGenerator`` as follows.\r\n\r\n```python\r\nsequence_length=3\r\n\r\ngen = tf.keras.preprocessing.sequence.TimeseriesGenerator(features, labels, sequence_length, batch_size=1)\r\n\r\nfor inputs, targets in gen:\r\n  print(\"Input:\", inputs, \"Target:\", targets)\r\n```\r\n\r\nThis correctly prints:\r\n\r\n```\r\nInput: [[1 2 3]] Target: [-4]\r\nInput: [[2 3 4]] Target: [-5]\r\nInput: [[3 4 5]] Target: [-6]\r\nInput: [[4 5 6]] Target: [-7]\r\nInput: [[5 6 7]] Target: [-8]\r\nInput: [[6 7 8]] Target: [-9]\r\n```\r\n\r\nNow, let's use ``timeseries_dataset_from_array()`` as a drop in replacement.\r\n\r\n```python\r\nds = tf.keras.preprocessing.timeseries_dataset_from_array(features, labels, sequence_length, batch_size=1)\r\n\r\nfor inputs, targets in ds:\r\n  print(\"Input:\", inputs.numpy(), \"Target:\", targets.numpy())\r\n```\r\n\r\nThis prints:\r\n\r\n```\r\nInput: [[1 2 3]] Target: [-1]\r\nInput: [[2 3 4]] Target: [-2]\r\nInput: [[3 4 5]] Target: [-3]\r\nInput: [[4 5 6]] Target: [-4]\r\nInput: [[5 6 7]] Target: [-5]\r\nInput: [[6 7 8]] Target: [-6]\r\nInput: [[7 8 9]] Target: [-7]\r\n```\r\n\r\nThis is not expected. To be honest, I am not quite sure in what scenarios one would want the above behavior.\r\n\r\nThe [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array) shows a way around this. We can follow the \"Example 2\" in the documentation and re-write our test like this:\r\n\r\n```python\r\nds = tf.keras.preprocessing.timeseries_dataset_from_array(features[:-sequence_length], labels[sequence_length:], sequence_length, batch_size=1)\r\n\r\nfor inputs, targets in ds:\r\n  print(\"Input:\", inputs.numpy(), \"Target:\", targets.numpy())\r\n```\r\n\r\nThis prints:\r\n\r\n```\r\nInput: [[1 2 3]] Target: [-4]\r\nInput: [[2 3 4]] Target: [-5]\r\nInput: [[3 4 5]] Target: [-6]\r\nInput: [[4 5 6]] Target: [-7]\r\n```\r\n\r\nThis is good. But we are now missing the last two sequences:\r\n\r\n```\r\nInput: [[5 6 7]] Target: [-8]\r\nInput: [[6 7 8]] Target: [-9]\r\n```\r\n\r\nIssue #40343 suggests the use of ``np.roll()`` as a work around. The resolution comment says that ``np.roll()`` was added to the documentation. But I fail to see it there. Also the way ``np.roll()`` is used in that issue seems problematic. Below we re-write our test using the way I think ``np.roll()`` should be used.\r\n\r\n```python\r\nds = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n  features[:-1], np.roll(labels, -sequence_length)[:-1], sequence_length, batch_size=1)\r\n\r\nfor inputs, targets in ds:\r\n  print(\"Input:\", inputs.numpy(), \"Target:\", targets.numpy())\r\n```\r\n\r\nThis prints:\r\n\r\n```\r\nInput: [[1 2 3]] Target: [-4]\r\nInput: [[2 3 4]] Target: [-5]\r\nInput: [[3 4 5]] Target: [-6]\r\nInput: [[4 5 6]] Target: [-7]\r\nInput: [[5 6 7]] Target: [-8]\r\nInput: [[6 7 8]] Target: [-9]\r\n```\r\n\r\nThis is now functionally equivalent to ``TimeseriesGenerator``.\r\n\r\n**Will this change the current api? How?**\r\nThe issue can be addressed either by enhancing the documentation or by enhancing the API. I will prefer the later. Perhaps a flag can be supplied to ``timeseries_dataset_from_array()`` that will make it behave like ``TimeseriesGenerator``.\r\n\r\n**Who will benefit with this feature?**\r\nCurrently ``timeseries_dataset_from_array()`` requires a bit of work to make it work for what I will consider as most common time series problems. This makes the API unintuitive. At minimum the documentation needs to be enhanced. It will be better if the API can be changed so one can easily use it in most common scenarios.\r\n\r\n**Any Other info.**\r\n", "comments": ["@bibhas2 \r\n\r\nPlease feel free to submit a PR if you have use cases that supports that feature.Thanks!", "I had a chance to think more about this and look at several projects on the Internet. I think what we need is a higher level API that is quick to use in most time series problems. \r\n\r\nThis [excellent tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series) from the TF documentation had to write good bit of code around ``timeseries_dataset_from_array`` to make it work. At that point I wonder if we are getting much value out of ``timeseries_dataset_from_array`` or not. \r\n\r\nWe are also seeing projects like [this](https://github.com/anencore94/SlidingWindowGenerator/blob/master/slidingwindow_generator/slidingwindow_generator.py) coming up to put a friendly face around the API. As you can see, the work there is non-trivial.\r\n\r\nI will be inclined to keep using ``tf.keras.preprocessing.sequence.TimeseriesGenerator`` as long as it is not fully deprecated. But, sadly it does not support multi-step label output. An [issue](https://github.com/keras-team/governance/pull/18) was opened to address this gap. But the issue has been closed suggesting the use of ``timeseries_dataset_from_array`` as an alternative.", "Hi mate,\r\n\r\nThanks for the comments. Do remark that there's a timeseries regression example right there in the docstring:\r\n\r\n```python\r\ninput_data = data[:-10]\r\ntargets = data[10:]\r\ndataset = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n    input_data, targets, sequence_length=10)\r\nfor batch in dataset:\r\n  inputs, targets = batch\r\n  assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]\r\n  assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10\r\n  break\r\n```\r\n\r\nNow, you seem to have found this workflow on your own, and you note:\r\n\r\n> But we are now missing the last two sequences\r\n\r\nYou're right, that's weird. There's actually no need on our part to force the input data and targets to have the same length. We can just use `min(num_seqs, len(targets))` where `num_seqs` is the number of sub sequences that can be generated from the data.\r\n\r\nSo I'll just go ahead and do that.", "Done", "I am surprised they closed, If they purpose of this API is to make sliding window, then why do they have a whole nine yard of data type checking and normalization calls in between ? This is make it not usable when we use with feature classification.", "I see that the issue is now closed. It's not clear from the comment from @fchollet what the proposed resolution is.\r\n\r\nAnyone serious about time series data should avoid ``timeseries_dataset_from_array()``. For two main reasons:\r\n\r\n- The API needs a lot of work arounds (as explained in my original issue description) to be actually useful.\r\n- It works with in-memory Numpy array and hence not well suited for very large amounts of data.\r\n\r\nI recommend using [DataSet.window()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window). See Approach 5 in [this blog post](https://mobiarch.wordpress.com/2020/11/13/preparing-time-series-data-for-rnn-in-tensorflow/) on how to do this.", "It is bewildering to me why this issue was closed. I agree that the way timeseries_dataset_from_array current works is not user-friendly at all, which was the entire intent of this function and to be honest the entire TensorFlow data pipeline. If I have to implement each step myself then I do not need a pipeline anymore. Also, IMHO, having to go through alternative routes makes me question whether I would derive any benefit at all from implementing the TensorFlow pre-processing pipeline (for input data that fit into memory) because the flow would become severely bottlenecked by the work-around. ", "The function `tf.keras.preprocessing.timeseries_dataset_from_array` is slow aswell, if you make the window in numpy, and pass the batch_size in the fit method of the model, is faster"]}, {"number": 44591, "title": "BUG!! tf.keras.model.summary() output is wrong", "body": "**System information**\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.5\r\n- GPU model and memory: Nvidia GeForce RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\n```\r\nclass DenseNormLayer(tf.keras.layers.Layer):\r\n    def __init__(self, units, activation, name=None):\r\n        super(DenseNormLayer, self).__init__(name=name)\r\n        self.dense_layer = tf.keras.layers.Dense(units=units, activation=activation)\r\n        self.batch_norm = tf.keras.layers.BatchNormalization()\r\n    \r\n    def call(self, x):\r\n        x = self.batch_norm(x)\r\n        x = self.dense_layer(x)\r\n        return x\r\n```\r\n\r\n```\r\nclass BaselineModel(tf.keras.Model):\r\n    def __init__(self, targets, name=\"BaselineModel\"):\r\n        super(BaselineModel, self).__init__(name=name)\r\n        self.block1 = DenseNormLayer(units=1024, activation=\"relu\", name=\"block1\")\r\n        self.block2 = DenseNormLayer(units=1024, activation=\"relu\", name=\"block2\")\r\n        self.d1 = tf.keras.layers.Dense(units=512, activation=\"relu\", name=\"d1\")\r\n        self.d2 = tf.keras.layers.Dense(units=1024, activation=\"relu\", name=\"d2\")\r\n        self.dp = tf.keras.layers.Dropout(rate=0.2, name=\"dropout_layer\")\r\n        self.sigmoid_layer = tf.keras.layers.Dense(units=targets, activation=\"sigmoid\", name=\"sigmoid_layer\")\r\n    \r\n    def call(self, X):\r\n        x = self.block1(X)\r\n        x = self.block2(x)\r\n        x = self.d1(x)\r\n        x = self.dp(x)\r\n        x = self.d2(x)\r\n        x = self.sigmoid_layer(x)\r\n        return x\r\n    \r\n    def build_graph(self, dim):\r\n        x = tf.keras.Input(shape=dim)\r\n        return tf.keras.Model(inputs=[x], outputs=self.call(x))\r\n```\r\n\r\nOutput summary:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nblock1 (DenseNormLayer)      multiple                  103824    \r\n_________________________________________________________________\r\nblock2 (DenseNormLayer)      multiple                  1053696   \r\n_________________________________________________________________\r\nd1 (Dense)                   multiple                  524800    \r\n_________________________________________________________________\r\nd2 (Dense)                   multiple                  525312    \r\n_________________________________________________________________\r\ndropout_layer (Dropout)      multiple                  0         \r\n_________________________________________________________________\r\nsigmoid_layer (Dense)        multiple                  10250     \r\n=================================================================\r\nTotal params: 2,217,882\r\nTrainable params: 2,215,634\r\nNon-trainable params: 2,248\r\n_________________________________________________________________\r\n```\r\n\r\nRun this to get summary:\r\n```\r\ntest_model = BaselineModel(targets=100)\r\ntest_model.build(input_shape=(None, 100))\r\ntest_model.summary()\r\n```\r\nAs one can see in above summary, the `dropout_layer` is present below `d2` layer which is incorrect. It should be in between of `d1` and `d2` layers.\r\nI tried cleaning python cache and also tried in Jupyter Notebook but no luck.\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen plotting with the code below, output is correct.\r\n```\r\nimport os\r\ntf.keras.utils.plot_model(\r\n    test_model.build_graph(dim=100), to_file=os.path.join(\".\", \"model.png\"),\r\n    dpi=96, show_shapes=True, show_layer_names=True, expand_nested=False\r\n)\r\n````\r\n![image](https://user-images.githubusercontent.com/23133817/98150766-303c2180-1ef5-11eb-8c73-2d5cdbf38ebf.png)\r\n\r\n", "comments": ["@nityansuman,\r\nOn subclassing the Model, I am facing an error stating `TypeError: __init__() missing 1 required positional argument: 'targets'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/595a785eec860a3211cf1b1d8c56112e/44591.ipynb).\r\n\r\nCould you please provide the complete code to reproduce the issue reported here? Thanks!", "@amahendrakar Updated the code snippet to successfully build model and get summary.\r\n\r\nUse the below snippet to build model and get summary.\r\n```\r\ntest_model = BaselineModel(targets=100)\r\ntest_model.build(input_shape=(None, 100))\r\ntest_model.summary()\r\n```\r\n\r\n_Note: Summary is only available once model has seen inputs._", "@amahendrakar Any updates on this ?", "@nityansuman,\r\nSorry for the delayed response. \r\n\r\n>         self.d2 = tf.keras.layers.Dense(units=1024, activation=\"relu\", name=\"d2\")\r\n>         self.dp = tf.keras.layers.Dropout(rate=0.2, name=\"dropout_layer\")\r\n\r\nInterchanging the above two layers gives the expected output. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/03578c94918d33230da3b570437d14c5/44591-v2.ipynb). Thanks!", "@amahendrakar \r\nHow is model summary getting effected with the order of layer initialisation ??? Weird behaviour. Layers are initialised dynamically at build and even though model graph is correct irrespective of the order, model summary is not. ", "@amahendrakar Why is model summary taking information from constructor rather than the call() method? Behaviour is wrong. Model summary displays the layer configuration and layer connection happens in call() method and not constructor.", "This is normal behavior. Your model is a subclassed model; such models don't have a graph structure (they're a blob of bytecode). As a result the layer ordering in `summary()` just reflects creation order.\r\n\r\nIf you want your model to have an inspectable graph structure (and hence an order-respecting summary), use the Functional API (e.g. `Model(inputs, outputs)`) to define your model.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44591\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44591\">No</a>\n", "@fchollet Interesting!! Thanks for the response.\r\n\r\n> This is normal behavior. Your model is a subclassed model; such models don't have a graph structure (they're a blob of bytecode). As a result the layer ordering in `summary()` just reflects creation order.\r\n> \r\n> If you want your model to have an inspectable graph structure (and hence an order-respecting summary), use the Functional API (e.g. `Model(inputs, outputs)`) to define your model.\r\n\r\n@fchollet Interesting!! Thanks for the response."]}, {"number": 44589, "title": "Introduce MHLO_BUILD_EMBEDDED build option", "body": "This option allows to skip calling `find_package(MLIR)`, enabling to\r\nembed MHLO into other project, e.g. IREE.", "comments": ["@GMNGeoffrey: FYI\r\n"]}, {"number": 44588, "title": "Add ESP32 examples build test", "body": "Following examples will be generated and built for ESP32 platform\r\n1. hello_world\r\n2. person_detection\r\n3. micro_speech\r\n\r\nJust the builds are tested for now\r\n\r\nAddresses https://github.com/tensorflow/tensorflow/issues/43450\r\n\r\nThis change adds a script for ESP32 but does not enable it as part of the Tensorflow CI. Enabling a continuous build and a build badge will be a separate change.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@vikramdattu  Can you please check @advaitjain's comments and keep us posted ? Thanks!", "> @vikramdattu Can you please check @advaitjain's comments and keep us posted ? Thanks!\r\n\r\nHi @advaitjain @gbaned this is addressed, PTAL."]}, {"number": 44587, "title": "Behaviour of ImageDataGenerator with subset='validation'", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow\r\n\r\n## Description of issue:\r\n\r\nThe documentation for `keras.preprocessing.image.ImageDataGenerator` doesn't specify how it handles validation data.\r\n\r\n### Clear description\r\n\r\nIn particular, documentation for the `.flow()` method doesn't specify whether transformations are applied to augment validation data, when `validation_split` has been given in the constructor and `subset='validation'` is passed.\r\n\r\n### Usage example\r\n\r\nThere are no usage examples that use the parameter `validation_split` with a previously undivided dataset. All examples employ a dataset that is already divided in training/validation and then create two `ImageDataGenerator` objects with different parameters. \r\n\r\n### Other\r\n\r\nI have read the [source code](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/preprocessing/image.py#L808) in order to understand this behaviour. However, I end up in the definition of [NumpyArrayIterator](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/preprocessing/image.py#L400), a class that apparently inherits from itself, and I haven't been able to find its actual implementation or the meaning of this strange inheritance.", "comments": ["It seems that `ImageDataGenerator` actually inherits from the implementation in the `keras_preprocessing` module, which is [imported](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/preprocessing/image.py#L24) in this implementation.\r\n\r\nFrom what I've seen in that package's source code, I'd think that it doesn't distinguish between training and validation subsets, and applies transformations to both."]}, {"number": 44586, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "\r\n![image](https://user-images.githubusercontent.com/19899607/98122360-7b930780-1ed6-11eb-8def-ae981ef57dcb.png)\r\n\r\nGetting this error while importing TensorFlow.\r\n\r\nIt installed successfully.\r\n\r\nCPU : Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz\r\nOS : Windows server 2016\r\npython: 3.7.6\r\nTensorwFlow : 2.3.0 and 2.2.0 both gives same error\r\n\r\nChecked parameters : \r\n-Installed Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019\r\n-msvcp140_1.dll present\r\n-AVX is enabled\r\n\r\nTensorFlow 2.0.0 already working on this system.\r\nThis server doesn't have internet access so to upgrade Tensorflow I have tried 2 ways.\r\n-Create conda environment pack in local machine and copy and run environment.\r\n-Download .whl files using pip in local machine and installed it on a remote server\r\n\r\nIn both these ways, I am able to install tensorflow successfully but while importing it gives this error.\r\nSo, Please suggest how can I upgrade TensorFlow 2.0.0 to 2.3.0\r\n\r\n \r\n", "comments": ["@parthshahengg,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!", "I can do this.\r\nBut I don't think this is related to anaconda. \r\nBecause I already tried this without anaconda with pip only. But getting the same error.", "Issue still not fixed", "@parthshahengg,\r\nCould you please uninstall TensorFlow and Python, restart the machine and check if a fresh install works? \r\n\r\nAnd make sure you have all the required dependencies as mentioned [here](https://www.tensorflow.org/install/pip#system-requirements). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44586\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44586\">No</a>\n"]}, {"number": 44584, "title": "Add several fixes for s390x architecture and other Big-Endian systems", "body": "The patch contains the following changes:\r\n1. Fix a typo in `tensorflow/compiler/xla/literal.cc`, as the annotation suggests, it should be modulo not division.\r\n2. Disable `MKLDNN_CONTRACTION_KERNEL` build for s390x, as it is not supported on s390x.\r\n3. Select the test file with correct endianness for `tensorflow/core/kernels/data/experimental/lmdb_dataset_op_test.cc`, since the big endian version test file is already available in the repo.\r\n4. Add support for s390x with cpuinfo package. s390x is not officially supported by cpuinfo, so only include the common sources.", "comments": []}, {"number": 44583, "title": "Could not load dynamic library 'libcudart.so.11.0'", "body": "Anybody can help me to solve this problem:\r\n![image](https://user-images.githubusercontent.com/37329790/98122003-fcf49500-1eea-11eb-8809-c068104943a5.png)\r\n\r\n\r\nInfos:\r\n    \r\n1. tf_nightly_gpu-2.4.0.dev20201023\r\n2.     cuda version:  11.1\r\n3.     cudnn version :  8.0.4\r\n\r\nthese question cause:  can't not use GPU to train my model, Thank's all of you!", "comments": ["@ZYG151541155 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nPlease paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.", "> @ZYG151541155\r\n> We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n> \r\n> Please paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.\r\n\r\nOh, Thank's, I get it and already solve this question, the reasons are all environmental mistakes.", "@ZYG151541155 \r\nThank you for your update, as the issue is resolved please move this to closed status,", "Moving this to closed status as resoved."]}, {"number": 44582, "title": "flex delegate cross-compilation failed for riscv because of boringssl", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit 218caef97666384211ea9171dc6029ec3ec7bdd6\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): using riscv-unknown-linux-gnu-gcc 9.2.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\nI want to cross-compile tflite with flex delegate for riscv ISA\r\nWhile I build tensorflow/lite/delegates/flex:delegate I got error from BoringSSL like below\r\n```\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=179\r\nINFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/jhjang/.virtualenvs/ml/bin/python3 --action_env PYTHON_LIB_PATH=/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages --python_path=/home/jhjang/.virtualenvs/ml/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:monolithic in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:linux in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Build options --compilation_mode, --cpu, --crosstool_top, and 3 more have changed, discarding analysis cache.\r\nINFO: Analyzed target //tensorflow/lite/tools:list_flex_ops_no_kernel_main (0 packages loaded, 746 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/tools:list_flex_ops_no_kernel_main up-to-date:\r\n  bazel-bin/tensorflow/lite/tools/list_flex_ops_no_kernel_main\r\nINFO: Elapsed time: 0.364s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\n~/ml-accelerator/tensorflow/tmp ~/ml-accelerator/tensorflow\r\nWARNING: The following configs were expanded more than once: [noaws]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=179\r\nINFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/jhjang/ml-accelerator/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/jhjang/.virtualenvs/ml/bin/python3 --action_env PYTHON_LIB_PATH=/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages --python_path=/home/jhjang/.virtualenvs/ml/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:noaws in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:noaws in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nohdfs in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:riscv64 in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --crosstool_top=@local_config_riscv64//:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=riscv64\r\nINFO: Found applicable config definition build:linux in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/jhjang/ml-accelerator/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Build options --compilation_mode, --cpu, --crosstool_top, and 3 more have changed, discarding analysis cache.\r\nINFO: Analyzed target //tmp:tensorflow-lite-select-tf-ops (1 packages loaded, 16446 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/jhjang/.cache/bazel/_bazel_jhjang/8a72e618241ede75353c494f725f5447/external/boringssl/BUILD:130:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1)\r\nIn file included from external/boringssl/src/include/openssl/asn1.h:61,\r\n                 from external/boringssl/src/crypto/x509/a_sign.c:57:\r\nexternal/boringssl/src/include/openssl/base.h:122:2: error: #error \"Unknown target CPU\"\r\n  122 | #error \"Unknown target CPU\"\r\n      |  ^~~~~\r\nIn file included from external/boringssl/src/include/openssl/asn1.h:68,\r\n                 from external/boringssl/src/crypto/x509/a_sign.c:57:\r\nexternal/boringssl/src/include/openssl/bn.h:165:2: error: #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n  165 | #error \"Must define either OPENSSL_32_BIT or OPENSSL_64_BIT\"\r\n      |  ^~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:219:44: error: unknown type name 'BN_ULONG'\r\n  219 | OPENSSL_EXPORT int BN_set_word(BIGNUM *bn, BN_ULONG value);\r\n      |                                            ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:309:16: error: unknown type name 'BN_ULONG'\r\n  309 | OPENSSL_EXPORT BN_ULONG BN_get_word(const BIGNUM *bn);\r\n      |                ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:377:43: error: unknown type name 'BN_ULONG'\r\n  377 | OPENSSL_EXPORT int BN_add_word(BIGNUM *a, BN_ULONG w);\r\n      |                                           ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:390:43: error: unknown type name 'BN_ULONG'\r\n  390 | OPENSSL_EXPORT int BN_sub_word(BIGNUM *a, BN_ULONG w);\r\n      |                                           ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:399:44: error: unknown type name 'BN_ULONG'\r\n  399 | OPENSSL_EXPORT int BN_mul_word(BIGNUM *bn, BN_ULONG w);\r\n      |                                            ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:417:16: error: unknown type name 'BN_ULONG'\r\n  417 | OPENSSL_EXPORT BN_ULONG BN_div_word(BIGNUM *numerator, BN_ULONG divisor);\r\n      |                ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:417:56: error: unknown type name 'BN_ULONG'\r\n  417 | OPENSSL_EXPORT BN_ULONG BN_div_word(BIGNUM *numerator, BN_ULONG divisor);\r\n      |                                                        ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:434:49: error: unknown type name 'BN_ULONG'\r\n  434 | OPENSSL_EXPORT int BN_cmp_word(const BIGNUM *a, BN_ULONG b);\r\n      |                                                 ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:448:53: error: unknown type name 'BN_ULONG'\r\n  448 | OPENSSL_EXPORT int BN_abs_is_word(const BIGNUM *bn, BN_ULONG w);\r\n      |                                                     ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:457:49: error: unknown type name 'BN_ULONG'\r\n  457 | OPENSSL_EXPORT int BN_is_word(const BIGNUM *bn, BN_ULONG w);\r\n      |                                                 ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:516:16: error: unknown type name 'BN_ULONG'\r\n  516 | OPENSSL_EXPORT BN_ULONG BN_mod_word(const BIGNUM *a, BN_ULONG w);\r\n      |                ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:516:54: error: unknown type name 'BN_ULONG'\r\n  516 | OPENSSL_EXPORT BN_ULONG BN_mod_word(const BIGNUM *a, BN_ULONG w);\r\n      |                                                      ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:629:48: error: unknown type name 'BN_ULONG'\r\n  629 | OPENSSL_EXPORT int BN_rand_range_ex(BIGNUM *r, BN_ULONG min_inclusive,\r\n      |                                                ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:921:52: error: unknown type name 'BN_ULONG'\r\n  921 | OPENSSL_EXPORT int BN_mod_exp_mont_word(BIGNUM *r, BN_ULONG a, const BIGNUM *p,\r\n      |                                                    ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:960:3: error: unknown type name 'BN_ULONG'\r\n  960 |   BN_ULONG *d;\r\n      |   ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:991:3: error: unknown type name 'BN_ULONG'\r\n  991 |   BN_ULONG n0[2];  // least significant words of (R*Ri-1)/N\r\n      |   ^~~~~~~~\r\nexternal/boringssl/src/include/openssl/bn.h:994:42: error: unknown type name 'BN_ULONG'\r\n  994 | OPENSSL_EXPORT unsigned BN_num_bits_word(BN_ULONG l);\r\n      |                                          ^~~~~~~~\r\nTarget //tmp:tensorflow-lite-select-tf-ops failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.357s, Critical Path: 0.31s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n*Can I somehow exclude boringssl from compilation?*\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI am modifying `tensorflow/lite/tools/build_aar.sh for riscv\r\nThe error is from `execute generate_flex_aar`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Flex delegate requires to build TF kernels and I'm not sure if TF is buildable with RISC-V.\r\nBTW, the build error you have looks strange. When I checked, BoringSSL does support 64bit mips.\r\nYou'd better if your toolchain defines \\_\\_mips__ and \\_\\_LP64__\r\n\r\nAlso I'm curious if you setup RISC-V toolchain configuration for Bazel. If you did, can you share it?", "Thanks for reply\r\n\r\nI figured it out by adding below to `bazel-tensorflow/external/boringssl/src/include/openssl/base.h`\r\n```\r\n#elif defined(__riscv__) || defined(__riscv)\r\n#if __riscv_xlen == 32\r\n#define OPENSSL_32_BIT\r\n#elif __riscv_xlen == 64\r\n#define OPENSSL_64_BIT\r\n```\r\n\r\nAbout my riscv toolchain configuration for bazel, you can visit my repo [here](https://github.com/junhyk/tensorflow)\r\nIt's not updated since August, though.\r\nYou can also refer the issue I made #41960", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44582\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44582\">No</a>\n"]}, {"number": 44581, "title": "Cadence xtensa_hifi kernels update", "body": "The following three sets of changes are included:\r\n\r\n1. Update the xtensa_hifi kernels to latest reference implementation and use the new TfLiteEvalTensor API.    \r\n    This fixes the build issues for HiFi 4, HiFi 3z, Fusion F1 configurations.\r\n    Fixes #43912\r\n\r\n2. Integrate the HiFi 5 Neural Network Library v1.2.1 -\r\n  Use HiFi 5 NN Library kernels for following operators:\r\n      - Convolution\r\n      - SVDF\r\n      - Average pool\r\n      - Relu\r\n      - Quantize\r\n\r\n3. Change the Cadence copyright header to a non restrictive one for the\r\n    integration code files in xtensa_hifi and xtensa_hifimini_staging folders.\r\n    Fixes #43467", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@pnikam-cad  Can you please resolve conflicts? Thanks!", "> @pnikam-cad Can you please resolve conflicts? Thanks!\r\n\r\nplease ignore this comment. Let's keep this PR as is without any updates.", "A note for anyone wondering about the status of this PR.\r\n\r\nWe (@advaitjain, @pnikam-cad) have discussed and this PR will not be reviewed or merged.\r\n\r\nInstead, it is going to be broken down into smaller changes that will be individually reviewed and merged (for example https://github.com/tensorflow/tensorflow/pull/46411).\r\n\r\n While that process is underway, the PR will be kept open. We will close this PR when we are ready to also delete the micro/kernels/xtensa_hifi directory."]}, {"number": 44580, "title": "Add more requant methods", "body": "1. Added more versions of requant\r\n2. Fixed some warnings for ESP32 build", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> Since the uint8 is deprecated (https://www.tensorflow.org/lite/performance/quantization_spec), we would like to avoid adding support unless there is a strong reason for this.\r\n> \r\n> If you can provide more context (via a github issue) on why you need uint8 requantization then that would help figure out the right path forward.\r\n> \r\n> And it would be great if you could separate the ESP32 build warning fixes into a separate pull request, that would be great.\r\n\r\n\r\n@advaitjain I have moved both commits to separate PRs.\r\n\r\nAbout uint8 deprecation, I think I have totally missed this. And since my model used uint8 input/output, it added uint8->int8 and int8->uint8 quantization, which obviously is not present in current implementation.\r\n\r\nI am moving this PR to draft.", "@advaitjain I think we still need requant methods?\r\n\r\nRaw images are uint8_t in nature. Pixel value ranging from [0, 255]!\r\nDon't we need a requant (uint8-->int8) at the very first step?\r\n\r\nGetting confused here!\r\n\r\nI use following lines in my model conversion code:\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nmodel_tflite = converter.convert()\r\n```\r\n\r\nThe reason is simple. Images are in fact uint8 pixels and it will need uint8-->int8 conversion before been feed to other OPs.", "You're right, what is deprecated is asymmetric quantization (https://github.com/tensorflow/tensorflow/issues/44912). Accepting uint8 as model inputs, for example via images as you describe should still be supported.\r\n\r\nYour particular use-case makes sense, Let's add for only this specific case of input == uint8, output == int8 and  document the reason.\r\n\r\nWhat I want to avoid is general uint8 support (e.g. taking an int8 input and making it uint8 which then means that the ops following will need to support asymmetric quantization).", "@advaitjain thanks.\r\nThe other version is also needed IMHO.\r\n\r\nThe output if has a range value should be in [0, 255] i.e., in uint8 format and not int8.\r\nHaving int8-->uint8 version helps that.\r\n\r\nOf course interpreting int8 output is option but having [int8->uint8] version seems much cleaner.", "If it works for you then let's have the output type for the model be `int8` instead of `uint8` and see how far we can go.", "@vikramdattu  This PR is in draft, any update on this? Please. Thanks!", "> @vikramdattu This PR is in draft, any update on this? Please. Thanks!\r\n\r\nAll of the changes are either merged via separate PRs or are to be merged. (Except int8_t->uint8_t requant, which this PR contains. And as per discussions we want to avoid it).\r\n\r\n@gbaned this MR will be closed. Will do it once a pending PR https://github.com/tensorflow/tensorflow/pull/45193 is merged."]}, {"number": 44579, "title": "Floating-point models", "body": "Can tensorflow Lite for Microcontrollers run fp models or just quantized ones? if so, are there examples to run fp models?", "comments": ["@peter197321 \r\ntensorflow Lite for Microcontrollers run just quantized, as this is not a bug or feature request please open this issue at [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) for any further queries.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44578, "title": "tf.data.dataset.batch returns different shape depending on drop_remainder parameter", "body": "Hi,\r\nI notice that when drop_reminder is not set in tf.data.dataset.batch , the dataset returned as None shape, but if drop_reminder is set, the batch_size is set as the shape of the returned dataset:\r\n\r\n**With drop_remainder=False:**\r\n\r\n`train_ds = train_ds.batch(32) \r\n\r\nprint(train_ds) \r\n\r\n_<BatchDataset shapes: ({age: (None,), sex: (None,), cp: (None,), trestbps: (None,), chol: (None,), fbs: (None,), restecg: (None,), thalach: (None,), exang: (None,), oldpeak: (None,), slope: (None,), ca: (None,), thal: (None,)}, (None,)), types: ({age: tf.int64, sex: tf.int64, cp: tf.int64, trestbps: tf.int64, chol: tf.int64, fbs: tf.int64, restecg: tf.int64, thalach: tf.int64, exang: tf.int64, oldpeak: tf.float64, slope: tf.int64, ca: tf.int64, thal: tf.string}, tf.int64)>_`\r\n\r\n**With drop_remainder=True,** \r\n\r\n`train_ds = train_ds.batch(32, drop_remainder=True)\r\n\r\nprint(train_ds)\r\n\r\n<BatchDataset shapes: ({age: (32,), sex: (32,), cp: (32,), trestbps: (32,), chol: (32,), fbs: (32,), restecg: (32,), thalach: (32,), exang: (32,), oldpeak: (32,), slope: (32,), ca: (32,), thal: (32,)}, (32,)), types: ({age: tf.int64, sex: tf.int64, cp: tf.int64, trestbps: tf.int64, chol: tf.int64, fbs: tf.int64, restecg: tf.int64, thalach: tf.int64, exang: tf.int64, oldpeak: tf.float64, slope: tf.int64, ca: tf.int64, thal: tf.string}, tf.int64)>`\r\n\r\nIs this the expected behaviour?\r\n", "comments": ["`drop remainder` forces the batch size to be constant. If it is not enabled, then the last minibatch will possibly be smaller than the batch size, thus the batch size is not a constant, and thus the shape cannot be determined statically (what you are seeing).\r\n\r\nImagine a dataset with 33 elements. If you batch with batch_size 32, then the minibatch sizes are 32 and 1. Drop remainder simply discards the smaller minibatch to fix this edge case, which only leaves the first minibatch of size 32.", "@botasb \r\n\r\nPlease, go through this [tutorial](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) and see if it helps you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44577, "title": "custom ops coredump when tf update from 2.2.0 to 2.3.0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.8.2003 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0, 2.3.0\r\n- Python version: Python 3.8.3\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI am updating my project from tf-2.2.0 to tf-2.3.0, but it coredump while register_op. \r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```c++\r\n// core/ops/balance_dataset_ops.cc\r\n#include \"tensorflow/core/framework/common_shape_fns.h\"\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"BalanceDataset\")\r\n    .Input(\"input_dataset: variant\")\r\n    .Output(\"handle: variant\")\r\n    .Attr(\"output_types: list(type) >= 1\")\r\n    .Attr(\"output_shapes: list(shape) >= 1\")\r\n    .SetDoNotOptimize()\r\n    .SetShapeFn(shape_inference::ScalarShape)\r\n    .Doc(R\"doc(balance input data between datasets\r\n    )doc\");\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n#0  0x00007f238f5a6387 in raise () from /lib64/libc.so.6\r\n#1  0x00007f238f5a7a78 in abort () from /lib64/libc.so.6\r\n#2  0x00007f238f5e8ed7 in __libc_message () from /lib64/libc.so.6\r\n#3  0x00007f238f5f1299 in _int_free () from /lib64/libc.so.6\r\n#4  0x00007f235bc3e031 in google::protobuf::internal::ArenaStringPtr::DestroyNoArena (this=0x7ffe0d537d48, \r\n    default_value=0x7f235d82c2c0 <google::protobuf::internal::fixed_address_empty_string[abi:cxx11]>)\r\n    at external/com_google_protobuf/src/google/protobuf/arenastring.h:325\r\n#5  0x00007f235c2dc44e in tensorflow::OpDef::SharedDtor (this=0x7ffe0d537cd0)\r\n    at bazel-out/k8-dbg/bin/external/org_tensorflow/tensorflow/core/framework/op_def.pb.cc:1629\r\n#6  0x00007f235c2d5780 in tensorflow::OpDef::~OpDef (this=0x7ffe0d537cd0, __in_chrg=<optimized out>)\r\n    at bazel-out/k8-dbg/bin/external/org_tensorflow/tensorflow/core/framework/op_def.pb.cc:1623\r\n#7  0x00007f235bbfc19a in tensorflow::OpRegistrationData::~OpRegistrationData (this=0x7ffe0d537cd0, __in_chrg=<optimized out>)\r\n    at external/org_tensorflow/tensorflow/core/framework/op_def_builder.h:39\r\n#8  0x00007f235bbfc222 in tensorflow::OpDefBuilder::~OpDefBuilder (this=0x7ffe0d537cd0, __in_chrg=<optimized out>)\r\n    at external/org_tensorflow/tensorflow/core/framework/op_def_builder.h:53\r\n#9  0x00007f235bbfc5fc in tensorflow::register_op::OpDefBuilderWrapper<true>::~OpDefBuilderWrapper (this=0x7ffe0d537cd0, \r\n    __in_chrg=<optimized out>) at external/org_tensorflow/tensorflow/core/framework/op.h:225\r\n#10 0x00007f235bbfbdbf in __static_initialization_and_destruction_0 (__initialize_p=1, __priority=65535) at core/ops/balance_dataset_ops.cc:23\r\n#11 0x00007f235bbfbf76 in _GLOBAL__sub_I_balance_dataset_ops.cc(void) () at core/ops/balance_dataset_ops.cc:28\r\n#12 0x00007f238fb699b3 in _dl_init_internal () from /lib64/ld-linux-x86-64.so.2\r\n#13 0x00007f238fb6e58e in dl_open_worker () from /lib64/ld-linux-x86-64.so.2\r\n#14 0x00007f238fb697c4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2\r\n#15 0x00007f238fb6db7b in _dl_open () from /lib64/ld-linux-x86-64.so.2\r\n#16 0x00007f238f36cfab in dlopen_doit () from /lib64/libdl.so.2\r\n#17 0x00007f238fb697c4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2\r\n#18 0x00007f238f36d5ad in _dlerror_run () from /lib64/libdl.so.2\r\n#19 0x00007f238f36d041 in dlopen@@GLIBC_2.2.5 () from /lib64/libdl.so.2\r\n#20 0x000056226d8191ad in ?? ()\r\n#21 0x00007f238fcbc1e0 in ?? ()\r\n#22 0xffffffffffffffff in ?? ()\r\n#23 0x00007f238fcd7290 in ?? ()\r\n#24 0x0000000000000000 in ?? ()\r\n```\r\n", "comments": []}, {"number": 44576, "title": "is For Loop supported? or proper way of Iterating over a 1d tensor?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):pip install\r\n- TensorFlow version (or github SHA if from source):2.3.1\r\n\r\n```\r\nimport os\r\nimport tempfile\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntmpdir = tempfile.mkdtemp()\r\n\r\nclass CustomModule(tf.Module):\r\n\r\n  def __init__(self):\r\n    super(CustomModule, self).__init__()\r\n    self.v = tf.Variable(1, dtype=tf.int64)\r\n    self.const = tf.constant(np.arange(10))\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec((), dtype=tf.int64)])\r\n  def __call__(self, x):\r\n    for ele in self.const:\r\n      self.v.assign(ele)\r\n    return x * self.const\r\n\r\nmodule = CustomModule()\r\n\r\nmodule_path = os.path.join(tmpdir)\r\nprint(module(tf.constant(2, dtype=tf.int64)))\r\nprint('Saving model...')\r\ntf.saved_model.save(module, module_path)\r\n\r\nimported = tf.saved_model.load(module_path)\r\nprint(imported(tf.constant(4, dtype=tf.int64)))\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(module_path)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\nerror:\r\n```\r\ntf.Tensor([ 0  2  4  6  8 10 12 14 16 18], shape=(10,), dtype=int64)\r\nSaving model...\r\n...\r\ntf.Tensor([ 0  4  8 12 16 20 24 28 32 36], shape=(10,), dtype=int64)\r\n...\r\ntensorflow.lite.python.convert.ConverterError: input resource[0] expected type resource != int64, the type of while_assignvariableop_resource_0[0]\r\n        In {{node while/AssignVariableOp}}\r\n```", "comments": ["I am able to replicate this issue please find the [gist here](https://colab.research.google.com/gist/Saduf2019/ee8ce88a1e0a9b2f2b902f6f0e3b5d78/untitled459.ipynb).", "Sorry for encountering this issue. Currently, TFLite does not support mutable variable support.\r\n\r\nYou might want to mimic the behavior with tf.Concat or other similar operation as a workaround.", "... I see.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44576\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44576\">No</a>\n"]}, {"number": 44575, "title": "tflite Converting unsupported operation: MatrixInverse", "body": "", "comments": ["@usename6,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nAnd also the complete code to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44574, "title": "Could not find any cudnn.h matching version '8' in any subdirectory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 1.14\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: RTX 3090\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nFor some specific purpose, I am building TF 1.14 from source with CUDA 11.1, CUDNN 8.0.4 & TensorRT 7.2 on Ubuntu 16.04 but getting an error:-\r\n![image](https://user-images.githubusercontent.com/51915348/98066809-20352b00-1e7e-11eb-8365-906eeb4dfa5b.png)\r\n\r\nI have made various checks like LD_LIBRARY_PATH, PATH are properly set and installation using tar, deb file of CUDNN but the error persists. Even if CUDA 11.1, CUDNN 8.0.4 is incompatible with TF 1.14, why I'm getting this error as this error shows unable to find the file. Maybe incompatibility error will generate during compilation.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Thunder003,\r\nTensorFlow 1.x is not actively supported. Every TensorFlow release is compatible with a certain CUDA/cuDNN version. Please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu) for more information. \r\n\r\nAlso, please check similar issue [#42895](https://github.com/tensorflow/tensorflow/issues/42895#issuecomment-685874736) for reference. Thanks!", "@amahendrakar let me rephrase my issue. When building Tensorflow 2 from source with CUDA 11.1, CUDNN 8.0.4, what TensorFlow 2.x checks at this stage( Configuration step) **inside the CUDNN file** which helps it recognize the CUDNN version. I have verified that there is no such issue with T.F. 2.3, CUDA 11.1, and CUDNN 8.0.4 \r\nI know the original issue is due to incompatible, but can you please give some insights by relating it to the CUDNN file( like what T.F 2.X checks inside it).  As T.F. 1.14 had recognized the CUDA 11.1 file. ", "@Thunder003,\r\nChecks are done in this [Build file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/BUILD). \r\n\r\nTo elaborate, the `Version` of `CUDNN` is identified from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/BUILD#L404). After that, compatibility is checked in [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/BUILD#L637), which in turn calls `cudnn_version.cc` where [compatibility is checked](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cudnn_version.cc#L21-L32).\r\n\r\nHope this helps. Thanks!", "@rmothukuru thanks for your answer.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44574\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44574\">No</a>\n", "Hey @Thunder003, were you ever able to get TF 1.14 to build with CUDA 11.1? I'm currently trying to do the same with TF 1.15.", "@smitheric95 no (I tried almost everything, but T.F. 1.14 & CUDA 11.1 are incompatible),  every time I try to build T.F. 1.14 & CUDA 11.1, I get an error:- \"\"Could not find any cudnn.h matching version '8' in any subdirectory\"\"\r\n\r\nI eventually migrated from T.F. 1.x to 2.x.", "@Thunder003, ah yes. I encountered that as well, but was able to get around it by changing `configure.py` to look for the CUDNN version in `cudnn_version.h` instead of `cudnn.h`. \r\n\r\n", "@smitheric95 ok, I have contacted a few people, saying to me that T.F. 1.14 & CUDA 11.1 are incompatible so may get an error on compilation time even I had fixed this issue. But another thing that was scratching my head is that if they are incompatible, then why NGC provide a container having T.F. 1.15 and CUDA 11.1. \r\nTry compiling that @smitheric95, if you will be able to make that, let me know here as well :)", "@Thunder003, that worked! \r\n\r\nI just had to copy the `tensorflow` directory from the docker container to my local `/opt/` directory. \r\n\r\nThen, I set `IN_CONTAINER` to `0` on line 108 of `nvbuild.sh`.\r\n\r\nFinally, I ran:\r\n```bash\r\nexport CUDA_VERSION=\"11.1\"\r\nexport CUDNN_VERSION=\"8.0.5\"\r\n./nvbuild.sh --v1\r\n```\r\n\r\nYou may run into some permissions issues along the way \u2013 I handled those as they came up.", "@smitheric95 nice!!, thanks for sharing as well,..... which docker container you are referring to, NGC T.F. 1.15, CUDA 11.1? You did build from source T.F. 1.14& CUDA 11.1 right?\r\n\r\nAnd have you tried running it, is it utilizing GPUs properly?", "@Thunder003, correct it's the `20.11-tf1-py3` container on NGC. I used that source code and built 1.15.4 natively. \r\n\r\nI can confirm that the GPUs are being properly utilized as well.", "hi @smitheric95, just checking, have you tested any code with your build from source TensorFlow? Is it working fine?  I got some information that builds from a source that will recognize GPU, but it is not working. My sources have checked on StyleGAN 2 code. It is giving only black images, no convergence at all.", "Hey @Thunder003, yes, I have tested TensorFlow code with this setup and am able to successfully train models just fine! \r\n\r\nAs far as StyleGAN goes, you might want to test some other simpler models first to rule out environment issues. ", "> Hey @Thunder003, yes, I have tested TensorFlow code with this setup and am able to successfully train models just fine!\r\n> \r\n> As far as StyleGAN goes, you might want to test some other simpler models first to rule out environment issues.\r\n\r\nHi,sorry to bother you ,but I was trying to compile tensorflow 1.15 with 3090 and cuda11.0 ,however the script report that it could not found libcurand.so.11. I tried to find \"libcurand\" with find /usr/local -type f | grep libcurand , but I found libcurand.so.10 instead of 11 in cuda11 folder. Have you encountered this problem\uff1f Or could you plz list what did you do to   built 1.15.4 successfully? Thanks!", "In the case of TF 2.0, I should have changed cudnn.h -> cudnn_version.h in the \"third_party/gpus/find_cuda_config.py\" file"]}]