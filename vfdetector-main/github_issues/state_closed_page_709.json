[{"number": 32310, "title": "ModuleNotFoundError: No module named 'tensorflow' ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIndows 7 64 bit\r\n- TensorFlow installed from (source or binary): from : https://www.tensorflow.org/install/pip\r\n------------------------------------------------------------------------------------------------------\r\nthe error generated while running -python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"- code mentioned in the site to verify the install. \r\n\r\ntensorflow \u2014Latest stable release for CPU-only (recommended for beginners), from tesnorflow website\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n\r\n(venv) C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>python -c \"import\r\n tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please post the full output of `pip debug --verbose` and `python -m pip --version` and `python -m pip install --upgrade pip && python -m pip install -vvv tensorflow && python -c \"import tensorflow\"` (please use them exactly as written)", "> Please post the full output of `pip debug --verbose` and `python -m pip --version` and `python -m pip install --upgrade pip && pyhon -m pip install -vvv tensorflow && pyhon -c \"import tensorflow\"` (please use them exactly as written)\r\n\r\n\r\n(venv) C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>pip debug --verbo\r\nse\r\nWARNING: This command is only meant for debugging. Do not use this with automati\r\non for parsing and getting these details, since the output and options of this c\r\nommand may change without notice.\r\npip version: pip 19.2.3 from c:\\users\\sharone\\appdata\\local\\programs\\python\\pyth\r\non36\\venv\\lib\\site-packages\\pip (python 3.6)\r\nsys.version: 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit\r\n (AMD64)]\r\nsys.executable: c:\\users\\sharone\\appdata\\local\\programs\\python\\python36\\venv\\scr\r\nipts\\python.exe\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: cp1252\r\nsys.platform: win32\r\nsys.implementation:\r\n  name: cpython\r\nConfig variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect\r\nConfig variable 'WITH_PYMALLOC' is unset, Python ABI tag may be incorrect\r\nCompatible tags: 13\r\n  cp36-cp36m-win_amd64\r\n  cp36-none-win_amd64\r\n  py3-none-win_amd64\r\n  cp36-none-any\r\n  cp3-none-any\r\n  py36-none-any\r\n  py3-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n\r\n(venv) C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>python -m pip --v\r\nersion\r\npip 19.2.3 from C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site\r\n-packages\\pip (python 3.6)\r\n\r\n(venv) C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>python -m pip ins\r\ntall --upgrade pip && pyhon -m pip install -vvv tensorflow && pyhon -c \"import t\r\nensorflow\"\r\nRequirement already up-to-date: pip in c:\\users\\sharone\\appdata\\local\\programs\\p\r\nython\\python36\\lib\\site-packages (19.2.3)\r\n'pyhon' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n\r\n**kindly pardon for pasting everything that popped up on screen**\r\n\r\nC:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>python -m pip ins\r\ntall --upgrade pip && python -m pip install -vvv tensorflow && python -c \"import t\r\nensorflow\"\r\n    Skipping link: none of the wheel's tags match: cp37-cp37m-win_amd64: https:/\r\n/files.pythonhosted.org/packages/4f/1e/89aa610afce8df6fd1f12647600a05e902238587a\r\ne6375442a3164b59d51/h5py-2.9.0-cp37-cp37m-win_amd64.whl#sha256=b0f03af381d33306c\r\ne67d18275b61acb4ca111ced645381387a02c8a5ee1b796 (from https://pypi.org/simple/h5\r\npy/)\r\n    Found link https://files.pythonhosted.org/packages/43/27/a6e7dcb8ae20a4dbf37\r\n25321058923fec262b6f7835179d78ccc8d98deec/h5py-2.9.0.tar.gz#sha256=9d41ca62daf36\r\nd6b6515ab8765e4c8c4388ee18e2a665701fef2b41563821002 (from https://pypi.org/simpl\r\ne/h5py/), version: 2.9.0\r\n    Skipping link: none of the wheel's tags match: cp27-cp27m-macosx_10_6_intel:\r\n https://files.pythonhosted.org/packages/2c/47/e0d58be6f292684a4541d10b1da953542\r\nff679f3ffc6096bee73634832b1/h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl#sha256=\r\necf4d0b56ee394a0984de15bceeb97cbe1fe485f1ac205121293fc44dcf3f31f (from https://p\r\nypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp27-cp27m-manylinux1_i686: h\r\nttps://files.pythonhosted.org/packages/3f/b6/23155e343f8719923449ccfebac296c1ab0\r\ndda9bdccc28242e1594469f5a/h5py-2.10.0-cp27-cp27m-manylinux1_i686.whl#sha256=8686\r\n8dc07b9cc8cb7627372a2e6636cdc7a53b7e2854ad020c9e9d8a4d3fd0f5 (from https://pypi.\r\norg/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp27-cp27m-manylinux1_x86_64:\r\n https://files.pythonhosted.org/packages/3a/9b/5b68a27110d459704550cfc0c765a1ae6\r\nee98981cbbbf0ca92983c87046a/h5py-2.10.0-cp27-cp27m-manylinux1_x86_64.whl#sha256=\r\naac4b57097ac29089f179bbc2a6e14102dd210618e94d77ee4831c65f82f17c0 (from https://p\r\nypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp27-cp27m-win32: https://fil\r\nes.pythonhosted.org/packages/29/e9/0e204d9503a2689b8b1b01ea1e110e7b0298be66114ee\r\nefe017a2e1841e1/h5py-2.10.0-cp27-cp27m-win32.whl#sha256=7be5754a159236e95bd19641\r\n9485343e2b5875e806fe68919e087b6351f40a70 (from https://pypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp27-cp27m-win_amd64: https:/\r\n/files.pythonhosted.org/packages/48/84/9dc231fd1a302d38bd4b420f2ea62dc986e459541\r\ne5f7642081abfb080f9/h5py-2.10.0-cp27-cp27m-win_amd64.whl#sha256=13c87efa24768a5e\r\n24e360a40e0bc4c49bcb7ce1bb13a3a7f9902cec302ccd36 (from https://pypi.org/simple/h\r\n5py/)\r\n    Skipping link: none of the wheel's tags match: cp27-cp27mu-manylinux1_i686:\r\nhttps://files.pythonhosted.org/packages/33/ba/624b58c9af05cdbe50e191843afb57435d\r\n74e13890d778ec67e8af59b0d5/h5py-2.10.0-cp27-cp27mu-manylinux1_i686.whl#sha256=79\r\nb23f47c6524d61f899254f5cd5e486e19868f1823298bc0c29d345c2447172 (from https://pyp\r\ni.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp27-cp27mu-manylinux1_x86_64\r\n: https://files.pythonhosted.org/packages/12/90/3216b8f6d69905a320352a9ca6802a8e\r\n39fdb1cd93133c3d4163db8d5f19/h5py-2.10.0-cp27-cp27mu-manylinux1_x86_64.whl#sha25\r\n6=cbf28ae4b5af0f05aa6e7551cee304f1d317dbed1eb7ac1d827cee2f1ef97a99 (from https:/\r\n/pypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp34-cp34m-manylinux1_i686: h\r\nttps://files.pythonhosted.org/packages/ff/fb/50a50d83dd7e721761d85865df5ebdb0180\r\ne83c01f6d87efd18dcb286870/h5py-2.10.0-cp34-cp34m-manylinux1_i686.whl#sha256=c0d4\r\nb04bbf96c47b6d360cd06939e72def512b20a18a8547fa4af810258355d5 (from https://pypi.\r\norg/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp34-cp34m-manylinux1_x86_64:\r\n https://files.pythonhosted.org/packages/9c/1a/7e7daac13bb7ecb02637cd2d2df5dad8d\r\n7fe21842214c5714d968b4c50df/h5py-2.10.0-cp34-cp34m-manylinux1_x86_64.whl#sha256=\r\n549ad124df27c056b2e255ea1c44d30fb7a17d17676d03096ad5cd85edb32dc1 (from https://p\r\nypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp35-cp35m-macosx_10_6_intel:\r\n https://files.pythonhosted.org/packages/62/0d/de4d889991536b2d6c8b15958e0eb70ef\r\naf1d89c87c175725bde46f8bd0f/h5py-2.10.0-cp35-cp35m-macosx_10_6_intel.whl#sha256=\r\na5f82cd4938ff8761d9760af3274acf55afc3c91c649c50ab18fcff5510a14a5 (from https://p\r\nypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp35-cp35m-manylinux1_i686: h\r\nttps://files.pythonhosted.org/packages/f9/c5/45fc133e09dc4488b58f00136cb23695117\r\n1893a4c509442a0f47479e49b/h5py-2.10.0-cp35-cp35m-manylinux1_i686.whl#sha256=3dad\r\n1730b6470fad853ef56d755d06bb916ee68a3d8272b3bab0c1ddf83bb99e (from https://pypi.\r\norg/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp35-cp35m-manylinux1_x86_64:\r\n https://files.pythonhosted.org/packages/10/56/d5c53cd170529bb40cd7dd43e2b68944c\r\nb65a45f65ab4c78a68f4ac9e51e/h5py-2.10.0-cp35-cp35m-manylinux1_x86_64.whl#sha256=\r\n063947eaed5f271679ed4ffa36bb96f57bc14f44dd4336a827d9a02702e6ce6b (from https://p\r\nypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp35-cp35m-win32: https://fil\r\nes.pythonhosted.org/packages/68/1a/4468f4f1b6d7e8fa2e017ad1be47ebac06c8995ffc1d3\r\nc8895cbba7bbce7/h5py-2.10.0-cp35-cp35m-win32.whl#sha256=c54a2c0dd4957776ace7f958\r\n79d81582298c5daf89e77fb8bee7378f132951de (from https://pypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp35-cp35m-win_amd64: https:/\r\n/files.pythonhosted.org/packages/25/81/be3e43b143ee6b40cebb3d6e61da8ce74a0322179\r\n1134f16dd016a497c23/h5py-2.10.0-cp35-cp35m-win_amd64.whl#sha256=6998be619c695910\r\ncb0effe5eb15d3a511d3d1a5d217d4bd0bebad1151ec2262 (from https://pypi.org/simple/h\r\n5py/)\r\n    Skipping link: none of the wheel's tags match: cp36-cp36m-macosx_10_6_intel:\r\n https://files.pythonhosted.org/packages/ec/7f/a833846f5628d00f82ad87010f9829425\r\n7535f1052e4a466888deba29f94/h5py-2.10.0-cp36-cp36m-macosx_10_6_intel.whl#sha256=\r\nff7d241f866b718e4584fa95f520cb19405220c501bd3a53ee11871ba5166ea2 (from https://p\r\nypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp36-cp36m-manylinux1_i686: h\r\nttps://files.pythonhosted.org/packages/d1/e1/b27542371620968aec7acf130d4318c292f\r\nd338e79957cf36b4ae23d1aab/h5py-2.10.0-cp36-cp36m-manylinux1_i686.whl#sha256=5481\r\n7b696e87eb9e403e42643305f142cd8b940fe9b3b490bbf98c3b8a894cf4 (from https://pypi.\r\norg/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp36-cp36m-manylinux1_x86_64:\r\n https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52\r\na8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl#sha256=\r\nd3c59549f90a891691991c17f8e58c8544060fdf3ccdea267100fa5f561ff62f (from https://p\r\nypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp36-cp36m-win32: https://fil\r\nes.pythonhosted.org/packages/c0/15/f275081ad974a835522c02de377dc4210685bd52040b4\r\n4c590710961142c/h5py-2.10.0-cp36-cp36m-win32.whl#sha256=d7ae7a0576b06cb8e8a1c265\r\na8bc4b73d05fdee6429bffc9a26a6eb531e79d72 (from https://pypi.org/simple/h5py/)\r\n    Found link https://files.pythonhosted.org/packages/0b/fa/bee65d2dbdbd3611702\r\naafd128139c53c90a1285f169ba5467aab252e27a/h5py-2.10.0-cp36-cp36m-win_amd64.whl#s\r\nha256=bffbc48331b4a801d2f4b7dac8a72609f0b10e6e516e5c480a3e3241e091c878 (from htt\r\nps://pypi.org/simple/h5py/), version: 2.10.0\r\n    Skipping link: none of the wheel's tags match: cp37-cp37m-macosx_10_6_intel:\r\n https://files.pythonhosted.org/packages/1a/8b/4d01ae9a9d50a0bcc7b0b9aae41785d8d\r\n9de6fa9bba04dc20b1582181d2d/h5py-2.10.0-cp37-cp37m-macosx_10_6_intel.whl#sha256=\r\n51ae56894c6c93159086ffa2c94b5b3388c0400548ab26555c143e7cfa05b8e5 (from https://p\r\nypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp37-cp37m-manylinux1_i686: h\r\nttps://files.pythonhosted.org/packages/7f/d9/0ddeea0a6dcb0e330c5d98a72c2fed52a04\r\n30f48d69e5bf4d50024f3c0ba/h5py-2.10.0-cp37-cp37m-manylinux1_i686.whl#sha256=16ea\r\nd3c57141101e3296ebeed79c9c143c32bdd0e82a61a2fc67e8e6d493e9d1 (from https://pypi.\r\norg/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp37-cp37m-manylinux1_x86_64:\r\n https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493\r\n521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl#sha256=\r\nf0e25bb91e7a02efccb50aba6591d3fe2c725479e34769802fcdd4076abfa917 (from https://p\r\nypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp37-cp37m-win32: https://fil\r\nes.pythonhosted.org/packages/81/67/1456efe696304577d13af7ba12292fbe2c68acc37c863\r\n8494e5b15c53bd2/h5py-2.10.0-cp37-cp37m-win32.whl#sha256=f23951a53d18398ef1344c18\r\n6fb04b26163ca6ce449ebd23404b153fd111ded9 (from https://pypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp37-cp37m-win_amd64: https:/\r\n/files.pythonhosted.org/packages/a1/6b/7f62017e3f0b32438dd90bdc1ff0b7b1448b6cb04\r\na1ed84f37b6de95cd7b/h5py-2.10.0-cp37-cp37m-win_amd64.whl#sha256=8bb1d2de101f3974\r\n3f91512a9750fb6c351c032e5cd3204b4487383e34da7f75 (from https://pypi.org/simple/h\r\n5py/)\r\n    Skipping link: none of the wheel's tags match: cp38-cp38-win32: https://file\r\ns.pythonhosted.org/packages/bc/13/0adae355e77c38856bf0c92de0562b9e6e0b1814610a6a\r\n414cf860a6b532/h5py-2.10.0-cp38-cp38-win32.whl#sha256=6ef7ab1089e3ef53ca099038f3\r\nc0a94d03e3560e6aff0e9d6c64c55fb13fc681 (from https://pypi.org/simple/h5py/)\r\n    Skipping link: none of the wheel's tags match: cp38-cp38-win_amd64: https://\r\nfiles.pythonhosted.org/packages/1e/63/0d32c1803c08518dd03e02f3cfe302335624f51115\r\n5be723bcc7329fed4e/h5py-2.10.0-cp38-cp38-win_amd64.whl#sha256=769e141512b54dee14\r\nec76ed354fcacfc7d97fea5a7646b709f7400cf1838630 (from https://pypi.org/simple/h5p\r\ny/)\r\n    Found link https://files.pythonhosted.org/packages/5f/97/a58afbcf40e8abecede\r\ndd9512978b4e4915374e5b80049af082f49cebe9a/h5py-2.10.0.tar.gz#sha256=84412798925d\r\nc870ffd7107f045d7659e60f5d46d1c70c700375248bf6bf512d (from https://pypi.org/simp\r\nle/h5py/), version: 2.10.0\r\n  Given no hashes to check 16 links for project 'h5py': discarding no candidates\r\n\r\n  Using version 2.10.0 (newest of versions: 2.2.1, 2.3.0, 2.3.1, 2.4.0, 2.5.0, 2\r\n.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.10.0)\r\n  Created temporary directory: C:\\Users\\sharone\\AppData\\Local\\Temp\\pip-unpack-bp\r\nggbpgk\r\n  Looking up \"https://files.pythonhosted.org/packages/0b/fa/bee65d2dbdbd3611702a\r\nafd128139c53c90a1285f169ba5467aab252e27a/h5py-2.10.0-cp36-cp36m-win_amd64.whl\" i\r\nn the cache\r\n  Current age based on date: 8422\r\n  Ignoring unknown cache-control directive: immutable\r\n  Freshness lifetime from max-age: 365000000\r\n  The response is \"fresh\", returning cached response\r\n  365000000 > 8422\r\n  Using cached https://files.pythonhosted.org/packages/0b/fa/bee65d2dbdbd3611702\r\naafd128139c53c90a1285f169ba5467aab252e27a/h5py-2.10.0-cp36-cp36m-win_amd64.whl\r\n  Downloading from URL https://files.pythonhosted.org/packages/0b/fa/bee65d2dbdb\r\nd3611702aafd128139c53c90a1285f169ba5467aab252e27a/h5py-2.10.0-cp36-cp36m-win_amd\r\n64.whl#sha256=bffbc48331b4a801d2f4b7dac8a72609f0b10e6e516e5c480a3e3241e091c878 (\r\nfrom https://pypi.org/simple/h5py/)\r\n  Added h5py from https://files.pythonhosted.org/packages/0b/fa/bee65d2dbdbd3611\r\n702aafd128139c53c90a1285f169ba5467aab252e27a/h5py-2.10.0-cp36-cp36m-win_amd64.wh\r\nl#sha256=bffbc48331b4a801d2f4b7dac8a72609f0b10e6e516e5c480a3e3241e091c878 (from\r\nkeras-applications>=1.0.6->tensorflow) to build tracker 'C:\\\\Users\\\\sharone\\\\App\r\nData\\\\Local\\\\Temp\\\\pip-req-tracker-2_86xm0h'\r\n  Removed h5py from https://files.pythonhosted.org/packages/0b/fa/bee65d2dbdbd36\r\n11702aafd128139c53c90a1285f169ba5467aab252e27a/h5py-2.10.0-cp36-cp36m-win_amd64.\r\nwhl#sha256=bffbc48331b4a801d2f4b7dac8a72609f0b10e6e516e5c480a3e3241e091c878 (fro\r\nm keras-applications>=1.0.6->tensorflow) from build tracker 'C:\\\\Users\\\\sharone\\\r\n\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-2_86xm0h'\r\nInstalling collected packages: numpy, wheel, six, keras-preprocessing, astor, te\r\nnsorflow-estimator, gast, absl-py, setuptools, protobuf, werkzeug, markdown, grp\r\ncio, tensorboard, termcolor, google-pasta, wrapt, h5py, keras-applications, tens\r\norflow\r\n\r\n\r\n\r\n\r\n\r\n\r\n  Found existing installation: setuptools 39.0.1\r\n    Uninstalling setuptools-39.0.1:\r\n      Created temporary directory: C:\\Users\\sharone\\AppData\\Local\\Temp\\pip-unins\r\ntall-gtqqq983\r\n      Removing file or directory c:\\users\\sharone\\appdata\\local\\programs\\python\\\r\npython36\\lib\\site-packages\\__pycache__\\easy_install.cpython-36.pyc\r\n      Created temporary directory: C:\\Users\\sharone\\AppData\\Local\\Temp\\pip-unins\r\ntall-bkp6oeo9\r\n      Removing file or directory c:\\users\\sharone\\appdata\\local\\programs\\python\\\r\npython36\\lib\\site-packages\\easy_install.py\r\n      Created temporary directory: c:\\users\\sharone\\appdata\\local\\programs\\pytho\r\nn\\python36\\lib\\site-packages\\~kg_resources\r\n      Removing file or directory c:\\users\\sharone\\appdata\\local\\programs\\python\\\r\npython36\\lib\\site-packages\\pkg_resources\\\r\n      Created temporary directory: c:\\users\\sharone\\appdata\\local\\programs\\pytho\r\nn\\python36\\lib\\site-packages\\~etuptools-39.0.1.dist-info\r\n      Removing file or directory c:\\users\\sharone\\appdata\\local\\programs\\python\\\r\npython36\\lib\\site-packages\\setuptools-39.0.1.dist-info\\\r\n      Created temporary directory: c:\\users\\sharone\\appdata\\local\\programs\\pytho\r\nn\\python36\\lib\\site-packages\\~etuptools\r\n      Removing file or directory c:\\users\\sharone\\appdata\\local\\programs\\python\\\r\npython36\\lib\\site-packages\\setuptools\\\r\n      Created temporary directory: C:\\Users\\sharone\\AppData\\Local\\Temp\\pip-unins\r\ntall-4f6vumnm\r\n      Removing file or directory c:\\users\\sharone\\appdata\\local\\programs\\python\\\r\npython36\\scripts\\easy_install-3.6.exe\r\n      Removing file or directory c:\\users\\sharone\\appdata\\local\\programs\\python\\\r\npython36\\scripts\\easy_install.exe\r\n      Successfully uninstalled setuptools-39.0.1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSuccessfully installed absl-py-0.8.0 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 g\r\nrpcio-1.23.0 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 mark\r\ndown-3.1.1 numpy-1.17.2 protobuf-3.9.1 setuptools-41.2.0 six-1.12.0 tensorboard-\r\n1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 werkzeug-0.\r\n15.6 wheel-0.33.6 wrapt-1.11.2\r\nCleaning up...\r\nRemoved build tracker 'C:\\\\Users\\\\sharone\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker\r\n-2_86xm0h'\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_help\r\ner\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", lin\r\ne 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", lin\r\ne 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_help\r\ner\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", lin\r\ne 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", lin\r\ne 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n(venv) C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>\r\n(venv) C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>python -m pip ins\r\ntall --upgrade pip && pyhon -m pip install -vvv tensorflow && python -c \"import\r\ntensorflow\"\r\nRequirement already up-to-date: pip in c:\\users\\sharone\\appdata\\local\\programs\\p\r\nython\\python36\\lib\\site-packages (19.2.3)\r\n'pyhon' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n\r\n(venv) C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>", "> Please post the full output of `pip debug --verbose` and `python -m pip --version` and `python -m pip install --upgrade pip && pyhon -m pip install -vvv tensorflow && pyhon -c \"import tensorflow\"` (please use them exactly as written)\r\n\r\n**_another set of execution results._** \r\nMicrosoft Windows [Version 6.1.7601]\r\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.\r\n\r\nC:\\Windows\\system32>cd C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\r\n\r\nC:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>.\\venv\\Scripts\\activate\r\n\r\n(venv) C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>pip install --upg\r\nrade tensorflow\r\nRequirement already up-to-date: tensorflow in c:\\users\\sharone\\appdata\\local\\pro\r\ngrams\\python\\python36\\venv\\lib\\site-packages (1.14.0)\r\nRequirement already satisfied, skipping upgrade: wheel>=0.26 in c:\\users\\sharone\r\n\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tensorflow)\r\n (0.33.6)\r\nRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in c\r\n:\\users\\sharone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (f\r\nrom tensorflow) (1.1.0)\r\nRequirement already satisfied, skipping upgrade: tensorboard<1.15.0,>=1.14.0 in\r\nc:\\users\\sharone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (\r\nfrom tensorflow) (1.14.0)\r\nRequirement already satisfied, skipping upgrade: protobuf>=3.6.1 in c:\\users\\sha\r\nrone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tensorf\r\nlow) (3.9.1)\r\nRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in c:\\users\r\n\\sharone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from ten\r\nsorflow) (0.1.7)\r\nRequirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in c:\\users\\\r\nsharone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tens\r\norflow) (1.17.2)\r\nRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\\users\\sh\r\narone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tensor\r\nflow) (1.1.0)\r\nRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in c:\\users\\sharo\r\nne\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tensorflo\r\nw) (1.11.2)\r\nRequirement already satisfied, skipping upgrade: gast>=0.2.0 in c:\\users\\sharone\r\n\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tensorflow)\r\n (0.2.2)\r\nRequirement already satisfied, skipping upgrade: six>=1.10.0 in c:\\users\\sharone\r\n\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tensorflow)\r\n (1.12.0)\r\nRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in c:\r\n\\users\\sharone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (fr\r\nom tensorflow) (1.0.8)\r\nRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in c:\\users\\sharo\r\nne\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tensorflo\r\nw) (1.23.0)\r\nRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in c:\\users\\shar\r\none\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tensorfl\r\now) (0.8.0)\r\nRequirement already satisfied, skipping upgrade: tensorflow-estimator<1.15.0rc0,\r\n>=1.14.0rc0 in c:\\users\\sharone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\\r\nsite-packages (from tensorflow) (1.14.0)\r\nRequirement already satisfied, skipping upgrade: astor>=0.6.0 in c:\\users\\sharon\r\ne\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tensorflow\r\n) (0.8.0)\r\nRequirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\users\\s\r\nharone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tenso\r\nrboard<1.15.0,>=1.14.0->tensorflow) (0.15.6)\r\nRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\sha\r\nrone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tensorb\r\noard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\r\nRequirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\users\\\r\nsharone\\appdata\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from tens\r\norboard<1.15.0,>=1.14.0->tensorflow) (41.2.0)\r\nRequirement already satisfied, skipping upgrade: h5py in c:\\users\\sharone\\appdat\r\na\\local\\programs\\python\\python36\\venv\\lib\\site-packages (from keras-applications\r\n>=1.0.6->tensorflow) (2.10.0)\r\n\r\n\r\n(venv) C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>python -c \"import\r\n tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_help\r\ner\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", lin\r\ne 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", lin\r\ne 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-package\r\ns\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_help\r\ner\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", lin\r\ne 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", lin\r\ne 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n(venv) C:\\Users\\sharone\\AppData\\Local\\Programs\\Python\\Python36>\r\n", "@sharonevarghese,\r\nTensorFlow release binaries version 1.6 and above are prebuilt with AVX instruction sets.\r\nTherefore on any CPU that does not have these instruction sets either CPU or GPU version of TF will fail to load.\r\nCan you check once, does your CPU supports AVX instruction set or not. Thanks!", "\r\n\r\n\r\n\r\n> @sharonevarghese,\r\n> TensorFlow release binaries version 1.6 and above are prebuilt with AVX instruction sets.\r\n> Therefore on any CPU that does not have these instruction sets either CPU or GPU version of TF will fail to load.\r\n> Can you check once, does your CPU supports AVX instruction set or not. Thanks!\r\n\r\nIntel(R) Core(TM) i3 CPU M380 @2.53GHz, 2533Mhz, 2 Core(s), 4 Logic - is my processor details. i'm not sure whether this supports AVX instruction set. Could you provide with the details on how to check whether the processor supports the AVX instruction set or not.", "> @sharonevarghese,\r\n> TensorFlow release binaries version 1.6 and above are prebuilt with AVX instruction sets.\r\n> Therefore on any CPU that does not have these instruction sets either CPU or GPU version of TF will fail to load.\r\n> Can you check once, does your CPU supports AVX instruction set or not. Thanks!\r\n\r\ndetails of my processor, kindly advise whether the processor supports AVX instruction set, if no is there any way to run TensorFlow of my CPU\r\nCPU-Z TXT Report\r\n-------------------------------------------------------------------------\r\n\r\nBinaries\r\n-------------------------------------------------------------------------\r\n\r\nCPU-Z version\t\t\t1.90.0.x64\r\n\r\nProcessors\r\n-------------------------------------------------------------------------\r\n\r\nNumber of sockets\t\t1\r\nNumber of threads\t\t4\r\n\r\nAPICs\r\n-------------------------------------------------------------------------\r\n\r\nSocket 0\t\r\n\t-- Core 0 (ID 0)\t\r\n\t\t-- Thread 0\t0\r\n\t\t-- Thread 1\t1\r\n\t-- Core 1 (ID 2)\t\r\n\t\t-- Thread 2\t4\r\n\t\t-- Thread 3\t5\r\n\r\nTimers\r\n-------------------------------------------------------------------------\r\n\r\n\tACPI timer\t\t3.580 MHz\r\n\tPerf timer\t\t2.468 MHz\r\n\tSys timer\t\t1.000 KHz\r\n\r\n\r\nProcessors Information\r\n-------------------------------------------------------------------------\r\n\r\nSocket 1\t\t\tID = 0\r\n\tNumber of cores\t\t2 (max 2)\r\n\tNumber of threads\t4 (max 4)\r\n\tManufacturer\t\tGenuineIntel\r\n\tName\t\t\tIntel Core i3 380M\r\n\tCodename\t\tArrandale\r\n\tSpecification\t\tIntel(R) Core(TM) i3 CPU       M 380  @ 2.53GHz\r\n\tPackage (platform ID)\tSocket 989 rPGA (0x4)\r\n\tCPUID\t\t\t6.5.5\r\n\tExtended CPUID\t\t6.25\r\n\tCore Stepping\t\tK0\r\n\tTechnology\t\t32 nm\r\n\tTDP Limit\t\t25.0 Watts\r\n\tCore Speed\t\t1596.2 MHz\r\n\tMultiplier x Bus Speed\t12.0 x 133.0 MHz\r\n\tBase frequency (cores)\t133.0 MHz\r\n\tBase frequency (ext.)\t133.0 MHz\r\n\tRated Bus speed\t\t2394.3 MHz\r\n\tStock frequency\t\t2533 MHz\r\n\tInstructions sets\tMMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T, VT-x\r\n\tMicrocode Revision\t0x2\r\n\tL1 Data cache\t\t2 x 32 KBytes, 8-way set associative, 64-byte line size\r\n\tL1 Instruction cache\t2 x 32 KBytes, 4-way set associative, 64-byte line size\r\n\tL2 cache\t\t2 x 256 KBytes, 8-way set associative, 64-byte line size\r\n\tL3 cache\t\t3 MBytes, 12-way set associative, 64-byte line size\r\n\tMax CPUID level\t\t0000000Bh\r\n\tMax CPUID ext. level\t80000008h\r\n\tCache descriptor\tLevel 1, D, 32 KB, 2 thread(s)\r\n\tCache descriptor\tLevel 1, I, 32 KB, 2 thread(s)\r\n\tCache descriptor\tLevel 2, U, 256 KB, 2 thread(s)\r\n\tCache descriptor\tLevel 3, U, 3 MB, 16 thread(s)\r\n\tFID/VID Control\t\tyes\r\n\r\n\r\n\tTurbo Mode\t\tnot supported\r\n\tMax turbo frequency\t2533 MHz\r\n\tMax non-turbo ratio\t19x\r\n\tMax turbo ratio\t\t19x\r\n\tMax efficiency ratio\t7x\r\n\tTDC Limit\t\t25 Amps\r\n\tMax bus number\t\t255\r\n\tAttached device\t\tPCI device at bus 255, device 2, function 1\r\n\r\n\tTemperature 0\t\t62 degC (143 degF) (Core #0)\r\n\tTemperature 1\t\t65 degC (149 degF) (Core #1)\r\n\tPower 00\t\t6.30 W (Package)\r\n\tClock Speed 0\t\t1596.19 MHz (Core #0)\r\n\tClock Speed 1\t\t1596.19 MHz (Core #1)\r\n\tCore 0 max ratio\t19.0 (effective 19.0)\r\n\tCore 1 max ratio\t19.0 (effective 19.0)", "@sharonevarghese,\r\nPlease refer this [link](https://superuser.com/questions/1251865/is-there-a-way-to-tell-if-my-hardware-supports-specific-instructions). Let us know if that helps. Thanks!", "`Instructions sets\tMMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T, VT-x` doesn't show AVX, so your CPU doesn't support it.\r\n\r\nYou can always compile from sources", "@sharonevarghese, Did you try the @mihaimaruseac's suggestion. ", "> \r\n> \r\n> @sharonevarghese, Did you try the @mihaimaruseac's suggestion.\r\n\r\ntrying to install. but not able to install the  install Bazel 0.23.0 as per the https://www.tensorflow.org/install/source_windows. kindly guide me through it", "@sharonevarghese,\r\nPlease follow the instructions mentioned in the [bazel website](https://docs.bazel.build/versions/0.24.0/install-windows.html). This [link](https://github.com/bazelbuild/bazel/releases/tag/0.23.0) provide the bazel 0.23.0.exe file for windows. Let us know how it progresses. Thanks!", "> \r\n> \r\n> @sharonevarghese, Did you try the @mihaimaruseac's suggestion.\r\n\r\ntrying bot cannot complete as held up in the previous replies", "@sharonevarghese, Did you install bazel using the instructions mentioned in the [bazel website](https://docs.bazel.build/versions/0.24.0/install-windows.html). OR if you are getting any error, please attach the error log here for faster move. Thanks! ", "### these are the steps i followed as per the bazel website\r\n![1](https://user-images.githubusercontent.com/55018635/65230672-e4110080-daeb-11e9-87c7-03221fb1af34.jpg)\r\n![2](https://user-images.githubusercontent.com/55018635/65230677-e7a48780-daeb-11e9-9945-0843a7ca189b.jpg)\r\n![3](https://user-images.githubusercontent.com/55018635/65230680-e8d5b480-daeb-11e9-9e80-c238354dba8f.jpg)\r\n\r\n### guide me through the installation", "> \r\n> \r\n> @sharonevarghese, Did you install bazel using the instructions mentioned in the [bazel website](https://docs.bazel.build/versions/0.24.0/install-windows.html). OR if you are getting any error, please attach the error log here for faster move. Thanks!\r\n\r\n![4](https://user-images.githubusercontent.com/55018635/65231323-09523e80-daed-11e9-9e41-569fd1f46f37.jpg)\r\n### how to find the \"binary\" file mentioned", "@sharonevarghese,\r\nIn this [link](https://github.com/bazelbuild/bazel/releases/tag/0.23.0), click on the Assets. There you can find the bazel binaries. Download the bazel-0.23.0-windows-x86_64.exe from it. Thanks!\r\n", "![5](https://user-images.githubusercontent.com/55018635/65237532-71f2e880-daf8-11e9-8270-2425bb30567d.jpg)\r\nkindly help", "@sharonevarghese, \r\nPlease follow the steps mentioned here https://docs.bazel.build/versions/0.24.0/install-windows.html. Thanks!", "@sharonevarghese,\r\nDid you build the Tensorflow using bazel. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32310\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32310\">No</a>\n"]}, {"number": 32309, "title": "Tensorflow Micro: Build issue", "body": "`micro_vision` and `micro_speech` examples are showing following error on invoking makefile to build\r\n```\r\n/bin/sh: 1: [[: not found\r\n```\r\n\r\nMy build command is:\r\n```\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=linux micro_vision\r\n```\r\nThe patch to fix this issue is:\r\n```\r\ndiff --git a/tensorflow/lite/experimental/micro/tools/make/Makefile b/tensorflow/lite/experimental/micro/tools/make/Makefile\r\nindex d2ed2e6234..2779530d02 100644\r\n--- a/tensorflow/lite/experimental/micro/tools/make/Makefile\r\n+++ b/tensorflow/lite/experimental/micro/tools/make/Makefile\r\n@@ -1,3 +1,4 @@\r\n+SHELL := /bin/bash\r\n \r\n ifneq (3.82,$(firstword $(sort $(MAKE_VERSION) 3.82)))\r\n   $(error \"Requires make version 3.82 or later (current is $(MAKE_VERSION))\")\r\n```\r\n\r\nTensorflow repo info:\r\n```\r\ncommit 92fc3d02fd3cd086e17659842bfee1bbd62d1838 (HEAD -> master)\r\nAuthor: Tiezhen WANG <wangtz@google.com>\r\nDate:   Thu Sep 5 07:10:01 2019 -0700\r\n\r\n    TFLM: Better format for debug log in `screen`\r\n    \r\n    Otherwise, it's showing up like\r\n    msg1\r\n        msg2\r\n            msg3\r\n    \r\n    PiperOrigin-RevId: 267365236\r\n```\r\nOS Platform and Distribution: Linux Ubuntu 18.04.3 LTS\r\nGCC/Compiler version: 7.3.1\r\n", "comments": ["Duplicate of #32037 \r\n\r\nPlease followup on the above bug.  We would like help testing a different fix for this issue.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32309\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32309\">No</a>\n", "This issue is still there. I am using Tensorflow tag v2.0.0."]}, {"number": 32308, "title": "Fix shape inference for default gradients of resources", "body": "Fix shape inference for default gradients of resources which are usually of the form:\r\n\r\n`tf.zeros(gen_resource_variable_ops.variable_shape(resource), dtype)`\r\nThis adds support for VariableShape to ShapeRefiner::ConstantPartialShape.\r\nThis change should provide better graph building shape inference but should not affect graph-building/runtime behavior.\r\n\r\nPiperOrigin-RevId: 267681703", "comments": ["Please make the PR against master and then cherry-pick. Thank you.", "Sorry, didn't notice this is a cherrypick"]}, {"number": 32307, "title": "Makefile to build TensorFlow Lite C library from source", "body": "After using `bazel` to build the library, I experienced performance hits compared to `make`.\r\n\r\nJust to keep record and for someone to reproduce my original issue, I left the `bazel` build steps in my repo here: [https://github.com/Jonarod/tensorflow_lite_alpine/blob/master/builder/Dockerfile.bazel](https://github.com/Jonarod/tensorflow_lite_alpine/blob/master/builder/Dockerfile.bazel).\r\n\r\nOn the same repo, one can build `libtensorflowlite_c.so` using `make` (using [this Dockerfile](https://github.com/Jonarod/tensorflow_lite_alpine/blob/master/builder/Dockerfile)) and/or `bazel` (using [this Dockerfile](https://github.com/Jonarod/tensorflow_lite_alpine/blob/master/builder/Dockerfile.bazel)) then compare performance on inference. You should see the `make`-build performs better than the `bazel` one.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32307) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32307) for more info**.\n\n<!-- ok -->", "What is the exact `bazel build` command you used?", "@jdduke [here](https://github.com/Jonarod/tensorflow_lite_alpine/blob/master/builder/Dockerfile.bazel#L73)", "> You should see the make-build performs better than the bazel one.\r\n\r\nCan you say more about the observed performance differences? How did you benchmark? Thanks for any info, we'd like to fully repro your conditions before committing to the Makefile build support.", "Sure !\r\n\r\nI used [go-tflite](https://github.com/mattn/go-tflite) that consumes the library as a C API. I started copy-pasting the example found [here](https://github.com/mattn/go-tflite/tree/master/_example/label_image) to run inference. The project requires you to have Golang installed to run the `main.go` script. The project also requires you to place `libtensorflowlite_c.so` in your `$GOPATH/tensorflow/tensorflow/tensorflow/lite/experimental/c/` and linker should also require you to place a copy of the same library in `/usr/local/lib/`.\r\n\r\nI just did exactly that, then I ran inference by switching between the bazel-based `libtensorflowlite_c.so` and the make-based `libtensorflowlite_c.so` (Trying 50 inferences with one and then 50 other inferences with the other version). I observed that bazel-based `libtensorflowlite_c.so` is consistently twice slower than the make-based one. For instance, on my CPU machine, the same inference script using the bazel-based library could classify the sample `peacock.png` image in 300ms (on average), meanwhile the make-based library would do the same in 120ms. I reproduced these metrics very consistently with any other sample image.\r\n\r\nI understand it is quite demanding to setup everything if you do not already work with Golang, for this reason I pushed a reproduction scenario for you [here](https://github.com/Jonarod/tensorflow_lite_alpine/tree/master/example). In this repo you will find:\r\n- `bazel_libtensorflowlite_c.so` bazel-based built using [this Dockerfile](https://github.com/Jonarod/tensorflow_lite_alpine/blob/master/builder/Dockerfile.bazel) (don't forget to trim off the `bazel_` before use)\r\n- `make_libtensorflowlite_c.so` make-based built using [this Dockerfile](https://github.com/Jonarod/tensorflow_lite_alpine/blob/master/builder/Dockerfile) (don't forget to trim off the `make_` before use)\r\n- the assets to run inference including the model used and its labels, as well as the sample image\r\n- my revised version of `main.go` with timings used for benchmarks\r\n- a `classify-gcc-t4` binary which is the same `main.go` using 4 threads but compiled using `gcc` (so you don't need to install golang to reproduce)\r\n- a `classify-musl-t4` binary which is the same `main.go` using 4 threads but compiled using `musl` (so you don't need to install golang to reproduce)\r\n- an example `Dockerfile` to see basic usage\r\n\r\nPing me if you need anything else.", "Thanks for the detailed repro steps? It's possible this is a simple function of how certain paths are switched on via default when using bazel (vs Makefile). Will try to repro on our end and get back to you, but a 2x performance difference is not expected (and hopefully we can get to performance parity with bazel).", "Jonarod, thanks for the patch.\r\nYou're using \"-O3\" for Makefile but bazel uses \"-O2\" for \"-c opt\".\r\nTo get the same performance with bazel,  you need to use \"bazel build -c opt --copt=-O3 <target>\". \r\nCould you verify this?", "A patch was just submitted to set -O3 for Linux build (including RPI3)"]}, {"number": 32306, "title": "[r2.0-CherryPick]:Keras SavedModel important bug fixes and refactor", "body": "1. Use of external tensors (e.g., generated by tf.sequence_mask) are converted to constants when saving.\r\n2. Fixed error preventing models with @tf.function decorated calls from being exported.\r\n3. Refactored Keras Serialization code (this change must be cherrypicked for the other two commits to be pushed)", "comments": []}, {"number": 32305, "title": "2.0.0-rc1 cherry-pick request: Import submodules using relative imports.", "body": "Import submodules using relative imports. This fixed a lot of the autocomplete (although not everything).", "comments": ["Seems like MacOS build failed. I will take a look.", "I think MacOS build might be failing because tensorboard it picks up doesn't have this change: https://github.com/tensorflow/tensorboard/pull/2593\r\n\r\nI will verify it though.", "So, the issue here is that tensorboard pip package is not installed. This change adds a test that tf.summary.image is available. This symbol is available in V1 without installing tensorboard, but would fail for r2.0 branch without the pip.\r\n\r\nNow, I got the package installed for the MacOS build but Ubuntu CPU still doesn't have it.", "TensorBoard is harder to install for Linux CPU build. So, for now I just disabled the test (added by this change) if `tensorboard` pip is not available."]}, {"number": 32304, "title": "Update installation guide for pip consolidation.", "body": "", "comments": ["Will send another change separately for docs repo."]}, {"number": 32303, "title": "[r2.0-CherryPick]:Add tf.saved_model.Asset public symbol.", "body": "`Asset` is the mechanism that allows to make hermetic SavedModels\r\nthat depend on files. It replaces functionality that on TF-1.x was\r\ntypically provided by the ASSET_FILEPATHS collection.\r\n\r\nPiperOrigin-RevId: 267534289", "comments": ["presubmits are failing, rerunning them", "the macos build failure in presubmit is due to tensorboard installation, that @annarev is looking into, but it is safe to merge this PR, as the other test failures are addressed."]}, {"number": 32302, "title": "Add support to cuDNN CTC loss", "body": "- This PR supports CUDNN CTC Loss as the backend of ctc_loss_v2()\r\n- Users need to use the environment variable TF_CUDNN_CTC_LOSS=1\r\n\r\n- Why can we make it default (not using TF_CUDNN_CTC_LOSS)?\r\n   - ctc_loss_v2 supports a variable blank index but will transpose it to the last index before calling the actual implementation. However, CUDNN implementation only supports the 0th blank index. This indicates ctc_loss_v2 has to select the correct implementation based on platform, which cannot be done inside operation definition. \r\n\r\n- What is the logic in the new ctc_loss_v2()?\r\n   - _ctc_loss_impl() will call the actual implementation based on given use_cudnn parameter\r\n      - If true, call the CUDNN implementation\r\n      - If false, call the original implementation \r\n   - ctc_loss_v2() will transpose the blank index to 0 if TF_CUDNN_CTC_LOSS=1 and then call _ctc_loss_impl(use_cudnn=true)\r\n   - ctc_loss_v2() will transpose the blank index to the last index if TF_CUDNN_CTC_LOSS=0 and then call _ctc_loss_impl(use_cudnn=false)\r\n\r\nfyi @nluehr ", "comments": ["@pkanwar23 would you please help to find someone to review this?", "@chsigg could you help to take a look at this? Thanks", "Adding Tim Shen to review the stream executor bits.", "@rmlarsen Tx for the commends. I have made several changes. PTAL.", "Can you address this comment from before? \r\n> This should replace the existing ctc loss for GPUs, right?\r\n\r\n", "> Can you address this comment from before?\r\n> \r\n> > This should replace the existing ctc loss for GPUs, right?\r\n\r\nYes, if the environment variable TF_CUDNN_CTC_LOSS is defined. The reason why I cannot make it as the default backend is that CPU and GPU require different data transpose before calling the CTC compute.", "Isn't this similar to how we deal with convolutions? Should we have some\ntype of layout argument / graph rewrite pass to address this?\n\nI worry this is a regression in usability as it stands...\n\nOn Wed, Nov 13, 2019 at 1:36 PM Kaixi Hou <notifications@github.com> wrote:\n\n> Can you address this comment from before?\n>\n> This should replace the existing ctc loss for GPUs, right?\n>\n> Yes, if the environment variable TF_CUDNN_CTC_LOSS is defined. The reason\n> why I cannot make it as the default backend is that CPU and GPU require\n> different data transpose before calling the CTC compute.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32302?email_source=notifications&email_token=AAABHROJOKBAXPKVSIV4WRTQTRXPVA5CNFSM4IUN3CM2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOED7XHNY#issuecomment-553612215>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNXOYCI6FJBGBNPKSDQTRXPVANCNFSM4IUN3CMQ>\n> .\n>\n\n\n-- \n - Alex\n", "@timshen91 Thanks for your clarification. I followed the convolution example and further simplified the code. PTAL.\r\nThx. ", "@timshen91 Thx for your comments. More changes are made accordingly. PTAL.", "Why do we  need an environment variable? Why can't we treat this the same way as we treat cudnn lstm (using the implementation selector API)? https://github.com/tensorflow/tensorflow/blob/c293fc8040144dd0efbc0f70e483550125fb146e/tensorflow/python/keras/layers/recurrent_v2.py#L1447\r\n\r\nOtherwise can't we push the extra transpose and whatnot inside the TF opkernel before dispatching to cudnn?", "@alextp \r\n >> Why do we need an environment variable?\r\n\r\nThe CPU/GPU preprocessing for the CTC loss is different. So I proposed the environment variable to let users to decide which one to use.\r\n>> Why can't we treat this the same way as we treat cudnn lstm (using the implementation selector API)?\r\n\r\nYes, I am also thinking of using the implementation selector you pointed to. We can define two TF functions like:\r\n```\r\nfunc1 = preprocessing1 + CPU_CTC_loss \r\nfunc2 = preprocessing2 + GPU_CTC_loss\r\ncall func1 and register func2\r\n``` \r\n@qlzh727 Please correct me if I am wrong.", "@sanjoy Thx for the review. More changes are made based on your comments. PTAL.\r\n\r\n>> Should we register the kernel even if cuDNN is older than 7.6.3 (and the kernel is guaranteed to fail)?\r\n\r\nIf users explicitly set the env var TF_CUDNN_CTC_LOSS and their cuDNN is < 7.6.3, it will error out from the stream exector during runtime (as we did for the RNN: if users set the variable sequence length params and their cuDNN < 7.2.1, we will error out. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1729).", "> @sanjoy Thx for the review. More changes are made based on your comments. PTAL.\r\n> \r\n> > > Should we register the kernel even if cuDNN is older than 7.6.3 (and the kernel is guaranteed to fail)?\r\n> \r\n> If users explicitly set the env var TF_CUDNN_CTC_LOSS and their cuDNN is < 7.6.3, it will error out from the stream exector during runtime (as we did for the RNN: if users set the variable sequence length params and their cuDNN < 7.2.1, we will error out. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1729).\r\n\r\nLet me rephrase: what is the use case for even registering a kernel that will always fail during runtime?  Since that's what will happen if cuDNN is older than 7.6.3 right?", "Do you think we need to add cudnn version checking macros to protect this?\n\nOn Thu, Dec 5, 2019 at 12:25 PM Sanjoy Das <notifications@github.com> wrote:\n\n> @sanjoy <https://github.com/sanjoy> Thx for the review. More changes are\n> made based on your comments. PTAL.\n>\n> Should we register the kernel even if cuDNN is older than 7.6.3 (and the\n> kernel is guaranteed to fail)?\n>\n> If users explicitly set the env var TF_CUDNN_CTC_LOSS and their cuDNN is <\n> 7.6.3, it will error out from the stream exector during runtime (as we did\n> for the RNN: if users set the variable sequence length params and their\n> cuDNN < 7.2.1, we will error out.\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1729\n> ).\n>\n> Let me rephrase: what is the use case for even registering a kernel that\n> will always fail during runtime? Since that's what will happen if cuDNN is\n> older than 7.6.3 right?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32302?email_source=notifications&email_token=AA6Q5EAZ3DAU7Y7JI6QQEO3QXFPUNA5CNFSM4IUN3CM2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGCA5NA#issuecomment-562302644>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AA6Q5EAPDXFS4XIIWKALALTQXFPUNANCNFSM4IUN3CMQ>\n> .\n>\n", "> Do you think we need to add cudnn version checking macros to protect this?\r\n\r\nYeah, I'm asking why not do:\r\n\r\n```\r\n#if GOOGLE_CUDA && CUDNN_VERSION >= 7603\r\nclass CTCLossOpGPU : public OpKernel {\r\n public:\r\n  explicit CTCLossOpGPU(OpKernelConstruction* ctx) : OpKernel(ctx) {\r\n    bool preprocess_collapse_repeated_;\r\n    bool ctc_merge_repeated_;\r\n    bool ignore_longer_outputs_than_inputs_;\r\n    ...\r\n\r\n private:\r\n  TF_DISALLOW_COPY_AND_ASSIGN(CTCLossOpGPU);\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"CTCLossV2\").Device(DEVICE_GPU)\r\n                                       .HostMemory(\"labels_indices\")\r\n                                       .HostMemory(\"labels_values\")\r\n                                       .HostMemory(\"sequence_length\"),\r\n                        CTCLossOpGPU);\r\n#endif\r\n```\r\n\r\nBtw, this is not a rhetorical question.  If you have a use case for always registering the kernel then I'm happy as well.", "> If you have a use case for always registering the kernel then I'm happy as well.\r\n\r\nYeah, we could do as you suggested. But I think it would bring more problems. What about the `REGISTER_OP(\"CTCLossV2\")` in `tensorflow/core/ops/ctc_ops.cc`. And what happens in python when we call `gen_ctc_ops.ctc_loss_v2`. Do we also need to protect them? It sounds like every time when we have some new op, we need to protect all the related code from front end python API, kernel definition, all the way to the stream executor. (I think that is why in the RNN we only have the macro protect code only in stream executor.)  \r\n\r\nAlso, I have never seen any example to protect the register-op codes using CUDNN_VERSION in the tensorflow/core/kernels (Please correct me if I am wrong.). ", "> > If you have a use case for always registering the kernel then I'm happy as well.\r\n> \r\n> Yeah, we could do as you suggested. But I think it would bring more problems. What about the `REGISTER_OP(\"CTCLossV2\")` in `tensorflow/core/ops/ctc_ops.cc`.\r\n\r\nI don't think we should guard the **op** because we could (in the future) have a CPU or CUDA kernel for it that does not depend on cuDNN.  I agree that piping in the cuDNN version check all the way up to Python would cumbersome.\r\n\r\nThe problem I have with the current formulation is that we're pretending that we have a GPU implementation for cuDNN < 7.6.3, even though we really don't have an implementation.", "@sanjoy Done. PTAL.", "@sanjoy Yes, I went though the code again and the empty class of CtcLossDescriptor in dnn.h seems to be unnecessary. Removed it. PTAL.", "@houtoms Can you please resolve conflicts? Thanks!", "The conflict is solved. PTAL. Thx.", "@sanjoy Thanks for pointing out. Removed that empty class. PTAL.", "> Other than the minor comment the non-Python code looks good to me. If Alex and Rasmus are OK with the API then this is good to go.\r\n> \r\n> /CC @alextp @rmlarsen\r\n\r\nThis still applies.", "@sanjoy @alextp  , I have replaced the previous environment variable with the implementation selector (Thx @qlzh727 for helping me out with some test cases.) Now, we don't need the env var to control if cuDNN is used or not. The runtime can automatically determine that if GPU is available or not.\r\n\r\nI added another python function to contain this new implement (ie. ctc_loss_v3), which is only available in TF2.\r\n\r\nPlease help me find the reviewers to review this part. Thx.", "Anything else I can do? Thx.", "Thanks for checking. We should be good. I'm looking at why it hasn't merged.", "> Thanks for checking. We should be good. I'm looking at why it hasn't merged.\r\n\r\nIt was waiting for an approval from me for some reason.  Should be good to go now.", "Thx for the update."]}, {"number": 32301, "title": "[r2.0-CherryPick]: Adds support for generator inputs w/ varying batch sizes & shapes n t\u2026", "body": "\u2026he keras unified execution path\r\n\r\nPiperOrigin-RevId: 267686850", "comments": []}, {"number": 32300, "title": "[r2.0-CherryPick]:Implementing RFC#126: Allow Op names of the form RepoName>OpName", "body": "This change maps '>' in the op names to underscores in the generated python op function names and export names.\r\n\r\nGetting '.' in the names instead is theoretically doable but would add too much complexity to the whole codegen loop to be worthwhile.\r\n(It would require analyzing the op names to group ops by their respective nested namespaces, code-gen'ing nested python classes that match the namespaces, then indenting the codegen'd python ops inside of the nested classes w/ the names set correctly).", "comments": []}, {"number": 32299, "title": "RNN stateful is incompatible with initial_state", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): binary via conda (1.14.0) and pip (2.0.0-rc0)\r\n- TensorFlow version (use command below): 1.14.0 and 2.0.0-rc0\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 1080 Ti\r\n\r\n**Describe the current behavior**\r\nWhen a stateful RNN is created and is given initial_state, it effectively resets the state to the initial state for every prediction. See the \"NOT EXPECTED\" example in the code below.\r\n\r\n**Describe the expected behavior**\r\nThe stateful RNN should use the initial state the first time, and then update the state and use it for each following time.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import backend as K\r\nimport numpy as np\r\n\r\nINPUT_SIZE = 4\r\nBATCH_SIZE = 1\r\nOUTPUT_SIZE = 2\r\n\r\n# Define a model without any state initialization\r\ninput_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)\r\nrnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer)\r\nmodel = keras.Model(input_layer, rnn_layer)\r\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\r\n\r\ntest = np.full((BATCH_SIZE,1,INPUT_SIZE), 0.5)\r\n\r\n# This generates different predictions for the two time steps, as expected\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\nprint(model.predict(test))\r\n# [[[0.15479106 0.3035699 ]]]\r\n# [[[0.22833839 0.4250579 ]]]\r\n\r\n# This generates the same prediction after resetting the state, as expected\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\n# [[[0.15479106 0.3035699 ]]]\r\n# [[[0.15479106 0.3035699 ]]]\r\n\r\n# Define a model with a constant state initialization\r\ninput_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)\r\ninitial_state_layer = K.constant(np.ones(OUTPUT_SIZE), shape=(1, OUTPUT_SIZE))\r\nrnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)\r\nmodel = keras.Model(input_layer, rnn_layer)\r\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\r\n\r\n# This generates the same prediction for the two time steps, NOT EXPECTED\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\nprint(model.predict(test))\r\n# [[[0.23247536 0.8585994 ]]]\r\n# [[[0.23247536 0.8585994 ]]]\r\n\r\n# This generates the same prediction after resetting the state, as expected\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\nmodel.reset_states()\r\nprint(model.predict(test))\r\n# [[[0.23247536 0.8585994 ]]]\r\n# [[[0.23247536 0.8585994 ]]]\r\n```\r\n", "comments": ["I am able to reproduce the issue on Colab with Tensorflow 2.0.0.rc0. Please see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/f2f267963a9047f5804fcd0892824762/untitled130.ipynb). ", "Thanks for reporting the issue, I can reproduce it even with 1.13, which means this bug has been there for long time. Let me check what's the root cause there.", "Adding Francois since this probably impact the keras-team/keras as well. I think currently the initial_state is taking priority if specified, and I feel the state from previous batch should be respected if stateful=True.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32299\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32299\">No</a>\n", "Thanks a lot for the quick followup, however I don't think the fix is correct. It seems that now the initial_state is being ignored.\r\n\r\nHere's an experiment I ran after making the changes locally to `recurrent.py` of 2.0.0-rc0 as in https://github.com/tensorflow/tensorflow/commit/dccf1c7030867865189924989c857bc1d3454216:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import backend as K\r\nimport numpy as np\r\n\r\nINPUT_SIZE = 4\r\nBATCH_SIZE = 1\r\nOUTPUT_SIZE = 2\r\n\r\ntest_input = np.full((BATCH_SIZE,1,INPUT_SIZE), 0.5)\r\n\r\n# Define some fixed weights for model consistency\r\n# These were obtained from model.layers[1].get_weights() on the model created below\r\nfixed_weights = [np.array([[ 0.43783283, -0.6397302 , -0.7594154 ,  0.2558545 ,  0.57092595,  0.35916996],\r\n        [-0.30622163,  0.1416316 , -0.25233066,  0.53959405,  0.22741866,  0.03710318],\r\n        [-0.3309809 ,  0.1201275 , -0.4858916 ,  0.4731754 ,  0.3292985 , -0.06378067],\r\n        [-0.6220065 ,  0.27017498,  0.03370708,  0.00933939, -0.56660485,  0.33552015]], dtype=np.float32),\r\n np.array([[-0.5473411 ,  0.36915207, -0.14257154, -0.32368308,  0.43306705,  0.50149775],\r\n        [ 0.33224842,  0.42010546, -0.77206314,  0.05650809, -0.30846536,  0.13673659]], dtype=np.float32),\r\n np.array([[0., 0., 0., 0., 0., 0.],\r\n        [0., 0., 0., 0., 0., 0.]], dtype=np.float32)]\r\n\r\n# Define a model with a constant state initialization to ONES\r\ninput_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)\r\ninitial_state_layer = K.constant(np.ones((1, OUTPUT_SIZE)))\r\nrnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)\r\nmodel = keras.Model(input_layer, rnn_layer)\r\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\r\n\r\n# Load the fixed weights\r\nmodel.layers[1].set_weights(fixed_weights)\r\n\r\n# Verify that this generates different predictions for the two time steps, as expected\r\nmodel.reset_states()\r\nprint(model.predict(test_input))\r\nprint(model.predict(test_input))\r\n#[[[0.16437379 0.1653973 ]]]\r\n#[[[0.23415121 0.2692895 ]]]\r\n\r\n# Verify that different predictions are made after explicitly setting the state to ZEROS and ONES, as expected\r\n# HOWEVER, the output from setting state to ZEROS matches the output above using the initial_state_layer of ONES\r\n# AND, the output from setting state to ONES does NOT match the output above using the initial_state_layer of ONES\r\n# This is NOT EXPECTED\r\nmodel.layers[1].reset_states(states=np.zeros((1, OUTPUT_SIZE)))\r\nprint(model.predict(test_input))\r\nmodel.layers[1].reset_states(states=np.ones((1, OUTPUT_SIZE)))\r\nprint(model.predict(test_input))\r\n#[[[0.16437379 0.1653973 ]]]\r\n#[[[0.5386554 0.8742118]]]\r\n\r\n# Define a model with a constant state initialization to ZEROS\r\ninput_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)\r\ninitial_state_layer = K.constant(np.zeros((1, OUTPUT_SIZE)))\r\nrnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)\r\nmodel = keras.Model(input_layer, rnn_layer)\r\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\r\n\r\n# Load the fixed weights\r\nmodel.layers[1].set_weights(fixed_weights)\r\n\r\n# Verify that this generates different predictions for the two time steps, as expected\r\n# HOWEVER, the states output matches the model with initial_state_layer of ONES, which is NOT EXPECTED\r\nmodel.reset_states()\r\nprint(model.predict(test_input))\r\nprint(model.predict(test_input))\r\n#[[[0.16437379 0.1653973 ]]]\r\n#[[[0.23415121 0.2692895 ]]]\r\n```\r\n\r\nThe experiment suggests that the supplied initial_state is ignored and zeros initialization (the default for GRU, I think) is used in its place. When the model is built with a `K.constant` initial state of `ones`, it should generate the same output as when ones are supplied to `model.layers[1].reset_states`. Also, that output should be different than when the model is built with a `K.constant` initial state of `zeros`.", "I just edited my experiment code because I had a typo. The second version of the model was incorrectly set to ones, but now I fixed it to use zeros. Now the code illustrates the issue.", "This is probably a better test to illustrate the issue. What we want is for the stateful=True model to match the predictions of the stateful=False model when an initial_state is specified. This experiment creates a stateful=False `model1` with non-zero initial state and uses it to predict 2 time steps. Then we build an equivalent stateful=True `model2` and see that its predictions are different when they should match `model1`. Then `model3` uses stateful=False and no initial state and it matches the output from `model2` when it shouldn't (though it correctly differs from `model1`).\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import backend as K\r\nimport numpy as np\r\n\r\nINPUT_SIZE = 4\r\nBATCH_SIZE = 1\r\nOUTPUT_SIZE = 2\r\n\r\ntest_input_one_step = np.full((BATCH_SIZE,1,INPUT_SIZE), 0.5)\r\ntest_input_two_steps = np.full((BATCH_SIZE,2,INPUT_SIZE), 0.5)\r\n\r\n# Define some fixed weights for model consistency\r\n# These were obtained from model.layers[1].get_weights() on the model created below\r\nfixed_weights = [np.array([[ 0.43783283, -0.6397302 , -0.7594154 ,  0.2558545 ,  0.57092595,  0.35916996],\r\n        [-0.30622163,  0.1416316 , -0.25233066,  0.53959405,  0.22741866,  0.03710318],\r\n        [-0.3309809 ,  0.1201275 , -0.4858916 ,  0.4731754 ,  0.3292985 , -0.06378067],\r\n        [-0.6220065 ,  0.27017498,  0.03370708,  0.00933939, -0.56660485,  0.33552015]], dtype=np.float32),\r\n np.array([[-0.5473411 ,  0.36915207, -0.14257154, -0.32368308,  0.43306705,  0.50149775],\r\n        [ 0.33224842,  0.42010546, -0.77206314,  0.05650809, -0.30846536,  0.13673659]], dtype=np.float32),\r\n np.array([[0., 0., 0., 0., 0., 0.],\r\n        [0., 0., 0., 0., 0., 0.]], dtype=np.float32)]\r\n\r\n# model1: a stateful=False model with a non-zero constant state initialization\r\ninput_layer = keras.Input(shape=(None, INPUT_SIZE))\r\ninitial_state_layer = K.constant(np.ones((1, OUTPUT_SIZE)))\r\nrnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=False)(input_layer, initial_state=initial_state_layer)\r\nmodel1 = keras.Model(input_layer, rnn_layer)\r\nmodel1.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\r\n\r\n# Load the fixed weights\r\nmodel1.layers[1].set_weights(fixed_weights)\r\n\r\n# Predict for 2 time steps\r\nmodel1.reset_states()\r\nprint(model1.predict(test_input_two_steps))\r\n#[[[0.5386554  0.8742118 ]\r\n#  [0.3751272  0.74190784]]]\r\n\r\n# model2: a stateful=True model with a non-zero constant state initialization\r\ninput_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)\r\ninitial_state_layer = K.constant(np.ones((1, OUTPUT_SIZE)))\r\nrnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)\r\nmodel2 = keras.Model(input_layer, rnn_layer)\r\nmodel2.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\r\n\r\n# Load the fixed weights\r\nmodel2.layers[1].set_weights(fixed_weights)\r\n\r\n# This generates different predictions from model1, which is NOT EXPECTED\r\nmodel2.reset_states()\r\nprint(model2.predict(test_input_one_step))\r\nprint(model2.predict(test_input_one_step))\r\n#[[[0.16437379 0.1653973 ]]]\r\n#[[[0.23415121 0.2692895 ]]]\r\n\r\n# model3: a stateful=False model with no state initialization (uses zeros by default)\r\ninput_layer = keras.Input(shape=(None, INPUT_SIZE))\r\nrnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=False)(input_layer)\r\nmodel3 = keras.Model(input_layer, rnn_layer)\r\nmodel3.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\r\n\r\n# Load the fixed weights\r\nmodel3.layers[1].set_weights(fixed_weights)\r\n\r\n# Note that model2's predictions match model3's when they should instead match model1's.\r\nmodel3.reset_states()\r\nprint(model3.predict(test_input_two_steps))\r\n#[[[0.16437379 0.1653973 ]\r\n#  [0.23415121 0.2692895 ]]]\r\n```\r\n\r\nI hope that helps.", "Thanks for catching this. Sending fix very soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32299\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32299\">No</a>\n", "Thanks. I think this is very close, but now it seems that there is a weird corner case where using `reset_states` to set the state to zeros has no effect. See this:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import backend as K\r\nimport numpy as np\r\n\r\nINPUT_SIZE = 4\r\nBATCH_SIZE = 1\r\nOUTPUT_SIZE = 2\r\n\r\ntest_input = np.full((BATCH_SIZE,1,INPUT_SIZE), 0.5)\r\n\r\n# Define some fixed weights for model consistency\r\n# These were obtained from model.layers[1].get_weights() on the model created below\r\nfixed_weights = [np.array([[ 0.43783283, -0.6397302 , -0.7594154 ,  0.2558545 ,  0.57092595,  0.35916996],\r\n        [-0.30622163,  0.1416316 , -0.25233066,  0.53959405,  0.22741866,  0.03710318],\r\n        [-0.3309809 ,  0.1201275 , -0.4858916 ,  0.4731754 ,  0.3292985 , -0.06378067],\r\n        [-0.6220065 ,  0.27017498,  0.03370708,  0.00933939, -0.56660485,  0.33552015]], dtype=np.float32),\r\n np.array([[-0.5473411 ,  0.36915207, -0.14257154, -0.32368308,  0.43306705,  0.50149775],\r\n        [ 0.33224842,  0.42010546, -0.77206314,  0.05650809, -0.30846536,  0.13673659]], dtype=np.float32),\r\n np.array([[0., 0., 0., 0., 0., 0.],\r\n        [0., 0., 0., 0., 0., 0.]], dtype=np.float32)]\r\n\r\n# Define a model with a constant state initialization to ONES\r\ninput_layer = keras.Input(shape=(None, INPUT_SIZE), batch_size=BATCH_SIZE)\r\ninitial_state_layer = K.constant(np.ones((1, OUTPUT_SIZE)))\r\nrnn_layer = keras.layers.GRU(units=OUTPUT_SIZE, return_sequences=True, stateful=True)(input_layer, initial_state=initial_state_layer)\r\nmodel = keras.Model(input_layer, rnn_layer)\r\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss='mse')\r\n\r\n# Load the fixed weights\r\nmodel.layers[1].set_weights(fixed_weights)\r\n\r\n# See what the model predicts when the initial state is ONES\r\nmodel.reset_states()\r\nprint(model.predict(test_input), \"expecting [[[0.5386554 0.8742118]]]\")\r\n\r\n# The same predictions are made after explicitly setting the state, which fails when setting to zeros\r\nmodel.layers[1].reset_states(states=np.zeros((1, OUTPUT_SIZE)))\r\nprint(model.predict(test_input), \"expecting [[[0.16437379 0.1653973 ]]]\") # FAILS HERE\r\nmodel.layers[1].reset_states(states=np.ones((1, OUTPUT_SIZE)))\r\nprint(model.predict(test_input), \"expecting [[[0.5386554 0.8742118]]]\") # correct\r\nmodel.layers[1].reset_states(states=np.full((1, OUTPUT_SIZE), 0.5))\r\nprint(model.predict(test_input), \"expecting [[[0.36637056 0.49489087]]]\") # correct\r\n```", "Thanks for pointing out the issue again. This corner case is actually hard to distinguish from the graph level. Since the initial_state param is always provided in the graph, there isn't an easy way to know whether the initial_state or the recorded state is the latest.\r\n\r\nFor now, I am inclined to leave it as is, since the most common use case is having a stateful RNN layer, and call model.reset_state().\r\n\r\nPlease let me know if you have other opinions.\r\n\r\nThanks."]}, {"number": 32298, "title": "[r2.0-CherryPick]:Keras SavedModel important bug fixes and refactor", "body": "1. Use of external tensors (e.g., generated by tf.sequence_mask) are converted to constants when saving.\r\n2. Fixed error preventing models with @tf.function decorated calls from being exported.\r\n3. Refactored Keras Serialization code (this change must be cherrypicked for the other two commits to be pushed)", "comments": ["Closing this, new PR here: #32306"]}, {"number": 32297, "title": "[r2.0 CherryPick]: Upgrading giflib to fix CVE-2019-15133", "body": "Add a patch file to fix giflib's compilation issue on Windows (replace a call to strtok_r with strtok_s).\r\nBased on PR: #32169.\r\n\r\n**NVD**: 2019/08/17 - CVSS v2.0 Base Score: 4.3 - CVSS v3.0 Base Score: 6.5\r\nIn GIFLIB before 2019-02-16, a malformed GIF file triggers a divide-by-zero exception in the decoder function DGifSlurp in dgif_lib.c if the height field of the ImageSize data structure is equal to zero.\r\n\r\nSource | Link | Type\r\n---- | ---- | ----\r\nMISC | bugs.chromium.org | Mailing List, Third Party Advisory\r\nUBUNTU | usn.ubuntu.com | Third Party Advisory\r\n\r\nPiperOrigin-RevId: 267533902", "comments": []}, {"number": 32296, "title": "sort NcclManager::Collective participants using device ID", "body": "Occasionally, the nccl_manager_test subtest `NcclManagerTest/0.BasicAllGather` would fail because the `NcclManagerTest/0.BasicSumReduction` would produce a device-to-rank ordering that might not be monotonically increasing.\r\n\r\n```\r\nhostname:25909:25993 [0] NCCL INFO Ring 00 :    0   1   2   3\r\nhostname:25909:25994 [1] NCCL INFO Ring 00 : 1[2] -> 2[1] via direct shared memory\r\nhostname:25909:25993 [0] NCCL INFO Ring 00 : 0[0] -> 1[2] via direct shared memory\r\nhostname:25909:25996 [3] NCCL INFO Ring 00 : 3[3] -> 0[0] via direct shared memory\r\nhostname:25909:25995 [2] NCCL INFO Ring 00 : 2[1] -> 3[3] via direct shared memory\r\n```\r\n\r\nThis communicator would be reused by the `BasicAllGather` test.  However, the allgather reduction is expected to be ordered by rank; no other collective result depends on rank order.  Sorting the participants based on the device ID resolves the failed test.", "comments": ["@rthadur Can you please help merge this PR?", "@dagamayank this PR has failed some internal tests and @chsigg is looking in to it, thanks for your patience.", "@rthadur and @chsigg thanks for looking into this PR as it is the only gating PR right now to bring back [ROCm nightly Community Supported Builds](https://github.com/tensorflow/tensorflow#community-supported-builds) to life.", "@chsigg please let me know if there is anything I can do from my end to help expedite this work.  Thanks.", "Sorry for dropping the ball. Will try to get it merged ASAP."]}, {"number": 32295, "title": "[r1.15 cherrypick] Remove cuda from path for windows integration build.", "body": "PiperOrigin-RevId: 264668323", "comments": []}, {"number": 32294, "title": "Unclear error", "body": "When I ran converter from tf to tf lite I have this error:\r\n`F tensorflow/lite/toco/graph_transformations/propagate_array_data_types.cc:305] Check failed: op->inputs.size() > 0 (0 vs. 0)`\r\nOr sometimes this error:\r\n`F tensorflow/lite/toco/tooling_util.cc:644] Check failed: dim >= 1 (0 vs. 1)`\r\n\r\nCan you explain, what is this errors mean?\r\nCan you make errors in tflite converter more readable?\r\n", "comments": ["The checks are internal from the code. You can locate the file (i.e. `tensorflow/lite/toco/tooling_util.cc`) and the line number (`644`) and from there go to the check (I'm linking to code on `r1.14` branch, assuming you got the error from 1.14 release; please use the corresponding branch when searching):\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/lite/toco/tooling_util.cc#L644\r\n\r\nThis is just checking that `dim >= 1` and reporting a failure otherwise. It's mostly for developers but might also mean an error in your code to be converted.", "I've seen this code at this branch. But at the moment I don't know, what problem with my model.\r\nThis my code to be converted:\r\n```\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph('graph', ['asr_model/features'],['asr_model/dnn/fast_log_posteriors/Reshape_1'], input_shapes={'asr_model/features': [None, 200, 40]})\r\ntflite_model = converter.convert()\r\nwith open('converted_model.tflite', 'wb') as fout:\r\n     fout.write(tflite_model) \r\n```\r\nMay be problem in some layer of my network, but I can't find this layer, because error is uncleared.", "Better to ask on stack overflow, as this is not a fault of the TF code.", "Fault of TF code, that this error is unclear.", "Based on the information that is available to the function, this error is as clear as possible.\r\n\r\nAlso, these errors are more for developers, they are assertions.", "But this function can be boolean and return false. And upper function can raise more readable error. And if this assertion for developers, why I get it, when I run my code?", "We cannot update all assertions in the way you suggest. It's better to debug the model yourself, editing it, layer by layer.", "But more easier, it's write good errors output in tflite converter code.", "Can you post the full output when running the converter?", "Yes, of course.\r\n```\r\n2019-09-08 20:41:57.632287: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2600215000 Hz\r\n2019-09-08 20:41:57.633731: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5375a50 executing computations on platform Host. Devices:\r\n2019-09-08 20:41:57.633766: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-09-08 20:41:57.909207: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-09-08 20:41:57.909370: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-09-08 20:41:58.581789: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-09-08 20:41:58.581898: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 699 nodes (-131), 823 edges (-136), time = 629.479ms.\r\n2019-09-08 20:41:58.581918: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 699 nodes (0), 823 edges (0), time = 22.309ms.\r\nTraceback (most recent call last):\r\n  File \"./main.py\", line 7, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/alexrak/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 898, in convert\r\n    **converter_kwargs)\r\n  File \"/home/alexrak/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 404, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/alexrak/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-09-08 20:42:00.710845: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 326 operators, 690 arrays (0 quantized)\r\n2019-09-08 20:42:00.718460: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 326 operators, 690 arrays (0 quantized)\r\n2019-09-08 20:42:00.727214: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 271 operators, 573 arrays (0 quantized)\r\n2019-09-08 20:42:00.738238: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 265 operators, 567 arrays (0 quantized)\r\n2019-09-08 20:42:00.742463: F tensorflow/lite/toco/tooling_util.cc:644] Check failed: dim >= 1 (0 vs. 1)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007fa087da5740 (most recent call first):\r\n  File \"/home/alexrak/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/alexrak/.local/lib/python3.6/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/home/alexrak/.local/lib/python3.6/site-packages/absl/app.py\", line 299 in run\r\n  File \"/home/alexrak/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/alexrak/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/alexrak/.local/bin/toco_from_protos\", line 10 in <module>\r\n```", "Are you using LSTM's in your model? Also what version of TF are you using? Thanks!", "Is this still an issue?\r\n", "I'm not using lstm in my model.", "Do you still have the code that produced the frozen graph?", "Sorry for late reply.\r\n\r\nThe current error reporting is not ideal in TF Lite converter, which makes the debugging hard to do. However we are working on a new converter which can provide better code location tracking.\r\n\r\nCould you try adding some debug info here to see which graph transformation causes the issue?\r\nhttps://github.com/tensorflow/tensorflow/blob/df191feaa323b4b35d3c45effecbba38fbcb550e/tensorflow/lite/toco/graph_transformations/graph_transformations.cc#L196\r\n\r\nIf you could share the frozen graph that has the issue, it's easier for me to debug. Thanks.", "closing due to in-activity."]}, {"number": 32293, "title": "AttributeError: module 'tensorflow' has no attribute 'optimizers'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tf-nightly-2.0-preview\r\n- TensorFlow version (use command below):\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.optimizers\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'optimizers'\r\n>>> tf.losses\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'losses'```\r\n```\r\n\r\n**Describe the expected behavior**\r\n`tf.optimizers`, `tf.losses` were present.\r\n\r\ncc @martinwicke ", "comments": ["I'll make a fix similar to 18c2cf989a2263ee212fbd5ac0b3085d9450b80a, need to check that this works.\r\n\r\nSorry about breakage.", "There is a fix, will land soon and then tonight's nightlies should be ok."]}, {"number": 32292, "title": "[r2.0-CherryPick]:Fix loss computation when y_true and y_pred is not same shape.", "body": "PiperOrigin-RevId: 267595602", "comments": []}, {"number": 32291, "title": "[TF.1.14] Train on TPU Pod with tf.keras ", "body": "Can I train on a TPU Pod -not a single device- with tf.keras and how on TF1.14?\r\n\r\nThank you", "comments": ["Yes you can train on cloud tpu with tf.keras https://www.tensorflow.org/guide/using_tpu\r\nFor more context see https://github.com/tensorflow/tensorflow/issues/27339#issuecomment-482716960 ", "@ymodak Thanks for the response, but the link contains documentation only for tpu.estimator, not tf.keras.  Could you provide a mini example of tf.keras code that runs on TPU Pod using tf.1.14?\r\n\r\nThank you", "Here is a colab for tf.keras executing on tpu.\r\nhttps://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32291\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32291\">No</a>\n"]}, {"number": 32290, "title": "UnsatisfiedLinkError for libtensorflow_jni.so when trying to load a newer model ", "body": "### System information\r\nSee attached [tf_env.txt](https://github.com/tensorflow/tensorflow/files/3584174/tf_env.txt) generated by `tf_env_collect.sh`.\r\n\r\ntf.version.VERSION **1.14.0**\r\ntf.version.GIT_VERSION unknown\r\n\r\n### Describe the problem\r\nWe have been successfully training Tensorflow models and loading them into Java project for inference. We have been training models using Tensorflow [1.8.0 - 1.10.0] and loading them into a gradle project compiled with 1.8.0. It worked.\r\n```\r\ndependencies {\r\n\r\n    compile(\r\n            \"org.tensorflow:tensorflow:\" + \"1.8.0\",\r\n            \"com.google.guava:guava:\" + guavaVersion,\r\n            \"com.opencsv:opencsv:\" + openCSVVersion,\r\n    )\r\n```\r\nRecently we have retrained the models using Tensorflow **1.14.0**. After replacing the older models and raising the gradle version number to 1.14.0\r\n```\r\ndependencies {\r\n\r\n    compile(\r\n            \"org.tensorflow:tensorflow:\" + \"1.14.0\",\r\n            \"com.google.guava:guava:\" + guavaVersion,\r\n            \"com.opencsv:opencsv:\" + openCSVVersion,\r\n    )\r\n```\r\n\r\none of the tests that attempts to load the model started throwing the following exception below:\r\n```\r\njava.lang.UnsatisfiedLinkError: /tmp/tensorflow_native_libraries-1567776737834-0/libtensorflow_jni.so: /tmp/tensorflow_native_libraries-1567776737834-0/libtensorflow_jni.so: undefined symbol: _ZN11tensorflow10FileSystem20RecursivelyCreateDirERKSs\r\n\r\n\tat java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\n\tat java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)\r\n\tat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)\r\n\tat java.lang.Runtime.load0(Runtime.java:809)\r\n\tat java.lang.System.load(System.java:1086)\r\n\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:101)\r\n\tat org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n\tat org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)\r\n\tat org.tensorflow.SavedModelBundle.<clinit>(SavedModelBundle.java:170)\r\n```\r\n\r\nThis does not look like a bug issue but it would be nice if you could provide some tips on how to fix this on our end. Thank you.\r\n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/30635", "waiting for answer"]}, {"number": 32289, "title": "Training a trivial keras model is not even close to deterministic despite call to set_random_seed.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip (binary)\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.5\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\noutput of tf_env_collect.sh:\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3584142/tf_env.txt)\r\n\r\n**Describe the current behavior**\r\nTraining a trivial one layer keras model is not deterministic.\r\n\r\n**Describe the expected behavior**\r\nI would like to initialize the seeds and train deterministically, at least up to floating point precision. \r\n\r\nWe are having severe problems with non-deterministic network behavior with more complex models (for both training an inference), and I minimized my example all the way down to a single layer. \r\n\r\nI have included a failing test case:\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy\r\nimport tensorflow\r\nfrom tensorflow import Session, global_variables_initializer, local_variables_initializer\r\nfrom tensorflow.python import debug as tf_debug\r\nfrom tensorflow.compat.v1 import Session\r\nfrom tensorflow.compat.v1.keras.backend import set_session\r\nfrom tensorflow.keras.layers import Conv2D, Input\r\nfrom tensorflow.keras.models import Model\r\n\r\ndef call_in_new_session():\r\n    import random\r\n    random.seed(0)\r\n\r\n    tensorflow.set_random_seed(1234)\r\n\r\n    config = tensorflow.ConfigProto(\r\n        device_count={'CPU': 1},\r\n      intra_op_parallelism_threads=1,\r\n      inter_op_parallelism_threads=1)\r\n    session = Session(config=config)\r\n\r\n    set_session(session)\r\n    with session.as_default():\r\n        print('---------------New Tensorflow Sesssion--------------------')\r\n        model_shape = [1024,1024,3]\r\n\r\n        numpy.random.seed(0)\r\n        X = numpy.random.randn(1,*model_shape) # batch dimension 1\r\n        y = numpy.random.randn(1,*model_shape)\r\n        #print(X.sum()) # Numpy is determinisic here.\r\n\r\n        tensorflow.compat.v1.random.set_random_seed(1234)\r\n\r\n        i = Input(shape=model_shape, name='i')\r\n        c = Conv2D(filters=1,kernel_size=(1,1))(i)\r\n        model_2d = Model(inputs=i,outputs=c)\r\n\r\n        model_2d.compile(loss='mean_squared_error', optimizer='adam')\r\n        # Weights still not initialized; contain garbage data.\r\n        \r\n        loss = model_2d.train_on_batch([X,y])\r\n        # loss is not determinisic here.\r\n\r\n        weights = model_2d.get_weights()[0]\r\n        print(\"OOOOOO weights sum:\",weights.sum())\r\n        print(\"OOOOOO loss sum:\",loss.sum())\r\n\r\n        return model_2d.predict_on_batch(X)\r\n\r\ndef test_deterministic_between_sessions():\r\n    result1 = call_in_new_session()\r\n    result2 = call_in_new_session()\r\n    assert numpy.allclose(result1, result2, atol=1e-5), \"Results were different when run in a fresh session!\"\r\n\r\nif __name__=='__main__':\r\n    test_deterministic_between_sessions()\r\n\r\n```\r\n", "comments": ["@drewm1980, \r\nI tried executing the given code on colab but it results in an error. Please see the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/44863a1cb686dae7db585d9f86ef22cc/untitled134.ipynb) and confirm on the same. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32289\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32289\">No</a>\n"]}, {"number": 32288, "title": "C api - Program failing during training when compiling with optimizations flag (-O2) or when libtensorflow is built with optimized instructions (AVX SSE), or when keras model is compiled with \"sparse_categorical_crossentropy\"", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Mixed setup. Keras model is built based on stock example. Training happens in custom C code that adapts an existing TF example\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04, CentOS 7.6 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Both, binary and source. Keras model is built using python package installed with pip. Training program uses libtensorflow compiled from source\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.6(Ubuntu), 3.7(CentOS)\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4(Ubuntu) and 9.1(CentOS)\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nMy env:\r\n    [tf_env.txt](https://gist.github.com/7PintsOfCherryGarcia/31247fa9e421bddd158272c01786911e)\r\n\r\n\r\n**Describe the current behavior**\r\nI am using TF 2.0 [C api](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/c/c_api.h) to recreate the tutorial in [here](https://www.tensorflow.org/beta/tutorials/keras/basic_classification)\r\n\r\nKeras model is built using python package as indicated in the tutorial with slight modifications. \"categorical_crossentropy\" is used as loss function instead of \"sparse_categorical_crossentropy\" as the latter produces the error referenced in this issue. Model is saved using keras.experimental.export_saved_model.\r\n\r\n\r\nModel is saved in a folder called \"saved_model\", **name of this folder** is **hard coded** in C program.\r\n\r\nI link against libtensorflow.so.2.0.0 and libtensorflow_framework.so.2.0.0 as compiled with the following command:\r\n\r\n`./configure; bazel build -c opt //tensorflow/tools/lib_package:libtensorflow` (no cuda support)\r\n\r\n**Describe the expected behavior**\r\nIf compiled **without** optimization (-O<num>) program runs normally and model can be trained successfully. As seen can be observed in the output.\r\nIf compiled **with** optimization (-O<num>) program fails at training step.\r\nIf libtensorflow is **compiled with AVX,SSE** support, program fails at training step.\r\nIf keras model is compiled using \"sparse_categorical_crossentropy\", program fails at training step.\r\nAll modes of failure end with the same error (see below)\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nModel is build using \r\n\r\nThe steps I follow are:\r\n1) Create keras model and save it to a file.\r\npython [createModel.py](https://gist.github.com/7PintsOfCherryGarcia/094e68585be228e42efc5764a97ab5d7)\r\n\r\n2) Download training data and store in two files: data.txt and labels.txt, the names of the input data are **hard coded in C program**. To download data:\r\npython [getData.py](https://gist.github.com/7PintsOfCherryGarcia/455370c9550a6c5c10f9135155a8b282)\r\n\r\n3) Run C [program](https://gist.github.com/7PintsOfCherryGarcia/a5935bc0410e64bca2a8f252620eff92), the program will read the saved model, load training data and labels. Make predictions using the untrained model, train the model, run predictions again.\r\nLD_LIBRARY_PATH=/path/to/libtensorflow/lib ./[CFashionMnist](https://gist.github.com/7PintsOfCherryGarcia/a5935bc0410e64bca2a8f252620eff92) The program will print predicted values as well as loss values during training.\r\n\r\nProgram is compiled with:\r\nFor normal execution\r\n  gcc -Wall -I /path/to/libtensorflow/include -L /path/to/libtensorflow/lib -o CFashionMnist CFashionMnist.c -ltensorflow\r\nFor failing execution\r\n  gcc -Wall **-O2** -I /path/to/libtensorflow/include -L /path/to/libtensorflow/lib -o CFashionMnist CFashionMnist.c -ltensorflow\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nWhen compiled with optimizations flags (-O<num>) I get:\r\n\r\n2019-09-06 14:16:51.617721: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n2019-09-06 14:16:51.642262: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1696290000 Hz\r\n2019-09-06 14:16:51.642842: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557c6f2b89a0 executing computations on platform Host. Devices:\r\n2019-09-06 14:16:51.642893: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-09-06 14:16:51.643335: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: keras_model\r\n2019-09-06 14:16:51.648699: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { train }\r\n2019-09-06 14:16:51.662135: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\r\n2019-09-06 14:16:51.693512: I tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: keras_model\r\n2019-09-06 14:16:51.712088: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { train }; Status: success. Took 68754 microseconds.\r\n**ERROR: Node 'dense_1_target' (type: 'Placeholder', num of outputs: 1) does not have output 2044864112**\r\n\r\nIf I build libtensorflow with support for optimized instructions I get the same error minus the optimized instructions warning. ", "comments": ["This errors seems to appear randomly. Which makes it very difficult to debug.\r\n\r\nI am getting the same error whenever I modify the struct that holds all my graph information\r\nFor example:\r\n**original struct**\r\n\r\n//Structure that holds the model\r\ntypedef struct {\r\n  TF_Graph* graph;\r\n  TF_Session* session;\r\n  TF_Status* status;\r\n\r\n  TF_Output input, target, output, metric;\r\n\r\n  TF_Operation *init_op, *train_op;\r\n  TF_Output checkpoint_file;\r\n} model_t;\r\n\r\n**struct that makes program fail**\r\n\r\n//Structure that holds the model\r\ntypedef struct {\r\n  TF_Graph* graph;\r\n  TF_Session* session;\r\n  TF_Status* status;\r\n\r\n  TF_Output input, target, output, metric;\r\n\r\n  TF_Operation *init_op, *train_op;\r\n  TF_Output checkpoint_file;\r\n  int a;\r\n} model_t;\r\n\r\nI never pass this struct to any function, only it's members", "So I figured it out if anybody is interested. Turns out I'm as dumb as it gets.\r\nWhen indicating the the index of my \"target\", I recycled the previous lines of code and just reset the index of \"output\" to 1 and left \"target\" index uninitialized:\r\n\r\nIn Belly_ModelOperations()\r\n  model->output.oper = TF_GraphOperationByName(model->graph, \"dense_1/Softmax\");\r\n  model->output.index = 0;\r\n  if(model->output.oper == NULL) {\r\n    printf(\"ERROR output\\n\");\r\n    return -1;\r\n  }\r\n\r\n  model->target.oper = TF_GraphOperationByName(model->graph, \"dense_1_target\");\r\n  model->output.index = 0; //  <===============BAD\r\n  //Chaneg to:\r\n  model->target.index = 0; //   <==================GOOD\r\n  if(model->target.oper == NULL) {\r\n    printf(\"ERROR target\\n\");\r\n    return -1;\r\n  }\r\n\r\nI still have no clue why not initializing the target index ended up in error on certain occasions  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32288\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32288\">No</a>\n"]}, {"number": 32287, "title": "Image recognition example for Tensorflow Lite Micro", "body": "This is a demo showing how to use Tensorflow Lite Micro to perform image recognition using a STM32F746G-DISCO discovery board with a STM32F4DIS-CAM camera module attached.", "comments": ["Sorry for the slow response. Overall this looks great, thanks! We will have to move the model file outside of the repo unfortunately, since it's 280KB and larger files like that cause problems as they grow the download for all users.\r\n\r\nYou can see how we do something similar for the person detection model here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_vision/Makefile.inc#L1\r\n\r\nIt would also be great if you could update the readme to mention the specific kinds of recognition that this model is doing, \"Plane\", \"Car\",  \"Bird\", \"Cat\", \"Deer\",  \"Dog\",  \"Frog\", \"Horse\", \"Ship\",  \"Truck\", and that it will always try to put what it's shown into one of these categories. Could you also rename it to cifar_recognition? I realize I started a confusing trend with \"micro_speech\", I should have named it something clearer.\r\n\r\n", "> Sorry for the slow response. Overall this looks great, thanks! We will have to move the model file outside of the repo unfortunately, since it's 280KB and larger files like that cause problems as they grow the download for all users.\r\n> \r\n> You can see how we do something similar for the person detection model here:\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/examples/micro_vision/Makefile.inc#L1\r\n> \r\n\r\nSure, I'll take a look at that.\r\n\r\n> It would also be great if you could update the readme to mention the specific kinds of recognition that this model is doing, \"Plane\", \"Car\", \"Bird\", \"Cat\", \"Deer\", \"Dog\", \"Frog\", \"Horse\", \"Ship\", \"Truck\", and that it will always try to put what it's shown into one of these categories. \r\n\r\nThat's a good idea, reading through the README I realize that it's unclear on what the example actually does.\r\n\r\n> Could you also rename it to cifar_recognition? I realize I started a confusing trend with \"micro_speech\", I should have named it something clearer.\r\n\r\nYes, no problem.", "I've updated the PR with regards to the review comments. I've removed the testcase from the Makefile.inc in order to prevent it from running until the model is made available for download, something which I'm working on.\r\n\r\nThe commits should be squashed before merging, since the model appears in the git-history twice, one when adding and one when removing. If it's not done automatically, I can do it myself and force push after the changes have been approved.", "@petewarden  I've added the URL for the network model, and it builds fine. And I've come to think of one thing.\r\n\r\nSince the testcase (image_recognition_test.cc) uses the first 10 images in the CIFAR10 dataset of test images, I've added so that the CIFAR10 image set is downloaded. I've only managed to find it distributed as a single tar.gz-file including all of the 60000 images, and it is quite large (~160 MB).\r\n\r\nSo it could take a while on a slow internet connection. I tried looking through the Makefile to see if there is a way do only download the image set if the image_recognition_test is being built.\r\n\r\nI wasn't able to really find a solution, since it seems like the Makefile.inc:s in micro/examples/ are called before it is determined which exact testcases are to be built.\r\n\r\nDo you (or anyone) know if this is a problem in the first case, and if it can be solved without making large changes to the make system?\r\n\r\nPS: Also remember to check if the commits will be squashed before merging.", "@petewarden gentle reminder.", "@jenselofsson Can you please resolve conflicts? Thanks!", "@gbaned Done!", "The \"Resolve conflicts\" button was greyed out, so I merge the upstream/master-branch locally. Let me know if I need to fix it further in any way.", "@jenselofsson Can you please check reviewer comments and keep us posted. Thanks!", "@tensorflowbutler It's being worked on.", "When I try to view the \"MacOS CPU Python3\" build logs, it shows a 404 error.", "@njeffrie @petewarden gentle ping for review", "@petewarden Ping for review.", "@jenselofsson Can you please check petewarden's comments and keep us posted? Thanks!", "@petewarden No problem, updating the PR ASAP.", "@petewarden Ping for review.", "Overall looks good.  If possible, could you rename the \"disco\" directory to something a bit more descriptive?  Perhaps stm32f746_discovery or stm32f746_disco.", "@njeffrie Done!", "@rthadur Done!", "The error in the Windows Bazel build seems to be due to keras, which is unrelated to this PR.", "@jenselofsson, @rthadur is it ready to merge?  ", "@njeffrie can you please help merge this PR internally ?", "@rthadur @njeffrie Ping for review", "@njeffrie is working internally to merge this PR , lets wait until he comes back.Thanks for your patience."]}, {"number": 32286, "title": "Simple `model.evaluate()` example floods output with `=` characters", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  - **Fedora 30**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n  - **(binary): pip install tensorflow==2.0.0-rc0**\r\n- TensorFlow version (use command below):\r\n  - **v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0**\r\n- Python version:\r\n  - **Python 3.7.4**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the code, I get an output flooded with hundreds of thousands of `=` characters when calling `model.evaluate()`.\r\n\r\n```\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n60000/60000 [==============================] - 3s 43us/sample - loss: 0.5010 - accuracy: 0.8228\r\nEpoch 2/5\r\n60000/60000 [==============================] - 2s 38us/sample - loss: 0.3766 - accuracy: 0.8639\r\nEpoch 3/5\r\n60000/60000 [==============================] - 2s 38us/sample - loss: 0.3408 - accuracy: 0.8753\r\nEpoch 4/5\r\n60000/60000 [==============================] - 2s 37us/sample - loss: 0.3155 - accuracy: 0.8839\r\nEpoch 5/5\r\n60000/60000 [==============================] - 2s 38us/sample - loss: 0.2980 - accuracy: 0.8903\r\n10000/1 [==================================================================\r\n===========================================================================\r\n===========================================================================\r\n===========================================================================\r\n===========================================================================\r\n===========================================================================\r\n===========================================================================\r\n===========================================================================\r\n...\r\n... Literally hundreds of thousands of `=` ...\r\n...\r\n===========================================================================\r\n===========================================================================\r\n===========================================================================\r\n===========================================================================\r\n===========================================================================\r\n===========================================================================\r\n===========================================] - 0s 26us/sample - loss: 0.2803 - accuracy: 0.8673\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n```\r\nTrain on 60000 samples\r\nEpoch 1/5\r\n60000/60000 [==============================] - 3s 43us/sample - loss: 0.5010 - accuracy: 0.8228\r\nEpoch 2/5\r\n60000/60000 [==============================] - 2s 38us/sample - loss: 0.3766 - accuracy: 0.8639\r\nEpoch 3/5\r\n60000/60000 [==============================] - 2s 38us/sample - loss: 0.3408 - accuracy: 0.8753\r\nEpoch 4/5\r\n60000/60000 [==============================] - 2s 37us/sample - loss: 0.3155 - accuracy: 0.8839\r\nEpoch 5/5\r\n60000/60000 [==============================] - 2s 38us/sample - loss: 0.2980 - accuracy: 0.8903\r\n10000/10000 [==============================] - 0s 26us/sample - loss: 0.2803 - accuracy: 0.8673\r\n```\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.fashion_mnist\r\n(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\r\ntraining_images = training_images / 255.0\r\ntest_images = test_images / 255.0\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\r\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\nmodel.fit(training_images, training_labels, epochs=5)\r\n\r\ntest_loss = model.evaluate(test_images, test_labels)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nRunning this code in a Jupyter Notebook results in a performance penalty for the huge, unnecessary output.\r\n", "comments": ["Perhaps you can use semi-verbose by setting ```verbose=2``` to avoid repeated logging of ```=``` character.\r\n[GitHub_Gist](https://colab.sandbox.google.com/gist/ymodak/da242ce544a5102fce8eddf002b9ef10/github_issue_32286.ipynb)\r\n```python\r\ntest_loss = model.evaluate(test_images, test_labels, verbose=2)\r\n```", "@ymodak Yeah, that is not a problem, I can set verbose to 0 as well, but I think it is a bug anyway. Note the `10000/1` in the progress bar, instead of `10000/10000`. I think that is unexpected. :wink:", "same problem here\r\nhttps://github.com/tensorflow/tensorflow/issues/32320#issuecomment-548883188", "@Peque This has been resolved recently in `tf-nightly` which is `2.1.0-dev20191108` that will be released in the future. I am closing this issue. Please check the gist [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/5ead8e4a4613f9bf58996093154d1290/untitled634.ipynb).\r\n\r\nPlease feel free to open the issue if it persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32286\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32286\">No</a>\n", "this issue still seems to still persist with the current tf-nightly - has anyone found a solution? ", "@warriorgiggles Can you please create a new issue with a simple standalone code to reproduce the issue? Thanks!"]}, {"number": 32285, "title": "Model unable to take input from dictionary(with input layer names as key) after loading it using tf.keras.models.load_model", "body": "**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 16.04):\r\ndevice:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0 rc\r\n- Python version: 3.6\r\n\r\nMy model works fine the first time when i create the architecture and train it, but once i save the model using **tf.keras.model.save_model**  and then load it again using **tf.keras.models.load_model** it fails to take input from the  dictionary(with input layer names as key) as if the names of layers are changed.\r\n\r\nExample:\r\ntraining_mask = tf.keras.layers.Input(shape=(None, None, 1), name='training_mask')\r\ntarget_score_map = tf.keras.layers.Input(shape=(None, None, 1), name='target_score_map')\r\ntarget_geo_map = tf.keras.layers.Input(shape = (None,None,5), name='target_geo_map')\r\n\r\nThese are three input layers that i am using to take input for training but once i save the model and later load it again to train it gives me the error \" No data provided for \"input_1\". Need data for each key in: ['input_1', 'input_2', 'input_3']\"\r\n\r\nI am feeding data by specifying layer names :\r\n\r\n        inputs = {\r\n                'target_score_map': score_maps,\r\n                'target_geo_map': geo_maps,\r\n                'training_mask': training_masks\r\n            }\r\n\r\nTherefore, it works the first time when create I the architecture from code then train and save it but later when i load the model from disk it fails.\r\n\r\nRegards\r\nShubham\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "Hey ravikyram,\r\n\r\nOn more trouble-shooting i found out that the name of the layers are not changing, something else is going on,\r\n\r\nFollowing is the code to reproduce the issue:\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n```\r\n\r\n```\r\ndef get_data():\r\n    images = np.zeros((64,224))\r\n    labels = np.zeros((64,5))\r\n    inputs = {\r\n        'Image_input':images\r\n    }\r\n    outputs = {\r\n        'output-softmax':labels\r\n    }\r\n    return inputs, outputs\r\n\r\n```\r\n```\r\ndef create_model():\r\n    input_layer = tf.keras.layers.Input(name='Image_input', shape=(224), dtype='float32')\r\n    model = tf.keras.layers.Dense(5)(input_layer)\r\n    model = tf.keras.layers.Activation('softmax', name = \"output-softmax\")(model)\r\n    model = tf.keras.models.Model(inputs=input_layer, outputs=[model])\r\n    return model\r\n    \r\nmodel = create_model()\r\n\r\noptimizer = tf.keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\r\n    \r\ndata = get_data()\r\nmodel.fit(data[0], data[1], batch_size=16, epochs=10)\r\n\r\ntf.keras.models.save_model(model,\"model\")\r\n\r\nmodel1 = tf.keras.models.load_model(\"model\")\r\n\r\nmodel1.fit(data[0], data[1], batch_size=16, epochs=10)\r\n```\r\n\r\nmodel.fit() works well but model1.fit() fails with error **ValueError: No data provided for \"input_1\". Need data for each key in: ['input_1']**\r\n\r\nBut if you provide the input without dictionary\r\n```\r\ndef get_data():\r\n        images = np.zeros((64,224))\r\n        labels = np.zeros((64,5))\r\n        return images,labels\r\n```\r\n\r\nboth **model.fit()** and **model1.fit()** both runs successfully.\r\n\r\nThanks\r\n", "I have tried on colab with TF version 2.0 rc0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d76d69451de64646e2917bb0b555e56c/untitled170.ipynb).Thanks!", "I believe that this is fixed with commit: https://github.com/tensorflow/tensorflow/commit/6a26d679113105820cf83a2447863a6a32488c47#diff-29e6348914dee326daf67d86aa5075db.\r\n\r\nRan the colab (thanks @ravikyram!) with tf-nightly-2.0-preview, and it worked.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32285\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32285\">No</a>\n", "Hello Everyone\r\n\r\nI am still facing this issue with stable release of tensorflow-2 i.e tf-2.0.0.\r\n\r\nAlso, I am unable to use \"model.summary()\" after loading the model. It gives error:\r\n\r\n**You tried to call `count_params` on Image_input, but the layer isn't built. You can build it manually via: `Image_input.build(batch_input_shape)`**", "@k-w-w Can you reopen this issue, it is not yet resolved. ", "@Shubham3101 As mentioned by @k-w-w this was already resolved in `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/89df5873b5a8fcf9a5214a354235525d/tf_32285.ipynb). Thanks!", "Thanks @jvishnuvardhan, it is working with tf-nightly-2.1.0 version."]}, {"number": 32284, "title": "how to use java implement functions,such as:Tensor\u3001Graph\u3001Operation etc.", "body": "@ry Could I implement funtions\uff08Tensor\uff09 using java?\r\nIf yes, would you like to give me an example.\r\nThanks very much!", "comments": ["@awsssix, Please refer the [Tensorflow official doc](https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/package-summary) for Java  implementation.  Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32284\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32284\">No</a>\n"]}, {"number": 32283, "title": "Commit 2f38d92fa854bb173bc866a89f5123e501cf35bd breaking unit test builds.", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: https://github.com/tensorflow/tensorflow/commit/2f38d92fa854bb173bc866a89f5123e501cf35bd problem occurs on master branch with and without --config=v2\r\n- Python version: py2.7\r\n- Installed using virtualenv? pip? conda?: docker tensorflow/tensorflow:devel\r\n- Bazel version (if compiling from source): 0.26.1 from docker tensorflow/tensorflow:devel\r\n- GCC/Compiler version (if compiling from source): tensorflow/tensorflow:devel (7.4.0)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n> SUBCOMMAND: # //tensorflow/core/platform:windows_env_time_impl [action 'Compiling tensorflow/core/platform/windows/env_time.cc']\r\n> (cd /jenkins/workspace/Nightly-Private-Tensorflow-Eigen/eigen_build/a638235ef1b5f50137189224e7b73e40/execroot/org_tensorflow && \\\r\n>   exec env - \\\r\n>     PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n>     PWD=/proc/self/cwd \\\r\n>     PYTHON_BIN_PATH=/usr/local/bin/python \\\r\n>     PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n>     TF_CONFIGURE_IOS=0 \\\r\n>   /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/platform/_objs/windows_env_time_impl/env_time.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/platform/_objs/windows_env_time_impl/env_time.pic.o' -fPIC -iquote . -iquote bazel-out/k8-opt/bin '-march=haswell' '-mtune=broadwell' -O3 -Wformat -Wformat-security -fstack-protector -fPIC -fpic '-std=c++14' '-D_GLIBCXX_USE_CXX11_ABI=0' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/platform/windows/env_time.cc -o bazel-out/k8-opt/bin/tensorflow/core/platform/_objs/windows_env_time_impl/env_time.pic.o)\r\n> ERROR: /jenkins/workspace/Nightly-Private-Tensorflow-Eigen/tensorflow/tensorflow/core/platform/BUILD:51:1: C++ compilation of rule '//tensorflow/core/platform:windows_env_time_impl' failed (Exit 1)\r\n> tensorflow/core/platform/windows/env_time.cc:19:10: fatal error: windows.h: No such file or directory\r\n>  #include <windows.h>\r\n>           ^~~~~~~~~~~\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n$ docker run tensorflow/tensorflow:devel\r\n# cd /tensorflow_src\r\n# git pull\r\n# yes \"\" | python configure.py\r\n# bazel --nosystem_rc --nohome_rc --output_user_root=/root/eigen_build test --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-march=haswell --copt=-mtune=broadwell --copt=-O3 --copt=-Wformat --copt=-Wformat-security --copt=-fstack-protector --copt=-fPIC --copt=-fpic --linkopt=-znoexecstack --linkopt=-zrelro --linkopt=-znow --linkopt=-fstack-protector --test_timeout 300,450,1200,3600 --test_env=KMP_BLOCKTIME=0 -s --cache_test_results=no --test_size_filters=small,medium,large,enormous -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/... -//tensorflow/core:test_lite_main -//tensorflow/contrib/tensorrt/... -//tensorflow/stream_executor/cuda/... -//tensorflow/python/autograph/pyct/... -//tensorflow/core/kernels:eigen_mkldnn_contraction_kernel_test -//tensorflow/python/debug:dist_session_debug_grpc_test\r\n\r\n```\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Quick link to commit: 2f38d92fa854bb173bc866a89f5123e501cf35bd\r\n\r\n@bmzhao, can you take a look at this? Was there a change on top of the original that didn't get rolled back as well?", "Pasting my reply from the commit (https://github.com/tensorflow/tensorflow/commit/2f38d92fa854bb173bc866a89f5123e501cf35bd#commitcomment-34983793):\r\n\r\nHi Clayne! Could you tell us the bazel command you executed that ran into this issue?\r\n\r\nFor context, we are refactoring build targets under tensorflow/core/platform/BUILD, such that platform-specific library implementations have their own build targets (eg: window's implementation of the env_time interface is now split into its own build target `windows_env_time_impl`). Our intention is that the implementation meant for your platform is the one that selected at build time.\r\n\r\nIn other words\r\n\r\n1. we now have both `windows_env_time_impl` and default `env_time_impl` build targets\r\n2. if you build on posix, only the default `env_time_impl` should ever be in the transitive closure of any build target's dependencies. (This is enforced through the select statement here: https://github.com/tensorflow/tensorflow/commit/2f38d92fa854bb173bc866a89f5123e501cf35bd#diff-c0864b5ac6e1b46699c5e0453e53c696R67-R70)\r\n\r\nHowever, one potential issue you might run into is if you attempt to execute the command `bazel test ...` or `bazel build ...`\r\n\r\nThe `...` syntax will recursively expand to all subpackages & targets (from wherever you invoked the command). See https://docs.bazel.build/versions/master/query.html#target-patterns\r\n\r\nIf this is the case, you can add the flag `--build_tag_filters=-no_oss` to your bazel flag, which will exclude any targets that have the `no_oss` tag, see: https://docs.bazel.build/versions/master/command-line-reference.html#flag--build_tag_filters\r\n\r\nNote that simply supplying `test_tag_filters=-no_oss` does not work, due to this bazel issue: https://github.com/bazelbuild/bazel/issues/8439\r\n\r\nAlso, note that supplying the `build_tag_filters=-no_oss` will still allow targets to be built if they are in the transitive closure of some other target you are building: https://github.com/bazelbuild/rules_docker/issues/269#issuecomment-354764562", "Ah I missed the docker run commands at the bottom of your comment; I see what's happening. Our ci_build scripts weren't also updated with the --build_tag_filters (my mistake)! I asked gunan@ and he confirmed that using \"manual\" as a tag would also suffice, without having to set the build_tag_filters flag. I will have a fix shortly.", "I've just added the new tags here, which should fix this issue:\r\nhttps://github.com/tensorflow/tensorflow/commit/5ee8685df5aac173729e76b51144bba73a536fa4\r\n\r\nCan you sync to head and confirm if the problem is fixed @claynerobison?", "Fixed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32283\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32283\">No</a>\n"]}, {"number": 32282, "title": "[r2.0-cherrypick]: Add incompatible_shape_error attribute to equal op", "body": "When tensor equality is enabled, if there is an incompatible shape we\r\ncurrently throw and exception. Ideally we'd like to return False when\r\ncalling __eq__ and True when calling __ne__. We thus modify the Equal\r\nand NotEqual ops to return a boolean upon a shape incompatibility. Due\r\nto this change the shape inference logic needs to be changed to either\r\nreturn a scalar bool if the shapes are incompatible, or else return an\r\nunknown shape to allow for either a boolean Tensor or scalar to be\r\nreturned.\r\n\r\nNote the behavior of tf.math.equal & tf.math.not_equal is unchanged as\r\nthey both use optimistic shape inference logic when dealing with unknown\r\ndimensions which allows for more efficient graphs rather than inserting\r\nRank operations.\r\n\r\nThis distinction between __eq__ & tf.math.equal is also found in numpy\r\nand as a result the tf.debugging.assert_equal and\r\ntf.debugging.assert_none_equal APIs needed to be change to utilize the\r\nnumpy operations.\r\n\r\nPiperOrigin-RevId: 267466043\r\n(cherry picked from commit e0e1efbe0811aa0913ad8400c532b33c76425427)", "comments": []}, {"number": 32281, "title": "The TF function for the TRT segment could not be empty", "body": "--\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):  when I convert pb to tensorrt, I use tf-nightly-gpu-1.15 from pip install. When I do infer, I use tf-1.14.0 from source.\r\n- TensorFlow version (use command below): tf-1.14.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- CUDA/cuDNN version: cuda 10 cudnn 17\r\n- GPU model and memory: 1060Ti\r\n\r\n\r\n**Describe the current problem**\r\nHi, \r\nI successfully used trt_convert to my pb model to tensorRT plan \r\nand I want to use c++ to infer my model. \r\nSo I use bazel complie tf with **tensorrt together**. \r\nIn my code, pb model can run successfully. \r\nI use the same code, and TensorRT model can load successfully but as session running, tf give me the following error: \r\n\r\n2019-09-06 06:56:37.455586: E tensorflow/core/common_runtime/executor.cc:642] Executor failed to create kernel. Invalid argument: The TF function for the TRT segment could not be empty\r\n\t [[{{node fa_layer4_c0/TRTEngineOp_108}}]]\r\n\r\n\r\n\r\nWhen I convert pb to tensorrt, it is shown as following:\r\n\r\n2019-09-06 06:25:59.150320: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-09-06 06:25:59.183609: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:831] TensorRT node fa_layer4/TRTEngineOp_107 added for segment 107 consisting of 3 nodes succeeded.\r\n2019-09-06 06:25:59.189007: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:831] TensorRT node fa_layer4_c0/TRTEngineOp_108 added for segment 108 consisting of 2 nodes succeeded.\r\n2019-09-06 06:25:59.189378: W tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:37] DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output.\r\n2019-09-06 06:25:59.189403: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:41] DefaultLogger Network must have at least one output\r\n2019-09-06 06:25:59.189426: W tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:834] TensorRT node fa_layer4_c0/conv0/bn/cond_1/TRTEngineOp_109 added for segment 109 consisting of 4 nodes failed: Internal: Failed to build TensorRT engine. Fallback to TF...\r\n\r\n\r\n\r\nAdditionally, I can use the tensorrt model in python code to infer. (python installed from pip) \r\nAnyone could provide some ideas how to solve it?\r\n\r\n\r\n", "comments": ["I met this problem too, have you solved this problem?", "I tried convert savedmodel to trt_savedmodel  successfully with below command line:\r\n```\r\nsaved_model_cli convert \\\r\n--dir \"/home/yilrr/tf-serving/faster-rcnn/saved_model/versions/1\" \\\r\n--output_dir \"/home/yilrr/tf-serving/trt-frcnn\" \\\r\n--tag_set serve \\\r\ntensorrt --precision_mode FP32 --max_batch_size 32 --is_dynamic_op True\r\n```\r\n\r\nAnd deploy the trt_savedmodel with tf-serving, no error found. But when send grpc request to tf-serving. error occurs below :\r\n```\r\n<_Rendezvous of RPC that terminated with:\r\n status = StatusCode.INVALID_ARGUMENT\r\n details = \"The TF function for the TRT segment could not be empty\r\n  [[{{node TRTEngineOp_0}}]]\"\r\n debug_error_string = \"{\"created\":\"@1572421829.964622904\",\"description\":\"Error received from peer ipv4:192.168.23.17:8500\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1055,\"grpc_message\":\"The TF function for the TRT segment could not be empty\\n\\t [[{{node TRTEngineOp_0}}]]\",\"grpc_status\":3}\"\r\n```\r\nanybody who can explain this? thanks in advance!\r\n\r\n", "@double344931987 @superhg2012 could you provide a detailed repro? E.g. which model you're using and how you deploy/run the model with TF serving?\r\n\r\nThanks.", "@aaroey I am ok now. I updated tensorflow version to 1.14.0 and tensorRT from 5.0.26 to 5.1.5.0.", "Thanks @superhg2012.\r\n\r\n@double344931987 I'm closing this, feel free to reopen with a repro (model+script) if you're still experiencing this issue. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32281\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32281\">No</a>\n"]}]