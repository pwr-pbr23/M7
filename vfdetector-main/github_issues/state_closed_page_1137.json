[{"number": 19106, "title": "Fix typo", "body": "fix typo", "comments": []}, {"number": 19105, "title": "unsafe_div op for division by zero ", "body": "Fix #15706, #17350, #20091. \r\n\r\nThis is a summary of discussion in #15706:\r\n\r\n+ op name: div_no_nan\r\n+ op behavior: return 0 when denominator is zero\r\n+ designed for float value only: float32, float 64\r\n+ op kernel: CPU for now (I'd like to add GPU kernel later, is it OK? cc @asimshankar)", "comments": ["@drpngx @aselle @tatatodd could you take a look? Thanks.", "Nagging Assignee @protoget: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 37 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 51 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 66 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 82 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@protoget @martinwicke @aselle @drpngx Hi, could you lend a hand? I think we pend the PR for a quite long time without any response ( ~ 3 months).", "Sorry for the long delay and thank you for your patience. Overall, I think it looks good! I will let @aselle or @martinwicke speak on whether we want to expose this in the `tf.math` API.", "@drpngx Many thanks. I have resolved all your comments. By the way, because the op is only necessary for `metrics`, `loss` and `math_ops` modules up till now,  I'd like to hide the op at first and we might expose it later.", "Could you take a look?\r\n\r\n```\r\nINFO: Server logs of failing action:\r\n   /home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/bazel-remote-logs/5e685ccc7a022ff0388691de94842464d8b2228c7279fbaae5805f70990b9904/docker_runner\r\nFAIL: //tensorflow/python:math_ops_test (see /home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/math_ops_test/test.log)\r\nINFO: From Testing //tensorflow/python:math_ops_test:\r\n==================== Test output for //tensorflow/python:math_ops_test:\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/math_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/math_ops_test.py\", line 476, in <module>\r\n    @test_util.with_c_api\r\nAttributeError: 'module' object has no attribute 'with_c_api'\r\n```", "@drpngx `with_c_api` decorator has been removed in the master branch, so I deleted it. Could you retest it? ", "Thank you, @martinwicke .  Internal CI infrastructure error.", "Unrelated failure? `ImportError: cannot import name 'gen_collective_ops'`", "Yeah, unrelated. However, could you run clang-format on the C++ code? \r\n\r\nThanks!", "Sure, thank you for reminding me.", "This is a summary of discussion in #15706:\r\n\r\n+ op name: div_no_nan\r\n+ op behavior: return 0 when denominator is zero\r\n+ designed for float value only: float32, float 64\r\n+ op kernel: CPU for now (I'd like to add GPU kernel later, is it OK? cc @asimshankar)", "@facaiy - a follow up PR for the GPU kernel should be fine.", "Wait, I think we pushed this internally.", "Already, incl. the changes?\n", "It has been merged in master branch by https://github.com/tensorflow/tensorflow/commit/d2981e0f61f5da46b2ab90a0779842565db43519 about 3 days ago. Hence renaming UnsafeDiv op in c++ side breaks BackwardsCompatibility test.  It seems harmless because the change still stays in master branch, right?", "Looks like we have already merged the first commit internally, and yes it should be harmless. @facaiy, was the first commit a squashed version of a bunch of changes? Do you mind opening another PR for all the new commits added after the merge? Thank you!", "Close it because PR #21621 is opened for further discussion."]}, {"number": 19104, "title": "Illegal instruction (core dumped)", "body": "I have install Tensorflow via using this commands:\r\nsudo apt-get install python3-pip python3-dev  :->  this for the 'pip' installation\r\npip3 install tensorflow   :-> this for a tensorflow installation\r\n\r\nI am using ubantu 16.04 64 bit version Configuration as below :-\r\n![screenshot from 2018-05-05 10-30-59](https://user-images.githubusercontent.com/30696388/39659867-076002a2-5050-11e8-8ee3-361881b92e3b.png)\r\n\r\n\r\nstill when as importing the tensorflow in python3 it gives \"Illegal instruction (core dumped)\" error\r\nas :-1: \r\n![screenshot from 2018-05-05 10-37-31](https://user-images.githubusercontent.com/30696388/39659879-55f635da-5050-11e8-8fb4-03b0fce0571b.png)\r\n\r\n", "comments": ["\"Starting from 1.6 release, our prebuilt binaries will use AVX instructions.\r\nThis may break TF on older CPUs.\"\r\n-- from https://github.com/tensorflow/tensorflow/releases?after=v1.6.0", "Is there anyway to get around this ? I trained a model on 1.7.0 due to it crashing on my 750ti from memory, but need to run this model on 1.4.0, should of thought about it earlier but any way to avoid an additional 3 days training ?", "So is there any other option is available for my CPU for tensorflow", "The options are:\r\n1. Build TF yourself\r\n2. Download third-party binaries built by others, e.g. https://github.com/mind/wheels/releases\r\n3. Use old version TF", "What @ppwwyyxx mentioned is almost certainly the case. If you have an older CPU that doesn't have AVX you can install either an older TensorFlow binary, or you can build TensorFlow from source."]}, {"number": 19103, "title": "Build Pip Package Failed on Windows v1.8.0 with GPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No!!\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.8.0\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: CMake\r\n- **GCC/Compiler version (if compiling from source)**: VS 2015\r\n- **CUDA/cuDNN version**: 9.1\r\n- **GPU model and memory**: GTX-1070 8GB\r\n- **Exact command to reproduce**: \r\ngit pull\r\ngit checkout refs/tags/v1.8.0\r\ncd tensorflow\\contrib\\cmake\r\nmkdir build\r\ncd build\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=C:\\swigwin-3.0.12\\swig.exe ^\r\n-DPYTHON_EXECUTABLE=C:\\Anaconda3\\python.exe ^\r\n-DPYTHON_LIBRARIES=C:\\Anaconda3\\libs\\python36.lib ^\r\n-Dtensorflow_ENABLE_GPU=ON ^\r\n-DCUDNN_HOME=\"%CUDA_PATH%\" ^\r\n-Dtensorflow_CUDA_VERSION=9.1 ^\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2\r\n\r\nMSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj\r\n\r\nMSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\n\r\nAfter this command, following error was occured. Sorry, this is Japanese System message.\r\nBut, problem is seems missing the \"tensorflow/python/framework/cpp_shape_inference.pb.h\"\r\n\r\n\"C:\\repo\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.\r\nvcxproj\" (\u65e2\u5b9a\u306e\u30bf\u30fc\u30b2\u30c3\u30c8) (1) ->\r\n\"C:\\repo\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.v\r\ncxproj\" (\u65e2\u5b9a\u306e\u30bf\u30fc\u30b2\u30c3\u30c8) (2) ->\r\n\"C:\\repo\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal_s\r\ntatic.vcxproj\" (\u65e2\u5b9a\u306e\u30bf\u30fc\u30b2\u30c3\u30c8) (3) ->\r\n\"C:\\repo\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_c_python_api.vcxproj\" (\u65e2 \u5b9a\r\n\u306e\u30bf\u30fc\u30b2\u30c3\u30c8) (134) ->\r\n(ClCompile \u30bf\u30fc\u30b2\u30c3\u30c8) ->\r\n  C:\\repo\\tensorflow\\tensorflow\\c\\python_api.cc(19): fatal error C1083: include\r\n \u30d5\u30a1\u30a4\u30eb\u3092\u958b\u3051\u307e\u305b\u3093\u3002'tensorflow/python/framework/cpp_shape_inference.pb.h':No such file\r\n or directory [C:\\repo\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_c_python_ap\r\ni.vcxproj]\r\n\r\n    90 \u500b\u306e\u8b66\u544a (Note: 90's Warning)\r\n    1 \u30a8\u30e9\u30fc (Note: Error)\r\n\r\nBest regards.\r\n", "comments": ["I have similar issue as well, except my errors are in english\r\n\r\n```\r\n\"C:\\Users\\user\\Documents\\tensorflow\\tensorflow\\contrib\\cmake\\buildr1.8\\tf_python_build_pip_package.vcxproj\" (default ta\r\nrget) (1) ->\r\n\"C:\\Users\\user\\Documents\\tensorflow\\tensorflow\\contrib\\cmake\\buildr1.8\\pywrap_tensorflow_internal.vcxproj\" (default tar\r\nget) (2) ->\r\n\"C:\\Users\\user\\Documents\\tensorflow\\tensorflow\\contrib\\cmake\\buildr1.8\\pywrap_tensorflow_internal_static.vcxproj\" (defa\r\nult target) (3) ->\r\n\"C:\\Users\\user\\Documents\\tensorflow\\tensorflow\\contrib\\cmake\\buildr1.8\\tf_c_python_api.vcxproj\" (default target) (134)\r\n->\r\n(ClCompile target) ->\r\n  C:\\Users\\user\\Documents\\tensorflow\\tensorflow\\c\\python_api.cc(19): fatal error C1083: Cannot open include file: 'tens\r\norflow/python/framework/cpp_shape_inference.pb.h': No such file or directory [C:\\Users\\user\\Documents\\tensorflow\\tensor\r\nflow\\contrib\\cmake\\buildr1.8\\tf_c_python_api.vcxproj]\r\n\r\n    20441 Warning(s)\r\n    1 Error(s)\r\n\r\n```", "@gunan could you take a look.", "It is possible we have missed a bug with `/arch:AVX2`.\r\nOtherwise, I think there is an earlier protobuf error we are missing, that results in this file not being generated on your system. Since r1.8 builds just fine in our CI, I am not sure what may be going wrong for you.", "I try to build v1.8 branch, but same error is occured.\r\nIn v 1.7.0 case, these command is works fine. I can get the Pip Package.\r\nFor this reason, I think this compile error was v1.8 related error.\r\n", "What is generating this file? I can see if that part is working or not.", "In v.1.7.0 case. generated file is \"tensorflow_gpu-1.7.0-cp36-cp36m-win_amd64.whl\"\r\nAnd, I try some function, it seems work fine. \r\nSorry, can not attach the file, it is too big for attach.", "Is this also the reason why there are no new windows builds for tf-nightly right now? ", "No, on windows our CI has another issue we are working to solve right now.\n\nOn Sun, May 13, 2018 at 12:49 AM Julian Niedermeier <\nnotifications@github.com> wrote:\n\n> Is this also the reason why there are no new windows builds for tf-nightly\n> right now?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19103#issuecomment-388608200>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOXkkXPDmhvewa5QKxXmEOHnBIGemks5tx-WNgaJpZM4TzfMK>\n> .\n>\n", "Same error here. Windows 10. VS 17. Master branch. \r\n\r\n```\r\ncmake .. -G \"Visual Studio 15 2017 Win64\" ^\r\n -T host=x64 ^\r\n -DCMAKE_BUILD_TYPE=RelWithDebInfo ^\r\n -DSWIG_EXECUTABLE=D:\\tf-build\\swigwin-3.0.12\\swig.exe ^\r\n -DPYTHON_EXECUTABLE=D:\\Program-Files\\anaconda\\envs\\tensorflow\\python.exe ^\r\n -DPYTHON_LIBRARIES=D:\\Program-Files\\anaconda\\envs\\tensorflow\\libs\\python35.lib ^\r\n -Dtensorflow_BUILD_PYTHON_TESTS=ON\r\n\r\nMSBuild /p:Configuration=RelWithDebInfo tf_python_build_pip_package.vcxproj\r\n\r\nmsg:\r\nd:\\tf-build\\tensorflow\\tensorflow\\c\\python_api.cc(19): fatal error C1083: Cannot open include file: 'tensorflow/pytho\r\nn/framework/cpp_shape_inference.pb.h': No such file or directory [D:\\tf-build\\tensorflow\\tensorflow\\contrib\\cmake\\tf18b\r\nld\\tf_c_python_api.vcxproj]\r\n```", "Could you share the full log using pastebin?\r\nThe error message means a proto file failed to build somewhere earlier in the build.", "Looks like it is too big for pastebin. So I attach the log here. Thanks! @gunan \r\n\r\n[tf.mb.cmake.cpu.vs17.log](https://github.com/tensorflow/tensorflow/files/2016945/tf.mb.cmake.cpu.vs17.log)\r\n", "What you are seeing is different. In your case, you are running into this failure.\r\nhttps://msdn.microsoft.com/en-us/library/zww6zdh7.aspx\r\n\r\nYou may want to try the \"Release\" configuration.", "Thank you @gunan . Using 'release' did remove all C2471. However C1083 is still there. Please find the attached log. \r\n[msbuild.log](https://github.com/tensorflow/tensorflow/files/2019005/msbuild.log)\r\n\r\nAnyway, since I just need the header files to build an user_opr under windows. I use branch r1.7 instead.", "Thanks to @Skydes's comment in #18931, I applied the patched line to tensorflow/contrib/cmake/tf_core_framework.cmake and successfully built r1.8.0 wheel with AVX2 support enabled.", "Hi AshimaChawla\n\n \n\nI download cudnn-9.1-windows10-x64-v7.1.zip, and extract this zip file.\n\nYou can find cuda/bin/cudnn64_7.dll, So copy this file to CUDA bin folder.\n\nIn my case, copy it to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.1\\bin\n\nThis works fine.\n\nPlease try it.\n\n \n\nBest Regards. \n\n \n\nFrom: AshimaChawla <notifications@github.com> \nSent: Thursday, June 7, 2018 3:48 AM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Yoshihiro Yoshimura <yoshimura@yqs.jp>; Mention <mention@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Build Pip Package Failed on Windows v1.8.0 with GPU (#19103)\n\n \n\nhi @yoshimur <https://github.com/yoshimur> , were u able to use tf after installing 1.7 version??\n\nI am getting the below error:\nCould not find 'cudnn64_7.1.4.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7.1.4 from this URL: https://developer.nvidia.com/cudnn\n\nCould you please guide.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/19103#issuecomment-395172782> , or mute the thread <https://github.com/notifications/unsubscribe-auth/ABBUB87Z0c36FNr2ypStCapPLnSp7lwcks5t6CPsgaJpZM4TzfMK> .  <https://github.com/notifications/beacon/ABBUB2MBOEs6nEAIfCozR-hRVd_WiHTsks5t6CPsgaJpZM4TzfMK.gif> \n\n", "Hi @yoshimur, Thanks very much..\r\n\r\nEverything is working fine now..\r\n\r\nThe only issue I am seeing is when I execute my code, it seem to be bit slower than I used to execute on GPU. Is it something because of Windows ??\r\n\r\nIs Ubuntu much faster than that?\r\n\r\n/Ashima", "Nagging Assignees @gunan, @shivaniag: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I had the same problem (Win10, VS2015, CUDA 9.1, AVX2, with MKL, without gRPC).  \r\n\r\nI checked tensorflow/python/framework and saw that the cpp_shape_inference.pb.h file was in there.  So I just ran the MSBuild command again and the build completed (WHL file produced)!  \r\n\r\nLmk if I should include more info.  This is my first post to a Github issue, plz be nice. :)", "Thanks for the tip, this is really helpful.\r\nCould you copy/paste your build output to pastebin for us to be able to see the error message?\r\n@annarev According to the comment by @luranhe could this be a missing dependency in cmake for api generation?", "Unfortunately, I don't have the build output from the failed build anymore, I didn't save it anywhere and the command line only saves so much history.  Many apologies.\r\n\r\nIt definitely failed on tf_c_python_api.vcxproj because of \"no such file or directory\" of file cpp_shape_inference.pb.h, which clearly existed at the indicated location.  "]}, {"number": 19102, "title": "Allow integer labels in BoostedTreesClassifier", "body": "Currently, `BoostedTreesClassifier` assumes that the labels are float in the gradient calculation and errors if they are integer. This change allows integer labels, to be consistent with other canned classifiers.", "comments": ["@protoget what's the status of this PR? Thanks!", "Nagging Assignee @protoget: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Seems to have been taken care of by https://github.com/tensorflow/tensorflow/commit/47e52f040efb13c4224e287e3878c670e2e8210f, closing."]}, {"number": 19101, "title": "fix file and variable names in documentation", "body": "Update documentation to match with actual code.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @protoget: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It looks like this didn't get merged before the migration...\r\n\r\nthat docs_src directory has moved to \r\n\r\nhttps://github.com/tensorflow/docs/site/en\r\n\r\nIf you want please re-submit there."]}, {"number": 19100, "title": "Installation error on macos for 1.8.0/1.7.0/1.6.0 (Illegal instruction: 4)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.13.4\r\n- **TensorFlow installed from (source or binary)**: binary (tensorflow-1.8.0-cp36-cp36m-macosx_10_11_x86_64.whl)\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: pip3 install tensorflow && python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI tried to install tensorflow and the module does not load. Same problem for all version up to 1.5.0 which then works fine. \r\n(with version 1.5.0)\r\n```\r\n python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n/Users/marco/coding/crypto/_python_env/_mac/nn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nv1.5.0-0-g37aa430d84 1.5.0\r\n```\r\n### Source code / logs\r\nI run a verbose import as attached for the latest version (`python3 -v -m tensorflow 2&> verbose_import.txt`).\r\n[verbose_import.txt](https://github.com/tensorflow/tensorflow/files/1976200/verbose_import.txt)\r\n\r\n", "comments": ["Nagging Assignee @rohan100jain: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 51 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 66 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 81 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "try python3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"  ?\r\n\r\nMaybe you have 2 versions of py installed? 2 / 3?", "Having the same issue with TF **1.9.0**.  1.5.0 seems to work, but nothing since then.\r\nMacOS 10.12.6 w/ conda,  python 3.6:\r\n\r\n```\r\npython3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nIllegal instruction: 4\r\n```", "the verbose import I run and which I attached was run with python3\r\n`python3 -v -m tensorflow 2&> verbose_import.txt`", "What is the CPU model on your machine?\r\nIs it possible that your CPU does not have AVX instruction set?", "I have the same issue on my two Mac models. \r\nThe last version to work is 1.5.\r\n\r\n**iMac - HighSierra (version 10.13.5)**\r\nThe processor is a 3.2 Ghz Intel Core i3\r\nGraphics: ATI Radeon HD 5670 512 MB\r\n\r\n**MacBook Pro - High Sierra (version 10.13.6)**\r\nThe processor is a 2.9 Ghz Intel Core i5\r\nGraphics Intel Iris Graphics 550 1536 MB", "Unfortunately, I need the full CPU model to be able to help. There are 8 generations of Core CPUs now, earlier ones do not have AVX support, and later ones do have it.", "Will the serial numbers work?\r\niMac - W80252D2DB6\r\nMB Pro -C02TH16FHF1T --> Just reinstalled TF 1.9 to make sure since it's a newer Laptop and it's working fine.", "Looking up the specs, your imac is a 2010 model with I3-550. this issue on it is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584\r\nThe 2nd one has a 6th gen Intel CPU, which should be fine, as you just verified.", "hello,\r\nMy laptop is indeed old and the CPU does not support AVX (same situation as #19584)\r\nI built it from source and confirmed I could compile successfully.\r\nThe issue is close for me, thanks for the support "]}, {"number": 19099, "title": "Add clcache to CMake CI build", "body": "Locally, my GPU build seems to run about 20 mins faster using clcache. I'm curious whether you see a difference in the runtime of the CI build.\r\n\r\nThis change installs clcache each build, which is a little silly, but should be enough to see if the build time changes. If it does, maybe we can just install ahead of time or add a check to see if already installed.", "comments": ["We will soon switch our CI to use bazel, therefore I am not sure we would like to pay the overhead to install ccache on our windows CI machines.", "@gunan I'm just curious how this build compares. Can you trigger the cmake CI?", "Unless you are caching remotely, our build system always sets up a clean environment(machine). It does not retain any data after a build is completed. VMs are reconstructed from fixed images for each build.\r\nSo there will be no benefits for using cache in our CI, as you will always be using an empty cache.\r\n\r\nWe actually have working remote cache setups with bazel, therefore migrating to bazel will help us speed up our windows build by a lot.", "True if there is no redundancy in the build. I got a speedup given an empty\ncache and clean build, so I think there might be some redundancy.\nExperimentally, I'm curious if the actual build time shifts or not\naccording to CI.\n\nOn Wed, May 9, 2018 at 1:34 PM, Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> Unless you are caching remotely, our build system always sets up a clean\n> environment(machine). It does not retain any data after a build is\n> completed. VMs are reconstructed from fixed images for each build.\n> So there will be no benefits for using cache in our CI, as you will always\n> be using an empty cache.\n>\n> We actually have working remote cache setups with bazel, therefore\n> migrating to bazel will help us speed up our windows build by a lot.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/19099#issuecomment-387815205>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AL4rbMHfP7QAk6u8Xy8O6_UK6IvxRjFaks5twyimgaJpZM4TzaeA>\n> .\n>\n", "@gunan thanks for checking! Looks like it actually bumped the time by about a minute, so closing the PR."]}, {"number": 19098, "title": "Use Configuration instead of CMAKE_BUILD_TYPE in MSVC", "body": "This uses the `Configuration` passed to MSBuild instead of `CMAKE_BUILD_TYPE`. You can actually omit `CMAKE_BUILD_TYPE` during configuration, and the `Configuration` selected during build is used.\r\n\r\nSeveral of the CMake scripts used `Configuration`, several used `CMAKE_BUILD_TYPE`, so this is just making things consistent for MSVC.\r\n\r\nOne less parameter to worry about during configuration and makes it possible to build debug and release from a single configuration.", "comments": ["Bump. Should be a really quick update. makes `cmake --build . --config [whatever]` respect the `[whatever]`.", "@bstriner gentle ping to rebase branch", "Please send this PR to tensorflow/addons instead; contrib is deprecated and stated to be removed so we're not making new changes to it.\r\n"]}, {"number": 19097, "title": "Use FindOpenMP instead of custom code", "body": "As suggested by @fo40225 , this switches from custom code to FindOpenMP. Seems to have been in CMake since at least 3.0. FindOpenMP would be preferred to anything custom. However, this might break the build for anyone still on CMake 2. Is that a supported configuration or something to not worry about?\r\n\r\n@gunan This also adds a flag to enable or disable OpenMP. Default is ON but it will warn and disable if not found. This maintains current functionality for existing builds that expect OpenMP but gives the option of disabling.\r\n\r\n@fo40225 , I saw you had a couple other ideas about changes to CMake. Were you going to put together a PR with your other changes?", "comments": ["@bstriner\r\n\r\nhttps://github.com/bstriner/tensorflow/blob/2fa2407ca68e24f3a9aed885ae420de932640264/tensorflow/contrib/cmake/CMakeLists.txt#L2\r\n\r\ntensorflow require CMake at least 3.5, no worry about CMake 2.\r\n\r\nI'd like to create PR for my refactoring, but it needs more testing. Building tensorflow once take too long.", "@fo40225 BTW, did you try clcache? Could make the builds a little faster, especially if you're rebuilding. Might need to make the cache huge to get full benefit though.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/19099", "Nagging Assignee @protoget: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 35 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 50 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 65 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 80 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 95 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for the PR.\r\n\r\nClosing PR since our cmake build is now deprecated. Please switch to building with Bazel if possible. Thanks!"]}, {"number": 19096, "title": "Include ThenBlasGemm", "body": "Hi everybody,\r\n\r\nI'm getting an unresolved external symbol `ThenBlasGemm` when compiling for GPU with CMake using MSVC 2015. This function isn't picked up by the script to create the def file for the DLL, so it isn't being exported. This PR just adds that function to the regex so the build works. Linker error below.\r\n\r\nDoes Windows CMake GPU not go through CI? I'm curious why this hasn't come up before.\r\n\r\nCheers\r\n\r\n```\r\n[D:\\Projects\\tensorflow\\tensorflow\\contrib\\cmake\\buildout\\_gru_ops.vcxproj]\r\n   199>blas_gemm.obj : error LNK2019: unresolved external symbol \"public: class stream_executor::Stream & __cdecl stream_executor::Stream::ThenBlasGemm(enum stream_executor::blas::Transpose,enum stream_executor::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,double,class stream_executor::DeviceMemory<double> const &,int,class stream_executor::DeviceMemory<double> const &,int,double,class stream_executor::DeviceMemory<double> *,int)\" (?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11NAEBV?$DeviceMemory@N@2@H2HNPEAV52@H@Z) referenced in function \"public: void __cdecl tensorflow::functor::TensorCuBlasGemm<double>::operator()(class tensorflow::OpKernelContext *,bool,bool,unsigned __int64,unsigned __int64,unsigned __int64,double,double const *,int,double const *,int,double,double *,int)\" (??R?$TensorCuBlasGemm@N@functor@tensorflow@@QEAAXPEAVOpKernelContext@2@_N1_K22NPEBNH3HNPEANH@Z) \r\n```", "comments": ["Nagging Assignee @protoget: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Related PR by @gunan resolves this issue: https://github.com/tensorflow/tensorflow/pull/19415\r\n\r\nIf it is so close to the export limit, would it make sense to just make multiple dlls? Would get to be less stingy about what gets exported and what doesn't.", "It actually does. One other things we are planning is defining a C/C++ API that can be used when building kernels and restricting exported symbols to only those. Right now, kernels can use anything in core TF framework."]}, {"number": 19095, "title": "Add release notes for TensorFlow Lite.", "body": "Add TensorFlow Lite release notes.", "comments": ["this is sick !"]}, {"number": 19094, "title": "Add release notes for TensorFlow Lite.", "body": "Add TensorFlow Lite release notes. ", "comments": ["Hold on... Double checking..."]}, {"number": 19093, "title": "tf.Variable uses about twice the memory (on the GPU)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary with pip using Anaconda\r\n- **TensorFlow version (use command below)**: tensorflow-gpu 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: V9.0.176\r\n- **GPU model and memory**: Quadro M2000M, 4G\r\n- **Exact command to reproduce**: Please see problem description. Thank you.\r\n\r\n### Describe the problem\r\nThis is almost the same to #13433 but on GPU. I think this is a bug/feature request. If not, please let me know. I use the following python code for testing both `tf.Variable` and `tf.get_variable`. The variable `v` should be 800MB.\r\n\r\n    import tensorflow as tf\r\n\r\n    sess_conf = tf.ConfigProto()\r\n    sess_conf.gpu_options.allow_growth = True\r\n\r\n    # Test the following for Tensor and Variable\r\n    v = tf.Variable(tf.random_uniform((1024*1024, 100), dtype=tf.float64))\r\n    # v = tf.get_variable('v', shape=(1024*1024, 100), initializer=tf.random_normal_initializer, dtype=tf.float64, trainable=False)\r\n\r\n    init = tf.global_variables_initializer()\r\n    with tf.Session(config=sess_conf) as sess:\r\n        sess.run(init)\r\n\r\n And I use `nvidia-smi dmon -c 10 -s m` to monitor the memory usage. For both cases got:\r\n\r\n    # gpu    fb  bar1\r\n    # Idx    MB    MB\r\n        0    42   227\r\n        0    42   227\r\n        0    42   227\r\n        0    42   227\r\n        0   102   227\r\n        0  2163   227\r\n        0  2163   227\r\n        0  2163   227\r\n        0  2163   227\r\n        0  2163   227\r\n\r\nCould you please let me know which part is wrong? Or is this an expected behavior? Thank you.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "@tensorflowbutler \"Exact command to reproduce\" has been included in my problem description. I've updated the \"Exact command to reproduce\" field with explanation. Please let me know if you cannot reproduce the problem. Thank you.", "Nagging Assignee @cy89: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 35 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 50 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 65 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@alextp, you seemed to have good suggestions on #13433. Can we declare this one to be the same problem (which also remains open)? ", "OK. That sounds reasonable to me.", "This is a known issue, and @rmlarsen is working on a fix which might need to be flag-gated for compatibility.", "Nagging Assignee @rmlarsen: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I submitted a fix recently which makes the issue go away with resource variables. Use them by passing use_resource=True to tf.variable or tf.get_variable when creating variables, or by setting `tf.get_variable_scope().set_use_resource(True)` in the beginning of your graph building code."]}, {"number": 19092, "title": "Fix a small typo.", "body": "", "comments": []}, {"number": 19091, "title": "Update bazel toolchains dependency.", "body": "PiperOrigin-RevId: 186650360", "comments": ["The failure looks legitimate, have we seen this before?", "@gunan I cannot tell since tests didn't even build prior to this. Can I still merge and debug that test after?", "I was thinking when we were doing the release.\r\nBut sure, let's first merge and then debug. I think this test was passing when we did 1.6, so I would like to avoid disabling the test for 1.6 release."]}, {"number": 19090, "title": "Branch 195443326", "body": "", "comments": ["Looks like //tensorflow/contrib/timeseries/python/timeseries:estimators_test is flaking in MacOS Contrib."]}, {"number": 19089, "title": "Adding release notes for 1.7.1 release", "body": "Updating release notes for 1.7.1 patch release. Please, let me know if more notes need to be added or if existing ones need to be corrected.", "comments": []}, {"number": 19088, "title": "fix training_hook and training_chief_hook processing of distributed training", "body": "The changes are listed as followed.\r\n```python3\r\n\"\"\"\r\ntraining_hooks = get_hooks_from_the_first_device(\r\n    grouped_estimator_spec.training_hooks)\r\ntraining_chief_hooks = get_hooks_from_the_first_device(\r\n    grouped_estimator_spec.training_chief_hooks)\r\n\"\"\"\r\ntraining_hooks = [\r\n    get_hooks_from_the_first_device(per_device_hook)\r\n    for per_device_hook in grouped_estimator_spec.training_hooks\r\n]\r\ntraining_chief_hooks = [\r\n    get_hooks_from_the_first_device(per_device_chief_hook)\r\n    for per_device_chief_hook in grouped_estimator_spec.training_chief_hooks\r\n]\r\n```\r\n`grouped_estimator_spec.training_hooks` is a `list` of `PerDevice`, and function `get_hooks_from_the_first_device` is only able to pick head item of *one* `PerDevice`, thus the old version doesn't work and keeps the PerDevice, leading to such errors:\r\n```\r\nTypeError: All hooks must be SessionRunHook instances, given: PerDevice:{'/job:localhost/replica:0/task:0/device:GPU:2': <XXXXXHook object at 0x7fc6f43f3080>, '/job:localhost/replica:0/task:0/device:GPU:0': <XXXXXHook object at 0x7fc71430bf60>, /job:localhost/replica:0/task:0/device:GPU:3': <XXXXXHook object at 0x7fc6b467e080>, '/job:localhost/replica:0/task:0/device:GPU:1': <XXXXXHook object at 0x7fc70430d048>}\r\n```\r\n(XXXXXHook inherits from SessionRunHook, and is hidden for some reason.)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @protoget: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 36 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 51 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 66 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 81 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 96 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Could you add a test for this change that would previously fail? Thanks for the PR and sorry for the delay", "I requested @guptapriya for review help as I do not have the context for the code. ", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@googlebot problem resolved!", "CLAs look good, thanks!\n\n<!-- ok -->", "The bug has been fixed by https://github.com/tensorflow/tensorflow/commit/007c234dfc3388f8bea96bcc9867d2f90d419098#diff-17d893fed7bb36da99ce6c80d933d00b. So closing this PR.", "There's a bug in the fix - I believe you should be calling self._train_distribution and not self._distribution when unwrapping, as self._distribution does not exist.", "Ah that's a good catch, thanks! @asaquib would you like to send a PR to fix it? ", "Submitted PR here: https://github.com/tensorflow/tensorflow/pull/22328", "This was a conflict and should have been resolved by merging, so why is it needed to fix it with a new commit and involve this bug?"]}, {"number": 19087, "title": "Fix typo", "body": "fix typo\r\nmetdata -> metadata", "comments": []}, {"number": 19086, "title": "[Feature request] SavedModelBuilder collection export", "body": "Hello!\r\nAs far as I'm concerned, there is no way to export custom collection with `SavedModelBuilder`.\r\nCurrent implementation uses `tf_saver.Saver.export_meta_graph ` method with empty `collection_list` parameter.\r\n\r\nMy suggestion is to add the `collection_list` parameter to the  `SavedModelBuilder.add_meta_graph_and_variables` and `SavedModelBuilder.add_meta_graph` methods. In this way, user can specify custom collections, that could be useful after model import.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Can you add the custom collection to the current graph (e.g. with `tf.add_to_collection`)? Then it will get picked up and saved automatically (`collection_list=None` defaults to `get_all_collection_keys()`).", "Hmm. So it defaults to all collections. It's weird, because I ran this snipped of code\r\n``` python\r\na = tf.placeholder(dtype=tf.float32)\r\nb = tf.placeholder(dtype=tf.float32)\r\ns = a + b\r\nsess.graph.add_to_collection(\"TEAM_AB\", a)\r\nsess.graph.add_to_collection(\"TEAM_AB\", b)\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(\"./model_other\")\r\nbuilder.add_meta_graph_and_variables(sess, ...)\r\nbuilder.save(as_text=True)\r\n```\r\nand I couldn't find any of the `collection_def` fields. \r\n\r\nBut now I checked it again but in other environment, and it works pretty good! \r\n```\r\ncollection_def {\r\n    key: \"TEAM_AB\"\r\n    value {\r\n      node_list {\r\n        value: \"Placeholder:0\"\r\n        value: \"Placeholder_1:0\"\r\n      }\r\n    }\r\n}\r\n```\r\n\r\nWell, that's awkward. I think we could close this FR. Thanks for the help @allenlavoie !"]}, {"number": 19085, "title": " Fixed memory leak with py_func (#18292)", "body": "This PR fixes the memory leak described in #18292. Tests for py_func are running.\r\n\r\nSome internals:\r\n - Replaced `dict` in `FuncRegistry._funcs` by `weakref.WeakValueDictionary` ([python 2.7](https://docs.python.org/2/library/weakref.html#weakref.WeakValueDictionary), [python 3.x](https://docs.python.org/3/library/weakref.html#weakref.WeakValueDictionary)).\r\n - Replaced the `CleanupFunc` by a direct reference to the function. Thus renamed `_cleanup_py_funcs_used_in_graph` to '_py_funcs_used_in_graph`.\r\n - Improved tests to cover the leak. Leak detection is now precise (uses `gc.collect()`).", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Added E-Mail adress to CLA", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @protoget: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19084, "title": "tensorflow.nn.dynamic_rnn with variable as init_state cannot work with Estimator.train", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows10\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.8.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI wrote my own function to generate the init state variable\r\n\r\n```python\r\ndef get_initial_cell_state(cell, batch_size, dtype):\r\n  state_size = cell.state_size\r\n  i = 0\r\n  def get_state_shape(s):\r\n    c = _concat(1, s, static=True)\r\n    nonlocal i\r\n    name = \"init_state_\" + str(i)\r\n    i = i + 1\r\n    size = tf.get_variable(name, shape=c, dtype=dtype, initializer=tf.initializers.zeros)\r\n    size = tf.tile(size, [batch_size] + [1] * (len(c) - 1))\r\n    return size\r\n  return nest.map_structure(get_state_shape, state_size)\r\n```\r\n\r\nAnd use it as below:\r\n\r\n```python\r\n  rnn_output, _ = tf.nn.dynamic_rnn(\r\n    cell=cell,\r\n    inputs=inputs,\r\n    initial_state=get_initial_cell_state(cell, batch_size=batch_size, dtype=tf.float32),\r\n    parallel_iterations=128,\r\n    dtype=tf.float32\r\n  )\r\n```\r\n\r\nIt's compilable and runnable. But got this error:\r\n\r\n`WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.`\r\n\r\nI use Estimator to train. If I don't use this init state variable, everything goes fine.\r\n", "comments": ["Same issue when using tensorflow 1.7.0", "@ebrevdo Can you take a look at this?", "@angersson @ebrevdo Any insight? Or Is there any workable sample about making initial state of RNN trainable?\r\nTrainable initial state will improve performance of RNN model, I think it's a very basic and important feature.", "@ispirmustafa any ideas?", "I don't have an idea.\r\n@iron9light could you please provide a simple script which reproduces the issue?", "@ispirmustafa Wired. I tried to reproduce this issue with a simple project (https://github.com/iron9light/tensorflow_issue19084), but everything goes fine. My real project still has this issue with tensorflow 2.10.\r\n\r\nAnyway, you can think it's a feature request:\r\nAdd an `init_state_trainable: bool` as the param of `nn.dynamic_rnn` or make it a feature of `nn.rnn_cell.RNNCell` class", "@qlzh727 could you please take a look?", "It seems to be a bit strange use case for having initial state as variable. Can u describe what's the intention behind that? eg u want to use those variable in a later step/layer?", "https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html", "Sorry for the very late reply. If the initial state need to be a variable, I think it should be attached to the cell itself. So the workaround could be:\r\n\r\n```\r\ndef create_initial_cell_state(cell, inputs, dtype=tf.float32):\r\n  state_size = cell.state_size\r\n  batch = inputs.shape[0].value\r\n\r\n  cell._initial_state_weight = cell.add_variable(\r\n      \"init_state\", shape=[batch, state_size], initializer=tf.initializers.zeros, dtype=dtype)\r\n\r\n  return cell._initial_state_weight\r\n\r\ntf.nn.dynamic_rnn(\r\n        cell=cell,\r\n        inputs=x,\r\n        dtype=tf.float32,\r\n        initial_state=create_initial_cell_state(cell, x) if self._rnn_init_state_trainable else None\r\n)\r\n```\r\n\r\nThis should work for the simple cell state, like u give in your sample code for GRU. LSTM might need some updates since it has tuple state.\r\n\r\nI ran some local test and it works without the warning message. ", "Your solution is nearly the same as my sample code (my sample code works fine).\r\nMy original issue is that, this way cannot work in my produce code, but works with the sample code.\r\nWhat I mean ask for a feature, is make it more easier to do this kinda things by making it a parameter of nn.dynamic_rnn function \"init_state_trainable: bool\"", "Without looking at your other production code, we won't be able to identify the problem. Since you are requesting a feature, change the issue from bug to FR.", "@iron9light,\r\nSorry for the delayed response. As [tf.compat.v1.nn.dynamic_rnn\r\n](https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/dynamic_rnn) has been deprecated in favor of [tf.keras.layers.RNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)\r\n and as **`Tensorflow 2.x Versions`** pre-dominantly use [Tensorflow Keras](https://www.tensorflow.org/api_docs/python/tf/keras), and not [Estimators](https://www.tensorflow.org/guide/estimator), can you please let us know if this issue is still relevant? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 19083, "title": "GPU remapping using visible_device_list is broken", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\nI have also tried this on 16.0 and 17.0, it crashes both of them. \r\n13.0 and 15.0 are fine.\r\n\r\n- **Python version**:   3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Both 8.0, and 9.1 (with 9.0 libraries)\r\n- **GPU model and memory**:   GeForce GTX 1080 Ti.  with 11178 MiB\r\n- **Exact command to reproduce**:\r\n\r\nimport tensorflow as tf\r\nG =tf.Graph()\r\nsess1 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='0')))\r\nsess2 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='1')))\r\n\r\nRunning the second tf.Session command crashes with the following error:\r\n\r\nF tensorflow/core/common_runtime/gpu/gpu_id_manager.cc:45] Check failed: cuda_gpu_id.value() == result.first->second (1 vs. 0)Mapping the same TfGpuId to a different CUDA GPU id. TfGpuId: 0 Existing mapped CUDA GPU id: 0 CUDA GPU id being tried to map to: 1\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThe GPU remapping using visible_device_list is broken. This works fine in Tensorflow 1.3 and 1.5, but is completely broken (crashes the program) in 1.6, 1.7 and 1.8.\r\nAs far as I can tell from reading tensorflow/include/tensorflow/core/common_runtime/gpu/gpu_id.h\r\nthis mechanism is supposed to still work the same way it used to.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nimport tensorflow as tf\r\nG =tf.Graph()\r\nsess1 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='0')))\r\nsess2 = tf.Session(graph=G, config=tf.ConfigProto(log_device_placement=False,gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='1')))\r\n\r\nF tensorflow/core/common_runtime/gpu/gpu_id_manager.cc:45] Check failed: cuda_gpu_id.value() == result.first->second (1 vs. 0)Mapping the same TfGpuId to a different CUDA GPU id. TfGpuId: 0 Existing mapped CUDA GPU id: 0 CUDA GPU id being tried to map to: 1\r\n", "comments": ["See https://github.com/tensorflow/tensorflow/issues/18861#issuecomment-388454669", "@aaroey Just in case, I am posting here since the other ticket is closed (it wasn't mine, so I don't think I can reopen it). I just added another commentwitw a follow-up question to https://github.com/tensorflow/tensorflow/issues/18861 ", "Nagging Assignee @aaroey: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe the problem is solved in #18861, so I'm closing this. Please re-open if there are any other questions."]}, {"number": 19082, "title": "change the swap threshold by adding an elastic percentage", "body": "Originally, heuristic memory swapping feature won't improve much on the batch size, that's because IdentifySwappingCandidates are fully trusting the statically analysis.\r\nPer my experiment, this did not improve the batch size at all.\r\n\r\nWhile If we make the swapping threshold to be lower,for example, when analyzed memory usage is 0.8 * GPU mem size, then swapping feature is triggered.\r\n\r\nAccording to my experiments, for my K80 (12G), ResNet50, batch size can be improved from 128 to 150.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "Nagging Assignee @protoget: It has been 22 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 39 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 54 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 69 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @protoget: It has been 84 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@simpeng, I don't see your signed CLA in our system. Do you mind try signing again?", "@yifeif i just tried to update the name in CLA to be simpeng (same as my github account). Not sure whether it works now. ", "I signed it!", "Thanks @simpeng. I still don't see your CLA. Are you covered by a corporate CLA? Is the email used for the commit the same as the one associated with the CLA?", "@yifeif this is weird, I created https://github.com/tensorflow/tensorflow/pull/21626 with my corp account. let me close this. "]}, {"number": 19081, "title": "tensorflow/python/ops/init_ops.py line 466 - initialization scaling incorrect?", "body": "### System information : NOT APPLICABLE\r\nHave I written custom code: No and N/A\r\nOS Platform and Distribution: GNU/Linux Gentoo and N/A\r\nTensorFlow installed from: https://www.tensorflow.org/install/install_sources\r\nTensorFlow version:  1.6.0 but relevant to stable version on GitHub\r\nBazel version: 0.11.1 and N/A\r\nCUDA/cuDNN version: 9.1/7.1 and N/A\r\nGPU model and memory: Titan XP x2 - 12GB x2 and N/A\r\nExact command to reproduce: N/A\r\n### Source code / logs:\r\ntensorflow/python/ops/init_ops.py - lines 465-469:\r\n```\r\n    if self.distribution == \"normal\":\r\n      stddev = math.sqrt(scale)\r\n      return random_ops.truncated_normal(\r\n          shape, 0.0, stddev, dtype, seed=self.seed)\r\n```\r\n### Describe the problem:\r\nAccording to He et al (2015) [http://arxiv.org/abs/1502.0185] (Eq. 10) the correct scaling for the standard deviation for kernel initialization is sqrt(2/n). However line 466 of tensorflow/python/ops/init_ops.py reads:\r\n\r\n`stddev = math.sqrt(scale)`\r\n\r\nwhere the default `scale` is 1/n (n depending on fan mode), corresponding to sqrt(1/n) rather than sqrt(2/n) - therefore giving half the correct scaling.\r\n\r\nAlso - since a truncated (-2SD to +2SD) normal distribution is used rather than a full distribution, shouldn't the scale be multiplied by 1.137? Perhaps the simplest fix is therefore:\r\n\r\n`stddev = math.sqrt(2.274*scale)`\r\n ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Apologies, and thanks for the template. I have amended the post to include the details.", "/CC @fchollet, can you take a look?", "There's a PR awaiting merge that fixes this issue: https://github.com/tensorflow/tensorflow/pull/18854", "Thanks. It looks like the issue was first raised at https://github.com/tensorflow/tensorflow/issues/18706 nearly a month ago. It seems a peculiarly long-winded procedure to correct 'stddev = math.sqrt(scale)' to 'stddev = math.sqrt(2.274*scale)' but never mind!", "Nagging Assignee @fchollet: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Pull request has been merged, so this issue could probably be closed."]}, {"number": 19080, "title": "Make canned estimators compute PR-AUC with right method.", "body": "As we are sure about the error produced by computing *PR-AUC* with **trapezoidal** rule according to [this commit](https://github.com/tensorflow/tensorflow/commit/1f5324ca69bc1017972eef8e418691cff9a86dd7).\r\nAnd also i found there is no proper workaround to make a canned estimator(Let's say [DNNClassifier](https://github.com/tensorflow/tensorflow/blob/2dc7575123ffa0e6413fc3d2700968ef25f049de/tensorflow/python/estimator/canned/dnn.py#L194)) to compute PR-AUC with the right method **careful_interpolation**.\r\nThen, i did a simple change to this embarrassed situation: https://github.com/tensorflow/tensorflow/pull/19079\r\n\r\nDoes anyone can confirm this for me?\r\n\r\n\r\n#### Update template content:\r\n\r\n- Have I written custom code: No, i just use a canned estimator DNNClassifier.\r\n- OS Platform and Distribution: macOS High Sierra\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 1.8.0\r\n- Bazel version: N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- Exact command to reproduce: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have updated the issue template.", "Nagging Assignee @aselle: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19079, "title": "Canned estimators use 'careful_interpolation' rule to compute PR-AUCs", "body": "Fix https://github.com/tensorflow/tensorflow/issues/19080", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!!", "CLAs look good, thanks!\n\n<!-- ok -->", "@protoget @googlebot Could you please start the CI build for this change?", "@protoget From the CI build result we can tell, all failed checks are related to the calculation of `AUC_PR` as we used a new way(`careful_interpolation`) to do the calculation. But i'm not able to confirm this result is correct, as this new way of calculating `AUC_PR` is out of my job(i don't invent the new way of calculation, i just used it.). This change happend by [this commit](https://github.com/tensorflow/tensorflow/commit/1f5324ca69bc1017972eef8e418691cff9a86dd7).\r\n\r\nSo i don't know how should i continue with this **failed CI build**?", "@roumposg could you take a look?", "Thank you for your contribution. We have discussed changing the default behavior of AUC metric to careful_interpolation. Unfortunately, changing the default will be backwards incompatible. E.g. if a user pipeline relies on the AUC metric to make some decisions, such as push the model to serving or not, this change may break their pipeline.\r\n\r\nWhat about the following: Add a `auc_pr_careful_interpolation` metric in addition to the existing ones. You can do the same to MultiLabelHead in tensorflow/contrib/estimator/python/estimator/head.py Thanks.", "@roumposg Thanks for your reply. You're right, this change will result in backwards incompatibility. And also i've thought about your proposal for a time. As the requirement to this careful_interpolation way to do AUC calculation seems not a very urgent need to most of people(at least we can tell from the activity of this pull request.), maybe we'd better to wait for this change can be taken into consideration in next big version(hopefully 2.0), and i'll do my work with some workarounds.", "As you wish, thanks. BTW, you can add metrics to an estimator using `tf.contrib.estimator.add_metrics`.", "Just ran into this - using batches of 256 I'm finding the trapezoidal AUC is way off. I know it's only ever going to be an estimate but careful_interpolation seems to at least be in the right ballpark. In case it saves someone else a few minutes, here is how to implement @roumposg's suggestion:\r\n\r\n    def my_auc(labels, predictions):\r\n        return {'auc_pr_careful_interpolation': tf.metrics.auc(labels, predictions['logistic'], curve='PR',\r\n                                                                summation_method='careful_interpolation')}\r\n\r\n    self._estimator = tf.contrib.estimator.add_metrics(estimator, my_auc)"]}, {"number": 19078, "title": "Getting an error while converting frozen inference graph (*.pb) to tflite graph with toco", "body": "I've trained the R-FCN model on my own training set. Now, I'd like to convert my frozen inference graph form the *.pb format to the *.tflite format to use it on an android mobile phone.\r\n\r\nAfter training, I exported the frozen inference graph with the following command:\r\n\r\n```\r\npython3 export_inference_graph.py \r\n--pipeline_config_path=\"training/ckpt/rfcn-69/pipeline.config\" \r\n--trained_checkpoint_prefix=\"training/ckpt/rfcn-69/model.ckpt-300000\" \r\n--output_directory=\"training/ckpt/rfcn-69/\"\r\n```\r\nAfterwards I run the transform_graph util to quantize the graph. I noticed that it doesn't matter wether I run the transform_graph or not. In the end I'm getting the same error with both of the graphs.\r\n\r\n```\r\nbazel run tensorflow/tools/graph_transforms/transform_graph -- \r\n--in_graph=\"/git/bda/frozen_graphs/rfcn-69/frozen_inference_graph.pb\" \r\n--out_graph=\"/git/bda/frozen_graphs/rfcn-69/quantized_graph_2.pb\" \r\n--inputs=image_tensor \r\n--outputs=\"num_detections,detection_boxes,detection_scores,detection_classes\" \r\n--transforms='fold_old_batch_norms quantize_weights strip_unused_nodes sort_by_execution_order obfuscate_names merge_duplicate_nodes'\r\n```\r\nFinally, I try to convert the quantized graph to an tflite graph with the toco util.\r\n\r\n```\r\nbazel run --config=opt  tensorflow/contrib/lite/toco:toco -- \r\n--input_file=/git/bda/frozen_graphs/rfcn-69/quantized_graph_2.pb \r\n--output_file=/git/bda/frozen_graphs/out.tflite --inference_type=FLOAT \r\n--input_shape=1,600,1024,3 \r\n--input_array=image_tensor \r\n--output_arrays=num_detections,detection_boxes,detection_scores,detection_classes\r\n```\r\nBut, the operation fails with the following error.\r\n\r\n```\r\n2018-04-11 13:19:58.364591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayV3\r\n2018-04-11 13:19:58.364606: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayV3\r\n2018-04-11 13:19:58.364656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArrayReadV3\r\n2018-04-11 13:19:58.364753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\r\n2018-04-11 13:19:58.364851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\r\n2018-04-11 13:19:58.364969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Dequantize\r\n2018-04-11 13:19:58.365001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: TensorArraySizeV3\r\n2018-04-11 13:19:58.365022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Dequantize\r\n2018-04-11 13:19:58.366115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1253] Converting unsupported operation: Where\r\n2018-04-11 13:20:00.620101: F tensorflow/contrib/lite/toco/tooling_util.cc:821] Check failed: d >= 1 (0 vs. 1)\r\n```\r\nI've got no clue what could be the problem. Any help would be highly appreciated.\r\n\r\nThanks in advance.", "comments": ["tflite is not supporting object detection models yet. ", "CC @bjacob this is from [the SO post I mentioned](https://stackoverflow.com/questions/49777020/getting-an-error-while-converting-frozen-inference-graph-pb-to-tflite-graph?noredirect=1#comment87356224_49777020). It's good to support zero-size Tensors where possible IMO.", "@achowdhery, could you offer some help?", "Hi, are you able to share a frozen graph of the r-fcn model?\r\nWe may be able to use something similar to this:\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193", "@MrMWalker Is this still an open issue?", "@MrMWalker Is this still an open issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19077, "title": "CSV parsing flow (TextLineDataset -> decode_csv) doesn't parse linebreak in quotes correctly", "body": "\r\nUsing tensorflow 1.8, parsing the following file with decode_csv:\r\n\"a\r\nb\",0\r\nraise the error \"Quoted field has to end with quote followed by delim or end\".\r\n\r\nTo reproduce:\r\n\r\n```\r\nfile = ['example.csv']\r\n\r\ndataset = tf.data.TextLineDataset(file)\r\ndataset = dataset.map(lambda record: tf.decode_csv(\r\n    record,\r\n    record_defaults=[[\"\"],[0]]\r\n))\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(next_element))\r\n```\r\n\r\nI think it is because the linebreak in the quote is not escaped. This behavior seems in contradiction with the RFC2048 specifications (section 2.6).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@rachellim Do we intend to fully support RFC2048?", "We fully adhere to RFC 4180 (https://tools.ietf.org/html/rfc4180) in the `tf.decode_csv` function.\r\n(I couldn't find anything in RFC 2048, section 2.6 about CSV... I assume that was a typo? Feel free to link me to a different RFC if there is one @PForet) \r\n\r\nHowever, the issue here is that `TextLineDataset` splits the file on all linebreaks, regardless of whether it's encapsulated in quotations (before any notion of CSV is introduced), so the `decode_csv` function receives an incomplete record as input. I'm working on a fused reading + parsing dataset type that will fix this issue.", "RFC 4180 indeed, my mistake. Thanks for the support. Should I edit the title to reflect the fact that the issue relates to `TextLineDataset`?"]}]