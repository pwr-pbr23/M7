[{"number": 40263, "title": "learned_unigram_candidate_sampler may enter infinite loop when range_max is a very large number", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0 & v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.random.learned_unigram_candidate_sampler`/`tf.nn.learned_unigram_candidate_sampler` may enter infinite loop when `range_max` is a very large number.\r\n\r\nWhen `range_max` is a large number, the execution may stuck in the while loop [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/random/weighted_picker.cc#L32-L34). The `LevelSize` function returns `1 << level`, so when `level` is a large number, the left shift will have undefined behavior. So `LevelSize(num_levels_ - 1)` may never be `>= N`, causing infinite loop.\r\n\r\n**Describe the expected behavior**\r\nIn `WeightedPicker::WeightedPicker(int N)`, there should be a boundary checking for `N` (same number as `range_max`). If input `N` is big enough to cause left shift to have undefined behavior, tensorflow should raise a warning to the user and perhaps an exception to stop the execution.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# this function call will not terminate\r\ntf.random.learned_unigram_candidate_sampler(\r\n    true_classes=np.random.rand(100, 10),\r\n    num_true=10,\r\n    num_sampled=100,\r\n    unique=True,\r\n    range_max=2000000000, # big enough to cause << to have undefined behavior & cause infinite loop\r\n)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.1, 2.2, nightly version(`2.3.0-dev20200605`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/8a6f0588ce2197d525ef1eed0715d6c6/untitled964.ipynb).Thanks!", "Was able to replicate the issue in TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/c274b12e078590ef5378634ddab17360/untitled83.ipynb).Thanks!"]}, {"number": 40255, "title": "Collect runtime information when using tf.function", "body": "**System information**\r\n- TensorFlow version (you are using): TF2.2\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn tf1.x, it is possible to use `RunOptions` and `RunMetadata` to get the runtime information for a single run. I wonder if it is possible to achieve the same functionality in tf2.x `tf.function`? I understand we have the beautiful profiling tool introduced in tf2.2, but it would be better if we can get the raw information besides viewing them through tensorboard. \r\n**Will this change the current api? How?**\r\nIt probably will. This may need to add a few more arguments to the function wrapped by `tf.function`.\r\n**Who will benefit with this feature?**\r\nThe developer who may need the raw runtime information to tune the model and help make the inherent optimization in tensorflow better.\r\n**Any Other info.**\r\nNot yet.", "comments": []}, {"number": 40253, "title": "Difference of 0.5 factor compared to Research Paper in Soft NMS implementation", "body": "Hi,\r\n\r\nI believe the below lines are for Gaussian implementation of Soft-NMS, since the variable 'scale' is used in:\r\nhttps://github.com/tensorflow/tensorflow/blob/7474d3c8345d663cea3f9132c47cd1bd342b0cec/tensorflow/core/kernels/non_max_suppression_op.cc#L189\r\nhttps://github.com/tensorflow/tensorflow/blob/7474d3c8345d663cea3f9132c47cd1bd342b0cec/tensorflow/core/kernels/non_max_suppression_op.cc#L194\r\n\r\nBut in the research paper, no multiplier of 0.5 is mentioned. Please refer page number 4 of [https://arxiv.org/pdf/1704.04503.pdf](https://arxiv.org/pdf/1704.04503.pdf).\r\n\r\nSo if paper suggests ideal sigma value of 0.5, should I configure value of 0.25 instead?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n", "Never mind I used 0.25 as sigma and got correct results. Just wondering why only tensorflow implementation has 0.5 in source code. There are various other implementations which work as is.", "Same question. Maybe it's a bug.", "Hi @jch1 @ymodak  this looks more like a bug. \r\n\r\nJonathan, I saw you implemented this code (thanks for the great implementation). I wonder is there any specific reason to use 0.5 instead of 1 (as the paper)?\r\nShould we either change it to 1 or update the document (https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression_with_scores) to mention this?\r\n\r\nThanks!", "@mingxingtan To me it looked like following a convention since in Gaussian Exponent, denominator is always 2*sigma. But still it is better to follow what the paper suggests.", "Any update?\r\nCould I submit a pull request?", "can I do it? :)", "@swapniel99 @fsx950223 I guess it is not easy to change the value because many existing code already has dependence on it, but you should definitely update the doc to reflect this point.", "> @swapniel99 @fsx950223 I guess it is not easy to change the value because many existing code already has dependence on it, but you should definitely update the doc to reflect this point.\r\n\r\nMaybe they set wrong soft_nms_sigma too.\r\nhttps://github.com/tensorflow/models/blob/451906e4e82f19712455066c1b27e2a6ba71b1dd/research/object_detection/core/multiclass_nms_test.py#L566", "> > @swapniel99 @fsx950223 I guess it is not easy to change the value because many existing code already has dependence on it, but you should definitely update the doc to reflect this point.\n> \n> Maybe they set wrong soft_nms_sigma too.\n> https://github.com/tensorflow/models/blob/451906e4e82f19712455066c1b27e2a6ba71b1dd/research/object_detection/core/multiclass_nms_test.py#L566\n\nWe can test it but I don't think so. I came across this issue while using object detection api itself."]}, {"number": 40252, "title": "Support including tensorflow directly in bazel WORKSPACEs", "body": "We recently started including tensorflow in our bazel based iOS builds. Because of the currently required `./configure` step we cannot include it directly in our WORKSPACE and depend on targets as you can with some bazel dependencies, instead we have to pre-compile the targets we're interested in, and vendor them somehow into our build.\r\n\r\nIt would be great if the tensorflow build system provided a way to be able to depend on it directly from within other bazel workspaces.\r\n\r\nGiven the current job of the configure script this might be a very difficult request.", "comments": []}, {"number": 40228, "title": "Tensor had Inf values (maybe from TensorArray)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): `pip3.7 install --user tensorflow-gpu`\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.7 (installed via Linuxbrew)\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): Not relevant\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6\r\n- GPU model and memory: GTX 2070\r\n\r\n**Describe the current behavior**\r\n\r\nWe observed strange unstable training behavior with some TF versions, mostly for attention-based LSTM-based encoder-decoder models (for speech recognition or translation). This results in getting inf or nan at some very early point in training (after 100-200 steps or so).\r\nNormally this might just be due to too high learning rate or so. However, the setup runs totally fine up to (including) TF 1.14.0, and it occured only since 1.15.0. This might just be due to bad luck. However, this problem was reported by a couple of individual people, for individual different setups. It was always the same observation: The setup ran fine with TF <=1.14, and produced inf or nan with TF 1.15. And it was always an attention-based LSTM-based encoder-decoder model.\r\nIn all cases, this was with CUDA 10.1. (I saw #31166 which reported a problem with CUDA 10.0, but this seems to be fixed in CUDA 10.1, so it seems that we have some different problem here.)\r\n\r\nThis looked a bit too suspicious. The bug in our framework [RETURNN](https://github.com/rwth-i6/returnn/) is reported [here](https://github.com/rwth-i6/returnn/issues/297).\r\n\r\nSo I tried to reproduce the problem. This was not really so simple but I finally managed to come up with a standalone script (independent of our framework) (see below for the code). This script can run on TF 1.14, TF 1.15 and TF 2.2 (and probably other TF versions as well). And it always produces Inf at some point (sometimes earlier, sometimes later), on GPU, with CUDA 10.1.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe training setup of our framework should also run stable with TF 1.15.\r\n\r\nThe simple example script below should never ever produce Inf.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe following code reproduces the problem for me:\r\n```\r\nimport numpy\r\nimport tensorflow as tf\r\nimport argparse\r\n\r\n\r\ndef add_check_numerics_ops(name=\"add_check_numerics_ops\"):\r\n  \"\"\"\r\n  This is similar to :func:`tf.add_check_numerics_ops` and based on similar code.\r\n  It adds some more logic and options.\r\n  Copied from RETURNN, and simplified.\r\n  :param str name: op-name for the final tf.group\r\n  :return: operation which performs all the checks\r\n  :rtype: tf.Operation\r\n  \"\"\"\r\n  ops = tf.compat.v1.get_default_graph().get_operations()\r\n  with tf.name_scope(name):\r\n    check_op = []\r\n    # This code relies on the ordering of ops in get_operations().\r\n    # The producer of a tensor always comes before that tensor's consumer in\r\n    # this list. This is true because get_operations() returns ops in the order\r\n    # added, and an op can only be added after its inputs are added.\r\n    for op in ops:\r\n      assert isinstance(op, tf.Operation)\r\n      # Frames from within a while-loop are partly broken.\r\n      # https://github.com/tensorflow/tensorflow/issues/2211\r\n      # noinspection PyProtectedMember\r\n      if op._get_control_flow_context() != tf.compat.v1.get_default_graph()._get_control_flow_context():\r\n        continue\r\n      for output in op.outputs:\r\n        if output.dtype not in [tf.float16, tf.float32, tf.float64]:\r\n          continue\r\n        message = op.name + \":\" + str(output.value_index)\r\n        with tf.control_dependencies(check_op):\r\n          print(\"add check for:\", output, op.type)\r\n          check_op = [tf.compat.v1.check_numerics(output, message=message, name=op.name + \"_check_numerics\")]\r\n    return tf.group(*check_op)\r\n\r\n\r\ndef main():\r\n  arg_parser = argparse.ArgumentParser()\r\n  arg_parser.add_argument(\"--nsteps\", type=int, default=-1)\r\n  arg_parser.add_argument(\"--reset_after_nsteps\", type=int, default=-1)\r\n  args = arg_parser.parse_args()\r\n\r\n  print(\"TF version:\", tf.__version__)\r\n\r\n  # tf.compat.v1.disable_eager_execution()\r\n  tf.compat.v1.disable_v2_behavior()\r\n\r\n  n_input_dim = 2\r\n  n_classes_dim = 3\r\n  x = tf.compat.v1.placeholder(tf.float32, shape=(None, None, n_input_dim), name=\"x\")\r\n  targets = tf.compat.v1.placeholder(tf.int32, shape=(None, None), name=\"targets\")\r\n  encoder = tf.keras.layers.Dense(units=5, activation=\"tanh\", name=\"encoder\")(x)\r\n  batch = tf.shape(encoder)[0]\r\n  size = tf.shape(encoder)[1]\r\n  orth_embed = tf.keras.layers.Embedding(input_dim=n_classes_dim, output_dim=6)(targets)  # (B,T,D)\r\n  orth_embed = tf.transpose(orth_embed, [1, 0, 2])  # (T,B,D)\r\n  prev_orth_embed = tf.concat(\r\n    [tf.zeros([1, batch, orth_embed.get_shape().as_list()[-1]]), orth_embed[:-1]], axis=0)  # (T,B,D)\r\n  prev_orth_embed_ta = tf.TensorArray(\r\n    tf.float32, name=\"prev_orth_embed_ta\", dynamic_size=True, size=0,\r\n    element_shape=(None, prev_orth_embed.get_shape().as_list()[-1]))\r\n  prev_orth_embed_ta = prev_orth_embed_ta.unstack(prev_orth_embed)\r\n  c_ta = tf.TensorArray(tf.float32, name=\"c_ta\", dynamic_size=True, size=0)\r\n  s_ta = tf.TensorArray(tf.float32, name=\"s_ta\", dynamic_size=True, size=0)\r\n\r\n  def loop_cond(t, *args):\r\n    return tf.less(t, size)\r\n\r\n  s_lstm = tf.keras.layers.LSTMCell(5, name=\"s\")  # originally was LSTMBlockCell\r\n\r\n  def loop_body(t, prev_c, prev_s_state, c_ta_, s_ta_):\r\n    assert isinstance(prev_c, tf.Tensor)\r\n    prev_c.set_shape((None, encoder.get_shape().as_list()[-1]))\r\n    prev_orth_embed_t = prev_orth_embed_ta.read(t)  # (B,D)\r\n    s_in_in = tf.concat([prev_c, prev_orth_embed_t], axis=-1)  # (B,D)\r\n    s_in = tf.keras.layers.Dense(units=5, name=\"s_in\", activation=\"tanh\")(s_in_in)\r\n    s, s_state = s_lstm(s_in, prev_s_state)\r\n    c_in = s  # (B,D)\r\n\r\n    # dot attention\r\n    base = encoder  # (batch, base_time, n_out)\r\n    base_ctx = encoder  # (batch, base_time, inner)\r\n    source = tf.expand_dims(c_in, axis=2)  # (batch, inner, 1)\r\n    energy = tf.matmul(base_ctx, source)  # (batch, base_time, 1)\r\n    energy.set_shape(tf.TensorShape([None, None, 1]))\r\n    energy = tf.squeeze(energy, axis=2)  # (batch, base_time)\r\n    energy_mask = tf.sequence_mask(tf.fill([batch], size), maxlen=tf.shape(energy)[1])\r\n    # NOTE: The following line seems to trigger it!\r\n    energy = tf.where(energy_mask, energy, float(\"-inf\") * tf.ones_like(energy))\r\n    base_weights = tf.nn.softmax(energy)  # (batch, base_time)\r\n    base_weights_bc = tf.expand_dims(base_weights, axis=1)  # (batch, 1, base_time)\r\n    out = tf.matmul(base_weights_bc, base)  # (batch, 1, n_out)\r\n    out.set_shape(tf.TensorShape([None, 1, base.get_shape().as_list()[-1]]))\r\n    c = tf.squeeze(out, axis=1)  # (batch, n_out)\r\n\r\n    assert isinstance(c_ta_, tf.TensorArray)\r\n    assert isinstance(s_ta_, tf.TensorArray)\r\n    c_ta_ = c_ta_.write(t, c)\r\n    s_ta_ = s_ta_.write(t, s)\r\n\r\n    return t + 1, c, s_state, c_ta_, s_ta_\r\n\r\n  _, _, _, c_ta, s_ta = tf.while_loop(\r\n    cond=loop_cond, body=loop_body,\r\n    loop_vars=(\r\n      0,  # t\r\n      tf.zeros([batch, tf.shape(encoder)[-1]]),  # prev_c\r\n      s_lstm.get_initial_state(batch_size=batch, dtype=tf.float32),  # prev_s_state\r\n      c_ta, s_ta))\r\n\r\n  assert isinstance(c_ta, tf.TensorArray)\r\n  assert isinstance(s_ta, tf.TensorArray)\r\n  c_ = c_ta.stack()  # (T,B,D)\r\n  s_ = s_ta.stack()  # (T,B,D)\r\n  cs = tf.concat([c_, s_], axis=-1)  # (T,B,D)\r\n  cs = tf.transpose(cs, [1, 0, 2])  # (B,T,D)\r\n  att = tf.keras.layers.Dense(units=6, name=\"att\", activation=\"tanh\")(cs)\r\n  output_logits = tf.keras.layers.Dense(units=n_classes_dim, name=\"output_prob\", activation=None)(att)\r\n  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=output_logits)  # (B,T)\r\n  loss = tf.reduce_mean(loss)\r\n  loss_eval = loss\r\n\r\n  opt = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)  # originally was NadamOptimizer...\r\n  minimize_op = opt.minimize(loss)\r\n\r\n  check_op = add_check_numerics_ops()\r\n  with tf.control_dependencies([check_op, minimize_op]):\r\n    loss = tf.identity(loss)\r\n  vars_init_op = tf.compat.v1.global_variables_initializer()\r\n\r\n  rnd = numpy.random.RandomState(42)\r\n  n_batch = 2\r\n  n_time = 5\r\n  x_np = rnd.normal(size=(n_batch, n_time, n_input_dim))\r\n  targets_np = rnd.randint(0, n_classes_dim, size=(n_batch, n_time))\r\n\r\n  with tf.compat.v1.Session() as session:\r\n    session.run(vars_init_op)\r\n    step = 0\r\n    while True:\r\n      print(\"step %i, loss:\" % step, session.run(loss, feed_dict={x: x_np, targets: targets_np}))\r\n      print(\"step %i, loss (eval):\" % step, session.run(loss_eval, feed_dict={x: x_np, targets: targets_np}))\r\n      step += 1\r\n      if 0 <= args.nsteps <= step:\r\n        print(\"Stop after %i steps.\" % args.nsteps)\r\n        break\r\n      if args.reset_after_nsteps >= 0 and step % args.reset_after_nsteps == 0:\r\n        print(\"Reset after %i steps.\" % args.reset_after_nsteps)\r\n        session.run(vars_init_op)\r\n\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\nI use the option `--reset_after_nsteps 100`.\r\n\r\nThe current version of this test case can also be found [here](https://github.com/albertz/playground/blob/master/tf-test-tensorarray-bug.py).\r\n\r\n**Other info / logs**\r\n\r\n```\r\n...\r\nstep 652, loss: 0.106880724                                                                                                 \r\nstep 652, loss (eval): 0.098194                                                                                             \r\nstep 653, loss: 0.098194                                                                                                    \r\nstep 653, loss (eval): 0.09007898                                                                                           \r\n2020-06-07 00:05:42.224810: E tensorflow/core/kernels/check_numerics_op.cc:289] abnormal_detected_host @0x7f149dc07d00 = {0, 1} gradients/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3:1                                 \r\nInvalidArgumentError: 2 root error(s) found.                                                                                \r\n  (0) Invalid argument: gradients/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3:1 : Tensor had Inf values                                                                                                                 \r\n         [[node add_check_numerics_ops/gradients/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3_check_numerics (defined at tf-test-tensorarray-bug.py:47) ]]                                                               \r\n         [[Identity/_69]]                                                                                                   \r\n  (1) Invalid argument: gradients/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3:1 : Tensor had Inf values                                                                                                                 \r\n         [[node add_check_numerics_ops/gradients/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3_check_numerics (defined at tf-test-tensorarray-bug.py:47) ]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node add_check_numerics_ops/gradients/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3_check_numerics:\r\n gradients/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 (defined at tf-test-tensorarray-bug.py:138)\r\n\r\nInput Source operations connected to node add_check_numerics_ops/gradients/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3_check_numerics:\r\n gradients/TensorArrayStack/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3 (defined at tf-test-tensorarray-bug.py:138)\r\n```\r\n", "comments": ["@albertz \r\n\r\nI have tried in colab with TF version 1.15,2.2.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1b0b552b9efa138daf6838947671dabd/untitled960.ipynb).Is this the expected behavior?.Thanks!", "No. You have some `ipykernel_launcher` error in there, which is related to Colab. You would need to adapt the script for Colab.\r\nI just tested it: You can just change `arg_parser.parse_args()` to `arg_parser.parse_args([\"--reset_after_nsteps\", \"100\"])`. Then it works in Colab.\r\n\r\nAnd it reproduces the error:\r\n```\r\nstep 22, loss (eval): 0.65236515\r\nstep 23, loss: 0.65236515\r\nstep 23, loss (eval): 0.63582206\r\nstep 24, loss: 0.63582206\r\nstep 24, loss (eval): 0.61860085\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n7 frames\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: s_ta:1 : Tensor had Inf values\r\n\t [[{{node add_check_numerics_ops/s_ta_check_numerics}}]]\r\n\t [[Identity/_69]]\r\n  (1) Invalid argument: s_ta:1 : Tensor had Inf values\r\n\t [[{{node add_check_numerics_ops/s_ta_check_numerics}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\nI also noticed that if you run the cell multiple times in Colab, you might need to add `tf.compat.v1.reset_default_graph()` after `tf.compat.v1.disable_v2_behavior()`.\r\nIt seems to reproduce the error in all cases:\r\n```\r\nTF version: 2.2.0\r\n...\r\nstep 17, loss: 0.81687677\r\nstep 17, loss (eval): 0.7994094\r\nstep 18, loss: 0.7994094\r\nstep 18, loss (eval): 0.78124017\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n7 frames\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: prev_orth_embed_ta:1 : Tensor had Inf values\r\n\t [[{{node add_check_numerics_ops/prev_orth_embed_ta_check_numerics}}]]\r\n\t [[Identity/_69]]\r\n  (1) Invalid argument: prev_orth_embed_ta:1 : Tensor had Inf values\r\n\t [[{{node add_check_numerics_ops/prev_orth_embed_ta_check_numerics}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "@albertz I was able to reproduce this issue. Please find the gist [here](https://colab.research.google.com/gist/gowthamkpr/1eae651a81bc489443a7fdb5119a4ef4/untitled960.ipynb). Thanks for reporting.", "Can you try turning on tf.debugging.enable_check_numerics() and then run it? That should pinpoint where the Inf values started coming from. The TensorArray might be a red herring as it just is a container for values.", "I tried with `tf.debugging.enable_check_numerics()` but that will raise at `tf.where(energy_mask, energy, float(\"-inf\") * tf.ones_like(energy))`, where the `inf` is intended, which should not be a problem, but maybe that triggers the wrong TF behavior.\r\n", "I just figured out that you can use `tf.debugging.enable_check_numerics()` and  just disable it check numerics when the energy mask it applied. I.e. like this:\r\n```\r\n    tf.debugging.disable_check_numerics()\r\n    # NOTE: The following line seems to trigger it!\r\n    energy = tf.where(energy_mask, energy, float(\"-inf\") * tf.ones_like(energy))\r\n    tf.debugging.enable_check_numerics()\r\n    base_weights = tf.nn.softmax(energy)  # (batch, base_time)\r\n```\r\nNote that there will be a check numerics on the output of the `softmax` here, and that looks fine.\r\n\r\nBut then check numerics (V2 now) again catches something related to `TensorArray`:\r\n```\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: \r\n\r\n!!! Detected Infinity or NaN in output 1 of graph op \"TensorArrayV3\" (# of outputs: 2) !!!\r\n  dtype: <dtype: 'float32'>\r\n  shape: ()\r\n\r\n  Input tensor: Tensor(\"s_ta/size:0\", shape=(), dtype=int32)\r\n\r\n  Stack trace of op's creation (\"->\": inferred user code):\r\n ...\r\n -> |   main()\r\n    + <ipython-input-1-1f33cd60be78> (L68) main\r\n -> |   s_ta = tf.TensorArray(tf.float32, name=\"s_ta\", dynamic_size=True, size=0)\r\n  ...\r\n\r\n : Tensor had -Inf values\r\n\t [[{{node s_ta/CheckNumericsV2}}]]\r\n\t [[Identity/_69]]\r\n  (1) Invalid argument: \r\n\r\n!!! Detected Infinity or NaN in output 1 of graph op \"TensorArrayV3\" (# of outputs: 2) !!!\r\n  dtype: <dtype: 'float32'>\r\n  shape: ()\r\n\r\n  Input tensor: Tensor(\"s_ta/size:0\", shape=(), dtype=int32)\r\n\r\n  Stack trace of op's creation (\"->\": inferred user code):\r\n ...\r\n -> |   main()\r\n    + <ipython-input-1-1f33cd60be78> (L68) main\r\n -> |   s_ta = tf.TensorArray(tf.float32, name=\"s_ta\", dynamic_size=True, size=0)\r\n    + ...packages/tensorflow/python/ops/tensor_array_ops.py (L1082) __init__\r\n    |   name=name)\r\n    + ...packages/tensorflow/python/ops/tensor_array_ops.py (L165) __init__\r\n    |   self._handle, self._flow = create()\r\n    ...\r\n\r\n : Tensor had -Inf values\r\n\t [[{{node s_ta/CheckNumericsV2}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\nI'm not sure I fully understand this output. The graph op `TensorArrayV3` returns just the resource handle, and this flow tensor, right?\r\nHowever, now I checked the code of the underlying kernel ([here](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/core/kernels/tensor_array_ops.cc#L143)), and it looks like the flow value is not initialized on GPU, right? So that means that could be anything, like inf or nan.", "I think this whole test case actually only triggers the problem that the flow of a TensorArray is uninitialized and can be inf or nan in some cases, and `check_numerics` catches that.\r\n`check_numerics_callback.IGNORE_OP_OUTPUTS` should probably be extended by `(b\"TensorArrayV3\", 1)` and maybe other entries.\r\nUnfortunately this does not explain my original problem then... (which is inf/nan or just unstable training behavior since TF >=1.15, but stable training with TF <=1.14).\r\n", "Note that with the same test case, without the `check_numerics` test, I also have seen strange hickups of the loss (going up and down) with TF 1.15. This probably points at another problem, and this might be more related to the original problem I described.\r\nUnfortunately I don't see the same behavior in Colab. The loss always seems to go down there. Maybe this needs some other GPU or other environment.\r\n", "I'm able to reproduce getting *nan* now. This only seems to occur on TF 1.15.\r\n(The full version of the modified test case can be found [here](https://github.com/albertz/playground/blob/21d081ef01a4b8a97b99ce12c99d7e72e4d5685b/tf-test-tensorarray-bug.py). This includes a copy of `LSTMBlockCell` as this was removed in TF 2.2.0, but the raw op is still there, so I copied in the cell class.)\r\n`LSTMBlockCell` seems to be crucial to reproduce the problem.\r\nA reduced test case (only for TF 1) is here:\r\n```\r\n# https://github.com/albertz/playground/blob/21d081ef01a4b8a97b99ce12c99d7e72e4d5685b/tf-test-tensorarray-bug.py\r\n# https://github.com/albertz/playground/blob/master/tf-test-tensorarray-bug.py\r\n\r\n\"\"\"\r\nTrying to reproduce inf/nan bug in TensorFlow.\r\nhttps://github.com/rwth-i6/returnn/issues/297\r\n\r\nFollowing test_rec_subnet_train_t3d_simple from RETURNN.\r\n\r\n\"\"\"\r\n\r\nimport sys\r\nimport numpy\r\nimport tensorflow as tf\r\nimport argparse\r\n\r\n\r\n# https://stackoverflow.com/questions/53581278/test-if-notebook-is-running-on-google-colab\r\nIN_COLAB = 'google.colab' in sys.modules\r\n\r\n\r\ndef main():\r\n  arg_parser = argparse.ArgumentParser()\r\n  arg_parser.add_argument(\"--nsteps\", type=int, default=-1)\r\n  arg_parser.add_argument(\"--reset_after_nsteps\", type=int, default=-1)\r\n  arg_parser.add_argument(\"--input_dim\", type=int, default=2)\r\n  arg_parser.add_argument(\"--classes_dim\", type=int, default=3)\r\n  arg_parser.add_argument(\"--batch_size\", type=int, default=2)\r\n  arg_parser.add_argument(\"--seq_len\", type=int, default=5)\r\n  args = [\"--reset_after_nsteps\", \"100\", \"--seq_len\", \"11\"]\r\n  if not IN_COLAB and len(sys.argv) > 1:\r\n    args = sys.argv[1:]\r\n  args = arg_parser.parse_args(args)\r\n\r\n  print(\"TF version:\", tf.__version__)\r\n\r\n  if tf.__version__.startswith(\"2.\"):\r\n    # tf.compat.v1.disable_eager_execution()\r\n    tf.compat.v1.disable_v2_behavior()\r\n  # If in Colab, and you run this repeatedly.\r\n  tf.compat.v1.reset_default_graph()\r\n  # enable_check_numerics_v2()\r\n\r\n  n_input_dim = args.input_dim\r\n  n_classes_dim = args.classes_dim\r\n  x = tf.compat.v1.placeholder(tf.float32, shape=(None, None, n_input_dim), name=\"x\")\r\n  targets = tf.compat.v1.placeholder(tf.int32, shape=(None, None), name=\"targets\")\r\n  encoder = tf.keras.layers.Dense(units=5, activation=\"tanh\", name=\"encoder\")(x)\r\n  batch = tf.shape(encoder)[0]\r\n  size = tf.shape(encoder)[1]\r\n  orth_embed = tf.keras.layers.Embedding(input_dim=n_classes_dim, output_dim=6)(targets)  # (B,T,D)\r\n  orth_embed = tf.transpose(orth_embed, [1, 0, 2])  # (T,B,D)\r\n  prev_orth_embed = tf.concat(\r\n    [tf.zeros([1, batch, orth_embed.get_shape().as_list()[-1]]), orth_embed[:-1]], axis=0)  # (T,B,D)\r\n  prev_orth_embed_ta = tf.TensorArray(\r\n    tf.float32, name=\"prev_orth_embed_ta\", dynamic_size=True, size=0,\r\n    element_shape=(None, prev_orth_embed.get_shape().as_list()[-1]))\r\n  prev_orth_embed_ta = prev_orth_embed_ta.unstack(prev_orth_embed)\r\n  c_ta = tf.TensorArray(tf.float32, name=\"c_ta\", dynamic_size=True, size=0)\r\n  s_ta = tf.TensorArray(tf.float32, name=\"s_ta\", dynamic_size=True, size=0)\r\n\r\n  def loop_cond(t, *args):\r\n    return tf.less(t, size)\r\n\r\n  # from tensorflow.python.ops import rnn_cell\r\n  # s_lstm = rnn_cell.LSTMCell(5, name=\"s\")  # originally was LSTMBlockCell\r\n  # s_lstm = tf.keras.layers.LSTMCell(5, name=\"s\")  # originally was LSTMBlockCell\r\n  from tensorflow.contrib.rnn.python.ops.lstm_ops import LSTMBlockCell\r\n  s_lstm = LSTMBlockCell(num_units=5, name=\"s\")\r\n\r\n  def loop_body(t, prev_c, prev_s_state, c_ta_, s_ta_):\r\n    assert isinstance(prev_c, tf.Tensor)\r\n    prev_c.set_shape((None, encoder.get_shape().as_list()[-1]))\r\n    prev_orth_embed_t = prev_orth_embed_ta.read(t)  # (B,D)\r\n    s_in_in = tf.concat([prev_c, prev_orth_embed_t], axis=-1)  # (B,D)\r\n    s_in = tf.keras.layers.Dense(units=5, name=\"s_in\", activation=\"tanh\")(s_in_in)\r\n    s, s_state = s_lstm(s_in, prev_s_state)\r\n    c_in = s  # (B,D)\r\n\r\n    # dot attention\r\n    base = encoder  # (batch, base_time, n_out)\r\n    base_ctx = encoder  # (batch, base_time, inner)\r\n    source = tf.expand_dims(c_in, axis=2)  # (batch, inner, 1)\r\n    energy = tf.matmul(base_ctx, source)  # (batch, base_time, 1)\r\n    energy.set_shape(tf.TensorShape([None, None, 1]))\r\n    energy = tf.squeeze(energy, axis=2)  # (batch, base_time)\r\n    energy_mask = tf.sequence_mask(tf.fill([batch], size), maxlen=tf.shape(energy)[1])\r\n    # NOTE: The following line seems to trigger it!\r\n    energy = tf.where(energy_mask, energy, float(\"-inf\") * tf.ones_like(energy))\r\n    base_weights = tf.nn.softmax(energy)  # (batch, base_time)\r\n    base_weights_bc = tf.expand_dims(base_weights, axis=1)  # (batch, 1, base_time)\r\n    out = tf.matmul(base_weights_bc, base)  # (batch, 1, n_out)\r\n    out.set_shape(tf.TensorShape([None, 1, base.get_shape().as_list()[-1]]))\r\n    c = tf.squeeze(out, axis=1)  # (batch, n_out)\r\n\r\n    assert isinstance(c_ta_, tf.TensorArray)\r\n    assert isinstance(s_ta_, tf.TensorArray)\r\n    c_ta_ = c_ta_.write(t, c)\r\n    s_ta_ = s_ta_.write(t, s)\r\n\r\n    return t + 1, c, s_state, c_ta_, s_ta_\r\n\r\n  _, _, _, c_ta, s_ta = tf.while_loop(\r\n    cond=loop_cond, body=loop_body,\r\n    loop_vars=(\r\n      0,  # t\r\n      tf.zeros([batch, tf.shape(encoder)[-1]]),  # prev_c\r\n      s_lstm.zero_state(batch_size=batch, dtype=tf.float32),  # prev_s_state\r\n      c_ta, s_ta))\r\n\r\n  assert isinstance(c_ta, tf.TensorArray)\r\n  assert isinstance(s_ta, tf.TensorArray)\r\n  c_ = c_ta.stack()  # (T,B,D)\r\n  s_ = s_ta.stack()  # (T,B,D)\r\n  cs = tf.concat([c_, s_], axis=-1)  # (T,B,D)\r\n  cs = tf.transpose(cs, [1, 0, 2])  # (B,T,D)\r\n  att = tf.keras.layers.Dense(units=6, name=\"att\", activation=\"tanh\")(cs)\r\n  output_logits = tf.keras.layers.Dense(units=n_classes_dim, name=\"output_prob\", activation=None)(att)\r\n  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=output_logits)  # (B,T)\r\n  loss = tf.reduce_mean(loss)\r\n  loss_eval = loss\r\n\r\n  opt = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)  # originally was NadamOptimizer...\r\n  minimize_op = opt.minimize(loss)\r\n\r\n  check_op = tf.no_op()  # add_check_numerics_v1_ops()\r\n  with tf.control_dependencies([check_op, minimize_op]):\r\n    loss = tf.identity(loss)\r\n  vars_init_op = tf.compat.v1.global_variables_initializer()\r\n\r\n  rnd = numpy.random.RandomState(42)\r\n  n_batch = args.batch_size\r\n  n_time = args.seq_len\r\n  x_np = rnd.normal(size=(n_batch, n_time, n_input_dim))\r\n  targets_np = rnd.randint(0, n_classes_dim, size=(n_batch, n_time))\r\n\r\n  count_errors = 0\r\n  loss_np = float(\"inf\")\r\n  with tf.compat.v1.Session() as session:\r\n    session.run(vars_init_op)\r\n    step = 0\r\n    while True:\r\n      print(\"step %i, loss:\" % step, session.run(loss, feed_dict={x: x_np, targets: targets_np}))\r\n      loss_np_ = session.run(loss_eval, feed_dict={x: x_np, targets: targets_np})\r\n      # print(\"step %i, loss (eval):\" % step, loss_np_)\r\n      if not numpy.isfinite(loss_np_):  # or loss_np_ > loss_np * 2.\r\n        print(\"ERR, loss invalid:\", loss_np_)\r\n        count_errors += 1\r\n        if count_errors >= 10:\r\n          sys.exit(1)\r\n      loss_np = loss_np_\r\n      step += 1\r\n      if 0 <= args.nsteps <= step:\r\n        print(\"Stop after %i steps.\" % args.nsteps)\r\n        break\r\n      if args.reset_after_nsteps >= 0 and step % args.reset_after_nsteps == 0:\r\n        print(\"Reset after %i steps.\" % args.reset_after_nsteps)\r\n        session.run(vars_init_op)\r\n        loss_np = float(\"inf\")\r\n\r\n\r\nif __name__ == '__main__':\r\n  try:\r\n    main()\r\n  except KeyboardInterrupt:\r\n    print(\"KeyboardInterrupt\")\r\n    sys.exit(1)\r\n```\r\n[Colab link](https://colab.research.google.com/drive/1CBqBLeE-jS3uNW-vpW7aUM9n_ITA1h0J?usp=sharing)\r\n\r\nAlso very interesting is that `energy = tf.where(energy_mask, energy, float(\"-inf\") * tf.ones_like(energy))` is also important to trigger the problem. Without that, it does not happen.\r\n\r\nRandom hypothesis: Only this line causes that there is some nan in the GPU memory, which is later released (but not reset), and somehow matmul makes use of uninitialized GPU memory, and gets this nan now.\r\n(This seems to only happen with the matmul inside LSTMBlockCell, not the normal matmul. I assume this because we also have an own native LSTM implementation in our framework, which does exactly the same matmul call as LSTMBlockCell, and we see the same problem with that.)\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Any update? I think my latest test case is a valid test case which should reproduce the problem on TF 1.15.", "@albertz It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version  2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40228\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40228\">No</a>\n", "Sorry for the late response. I updated my colab [here](https://colab.research.google.com/drive/1CBqBLeE-jS3uNW-vpW7aUM9n_ITA1h0J?usp=sharing#scrollTo=3kejg29elFZt) for TensorFlow 2.6. The problem still persists.", "@rohan100jain @googlebot @sushreebarsa @gowthamkpr @tensorflowbutler I cannot reopen myself. Can someone reopen?"]}, {"number": 40222, "title": "remove training nodes from freeze_graph failure using TransformGraph.", "body": "Getting TensorRT conversion failure due to ops not supported in TensorRT. Till now I have come to conclusion to optimize/prune the original tensorflow flow graph so that freeze graph should have only supported ops as per TensorRT and that should then convert easily to uff.\r\n\r\nTo do this, i want to remove training nodes (map/*) using TransformGraph. Option i used are below, other then Input shape change and \u201cIdentity\u201d node no other node removed like : \u2018switch\u2019, \u2018exit\u2019, \u2018add\u2019.\r\n\r\n1) what options to choose in TransformGraph() to remove \u2018switch\u2019, \u2018exit\u2019, \u2018add\u2019 nodes?\r\n2) Used Optimize_for_inference() and remvoe_traing_nodes() without  success.\r\n\r\nSaved_model:\r\nhttps://drive.google.com/file/d/15VWWKp-F4LNcajbcG7a_awJayAjfM5r_/view?usp=sharing\r\n\r\nFreeze_graph.pb:\r\nhttps://drive.google.com/file/d/1OhWTui8Jdsh-ZBvBMYvMvyYrVh_t88cO/view?usp=sharing\r\n\r\nTransformGraph notebook:\r\nhttps://drive.google.com/file/d/1ePqNPHjcbf5IhrLAK1yRpHM9IOnhk7aG/view?usp=sharing\r\n\r\nI want to remove thses nodes. (Graph Image)\r\nhttps://drive.google.com/file/d/1SFKn-EHjP7hYrvUSNG8aaZ1p9ZYPbeG0/view?usp=sharing\r\n\r\nThanks\r\n", "comments": ["@yasingh2007 \r\nPlease share the tensorflow version on which the error is faced.", "(base) ubuntu@usva-asd-ml-ear1:~$ pip list | grep tensor\r\ntensorboard                        1.15.0\r\ntensorflow                         1.15.2\r\ntensorflow-datasets                3.1.0\r\ntensorflow-estimator               1.15.1\r\ntensorflow-gpu                     1.15.2\r\ntensorflow-metadata                0.22.0\r\ntensorrt                           7.0.0.11", "@yasingh2007 \r\nIs there any particular reason for using an older version of tensorflow can you please use later version and let us know if you are facing any issues.", "Yes, We are developing whole project since last year when TF 1.15 was released and Porting to TF2.0 has not been successful due to API differences.  But training nodes should be removed in TF1.5 version also. Can you please check the notebook I am using to strip the training nodes from freezed graph?\r\n\r\nAll the map nodes below are training nodes, these are not coded in architecture layers but introduced when training. Do you believ that they should have been removed by \"remove_traing_nodes\", + \"Optimize_for_refrence\" + TransformGraph()? Can you check this notebook and tell us that if our method to strip training nodes is correct or need some other options?\r\n\r\nTransformGraph notebook:\r\nhttps://drive.google.com/file/d/1ePqNPHjcbf5IhrLAK1yRpHM9IOnhk7aG/view?usp=sharing\r\n\r\npython3 -m tensorflow.python.tools.freeze_graph --input_saved_model_dir ../head-pose-estimation/assets/pose_model/ --output_node_names layer6/final_dense --output_graph frozen_graph.pb\r\n\r\nbutterfly/image_tensor\r\n**butterfly/map/Shape\r\nbutterfly/map/strided_slice/stack\r\nbutterfly/map/strided_slice/stack_1\r\nbutterfly/map/strided_slice/stack_2\r\nbutterfly/map/strided_slice\r\nbutterfly/map/TensorArray\r\nbutterfly/map/TensorArrayUnstack/Shape\r\nbutterfly/map/TensorArrayUnstack/strided_slice/stack\r\nbutterfly/map/TensorArrayUnstack/strided_slice/stack_1\r\nbutterfly/map/TensorArrayUnstack/strided_slice/stack_2\r\nbutterfly/map/TensorArrayUnstack/strided_slice\r\nbutterfly/map/TensorArrayUnstack/range/start\r\nbutterfly/map/TensorArrayUnstack/range/delta\r\nbutterfly/map/TensorArrayUnstack/range\r\nbutterfly/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3\r\nbutterfly/map/Const\r\nbutterfly/map/TensorArray_1\r\nbutterfly/map/while/iteration_counter\r\nbutterfly/map/while/Enter\r\nbutterfly/map/while/Enter_1\r\nbutterfly/map/while/Enter_2\r\nbutterfly/map/while/Merge\r\nbutterfly/map/while/Merge_1\r\nbutterfly/map/while/Merge_2\r\nbutterfly/map/while/Less/Enter\r\nbutterfly/map/while/Less\r\nbutterfly/map/while/Less_1\r\nbutterfly/map/while/LogicalAnd\r\nbutterfly/map/while/LoopCond\r\nbutterfly/map/while/Switch\r\nbutterfly/map/while/Switch_1\r\nbutterfly/map/while/Switch_2\r\nbutterfly/map/while/Identity\r\nbutterfly/map/while/Identity_1\r\nbutterfly/map/while/Identity_2\r\nbutterfly/map/while/add/y\r\nbutterfly/map/while/add\r\nbutterfly/map/while/TensorArrayReadV3/Enter\r\nbutterfly/map/while/TensorArrayReadV3/Enter_1\r\nbutterfly/map/while/TensorArrayReadV3\r\nbutterfly/map/while/resize/ExpandDims/dim\r\nbutterfly/map/while/resize/ExpandDims\r\nbutterfly/map/while/resize/size\r\nbutterfly/map/while/resize/ResizeBilinear\r\nbutterfly/map/while/resize/Squeeze\r\nbutterfly/map/while/TensorArrayWrite/TensorArrayWriteV3/Enter\r\nbutterfly/map/while/TensorArrayWrite/TensorArrayWriteV3\r\nbutterfly/map/while/add_1/y\r\nbutterfly/map/while/add_1\r\nbutterfly/map/while/NextIteration\r\nbutterfly/map/while/NextIteration_1\r\nbutterfly/map/while/NextIteration_2\r\nbutterfly/map/while/Exit_2\r\nbutterfly/map/TensorArrayStack/TensorArraySizeV3\r\nbutterfly/map/TensorArrayStack/range/start\r\nbutterfly/map/TensorArrayStack/range/delta\r\nbutterfly/map/TensorArrayStack/range\r\nbutterfly/map/TensorArrayStack/TensorArrayGatherV3**\r\nbutterfly/layer1/conv2d/kernel\r\nbutterfly/layer1/conv2d/kernel/read\r\nbutterfly/layer1/conv2d/bias\r\nbutterfly/layer1/conv2d/bias/read\r\nbutterfly/layer1/conv2d/Conv2D\r\nbutterfly/layer1/conv2d/BiasAdd\r\nbutterfly/layer1/conv2d/Relu\r\nbutterfly/layer1/max_pooling2d/MaxPool\r\nbutterfly/layer2/conv2d/kernel\r\nbutterfly/layer2/conv2d/kernel/read\r\nbutterfly/layer2/conv2d/bias\r\nbutterfly/layer2/conv2d/bias/read\r\nbutterfly/layer2/conv2d/Conv2D\r\nbutterfly/layer2/conv2d/BiasAdd\r\nbutterfly/layer2/conv2d/Relu\r\nbutterfly/layer2/conv2d_1/kernel\r\nbutterfly/layer2/conv2d_1/kernel/read\r\nbutterfly/layer2/conv2d_1/bias\r\nbutterfly/layer2/conv2d_1/bias/read\r\nbutterfly/layer2/conv2d_1/Conv2D\r\nbutterfly/layer2/conv2d_1/BiasAdd\r\nbutterfly/layer2/conv2d_1/Relu\r\nbutterfly/layer2/max_pooling2d/MaxPool\r\nbutterfly/layer3/conv2d/kernel\r\nbutterfly/layer3/conv2d/kernel/read\r\nbutterfly/layer3/conv2d/bias\r\nbutterfly/layer3/conv2d/bias/read\r\nbutterfly/layer3/conv2d/Conv2D\r\nbutterfly/layer3/conv2d/BiasAdd\r\nbutterfly/layer3/conv2d/Relu\r\nbutterfly/layer3/conv2d_1/kernel\r\nbutterfly/layer3/conv2d_1/kernel/read\r\nbutterfly/layer3/conv2d_1/bias\r\nbutterfly/layer3/conv2d_1/bias/read\r\nbutterfly/layer3/conv2d_1/Conv2D\r\nbutterfly/layer3/conv2d_1/BiasAdd\r\nbutterfly/layer3/conv2d_1/Relu\r\nbutterfly/layer3/max_pooling2d/MaxPool\r\nbutterfly/layer4/conv2d/kernel\r\nbutterfly/layer4/conv2d/kernel/read\r\nbutterfly/layer4/conv2d/bias\r\nbutterfly/layer4/conv2d/bias/read\r\nbutterfly/layer4/conv2d/Conv2D\r\nbutterfly/layer4/conv2d/BiasAdd\r\nbutterfly/layer4/conv2d/Relu\r\nbutterfly/layer4/conv2d_1/kernel\r\nbutterfly/layer4/conv2d_1/kernel/read\r\nbutterfly/layer4/conv2d_1/bias\r\nbutterfly/layer4/conv2d_1/bias/read\r\nbutterfly/layer4/conv2d_1/Conv2D\r\nbutterfly/layer4/conv2d_1/BiasAdd\r\nbutterfly/layer4/conv2d_1/Relu\r\nbutterfly/layer4/max_pooling2d/MaxPool\r\nbutterfly/layer5/conv2d/kernel\r\nbutterfly/layer5/conv2d/kernel/read\r\nbutterfly/layer5/conv2d/bias\r\nbutterfly/layer5/conv2d/bias/read\r\nbutterfly/layer5/conv2d/Conv2D\r\nbutterfly/layer5/conv2d/BiasAdd\r\nbutterfly/layer5/conv2d/Relu\r\nbutterfly/layer6/flatten/Shape\r\nbutterfly/layer6/flatten/strided_slice/stack\r\nbutterfly/layer6/flatten/strided_slice/stack_1\r\nbutterfly/layer6/flatten/strided_slice/stack_2\r\nbutterfly/layer6/flatten/strided_slice\r\nbutterfly/layer6/flatten/Reshape/shape/1\r\nbutterfly/layer6/flatten/Reshape/shape\r\nbutterfly/layer6/flatten/Reshape\r\nbutterfly/layer6/dense/kernel\r\nbutterfly/layer6/dense/kernel/read\r\nbutterfly/layer6/dense/bias\r\nbutterfly/layer6/dense/bias/read\r\nbutterfly/layer6/dense/MatMul\r\nbutterfly/layer6/dense/BiasAdd\r\nbutterfly/layer6/dense/Relu\r\nbutterfly/layer6/logits/kernel\r\nbutterfly/layer6/logits/kernel/read\r\nbutterfly/layer6/logits/bias\r\nbutterfly/layer6/logits/bias/read\r\nbutterfly/layer6/logits/MatMul\r\nbutterfly/layer6/logits/BiasAdd\r\nbutterfly/layer6/final_dense", "I have trained model with TF2.2.0, it shows different training nodes. How to remove these \"/map/* \" training nodes?\r\n\r\npython -m tensorflow.python.tools.freeze_graph --input_saved_model_dir saved_model/1591713086/ --output_node_names layer6/final_dense     --output_graph frozen_graph_t2.pb\r\n\r\nbutterfly/image_tensor \r\n**butterfly/map/Shape\r\nbutterfly/map/strided_slice/stack\r\nbutterfly/map/strided_slice/stack_1\r\nbutterfly/map/strided_slice/stack_2\r\nbutterfly/map/strided_slice\r\nbutterfly/map/TensorArrayUnstack/TensorListFromTensor/element_shape\r\nbutterfly/map/TensorArrayUnstack/TensorListFromTensor\r\nbutterfly/map/Const\r\nbutterfly/map/TensorArrayV2_1/element_shape\r\nbutterfly/map/TensorArrayV2_1\r\nbutterfly/map/while/loop_counter\r\nbutterfly/map/while\r\nbutterfly/map/while/Identity_3\r\nbutterfly/map/StopGradient_1\r\nbutterfly/map/TensorArrayV2Stack/TensorListStack/element_shape\r\nbutterfly/map/TensorArrayV2Stack/TensorListStack**\r\nbutterfly/layer1/conv2d/kernel\r\nbutterfly/layer1/conv2d/bias\r\nbutterfly/layer1/conv2d/Conv2D/ReadVariableOp\r\nbutterfly/layer1/conv2d/Conv2D\r\nbutterfly/layer1/conv2d/BiasAdd/ReadVariableOp\r\nbutterfly/layer1/conv2d/BiasAdd\r\nbutterfly/layer1/conv2d/Relu\r\nbutterfly/layer1/max_pooling2d/MaxPool\r\nbutterfly/layer2/conv2d/kernel\r\nbutterfly/layer2/conv2d/bias\r\nbutterfly/layer2/conv2d/Conv2D/ReadVariableOp\r\nbutterfly/layer2/conv2d/Conv2D\r\nbutterfly/layer2/conv2d/BiasAdd/ReadVariableOp\r\nbutterfly/layer2/conv2d/BiasAdd\r\nbutterfly/layer2/conv2d/Relu\r\nbutterfly/layer2/conv2d_1/kernel\r\nbutterfly/layer2/conv2d_1/bias\r\nbutterfly/layer2/conv2d_1/Conv2D/ReadVariableOp\r\nbutterfly/layer2/conv2d_1/Conv2D\r\nbutterfly/layer2/conv2d_1/BiasAdd/ReadVariableOp\r\nbutterfly/layer2/conv2d_1/BiasAdd\r\nbutterfly/layer2/conv2d_1/Relu\r\nbutterfly/layer2/max_pooling2d/MaxPool\r\nbutterfly/layer3/conv2d/kernel\r\nbutterfly/layer3/conv2d/bias\r\nbutterfly/layer3/conv2d/Conv2D/ReadVariableOp\r\nbutterfly/layer3/conv2d/Conv2D\r\nbutterfly/layer3/conv2d/BiasAdd/ReadVariableOp\r\nbutterfly/layer3/conv2d/BiasAdd\r\nbutterfly/layer3/conv2d/Relu\r\nbutterfly/layer3/conv2d_1/kernel\r\nbutterfly/layer3/conv2d_1/bias\r\nbutterfly/layer3/conv2d_1/Conv2D/ReadVariableOp\r\nbutterfly/layer3/conv2d_1/Conv2D\r\nbutterfly/layer3/conv2d_1/BiasAdd/ReadVariableOp\r\nbutterfly/layer3/conv2d_1/BiasAdd\r\nbutterfly/layer3/conv2d_1/Relu\r\nbutterfly/layer3/max_pooling2d/MaxPool\r\nbutterfly/layer4/conv2d/kernel\r\nbutterfly/layer4/conv2d/bias\r\nbutterfly/layer4/conv2d/Conv2D/ReadVariableOp\r\nbutterfly/layer4/conv2d/Conv2D\r\nbutterfly/layer4/conv2d/BiasAdd/ReadVariableOp\r\nbutterfly/layer4/conv2d/BiasAdd\r\nbutterfly/layer4/conv2d/Relu\r\nbutterfly/layer4/conv2d_1/kernel\r\nbutterfly/layer4/conv2d_1/bias\r\nbutterfly/layer4/conv2d_1/Conv2D/ReadVariableOp\r\nbutterfly/layer4/conv2d_1/Conv2D\r\nbutterfly/layer4/conv2d_1/BiasAdd/ReadVariableOp\r\nbutterfly/layer4/conv2d_1/BiasAdd\r\nbutterfly/layer4/conv2d_1/Relu\r\nbutterfly/layer4/max_pooling2d/MaxPool\r\nbutterfly/layer5/conv2d/kernel\r\nbutterfly/layer5/conv2d/bias\r\nbutterfly/layer5/conv2d/Conv2D/ReadVariableOp\r\nbutterfly/layer5/conv2d/Conv2D\r\nbutterfly/layer5/conv2d/BiasAdd/ReadVariableOp\r\nbutterfly/layer5/conv2d/BiasAdd\r\nbutterfly/layer5/conv2d/Relu\r\nbutterfly/layer6/flatten/Const\r\nbutterfly/layer6/flatten/Reshape\r\nbutterfly/layer6/dense/kernel\r\nbutterfly/layer6/dense/bias\r\nbutterfly/layer6/dense/MatMul/ReadVariableOp\r\nbutterfly/layer6/dense/MatMul\r\nbutterfly/layer6/dense/BiasAdd/ReadVariableOp\r\nbutterfly/layer6/dense/BiasAdd\r\nbutterfly/layer6/dense/Relu\r\nbutterfly/layer6/logits/kernel\r\nbutterfly/layer6/logits/bias\r\nbutterfly/layer6/logits/MatMul/ReadVariableOp\r\nbutterfly/layer6/logits/MatMul\r\nbutterfly/layer6/logits/BiasAdd/ReadVariableOp\r\nbutterfly/layer6/logits/BiasAdd\r\nbutterfly/layer6/final_dense\r\n\r\n**frozen_graph_t2.pb:**\r\ndrive.google.com/file/d/1DPyry67p6VGOTR6qTfCDT5Kr_QsuLRa_/view?usp=sharing\r\n\r\n**saved_model:**\r\ndrive.google.com/file/d/17LbKTz6yHcZU-PIGcoqchxFZne97FHeo/view?usp=sharing\r\n\r\nI am using code in this notebook to optimize but it was valid for only TF1.15. \r\nFor TF2.2 it says no graph_transfroms.\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-2-f1515cd131de> in <module>\r\n     11 from tensorflow.python.tools import freeze_graph\r\n     12 from tensorflow.python import ops\r\n---> 13 from tensorflow.tools.graph_transforms import TransformGraph\r\n     15 def describe_graph(graph_def, show_nodes=False):\r\nModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'\r\n \r\nTransformGraph notebook:\r\nhttps://drive.google.com/file/d/1ePqNPHjcbf5IhrLAK1yRpHM9IOnhk7aG/view?usp=sharing\r\n\r\n"]}, {"number": 40203, "title": "Provide compatibility with previous Keras-Preprocessing API.", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\nIn response to https://github.com/keras-team/keras-preprocessing/issues/299\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, matching image datasets iterations with real pathnames is not as trivial as it was in keras-preprocessing. It is also not explained clearly in the documentation.\r\n\r\n```python\r\n# Keras preprocessing\r\n\r\nflow = ImageDataGenerator.flow_from_directory('a_path', ...)\r\nfilenames = flow.filenames\r\n\r\n# TF Dataset \r\n\r\ndataset = `image_dataset_from_directory('a_path', ...)\r\nimage_paths, labels, class_names = keras.preprocessing.dataset_utils.index_directory('a_path')\r\n```\r\n\r\n**Suggestion**\r\nInclude a Dataset that will return filenames as an attribute of the returned Dataset.\r\n\r\nThis dataset would also be shuffled, batched so that if we zip both datasets, we can easily track what is the filename currently being used. This is useful when debugging models, tracking performance, etc.\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo, we add an attribute to the returned Dataset.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEvery user using `flow_from_directory(...).filenames`.\r\n\r\n**Any Other info.**\r\n\r\nI can submit a PR, but I wanted to know if this is a good fit or should we suggest users use the example above.\r\n", "comments": ["Thank you @Dref360. I think my intention has been included in your submission clearly and precisely, so nothing to add. I personally would vote for the solution just proposed instead of the example above. ", "Hi, I am interested in this and wondering if there is a way I could contribute to this?  It seems like a simple solution could be add an optional parameter to the image_dataset_from_directory function for those users that want the image paths, such as: \r\n\r\n``` python\r\ntf.keras.preprocessing.image_dataset_from_directory(\r\n    directory,\r\n    labels=\"inferred\",\r\n    label_mode=\"int\",\r\n    class_names=None,\r\n    color_mode=\"rgb\",\r\n    batch_size=32,\r\n    image_size=(256, 256),\r\n    shuffle=True,\r\n    seed=None,\r\n    validation_split=None,\r\n    subset=None,\r\n    interpolation=\"bilinear\",\r\n    follow_links=False,\r\n    ## new parameter ##\r\n    img_paths=False\r\n)\r\n```\r\n\r\nThen at the end of the function, just:\r\n\r\n``` python\r\nif img_paths:\r\n   return dataset, image_paths\r\n```\r\nThis shouldn't break anything else.  Alternatively, I like the dataset attribute approach as well (it seems a bit more OOP but also maybe not applicable to datasets that are not image datasets?).  Is anyone working on either solution?  I would be happy to help contribute to it.", "Yep, this should be fairly simple, contributions welcome!", "I can submit a pull request, wasn't sure exactly how to document it, would that be better to discuss in the pull request?  Otherwise, what about adding this to the docstring:\r\n\r\n`img_paths: Whether to return the filepaths of the images read from the dir.\r\n`\r\n\r\n```\r\nIf img_paths=True, it also returns a list of filepaths for each image in\r\n    the dataset.\r\n```", "> I can submit a pull request, wasn't sure exactly how to document it, would that be better to discuss in the pull request? Otherwise, what about adding this to the docstring:\r\n> \r\n> `img_paths: Whether to return the filepaths of the images read from the dir. `\r\n> \r\n> ```\r\n> If img_paths=True, it also returns a list of filepaths for each image in\r\n>     the dataset.\r\n> ```\r\n\r\nI think you can turn on by default, without any API change? This doesn't seem like a backward-incompatible change to me.", "> > I can submit a pull request, wasn't sure exactly how to document it, would that be better to discuss in the pull request? Otherwise, what about adding this to the docstring:\r\n> > `img_paths: Whether to return the filepaths of the images read from the dir. `\r\n> > ```\r\n> > If img_paths=True, it also returns a list of filepaths for each image in\r\n> >     the dataset.\r\n> > ```\r\n> \r\n> I think you can turn on by default, without any API change? This doesn't seem like a backward-incompatible change to me.\r\n\r\nIf people have scripts setup only expecting a dataset to be returned it could break things, no?  It would put the dataset and image paths into a tuple.", "> > > I can submit a pull request, wasn't sure exactly how to document it, would that be better to discuss in the pull request? Otherwise, what about adding this to the docstring:\r\n> > > `img_paths: Whether to return the filepaths of the images read from the dir. `\r\n> > > ```\r\n> > > If img_paths=True, it also returns a list of filepaths for each image in\r\n> > >     the dataset.\r\n> > > ```\r\n> > \r\n> > \r\n> > I think you can turn on by default, without any API change? This doesn't seem like a backward-incompatible change to me.\r\n> \r\n> If people have scripts setup only expecting a dataset to be returned it could break things, no? It would put the dataset and image paths into a tuple.\r\n\r\nYou just need to add a `dataset.file_names = ...`, right? ", "Currently it just creates a Dataset not TFRecordDataset so it doesn't have a file_names attribute, but could change that.. maybe more involved.  Would it make sense to do that inside this paths_and_labels_to_dataset? \r\n\r\n``` python\r\ndef paths_and_labels_to_dataset(image_paths,\r\n                                image_size,\r\n                                num_channels,\r\n                                labels,\r\n                                label_mode,\r\n                                num_classes,\r\n                                interpolation):\r\n  \"\"\"Constructs a dataset of images and labels.\"\"\"\r\n  # TODO(fchollet): consider making num_parallel_calls settable\r\n  path_ds = dataset_ops.Dataset.from_tensor_slices(image_paths)\r\n  img_ds = path_ds.map(\r\n      lambda x: path_to_image(x, image_size, num_channels, interpolation))\r\n  if label_mode:\r\n    label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\r\n    img_ds = dataset_ops.Dataset.zip((img_ds, label_ds))\r\n  return img_ds\r\n````", "> Currently it just creates a Dataset not TFRecordDataset so it doesn't have a file_names attribute, but could change that.. maybe more involved. Would it make sense to do that inside this paths_and_labels_to_dataset?\r\n> \r\n> ```python\r\n> def paths_and_labels_to_dataset(image_paths,\r\n>                                 image_size,\r\n>                                 num_channels,\r\n>                                 labels,\r\n>                                 label_mode,\r\n>                                 num_classes,\r\n>                                 interpolation):\r\n>   \"\"\"Constructs a dataset of images and labels.\"\"\"\r\n>   # TODO(fchollet): consider making num_parallel_calls settable\r\n>   path_ds = dataset_ops.Dataset.from_tensor_slices(image_paths)\r\n>   img_ds = path_ds.map(\r\n>       lambda x: path_to_image(x, image_size, num_channels, interpolation))\r\n>   if label_mode:\r\n>     label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\r\n>     img_ds = dataset_ops.Dataset.zip((img_ds, label_ds))\r\n>   return img_ds\r\n> ```\r\n\r\nYes the assumption here is that once you get the Dataset object, you can retrieve `file_names` from it otherwise the information will get lost after map or batch functions, or any conversion to TFRecordDataset in your case.", "Hmm, not exactly sure where to do that (I'm thinking this: `dataset_ops.Dataset.file_paths = image_paths`), and if it needs to be shuffled along with the batches?  Also, I notice that the way class_names is defined is just an instance attribute that then gets lost after batch functions etc.. so maybe whatever we do with file_names can also be done with class_names to be carried through the pipeline and accessible later?", "> Hmm, not exactly sure where to do that (I'm thinking this: `dataset_ops.Dataset.file_paths = image_paths`), and if it needs to be shuffled along with the batches? Also, I notice that the way class_names is defined is just an instance attribute that then gets lost after batch functions etc.. so maybe whatever we do with file_names can also be done with class_names to be carried through the pipeline and accessible later?\r\n\r\nYeah exactly what I meant above -- you can define them as instance attribute, which will get lost but the hope would be you retrieve this information before any further processing. That's probably the only way to work with `Dataset` today"]}, {"number": 40195, "title": "Allow overriding the build method for Metrics", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nMost `tf.keras.metrics.Metric` subclasses today are accumulated as scalars and hence can have their accumulation tensors statically initialized in `__init__` before the Metric is used. There are exceptions to this where tensor shape is dependent values only accessible during runtime. An example of an exception is the `MeanTensor` metric (https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/metrics.py#L2812-L2921), where the tensor shape is determined in a special `_build` method. That unfortunately uses protected parts of the API.\r\n\r\nThis was a problem for me when implementing/augmenting a custom RSquare metric for `tensorflow_addons` (https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/RSquare). When performing multivariate regression, it is necessary to calculate the R2 metric for each variable separately, and hence I would really like to do something similar to what is done in `MeanTensor`, i.e. setting the shape of the accumulation tensors based on the de facto dimensionality of model outputs/labels during runtime. I shied away from this, however, due to the use of protected API components. The consequence is that the user of the Metric needs to specify the shape of the labels up front, which is error prone and annoying. Discussing this with one of the maintainers of `tensorflow_addons`, it was mentioned that there are reasons to expect `Metric` to properly support a `build` method in the future (https://github.com/tensorflow/addons/pull/1310#issuecomment-599143485). This feature request is partly to communicate this need, but with a focus on a specific use case rather than a specific implementation. I don't know if this has been covered in other issues. Perhaps you know, @gabrieldemarmiesse?\r\n\r\n**Will this change the current api? How?**\r\nI don't want to recommend any implementation over others as long as the use case is catered to: Dynamic shape metrics.\r\n\r\n**Who will benefit with this feature?**\r\nDevelopers and users of custom `tf.keras.metrics.Metric` subclasses.\r\n\r\n**Any Other info.**\r\n", "comments": ["I did not encounter other issues, but since Metric is a subclass of layer, and layer has a \"build\" method, it makes sense to expose it since the use case is exactly the same for metrics and layers. It would lead to a cleaner API for metrics in general. ", "@harahu  the description of the issue is really good and I believe it's worth mentioning it in the next keras meeting, but the title of the issue can be misleading. Maybe \"Allow overriding the build method for Metrics\" would be more accurate?", "> @harahu the description of the issue is really good and I believe it's worth mentioning it in the next keras meeting, but the title of the issue can be misleading. Maybe \"Allow overriding the build method for Metrics\" would be more accurate?\r\n\r\nWas trying to focus more on the use case rather than the implementation, but considering this is the most likely approach to solving this, your suggestion works as a title. Done. I really appreciate if this is brought up for discussion.", "This was discussed in the last keras meeting, and we also talked about allowing `dynamic=True` for metrics. @fchollet agrees that both are good ideas and should be implemented (there is no reason why we shouldn't implement them, they're part of the Layer API). We're just lacking someone to implement it at this point. But a least it's just a question of time before we get `build` and `dynamic` in metrics :) ", "Glad to hear that! Let me know if I can be of assistance."]}, {"number": 40193, "title": "Would be better to replace the signature of ReadNBytes to not have `size_t` as the type of the last argument.", "body": "Would be better to replace the signature of ReadNBytes to not have `size_t` as the type of the last argument. Let's try to do this in another PR (as there would be multiple places to change)\r\n\r\n_Originally posted by @mihaimaruseac in https://github.com/tensorflow/tensorflow/pull/40133/files_", "comments": []}, {"number": 40183, "title": "Very slow quantized tflite model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntf_lite_model = converter.convert()\r\n\r\nwith open(\"model.tflite\", \"wb\") as f:\r\n    f.write(tf_lite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-06-05 10:53:29.063149: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-06-05 10:53:29.063233: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-05 10:53:29.080730: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-05 10:53:29.080748: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.006ms.\r\n2020-06-05 10:53:29.080752: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-06-05 10:53:32.284115: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-06-05 10:53:32.284242: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-06-05 10:53:33.407982: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-06-05 10:53:33.408011: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 1092 nodes (-568), 1139 edges (-568), time = 474.12ms.\r\n2020-06-05 10:53:33.408016: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 1092 nodes (0), 1139 edges (0), time = 213.886ms.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://drive.google.com/file/d/1imjVvw8IqQ6tvQRYaKJi_ynxQUHBBSH_/view?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nBefore conversion, running standard keras model on CPU took ~300ms per frame. After conversion it takes ~55s. \r\nEventually I want to deploy the model on Coral Dev Board. Currently after compiling it for edge TPU inference takes ~4s using Coral.\r\n\r\nIs it normal that it's so slow? I expect it to be at least not slower than before conversion. \r\n\r\n**Any other info / logs**\r\nLogs from edge tpu compiler:\r\n```\r\nEdge TPU Compiler version 2.1.302470888\r\nInput: model.tflite\r\nOutput: model_edgetpu.tflite\r\n\r\nOperator                       Count      Status\r\n\r\nADD                            1          More than one subgraph is not supported\r\nADD                            71         Mapped to Edge TPU\r\nMAX_POOL_2D                    1          Mapped to Edge TPU\r\nPAD                            35         Mapped to Edge TPU\r\nMUL                            35         Mapped to Edge TPU\r\nCONCATENATION                  1          More than one subgraph is not supported\r\nQUANTIZE                       1          Operation is otherwise supported, but not mapped due to some unspecified limitation\r\nQUANTIZE                       3          Mapped to Edge TPU\r\nCONV_2D                        115        Mapped to Edge TPU\r\nCONV_2D                        4          More than one subgraph is not supported\r\nDEQUANTIZE                     1          Operation is working on an unsupported data type\r\nRESIZE_BILINEAR                2          Operation is otherwise supported, but not mapped due to some unspecified limitation\r\nRESIZE_BILINEAR                6          Mapped to Edge TPU\r\nSOFTMAX                        1          Max 16000 elements supported\r\n\r\n```\r\n\r\n", "comments": ["I'm having the exact same issue: \r\nMobilenetV2 which I trained and quantized with the same settings as above, runs about 1.7FPS\r\nMobilenetV2 from here https://www.tensorflow.org/lite/guide/hosted_models, runs about 7FPS", "wonder what model is it?\r\n\r\ncan you use benchmark tool (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark)\r\n\r\nto get the detailed profiling?\r\n\r\nthanks", "It's semenatic segmentation FPN with ResNet101 backbone (trained using https://github.com/qubvel/segmentation_models). \r\nOutput from the benchmark tool:\r\n```\r\nSTARTING!\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [8]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [model.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nInput value ranges: []\r\nInput layer values files: []\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [0]\r\nMax profiling buffer entries: [1024]\r\nCSV File to export profiling data to: []\r\nMax number of delegated partitions : [0]\r\nUse gpu : [0]\r\nUse xnnpack : [0]\r\nLoaded model model.tflite\r\nThe input model file size (MB): 47.4346\r\nInitialized session in 48.019ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=1 curr=15908626\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=9 first=16856867 curr=16865810 min=16767506 max=17015249 avg=1.68505e+07 std=69772\r\n\r\nAverage inference timings in us: Warmup: 1.59086e+07, Init: 48019, Inference: 1.68505e+07\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=11.707 overall=181.352\r\n```", "Hi Chao, can you help verify if there's regression?\r\n\r\nthanks", "> It's semenatic segmentation FPN with ResNet101 backbone (trained using https://github.com/qubvel/segmentation_models).\r\n> Output from the benchmark tool:\r\n> \r\n> ```\r\n> STARTING!\r\n> Min num runs: [50]\r\n> Min runs duration (seconds): [1]\r\n> Max runs duration (seconds): [150]\r\n> Inter-run delay (seconds): [-1]\r\n> Num threads: [8]\r\n> Benchmark name: []\r\n> Output prefix: []\r\n> Min warmup runs: [1]\r\n> Min warmup runs duration (seconds): [0.5]\r\n> Graph: [model.tflite]\r\n> Input layers: []\r\n> Input shapes: []\r\n> Input value ranges: []\r\n> Input layer values files: []\r\n> Allow fp16 : [0]\r\n> Require full delegation : [0]\r\n> Enable op profiling: [0]\r\n> Max profiling buffer entries: [1024]\r\n> CSV File to export profiling data to: []\r\n> Max number of delegated partitions : [0]\r\n> Use gpu : [0]\r\n> Use xnnpack : [0]\r\n> Loaded model model.tflite\r\n> The input model file size (MB): 47.4346\r\n> Initialized session in 48.019ms.\r\n> Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n> count=1 curr=15908626\r\n> \r\n> Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n> count=9 first=16856867 curr=16865810 min=16767506 max=17015249 avg=1.68505e+07 std=69772\r\n> \r\n> Average inference timings in us: Warmup: 1.59086e+07, Init: 48019, Inference: 1.68505e+07\r\n> Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\n> Peak memory footprint (MB): init=11.707 overall=181.352\r\n> ```\r\n\r\nOn what HW architecture was this benchmarking performed? Coral board? And what's the compiling option did you use to compile the binary?", "On the PC with i7-8650U CPU. All options default, I followed instructions from readme file. ", "> On the PC with i7-8650U CPU. All options default, I followed instructions from readme file.\r\n\r\nI see. I added T.J who could give more insights here. He is much more familiar with the x86_64 optimization in the underlying math library RUY fo quant models in TFLite.\r\n\r\nBtw, if you are ok w/ float models on x86, then you could try the new xnnpack delegate (i.e. see https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack#enable-xnnpack-via-bazel-build-flags-recommended) that will deliver much better performance on x86.\r\n\r\nAs for perf. on Coral dev board (esp. w/ the edgetpu inside the board), could you report this issue to the Coral repo(i.e. https://github.com/google-coral/edgetpu/issues)?", "Here is a detailed report of my case, please note that I'm benchmarking the cpu-tflite models, the edge compiler output is only added for additional information.\r\n\r\nCPU: Threadripper 1920x\r\n\r\n###My Mobilenetv2:\r\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\r\n                                              include_top=False, \r\n                                              weights='imagenet')\r\nbase_model.trainable = True\r\nmodel = tf.keras.Sequential([\r\n  base_model,\r\n  tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.GlobalAveragePooling2D(),\r\n  tf.keras.layers.Dense(units=2, activation='softmax')\r\n])\r\nmodel.build([None, 96, 512, 3])\r\n\r\n...\r\n#custom tf traning loop with gradienttape\r\n...\r\nmodel.save(f\"./models/model_{epoch}.hdf5\")\r\n##\r\n\r\n\r\nimport tensorflow.compat.v1 as tf\r\ndef representative_data_gen():\r\n    dataset_list = tf.data.Dataset.list_files(df.Fn.values)\r\n    for i in range(df.shape[0]):\r\n        image = next(iter(dataset_list))\r\n        image = tf.io.read_file(image)\r\n        image = tf.io.decode_jpeg(image, channels=3)\r\n        image = tf.cast(image, tf.float32) / 255.0\r\n        image = tf.expand_dims(image, 0)\r\n        yield [image]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(chkp)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_data_gen\r\ntflite_model = converter.convert()\r\n\r\nwith open('mobilenet_v2_1.0_224_quant.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n\r\n\r\n\r\nkriszfekete@datascience01:~/jupyter/tensorflow$ bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model   --graph=mobilenet_v2_1.0_224_quant.tflite   --num_threads=1\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nUse caching: [0]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [mobilenet_v2_1.0_224_quant.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nInput value ranges: []\r\nInput layer values files: []\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [0]\r\nMax profiling buffer entries: [1024]\r\nCSV File to export profiling data to: []\r\nEnable platform-wide tracing: [0]\r\n#threads used for CPU inference: [1]\r\nMax number of delegated partitions : [0]\r\nMin nodes per partition : [0]\r\nExternal delegate path : []\r\nExternal delegate options : []\r\nUse gpu : [0]\r\nUse xnnpack : [0]\r\nLoaded model mobilenet_v2_1.0_224_quant.tflite\r\nThe input model file size (MB): 3.22496\r\nInitialized session in 1.063ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=1 curr=630344\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=621263 curr=631745 min=603622 max=655479 avg=623294 std=14346\r\n\r\nInference timings in us: Init: 1063, First inference: 630344, Warmup (avg): 630344, Inference (avg): 623294\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=2.38672 overall=5.89453\r\n\r\n----------------------------------------------------------------------------------------------------------------------------------------------\r\n----------------------------------------------------------------------------------------------------------------------------------------------\r\n#Pretrained net from: https://www.tensorflow.org/lite/guide/hosted_models, Mobilenet_V2_1.0_224_quant\r\nbazel-bin/tensorflow/lite/tools/benchmark/benchmark_model \\\r\n>   --graph=hub_mobilenet_v2_1.0_224_quant.tflite \\\r\n>   --num_threads=1\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nMin num runs: [50]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nUse caching: [0]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [hub_mobilenet_v2_1.0_224_quant.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nInput value ranges: []\r\nInput layer values files: []\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [0]\r\nMax profiling buffer entries: [1024]\r\nCSV File to export profiling data to: []\r\nEnable platform-wide tracing: [0]\r\n#threads used for CPU inference: [1]\r\nMax number of delegated partitions : [0]\r\nMin nodes per partition : [0]\r\nExternal delegate path : []\r\nExternal delegate options : []\r\nUse gpu : [0]\r\nUse xnnpack : [0]\r\nLoaded model hub_mobilenet_v2_1.0_224_quant.tflite\r\nThe input model file size (MB): 3.57776\r\nInitialized session in 0.772ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=4 first=166513 curr=159202 min=156239 max=166513 avg=159732 std=4064\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=158890 curr=162784 min=156544 max=167988 avg=161145 std=1948\r\n\r\nInference timings in us: Init: 772, First inference: 166513, Warmup (avg): 159732, Inference (avg): 161145\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=1.55859 overall=9.98047\r\n\r\n\r\n#######################################################\r\nEdge-TPU compiler outputs:\r\nMy model:\r\nedgetpu_compiler mobilenet_v2_1.0_224_quant.tflite -s\r\nEdge TPU Compiler version 2.1.302470888\r\n\r\nModel compiled successfully in 478 ms.\r\n\r\nInput model: mobilenet_v2_1.0_224_quant.tflite\r\nInput size: 3.08MiB\r\nOutput model: mobilenet_v2_1.0_224_quant_edgetpu.tflite\r\nOutput size: 3.07MiB\r\nOn-chip memory used for caching model parameters: 3.33MiB\r\nOn-chip memory remaining for caching model parameters: 4.39MiB\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 1\r\nTotal number of operations: 73\r\nOperation log: mobilenet_v2_1.0_224_quant_edgetpu.log\r\n\r\nOperator                       Count      Status\r\n\r\nPAD                            5          Mapped to Edge TPU\r\nQUANTIZE                       2          Mapped to Edge TPU\r\nCONV_2D                        36         Mapped to Edge TPU\r\nDEPTHWISE_CONV_2D              17         Mapped to Edge TPU\r\nMEAN                           1          Mapped to Edge TPU\r\nSOFTMAX                        1          Mapped to Edge TPU\r\nFULLY_CONNECTED                1          Mapped to Edge TPU\r\nADD                            10         Mapped to Edge TPU\r\n____________________________________________________________\r\nMobileNet_V2 from hosted models:\r\nedgetpu_compiler hub_mobilenet_v2_1.0_224_quant.tflite -s \r\nEdge TPU Compiler version 2.1.302470888\r\n\r\nModel compiled successfully in 378 ms.\r\n\r\nInput model: hub_mobilenet_v2_1.0_224_quant.tflite\r\nInput size: 3.41MiB\r\nOutput model: hub_mobilenet_v2_1.0_224_quant_edgetpu.tflite\r\nOutput size: 3.88MiB\r\nOn-chip memory used for caching model parameters: 3.75MiB\r\nOn-chip memory remaining for caching model parameters: 3.16MiB\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 1\r\nTotal number of operations: 65\r\nOperation log: hub_mobilenet_v2_1.0_224_quant_edgetpu.log\r\n\r\nOperator                       Count      Status\r\n\r\nCONV_2D                        36         Mapped to Edge TPU\r\nDEPTHWISE_CONV_2D              17         Mapped to Edge TPU\r\nRESHAPE                        1          Mapped to Edge TPU\r\nADD                            10         Mapped to Edge TPU\r\nAVERAGE_POOL_2D                1          Mapped to Edge TPU\r\n\r\n\r\nUPDATE: My coral edgetpu (usb stick) arrived, this is even weirder, my model which ran about 5* as slow (on CPU) as the one from the hosted models, is actually faster on edgetpu.\r\n\r\nMy model had a latency about 3.7-3.8 ms\r\nThe hosted model: 3.8-3.9 ms\r\n", "> Here is a detailed report of my case, please note that I'm benchmarking the cpu-tflite models, the edge compiler output is only added for additional information.\r\n> \r\n> CPU: Threadripper 1920x\r\n> \r\n> ###My Mobilenetv2:\r\n> base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\r\n> include_top=False,\r\n> weights='imagenet')\r\n> base_model.trainable = True\r\n> model = tf.keras.Sequential([\r\n> base_model,\r\n> tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\r\n> tf.keras.layers.Dropout(0.2),\r\n> tf.keras.layers.GlobalAveragePooling2D(),\r\n> tf.keras.layers.Dense(units=2, activation='softmax')\r\n> ])\r\n> model.build([None, 96, 512, 3])\r\n> \r\n> ...\r\n> #custom tf traning loop with gradienttape\r\n> ...\r\n> model.save(f\"./models/model_{epoch}.hdf5\")\r\n> \r\n> import tensorflow.compat.v1 as tf\r\n> def representative_data_gen():\r\n> dataset_list = tf.data.Dataset.list_files(df.Fn.values)\r\n> for i in range(df.shape[0]):\r\n> image = next(iter(dataset_list))\r\n> image = tf.io.read_file(image)\r\n> image = tf.io.decode_jpeg(image, channels=3)\r\n> image = tf.cast(image, tf.float32) / 255.0\r\n> image = tf.expand_dims(image, 0)\r\n> yield [image]\r\n> \r\n> converter = tf.lite.TFLiteConverter.from_keras_model_file(chkp)\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n> converter.inference_input_type = tf.uint8\r\n> converter.inference_output_type = tf.uint8\r\n> converter.representative_dataset = representative_data_gen\r\n> tflite_model = converter.convert()\r\n> \r\n> with open('mobilenet_v2_1.0_224_quant.tflite', 'wb') as f:\r\n> f.write(tflite_model)\r\n> \r\n> kriszfekete@datascience01:~/jupyter/tensorflow$ bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=mobilenet_v2_1.0_224_quant.tflite --num_threads=1\r\n> STARTING!\r\n> Duplicate flags: num_threads\r\n> Min num runs: [50]\r\n> Min runs duration (seconds): [1]\r\n> Max runs duration (seconds): [150]\r\n> Inter-run delay (seconds): [-1]\r\n> Num threads: [1]\r\n> Use caching: [0]\r\n> Benchmark name: []\r\n> Output prefix: []\r\n> Min warmup runs: [1]\r\n> Min warmup runs duration (seconds): [0.5]\r\n> Graph: [mobilenet_v2_1.0_224_quant.tflite]\r\n> Input layers: []\r\n> Input shapes: []\r\n> Input value ranges: []\r\n> Input layer values files: []\r\n> Allow fp16 : [0]\r\n> Require full delegation : [0]\r\n> Enable op profiling: [0]\r\n> Max profiling buffer entries: [1024]\r\n> CSV File to export profiling data to: []\r\n> Enable platform-wide tracing: [0]\r\n> #threads used for CPU inference: [1]\r\n> Max number of delegated partitions : [0]\r\n> Min nodes per partition : [0]\r\n> External delegate path : []\r\n> External delegate options : []\r\n> Use gpu : [0]\r\n> Use xnnpack : [0]\r\n> Loaded model mobilenet_v2_1.0_224_quant.tflite\r\n> The input model file size (MB): 3.22496\r\n> Initialized session in 1.063ms.\r\n> Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n> count=1 curr=630344\r\n> \r\n> Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n> count=50 first=621263 curr=631745 min=603622 max=655479 avg=623294 std=14346\r\n> \r\n> Inference timings in us: Init: 1063, First inference: 630344, Warmup (avg): 630344, Inference (avg): 623294\r\n> Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\n> Peak memory footprint (MB): init=2.38672 overall=5.89453\r\n> \r\n> #Pretrained net from: https://www.tensorflow.org/lite/guide/hosted_models, Mobilenet_V2_1.0_224_quant\r\n> bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model \\\r\n> \r\n> > --graph=hub_mobilenet_v2_1.0_224_quant.tflite \r\n> > --num_threads=1\r\n> > STARTING!\r\n> > Duplicate flags: num_threads\r\n> > Min num runs: [50]\r\n> > Min runs duration (seconds): [1]\r\n> > Max runs duration (seconds): [150]\r\n> > Inter-run delay (seconds): [-1]\r\n> > Num threads: [1]\r\n> > Use caching: [0]\r\n> > Benchmark name: []\r\n> > Output prefix: []\r\n> > Min warmup runs: [1]\r\n> > Min warmup runs duration (seconds): [0.5]\r\n> > Graph: [hub_mobilenet_v2_1.0_224_quant.tflite]\r\n> > Input layers: []\r\n> > Input shapes: []\r\n> > Input value ranges: []\r\n> > Input layer values files: []\r\n> > Allow fp16 : [0]\r\n> > Require full delegation : [0]\r\n> > Enable op profiling: [0]\r\n> > Max profiling buffer entries: [1024]\r\n> > CSV File to export profiling data to: []\r\n> > Enable platform-wide tracing: [0]\r\n> > #threads used for CPU inference: [1]\r\n> > Max number of delegated partitions : [0]\r\n> > Min nodes per partition : [0]\r\n> > External delegate path : []\r\n> > External delegate options : []\r\n> > Use gpu : [0]\r\n> > Use xnnpack : [0]\r\n> > Loaded model hub_mobilenet_v2_1.0_224_quant.tflite\r\n> > The input model file size (MB): 3.57776\r\n> > Initialized session in 0.772ms.\r\n> > Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\n> > count=4 first=166513 curr=159202 min=156239 max=166513 avg=159732 std=4064\r\n> \r\n> Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\n> count=50 first=158890 curr=162784 min=156544 max=167988 avg=161145 std=1948\r\n> \r\n> Inference timings in us: Init: 772, First inference: 166513, Warmup (avg): 159732, Inference (avg): 161145\r\n> Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\n> Peak memory footprint (MB): init=1.55859 overall=9.98047\r\n> \r\n> #######################################################\r\n> Edge-TPU compiler outputs:\r\n> My model:\r\n> edgetpu_compiler mobilenet_v2_1.0_224_quant.tflite -s\r\n> Edge TPU Compiler version 2.1.302470888\r\n> \r\n> Model compiled successfully in 478 ms.\r\n> \r\n> Input model: mobilenet_v2_1.0_224_quant.tflite\r\n> Input size: 3.08MiB\r\n> Output model: mobilenet_v2_1.0_224_quant_edgetpu.tflite\r\n> Output size: 3.07MiB\r\n> On-chip memory used for caching model parameters: 3.33MiB\r\n> On-chip memory remaining for caching model parameters: 4.39MiB\r\n> Off-chip memory used for streaming uncached model parameters: 0.00B\r\n> Number of Edge TPU subgraphs: 1\r\n> Total number of operations: 73\r\n> Operation log: mobilenet_v2_1.0_224_quant_edgetpu.log\r\n> \r\n> Operator Count Status\r\n> \r\n> PAD 5 Mapped to Edge TPU\r\n> QUANTIZE 2 Mapped to Edge TPU\r\n> CONV_2D 36 Mapped to Edge TPU\r\n> DEPTHWISE_CONV_2D 17 Mapped to Edge TPU\r\n> MEAN 1 Mapped to Edge TPU\r\n> SOFTMAX 1 Mapped to Edge TPU\r\n> FULLY_CONNECTED 1 Mapped to Edge TPU\r\n> ADD 10 Mapped to Edge TPU\r\n> \r\n> MobileNet_V2 from hosted models:\r\n> edgetpu_compiler hub_mobilenet_v2_1.0_224_quant.tflite -s\r\n> Edge TPU Compiler version 2.1.302470888\r\n> \r\n> Model compiled successfully in 378 ms.\r\n> \r\n> Input model: hub_mobilenet_v2_1.0_224_quant.tflite\r\n> Input size: 3.41MiB\r\n> Output model: hub_mobilenet_v2_1.0_224_quant_edgetpu.tflite\r\n> Output size: 3.88MiB\r\n> On-chip memory used for caching model parameters: 3.75MiB\r\n> On-chip memory remaining for caching model parameters: 3.16MiB\r\n> Off-chip memory used for streaming uncached model parameters: 0.00B\r\n> Number of Edge TPU subgraphs: 1\r\n> Total number of operations: 65\r\n> Operation log: hub_mobilenet_v2_1.0_224_quant_edgetpu.log\r\n> \r\n> Operator Count Status\r\n> \r\n> CONV_2D 36 Mapped to Edge TPU\r\n> DEPTHWISE_CONV_2D 17 Mapped to Edge TPU\r\n> RESHAPE 1 Mapped to Edge TPU\r\n> ADD 10 Mapped to Edge TPU\r\n> AVERAGE_POOL_2D 1 Mapped to Edge TPU\r\n> \r\n> UPDATE: My coral edgetpu (usb stick) arrived, this is even weirder, my model which ran about 5* as slow (on CPU) as the one from the hosted models, is actually faster on edgetpu.\r\nAcked. This is possible as it's possible that the quant execution path on x86-64 in TFLite hasn't been that well-optimized for that on ARM CPUs, edgetpu etc. \r\n> \r\n> My model had a latency about 3.7-3.8 ms\r\n> The hosted model: 3.8-3.9 ms\r\n\r\n", "Hi,\r\n\r\nOS Linux 18.04.04 LTS\r\nTensorFLow nightly 2.3.0-dev20200601\r\nCPU Intel Core i7-855OU \r\n\r\nI have a very similar issue. A trained a ResNet-50 V2 model and it takes ~ 40ms per inference on my CPU with efficiency 64.15 %. I then converted first to tf lite and speed was ~ 89 ms efficiency identical. Then with dynamic range quantization and it took ~ 548 ms per inference efficiency dropped to 54.35%. On Monday I converted it to full integer quantization with uint8 input/output and it takes ~ 7 seconds on cpu and ~40 ms on Edge TPU. Worth mentioning that besides ridiculously slow speed on CPU the full integer quantized model predicts the same value on CPU and Edge TPU all the time. Therefore, slow and broken. Does anyone have a clue how to solve this? Here you can find a link to a google folder with original mode, conversion code, converted model, and the code I use to test the tf lite model. Many thanks!\r\n\r\nhttps://drive.google.com/file/d/1hNc6xCLch1T9EEqahpiT6FzIDg3P423u/view?usp=sharing", "Hi all,\r\nThe model speeds up after deploying on the edgetpu (compiling from cpu tflite to edgetpu tflite) for both cases so it is the expect behavior. Since the compiler can only delegates from a fully quantized cpu tflite model, it can't do much about the original graph. It seems very odd to me that tflite model is performing much worse than the original graph model though.\r\n\r\nIt's also good to mention that I've also observed similar behavior (tiny bit difference) when testing out a yolov4 model (note that unfortunately, only 1/962 ops were mapped to edgetpu so we don't see much speed up here):\r\n\r\n```\r\nOn my x86_64 debian 10:\r\nOriginal model: ~55 seconds on CPU\r\n(non quantized) tflite modes: ~5 seconds\r\n(fully quantized) tflite model: ~56 seconds\r\n(edgetpu) tflite model: ~55 seconds\r\n```\r\nA quick look into the model with `netron`, I can see many quantized/dequantized ops that I suspect is what's causing the slowdown. Again, tflite models wasn't optimized for x86_64, so I suspect this the issue.\r\n\r\nNow let's check this again on my dev board, everything is as expected:\r\n```\r\nOn my dev board:\r\nOriginal model: Unfortunately cannot run this on the dev board.\r\n(non quantized) tflite modes: ~ 27 seconds\r\n(fully quantized) tflite model: ~ 13 seconds\r\n(edgetpu) tflite model: ~12 seconds\r\n```\r\n\r\nMy suggestion for all is to run the tflite model on arm platform since that's what tflite models are optimized for. If you are benchmarking a tflite model against a CPU graph model is not ideal. \r\n\r\nHope these finding is helpful!", "> Hi all,\r\n> The model speeds up after deploying on the edgetpu (compiling from cpu tflite to edgetpu tflite) for both cases so it is the expect behavior. Since the compiler can only delegates from a fully quantized cpu tflite model, it can't do much about the original graph. It seems very odd to me that tflite model is performing much worse than the original graph model though.\r\n> \r\n> It's also good to mention that I've also observed similar behavior (tiny bit difference) when testing out a yolov4 model (note that unfortunately, only 1/962 ops were mapped to edgetpu so we don't see much speed up here):\r\n> \r\n> ```\r\n> On my x86_64 debian 10:\r\n> Original model: ~55 seconds on CPU\r\n> (non quantized) tflite modes: ~5 seconds\r\n> (fully quantized) tflite model: ~56 seconds\r\n> (edgetpu) tflite model: ~55 seconds\r\n> ```\r\n> \r\n> A quick look into the model with `netron`, I can see many quantized/dequantized ops that I suspect is what's causing the slowdown. Again, tflite models wasn't optimized for x86_64, so I suspect this the issue.\r\n> \r\n> Now let's check this again on my dev board, everything is as expected:\r\n> \r\n> ```\r\n> On my dev board:\r\n> Original model: Unfortunately cannot run this on the dev board.\r\n> (non quantized) tflite modes: ~ 27 seconds\r\n> (fully quantized) tflite model: ~ 13 seconds\r\n> (edgetpu) tflite model: ~12 seconds\r\n> ```\r\n> \r\n> My suggestion for all is to run the tflite model on arm platform since that's what tflite models are optimized for. If you are benchmarking a tflite model against a CPU graph model is not ideal.\r\n> \r\n> Hope these finding is helpful!\r\n\r\nTFLite on x86 CPUs may not have been fully optimized for quantized models. But w/ XNNPACK delegate, as mentioned in https://github.com/tensorflow/tensorflow/issues/40183#issuecomment-640436345, will deliver significant x86 performance improvements for float models.", "It would be interesting to hear if performance is significantly different if you build with this flag,\r\n\r\n`bazel build -c opt --define=tflite_with_ruy=true`\r\n\r\nruy is not heavily optimized for x86 as it is for ARM, which is part of why it isn't the default yet, but it might already perform better than the default.\r\n\r\nHowever, ruy is only an implementation of matrix multiplication. If your model spends most of its time in other nodes, it will run into the fact that tflite's operators are implemented in NEON intrinsics, compiling on x86 thanks to a NEON->SSE intrinsics translation header. In other words, the compromise here has been minimal x86 implementation effort at the expense of x86 performance. It is to be expected that another inference engine with more first-class x86 implementation would outperform it, as mentioned in the previous comment.", "> TFLite on x86 CPUs may not have been fully optimized for quantized models. But w/ XNNPACK delegate, as mentioned in [#40183 (comment)](https://github.com/tensorflow/tensorflow/issues/40183#issuecomment-640436345), will deliver significant x86 performance improvements for float models.\r\n\r\nI understand that, I was suggesting that users deploy there tflite models to arm machines instead of comparing it to the graph on an x86.", "I've checked the same model conversion but using much smaller MobileNetV2 classification model from ```tf.keras.applications```. I can still observe increased processing time when running tflite model on x86 cpu(700ms using tflite vs 30ms using keras model), but as you said it's normal since it's optimized for ARM. After deploying to Coral Dev Board single frame processing time is ~7ms which is even faster then using CPU on my PC.\r\nHow can I check what makes my original segmentation model so slow? Is it simply too big or it contains some operations that cannot be mapped to Edge TPU? ", "TFLite has a couple of built-in profilers that are available wherever you can run tflite and look at terminal output. One is enabled by passing `--define=ruy_profiler=true` to `bazel build`, or equivalently in other buildsystems just add `-DRUY_PROFILER` to the compiler flags. If you build the `benchmark_model` binary with that, it will dump an ascii \"treeview\" in the terminal with % time spent in each node.  (Despite having \"ruy\" in the name, this profiler is available regardless of whether `tflite_with_ruy` is `true`).", "So can we conclude that tFLite post training optimizations are not for x86 cpu?\r\nIs there any inference optimization for x86 CPUs?", "Hi,\r\n\r\nApologies that this issue has gone stale. Some additional x86 optimizations have landed (for AVX, AVX2, and AVX512) and they will soon be the default on x86, but aren't yet. For this issue, it would be good to know if the poor performance persists for you on x86 CPU. Can you please do as follows:\r\n\r\n1. Please build with:\r\n\r\n```\r\nbazel build -c opt --define=tflite_with_ruy=true -copt=-DRUY_PROFILER\r\n```\r\n\r\n2. Please run the `benchmark_model` tool with `--enable_op_profiling=true`\r\n\r\nand then post the output to this issue. Also, please provide your exact build line for any executable you are running. Thanks!\r\n\r\n", "> Hi,\r\n> \r\n> Apologies that this issue has gone stale. Some additional x86 optimizations have landed (for AVX, AVX2, and AVX512) and they will soon be the default on x86, but aren't yet. For this issue, it would be good to know if the poor performance persists for you on x86 CPU. Can you please do as follows:\r\n> \r\n> 1. Please build with:\r\n> \r\n> ```\r\n> bazel build -c opt --define=tflite_with_ruy=true -copt=-DRUY_PROFILER\r\n> ```\r\n> \r\n> 1. Please run the `benchmark_model` tool with `--enable_op_profiling=true`\r\n> \r\n> and then post the output to this issue. Also, please provide your exact build line for any executable you are running. Thanks!\r\n\r\nany plan on make this default?"]}, {"number": 40166, "title": "Make possible loading model with custom gradient", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tf2.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently when a saved model is loaded, a custom gradient is not loaded.\r\nWarnings like \r\n\"WARNING: Importing a function _function-name_ with ops with custom gradients. Will likely fail if a gradient is requested.\" \r\nare shown and the model is failing during training with error \r\n\"LookupError: No gradient defined for operation '_namespace_/IdentityN' (op type: IdentityN)\"\r\n\r\nLink to TODO in code:\r\nhttps://github.com/tensorflow/tensorflow/blob/b794497e61fdb976448c562e9b87b09e24cbfabf/tensorflow/python/saved_model/function_deserialization.py#L351\r\nIt would be beneficial to have it working.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?*\r\nPeople using custom gradient\r\n\r\n**Any Other info.**\r\n", "comments": ["@Saduf2019 @rmothukuru @jvishnuvardhan is there any progress on this issue?\r\n\r\nWe are using custom gradients in many places in https://github.com/larq/larq to support training of quantized neural networks which currently prevents us from switching to `SavedModel` as a canonical serialization format for our models.\r\n\r\nI am unfamiliar with this part of the codebase but I would be happy to send PRs to fix this if you have a rough idea of how this could be fixed.", "Just a note, the implementation from: https://github.com/qubvel/efficientnet seems not to have this error, so switching to that implementation may be a possible hacky workaround for now. The API is practically identical.", "@Lewington-pitsos\r\nI trained https://github.com/qubvel/efficientnet\r\n, and I found loss is 100x bigger than tensorflow's efficientnet. It's not a good alternative.", "+1 Isn't keras efficientnet one of the most commonly used models? Seems like a big hole in the SavedModel format", "Just stumbled upon this issue.\r\nI am using a function which absolutely needs a custom gradient ([Non-uniform fourier transform](https://github.com/zaccharieramzi/tfkbnufft)), and I also absolutely need to save the model with the `SaveModel` format because I want to keep the optimizer's state (I am training in multiple sessions in an HPC setup, so I have to checkpoint).\r\n\r\nI am considering using [this SO answer](https://stackoverflow.com/a/49504376/4332585) to manually save the optimizer weights. Has anyone tried this for tf2.x?\r\n\r\nIt would however be much easier with the `SaveModel` format out-of-the-box. Do you have any updates on this @k-w-w ?\r\n\r\nEDIT\r\n-----\r\n\r\nI have used the solution provided in the SO answer, and it worked like a charm for me.\r\nYou can see it used [here in my code.](https://github.com/zaccharieramzi/fastmri-reproducible-benchmark/blob/master/fastmri_recon/training_scripts/nc_train.py#L152-L178)\r\n\r\nEDIT n2\r\n-------\r\n\r\nThe solution provided in the SO answer was failing for distributed training, so I had to resort to the following tweak to get it working, see [here](https://stackoverflow.com/a/66261284/4332585). An example in code is the one [here](https://github.com/zaccharieramzi/fastmri-reproducible-benchmark/blob/master/fastmri_recon/training_scripts/xpdnet_train.py#L355-L384).", "What's the status on this please? I've run several quite expensive computations and all of them ended up like this which is annoying.", "+1 It's making experiments quite hard as I now have to save all arguments to a file and reload them to rebuild the model from scratch, because I need the gradients from my decode function.", "I'm having the same issue when I load a model that uses a Tensorflow Probability layer (a custom IndependentGamma layer, identical class to IndependentNormal but with Gamma distribution). \r\n\r\n`WARNING:tensorflow:Importing a function (__inference_random_gamma_3606) with ops with custom gradients. Will likely fail if a gradient is requested.`\r\n\r\nI'm using TF2.3 and TFP0.11", "Any news on this? I have exported an efficientnet from keras (.h5) to tensorflow (.pb) format for **inference only**.  It does work, but I get tons of those annoying \"_with ops with custom gradients. Will likely fail if a gradient is requested._\" warnings. If I'm not mistaken gradients are only required for training, so perhaps there's a way to load the model for inference model so they are ignored?", "Any update on this @k-w-w? \r\nI am running into the same issue with tensorflow 2.4.1. \r\n@zaccharieramzi 's solution works nicely for saving optimizer state during training. \r\nBut as @4sfaloth mentions, when using the saved_model format for storing inference models, model loading causes many unnecessary warnings.", "This is really a major problem with the tf SavedModel format as you cannot even continue training a saved EfficientNet model currently.\r\n\r\nMinimal example:\r\n\r\n```\r\nimport tensorflow as tf\r\nmodel = tf.keras.applications.EfficientNetB0()\r\nmodel.compile()\r\nmodel.save(\"broken\")\r\n\r\nmodel = tf.keras.models.load_model(\"broken\")\r\n```\r\n\r\nTrying to train it results in a LookupError.", "It is experimental supported in nightly for the [next 2.6 release](https://www.tensorflow.org/guide/advanced_autodiff#custom_gradients_in_savedmodel) \r\n\r\n`pip install tf-nightly` \r\n\r\n```python\r\nimport tensorflow as tf\r\nmodel = tf.keras.applications.EfficientNetB0()\r\nmodel.compile()\r\ntf.keras.models.save_model(\r\n    model, 'broken',\r\n    options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\r\n\r\nmodel = tf.keras.models.load_model(\"broken\")\r\n```", "I was trying to run an example in tensorflow object detection api where in they were using a pretrained centernet model.Any ideas what could be the possible reason behind this."]}, {"number": 40159, "title": "tf.ragged.constant does not detect dense dimensions", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\nNote: I am reporting this as a bug but I am not sure if it may actually be a feature request, as I am not entirely sure if the described behaviour is fully expected or not.\r\n\r\n[`tf.ragged.constant`](https://www.tensorflow.org/api_docs/python/tf/ragged/constant) does not properly detect which dimensions should be ragged from the given Python list. By default, only the outermost dimension is considered as dense, even if other coherent dimensions exist in the data. One can use `ragged_rank` and/or `inner_shape` to mark a number of innermost dimensions as dense, but it does not seem to be possible to do the opposite, that is, marking some outermost dimensions after the first one as dense. And, in general, it does not detect nor allow to make a ragged tensor with an arbitrary combination of ragged and dense dimensions (even though it is possible to build such ragged tensors in other ways).\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect that all coherent dimensions of a Python nested list are detected as dense dimensions:\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.ragged.constant([[[1], [2, 3], [4]], [[5, 6], [], [7]]]).shape)\r\n# (2, 3, None)\r\n```\r\n\r\nAs a feature addition, having the possibility to specify which arbitrary dimensions are ragged or not would also be nice, although it would have to be with a different API. Maybe I could have for example:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.ragged.constant([[[1], [2, 3], [4]], [[5, 6], [], [7]]], shape=[2, -1, None])\r\n```\r\n\r\nWith `-1` meaning \"detect automatically from data\" and `None` meaning `ragged dimension`.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n# Dense inner dimensions are not detected\r\nprint(tf.ragged.constant([[[1], [2, 3], [4]], [[5, 6], [], [7]]]).shape)\r\n# (2, None, None)\r\n# The outermost dimension is always dense\r\nprint(tf.ragged.constant([[1], [2, 3], [4]]).shape)\r\n# (3, None)\r\n# But simply adding a couple of brackets makes the dimension ragged\r\nprint(tf.ragged.constant([[[1], [2, 3], [4]]]).shape)\r\n# (1, None, None)\r\n```\r\n\r\n**Other info / logs**\r\nNA\r\n", "comments": ["this would be very helpful to have this addressed. would like more control on which dimensions to make ragged."]}, {"number": 40132, "title": "Type annotations for tf.saturate_cast & tf.constant ops", "body": "**Changes:**\r\n- https://github.com/tensorflow/tensorflow/pull/40132/commits/c8f92e1d8ad33d6ad65d1c2815d90484795df8a7 - modify Tensor class to be a Generic class with a DataType parameter which represents the dtype of the Tensor\r\n- b62fb43 - create classes for each TF dtype to extend `DType` base class\r\n- ce3dae0 - add stub file to give each TF dtype their respective type\r\n- 6dceb41 - annotations for dtype argument and return tensor for `tf.saturate_cast` op\r\n- 23ebb34 & c633769 - type decorators\r\n- 6439ae7 - annotations for dtype argument and return tensor for `tf.constant` op", "comments": ["Note: temporarily approving to trigger the import process - the PR is not ready to merge yet."]}, {"number": 40125, "title": "tf.random.uniform unexpected behaviour for unknown shape", "body": "Hi,\r\n\r\nUsing `tf.random.uniform` with a tensor of equal `minval` and `maxval` values and unknown shape does not generate the expected independent variates:\r\n```\r\ntf.random.uniform((), minval=[0, 0, 0], maxval=[4,4,4])\r\n <tf.Tensor: shape=(3,), dtype=float32, numpy=array([2.1699166, 2.1699166, 2.1699166], dtype=float32)>\r\n```\r\nIf the shape is specified, however, independent random variates are returned:\r\n```\r\ntf.random.uniform([3], minval=[0, 0, 0], maxval=[4,4,4])\r\n<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1.3341255 , 0.45406246, 3.4779797 ], dtype=float32)>\r\n```\r\nIs this a bug or a known feature?  If the latter, should it be documented?\r\n\r\nThanks.\r\n\r\nTF version 2.3.0-dev20200528 (tf-nightly)\r\n\r\n", "comments": ["@chrism0dwk the issue was that `tf.random.uniform` had the usage of `math_ops.add` which implicitly broadcast the result from the random and causes the seemingly definitely result. The fix for this issue is the same as some other issues such as #34363 \r\n\r\nThere were several attempts to fix this issue in #34399 / #38585. However, due to internal tests failures they were not merged yet (merged/reverted).\r\n\r\nOnce the internal test failures could be fixed, the fix could be merged and the issue could be resolved then.", "I could reproduce the issue in colab with TF nightly version.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/433b2432284a5833c916c86fb1cde531/untitled48.ipynb).\r\nAs suggested by @yongtang we will verify the issue once PRs get merged and close the issue.Thanks!", "It seems the fix in #38585 fails with\r\n\r\n```\r\nthird_party.tensorflow.python.framework.errors_impl.InvalidArgumentError: Rank of input (1) must be no greater than rank of output shape (0). [Op:BroadcastTo]\r\n```", "Was able to reproduce the issue in TF 2.5 and Nightly versions. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/b5cf845e4ff02c1a484ba6ab88647334/untitled82.ipynb).Thanks!\r\n\r\n### Updated\r\nThis is still an issue with `tf-nightly` (2.9.0-dev20220201). [Here](https://colab.research.google.com/gist/jvishnuvardhan/6b8009e7f7e898cd6d4dbe7826eb324d/untitled82.ipynb) is a gist for reference. Thanks!", "I would like to take this issue and try to fix it."]}, {"number": 40114, "title": "[Feature Request] Logging of validation metrics when using validation_freq > 1", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe `tf.keras.callbacks.CSVLogger` currently does not log metrics which are not computed every epoch during training. By this I mean that the columns in the produced .csv file do not contain certain metrics. This includes validation scores if they are (for performace reasons) only computed every validation_freq training epochs when using Model.fit.\r\nAllowing the user to pass another (optional) parameter to the CSVLogger constructor called, say, 'missing_value_string' which is then used to fill the rows where no value is available for a metric.\r\nI fear the reasons for why validation metrics are not being logged for me lie deeper, however, wherever the metrics to be logged are selected (which, I think, is not in CSVLogger).\r\n\r\n**Will this change the current api? How?**\r\nI can't quite tell. I would not expect significant changes of the API.\r\n\r\n**Who will benefit with this feature?**\r\nPeople who cannot, for some reason, change the number of steps in an epoch circumventing the need to set validation_freq!=1.\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 40100, "title": "Design a generic type Python API for the hparams plugin", "body": "**System information**\r\n- TensorFlow version (you are using): tf2.20\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhile I follow the following tutorial to do the hyperparameter tuning. I cannot choose from list type object.\r\nhttps://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\r\n\r\n```\r\nHP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([[256, 512, 256, 128,64],[256, 512, 1024, 512, 256, 128],[256, 512, 1024, 512, 256, 128, 64, 32]]))\r\n\r\nValueError: Unknown dtype: <class 'list'>\r\n\r\n```\r\n\r\nI am playing with the canned estimator and tune the 'dnn_hidden_units' hyperparameter with 'from tensorboard.plugins.hparams import api as hp'. But it seems that I cannot tune 'dnn_hidden_units' with this library. \r\n\r\n```\r\n    estimator = tf.estimator.DNNLinearCombinedClassifier(\r\n        # wide settings\r\n        linear_feature_columns=feature_columns,\r\n        linear_optimizer=FtrlwithParams,\r\n        # deep settings\r\n        dnn_feature_columns=feature_columns,\r\n        dnn_hidden_units=[256, 512, 256, 128, 64],\r\n#         dnn_hidden_units=[1000, 500,100],\r\n        dnn_optimizer=AdamWithParams,\r\n        batch_norm=True,\r\n        dnn_dropout=0.5,\r\n        n_classes=NUM_LABEL,\r\n        config=RUN_CONFIG,\r\n        # warm-start settings\r\n        warm_start_from=MODEL_DIR\r\n    )\r\n```\r\n\r\n**Will this change the current api? How?**\r\nNo. Maybe we can add a new API or a generic API.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone need to do hyperparameter tuning.\r\n\r\nI expect that people build models with list-like parameter. So I think this is a common feature.\r\n\r\n", "comments": []}, {"number": 40075, "title": "Tensorflow Lite GPU delegate using C++ thread detach on Galaxy Tab S6 is 2.5x slower than without a thread detach", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15 / Android 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy Tab S6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2\r\n- Python version:\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.59)\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Adreno 640 / 128GB 6GB RAM, 256GB 8GB RAM\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI'm running a Tensorflow Lite model with C++ in Android devices.\r\nBut when I run a model with GPU delegate with C++ std::thread(s) in Galaxy Tab S6, it becomes \r\nslower than without using std::thread.\r\nIt happens when just using a thread, even if it's number is just 1.\r\nBut without a thread, everything works fine.\r\n\r\nWith the very same code, no such behavior is happening in other devices, such as Galaxy Tab S5, Galaxy Fold, Oppo FindX, and other Android devices.\r\nBut only in Galaxy Tab S6, using std::thread with GPU delegate is slower than without using a thread(13ms -> 30ms).\r\nCreating a model and running a model is called from the same thread.\r\nTensorflow Lite libraries, C++ native codes, Java code is same in all devices.\r\n\r\n**Describe the expected behavior**\r\nSame inference time when with or without thread(s).\r\n\r\n**Standalone code to reproduce the issue**\r\nSince the model I'm using is the company's model, I can't provide the model's data. \r\nSo I created a simple test app repository at here(it's not working since the model is absent): https://github.com/lackhole/hellovmex\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n### C/C++ Function call profiling\r\n![No thread](https://user-images.githubusercontent.com/35574936/83500301-8a295d00-a4f9-11ea-83ee-3206b0ab7885.png)\r\n * Without using std::thread and run on main thread.\r\n\r\n\r\n![Thread](https://user-images.githubusercontent.com/35574936/83500318-8f86a780-a4f9-11ea-83bd-498e92a200fd.png)\r\n * With using 2 detached std::thread\r\n\r\n\r\nIf any additional information is needed, please let me know.", "comments": ["+ \r\nWorks well if not detaching and using join.\r\nSpeed drops only when using thread detaching"]}, {"number": 40065, "title": "tf.io.gfile / GCS fails to work on OpenSUSE", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux OpenSUSE Tumbleweed\r\n- TensorFlow installed from (source or binary): Binary (conda)\r\n- TensorFlow version (use command below): unknown 2.1.0\r\n- Python version: 3.7.5\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport tensorflow as tf\r\ntf.io.gfile.listdir(\"gs://some-bucket\") # replace w/ bucket of your choice\r\n```\r\n\r\nThis code gives an error:\r\n```\r\n2020-06-01 15:43:56.684531: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Unavailable: Error executing an HTTP request: libcurl code 77 meaning 'Problem with the SSL CA cert (path? access rights?)', error details: error setting certificate verify locations:\r\n  CAfile: /etc/ssl/certs/ca-certificates.crt\r\n  CApath: none\". Retrieving token from GCE failed with \"Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'\".\r\n```\r\n\r\nAfter that it hangs for a while, and then raises a `NotFoundError`.\r\n\r\nI believe this is because the libcurl packaged with tensorflow doesn't know where to find the ca-certificates bundle file on OpenSUSE, which is at `/etc/ssl/ca-bundle.pem` rather than `/etc/ssl/certs/ca-certificates.crt`. Also, I installed through miniconda so there's another equivalent file at `$CONDA_PREFIX/ssl/cacert.pem`. Neither of these seems to be found by tensorflow.\r\n\r\n[This code](https://github.com/tensorflow/tensorflow/blob/5597c17b6a677be5264ebda7cc31404f0ae8a434/tensorflow/core/platform/cloud/curl_http_request.cc#L129-L132) suggests that the bundle file's location can be customized with the `CURL_CA_BUNDLE` env variable. However, this doesn't change the behavior as far as i can tell; the error is still raised.\r\n\r\n**Describe the expected behavior**\r\nIt should list the contents of the bucket.", "comments": ["Ping on this", "Can you run the following please?\r\n\r\n```\r\nstrace python-c \"import tensorflow as tf; tf.io.gfile.listdir(\"gs://some-bucket\") &> log1.txt\r\nstrace CURL_CA_BUNDLE=path/to/good/certs python-c \"import tensorflow as tf; tf.io.gfile.listdir(\"gs://some-bucket\") &> log2.txt\r\n```\r\n\r\nAnd then attach the `log*.txt` files in a reply? You can use multiple paths for the certs and attach multiple logs.\r\n\r\nNote that since we only build and test on ubuntu, it might be slow to solve the issue. We don't have enough headcount to solve issues on all operating systems, unfortunately.", "We had a similar problem with\r\n\r\nCentOS 7\r\nPython 3.7.5\r\ntensorflow 2.1.0\r\n\r\nManually set CURL_CA_BUNDLE do not seems work.", "I ran into this again on RHEL 7 with TF 2.2 and 2.3 and dug a bit:\r\n\r\n- CURL_CA_BUNDLE is for the binary only, not the library. See https://github.com/curl/curl/blob/0d0537aeae2acdbdb43c6aab834abf84ca7ba247/docs/SSLCERTS.md\r\n- The CMake files for CURL correctly search for an existing cert: https://github.com/curl/curl/blob/13030d08ad392e3a6c139400f0dfa827f1e9f2a8/CMakeLists.txt#L830-L835\r\n- TensorFlow hard-codes the location at https://github.com/tensorflow/tensorflow/blob/50db873b7c593ecb573cb44a8cf0c91292cfc515/third_party/curl.BUILD#L487\r\n\r\nHence for TF to work the CA cert MUST be at `/etc/ssl/certs/ca-certificates.crt` and it is NOT possible to change that.\r\n\r\nThe only solutions are to use the system curl (see TF_SYSTEM_LIBS, but that is partly broken), make TF use the CMake install of curl (but I doubt that is gonna happen) or implement the location detection in Bazel", "@kazimuth \r\nCan you please verify if this is still an issue on later tf versions.", "@Saduf2019 Yes the problem is still present because the problematic code hasn't changed: https://github.com/tensorflow/tensorflow/blob/7e193718033d18a159fcae75be4dd0a59c32b897/third_party/curl.BUILD#L572", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This is still an issue, please do not close", "ping @jhseu ... this is still an issue. Anyone?", "Gentle ping @jhseu @mihaimaruseac \r\n\r\nThis problem is still around atleast on Fedora 35 with latest version of TensorFlow. (expecting the same to occur on other RPM based distros)\r\n\r\nI could use this workaround for the time being:\r\n```sh\r\nsudo ln -s /etc/ssl/certs/ca-bundle.crt /etc/ssl/certs/ca-certificates.crt\r\n```", " fixing CURL certificates path as suggested in the [code](https://github.com/tensorflow/tensorflow/blob/5597c17b6a677be5264ebda7cc31404f0ae8a434/tensorflow/core/platform/cloud/curl_http_request.cc#L129-L132) mentioned above seems to work for me for Tensorflow 2.4.1 and Fedora 28.\r\nadded the environment variable in .bashrc :\r\n> export CURL_CA_BUNDLE=/etc/ssl/certs/ca-bundle.crt\r\n", "Can you send a PR please?"]}, {"number": 40055, "title": "DefaultQuantParamsPass doesn't work correctly if bias constant has multiple users", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.0.dev20200601 \r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): 3.0.0\r\n\r\n**Describe the current behavior**\r\n\r\n[`DefaultQuantParamsPass`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/default_quant_params.cc) doesn't correctly handle quantization if a  `bias` constant has multiple users. This issue can be seen in the following toy model:\r\n```python\r\nimg = tf.keras.layers.Input((32, 32, 3))\r\nx = tf.keras.layers.Conv2D(64, 3, activation=\"relu\", use_bias=True)(img)\r\nx = tf.keras.layers.BatchNormalization()(x)\r\nmodel = tf.keras.models.Model(img, x)\r\n```\r\nSince the `bias` of the `Conv2D` and `beta` from the `BatchNormalization` are both zero at initialization and have the same shape these tensors are shared by MLIR and don't convert correctly since `DefaultQuantParamsPass` doesn't duplicate the tensor to satisfy the differet quantization constraints by `tfl.conv_2d` and `tfl.mul`.\r\n\r\n**Describe the expected behavior**\r\nThe `bias` of the `Conv2D` layer should be quantized to `int32` as required by `tfl.conv_2d`  and `beta` used by `tfl.mul` from the `BatchNormalization` layer should be quantized to `int8` as required my `tfl.mul`.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe issue can be reproduced by the following MLIR test:\r\n```mlir\r\n// RUN: tf-opt %s --tfl-default-quant --tfl-quantize --tfl-post-quantize | FileCheck %s  --dump-input-on-failure\r\n\r\n// CHECK-LABEL: test_quantize_shared_bias\r\nfunc @test_quantize_shared_bias(%arg0: tensor<1x112x112x32xf32>, %arg1: tensor<32x3x3x3xf32>) -> tensor<1x56x56x32xf32> {\r\n  %cst = constant dense<0.0> : tensor<32xf32>\r\n  %conv = \"tfl.conv_2d\"(%arg0, %arg1, %cst) {dilation_h_factor = 2 : i32, dilation_w_factor = 3 : i32, fused_activation_function = \"RELU\", padding = \"SAME\", stride_h = 4 : i32, stride_w = 5 : i32} : (tensor<1x112x112x32xf32>, tensor<32x3x3x3xf32>, tensor<32xf32>) -> tensor<1x56x56x32xf32>\r\n  %add = \"tfl.add\"(%conv, %cst) {fused_activation_function = \"NONE\"} : (tensor<1x56x56x32xf32>, tensor<32xf32>) -> tensor<1x56x56x32xf32>\r\n  return %add : tensor<1x56x56x32xf32>\r\n\r\n  // CHECK: %0 = \"tfl.pseudo_qconst\"() {qtype = tensor<32x!quant.uniform<i32:f32, 6.1514801999231058E-5>>, value = dense<0> : tensor<32xi32>} : () -> tensor<32x!quant.uniform<i32:f32, 6.1514801999231058E-5>>\r\n  // CHECK: %1 = \"tfl.pseudo_qconst\"() {qtype = tensor<32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>, value = dense<-128> : tensor<32xi8>} : () -> tensor<32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>\r\n  // CHECK: %2 = \"tfl.conv_2d\"(%arg0, %arg1, %0) {dilation_h_factor = 2 : i32, dilation_w_factor = 3 : i32, fused_activation_function = \"RELU\", padding = \"SAME\", stride_h = 4 : i32, stride_w = 5 : i32} : (tensor<1x112x112x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>, tensor<32x3x3x3x!quant.uniform<u8:f32, 0.0078431372549019607:128>>, tensor<32x!quant.uniform<i32:f32, 6.1514801999231058E-5>>) -> tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>\r\n  // CHECK: %3 = \"tfl.add\"(%2, %1) {fused_activation_function = \"NONE\"} : (tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>, tensor<32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>) -> tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>\r\n  // CHECK: return %3 : tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>\r\n}\r\n```\r\n\r\nThe current implementation outputs the following graph which fails to quantize the convolution:\r\n```mlir\r\nfunc @test_quantize_shared_bias(%arg0: tensor<1x112x112x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>, %arg1: tensor<32x3x3x3x!quant.uniform<u8:f32, 0.0078431372549019607:128>>) -> tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>> {\r\n  %cst = constant dense<0.000000e+00> : tensor<32xf32>\r\n  %0 = \"tfl.dequantize\"(%arg1) : (tensor<32x3x3x3x!quant.uniform<u8:f32, 0.0078431372549019607:128>>) -> tensor<32x3x3x3xf32>\r\n  %1 = \"tfl.dequantize\"(%arg0) : (tensor<1x112x112x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>) -> tensor<1x112x112x32xf32>\r\n  %2 = \"tfl.conv_2d\"(%1, %0, %cst) {dilation_h_factor = 2 : i32, dilation_w_factor = 3 : i32, fused_activation_function = \"RELU\", padding = \"SAME\", stride_h = 4 : i32, stride_w = 5 : i32} : (tensor<1x112x112x32xf32>, tensor<32x3x3x3xf32>, tensor<32xf32>) -> tensor<1x56x56x32xf32>\r\n  %3 = \"tfl.quantize\"(%2) {qtype = tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>} : (tensor<1x56x56x32xf32>) -> tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>\r\n  %4 = \"tfl.dequantize\"(%3) : (tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>) -> tensor<1x56x56x32xf32>\r\n  %5 = \"tfl.add\"(%4, %cst) {fused_activation_function = \"NONE\"} : (tensor<1x56x56x32xf32>, tensor<32xf32>) -> tensor<1x56x56x32xf32>\r\n  %6 = \"tfl.quantize\"(%5) {qtype = tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>} : (tensor<1x56x56x32xf32>) -> tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>\r\n  return %6 : tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>\r\n}\r\n```\r\n\r\nWhen allowing hybrid operands the issue becomes clearer and it generates a graph that will fail to execute in TFLite due to a floating point bias being used in the quantized convolution:\r\nhttps://github.com/tensorflow/tensorflow/blob/de32c75f2f0b9c298d858180fc19fa8881bfab41/tensorflow/compiler/mlir/lite/transforms/quantize.cc#L72\r\n```mlir\r\nfunc @test_quantize_shared_bias(%arg0: tensor<1x112x112x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>, %arg1: tensor<32x3x3x3x!quant.uniform<u8:f32, 0.0078431372549019607:128>>) -> tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>> {\r\n  %cst = constant dense<0.000000e+00> : tensor<32xf32>\r\n  %0 = \"tfl.conv_2d\"(%arg0, %arg1, %cst) {dilation_h_factor = 2 : i32, dilation_w_factor = 3 : i32, fused_activation_function = \"RELU\", padding = \"SAME\", stride_h = 4 : i32, stride_w = 5 : i32} : (tensor<1x112x112x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>, tensor<32x3x3x3x!quant.uniform<u8:f32, 0.0078431372549019607:128>>, tensor<32xf32>) -> tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>\r\n  %1 = \"tfl.add\"(%0, %cst) {fused_activation_function = \"NONE\"} : (tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>, tensor<32xf32>) -> tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>\r\n  return %1 : tensor<1x56x56x32x!quant.uniform<u8:f32, 0.0078431372549019607:128>>\r\n}\r\n```\r\n\r\n**Other info / logs**\r\n\r\n@liufengdb fixed a very similar issue in the quantization driver with https://github.com/tensorflow/tensorflow/commit/4a17afaf6e5d891a3e21561fa20ca093fe09b4e2 and https://github.com/tensorflow/tensorflow/commit/2f01bf6606ee6f10c7e57b98d16a3926d2087fd5 so it would be great to integrate the fixes in the `DefaultQuantParamsPass` as well. This pass is very useful for benchmarking quantized models without the need to correctly include fake quant ops in the graph. I tried using the post-training quantization workflow, but in my testing it failed when used together with custom ops.", "comments": ["@amahendrakar @daverim This has been labeled with `TF 2.3` are there any updates on whether you are still planing to look into a fix for the upcoming release?", "What is the status of this?", "> What is the status of this?\r\n\r\nAs far as I can tell, this is still an issue and hasn't been fixed yet.", "Sorry for the delay on this. We're looking into the issue and trying to\nresolve it.\n\n\nOn Thu, Jun 10, 2021 at 7:49 PM Lukas Geiger ***@***.***>\nwrote:\n\n> What is the status of this?\n>\n> As far as I can tell, this is still an issue and hasn't been fixed yet.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/40055#issuecomment-858518836>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AASV4JPKTXTOFUZBWMVM6CLTSCKDFANCNFSM4NP5TPXQ>\n> .\n>\n", "Recently tested this with tf-nightly -- seems to be resolved now? The\nconstants are properly duplicated on the toy model originally posted.\n\nOn Fri, Jun 11, 2021 at 8:02 AM David Rim ***@***.***> wrote:\n\n> Sorry for the delay on this. We're looking into the issue and trying to\n> resolve it.\n>\n>\n> On Thu, Jun 10, 2021 at 7:49 PM Lukas Geiger ***@***.***>\n> wrote:\n>\n>> What is the status of this?\n>>\n>> As far as I can tell, this is still an issue and hasn't been fixed yet.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/40055#issuecomment-858518836>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AASV4JPKTXTOFUZBWMVM6CLTSCKDFANCNFSM4NP5TPXQ>\n>> .\n>>\n>\n", "> Recently tested this with tf-nightly -- seems to be resolved now? The\r\nconstants are properly duplicated on the toy model originally posted.\r\n\r\nAre you sure? I recently tested the MLIR filechecks again, and it still seems to be an issue or am I missing something?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\r\n\r\nI think this is still an issue :)"]}, {"number": 40045, "title": "[MLIR/XLA] Invalid IR passes verification", "body": "With TensorFlow HEAD at 3ffb4ad2d43311f41155b6e00fd105e50df685da (May 31), the following snippet should fail verification with `tf-opt` but it doesn't:\r\n\r\n```\r\n  func @main(%arg0: memref<4x64x128x3xf32>) -> tuple<tensor<4x64x128x3xf32>> {\r\n    \"xla_lhlo.copy\"(%arg0, %arg0) : (memref<4x64x128x3xf32>, memref<4x64x128x3xf32>) -> ()\r\n    \"xla_lhlo.terminator\"() : () -> ()\r\n  }\r\n```\r\n\r\nTo reproduce: `$ bazel-bin/tensorflow/compiler/mlir/tf-opt  verify.mlir`.\r\n\r\n@joker-eph @pifon2a @sherhut \r\n", "comments": ["@bondhugula Why exactly should it fail? Is that because `xla_lhlo.copy` source and destination buffers are the same? \r\n\r\nCurrently, there are no custom verifiers for lhlo ops. Also I am a bit surprised to see `tuple` here. ", "> @bondhugula Why exactly should it fail? Is that because `xla_lhlo.copy` source and destination buffers are the same?\r\n\r\nNot really - this part is fine. (The src/dest of the copy given that this has memory semantics can be the same.)\r\n\r\n> \r\n> Currently, there are no custom verifiers for lhlo ops. Also I am a bit surprised to see `tuple` here.\r\n\r\nIt was the return type signature that I felt was out of line with the terminator -- as to where the function was getting its return value (tuple tensor) from.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "So the issue is with the custom terminator which isn't strict about the possible parent operations?\r\nThen the minimal example is something like:\r\n\r\n```\r\nfunc @main() -> f32 {\r\n  \"xla_lhlo.terminator\"() : () -> ()\r\n}\r\n```", "> So the issue is with the custom terminator which isn't strict about the possible parent operations?\r\n> Then the minimal example is something like:\r\n> \r\n> ```\r\n> func @main() -> f32 {\r\n>   \"xla_lhlo.terminator\"() : () -> ()\r\n> }\r\n> ```\r\n\r\nThat's right - that would be the minimal form - the copy has nothing to do with this issue. Not a high priority issue, but would be good to fail verification in such cases.\r\n", "https://reviews.llvm.org/D81045 should help here.", "What is the status of this?"]}, {"number": 40044, "title": "TFLite: Support grouped convolutions", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): tf-nightly==2.3.0.dev20200531\r\n\r\n**Motivation**\r\n#25818 added support for native grouped convolutions a year ago. This feature is now also available via Keras layers in the latest nightly release (#36773, #39516).\r\nConverting a model using grouped convolutions to TFLite works fine, though the TFLite runtime currently doesn't support this feature and will throw an error when trying to allocate the Tensors.\r\n\r\nIt would be great to have this feature available in TFLite in order to have consistent behaviour accross TensorFlow and TFLite. The PRs linked above provide more detail about why this feature something that people would want to use.\r\n\r\n**Standalone code to reproduce the issue** \r\nThe issue can be reproduced using [this colab notebook](https://colab.research.google.com/drive/1ngxLfGs0lrGZV1Y8zwTP5X6WAEEEGp1P?usp=sharing).\r\n\r\n**Any other info / logs**\r\n\r\nI guess adding a reference implementation and implementing optimized kernels in the XNNPack delegate would be pretty straight forward as it already has native support for grouped convolutions:\r\nhttps://github.com/tensorflow/tensorflow/blob/add80cd47acfa2335b260b8ab877e4dc5cff499b/tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc#L1072\r\n\r\nThough, I am not sure how much effort it would be to support it in the other optimized code paths with Ruy.\r\n\r\nIf there is a fundamental reason why support for grouped convolutions cannot be added to TFLite it would be great to handle this in the MLIR based converter and translate native TF grouped convolutions to a naive loop based implementation using `tfl.split` and `tfl.concat` which would allow people to use TF group convolutions and and fall back to a loop based implementation in TFLite for now.", "comments": ["@srjoglekar246 Do you know if anyone is looking into that?", "@srjoglekar246 @talumbau Do you have any information whether this is on the roadmap?", "Hi,\r\nAny news?", "Sorry for the late reply on this :-(. @talumbau is looking into this, and has a plan forward. We were busy with some other bugs last quarter, but we have a TODO for it to land this quarter. Thanks for your patience!", "news? related: [Group convolutions for CPUs](https://github.com/tensorflow/tensorflow/commit/7b8db6083b34520688dbc71f341f7aeaf156bf17)", "If I wanted to try and attempt this, where would I start? This seems relevant because in python it's just a different kernel shape. https://www.tensorflow.org/lite/guide/ops_version#change_c_structures_and_kernel_implementation", "Does this issue have any progress?\r\nI have done a small experiment which showed that group convolution is the reason why tflite model failed to do inference with this error message.\r\n\r\nTF version == 2.5.0\r\n\r\n![image](https://user-images.githubusercontent.com/47134502/122755571-bf047f80-d2c7-11eb-97eb-05f9089ba877.png)\r\n\r\nexperiment model\r\n```\r\ntest_data = tf.constant(0,shape=[1,320,320,3])\r\nmodel = tf.keras.Sequential([\r\n                tf.keras.layers.InputLayer([320,320,3]),\r\n                tf.keras.layers.Conv2D(8,3),\r\n                tf.keras.layers.Conv2D(8,3,groups=8)\r\n])\r\nmodel.predict(test_data).shape\r\ntf.keras.models.save_model(model,'/content/test')\r\n```\r\n\r\nand convert command\r\n```tflite_convert --saved_model /content/test --output_file /content/test.tflite```\r\n", "I know this won't be helpful to everyone, but this is my special work-around so far. If this is confusing, please ignore it.\r\n\r\nI implemented GoupConvolusion with the standard Conv2D and Split, Concat, although I may have failed to transpose the weights. This model is chaotic.\r\n\r\nhttps://github.com/PINTO0309/PINTO_model_zoo/issues/15\r\nhttps://github.com/PINTO0309/openvino2tensorflow\r\n\r\n- Midasnet - Float32 - GroupConvolusion - TFLite(.tflite)\r\n![image](https://user-images.githubusercontent.com/33194443/129143153-a217a412-1e21-427e-a57e-c32ff98e1d47.png)\r\n\r\nI have been waiting for this issue to be resolved for a long time.", "> Does this issue have any progress? I have done a small experiment which showed that group convolution is the reason why tflite model failed to do inference with this error message.\r\n> \r\n> TF version == 2.5.0\r\n> \r\n> ![image](https://user-images.githubusercontent.com/47134502/122755571-bf047f80-d2c7-11eb-97eb-05f9089ba877.png)\r\n> \r\n> experiment model\r\n> \r\n> ```\r\n> test_data = tf.constant(0,shape=[1,320,320,3])\r\n> model = tf.keras.Sequential([\r\n>                 tf.keras.layers.InputLayer([320,320,3]),\r\n>                 tf.keras.layers.Conv2D(8,3),\r\n>                 tf.keras.layers.Conv2D(8,3,groups=8)\r\n> ])\r\n> model.predict(test_data).shape\r\n> tf.keras.models.save_model(model,'/content/test')\r\n> ```\r\n> \r\n> and convert command `tflite_convert --saved_model /content/test --output_file /content/test.tflite`\r\n\r\nI meet the same problem, when my h5 model trans to tflite, I use tflite to do inference, but conv2d with groups failed. Does this issue have any progress?", "I also faced the same issue.", "I made a work around in my repo [Github leondgarse/keras_cv_attention_models](https://github.com/leondgarse/keras_cv_attention_models), that replacing `Conv2D groups != 1` with `split -> conv -> concat` like\r\n![](https://user-images.githubusercontent.com/5744524/146923045-c817edd2-a74a-42a1-bb17-82481cd0464f.png)\r\n- Basic test:\r\n  ```py\r\n  !pip install keras-cv-attention-models\r\n\r\n  import tensorflow as tf\r\n  import numpy as np\r\n  from tensorflow import keras\r\n  from keras_cv_attention_models.imagenet import eval_func\r\n  from keras_cv_attention_models import model_surgery\r\n  \r\n  mm = keras.Sequential([keras.layers.InputLayer([320, 320, 32]), keras.layers.Conv2D(64, 3, groups=8)])\r\n  bb = model_surgery.convert_groups_conv2d_2_split_conv2d(mm)\r\n  test_inputs = tf.random.uniform([1, *mm.input_shape[1:]])\r\n  print(np.allclose(mm(test_inputs), bb(test_inputs)))\r\n  # True\r\n  \r\n  converter = tf.lite.TFLiteConverter.from_keras_model(bb)\r\n  converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n  open(\"aa_dynamic.tflite\", \"wb\").write(converter.convert())\r\n  print(np.allclose(mm(test_inputs), eval_func.TFLiteModelInterf('aa_dynamic.tflite')(test_inputs), atol=1e-6))\r\n  # True\r\n  ```\r\nWe have a discussion here [Freezing a trained keras CV attention model #17](https://github.com/leondgarse/keras_cv_attention_models/discussions/17), where you can find more detail usage.", "Hi,\r\nIn Tensorflow 2.8 an error is thrown earlier (during conversion):\r\n```\r\nTraceback (most recent call last):\r\n...\r\ntensorflow.lite.python.convert_phase.ConverterError: /.../lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1369:0: error: 'tf.Conv2D' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n/.../lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1369:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n<unknown>:0: error: failed while converting: 'main': \r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: Conv2D\r\nDetails:\r\n\ttf.Conv2D(tensor<?x26x26x32xf32>, tensor<3x3x8x32xf32>) -> (tensor<?x24x24x32xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\r\n```\r\n", "> Hi, In Tensorflow 2.8 an error is thrown earlier (during conversion):\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n> ...\r\n> tensorflow.lite.python.convert_phase.ConverterError: /.../lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1369:0: error: 'tf.Conv2D' op is neither a custom op nor a flex op\r\n> <unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n> /.../lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1369:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> <unknown>:0: error: failed while converting: 'main': \r\n> Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\n> TF Select ops: Conv2D\r\n> Details:\r\n> \ttf.Conv2D(tensor<?x26x26x32xf32>, tensor<3x3x8x32xf32>) -> (tensor<?x24x24x32xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\r\n> ```\r\n\r\nYou need to use SELECT OPS. Add the following code for that\r\n\r\n`converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OP]\r\n`\r\n", "> > Hi, In Tensorflow 2.8 an error is thrown earlier (during conversion):\r\n> > ```\r\n> > Traceback (most recent call last):\r\n> > ...\r\n> > tensorflow.lite.python.convert_phase.ConverterError: /.../lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1369:0: error: 'tf.Conv2D' op is neither a custom op nor a flex op\r\n> > <unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\r\n> > /.../lib/python3.8/site-packages/tensorflow/python/saved_model/save.py:1369:0: note: Error code: ERROR_NEEDS_FLEX_OPS\r\n> > <unknown>:0: error: failed while converting: 'main': \r\n> > Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\n> > TF Select ops: Conv2D\r\n> > Details:\r\n> > \ttf.Conv2D(tensor<?x26x26x32xf32>, tensor<3x3x8x32xf32>) -> (tensor<?x24x24x32xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\r\n> > ```\r\n> \r\n> You need to use SELECT OPS. Add the following code for that\r\n> \r\n> `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OP] `\r\n\r\nBut inference still fails. I convert and infer this way:\r\n```\r\nverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\ninput_index = interpreter.get_input_details()[0][\"index\"]\r\ntest_image = np.random.randn(1, 28, 28, 1).astype(np.float32)\r\ninterpreter.set_tensor(input_index, test_image)\r\n# Run inference.\r\ninterpreter.invoke()\r\n```\r\nIt fails with:\r\n```\r\nProcess finished with exit code 139 (interrupted by signal 11: SIGSEGV)\r\n```", "> I made a work around in my repo [Github leondgarse/keras_cv_attention_models](https://github.com/leondgarse/keras_cv_attention_models), that replacing `Conv2D groups != 1` with `split -> conv -> concat` like ![](https://user-images.githubusercontent.com/5744524/146923045-c817edd2-a74a-42a1-bb17-82481cd0464f.png)\r\n> \r\n> * Basic test:\r\n>   ```python\r\n>   !pip install keras-cv-attention-models\r\n>   \r\n>   import tensorflow as tf\r\n>   import numpy as np\r\n>   from tensorflow import keras\r\n>   from keras_cv_attention_models.imagenet import eval_func\r\n>   from keras_cv_attention_models import model_surgery\r\n>   \r\n>   mm = keras.Sequential([keras.layers.InputLayer([320, 320, 32]), keras.layers.Conv2D(64, 3, groups=8)])\r\n>   bb = model_surgery.convert_groups_conv2d_2_split_conv2d(mm)\r\n>   test_inputs = tf.random.uniform([1, *mm.input_shape[1:]])\r\n>   print(np.allclose(mm(test_inputs), bb(test_inputs)))\r\n>   # True\r\n>   \r\n>   converter = tf.lite.TFLiteConverter.from_keras_model(bb)\r\n>   converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n>   open(\"aa_dynamic.tflite\", \"wb\").write(converter.convert())\r\n>   print(np.allclose(mm(test_inputs), eval_func.TFLiteModelInterf('aa_dynamic.tflite')(test_inputs), atol=1e-6))\r\n>   # True\r\n>   ```\r\n> \r\n> We have a discussion here [Freezing a trained keras CV attention model #17](https://github.com/leondgarse/keras_cv_attention_models/discussions/17), where you can find more detail usage.\r\n\r\nWorks for me, thx! \r\nMy model contains depthwise conv but with different length strides in row and column ( strides=[2, 1] ), which layers.DepthwiseConv2D not implmented currently. \r\n\r\nTensorflow Verison: 2.7.0"]}, {"number": 39991, "title": "Correct way of using tf.keras.layers.experimental.preprocessing layers under strategy scope", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yes\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): nightly\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nHow should I use the preprocessing layers together with distribute strategy? Since strategy requires something like this:\r\n\r\n```python\r\nwith strategy.scope():\r\n    model = build_model()\r\n    model.fit(...)\r\n```\r\n\r\nBut when I use the preprocessing layers, I got the following errors:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"make_image_classifier.py\", line 307, in <module>\r\n    run_main()\r\n  File \"make_image_classifier.py\", line 303, in run_main\r\n    app.run(main)\r\n  File \"/home/ml/users/jdong25/.conda/envs/tf/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/ml/users/jdong25/.conda/envs/tf/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"make_image_classifier.py\", line 260, in main\r\n    FLAGS.summaries_dir)\r\n  File \"/home/ml/users/jdong25/.conda/envs/tf/lib/python3.7/site-packages/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py\", line 380, in make_image_classifier\r\n    hparams.do_data_augmentation, augment_params)\r\n  File \"/home/ml/users/jdong25/.conda/envs/tf/lib/python3.7/site-packages/tensorflow_hub/tools/make_image_classifier/make_image_classifier_lib.py\", line 261, in build_model\r\n    preprocessing.RandomRotation(factor=augment_params['rotation_range']),\r\n  File \"/home/ml/users/jdong25/.conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\", line 787, in __init__\r\n    self._rng = make_generator(self.seed)\r\n  File \"/home/ml/users/jdong25/.conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\", line 1289, in make_generator\r\n    return stateful_random_ops.Generator.from_non_deterministic_state()\r\n  File \"/home/ml/users/jdong25/.conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/stateful_random_ops.py\", line 471, in from_non_deterministic_state\r\n    return cls(state=state, alg=alg)\r\n  File \"/home/ml/users/jdong25/.conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/stateful_random_ops.py\", line 384, in __init__\r\n    trainable=False)\r\n  File \"/home/ml/users/jdong25/.conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/stateful_random_ops.py\", line 270, in _create_variable\r\n    \"Creating a generator within a strategy scope is disallowed, because \"\r\nValueError: Creating a generator within a strategy scope is disallowed, because there is ambiguity on how to replicate a generator (e.g. should it be copied so that each replica gets the same random numbers, or 'split' so that each replica gets different random numbers).\r\n```\r\n\r\nThe error message is reasonable to me, but how should I use them correctly? As a reference, my model was built like:\r\n\r\n```python\r\ndef build_model(...):\r\n  model = tf.keras.Sequential([\r\n      tf.keras.Input(shape=(image_size[0], image_size[1], 3)),])\r\n\r\n  aug_preprocessor = None\r\n  if do_data_augmentation:\r\n    preprocessing = tf.keras.layers.experimental.preprocessing\r\n    aug_preprocessor = tf.keras.Sequential(\r\n      preprocessing.RandomRotation(factor=augment_params['rotation_range']),\r\n      preprocessing.RandomWidth(factor=augment_params['width_shift_range']),\r\n      preprocessing.RandomHeight(factor=augment_params['height_shift_range']),\r\n      preprocessing.RandomZoom(factor=augment_params['zoom_range']),\r\n      preprocessing.RandomFlip(mode='horizontal')\r\n    )\r\n    model.add(aug_preprocessor)\r\n\r\n  model.add(module_layer)\r\n  model.add(tf.keras.layers.Dropout(rate=hparams.dropout_rate))\r\n  model.add(tf.keras.layers.Dense(\r\n      num_classes,\r\n      activation=\"softmax\",\r\n      kernel_regularizer=tf.keras.regularizers.l1_l2(l1=hparams.l1_regularizer,\r\n                                                     l2=hparams.l2_regularizer)))\r\n\r\n  print(model.summary())\r\n  return model\r\n\r\nwith strategy.scope():\r\n    model = build_model(model)\r\n    summ = train_model(model,...)\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@djdongjin \r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "> @djdongjin\r\n> This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nOkay. I do think this is a feature request. For me, the current preprocessing layers do not support distribute strategy, which prevents them from being used in any model that wants to support distribute strategy. ", "Any updates?", "This is a legitimate feature request that we should resolve", "@djdongjin thanks for reporting the issue. We are looking into it. "]}, {"number": 39944, "title": "per-channel quantization", "body": "Hello,\r\n\r\nIt seem the tool by default uses \"per-channel-quantization\" is this possible to turn-off during quantization? I need that support to switch-off (switch-off per-channel quantization) since my device does not support it and is an integer only hardware accelerator.\r\n\r\nI am looking for a way to quantize it all uniformly, so all input channels are quantized equally (in the same range) and fully. is there some special setting or the way to do it so?\r\n\r\nThank you for your help and advice.", "comments": ["@peter197321 \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "here is a snippet from code\r\n\r\n```\r\n            # full integer quantization - weights and activations\r\n            converter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\r\n            filename += \"_quant_full\"\r\n            converter.representative_dataset = __imagenet_sample_gen_fn\r\n            if force_full_quant:  # experimental code for compatibility\r\n                converter.target_spec.supported_ops = [tensorflow.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n                converter.inference_input_type = tensorflow.uint8\r\n                converter.inference_output_type = tensorflow.uint8\r\n                filename += \"_forced\"\r\n            else:\r\n                filename += \"_default\"\r\n\r\n    tf_model = converter.convert()\r\n```\r\n\r\n", "You may try [Post-training integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant) in this case.\r\nIt ensures compatibility with integer only devices.\r\nSee https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only", "this way of quantization doesn't work if per-channel quantization for the device I have ... So the question of how to turn-off this way quantization and not to use per-channel quantization as you are implicitly suggesting.\r\n\r\nis there any configuration to choose from of what quantization method is applied?\r\n\r\nBTW on my side quantized models for the Coral Edge TPU do not work even I have integer computation unit. So the issue is the quantization method applied. for this case they used per-channel-quantization. I need to turn off and have a uniform quantization over the entire model-space.\r\nIn the case for TensorFlow Lite for Microcontrollers is the opposite problem models that are not per-channel-quantized are not executed using accelerator optimal kernels. So models which do not use this technique (per-channel-quantization) were formerly quantized by the old tool - that old-tool quantized method those models does not work.\r\n\r\nApparently TensorFlow guys are in the process learning how to use embedded devices ...", "just fyi: models from [Hosted models](https://www.tensorflow.org/lite/guide/hosted_models) works for me but models from [Coral Edge TPU](https://coral.ai/models/) do not work.\r\n\r\nWhat is a different quantization method used?", "Prior to addition of per-axis quantization in optimization toolkit, post-training integer quantization was used, is it possible to enforce Per-axis (aka per-channel) or post-training integer quantization on the model? "]}, {"number": 39895, "title": "[MLIR] xla hlo -> lhlo conversion issue when operand is a constant tensor", "body": "With master branch as of 831a55584749593400807e0baa7478476b5dbc70 (May 26):\r\nThe xla hlo to lhlo lowering doesn't convert completely when the operand of an op (in this example below, that of broadcast_in_dim) is a constant tensor. To reproduce, please use:\r\n\r\n```\r\nfunc @main(%arg0: tensor<4x64x128x3xf32>, %arg1: tensor<5x3x3x10xf32>) {\r\n  %cst = constant  dense<0.000000e+00> : tensor<f32>\r\n  %0 = \"xla_hlo.broadcast_in_dim\"(%cst) {broadcast_dimensions = dense<[]> : tensor<0xi64>, name = \"broadcast.6\"} : (tensor<f32>) -> tensor<4x30x42x10xf32>\r\n  return\r\n}\r\n```\r\n\r\nand\r\n\r\n```\r\n$ tf-opt -hlo-legalize-to-lhlo constant-tensor-lowering.mlir \r\nconstant-tensor-lowering.mlir:3:8: error: 'xla_lhlo.broadcast_in_dim' op operand #0 must be memref of floating-point or signless integer or complex-type values, but got 'tensor<f32>'\r\n  %0 = \"xla_hlo.broadcast_in_dim\"(%cst) {broadcast_dimensions = dense<[]> : tensor<0xi64>, name = \"broadcast.6\"} : (tensor<f32>) -> tensor<4x30x42x10xf32>\r\n       ^\r\nconstant-tensor-lowering.mlir:3:8: note: see current operation: \"xla_lhlo.broadcast_in_dim\"(%cst, %0) {broadcast_dimensions = dense<[]> : tensor<0xi64>, name = \"broadcast.6\"} : (tensor<f32>, memref<4x30x42x10xf32>) -> ()\r\n```\r\nUsing `-print-ir-after-all` reveals the operand for the lhlo broadcast_in_dim op wasn't converted to a memref:\r\n\r\n\r\n```\r\ntuple.mlir:4:10: error: 'xla_lhlo.broadcast_in_dim' op operand #0 must be memref of floating-point or signless integer or complex-type values, but got 'tensor<f32>'\r\n    %0 = \"xla_hlo.broadcast_in_dim\"(%cst) {broadcast_dimensions = dense<[]> : tensor<0xi64>, name = \"broadcast.6\"} : (tensor<f32>) -> tensor<4x30x42x10xf32>\r\n         ^\r\ntuple.mlir:4:10: note: see current operation: \"xla_lhlo.broadcast_in_dim\"(%cst, %0) {broadcast_dimensions = dense<[]> : tensor<0xi64>, name = \"broadcast.6\"} : (tensor<f32>, memref<4x30x42x10xf32>) -> ()\r\n// *** IR Dump After mlir::detail::VerifierPass Failed ***\r\n\r\n\r\n\"module\"() ( {\r\n  \"func\"() ( {\r\n  ^bb0(%arg0: memref<4x64x128x3xf32>, %arg1: memref<5x3x3x10xf32>):  // no predecessors\r\n    %cst = \"std.constant\"() {name = \"constant.5\", value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>\r\n    %0 = \"std.alloc\"() : () -> memref<4x30x42x10xf32>\r\n    \"xla_lhlo.broadcast_in_dim\"(%cst, %0) {broadcast_dimensions = dense<[]> : tensor<0xi64>, name = \"broadcast.6\"} : (tensor<f32>, memref<4x30x42x10xf32>) -> ()\r\n    %1 = \"std.alloc\"() : () -> memref<4x30x42x10xf32>\r\n    \"xla_lhlo.convolution\"(%arg0, %arg1, %1) {batch_group_count = 1 : i64, dimension_numbers = {input_batch_dimension = 0 : i64, input_feature_dimension = 3 : i64, input_spatial_dimensions = dense<[1, 2]> : tensor<2xi64>, kernel_input_feature_dimension = 2 : i64, kernel_output_feature_dimension = 3 : i64, kernel_spatial_dimensions = dense<[0, 1]> : tensor<2xi64>, output_batch_dimension = 0 : i64, output_feature_dimension = 3 : i64, output_spatial_dimensions = dense<[1, 2]> : tensor<2xi64>}, feature_group_count = 1 : i64, lhs_dilations = dense<1> : tensor<2xi64>, name = \"convolution.4\", padding = dense<0> : tensor<2x2xi64>, precision_config = [\"DEFAULT\", \"DEFAULT\"], rhs_dilations = dense<1> : tensor<2xi64>, window_strides = dense<[2, 3]> : tensor<2xi64>} : (memref<4x64x128x3xf32>, memref<5x3x3x10xf32>, memref<4x30x42x10xf32>) -> ()\r\n    %2 = \"std.alloc\"() : () -> memref<4x30x42x10xf32>\r\n    \"xla_lhlo.maximum\"(%0, %1, %2) {name = \"maximum.7\"} : (memref<4x30x42x10xf32>, memref<4x30x42x10xf32>, memref<4x30x42x10xf32>) -> ()\r\n    \"xla_lhlo.terminator\"() : () -> ()\r\n  }) {sym_name = \"main\", type = (memref<4x64x128x3xf32>, memref<5x3x3x10xf32>) -> ()} : () -> ()\r\n  \"module_terminator\"() : () -> ()\r\n}) : () -> ()\r\n```\r\nThis is nothing specific to broadcast_in_dim (happens with say xla_hlo.add as well). This is likely a missing conversion for std.constant that needs to be completed?", "comments": ["@bondhugula \r\nPlease share simple stand alone code for us to replicate the issue faced.", "> @bondhugula\r\n> Please share simple stand alone code for us to replicate the issue faced.\r\n\r\nBut that's exactly what I provided above - it's a 2-line MLIR standalone test input. Did you expect something else?\r\n", "@r4nt Please assign this issue to me - I'll add the missing rewrites needed to get this working. @MaheshRavishankar @joker-eph for visibility and their information.", "THanks Uday! But is the expectation to have xla_hlo.const and I think there is a conversion from xla_hlo.const to xla_lhlo.const. Then the conversion to memref does work. I am not sure there is an expectation to mix dialects as in your example. But I dont see any reason not to have such a pattern, but maybe you could reuse some of the conversion from hlo -> lhlo for std -> lhlo conversion.", "> THanks Uday! But is the expectation to have xla_hlo.const and I think there is a conversion from xla_hlo.const to xla_lhlo.const. Then the conversion to memref does work. I am not sure there is an expectation to mix dialects as in your example. But I dont see any reason not to have such a pattern, but maybe you could reuse some of the conversion from hlo -> lhlo for std -> lhlo conversion.\r\n\r\nDumping an XLA protobuf and using tf-mlir-translate -hlo-to-mlir-hlo actually yields such an input (std.constant instead of an xla_hlo.const). xla_hlo.const -> xla_lhlo.const would work, but the -hlo-legalize-to-lhlo pass could potentially be presented with such an input in other ways, and so making -hlo-to-mlir-hlo generate hlo.const isn't a real solution. The std.constant in these cases is isomorphic to xla_hlo.const (generates the same result type), and so as you point out, we could reuse the conversion pattern hlo -> lhlo. Since this is really \"a terminal conversion\" (tensor operands of some ops get their values from std.constants) and that we can't generate constant memrefs from std.constant, I guess it's fine to have this conversion pattern live in hlo_legalize_to_lhlo. \r\n\r\n\r\n", "@bondhugula What \"other ways\" did you mean in `potentially be presented with such an input in other ways, and so making -hlo-to-mlir-hlo generate hlo.const isn't a real solution. `?", "> @bondhugula What \"other ways\" did you mean in `potentially be presented with such an input in other ways, and so making -hlo-to-mlir-hlo generate hlo.const isn't a real solution. `?\r\n\r\nAn IR generator can choose to generate std.constant or you can end up with such an input through a composition of other transformations/rewrites - there isn't really a reason to not handle std.constant in conjunction given that they are isomorphic for this part with hlo.constant. For consistency and completeness, I think we definitely should.\r\n\r\nOn a general note, supporting mixing with lower level dialects that a dialect already depends on (std in this case) is I think important for flexibility and completeness - or else things become unnecessarily rigid/restricted on how you can mix and build transformations.\r\n", "I agree about mixing, I don't understand the comment about `IR generator`. Aren't we in control of what an IR generator can \"choose\" to generate? Why can't it generate xla_hlo.constant? Then we can lower it to xla_lhlo.constant? Going std.constant -> xla_lhlo.const -> std.constant + std.store is a bit weird.", "> I agree about mixing, I don't understand the comment about `IR generator`. Aren't we in control of \r\n\r\nWhat I meant was that you can't really control what input the pass will receive. The IR is valid and the mix makes sense and supporting such a mix is pretty important.\r\n\r\n> what an IR generator can \"choose\" to generate? Why can't it generate xla_hlo.constant? Then we \r\n\r\nThe tf-mlir-translate -hlo-to-mlir-hlo isn't the only thing creating input for this conversion pass. You can't choose where you can get input from! :-)\r\n\r\n>  Going std.constant -> xla_lhlo.const -> std.constant + std.store is a bit weird.\r\n\r\nIf std.constant can be presented as input to this pass, you have no choice but to deal with it here. It's converting std.constant to hlo.const in this pass is what would be weird! :-)\r\n\r\n", "If std.constant (and other ops) are presented in a mix with HLO, we can add this pattern separately after calling `populateHLOToLHLOConversionPattern`. I don't think this pattern belongs to HLO->LHLO patterns. @joker-eph are there some recommendations on what patterns should be a part of Dialect 1 -> Dialect 2 conversion and what patterns have to be appended separately?", "@bondhugula strictly speaking this pattern is STD-> LHLO. It might be better to have it in std_legalize_to_lhlo.cc or maybe in the same file with a custom pass implementation where it is actually used. \r\nMaking `tf-mlir-translate -hlo-to-mlir-hlo` pipeline generate xla_hlo.constants would also make more sense, I think.", "> Making `tf-mlir-translate -hlo-to-mlir-hlo` pipeline generate xla_hlo.constants would also make more sense, I think.\r\n\r\nThis should be done anyway I think, but I don't see this versus the other part of the argument. It's not a question of choosing between the two: you need both independently. As I said, there is really no reason to not separately have std.constant -to lhlo -- the former is tensor value semantics (no way to create constant memory via std.constant), and the latter provides memory semantics. std.constant may not be a HLO op but it does provides values at the leaves to xla_hlo ops. So, std.constant -> lhlo.const is indeed a lowering that is complementary to have within hlo to lhlo. hlo already depends on std - so there is no additional dependence as well FWIW.\r\n\r\n\r\n\r\n", "@joker-eph @MaheshRavishankar Need your input here - please see comment history above as well as the PR. https://github.com/tensorflow/tensorflow/pull/40120 Thanks!", "I dont really work on the HLO -> LHLO conversion parts, but I agree with what @bondhugula  is saying here. Unless you can gaurantee that the input is going to always have `xla_hlo.const` instead of `std.constant` you need a way to convert `std.constant` to `xla_lhlo.const` (similar to how `xla_hlo.const` is converted to `xla_lhlo.const`). I think @pifon2a  is not opposed to adding such a pattern. The question might be just about file organization. I have no comments on that either :) \r\n\r\nJust to provide context in IREE. I eventually want to use the HLOToStd conversion pass (I dont know off the top of my head where it lives), so that we dont duplicate such patterns and bias towards using Standard Dialect -> ... conversions. This obviously not what you would want for TF. Maybe you can do the reverse. Add a Standard to HLO conversion so that you dont replicate code (this is a one line using DRR). Really dont see either approaches being better than the other.", "> Standard Dialect -> ... conversions. This obviously not what you would want for TF. Maybe you can do the reverse. Add a Standard to HLO conversion so that you dont replicate code (this is a one line using DRR). Really dont see either approaches being better than the other.\r\n\r\nstd.const -> hlo.const would be weird since you are going from a base/lower dialect to a higher-level domain-specific one with both having tensor value semantics. :) OTOH, since lhlo has memory/buffer semantics, std.const -> lhlo.const is seen as a lowering (values to memory) that can live in hlo->lhlo. That's the reason I didn't do a std.const -> hlo.const - I don't think we have any conversion in such a direction.\r\n", "I think the underlying problem here is that std does not have the concept of constants on memrefs. That is an omission as every user of std will run into this when lowering tensors to memrefs. Transforming std.const to lhlo.const is just hiding this omission by escaping to LHLO.\r\n\r\nI would also argue that the TF legalization should produce an HLO constant so that its output is closed in HLO. This should also help with the export to proto (or does that have a rule for std.const to hlo.const?).\r\n\r\nLastly, if you insist to have this pattern, it should at least live in a different file. Putting it into hlo to lhlo would hide the fact that the pattern transforms part of std and maybe not all users want that lowering to happen.", "> I think the underlying problem here is that std does not have the concept of constants on memrefs. That is an omission as every user of std will run into this when lowering tensors to memrefs. Transforming std.const to lhlo.const is just hiding this omission by escaping to LHLO.\r\n\r\nAs you know, memrefs aren't necessarily contiguous and can have arbitrary affine layouts, striding, padding, and so an op that stores a constant to a memref just cannot live in the std dialect! If it did, its valid lowering for codegen would have to again pass through loops, which are themselves not in the std dialect. So, I don't see the argument that the underlying problem is that of a memref constant store/assignment/filling in the std dialect. Given the nuances with memrefs, such a constant filling op cannot live in std, but only live in a dialect meant for dense tensors/tcp/linalg/lhlo etc. OTOH, tensors are value types and one can argue for having constant ops defining them in std. Transforming std.const to lhlo.const isn't thus a substitute for something missing: if const tensor -> memref is where you are going, you either need lhlo or linalg, and I see this conversion as best fitting the hlo to lhlo lowering.\r\n \r\n> I would also argue that the TF legalization should produce an HLO constant so that its output is closed in HLO. This should also help with the export to proto (or does that have a rule for std.const \r\n\r\nI'm not sure what's to argue for here. TF legalization should produce HLO constant and that's an  independent issue to be fixed separately. As I already mentioned a couple of times above, I don't think we should choose to not deal with std.constant when it's presented to this pass.\r\n\r\n> \r\n> Lastly, if you insist to have this pattern, it should at least live in a different file. Putting it into hlo to lhlo would hide the fact that the pattern transforms part of std and maybe not all users want that lowering to happen.\r\n\r\nI don't see the reasoning here. There is an independent discussion that's going on in the thread below on why forcing 0-d memrefs to model the \"functional\" regions of lhlo ops (like reduce/reduce_window, select_and_scatter, etc.) isn't the right design choice, and it's certainly going in the direction of actually using std dialect ops inside those regions since you'd just want to express those computations on elemental types and not force 0-d memrefs.\r\nhttps://groups.google.com/a/tensorflow.org/d/msg/mlir/Ip55os0xgfU/56WXd3ZxAwAJ\r\n\r\nSo std dialect ops are going to be in the legal target set for these conversions - hlo to lhlo, and already lhlo to linalg. This whole idea of not letting std dialect ops mix with lhlo would only hurt transformations and conversions out of the dialect. You'll be forced to spill to memory when you don't need to and use fake 0-d memrefs for scalars unnecessarily.", "> > I think the underlying problem here is that std does not have the concept of constants on memrefs. That is an omission as every user of std will run into this when lowering tensors to memrefs. Transforming std.const to lhlo.const is just hiding this omission by escaping to LHLO.\r\n> \r\n> As you know, memrefs aren't necessarily contiguous and can have arbitrary affine layouts, striding, padding, and so an op that stores a constant to a memref just cannot live in the std dialect! If it did, its valid lowering for codegen would have to again pass through loops, which are themselves not in the std dialect. \r\n\r\nHaving a `std.const` on memrefs in the standard dialect would not imply any specific lowering. You could even have a version that allocates the buffer if you wanted. In either case, the lowering of such a memref constant is independent of the semantics. You could decide to implement `std.const` by lowering via linalg or going to `hlo.cost`. \r\n\r\n> > Lastly, if you insist to have this pattern, it should at least live in a different file. Putting it into hlo to lhlo would hide the fact that the pattern transforms part of std and maybe not all users want that lowering to happen.\r\n> \r\n> I don't see the reasoning here. There is an independent discussion that's going on in the thread below on why forcing 0-d memrefs to model the \"functional\" regions of lhlo ops (like reduce/reduce_window, select_and_scatter, etc.) isn't the right design choice, and it's certainly going in the direction of actually using std dialect ops inside those regions since you'd just want to express those computations on elemental types and not force 0-d memrefs.\r\n\r\nI would also disagree with that choice (using standard ops). Lets have that discussion there though.\r\n\r\n> So std dialect ops are going to be in the legal target set for these conversions - hlo to lhlo, and already lhlo to linalg. This whole idea of not letting std dialect ops mix with lhlo would only hurt transformations and conversions out of the dialect. You'll be forced to spill to memory when you don't need to and use fake 0-d memrefs for scalars unnecessarily.\r\n\r\nWe have to separate two concerns here. During lowering, we gradually introduce std/linalg/whatever operations into the HLO/LHLO mix. That is different from the original form that we generate though. Coming from TensorFlow, I would expect we legalize to HLO (without standard in the mix), including the nested regions of reductions. One reason for this is the easier transition back to legacy systems.\r\n\r\nAlso, I am fine with adding this pattern from `std.const` to `hlo.const` but it should not be part of the HLO to LHLO set of patterns because that is not what it does.\r\n\r\n\r\n\r\n", "> Also, I am fine with adding this pattern from std.const to hlo.const \r\n\r\nThis makes sense to me as well: seems like a good application of transitive lowering here, isn't it?\r\n", "> > Also, I am fine with adding this pattern from std.const to hlo.const\r\n> \r\n> This makes sense to me as well: seems like a good application of transitive lowering here, isn't it?\r\n\r\nBut where would you have this pattern live? I'm completely fine with this as long as this pattern can run as part of -hlo-legalize-to-lhlo. If not, it's weirder to have to run a separate pass do std to hlo, and I don't see how this solves the original issue. I would then prefer having std.const to lhlo.const in the hlo to lhlo conversion which was the original proposal.", ">  If not, it's weirder to have to run a separate pass do std to hlo, and I don't see how this solves the original issue\r\n\r\nWhy doesn't it solve the original issue? If you're pipeline is always including -std-to-hlo before -hlo-to-lhlo shouldn't this just work then?\r\n\r\nNote that in practice in a real pipeline, one would include all the patterns to legalize to HLO, include the STD->HLO, and legalize there as a single pass before running the LHLO buffer alloc.", "> > If not, it's weirder to have to run a separate pass do std to hlo, and I don't see how this solves the original issue\r\n> \r\n> Why doesn't it solve the original issue? If you're pipeline is always including -std-to-hlo before -hlo-to-lhlo shouldn't this just work then?\r\n\r\nThe original issue was of making the -hlo to lhlo pass deal with this input. \r\n\r\n> \r\n> Note that in practice in a real pipeline, one would include all the patterns to legalize to HLO, include the STD->HLO, and legalize there as a single pass before running the LHLO buffer alloc.\r\n\r\nA new pass std to hlo just for this?! What other conversions do you foresee that to have? The conversion looks backwards intuitively.\r\n", "> The original issue was of making the -hlo to lhlo pass deal with this input.\r\n\r\nSuch passes are mostly testing passes, they can't support the world. This is the case with most passes that are loading A->B conversion pattern right now. If you want A+C->B you need another pass loading the right set of patterns. I don't think there is anything specific to this pass right now.\r\n\r\n> The conversion looks backwards intuitively.\r\n\r\nWhy? A lot of `std` dialect op can be seen as the same level of abstraction as HLO, using the existing XLA compiler to target TPU from a tensor-level computation involving `std` ops could be useful to someone for example.\r\nConversion don't need to be one-way: for example we have a conversion from TF->HLO and from HLO->TF.", "> > The original issue was of making the -hlo to lhlo pass deal with this input.\r\n> \r\n> Such passes are mostly testing passes, they can't support the world. This is the case with most passes that are loading A->B conversion pattern right now. If you want A+C->B you need another pass loading the right set of patterns. I don't think there is anything specific to this pass right now.\r\n> \r\n> > The conversion looks backwards intuitively.\r\n> \r\n> Why? A lot of `std` dialect op can be seen as the same level of abstraction as HLO, using the existing XLA compiler to target TPU from a tensor-level computation involving `std` ops could be useful to someone for example.\r\n> Conversion don't need to be one-way: for example we have a conversion from TF->HLO and from HLO->TF.\r\n\r\nTF -> HLO and HLO -> TF is a different scenario - I assume you have many conversions there. Creating one full pass and file for a std.const to hlo.const sounds needless, and more importantly,  getting back to the main question: what's the issue with having std.const to hlo.const in hlo to lhlo? It's just an intermediate step and would anyway converge at lhlo.const. Why shouldn't one have such trivial intermediate patterns for canonicalization or to facilitate / trigger other conversions towards your legal target?\r\n", "Bringing this back up: where should the conversion for std.const to hlo.const live? If we do this as a conversion pattern in HLO to LHLO, wouldn't it fail? (since the xla_hlo op generated out of the pattern would have been marked as illegal for the conversion target, making the dialect conversion reset state). Could one add an op rewrite pattern as part of a dialect conversion? I completely forget whether it would make sense (in spite of having reviewed a recent revision updating the documentation of pattern rewriting and dialect conversion).\r\n\r\n@joker-eph @sherhut ", "What is the status of this?"]}, {"number": 39862, "title": "sys.exc_info() can't put to queue in MultiProcessRunner", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):none\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:none\r\n- TensorFlow installed from (source or binary):none\r\n- TensorFlow version (use command below): master branch https://github.com/tensorflow/tensorflow/commit/498e5b4f6db80df13b54c44cbd657a2750067564\r\n- Python version:nan\r\n- Bazel version (if compiling from source):none\r\n- GCC/Compiler version (if compiling from source):none\r\n- CUDA/cuDNN version:none\r\n- GPU model and memory:none\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\ncode line see [multi_process_runner.py:594](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/multi_process_runner.py#L594)\r\n\r\nsys.exc_info() can't put to queue in MultiProcessRunner, because the traceback object can't be pickle, see [stackoverflow](https://stackoverflow.com/a/6132584).\r\n\r\nwhen a subprocess raise an error, the line 497 of multi_process_runner.py will throw an error\r\n`TypeError: can't pickle traceback objects`\r\n\r\n**Describe the expected behavior**\r\n\r\nPls use other way to implement this. one way is using traceback.print_exc() or format_exc() functions. Don't put traceback objects in queue.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nnone\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nnone", "comments": ["@Hacky-DH \r\n\r\nRequest you to provide colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "I rewrite the MultiProcessRunner class, so it can run any where, not only in unittest.\r\n\r\nSee [link](https://github.com/Hacky-DH/learn/blob/master/tools/multi_process_runner.py#L200).\r\n\r\nI use `traceback.print_exc()` fix this.\r\n\r\nI wrote an unittest scripte, but skipped:\r\nskipped 'TODO(b/141874796): Implement OSS version of `multi_process_lib`'"]}, {"number": 39857, "title": "device_spec from_string method not accepting some inputs", "body": "The method `from_string` of `tensorflow.python.framework.device_spec` does not accept device names reported by `tf` itself:\r\n\r\n`tf.config.list_physical_devices()` returns device names of the form `'/physical_device:GPU:0'`, However `from_string` does not accept this as input. Only works when `'/physical_device:'` is cut out.\r\n\r\n**Expected behaviour:**\r\n```python\r\ntensorflow.python.framework.device_spec.DeviceSpecV2().from_string('/physical_device:GPU:0')\r\n<tensorflow.python.framework.device_spec.DeviceSpecV2 at 0x7f7ec46fe8a0>\r\n```\r\n\r\n**Actual behaviour:**\r\n```python\r\ntensorflow.python.framework.device_spec.DeviceSpecV2().from_string('/physical_device:GPU:0')\r\nValueError: Unknown attribute: 'physical_device' in '/physical_device:GPU:0'\r\n```", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Please, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).Request you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "system: Linux 4.15.0-99-generic #100~16.04.1-Ubuntu x86_64 GNU/Linux\r\ntensorflow-gpu v2.2.0 installed via pip\r\nPython 3.7.4 (default, Aug 13 2019, 20:35:49)                                                                                                                              [GCC 7.3.0] :: Anaconda, Inc. on linux  \r\n\r\nMinimal example:\r\n```python\r\ntf.python.framework.device_spec.DeviceSpecV2().from_string(tf.config.list_physical_devices()\r\n[0].name)\r\n```\r\n```\r\nTraceback (most recent call last):\r\nFile \"<stdin>\", line 1, in <module>\r\nFile \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/device_spec.py\", line 157, in from_string\r\nreturn cls(*cls._string_to_components(spec))\r\nFile \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/device_spec.py\", line 344, in _string_to_components\r\nraise ValueError(\"Unknown attribute: '%s' in '%s'\" % (y[0], spec))\r\nValueError: Unknown attribute: 'physical_device' in '/physical_device:CPU:0'\r\n```", "@jaingaurav do you want to take this?", "Hi,\r\nI encountered the same issue. My workaround is to use `tf.config.list_logical_devices()` instead of   `tf.config.list_physical_devices()`  but I am not really satisfied.\r\nAny news about this problem?\r\nThanks!", "@philippeller  Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi, looks to me as this is still the same issue in tf 2.5:\r\n```python\r\n>>> tf.DeviceSpec().from_string(tf.config.list_physical_devices()[0].name)                                                                                                                                                                    \r\nraise ValueError(\"Unknown attribute: '%s' in '%s'\" % (y[0], spec))\r\nValueError: Unknown attribute: 'physical_device' in '/physical_device:CPU:0'\r\n>>> tf.__version__\r\n'2.5.0'\r\n```"]}, {"number": 39839, "title": "Passing initial_epoch parameter to callbacks' self.params in tf.keras.model.fit", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.x\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTensorFlow Addons are developing a progress bar (TQDM Progress bar) and we receive and issue where when user specify `initialEpoch > 0` during `model.fit` and when using `tfa.callbacks.TQDMProgressBar` the progress bar will never reach 100% because in `TQDMProgressBar` code we never consider the case when user set `initialEpoch`\r\n\r\n`\r\nmodel.fit(x=X, y=y, class_weight=None, batch_size=batchSize, verbose=0, callbacks=tfa.callbacks.TQDMProgressBar(), validation_split=0.2, shuffle=True, epochs=epochCount, initial_epoch=initialEpoch)\r\n`\r\n\r\nhttps://github.com/tensorflow/addons/issues/1748 \r\n\r\nWhile searching for a solution, I noticed that `initial_epoch` is never passed to Callback's `self.params` dictionary and thus making it hard to implement the feature where users set an initial epoch, thus I am asking if it would make sense for TensorFlow to pass `initial_epoch` to Callback's `self.params` for us to implement that feature. The other way around would be to ask user to specify `initial_epoch` again in the progress bar but that would not be ideal. Thank you so much for your time!\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo, but this will add another key into Callbacks' `self.params` dictionary.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers of TensorFlow Addons TQDM Progress Bar and thus that may need `initial_epoch` in their custom callbacks. \r\n\r\n**Any Other info.**\r\n\r\nTensorFlow Addons TQDM Progress Bar Source code: \r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/callbacks/tqdm_progress_bar.py\r\n", "comments": []}, {"number": 39833, "title": "Hashing functions for tf.string", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): depending on the time commitment\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI want to apply SHA-512 to a `tf.string`, so this is what I'm doing\r\n```python\r\ndef convert_to_hex_dig(input, length):\r\n  return hashlib.sha512(\r\n      input.numpy().decode('utf-8').encode('utf-8')\r\n  ).hexdigest()[0:length]\r\n\r\ncurrent_key = tf.py_function(\r\n  convert_to_hex_dig,\r\n  [mystr, int(mystr_len)],\r\n  Tout=tf.string\r\n)\r\n```\r\n\r\n**Will this change the current API? How?**\r\nA new method (or methods) will be added to `tf.strings` for SHA-512 hashing. Other methods may include other types of hashing. Alternatively, a TF addon could be created for hashing/encrypting `tf.string`s. \r\n\r\n**Who will benefit with this feature?**\r\nUsers who use TensorFlow for research relating to encryption systems.\r\n\r\n**Any Other info.**\r\nPython's `hashlib.sha512` source is here: https://github.com/python/cpython/blob/1ae035b7e847064d09df01ca62b8a761e9b5aae3/Modules/sha512module.c", "comments": []}, {"number": 39820, "title": "tf.queue.FIFOQueue throws NotFoundError on enqueue / dequeue if created in graph mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Windows 7 and Ubuntu 18.04.3 LTS (colab)\r\n- TensorFlow installed from (source or binary): pip install tf-nightly-gpu\r\n- TensorFlow version (use command below):    v1.12.1-32502-g2544e4e277 2.3.0-dev20200523 (Windows and colab)\r\n- Python version: 3.6.6 (Windows), 3.6.9 (colab)\r\n- CUDA/cuDNN version: 10.1/7.6.3.30 (Windows), unknown (colab)\r\n- GPU model and memory: NVidia 1080Ti ~11GB (Windows), unknown (colab)\r\n\r\n**Describe the current behavior**\r\n\r\nWhen creating a tf.FIFOQueue inside a tf.function decorated function, enqueueing / dequeueing tensors from the queue fails with a NotFoundError mentioning a non-existing resource of name \"localhost/{Some number}/{C++ mangled name of class tensorflow::QueueInterface}\"\r\n\r\nAll operations work fine if the queue is created in eager mode.\r\n\r\n**Describe the expected behavior**\r\n\r\nEither the graph mode execution should succeed just like in eager mode, or the unsuitability of queue creation for graph mode should be documented as API.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nLinux case:\r\n\r\nhttps://colab.research.google.com/drive/1OQ68ibI-9u-6f4nslwIzDuMzdztxVGwX?usp=sharing\r\n\r\n\r\nWindows case:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef foo():\t\r\n\tqueue = tf.queue.FIFOQueue(-1, [tf.string], tf.TensorShape([]))\r\n\tqueue.enqueue('Hello')\r\n\ts = queue.dequeue()\r\n\ttf.print(s)\r\n\r\nfoo()\r\n```\r\n\r\nLog is attached\r\n\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/4672215/log.txt)", "comments": ["I have tried in colab with TF - GPU nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a2e6523f42fee5297dfb2a1ed03457f7/untitled33.ipynb).Thanks!\r\n", "Currently you need to hold on to the Python object that owns the queue resource outside of the function, like this: https://github.com/tensorflow/tensorflow/blob/b1a712d40d67a7a9f88d6e2f5f5fe28fa10c7f1e/tensorflow/python/kernel_tests/fifo_queue_test.py#L145-L157\r\n\r\nIt'd be good if we threw an error message, or alternatively let the function create and destroy a transient queue resource every time it's run, so I'm leaving the bug open,", "I tried to reproduce the issue in latest TF version 2.5 and the issue still exists.Thanks!"]}, {"number": 39798, "title": "Unable to log scalar summaries in XLA", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nForcing XLA compilation via `experimental_compile=True` in a `tf.function` raises `tensorflow.python.framework.errors_impl.InvalidArgumentError` when the function contains `tf.summary.scalar` (I haven't tried other summaries). \r\n\r\n**Describe the expected behavior**\r\nPassing `experimental_compile=True` should log the scalar without raising any errors. \r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function(\r\n    experimental_compile=True,\r\n)\r\ndef test_summaries():\r\n    tf.summary.scalar('testing', 12.3)\r\n\r\nwith tf.summary.create_file_writer('./logs').as_default():\r\n    tf.summary.experimental.set_step(0)\r\n    test_summaries()\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2020-05-22 22:30:57.072264: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-22 22:30:57.091548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f99eef80060 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-22 22:30:57.091590: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"testsum.py\", line 12, in <module>\r\n    test_summaries()\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 576, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 650, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\r\n    return self._call_flat(\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\r\n    outputs = execute.execute(\r\n  File \"/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_test_summaries_22}} = __inference_test_summaries_22[_XlaMustCompile=true, config_proto=\"\\n\\007\\n\\003GPU\\020\\000\\n\\007\\n\\003CPU\\020\\0012\\002J\\0008\\001\", executor_type=\"\"](dummy_input).\r\nUncompilable nodes:\r\ntesting/write_summary/tag: unsupported op: Const op with type DT_STRING is not supported by XLA.\r\n\tStacktrace:\r\n\t\tNode: __inference_test_summaries_22, function: \r\n\t\tNode: testing/write_summary/tag, function: __inference_test_summaries_22\r\n\r\ntesting/write_summary/summary_metadata: unsupported op: Const op with type DT_STRING is not supported by XLA.\r\n\tStacktrace:\r\n\t\tNode: __inference_test_summaries_22, function: \r\n\t\tNode: testing/write_summary/summary_metadata, function: __inference_test_summaries_22\r\n\r\ntesting/write_summary: unsupported op: No registered 'WriteSummary' OpKernel for XLA_CPU_JIT devices compatible with node {{node testing/write_summary}}\r\n\tStacktrace:\r\n\t\tNode: __inference_test_summaries_22, function: \r\n\t\tNode: testing/write_summary, function: __inference_test_summaries_22\r\n [Op:__inference_test_summaries_22]\r\n```\r\n", "comments": ["@sumanthratna \r\n\r\nI have tried in colab with TF version 2.2.0 , nightly version and was able to reproduce the issue.Please, find the gist here.If we comment `@tf.function ` or if we give compile `experimental_compile=False` then i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ef8795cbdefbf1b9c52caec7cb0e7474/untitled32.ipynb).Thanks!", "Was able to replicate the issue in TF v2.7.0,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/98cbf0f8469f3119a123a2129985fd08/untitled321.ipynb#scrollTo=QrtlWMG9cneJ)..Thanks!"]}]