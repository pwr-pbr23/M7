[{"number": 44207, "title": "Unable to import tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):OS X\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip install tensorflow\r\n- TensorFlow version:2.0.1\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nUnable to import tensorflow\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npython -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n\r\n**Any other info / logs:\r\n\r\n(base) [deeplearning]$ python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\nTraceback (most recent call last):\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /Users/username/anaconda3/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.2.dylib\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n(base) [deeplearning]$ \r\n", "comments": ["@wggbullet,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!\r\n", "> @wggbullet,\r\n> Installation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n> \r\n> Could you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!\r\n\r\nI am confused. The link you provided says report issue with anaconda. Does that seem right? ", "@wggbullet,\r\n >I am confused. The link you provided says report issue with anaconda. Does that seem right?\r\n\r\nFrom the error log you have posted, looks like you are facing issues within the Anaconda environment. Hence, I have linked the appropriate repo for the issue.", "In this case, I'd suggest you to exit the Anaconda environment using the below command. \r\n```\r\nconda deactivate\r\n```\r\nRefer these links for more information: [guide #1](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#deactivating-an-environment), [guide #2](https://problemsolvingwithpython.com/99-Appendix/99.03-Virtual-Environments/#deactivate-a-virtual-environment).\r\n\r\n\r\nAnd then follow the [pip installation guide](https://www.tensorflow.org/install/pip#macos) to install TensorFlow without Anaconda. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44207\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44207\">No</a>\n"]}, {"number": 44206, "title": "TensorFlow on Dell XPS 9300", "body": "I recently purchased a new laptop, and after setting up the environment on pycharm and running a test code i get several warning messages:\r\n\r\n```\r\n2020-10-21 16:03:00.527974: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nFound 24958 images belonging to 2 classes.\r\nFound 2600 images belonging to 2 classes.\r\n2020-10-21 16:03:01.695574: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-21 16:03:03.773517: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-10-21 16:03:03.773567: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fabio-XPS): /proc/driver/nvidia/version does not exist\r\n2020-10-21 16:03:03.774077: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-21 16:03:03.802935: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1497600000 Hz\r\n2020-10-21 16:03:03.803743: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6270580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-21 16:03:03.803791: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n```\r\nwhat am i missing?\r\n\r\nthis is the system configuration\r\n>                           system         XPS 13 9300 (096D)\r\n> /0                        bus            077Y9N\r\n> /0/1                      memory         1MiB BIOS\r\n> /0/400                    processor      Intel(R) Core(TM) i7-1065G7 CPU @ 1.30GHz\r\n> /0/400/701                memory         128KiB L1 cache\r\n> /0/400/702                memory         2MiB L2 cache\r\n> /0/400/703                memory         8MiB L3 cache\r\n> /0/700                    memory         192KiB L1 cache\r\n> /0/1000                   memory         16GiB System Memory\r\n> /0/1000/0                 memory         8GiB Row of chips LPDDR4 Synchronous 4267 MHz (0.2 ns)\r\n> /0/1000/1                 memory         8GiB Row of chips LPDDR4 Synchronous 4267 MHz (0.2 ns)\r\n> /0/100                    bridge         Intel Corporation\r\n> /0/100/2       /dev/fb0   display        Iris Plus Graphics G7\r\n> /0/100/4                  generic        Intel Corporation\r\n> /0/100/7                  bridge         Ice Lake Thunderbolt 3 PCI Express Root Port #0\r\n> /0/100/7.2                bridge         Ice Lake Thunderbolt 3 PCI Express Root Port #2\r\n> /0/100/d                  bus            Ice Lake Thunderbolt 3 USB Controller\r\n> /0/100/d/0     usb1       bus            xHCI Host Controller\r\n> /0/100/d/1     usb2       bus            xHCI Host Controller\r\n> /0/100/d.2                generic        Ice Lake Thunderbolt 3 NHI #0\r\n> /0/100/d.3                generic        Ice Lake Thunderbolt 3 NHI #1\r\n> /0/100/12                 communication  Intel Corporation\r\n> /0/100/14                 bus            Ice Lake-LP USB 3.1 xHCI Host Controller\r\n> /0/100/14/0    usb3       bus            xHCI Host Controller\r\n> /0/100/14/0/4             input          USB Optical Mouse\r\n> /0/100/14/0/5             generic        FingerPrint\r\n> /0/100/14/0/9             multimedia     Integrated_Webcam_HD\r\n> /0/100/14/0/a             communication  Bluetooth wireless interface\r\n> /0/100/14/1    usb4       bus            xHCI Host Controller\r\n> /0/100/14.2               memory         RAM memory\r\n> /0/100/14.3    wlp0s20f3  network        Killer Wi-Fi 6 AX1650i 160MHz Wireless Network Adapter (201NGW)\r\n> /0/100/15                 bus            Ice Lake-LP Serial IO I2C Controller #0\r\n> /0/100/15.1               bus            Ice Lake-LP Serial IO I2C Controller #1\r\n> /0/100/16                 communication  Management Engine Interface\r\n> /0/100/1d                 bridge         Ice Lake-LP PCI Express Root Port #9\r\n> /0/100/1d/0               storage        SK hynix\r\n> /0/100/1d.7               bridge         Intel Corporation\r\n> /0/100/1d.7/0             generic        RTS525A PCI Express Card Reader\r\n> /0/100/1e                 communication  Ice Lake-LP Serial IO UART Controller #0\r\n> /0/100/1f                 bridge         Ice Lake-LP LPC Controller\r\n> /0/100/1f.3               multimedia     Smart Sound Technology Audio Controller\r\n> /0/100/1f.4               bus            Ice Lake-LP SMBus Controller\r\n> /0/100/1f.5               bus            Ice Lake-LP SPI Controller\r\n> /0/0                      system         PnP device PNP0c02\r\n> /0/2                      system         PnP device PNP0b00\r\n> /0/3                      generic        PnP device INT3f0d\r\n> /0/4                      input          PnP device PNP0303\r\n> /0/5                      generic        PnP device DLL096d\r\n> /0/6                      system         PnP device PNP0c02\r\n> /0/7                      system         PnP device PNP0c02\r\n> /0/8                      system         PnP device PNP0c02\r\n> /0/9                      system         PnP device PNP0c02\r\n> /1                        power          DELL 2XXFW05\r\n> /2                        power          To Be Filled by O.E.M.\r\n\r\n\r\n> device_lib.list_local_devices()\r\n> [name: \"/device:CPU:0\"\r\n> device_type: \"CPU\"\r\n> memory_limit: 268435456\r\n> locality {\r\n> }\r\n> incarnation: 322266644154681505\r\n> , name: \"/device:XLA_CPU:0\"\r\n> device_type: \"XLA_CPU\"", "comments": ["@fabiogeraci \r\nPlease refer to [this link](https://www.tensorflow.org/install/source#gpu), if you wants GPU.\r\nif not GPU, ignore the errors.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44205, "title": "Unable to Convert Faster RCNN to TF Lite model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- TensorFlow installed from (source or binary): pip package\r\n- TensorFlow version (or github SHA if from source): 2.3.1\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_path)\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\shehr\\anaconda3\\envs\\nn_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 199, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"C:\\Users\\shehr\\anaconda3\\envs\\nn_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:/TensorFlow/models/to_tflite.py\", line 4, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\shehr\\anaconda3\\envs\\nn_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"C:\\Users\\shehr\\anaconda3\\envs\\nn_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 900, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"C:\\Users\\shehr\\anaconda3\\envs\\nn_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 633, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\shehr\\anaconda3\\envs\\nn_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 574, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\shehr\\anaconda3\\envs\\nn_gpu\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 202, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>\r\n```\r\n\r\n\r\nM trying to convert my custom trained Faster RCNN model to TF Lite model. Does TF Lite supports RCNN models? If not can you guys please add a list to supported model's list to the website so that one easily identify which model to use from the start. Currently I trained a model but I dont have any info which models are supported and which are not. Adding a list of models will save lot of for people.\r\n\r\n", "comments": ["@shehroz010 \r\n\r\nI tried in colab with TF nightly version(`2.4.0-dev20201021`) and was able to convert Faster RCNN model to TF lite. PLease, find the gist [here](https://colab.research.google.com/gist/ravikyram/2bc955491d4e4e426178164c23e737b4/untitled476.ipynb).Please,verify once and close the issue. Thanks!", "I see this is done using a nightly build. But my main question is does it also have a support in current stable release too? And also m using a tensorflow gpu of same version 2.3.1. So do i need to install nightly build of tf gpu too to make it work? M training the models using Object detection model zoo for object detection.", "@shehroz010 \r\n\r\nYou could use tf-nightly for now and in the next couple of months new stable version will be released. Please feel free to open the issue if it persists with the new version. Please, close the issue issue as the issue was resolved in nightly version.Thanks!", "@ravikyram ok thanks. It works fine with the nightly build."]}, {"number": 44204, "title": "Illegal instruction (core dumped)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint19.03\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am getting an error while importing the TensorFlow.\r\n```bash\r\n>>> import tensorflow\r\nIllegal instruction (core dumped)\r\n```\r\nI tried to downgrade the TensorFlow but still it's not working.\r\n\r\n**Describe the expected behaviour**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["> TensorFlow installed from (source or binary): source\r\n\r\nAre you compiling TF from source?", "@codePerfectPlus,\r\nCould you please provide the make and model of your CPU? Starting with v1.6, TensorFlow binaries use AVX instructions which may not run on older CPUs.\r\n\r\nAlso, please take a look at similar issues [#33038](https://github.com/tensorflow/tensorflow/issues/33038#issuecomment-538470776), [#29788](https://github.com/tensorflow/tensorflow/issues/29788#issuecomment-505571104) and let us know if it helps. Thanks!", "```\r\nSystem:    Host: Inspiron Kernel: 5.4.0-52-generic x86_64 bits: 64 compiler: gcc v: 7.5.0 \r\n           Desktop: Cinnamon 4.4.8 wm: muffin dm: LightDM Distro: Linux Mint 19.3 Tricia \r\n           base: Ubuntu 18.04 bionic \r\nMachine:   Type: Portable System: Dell product: Inspiron 3551 v: A00 serial: <filter> Chassis: \r\n           type: 8 serial: <filter> \r\n           Mobo: Dell model: 0CMF7W v: A00 serial: <filter> UEFI [Legacy]: Dell v: A00 \r\n           date: 12/05/2014 \r\nBattery:   ID-1: BAT0 charge: 24.2 Wh condition: 36.8/41.4 Wh (89%) volts: 15.6/14.8 \r\n           model: PANASONIC DELL 78V9D7C serial: <filter> status: Discharging \r\nCPU:       Topology: Quad Core model: Intel Pentium N3540 bits: 64 type: MCP arch: Silvermont \r\n           rev: 8 L2 cache: 1024 KiB \r\n           flags: lm nx pae sse sse2 sse3 sse4_1 sse4_2 ssse3 vmx bogomips: 17333 \r\n           Speed: 1061 MHz min/max: 500/2666 MHz Core speeds (MHz): 1: 754 2: 853 3: 758 4: 581 \r\nGraphics:  Device-1: Intel Atom Processor Z36xxx/Z37xxx Series Graphics & Display vendor: Dell \r\n           driver: i915 v: kernel bus ID: 00:02.0 chip ID: 8086:0f31 \r\n           Display: x11 server: X.Org 1.19.6 driver: modesetting unloaded: fbdev,vesa \r\n           resolution: 1366x768~60Hz \r\n           OpenGL: renderer: Mesa DRI Intel HD Graphics (BYT) v: 4.2 Mesa 20.0.8 compat-v: 3.0 \r\n           direct render: Yes \r\nAudio:     Device-1: Intel Atom Processor Z36xxx/Z37xxx Series High Definition Audio vendor: Dell \r\n           driver: snd_hda_intel v: kernel bus ID: 00:1b.0 chip ID: 8086:0f04 \r\n           Sound Server: ALSA v: k5.4.0-52-generic \r\nNetwork:   Device-1: Qualcomm Atheros QCA9565 / AR9565 Wireless Network Adapter vendor: Dell \r\n           driver: ath9k v: kernel port: f000 bus ID: 03:00.0 chip ID: 168c:0036 \r\n           IF: wlp3s0 state: up mac: <filter> \r\n           Device-2: Atheros type: USB driver: btusb bus ID: 2-2.1:6 chip ID: 0cf3:e005 \r\n           IF-ID-1: br-cb6c1f33c3f4 state: down mac: <filter> \r\n           IF-ID-2: docker0 state: down mac: <filter> \r\nDrives:    Local Storage: total: 111.79 GiB used: 95.80 GiB (85.7%) \r\n           ID-1: /dev/sda vendor: Kingston model: SA400S37120G size: 111.79 GiB speed: 3.0 Gb/s \r\n           serial: <filter> \r\nPartition: ID-1: / size: 109.53 GiB used: 95.80 GiB (87.5%) fs: ext4 dev: /dev/sda1 \r\nSensors:   System Temperatures: cpu: 45.0 C mobo: 45.0 C sodimm: 0.0 C \r\n           Fan Speeds (RPM): cpu: 2505 \r\nInfo:      Processes: 246 Uptime: 15m Memory: 3.73 GiB used: 1.44 GiB (38.6%) Init: systemd v: 237 \r\n           runlevel: 5 Compilers: gcc: 7.5.0 alt: 7 Client: Unknown python3.6 client inxi: 3.0.32\r\n```", "Check https://github.com/tensorflow/tensorflow/issues/24548", "@codePerfectPlus,\r\nAny updates regarding this? Please feel free to close the issue if resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44204\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44204\">No</a>\n"]}, {"number": 44203, "title": "Minor fixes for numpy api docs", "body": "", "comments": ["@53jk1 I repeat the plea to stop approving PRs if not asked for reviews. This will be the last time."]}, {"number": 44202, "title": "Allow unit-length Concatenate-layer input", "body": "Closes #23176", "comments": ["@fchollet @gbaned mind checking this again? I fixed the tests since your latest approval."]}, {"number": 44201, "title": "TFLite convert supporting problem for the OP 'tf.image.crop_and_resize' in 2.3.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["Windows 10; Tensorflow 1.14.0\r\nI sucessfully convert the model to a PB file and TFLite file. But exactly the OP: **tf.image.crop_and_resize** was not support when using tf.lite.Interpreter\r\nIs there any solution or alternatives? tf.image.crop_and_resize seems extremely important when implement Mask RCNN!", "@Zhuxinpei \r\nCan you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/24407#issuecomment-650140464) and let us know.\r\nPlease provide with complete stand alone code,and error log, please try on 2.x as we officially support 2.x and let us know if the issue persist on 2.x.", "I still can not convert the **tf.image.crop_and_resize** op to TFlite in tf-2.3.0 and tf-nighly-20201023.\r\n\r\nThe demo code as follow:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef roiAlign(feature, select_bbox):\r\n    roi_feature = tf.image.crop_and_resize(feature, select_bbox, box_indices=[0], crop_size=[7, 7])\r\n    return roi_feature\r\n\r\nfeature = tf.random.normal(shape=[1, 14, 14, 32])\r\nselect_bbox = tf.constant([[0.1, 0.1, 0.6, 0.6]], dtype=tf.float32)\r\nconcrete_func = roiAlign.get_concrete_function(feature, select_bbox)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\ntflite_model = converter.convert()\r\n```\r\n\r\nThe error log in tf-2.3.0 as following:\r\n\r\n```\r\n 2020-10-23 20:12:18.012602: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-10-23 20:12:18.012762: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-10-23 20:12:19.160621: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-10-23 20:12:19.177193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.8225GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-10-23 20:12:19.178061: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-10-23 20:12:19.178874: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-10-23 20:12:19.179553: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2020-10-23 20:12:19.180223: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2020-10-23 20:12:19.180892: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-10-23 20:12:19.181557: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2020-10-23 20:12:19.187045: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-10-23 20:12:19.187172: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-10-23 20:12:19.187716: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-23 20:12:19.193840: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1783ad992e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-23 20:12:19.194034: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-23 20:12:19.194206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-23 20:12:19.194327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \r\n2020-10-23 20:12:19.240442: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-10-23 20:12:19.240660: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-10-23 20:12:19.241214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.8225GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-10-23 20:12:19.242131: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-10-23 20:12:19.242838: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-10-23 20:12:19.243533: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2020-10-23 20:12:19.244210: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2020-10-23 20:12:19.244886: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-10-23 20:12:19.245564: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2020-10-23 20:12:19.245711: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-10-23 20:12:19.245827: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-10-23 20:12:19.314251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-23 20:12:19.314380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-23 20:12:19.314455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-23 20:12:19.315107: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1783b5c1880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-23 20:12:19.315259: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n2020-10-23 20:12:19.316168: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-10-23 20:12:19.316290: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2020-10-23 20:12:19.316418: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-10-23 20:12:19.323991: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-10-23 20:12:19.324102: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\nloc(callsite(\"CropAndResize\"(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py\":4066:0) at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\":201:0 at callsite(\"E:/Zhuxinpei/Code/RCNN-Pose/__anaconda_interpret__/test.py\":5:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\":969:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\":600:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\":986:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":3075:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":3213:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":2855:0 at \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\":697:0)))))))))): error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.CropAndResize {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 199, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:4066:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201:0: note: called from\r\nE:/Zhuxinpei/Code/RCNN-Pose/__anaconda_interpret__/test.py:5:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:969:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:600:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:986:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3075:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3213:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2855:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:697:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:4066:0: note: see current operation: %0 = \"tf.CropAndResize\"(%arg0, %arg1, %cst, %cst_0) {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"} : (tensor<1x14x14x32xf32>, tensor<1x4xf32>, tensor<1xi32>, tensor<2xi32>) -> tensor<1x7x7x32xf32>\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.CropAndResize {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\n<unknown>:0: note: see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<1x14x14x32xf32>, %arg1: tensor<1x4xf32>):  // no predecessors\r\n  %cst = \"std.constant\"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_0 = \"std.constant\"() {value = dense<7> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %0 = \"tf.CropAndResize\"(%arg0, %arg1, %cst, %cst_0) {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"} : (tensor<1x14x14x32xf32>, tensor<1x4xf32>, tensor<1xi32>, tensor<2xi32>) -> tensor<1x7x7x32xf32>\r\n  \"std.return\"(%0) : (tensor<1x7x7x32xf32>) -> ()\r\n}) {sym_name = \"main\", tf.entry_function = {control_outputs = \"\", inputs = \"feature,select_bbox\", outputs = \"Identity\"}, type = (tensor<1x14x14x32xf32>, tensor<1x4xf32>) -> tensor<1x7x7x32xf32>} : () -> ()\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:/Zhuxinpei/Code/RCNN-Pose/__anaconda_interpret__/test.py\", line 13, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 900, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 633, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 574, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 202, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:4066:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201:0: note: called from\r\nE:/Zhuxinpei/Code/RCNN-Pose/__anaconda_interpret__/test.py:5:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:969:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:600:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:986:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3075:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3213:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2855:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:697:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:4066:0: note: see current operation: %0 = \"tf.CropAndResize\"(%arg0, %arg1, %cst, %cst_0) {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"} : (tensor<1x14x14x32xf32>, tensor<1x4xf32>, tensor<1xi32>, tensor<2xi32>) -> tensor<1x7x7x32xf32>\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.CropAndResize {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\n<unknown>:0: note: see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<1x14x14x32xf32>, %arg1: tensor<1x4xf32>):  // no predecessors\r\n  %cst = \"std.constant\"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_0 = \"std.constant\"() {value = dense<7> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %0 = \"tf.CropAndResize\"(%arg0, %arg1, %cst, %cst_0) {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"} : (tensor<1x14x14x32xf32>, tensor<1x4xf32>, tensor<1xi32>, tensor<2xi32>) -> tensor<1x7x7x32xf32>\r\n  \"std.return\"(%0) : (tensor<1x7x7x32xf32>) -> ()\r\n}) {sym_name = \"main\", tf.entry_function = {control_outputs = \"\", inputs = \"feature,select_bbox\", outputs = \"Identity\"}, type = (tensor<1x14x14x32xf32>, tensor<1x4xf32>) -> tensor<1x7x7x32xf32>} : () -> ()\r\n\r\nProcess finished with exit code 1\r\n```\r\nThe error log in tf-nightly as following:\r\n```\r\n2020-10-23 20:17:00.596705: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2020-10-23 20:17:00.596865: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-10-23 20:17:02.004845: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-23 20:17:02.005488: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-10-23 20:17:02.021906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.8225GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-10-23 20:17:02.022775: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2020-10-23 20:17:02.023453: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\r\n2020-10-23 20:17:02.024125: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\r\n2020-10-23 20:17:02.024799: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2020-10-23 20:17:02.025462: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2020-10-23 20:17:02.026132: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-10-23 20:17:02.026802: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\r\n2020-10-23 20:17:02.027636: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\r\n2020-10-23 20:17:02.027776: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-10-23 20:17:02.028383: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-23 20:17:02.028968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-23 20:17:02.029100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \r\n2020-10-23 20:17:02.029183: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-23 20:17:02.063630: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-10-23 20:17:02.063860: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-10-23 20:17:02.064364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.8225GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-10-23 20:17:02.065325: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2020-10-23 20:17:02.066037: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\r\n2020-10-23 20:17:02.066702: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\r\n2020-10-23 20:17:02.067367: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2020-10-23 20:17:02.068037: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2020-10-23 20:17:02.068708: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-10-23 20:17:02.069580: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\r\n2020-10-23 20:17:02.070247: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\r\n2020-10-23 20:17:02.070387: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-10-23 20:17:02.131789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-23 20:17:02.131919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2020-10-23 20:17:02.131992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2020-10-23 20:17:02.132070: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-23 20:17:02.133057: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n\r\n2020-10-23 20:17:02.140262: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\r\n2020-10-23 20:17:02.140376: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\r\nloc(callsite(\"CropAndResize\"(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py\":4576:0) at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\":201:0 at callsite(\"E:/Zhuxinpei/Code/RCNN-Pose/__anaconda_interpret__/test.py\":5:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\":973:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\":634:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\":990:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":3206:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":3361:0 at callsite(\"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\":2969:0 at \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\":726:0)))))))))): error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.CropAndResize {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 213, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:4576:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201:0: note: called from\r\nE:/Zhuxinpei/Code/RCNN-Pose/__anaconda_interpret__/test.py:5:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:973:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:634:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:990:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3206:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3361:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2969:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:726:0: note: called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.CropAndResize {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"E:/Zhuxinpei/Code/RCNN-Pose/__anaconda_interpret__/test.py\", line 13, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1117, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 943, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 628, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 613, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 216, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:4576:0: error: 'tf.CropAndResize' op is neither a custom op nor a flex op\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201:0: note: called from\r\nE:/Zhuxinpei/Code/RCNN-Pose/__anaconda_interpret__/test.py:5:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:973:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:634:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:990:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3206:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3361:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2969:0: note: called from\r\nC:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:726:0: note: called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.CropAndResize {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\n\r\nException ignored in: <function Buckets.__del__ at 0x0000019D751B8438>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-nighly\\lib\\site-packages\\tensorflow\\python\\eager\\monitoring.py\", line 407, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nIt should be mentioned that I replace the **tf.image.crop_and_resize** with other fucntions in demo code such as **tf.image.crop_to_bounding_box**. Only **tf.image.crop_and_resize** raised error.\r\n\r\nAnthor thing is if add:\r\n`converter.allow_custom_ops = True\r\n`\r\nin tf-2.3.0, It would be successfully converted, but can not be interpreter with following code:\r\n```\r\ninterpreter_RoiAlign = tf.lite.Interpreter(model_path='C:\\\\Users\\\\hisense\\\\Desktop\\\\test.tflite')\r\ninterpreter_RoiAlign.allocate_tensors()\r\n```\r\nThe error log was:\r\n```\r\n2020-10-23 20:28:09.864957: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-10-23 20:28:09.865118: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"E:/Zhuxinpei/Code/RCNN-Pose/__anaconda_interpret__/test.py\", line 19, in <module>\r\n    interpreter_RoiAlign.allocate_tensors()\r\n  File \"C:\\Users\\hisense\\anaconda3\\envs\\Tensorflow-2-3\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py\", line 243, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\nRuntimeError: Encountered unresolved custom op: CropAndResize.Node number 0 (CropAndResize) failed to prepare.\r\n\r\nProcess finished with exit code 1\r\n```\r\n**Is there any problems of my code? Or just tf-2.3 do not support tf.image.crop_and_resize? Please help!**", "I am able to replicate the issue on tf nightly [2.4.0-dev20201023], please find the [gist here](https://colab.research.google.com/gist/Saduf2019/9ce3148778e771655beef7508e05b2ca/untitled450.ipynb)", "@ymodak @Saduf2019 Any news or reply?", "You may try to generate a TensorFlow Lite model using select TensorFlow ops.\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef roiAlign(feature, select_bbox):\r\n    roi_feature = tf.image.crop_and_resize(feature, select_bbox, box_indices=[0], crop_size=[7, 7])\r\n    return roi_feature\r\n\r\nfeature = tf.random.normal(shape=[1, 14, 14, 32])\r\nselect_bbox = tf.constant([[0.1, 0.1, 0.6, 0.6]], dtype=tf.float32)\r\nconcrete_func = roiAlign.get_concrete_function(feature, select_bbox)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS] # enable TensorFlow ops.\r\ntflite_model = converter.convert()\r\n```", "@ymodak Thanks for your information. I converted TFLite file successfully. \r\nBut the problem was that it only works in tf-nightly instead of tf-2.3.0, the code as :\r\n```\r\ninterpreter_RoiAlign = tf.lite.Interpreter(model_path = 'C:\\\\Users\\\\hisense\\\\Desktop\\\\test.tflite')\r\ninterpreter_RoiAlign.allocate_tensors()\r\n```\r\nIn tf-2.3.0 the error log was\uff1a\r\n```\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 0 (FlexCropAndResize) failed to prepare.\r\n```\r\nThe problem is when implement this TFLite file to Android, the lasted version of \"tensorflow-lite\" was 2.3.0 in app/build.gradle.\r\n```\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite: 2.3.0'\r\n}\r\n```\r\nhttps://bintray.com/google/tensorflow/tensorflow-lite\r\nSo, was there any solution or any tensorflow-lite in Android version could corresponding to tf-nightly? ", "We can expect this fix to be part of TF 2.4 stable version in upcoming release. \r\nYou may try TF Lite nightly runtime since we are converting model using Select-ops in tf-nightly.\r\n```\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    // This dependency adds the necessary TF op support.\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n}\r\n```\r\nSee https://www.tensorflow.org/lite/guide/ops_select#run_inference to know more.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44201\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44201\">No</a>\n"]}, {"number": 44200, "title": "TENSORFLOW 1.14 STYLEAGN 2 PERFORMANCE ISSUE ON RTX 3090 MULTIPLE GPU", "body": "I am running Stylegan 2 model on 4x RTX 3090 and it is taking a long time **to start up the training** than as in 1x RTX 3090. Although, as training starts, it gets finished up earlier in 4x than in 1x. I am using CUDA 11.1 and TensorFlow 1.14 in both the GPUs. \r\n\r\nSecondly, When I am using 1x RTX 2080ti, with CUDA 10.2 and TensorFlow 1.14, it is taking less amount **to start the training** as compared to 1x RTX 3090 with 11.1 CUDA and Tensorflow 1.14. Tentatively, it is taking 5 min in 1x RTX 2080ti, 30-35 minutes in 1x RTX 3090, and 1.5 hrs in 4x RTX 3090 **to start the training** for one of the dataset.\r\n\r\nI'll be grateful if anyone can help me to resolve this issue.\r\n\r\nI am using Ubuntu 16.04, Core\u2122 i9-10980XE CPU, and 32 GB ram both in 2080ti and 3090 machines.", "comments": ["@Thunder003 \r\n\r\nCan you please share the standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ravikyram check the code here:- https://github.com/NVlabs/stylegan2  \r\n\r\n> @Thunder003\r\n> \r\n> Can you please share the standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!\r\n\r\n", "Are you the building TensorFlow from source against those cuda versions (10.2 and 11.1)?\r\nIf you are you using pre built pip packages then I suspect your gpu is not being utilized for computing. Reason being that currently we ship TF (2.3) packages that support cuda 10.1.\r\nSee tested [build configurations](https://www.tensorflow.org/install/source_windows#gpu) to know more.\r\n\r\nOn a side note TF 1.14 is out of the support window, you may want to try latest TF versions such as 2.3 which offers much better performance. Thanks!\r\n", "@ymodak as mentioned on Stylegan 2 GitHub (https://github.com/NVlabs/stylegan2) it is compatible with TF 1.14 and 1.15 only and 1.15 is not working for CUDA 11.1. As I am using Nvidia RTX 3090 which has Ampere GPU with CUDA 11.1 so the one possible issue could be as mentioned in https://www.tensorflow.org/install/gpu, can you please confirm whether this is the main problem and if, then can you suggest any solution for it?", "You should switch to TF 2.x. We no longer fix code on TF 1.x.", "@Thunder003 That's correct. Your configuration  is not using gpu computing power due incompatible cuda versions. We do not provide TF binaries that support cuda 11.1 at the moment.\r\nFor this you may try building TF from source yourself.\r\n\r\nOn a side note - Current `tf-nightly` version supports cuda 11.0 \r\nIf you are okay with unstable version (tf-nightly) you can give it a try or wait for upcoming stable TF 2.4 release.", "Thanks, @ymodak for your answer. I'm trying to build from source but getting an issue:-\r\n\r\nCould not find any cudnn.h matching version '8' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        '/lib'\r\n        '/lib/x86_64-linux-gnu'\r\n        '/usr'\r\n        '/usr/include/'\r\n        '/usr/include/cudnn.h'\r\n        '/usr/local/cuda'\r\n        '/usr/local/cuda-11.1'\r\n        '/usr/local/cuda-11.1/targets/x86_64-linux/lib'\r\nAsking for detailed CUDA configuration... \r\n\r\nI have checked that CUDNN has properly installed on /usr/include/cudnn.h path. Following [this](https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation#:~:text=Install%20CuDNN&text=For%20most%20people%2C%20it%20will,check%20it%20with%20which%20nvcc%20.) , I have copy-pasted a cudnn.h file to /usr/local/cuda/ and libcudnn*  CUDNN installation file to /usr/local/cuda.  Can you please tell me a solution for it. \r\n\r\nI am building TF 1.14 with CUDA 11.1 & CUDNN 8.0.4", "Note that code might also need to change to support newer versions of CUDA.", "Oh, I have the same problem. \r\n\r\nI have one RTX 3090 with CUDA version of 11.1 on Windows 10. Conda was used to install the cudatoolkit which provides CUDA 10.1 and cudnn 7.6, and I have tensorflow 1.14 installed. Tensorflow recognized my RTX 3090 well, but it spent a long time to begin or finish (I had to go to sleep ...) the training process.\r\n\r\nI wonder if I build the tensorflow1.14 (yes, I need this old version) from source combines with CUDA 11 and cudnn 8, I can use the RTX 3090 well. Thanks.", "@mihaimaruseac, yes, it **might** need to be changed. To get assured I'm trying to build from source, but got stuck in another problem( As mentioned in the quote). Can you take a look at that? Or if you think it's off-topic then I can raise another issue for this.\r\n\r\n@psycho2012 have you got any good images with TF 1.14 on RTX 3090? I'm just getting black images with RTX 3090 installed with CUDA 11.1, TF 1.14 ( This is probably a compatibility issue of TF with CUDA version). If you are getting good images with stylegan then can you tell me which version of CUDA, CUDNN, and TF are you using with RTX 3090?\r\n\r\n> Thanks, @ymodak for your answer. I'm trying to build from source but getting an issue:-\r\n> \r\n> Could not find any cudnn.h matching version '8' in any subdirectory:\r\n> ''\r\n> 'include'\r\n> 'include/cuda'\r\n> 'include/*-linux-gnu'\r\n> 'extras/CUPTI/include'\r\n> 'include/cuda/CUPTI'\r\n> of:\r\n> '/lib'\r\n> '/lib/x86_64-linux-gnu'\r\n> '/usr'\r\n> '/usr/include/'\r\n> '/usr/include/cudnn.h'\r\n> '/usr/local/cuda'\r\n> '/usr/local/cuda-11.1'\r\n> '/usr/local/cuda-11.1/targets/x86_64-linux/lib'\r\n> Asking for detailed CUDA configuration...\r\n> \r\n> I have checked that CUDNN has properly installed on /usr/include/cudnn.h path. Following [this](https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation#:~:text=Install%20CuDNN&text=For%20most%20people%2C%20it%20will,check%20it%20with%20which%20nvcc%20.) , I have copy-pasted a cudnn.h file to /usr/local/cuda/ and libcudnn* CUDNN installation file to /usr/local/cuda. Can you please tell me a solution for it.\r\n> \r\n> I am building TF 1.14 with CUDA 11.1 & CUDNN 8.0.4\r\n\r\n", "@Thunder003 I didn't get any good results.", "@psycho2012 can you tell me the version of CUDA, CUDNN you used with TF 1.14 in RTX 3090?", "@Thunder003 CUDA 11.1 and cuDNN 8.04, but I failed to run it on GPU for TF 1.14. I have tried to build it from source with TF 1.14, but failed.  According to the experiment, I think the current version of TF 1.14 can not support CUDA11.1. ", "@psycho2012 thanks for your answer. It seems like TF 1.14 is incompatible with CUDA 11.1. One thing that is scratching my head is the type of issue I'm getting when building from source( TF 1.14, CUDA 11.1, CUDNN 8.0.4), pls check the image. I have checked the path and files, they are proper. Are you getting stuck at the same step?\r\n\r\n![image](https://user-images.githubusercontent.com/51915348/98187495-13c0d900-1f37-11eb-95d4-3f26a92a5a82.png)\r\n\r\nI have added more paths for Libcudnn files also, but the error persists. If you are not stuck at this step can you tell me the path you have provided, just for reference (I know it may differ) ", "@Thunder003 I just tried on Windows but also failed on the step of the configuration. Maybe TF 1.14 is not supported by CUDA 11.1.", "@psycho2012 thanks for your answer. I have checked from another source also that TF 1.14 is incompatible with CUDA 11.1. and probably the error popped-up due to that only.\r\nI'm closing the issue now.", "> @psycho2012 thanks for your answer. I have checked from another source also that TF 1.14 is incompatible with CUDA 11.1. and probably the error popped-up due to that only.\r\n> I'm closing the issue now.\r\n\r\nCan I know how do you solve the problem?because i have the same trouble.  ", "@C-SJK for start-up time you can increase the cache size. But still, you may not get a good result. I'm getting a black screen only in the resulting image. I assumed that there is an incompatibility issue between TF 1.14 & CUDA 11.1. \r\nI saw NGC is using TF 1.15.4 & CUDA 11.1. But with me, TF 1.15 is not able to detect the GPU", "> @C-SJK for start-up time you can increase the cache size. But still, you may not get a good result. I'm getting a black screen only in the resulting image. I assumed that there is an incompatibility issue between TF 1.14 & CUDA 11.1.\r\n> I saw NGC is using TF 1.15.4 & CUDA 11.1. But with me, TF 1.15 is not able to detect the GPU\r\n\r\nNGC, I know it.\r\nDo you know how to use it?", "> @psycho2012 thanks for your answer. I have checked from another source also that TF 1.14 is incompatible with CUDA 11.1. and probably the error popped-up due to that only.\r\n> I'm closing the issue now.\r\n\r\nthanks\uff0ci get it!", "> Oh, I have the same problem.\r\n> \r\n> I have one RTX 3090 with CUDA version of 11.1 on Windows 10. Conda was used to install the cudatoolkit which provides CUDA 10.1 and cudnn 7.6, and I have tensorflow 1.14 installed. Tensorflow recognized my RTX 3090 well, but it spent a long time to begin or finish (I had to go to sleep ...) the training process.\r\n> \r\n> I wonder if I build the tensorflow1.14 (yes, I need this old version) from source combines with CUDA 11 and cudnn 8, I can use the RTX 3090 well. Thanks.\r\n\r\nDid you manage to build tf 1.14 and maybe run stylegan2 on top of it?\r\nI am strugling to make it work. Ubuntu 18.04 rtx 3090.\r\nsm_86 error is what I run into. Tried to change some nvcc option from 0 to 1 as suggested and ran into segmentation fault.\r\n\r\n> Setting up TensorFlow plugin \"upfirdn_2d.cu\": iulian  device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\r\nCompiling... Loading... Done.\r\nSegmentation fault (core dumped)\r\n\r\nor \r\n\r\n> venv/lib/python3.7/site-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/username/.cache/dnnlib/tflib-cudacache/fused_bias_act_b854f54134b47f099f4349d891d819ed.so: undefined symbol: _ZN10tensorflow12OpDefBuilder4AttrENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\r\n\r\ndepending on that tweak:\r\n\r\n>              compile_opts += f' --compiler-options \\'{\" \".join(tf.sysconfig.get_compile_flags())}\\''\r\n+            # compile_opts += f' --compiler-options \\'-fPIC -D_GLIBCXX_USE_CXX11_ABI=1\\''\r\n\r\n", "as stylegan2 doesn't have an issues tracker - I overloaded the issue into this git commit\r\n\r\nrelated - https://github.com/NVlabs/stylegan2/commit/23f8bed55f4b220c69cff98139a000d4c77cd558\r\n\r\nthere is a script we can run to upgrade stylegan2 to use tensorflow 2 - automagically. \r\ntf_upgrade_v2 \\\r\n  --intree stylegan2/ \\\r\n  --outtree stylegan2-tf2/ \\\r\n  --reportfile report.txt\r\n\r\nhowever - we need to address the follow failed conversions\r\nI beseech Nvidia to create an \"UNSUPPORTED\" tensorflow2 branch which we can all fix.\r\n\r\ntf.contrib. tf.contrib.memory_stats.MaxBytesInUse\r\nUsing member tf.contrib in deprecated module tf.contrib. tf.contrib cannot be converted automatically.\r\ntf.contrib. tf.contrib.opt.ScipyOptimizerInterface cannot be converted automatically. tf.contrib\r\ntf.contrib. tf.contrib.opt.GGTOptimizer cannot be converted automatically\r\n\r\nprobably would help if someone from tensorflow could also guide support in getting this over the line. the nvidia labs don't wont to support tensorflow 2. but it seems like push comes to shove here. Unless we're holding out for a stylegan3 to drop -  fyi @tkarras  \r\n"]}, {"number": 44199, "title": "error occurred when i loaded a model contain TextVectorization layer", "body": "**System information**\r\n- TensorFlow version:  2.3.1\r\n\r\n\r\n**error message**\r\n\r\n\r\n**RuntimeError: Unable to restore a layer of class TextVectorization. Layers of class TextVectorization require that the class be provided to the model loading code, either by registering the class using @keras.utils.register_keras_serializable on the class def and including that file in your program, or by passing the class in a keras.utils.CustomObjectScope that wraps this load call.**\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue** \r\ni built a model like below:\r\n```\r\nmodel = tf.keras.Sequential([\r\n   tf.keras.Input(shape=(1,), dtype=tf.string),\r\n   vectorize_layer(a TextVectorization layer that has adapt),\r\n   layers.Embedding(max_features + 1, embedding_dim),\r\n   layers.Dropout(0.2),\r\n   layers.GlobalAveragePooling1D(),\r\n   layers.Dropout(0.2),\r\n   layers.Dense(1),\r\n  layers.Activation('sigmoid')\r\n])\r\n```\r\n\r\nand i saved model like this:\r\n`model.save('./model/basic-text-class-export', save_format='tf')`\r\n\r\nbut when i loaded model like below, i got a error as shown before\r\n`model = tf.keras.models.load_model('./model/basic-text-class-export')`\r\n\r\nhow can i do next?\r\nbest wishes for you", "comments": ["@Clover0315,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "thanks for your answer\uff01\r\ni just copyed  the code from https://tensorflow.google.cn/tutorials/keras/text_classification, and added some code at the end and then got the error shown before: \r\n```\r\nexport_model.save('./model/test/basic-text-class-export', save_format='tf')\r\n\r\n# load model\r\nloaded_model = tf.keras.models.load_model('./model/test/basic-text-class-export')\r\nprint(loaded_model.summary())\r\n```\r\n\r\nThe complete code can be downloaded in https://github.com/Clover0315/DeepLearning/blob/main/text_classification.py.\r\nLooking forward to your reply.\r\nbest wishes for you!", "Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/8b65a688dc87ce9ca07ffb0ce50b84c7/44199.ipynb). Thanks!", "Any update on this issue? I'm seeing the same error.", "Why not just directly use this layer? `tf.keras.layers.experimental.preprocessing.TextVectorization` instead of direct import?", "@Clover0315 It seems like the problem can be solved by adding the decorator `@tf.keras.utils.register_keras_serializable()` to the function `custom_standardization()`:\r\n\r\n```python\r\n@tf.keras.utils.register_keras_serializable()\r\ndef custom_standardization(input_data):\r\n  lowercase = tf.strings.lower(input_data)\r\n  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\r\n  return tf.strings.regex_replace(stripped_html,\r\n                                  '[%s]' % re.escape(string.punctuation),\r\n                                  '')\r\n```\r\n\r\nTry it out and let me know if it doesn't work", "Oh I didn't realize there's a customized op -- yeah that should be the right fix", "@tanzhenyu \r\n\r\n> @Clover0315 It seems like the problem can be solved by adding the decorator `@tf.keras.utils.register_keras_serializable()` to the function `custom_standardization()`:\r\n> \r\n> ```python\r\n> @tf.keras.utils.register_keras_serializable()\r\n> def custom_standardization(input_data):\r\n>   lowercase = tf.strings.lower(input_data)\r\n>   stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\r\n>   return tf.strings.regex_replace(stripped_html,\r\n>                                   '[%s]' % re.escape(string.punctuation),\r\n>                                   '')\r\n> ```\r\n> \r\n> Try it out and let me know if it doesn't work\r\nI did this but for the error is still same.\r\n\r\n@tf.keras.utils.register_keras_serializable()\r\ndef custom_standardization(input_data):\r\n    lowercase = tf.strings.lower(input_data)\r\n    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\r\n    return tf.strings.regex_replace(\r\n        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\r\n    )\r\n"]}, {"number": 44198, "title": "configure.py not working on r1.14 - aarch64 nivida jetson nano", "body": "\r\n\r\n**Describe the problem**\r\n\r\nI am trying to build tensorflow c-library 1.14 from source on my jetson nano dev kit https://developer.nvidia.com/embedded/jetson-nano-developer-kit. I am running ubuntu 18.04 with aarch64, I have python3.6.9 installed.\r\n\r\nI have cloned the tensorflow repo and checked out r1.14 branch, when I run\r\n\r\n```bash\r\npython3 configure.py\r\n```\r\n\r\nI get the following\r\n\r\n```bash\r\n\r\npython3 configure.py \r\nWARNING: ignoring LD_PRELOAD in environment.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3.6/dist-packages\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.6/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3.6/dist-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nTraceback (most recent call last):\r\n  File \"configure.py\", line 1571, in <module>\r\n    main()\r\n  File \"configure.py\", line 1448, in main\r\n    if validate_cuda_config(environ_cp):\r\n  File \"configure.py\", line 1340, in validate_cuda_config\r\n    tuple(line.decode('ascii').rstrip().split(': ')) for line in proc.stdout)\r\nValueError: dictionary update sequence element #9 has length 1; 2 is required\r\n\r\n````\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nexact sequnce:\r\n\r\n```bash\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r1.14\r\npython3 configure.py\r\n```\r\n", "comments": ["Printing output of the processes finding cuda things\r\n```bash\r\nb'cublas_include_dir: /usr/include\\n'\r\nb'cublas_library_dir: /usr/lib/aarch64-linux-gnu\\n'\r\nb'cuda_binary_dir: /usr/local/cuda/bin\\n'\r\nb'cuda_include_dir: /usr/local/cuda/include\\n'\r\nb'cuda_library_dir: /usr/local/cuda/lib64\\n'\r\nb'cuda_toolkit_path: /usr/local/cuda\\n'\r\nb'cuda_version: 10.2\\n'\r\nb'cudnn_include_dir: /usr/include\\n'\r\nb'cudnn_library_dir: /usr/lib/aarch64-linux-gnu\\n'\r\nb'cudnn_version: \\n'\r\nb'cupti_include_dir: /usr/local/cuda/include\\n'\r\nb'cupti_library_dir: /usr/local/cuda/lib64\\n'\r\nb'nvvm_library_dir: /usr/local/cuda/nvvm/libdevice\\n'\r\nb'tensorrt_include_dir: /usr/include/aarch64-linux-gnu\\n'\r\nb'tensorrt_library_dir: /usr/lib/aarch64-linux-gnu\\n'\r\nb'tensorrt_version: 7\\n'\r\n````\r\n\r\n\r\nI think it might be because cuda-nn seems to be empty\r\n", "But I have cudann on my compute under /usr/lib/aarch64-linux-gnu/libcudnn.so", "setting cuda version manully in third_part/gpus/finda_cuda_config.py at 348 made it work for me\r\n\r\n```bash\r\ncudnn_version = \"8\"\r\n```\r\n\r\nor well appears to work, not done building tensorflow yet.", "@JonasRSV \r\nCan you try with [tested build configuration](https://www.tensorflow.org/install/source#gpu) for TF required and see whether the problem still persists?.\r\nIs there any particular reason for using 1.x as there is support for 2.x , please upgrade and let us know if the issue exists.\r\nIf you want CUDA 11 you can use nightly. If you want to keep 1.x APIs then you can only use CUDA 10.0\r\n", "Jetson Nano comes with these cuda version pre-installed so I'd prefer to to try to change.\r\n\r\nUnfortunately the compilation now crashes with some obscure error:\r\n\r\n\r\n```bash\r\nINFO: From Compiling external/highwayhash/highwayhash/sip_hash.cc:\r\nIn file included from external/highwayhash/highwayhash/arch_specific.h:39:0,\r\n                 from external/highwayhash/highwayhash/sip_hash.h:23,\r\n                 from external/highwayhash/highwayhash/sip_hash.cc:15:\r\nexternal/highwayhash/highwayhash/state_helpers.h: In function 'void highwayhash::PaddedUpdate(highwayhash::HH_U64, const char*, highwayhash::HH_U64, State*)':\r\nexternal/highwayhash/highwayhash/compiler_specific.h:52:46: warning: requested alignment 32 is larger than 16 [-Wattributes]\r\n #define HH_ALIGNAS(multiple) alignas(multiple)  // C++11\r\n                                              ^\r\nexternal/highwayhash/highwayhash/state_helpers.h:49:41: note: in expansion of macro 'HH_ALIGNAS'\r\n   char final_packet[State::kPacketSize] HH_ALIGNAS(32) = {0};\r\n                                         ^~~~~~~~~~\r\nERROR: /home/jonasrsv/.cache/bazel/_bazel_jonasrsv/71718566c3609207e5a44599481f772c/external/nccl_archive/BUILD.bazel:67:1: fatbinary external/nccl_archive/device_dlink_hdrs.fatbin failed (Exit 1)\r\nfatbinary fatal   : Unknown option '-bin2c-path'\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1732.677s, Critical Path: 143.12s\r\nINFO: 2101 processes: 2101 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\nShould I close this issue and re-open a new one for this compilation error? Since the original problem was with configuration, but by manually setting version that got resolved.\r\n\r\nThe reason why not using 2.x is simply because my code base is in 1.14 so I assume I need a 1.14 library file to run it.\r\n", "Seems to be related to this: https://github.com/tensorflow/tensorflow/issues/34429", "So essentially I have some code in tf 1.14 that creates a saved_model, I then have a c++ library that uses saved models.. in that issue it seems as if tf 1.14 will not ever get a fix for this error?\r\n\r\nSo would it be possible to load saved_models trained with tf 1.14 with a later tensorflow version that resolves this issue?", "@JonasRSV \r\nYes please as the configuration issue is resolved, please create a new issue for the compilation error faced.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44198\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44198\">No</a>\n"]}, {"number": 44197, "title": "Cannot convert keras.layers.Embedding", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary (by pip)\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\nHi, I found simple model with keras.layers.Embedding cannot be converted to tflite.\r\nI built my model with python API and then tried to convert it using `tflite_convert` CLI\r\nAttached code/command can reproduce this bug.\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```tflite_convert --saved_model_dir=embedding --output_file=embedding.tflite```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-10-21 06:20:51.501787: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-21 06:20:52.177539: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-21 06:20:52.223256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.223582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-10-21 06:20:52.223600: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-21 06:20:52.224542: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-21 06:20:52.225557: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-21 06:20:52.225744: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-21 06:20:52.226751: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-21 06:20:52.227345: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-21 06:20:52.229557: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-21 06:20:52.229661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.229986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.230255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-21 06:20:52.230467: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-21 06:20:52.234982: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3699850000 Hz\r\n2020-10-21 06:20:52.235474: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a1d6c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-21 06:20:52.235487: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-21 06:20:52.337971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.338757: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a404e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-21 06:20:52.338789: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5\r\n2020-10-21 06:20:52.339061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.339714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-10-21 06:20:52.339756: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-21 06:20:52.339791: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-21 06:20:52.339817: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-21 06:20:52.339841: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-21 06:20:52.339865: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-21 06:20:52.339888: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-21 06:20:52.339911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-21 06:20:52.340007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.340695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.341326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-21 06:20:52.341370: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-21 06:20:52.622897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-21 06:20:52.622925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-21 06:20:52.622930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-21 06:20:52.623098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.623413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.623714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7272 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-10-21 06:20:52.735265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.735499: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-10-21 06:20:52.735547: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-10-21 06:20:52.735866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.736073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-10-21 06:20:52.736097: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-21 06:20:52.736116: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-21 06:20:52.736128: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-21 06:20:52.736139: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-21 06:20:52.736151: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-21 06:20:52.736162: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-21 06:20:52.736174: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-21 06:20:52.736214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.736422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.736600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-21 06:20:52.736619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-21 06:20:52.736625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-21 06:20:52.736629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-21 06:20:52.736685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.736893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-21 06:20:52.737083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7272 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-10-21 06:20:52.742399: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-10-21 06:20:52.742415: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 4 nodes (0), 4 edges (0), time = 0.333ms.\r\n2020-10-21 06:20:52.742420: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 4 nodes (0), 4 edges (0), time = 0.257ms.\r\n2020-10-21 06:20:52.742424: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: __inference__wrapped_model_77\r\n2020-10-21 06:20:52.742428: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-10-21 06:20:52.742432: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nTraceback (most recent call last):\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 496, in _import_graph_def_internal\r\n    results = c_api.TF_GraphImportGraphDefWithResults(\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/jhjang/.virtualenvs/ml/bin/tflite_convert\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/tflite_convert.py\", line 640, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/tflite_convert.py\", line 623, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/tflite_convert.py\", line 239, in _convert_tf2_model\r\n    tflite_model = converter.convert()\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 877, in convert\r\n    _convert_to_constants.convert_variables_to_constants_v2_as_graph(\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 1108, in convert_variables_to_constants_v2_as_graph\r\n    frozen_func = _construct_concrete_function(func, output_graph_def,\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\", line 999, in _construct_concrete_function\r\n    new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 650, in function_from_graph_def\r\n    wrapped_import = wrap_function(_imports_graph_def, [])\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 621, in wrap_function\r\n    func_graph.func_graph_from_py_func(\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 87, in __call__\r\n    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 93, in wrapped\r\n    return fn(*args, **kwargs)\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py\", line 648, in _imports_graph_def\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 400, in import_graph_def\r\n    return _import_graph_def_internal(\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/framework/importer.py\", line 501, in _import_graph_def_internal\r\n    raise ValueError(str(e))\r\nValueError: Input 1 of node StatefulPartitionedCall was passed float from unknown:0 incompatible with expected resource.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\nI built my model 'embedding' with below python code\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Embedding(1000, 64, name=\"embedding\"),\r\n    keras.layers.GlobalAveragePooling1D()\r\n])\r\n\r\nmodel.save('embedding')\r\n```\r\n\r\n**Failure details**\r\nNone\r\n\r\n**Any other info / logs**\r\nNone\r\n", "comments": ["I also tried python API of converter but I faced the same error\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\nimport os\r\n\r\nmodel_path = './embedding'\r\n\r\n# random vector for embedding\r\nx_test = np.random.randint(1000, size=(100, 10))\r\nx_test = tf.data.Dataset.from_tensor_slices((x_test)).batch(1)\r\n\r\ndef representative_dataset_gen():\r\n    for x in x_test.take(100):\r\n        yield [x]\r\n\r\n# convert to tflite model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_path)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\n# save\r\nopen('tflite_{}.tflite'.format(os.path.basename(model_path)), 'wb').write(tflite_model)\r\n```", "@junhyk \r\n\r\nI have tried in colab with TF nightly version(`2.4.0-dev20201020`) and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/ac3944775c59863558a51f0f444a1819/untitled475.ipynb).Please, verify once and close the issue.Thanks!", "That is a working example using TF version `2.3.1`. \r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\n# Create Model\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Input(shape=(128, ), dtype=tf.int32),\r\n    keras.layers.Embedding(1000, 64, name=\"embedding\"),\r\n    keras.layers.GlobalAveragePooling1D()\r\n])\r\n\r\n# Convert and save\r\nlite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nlite_converter.experimental_new_converter = True\r\nlite_model = lite_converter.convert()\r\n\r\nopen('/tmp/model.tflite', \"wb\").write(lite_model)\r\n\r\n\r\n# Test input for tflite model\r\nmodel_input = np.random.randint(1, 128, 128, dtype=np.int32).reshape((1, -1))\r\n\r\n# Load model\r\ninference_model = tf.lite.Interpreter('/tmp/model.tflite')\r\ninference_model.allocate_tensors()\r\n\r\ninput_details = inference_model.get_input_details()\r\noutput_details = inference_model.get_output_details()\r\n\r\n# Run inference\r\ninference_model.set_tensor(input_details[0]['index'], model_input)\r\ninference_model.invoke()\r\n\r\n# Get output and print\r\npred = inference_model.get_tensor(output_details[0]['index'])\r\nprint(pred)\r\n```", "@ravikyram \r\n\r\nYour script worked!\r\nBut when I set `representative_dataset` to `representative_dataset_gen()` above,\r\nI got the following error.\r\n\r\n```\r\n2020-10-22 15:08:29.007562: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-22 15:08:29.878262: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-22 15:08:29.878738: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-10-22 15:08:29.899992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-22 15:08:29.900281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-10-22 15:08:29.900297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-22 15:08:29.900424: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/home/jhjang/ml-accelerator/chipyard/esp-tools-install/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n2020-10-22 15:08:29.900462: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/home/jhjang/ml-accelerator/chipyard/esp-tools-install/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n2020-10-22 15:08:29.901559: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-10-22 15:08:29.901774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-10-22 15:08:29.902833: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-22 15:08:29.902911: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/home/jhjang/ml-accelerator/chipyard/esp-tools-install/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n2020-10-22 15:08:29.903049: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-10-22 15:08:29.903059: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-10-22 15:08:29.903330: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-22 15:08:29.903872: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-22 15:08:29.903896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-22 15:08:29.903902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]\r\n2020-10-22 15:08:29.973169: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\r\n2020-10-22 15:08:29.973191: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\r\n2020-10-22 15:08:29.973211: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:325] Ignored change_concat_input_ranges.\r\n2020-10-22 15:08:29.973908: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: ./embedding\r\n2020-10-22 15:08:29.974534: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }\r\n2020-10-22 15:08:29.974562: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: ./embedding\r\n2020-10-22 15:08:29.974588: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-22 15:08:29.974600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-22 15:08:29.974604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]\r\n2020-10-22 15:08:29.976016: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n2020-10-22 15:08:29.976313: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\r\n2020-10-22 15:08:29.976740: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3699850000 Hz\r\n2020-10-22 15:08:29.983827: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: ./embedding\r\n2020-10-22 15:08:29.986068: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 12161 microseconds.\r\n2020-10-22 15:08:29.991337: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2020-10-22 15:08:29.996500: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-22 15:08:29.996669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-22 15:08:29.996981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-10-22 15:08:29.997017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-22 15:08:29.997133: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/home/jhjang/ml-accelerator/chipyard/esp-tools-install/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n2020-10-22 15:08:29.997214: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/home/jhjang/ml-accelerator/chipyard/esp-tools-install/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n2020-10-22 15:08:29.997243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-10-22 15:08:29.997252: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-10-22 15:08:29.997260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-22 15:08:29.997297: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/home/jhjang/ml-accelerator/chipyard/esp-tools-install/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\n2020-10-22 15:08:29.997309: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-10-22 15:08:29.997314: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-10-22 15:08:30.086562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-22 15:08:30.086587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2020-10-22 15:08:30.086593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\nTraceback (most recent call last):\r\n  File \"tflite_conversion.py\", line 32, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/jhjang/.virtualenvs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 742, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/jhjang/.virtualenvs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 461, in _calibrate_quantize_model\r\n    inference_output_type, allow_float, activations_type)\r\n  File \"/home/jhjang/.virtualenvs/tf-nightly/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 100, in calibrate_and_quantize\r\n    self._calibrator.FeedTensor(sample)\r\nValueError: Cannot set tensor: Got value of type INT64 but expected type FLOAT32 for input 0, name: serving_default_embedding_input:0\r\nException ignored in: <bound method Buckets.__del__ of <tensorflow.python.eager.monitoring.ExponentialBuckets object at 0x7f55d67c3d08>>\r\nTraceback (most recent call last):\r\n  File \"/home/jhjang/.virtualenvs/tf-nightly/lib/python3.6/site-packages/tensorflow/python/eager/monitoring.py\", line 407, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'\r\n```\r\n\r\nAnd I couldn't set the `inference_input_type` to int64\r\n```\r\nValueError: The inference_input_type and inference_output_type must be in ['tf.float32', 'tf.int8', 'tf.uint8']\r\n```\r\n\r\n@gcuder \r\nThanks. \r\nIt seems `tf.lite.TFLiteConverter.from_keras_model()` works better", "I found adding an input layer with dtype like @gcuder 's solution can be a workaround\r\n\r\nHowever I think tflite should support `inference_input_type` of int32 or int64 for an embedding index.", "In addition,\r\nIt seems that tflite currently does not support RaggedTensor, since I failed to convert the same model with `ragged=True` option on `tf.keras.layers.Input` layer.\r\n\r\nIs this true?\r\nThen, do you guys have plan for RaggedTensor support of TFLite?", "@junhyk Can you please share a standalone code in a gist form to reproduce the error? Thanks!", "I figured out to convert the model by adding `tf.lite.OpsSet.SELECT_TF_OPS` to support_ops.\r\nbut now I cannot set an appropriate input to the converted model\r\n\r\nBelow code should reproduce my error\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\nnum_entry = 1000\r\nembedding_dim = 8\r\n\r\n# Create Model\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Input(shape=[None], dtype=tf.int32, ragged=True),\r\n    keras.layers.Embedding(num_entry, embedding_dim, name=\"embedding\"),\r\n    keras.layers.GlobalAveragePooling1D()\r\n])\r\n\r\nmodel_input = tf.ragged.constant(\r\n    [[[1, 2], [3, 7, 8], [4, 5]]], dtype=tf.int32)\r\nprint(model(model_input))\r\n\r\n\r\n# Convert and save\r\nlite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nlite_converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nlite_converter.experimental_new_converter = True\r\nlite_converter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n]\r\nlite_model = lite_converter.convert()\r\n\r\nopen('embedding.tflite', \"wb\").write(lite_model)\r\n\r\n# Load model\r\ninference_model = tf.lite.Interpreter('embedding.tflite')\r\ninference_model.allocate_tensors()\r\n\r\ninput_details = inference_model.get_input_details()\r\noutput_details = inference_model.get_output_details()\r\n\r\n# Run inference\r\ninference_model.set_tensor(input_details[0]['index'], model_input)\r\ninference_model.invoke()\r\n\r\n# Get output and print\r\npred = inference_model.get_tensor(output_details[0]['index'])\r\nprint(pred)\r\n```\r\n\r\noutput\r\n```\r\n2020-10-29 22:32:38.607135: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-29 22:32:39.282562: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-29 22:32:39.327549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:39.327921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-10-29 22:32:39.327937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-29 22:32:39.328964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-29 22:32:39.330017: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-29 22:32:39.330185: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-29 22:32:39.331280: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-29 22:32:39.331866: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-29 22:32:39.334158: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-29 22:32:39.334242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:39.334565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:39.334813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-29 22:32:39.335008: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-29 22:32:39.339416: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3699850000 Hz\r\n2020-10-29 22:32:39.339746: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52c0d90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-29 22:32:39.339772: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-29 22:32:39.440929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:39.441281: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x534ced0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-29 22:32:39.441295: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5\r\n2020-10-29 22:32:39.441427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:39.441680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-10-29 22:32:39.441698: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-29 22:32:39.441712: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-29 22:32:39.441720: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-29 22:32:39.441729: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-29 22:32:39.441737: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-29 22:32:39.441745: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-29 22:32:39.441753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-29 22:32:39.441786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:39.442046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:39.442280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-29 22:32:39.442298: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-29 22:32:39.723178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-29 22:32:39.723207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-29 22:32:39.723228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-29 22:32:39.723367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:39.723662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:39.723935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7272 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n<tf.RaggedTensor [[[-0.023646922782063484, 0.007384554948657751, -0.002287632552906871, 0.027374044060707092, -0.017715398222208023, 0.004941681865602732, 0.010069675743579865, 0.010894370265305042], [-0.0037146415561437607, -0.028481630608439445, -0.006638674531131983, 0.032867029309272766, 0.02433614991605282, 0.025300944223999977, 0.018142852932214737, -0.01105849351733923], [0.03774816915392876, 0.017598140984773636, -0.043984342366456985, 0.013838265091180801, 0.03936057165265083, -0.012402426451444626, -0.0294428002089262, 0.037441615015268326]]]>\r\nWARNING:tensorflow:From /home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-10-29 22:32:40.010020: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From /home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-10-29 22:32:40.244208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.244429: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-10-29 22:32:40.244521: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-10-29 22:32:40.245152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.245335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-10-29 22:32:40.245367: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-29 22:32:40.245382: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-29 22:32:40.245390: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-29 22:32:40.245398: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-29 22:32:40.245405: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-29 22:32:40.245413: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-29 22:32:40.245421: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-29 22:32:40.245449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.245613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.245754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-29 22:32:40.245772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-29 22:32:40.245777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-29 22:32:40.245780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-29 22:32:40.245837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.246030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.246203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7272 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-10-29 22:32:40.251699: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-10-29 22:32:40.251712: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2020-10-29 22:32:40.251717: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-10-29 22:32:40.344383: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-10-29 22:32:40.344407: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n2020-10-29 22:32:40.350008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.350203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-10-29 22:32:40.350223: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-29 22:32:40.350241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-29 22:32:40.350252: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-29 22:32:40.350262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-29 22:32:40.350271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-29 22:32:40.350280: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-29 22:32:40.350290: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-29 22:32:40.350324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.350532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.350673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-29 22:32:40.350692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-29 22:32:40.350697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-29 22:32:40.350700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-29 22:32:40.350749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.350918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.351066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7272 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-10-29 22:32:40.362114: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor sequential/global_average_pooling1d/RaggedReduceMean/RaggedReduceSum/RaggedSplitsToSegmentIds/Repeat/boolean_mask/Reshape that is not type float.\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2020-10-29 22:32:40.363990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.364190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-10-29 22:32:40.364208: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-29 22:32:40.364225: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-29 22:32:40.364234: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-29 22:32:40.364244: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-29 22:32:40.364253: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-29 22:32:40.364262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-29 22:32:40.364272: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-29 22:32:40.364310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.364498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.364644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-29 22:32:40.364663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-29 22:32:40.364669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-29 22:32:40.364673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-29 22:32:40.364722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.364896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-29 22:32:40.365092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7272 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nINFO: TfLiteFlexDelegate delegate: 5 nodes delegated out of 40 nodes with 2 partitions.\r\n\r\nTraceback (most recent call last):\r\n  File \"ragged-tensor-test.py\", line 40, in <module>\r\n    inference_model.set_tensor(input_details[0]['index'], model_input)\r\n  File \"/home/jhjang/.virtualenvs/ml/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py\", line 407, in set_tensor\r\n    self._interpreter.SetTensor(tensor_index, value)\r\nValueError: Cannot set tensor: Got value of type STRING but expected type INT32 for input 0, name: args_0 \r\n```", "@junhyk I am seeing different error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d7e5b9286f9a947ad8975b308b535c7e/untitled.ipynb). Can you please update in the gist and share? Thanks!", "Please check below gist\r\nI reduced the rank of model_input from 3 -> 2\r\nhttps://colab.research.google.com/gist/junhyk/894393ba64f5b511a643c1b7528330a4/untitled.ipynb"]}, {"number": 44196, "title": "Add gcc specific linker flag only when building with gcc.", "body": "The additional linker flag for fatal-warnings should not be used whe\r\ncross compiling with armclang, or on MacOS.\r\n\r\nFixes #43885 #44181 #43887 #43865\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@nkreeger: this should fix the MacOS build but I have no way of testing it. Can you please confirm?\r\n\r\n@jenselofsson, @mansnils: the previous check for armclang was likely not useful since we pulled armclang out of TAGS as part of the review for #43726. This new check should do the trick but I don't have any way to test.", "@advaitjain @nkreeger I think this will work fine for Armclang as well. I create a new issue if not."]}, {"number": 44195, "title": "Random ops the same for different iterations over mapped datasets", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (copy below + colab link)\r\n- OS Platform and Distribution Linux Ubuntu 18.04:\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): 2.3 / tf-nightly\r\n- Python version: 3.7.7\r\n\r\n**Describe the current behavior**\r\nRandom ops in `tf.data.Dataset.map`ped datasets produce the same values for repeated iterations over the same dataset (though not for datasets using `Dataset.repeat`) when `tf.random.set_seed` has been called. This nullifies the effect of any data augmentation when the dataset is looped over and the seed is set.\r\n\r\n**Describe the expected behavior**\r\nLooping over a dataset multpile times should have at least qualitatively equivalent behaviour to looping over a `repeat`ed dataset (potentially quantitatively too), and should be qualitatively the same with or without using `tf.random.set_seed`.\r\n\r\n**Standalone code to reproduce the issue**\r\nColab [here](https://colab.research.google.com/drive/1Y9wajzK5TYer_DYOK5l_ySexq_A-5tLd?usp=sharing)\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef get_dataset():\r\n  return tf.data.Dataset.range(2).map(\r\n      lambda x: tf.cast(x, tf.float32) + tf.random.uniform(()))\r\n\r\ndef print_looped_epochs(num_epochs=2):\r\n  print('Looped epochs:')\r\n  dataset = get_dataset()\r\n  for _ in range(num_epochs):\r\n    print([e.numpy() for e in dataset])\r\n\r\ndef print_repeated_epochs(num_epochs=2):\r\n  print(\"Repeated epochs:\")\r\n  dataset = get_dataset().repeat(num_epochs)\r\n  print([e.numpy() for e in dataset])\r\n\r\nprint_looped_epochs()\r\nprint_repeated_epochs()\r\n\r\ntf.random.set_seed(0)\r\nprint('Seed set')\r\nprint_looped_epochs()\r\nprint_repeated_epochs()\r\n```\r\n**Other info / logs**\r\n```txt\r\nLooped epochs:\r\n[0.6892859, 1.171408]\r\n[0.0070821047, 1.629379]\r\nRepeated epochs:\r\n[0.6894361, 1.446699, 0.60574067, 1.9869001]\r\nSeed set\r\nLooped epochs:\r\n[0.019757032, 1.5400312]\r\n[0.019757032, 1.5400312]\r\nRepeated epochs:\r\n[0.019757032, 1.5400312, 0.51667833, 1.4683528]\r\n```", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/ac7c01702681aa361f0a63c75ad41b8d/44195.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/6ecd7161fc2d1bf13edca166cd70e6ea/44195-tf-nightly.ipynb). Please find the attached gist. Thanks!", "When your `get_dataset` method constructs the input pipeline graph, the seed ends up being embedded in the input pipeline graph, which is why executing the same input pipeline graph multiple times will produce identical results across different epochs.\r\n\r\nIn other words, the dataset graph does not maintain the concept of an \"epoch\" and in the absence of unseeded operations, its execution will be fully deterministic.\r\n\r\nIf you would like your input pipeline to execute both 1) deterministically and 2) differently across different epochs, you will have to maintain the notion of an epoch yourself:\r\n\r\n```\r\nepoch_counter = tf.Variable(0, dtype=tf.int64)\r\n\r\nds = tf.data.Dataset.range(5)\r\nseeds = tf.data.experimental.RandomDataset(seed=42)\r\nds = tf.data.Dataset.zip((ds, seeds))\r\n\r\ndef map_fn(unused, seed):\r\n  return tf.random.stateless_uniform([], (42, seed + epoch_counter))\r\n\r\nds = ds.map(map_fn)\r\n\r\nfor i in range(2):\r\n  print(\"Epoch: \", i)\r\n  for elem in ds:\r\n    print(elem.numpy())\r\n  epoch_counter.assign_add(1)\r\n```\r\n\r\nThis example should always produce the following output:\r\n\r\n```\r\nEpoch:  0\r\n0.12457669\r\n0.18042064\r\n0.12596941\r\n0.04956436\r\n0.813197\r\nEpoch:  1\r\n0.98221445\r\n0.53253984\r\n0.20035768\r\n0.276451\r\n0.2731657\r\n```", "@jsimsa I'm sure there are ways around this (using `tf.random.Generator` ops for one) but is this really the desired behaviour? And why does the issue only occur when `tf.random.set_seed` has been explicitly called?", "The fact that an input pipeline with no unseeded random operations (and sources of external state such as `shuffle` or `cache`) produces the same order every time it is executed is an important invariant and the expected behavior (and changing it is not an option for backwards compatibility reasons). The `tf.random.Generator` you mentioned is an example of external state that allows users to explicitly control that across different epochs the behavior of the input pipeline should be (deterministically) different.\r\n\r\nWithout setting the `tf.random.set_seed`, the random operations in the input pipeline graph will be unseeded (and thus behave non-deterministically).", "I must say it's extremely counter-intuitive that setting the random seed changes the behaviour of this in such a qualitative way and can potentially completely break data augmentation strategies, but I suppose that's just another reason to avoid `tf.random` ops.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44195\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44195\">No</a>\n", "Bumped into this as well.\r\n\r\nI would expect that setting a seed should make the program run with exactly the same effect every time it is run, not that every internal 'cycle' will run exactly the same.\r\n\r\nNo seed should be the same as specifying some unknown seed at the beginning."]}, {"number": 44194, "title": "tf.keras.layers.Embedding forces CPU placement if eager execution is enabled and GPUs are present.", "body": "**System information**\r\n- Have I written custom code: **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Google AI Platform**\r\n- TensorFlow installed from **binary**\r\n- TensorFlow version: reproducible on 2.2 or 2.3\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: Unknown\r\n- GPU model and memory: NVIDIA Tesla K80\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.layers.Embedding` layers are pinned to the CPU when created in eager execution mode (the default), resulting in significant slowdowns compared with GPU placement. In non-toy examples, this results in a 2-3x slowdown when run on a modest training machine (AI Platform with one NVIDIA Tesla K80).\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.keras.layers.Embedding` layers would be placed on the GPU, avoiding unnecessary MemcpyH2D and MemcpyD2H calls (as viewed in the TensorFlow profiler) and avoiding bottlenecking the job on the CPU.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# Uncomment the following line to get proper GPU placement for the Embedding layer\r\n# tf.compat.v1.disable_eager_execution()\r\ntf.debugging.set_log_device_placement(True)\r\n\r\nmodel = tf.keras.Sequential(\r\n    [\r\n        tf.keras.layers.Embedding(10, 1, input_shape=(1,)),\r\n        tf.keras.layers.Dense(1),\r\n    ]\r\n)\r\n\r\nmodel.compile(\r\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n    optimizer=tf.keras.optimizers.Adam(),\r\n    metrics=[\"accuracy\"],\r\n)\r\n\r\n# Train on some dummy data\r\nx = np.random.randint(1, 10, (10, 1))\r\ny = np.random.uniform(0, 1, (10, 1))\r\nmodel.fit(x, y, epochs=12)\r\n```\r\n\r\nWhen run on a machine with a GPU, the above code prints out all op assignments, including:\r\n\r\n```\r\n...\r\n\"sequential_embedding_embedding_lookup_488: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\"\r\n...\r\n```\r\n\r\n...however, when eager execution is disabled entirely, the above code shows that the embedding-related ops are being assigned to the GPU instead:\r\n```\r\n...\r\nembedding/Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0\r\nembedding/embedding_lookup: (ResourceGather): /job:localhost/replica:0/task:0/device:GPU:0\r\nembedding/embedding_lookup/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\r\nembedding/embedding_lookup/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\r\n...\r\n```\r\n\r\nThis issue seems to be caused by https://github.com/tensorflow/tensorflow/commit/a1b64cf2a6a995ffaaf384cf8643221f1c27db48, landed by @alextp a couple years ago. It seems that the assertion [in the comment of `Embedding#build`](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/layers/embeddings.py#L126-L150) (\"Most sparse optimizers do not have GPU kernels defined\") may no longer hold, and that this pinning of the Embedding layer to CPU only is adversely affecting performance when no longer necessary in all cases.\r\n\r\nIt does seem to be possible to circumvent this issue by subclassing `tf.keras.layers.Embedding` and manually reimplementing the method:\r\n\r\n```python\r\nclass GPUCompatibleEmbedding(layers.Embedding):\r\n    @tf_utils.shape_type_conversion\r\n    def build(self, input_shape):\r\n        self.embeddings = self.add_weight(\r\n            shape=(self.input_dim, self.output_dim),\r\n            initializer=self.embeddings_initializer,\r\n            name=\"embeddings\",\r\n            regularizer=self.embeddings_regularizer,\r\n            constraint=self.embeddings_constraint,\r\n        )\r\n        self.built = True\r\n```\r\n\r\nHowever, this seems like a bit of a hacky workaround, as TensorFlow/Keras should allow Embedding layers to run on GPUs out of the box - as it does seem entirely possible to do so.", "comments": ["@psobot \r\nI ran the code shared, please let us know if [this confirms](https://colab.research.google.com/gist/Saduf2019/2cbcd74c959f76d8ff3cf3c30f056eae/untitled454.ipynb) your issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @Saduf2019! The collab notebook you've provided doesn't seem to print out the ops related to the model itself, for reasons I have yet to determine. I can confirm that this issue is still happening when run outside of collab.", "Hi @psobot, sorry for the very delayed response here! Please take a look at https://github.com/tensorflow/tensorflow/commit/a279b46a97a386e2f15182826779192fced3473e", "Thanks @nikitamaia! It looks like a279b46 should solve the problem and restore performance to what's expected - thanks @fchollet!"]}, {"number": 44193, "title": "exit error when using tftrt in jupyterlab", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu18.04.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0 master\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.7.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 11.1/8.0.4\r\n- GPU model and memory: tesla p100 X 2\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nusing jupyterlab\r\nimport tensorrt\r\nfrom tensorflow import saved_model\r\n\r\nthen import saved tensorrtengine ( built from tftrt )\r\n\r\n**Describe the expected behavior**\r\nexit and no errors\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nusing tensorflow, from any saved model, import tensorflow tensorrt, convert, build and save\r\nand load from jupyterlab, and then exit jupyterlab\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n[C 13:57:47.406 LabApp] Shutdown confirmed\r\n[I 13:57:47.407 LabApp] Shutting down 2 kernels\r\n[I 13:57:47.608 LabApp] Kernel shutdown: ef64bc65-6695-40bf-ab9b-e2c7ff62e1bd\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/alan/anaconda3/lib/python3.8/logging/__init__.py\", line 1084, in emit\r\n    stream.write(msg + self.terminator)\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\", line 414, in write\r\n    self._schedule_flush()\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\", line 335, in _schedule_flush\r\n    self.pub_thread.schedule(_schedule_in_thread)\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\", line 207, in schedule\r\n    f()\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\", line 334, in _schedule_in_thread\r\n    self._io_loop.call_later(self.flush_interval, self._flush)\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py\", line 602, in call_later\r\n    return self.call_at(self.time() + delay, callback, *args, **kwargs)\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 162, in call_at\r\n    return self.asyncio_loop.call_later(\r\n  File \"/home/alan/anaconda3/lib/python3.8/asyncio/base_events.py\", line 687, in call_later\r\n    timer = self.call_at(self.time() + delay, callback, *args,\r\n  File \"/home/alan/anaconda3/lib/python3.8/asyncio/base_events.py\", line 698, in call_at\r\n    self._check_closed()\r\n  File \"/home/alan/anaconda3/lib/python3.8/asyncio/base_events.py\", line 508, in _check_closed\r\n    raise RuntimeError('Event loop is closed')\r\nRuntimeError: Event loop is closed\r\nCall stack:\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py\", line 160, in __del__\r\n    log_fn(\"Unresolved object in checkpoint: {}\"\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/platform/tf_logging.py\", line 178, in warning\r\n    get_logger().warning(msg, *args, **kwargs)\r\nMessage: 'Unresolved object in checkpoint: (root).trt_engine_resources.TRTEngineOp_0_0._serialized_trt_resource_filename'\r\nArguments: ()\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/alan/anaconda3/lib/python3.8/logging/__init__.py\", line 1085, in emit\r\n    self.flush()\r\n  File \"/home/alan/anaconda3/lib/python3.8/logging/__init__.py\", line 1065, in flush\r\n    self.stream.flush()\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\", line 357, in flush\r\n    self._flush()\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\", line 383, in _flush\r\n    self.session.send(self.pub_thread, u'stream', content=content,\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/jupyter_client/session.py\", line 751, in send\r\n    stream.send_multipart(to_send, copy=copy)\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\", line 214, in send_multipart\r\n    self.schedule(lambda : self._really_send(*args, **kwargs))\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\", line 207, in schedule\r\n    f()\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\", line 214, in <lambda>\r\n    self.schedule(lambda : self._really_send(*args, **kwargs))\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/ipykernel/iostream.py\", line 222, in _really_send\r\n    self.socket.send_multipart(msg, *args, **kwargs)\r\nAttributeError: 'NoneType' object has no attribute 'send_multipart'\r\nCall stack:\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py\", line 168, in __del__\r\n    log_fn(\r\n  File \"/home/alan/anaconda3/lib/python3.8/site-packages/tensorflow/python/platform/tf_logging.py\", line 178, in warning\r\n    get_logger().warning(msg, *args, **kwargs)\r\nMessage: 'A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.'\r\nArguments: ()\r\n[I 13:57:48.411 LabApp] Kernel shutdown: fd4e7ab1-7f88-485f-9270-296fbbaca6c4\r\n[I 13:57:48.411 LabApp] Shutting down 0 terminals\r\n```", "comments": ["@alanpurple \r\n\r\nCan you please share code snippet to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44193\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44193\">No</a>\n"]}, {"number": 44192, "title": "error when exit kernel", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0 master\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source): 3.7.0\r\n- GCC/Compiler version (if compiling from source):  7.5.0\r\n- CUDA/cuDNN version: 11.1 / 8.0.4\r\n- GPU model and memory: tesla P100\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nstart python and import tensorflow and exit\r\n\r\n**Describe the expected behavior**\r\nno error messages\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nPython 3.7.9 (default, Aug 31 2020, 12:42:55)\r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-10-21 13:48:47.162423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>>\r\n>>> exit()\r\nException ignored in: <function Buckets.__del__ at 0x7f8b017e2830>\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/monitoring.py\", line 407, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@alanpurple,\r\nI did not face any errors with TF v2.3 and TF-nightly. Please find the attached screenshots for reference. \r\n\r\n![Screenshot 2020-10-21 at 6 21 09 PM](https://user-images.githubusercontent.com/57165142/96723593-5c6f8280-13cc-11eb-9282-63776579c13d.png)\r\n\r\n![Screenshot 2020-10-21 at 6 23 57 PM](https://user-images.githubusercontent.com/57165142/96723610-62656380-13cc-11eb-8d3a-f23554b7f16b.png)\r\n\r\nLooks like you are running the commands inside the Anaconda environment, could you please check if you are facing the same issue outside the Anaconda environment as well? Thanks!", "I can confirm that this bug occurs with:\r\n1) TensorFlow built from source (master, 2.4.0)\r\n2) Conda environment running python 3.7.9\r\nThis is quite new.  I built from master 2.4.0 a few days ago and this error did not occur.\r\nCheers", "@amahendrakar I can see you don't seem to be getting this error with python 3.8.5.  Maybe it is a problem with the tf-nightly for python 3.7.\r\nI have also confirmed this bug occurs with:\r\n\r\n- tf_nightly-2.4.0.dev20201021-cp37-cp37m-manylinux2010_x86_64.whl (pip install tf-nightly)\r\n- Conda environment running python 3.7.9 (I also tried a conda environment running python 3.7.8 and the bug persists)\r\n\r\n\r\nIt also occurs with:\r\n\r\n- tf_nightly-2.4.0.dev20201021-cp37-cp37m-manylinux2010_x86_64.whl (pip install tf-nightly)\r\n- Virtual environment created with the standard python module venv using python 3.7.9 from pyenv (python -m venv trialvenv)\r\n", "The bug also occurs with:\r\n\r\n- tf_nightly-2.4.0.dev20201022     (recently released)\r\n- Conda environment running python 3.7.9\r\n\r\nThe bug does not occur with tf_nightly-dev20201020 and previous tf_nightly's so it is new.", "The bug also occurs with tf_nightly-2.4.0-dev20201023", "The bug still propages to next versions and appears with tf-nightly 2.5.0.dev20201026", "I confirm I can also see the issue in the docker container `docker pull tensorflow/tensorflow:nightly-gpu` built yesterday: https://hub.docker.com/layers/tensorflow/tensorflow/nightly-gpu/images/sha256-a878f2a5309a037229b8b0002ae8cd0243ca0016e2716e6ebc54192d356f7ed2?context=explore", "Just use tensorflow==2.3.1", "Was able to reproduce\r\n\r\n```\r\nmihaimaruseac@ankh:/tmp$ python3.7 -m venv venv\r\nmihaimaruseac@ankh:/tmp$ source venv/bin/activate\r\n(venv) mihaimaruseac@ankh:/tmp$ pip install -q tf-nightly\r\n(venv) mihaimaruseac@ankh:/tmp$ pip list | grep tf\r\ntf-estimator-nightly   2.4.0.dev2020102301\r\ntf-nightly             2.5.0.dev20201027\r\n(venv) mihaimaruseac@ankh:/tmp$ python\r\nPython 3.7.7 (default, May 20 2020, 19:38:38) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-10-27 12:09:10.257298: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2020-10-27 12:09:10.257332: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>> exit()\r\nException ignored in: <function Buckets.__del__ at 0x7f47dfd2de60>\r\nTraceback (most recent call last):\r\n  File \"/tmp/venv/lib/python3.7/site-packages/tensorflow/python/eager/monitoring.py\", line 407, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'\r\n(venv) mihaimaruseac@ankh:/tmp$ python -c \"import tensorflow as tf\"\r\n2020-10-27 12:09:39.451204: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2020-10-27 12:09:39.451246: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nException ignored in: <function Buckets.__del__ at 0x7fa003a6a560>\r\nTraceback (most recent call last):\r\n  File \"/tmp/venv/lib/python3.7/site-packages/tensorflow/python/eager/monitoring.py\", line 407, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'\r\n```\r\n\r\nLooking into this", "Note though that the message is just a warning about an ignored exception. Exit code is still success\r\n\r\n```\r\n(venv) mihaimaruseac@ankh:/tmp$ python -c \"import tensorflow as tf\"\r\n2020-10-27 12:23:51.950878: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2020-10-27 12:23:51.950930: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nException ignored in: <function Buckets.__del__ at 0x7fb5abc5e560>\r\nTraceback (most recent call last):\r\n  File \"/tmp/venv/lib/python3.7/site-packages/tensorflow/python/eager/monitoring.py\", line 407, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'\r\n(venv) mihaimaruseac@ankh:/tmp$ echo $?\r\n0\r\n```", "@alanpurple,\r\nAny updates regarding this? Please take a look at the above comment and let us know if this is still an issue. Thanks!", "@amahendrakar \r\nstill error, but as @mihaimaruseac said, it seems like just an warning message", "Same error for me:\r\nTensorflow version: `2.5.0-dev20201027`\r\nPython version: `3.6.12` |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56) \r\n[GCC 7.3.0]", "Same \r\nI can add you get this Error when you train a faster_rcnn_inception_v2\r\nssd_mobilenet_v2 no problem", "Could it be related to commit 9b5e180a69e29a6266224a6c74868295ff5631a4?\r\n\r\nIt changes `tensorflow/python/eager/pywrap_tfe_src.cc` and the commit was roughly 10 days ago.", "Bisection points to 32f35aabce2b9c65d5244a8edf5904f367f28296 but that does not seem relevant, perhaps only surfacing an issue that exists from before.", "Same error for me at the moment\r\nWindows 10, Python 3.7\r\n```\r\n\r\nD:\\python37\\Scripts>pip install tf-nightly\r\nCollecting tf-nightly\r\n  Using cached https://files.pythonhosted.org/packages/ed/e1/3b28660594bea78e17d43ba22923bfb5299de1c324f5b521f2b089491ca8/tf_nightly-2.5.0.dev20201103-cp37-cp37m-win_amd64.whl\r\nCollecting flatbuffers~=1.12.0 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\r\nCollecting astunparse~=1.6.3 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\r\nCollecting h5py~=2.10.0 (from tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/a1/6b/7f62017e3f0b32438dd90bdc1ff0b7b1448b6cb04a1ed84f37b6de95cd7b/h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.5MB 1.6MB/s\r\nCollecting tf-estimator-nightly~=2.4.0.dev (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/89/d2/2131f5a0f0d14bae7f4d332724748b9ca6746b0d32f5c76145f0707f47d8/tf_estimator_nightly-2.4.0.dev2020102301-py2.py3-none-any.whl\r\nCollecting termcolor~=1.1.0 (from tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\r\nCollecting grpcio~=1.32.0 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/67/5f/bf822211f7f94a2f6d0f8fd3bda3b804d7b24b6d5c84dbc6e6c9df4c74c2/grpcio-1.32.0-cp37-cp37m-win_amd64.whl\r\nCollecting typing-extensions~=3.7.4 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\r\nCollecting protobuf~=3.13.0 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/6b/2e/28425c709c26525998be0b7a91c4090c87c38a1a9644fd43fefaea2e16c0/protobuf-3.13.0-cp37-cp37m-win_amd64.whl\r\nCollecting keras-preprocessing~=1.1.2 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\r\nCollecting wrapt~=1.12.1 (from tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\r\nCollecting tb-nightly~=2.4.0.a (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/40/a1/a6eabdbe2eae2a6c745ffa473a1d9d6f8d6d1d5d2e5fd1b86c95f821bf0b/tb_nightly-2.4.0a20201103-py3-none-any.whl\r\nCollecting google-pasta~=0.2 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl\r\nCollecting absl-py~=0.10 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/bc/58/0aa6fb779dc69cfc811df3398fcbeaeefbf18561b6e36b185df0782781cc/absl_py-0.11.0-py3-none-any.whl\r\nCollecting six~=1.15.0 (from tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\r\nCollecting gast==0.3.3 (from tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\r\nCollecting wheel~=0.35 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/a7/00/3df031b3ecd5444d572141321537080b40c1c25e1caa3d86cdd12e5e919c/wheel-0.35.1-py2.py3-none-any.whl\r\nCollecting opt-einsum~=3.3.0 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl\r\nCollecting numpy~=1.19.2 (from tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/5f/a5/24db9dd5c4a8b6c8e495289f17c28e55601769798b0e2e5a5aeb2abd247b/numpy-1.19.4-cp37-cp37m-win_amd64.whl\r\nRequirement already satisfied: setuptools in d:\\python37\\lib\\site-packages (from protobuf~=3.13.0->tf-nightly) (41.2.0)\r\nCollecting google-auth<2,>=1.6.3 (from tb-nightly~=2.4.0.a->tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/1d/60/81e68e70eea91ef05bb00bcdac243d67b61f826c65aaca6961de622dffd7/google_auth-1.23.0-py2.py3-none-any.whl\r\nCollecting requests<3,>=2.21.0 (from tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 2.2MB/s\r\nCollecting werkzeug>=0.11.15 (from tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 307kB 2.2MB/s\r\nCollecting tensorboard-plugin-wit>=1.6.0 (from tb-nightly~=2.4.0.a->tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/b6/85/5c5ac0a8c5efdfab916e9c6bc18963f6a6996a8a1e19ec4ad8c9ac9c623c/tensorboard_plugin_wit-1.7.0-py3-none-any.whl\r\nCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tb-nightly~=2.4.0.a->tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/81/67/e2c34bb0628984c7ce71cce6ba6964cb29c418873847fc285f826e032e6e/google_auth_oauthlib-0.4.2-py2.py3-none-any.whl\r\nCollecting markdown>=2.6.8 (from tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/ac/ef/24a91ca96efa0d7802dffb83ccc7a3c677027bea19ec3c9ee80be740408e/Markdown-3.3.3-py3-none-any.whl (96kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 3.2MB/s\r\nCollecting rsa<5,>=3.1.4; python_version >= \"3.5\" (from google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 3.4MB/s\r\nCollecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\r\nCollecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 2.2MB/s\r\nCollecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 2.2MB/s\r\nCollecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 2.2MB/s\r\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 2.2MB/s\r\nCollecting idna<3,>=2.5 (from requests<3,>=2.21.0->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 2.0MB/s\r\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\nCollecting importlib-metadata; python_version < \"3.8\" (from markdown>=2.6.8->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/6d/6d/f4bb28424bc677bce1210bc19f69a43efe823e294325606ead595211f93e/importlib_metadata-2.0.0-py2.py3-none-any.whl\r\nCollecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 1.3MB/s\r\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\r\nCollecting zipp>=0.5 (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.4.0.a->tf-nightly)\r\n  Downloading https://files.pythonhosted.org/packages/41/ad/6a4f1a124b325618a7fb758b885b68ff7b058eec47d9220a12ab38d90b1f/zipp-3.4.0-py3-none-any.whl\r\nInstalling collected packages: flatbuffers, wheel, six, astunparse, numpy, h5py, tf-estimator-nightly, termcolor, grpcio, typing-extensions, protobuf, keras-preprocessing, wrapt, absl-py, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, chardet, certifi, urllib3, idna, requests, werkzeug, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, zipp, importlib-metadata, markdown, tb-nightly, google-pasta, gast, opt-einsum, tf-nightly\r\n  WARNING: The script wheel.exe is installed in 'd:\\python37\\Scripts' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n  WARNING: The script f2py.exe is installed in 'd:\\python37\\Scripts' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n  Running setup.py install for termcolor ... done\r\n  Running setup.py install for wrapt ... done\r\n  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'd:\\python37\\Scripts' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n  WARNING: The script chardetect.exe is installed in 'd:\\python37\\Scripts' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n  WARNING: The script google-oauthlib-tool.exe is installed in 'd:\\python37\\Scripts' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n  WARNING: The script markdown_py.exe is installed in 'd:\\python37\\Scripts' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n  WARNING: The script tensorboard.exe is installed in 'd:\\python37\\Scripts' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'd:\\python37\\Scripts' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\nSuccessfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 flatbuffers-1.12 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 idna-2.10 importlib-metadata-2.0.0 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.4 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 six-1.15.0 tb-nightly-2.4.0a20201103 tensorboard-plugin-wit-1.7.0 termcolor-1.1.0 tf-estimator-nightly-2.4.0.dev2020102301 tf-nightly-2.5.0.dev20201103 typing-extensions-3.7.4.3 urllib3-1.25.11 werkzeug-1.0.1 wheel-0.35.1 wrapt-1.12.1 zipp-3.4.0\r\nWARNING: You are using pip version 19.2.3, however version 20.2.4 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\r\n\r\nD:\\python37\\Scripts>cd ..\r\n\r\nD:\\python37>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n2020-11-04 17:04:04.516463: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'\r\nException ignored in: <function Buckets.__del__ at 0x000001B4B782AA68>\r\nTraceback (most recent call last):\r\n  File \"D:\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\monitoring.py\", line 407, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'\r\n```", "Note that your error is actually `AttributeError: module 'tensorflow' has no attribute 'GIT_VERSION'`", "@mihaimaruseac Oh, I am sorry, but `python -c \"import tensorflow as tf; print(tf.__version__)\"`  also failed.", "(this also happens in windows10 with tf@2.4-rc0, can remove the `subtype: ubuntu/linux`)", "32f35aabce2b9c65d5244a8edf5904f367f28296 is indeed the culprit and fix is in progress", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44192\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44192\">No</a>\n", "Error persist in version 2.5.0-dev20201105.", "It seems like a bug in the recent tf-nightly version, The bug will be gone with version 2.4.0.dev20200901.", "tf_nightly-2.5.0.dev20201106 which was pushed recently should be ok. At least the error no longer shows up on our log.", "I just built `master` and the issue did not occur anymore. Thanks for resolving :)", "My solution:\r\nThis error seems to happen when python version is 3.7.x and lower version, so I upgrade my python version to 3.8.0 and then the error did not happen.  \r\n \r\nMy environment\r\n--Windows 10\r\n--python 3.8.0\r\n--tensorflow2.4.0rc0 \r\n\r\nThanks for everyone who provides great solutions for this issue!!"]}, {"number": 44190, "title": "Fix that when hdfsBuilderConnect returns NULL, HadoopFileSystem::FileExists returns errors::NotFound exception", "body": "Fix that when `hdfsBuilderConnect` returns NULL, `HadoopFileSystem::Connect` returns `errors::NotFound`, which conflicts with `errors::NotFound` in `HadoopFileSystem::FileExists`. `errors::NotFound` means that [the file does not exist](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/errors_impl.py#L297)", "comments": ["/cc @jhseu @mihaimaruseac @vnvo2409", "@53jk1 please don't approve PRs where you don't have approval rights and was not asked for approval.", "@mihaimaruseac thanks!"]}, {"number": 44188, "title": "Fixing the markdown and disabling internal auto-formatting.", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44187, "title": "Use collections.deque for double-ended queues", "body": "This PR changes double-ended queues to [`collections.deque`](https://docs.python.org/3/library/collections.html#collections.deque) which is built for \"memory efficient appends and pops from either side with approximately the same O(1) performance in either direction.\"", "comments": []}, {"number": 44186, "title": "how save the value of a tensorflow operation in an array", "body": "save the value of a tensorflow operation in an array.\r\n\r\ni am using tensorflow v2 in jupyter\r\n\r\nI have a code that calculates the loss error, I would like to save the value for each time in the gen_loss_epoch array. I try to save the error value with the following code\r\n\r\n`gen_loss_epoch[epoch] = tf.print (gen_loss)`\r\n\r\nwhile training the error value is printed eg 0.990000 0.800 0.2000 etc thanks to tf.print (gen_loss)\r\n\r\nI thought that if I did ggg [time] = tf.print (gen_loss) it would save those loss values in the gen_loss_epoch array.\r\n\r\n```\r\ngen_loss_epoch=np.linspace(0,0,100)\r\n\r\n@tf.function()\r\ndef train_step(input_image, target,epoch):\r\n    global gen_loss_epoch\r\n    \r\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as discr_tape:\r\n        \r\n        output_image = generator(input_image, training=True)\r\n\r\n        output_gen_discr = discriminator([output_image, input_image], training=True)\r\n\r\n        output_trg_discr = discriminator([target, input_image], training=True)\r\n\r\n        discr_loss = discriminator_loss(output_trg_discr, output_gen_discr)\r\n      \r\n        gen_loss = generator_loss(output_gen_discr, output_image, target)\r\n        \r\n        #with tf.Session() as sess:\r\n        tf.print(\"error del discriminador=\", discr_loss)\r\n        tf.print(\"error del generador=\",  gen_loss)\r\n\r\n        gen_loss_epoch[epoch]=tf.print(gen_loss)\r\n        generator_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\r\n        \r\n        discriminator_grads = discr_tape.gradient(discr_loss, discriminator.trainable_variables)\r\n        \r\n        generator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))\r\n        \r\n        discriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_variables))\r\n```\r\n        \r\nbut when he finishes training I see that\r\n\r\n  >> type(gen_loss_epoch) \r\n  <tf.Operation 'PrintV2_2' type = PrintV2>\r\nthey are not numbers\r\n\r\nWhy does it return a tf.Operation? Why doesn't it save me the error value as it shows when training and just save a <tf.Operation 'PrintV2_2' type = PrintV2>?", "comments": ["@molo32,\r\nWith `gen_loss_epoch[epoch] = tf.print (gen_loss)`, you are assigning the `tf.print` method to the array instead of the value of `gen_loss`. Instead please try \r\n\r\n - `gen_loss_epoch[epoch] = gen_loss`, if `gen_loss` is a number or\r\n - `gen_loss_epoch[epoch] = gen_loss.numpy()`, if `gen_loss` is a Tensor.\r\n\r\nAlso, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44185, "title": "Multi-output custom loss model crashes: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()  ...  Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated. \t [[{{node PyFunc}}]]", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Docker container in CentOS Linux release 7.8.2003 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Docker container - tensorflow/tensorflow:latest-gpu\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 2x NVIDIA Tesla V100, 32510MiB (~34GB) of memory each\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI'm implementing a source-separation 2-output model, and it's made with the functional API. I'm running a custom loss function which uses 2-output targets and predictions (4 in total). For my loss function to work, my model is \"wrapped\" within a subclassed model. It crashes during training.\r\n\r\n**Describe the expected behavior**\r\nI expect it not to crash during training.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nfrom scipy.io import wavfile\r\nimport scipy.signal as sg\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, SimpleRNN, Dense, Lambda, TimeDistributed, Layer, LSTM, Bidirectional, BatchNormalization, Concatenate\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.activations import relu\r\nfrom tensorflow.keras.callbacks import EarlyStopping\r\nimport numpy as np\r\nimport datetime\r\nimport numpy as np\r\nimport math\r\nimport random\r\nimport json\r\nimport os\r\nimport sys\r\n\r\n\r\n\r\n# Loss function\r\ndef discriminative_loss(piano_true, noise_true, piano_pred, noise_pred, loss_const):\r\n    last_dim = piano_pred.shape[1] * piano_pred.shape[2]\r\n    return (\r\n        tf.math.reduce_mean(tf.reshape(noise_pred - noise_true, shape=(-1, last_dim)) ** 2, axis=-1) - \r\n        (loss_const * tf.math.reduce_mean(tf.reshape(noise_pred - piano_true, shape=(-1, last_dim)) ** 2, axis=-1)) +\r\n        tf.math.reduce_mean(tf.reshape(piano_pred - piano_true, shape=(-1, last_dim)) ** 2, axis=-1) -\r\n        (loss_const * tf.math.reduce_mean(tf.reshape(piano_pred - noise_true, shape=(-1, last_dim)) ** 2, axis=-1))\r\n    )\r\n\r\n\r\n\r\ndef make_model(features, sequences, name='Model'):\r\n\r\n    input_layer = Input(shape=(sequences, features), dtype='float32', \r\n                        name='piano_noise_mixed')\r\n    piano_true = Input(shape=(sequences, features), dtype='float32', \r\n                       name='piano_true')\r\n    noise_true = Input(shape=(sequences, features), dtype='float32', \r\n                       name='noise_true')\r\n\r\n    x = SimpleRNN(features // 2, \r\n                  activation='relu', \r\n                  return_sequences=True) (input_layer) \r\n    piano_pred = TimeDistributed(Dense(features), name='piano_hat') (x)  # source 1 branch\r\n    noise_pred = TimeDistributed(Dense(features), name='noise_hat') (x)  # source 2 branch\r\n  \r\n    model = Model(inputs=[input_layer, piano_true, noise_true],\r\n                  outputs=[piano_pred, noise_pred])\r\n\r\n    return model\r\n\r\n\r\n\r\n# Model \"wrapper\" for many-input loss function\r\nclass RestorationModel2(Model):\r\n    def __init__(self, model, loss_const):\r\n        super(RestorationModel2, self).__init__()\r\n        self.model = model\r\n        self.loss_const = loss_const\r\n       \r\n    def call(self, inputs):\r\n        return self.model(inputs)\r\n\r\n    def compile(self, optimizer, loss):\r\n        super(RestorationModel2, self).compile()\r\n        self.optimizer = optimizer\r\n        self.loss = loss\r\n\r\n    def train_step(self, data):\r\n        # Unpack data - what generator yeilds\r\n        x, piano_true, noise_true = data\r\n\r\n        with tf.GradientTape() as tape:\r\n            piano_pred, noise_pred = self.model((x, piano_true, noise_true), training=True)\r\n            loss = self.loss(piano_true, noise_true, piano_pred, noise_pred, self.loss_const)\r\n\r\n        trainable_vars = self.model.trainable_variables\r\n        gradients = tape.gradient(loss, trainable_vars)\r\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n        \r\n        return {'loss': loss}\r\n\r\n    def test_step(self, data):\r\n        x, piano_true, noise_true = data\r\n\r\n        piano_pred, noise_pred = self.model((x, piano_true, noise_true), training=False)\r\n        loss = self.loss(piano_true, noise_true, piano_pred, noise_pred, self.loss_const)\r\n        \r\n        return {'loss': loss}\r\n\r\n\r\n\r\ndef make_imp_model(features, sequences, loss_const=0.05, \r\n                   optimizer=tf.keras.optimizers.RMSprop(clipvalue=0.7),\r\n                   name='Restoration Model', epsilon=10 ** (-10)):\r\n    \r\n    # NEW Semi-imperative model\r\n    model = RestorationModel2(make_model(features, sequences, name='Training Model'),\r\n                              loss_const=loss_const)\r\n\r\n    model.compile(optimizer=optimizer, loss=discriminative_loss)\r\n\r\n    return model\r\n\r\n\r\n\r\n# MODEL TRAIN & EVAL FUNCTION\r\ndef evaluate_source_sep(train_generator, validation_generator,\r\n                        num_train, num_val, n_feat, n_seq, batch_size, \r\n                        loss_const, epochs=20, \r\n                        optimizer=tf.keras.optimizers.RMSprop(clipvalue=0.75),\r\n                        patience=10, epsilon=10 ** (-10)):\r\n   \r\n    print('Making model...')    # IMPERATIVE MODEL - Customize Fit\r\n    model = make_imp_model(n_feat, n_seq, loss_const=loss_const, optimizer=optimizer, epsilon=epsilon)\r\n    \r\n    print('Going into training now...')\r\n    hist = model.fit(train_generator,\r\n                     steps_per_epoch=math.ceil(num_train / batch_size),\r\n                     epochs=epochs,\r\n                     validation_data=validation_generator,\r\n                     validation_steps=math.ceil(num_val / batch_size),\r\n                     callbacks=[EarlyStopping('val_loss', patience=patience, mode='min')])\r\n    print(model.summary())\r\n\r\n\r\n\r\n# NEURAL NETWORK DATA GENERATOR\r\ndef my_dummy_generator(num_samples, batch_size, train_seq, train_feat):\r\n\r\n    while True:\r\n        for offset in range(0, num_samples, batch_size):\r\n\r\n            # Initialise x, y1 and y2 arrays for this batch\r\n            x, y1, y2 = (np.empty((batch_size, train_seq, train_feat)),\r\n                            np.empty((batch_size, train_seq, train_feat)),\r\n                            np.empty((batch_size, train_seq, train_feat)))\r\n\r\n            yield (x, y1, y2)\r\n\r\n\r\n\r\ndef main():\r\n    epsilon = 10 ** (-10)\r\n    train_batch_size = 5\r\n    loss_const, epochs, val_split = 0.05, 10, 0.25\r\n    optimizer = tf.keras.optimizers.RMSprop(clipvalue=0.9)\r\n\r\n    TRAIN_SEQ_LEN, TRAIN_FEAT_LEN = 1847, 2049\r\n    TOTAL_SMPLS = 60 \r\n\r\n    # Validation & Training Split\r\n    indices = list(range(TOTAL_SMPLS))\r\n    val_indices = indices[:math.ceil(TOTAL_SMPLS * val_split)]\r\n    num_val = len(val_indices)\r\n    num_train = TOTAL_SMPLS - num_val\r\n   \r\n    train_seq, train_feat = TRAIN_SEQ_LEN, TRAIN_FEAT_LEN\r\n    print('Train Input Stats:')\r\n    print('N Feat:', train_feat, 'Seq Len:', train_seq, 'Batch Size:', train_batch_size)\r\n\r\n    # Create data generators and evaluate model with them\r\n    train_generator = my_dummy_generator(num_train,\r\n                        batch_size=train_batch_size, train_seq=train_seq,\r\n                        train_feat=train_feat)\r\n    validation_generator = my_dummy_generator(num_val,\r\n                        batch_size=train_batch_size, train_seq=train_seq,\r\n                        train_feat=train_feat)\r\n\r\n    evaluate_source_sep(train_generator, validation_generator, num_train, num_val,\r\n                            n_feat=train_feat, n_seq=train_seq, \r\n                            batch_size=train_batch_size, \r\n                            loss_const=loss_const, epochs=epochs,\r\n                            optimizer=optimizer, epsilon=epsilon)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\nMatplotlib created a temporary config/cache directory at /tmp/matplotlib-w351htm7 because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\r\n2020-10-20 20:59:48.073656: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTrain Input Stats:\r\nN Feat: 2049 Seq Len: 1847 Batch Size: 5\r\nMaking model...\r\n2020-10-20 20:59:49.685893: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-20 20:59:51.341091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-20 20:59:51.343325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:83:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-20 20:59:51.343415: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-20 20:59:51.346449: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-20 20:59:51.349214: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-20 20:59:51.349659: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-20 20:59:51.352344: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-20 20:59:51.353860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-20 20:59:51.359411: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-20 20:59:51.367984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-10-20 20:59:51.368576: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-20 20:59:51.405603: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2245615000 Hz\r\n2020-10-20 20:59:51.435047: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c0cdc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-20 20:59:51.435197: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-20 20:59:51.659719: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44a7910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-20 20:59:51.659822: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100S-PCIE-32GB, Compute Capability 7.0\r\n2020-10-20 20:59:51.659849: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100S-PCIE-32GB, Compute Capability 7.0\r\n2020-10-20 20:59:51.665940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-20 20:59:51.668089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:83:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-20 20:59:51.668184: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-20 20:59:51.668387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-20 20:59:51.668620: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-20 20:59:51.668671: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-20 20:59:51.668786: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-20 20:59:51.668836: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-20 20:59:51.668883: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-20 20:59:51.677143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-10-20 20:59:51.677300: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-20 20:59:52.875191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-20 20:59:52.875315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \r\n2020-10-20 20:59:52.875346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y \r\n2020-10-20 20:59:52.875557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N \r\n2020-10-20 20:59:52.882402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:27:00.0, compute capability: 7.0)\r\n2020-10-20 20:59:52.885327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30132 MB memory) -> physical GPU (device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:83:00.0, compute capability: 7.0)\r\nGoing into training now...\r\n2020-10-20 20:59:53.516824: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\nEpoch 1/10\r\n9/9 [==============================] - ETA: 0s - loss: 0.0000e+00Traceback (most recent call last):\r\n  File \"dlnn_brahms_restore_clean.py\", line 212, in <module>\r\n    main()\r\n  File \"dlnn_brahms_restore_clean.py\", line 209, in main\r\n    optimizer=optimizer, epsilon=epsilon)\r\n  File \"dlnn_brahms_restore_clean.py\", line 158, in evaluate_source_sep\r\n    callbacks=[EarlyStopping('val_loss', patience=patience, mode='min')])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1137, in fit\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 416, in on_epoch_end\r\n    callback.on_epoch_end(epoch, numpy_logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 1664, in on_epoch_end\r\n    if self.monitor_op(current - self.min_delta, self.best):\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n2020-10-20 21:00:05.825863: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n\t [[{{node PyFunc}}]]\r\n", "comments": ["The problem is that in the callback your `current` is an array and `self.min_delta` is just an `int`.\r\nhttps://github.com/tensorflow/tensorflow/blob/c587b9a2dcd6595b70b57bd8d8a4563968cdb128/tensorflow/python/keras/callbacks.py#L1762-L1765", "I also got an similar error \r\n```273/273 [==============================] - ETA: 0s - loss: 0.5586 - accuracy: 0.5598Traceback (most recent call last):\r\n  File \"src/models/efficientNet2.py\", line 129, in <module>\r\n    callbacks=callbacks_list\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1122, in fit\r\n    steps_per_execution=self._steps_per_execution)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1117, in __init__\r\n    model=model)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 786, in __init__\r\n    peek, x = self._peek_and_restore(x)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 843, in _peek_and_restore\r\n    peek = next(x)\r\n  File \"/home/gl/EfficientNetCana/src/models/cromai/generator.py\", line 339, in get_next_batch\r\n    self.apply_labels()\r\n  File \"/home/gl/EfficientNetCana/src/models/cromai/generator.py\", line 328, in apply_labels\r\n    self.XY = self.img_data_gen.flow(self.X, self.Y, self.batch_size)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/preprocessing/image.py\", line 866, in flow\r\n    subset=subset)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/preprocessing/image.py\", line 461, in __init__\r\n    **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 126, in __init__\r\n    'with shape', self.x.shape)\r\nValueError: ('Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape', (0,))\r\n2020-10-21 19:29:44.298458: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n\t [[{{node PyFunc}}]]\r\n```\r\n\r\nin my case I was able to train the model changing the batch size (not ideal but worked).", "@Gabrielllopes I don't think it is similar.", "@Gabrielllopes sorry for the false bug alarm if that's the case. \r\n\r\nDoes this mean it's impossible to use model.fit() if you have a custom loss dealing with all multi-output targets? I couldn't find this on the documentation.", "@QColeman97 Can you try to run your example on `tf-nightly` cause It seems to have other errors.", "@QColeman97 I modified you example also to run to the next TF version. \r\nI think that problem is that you are producing a `loss: `  in the log dictionary with an array of batch size len.\r\nThe monitor is expected to be a single value.", "@bhack I changed my loss to only return a single value now.\r\n\r\n```\r\ndef discriminative_loss(piano_true, noise_true, piano_pred, noise_pred, loss_const):\r\n    last_dim = piano_pred.shape[1] * piano_pred.shape[2]\r\n    return (\r\n        tf.math.reduce_mean(tf.reshape(noise_pred - noise_true, shape=(-1, last_dim)) ** 2) - \r\n        (loss_const * tf.math.reduce_mean(tf.reshape(noise_pred - piano_true, shape=(-1, last_dim)) ** 2)) +\r\n        tf.math.reduce_mean(tf.reshape(piano_pred - piano_true, shape=(-1, last_dim)) ** 2) -\r\n        (loss_const * tf.math.reduce_mean(tf.reshape(piano_pred - noise_true, shape=(-1, last_dim)) ** 2))\r\n    )\r\n```\r\n\r\nHere is the new output on tf-nightly:\r\n\r\nMatplotlib created a temporary config/cache directory at /tmp/matplotlib-k5_af0cg because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\r\n2020-10-25 22:12:04.932403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nTrain Input Stats:\r\nN Feat: 2049 Seq Len: 1847 Batch Size: 5\r\nMaking model...\r\n2020-10-25 22:12:06.998388: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-25 22:12:07.000269: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-10-25 22:12:07.018113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-25 22:12:07.020319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:83:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-25 22:12:07.020408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-25 22:12:07.025238: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-10-25 22:12:07.025333: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-10-25 22:12:07.027602: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-10-25 22:12:07.028092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-10-25 22:12:07.032374: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-25 22:12:07.033319: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-10-25 22:12:07.033639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-10-25 22:12:07.039718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2020-10-25 22:12:07.040347: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-25 22:12:07.069815: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-25 22:12:07.482667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-25 22:12:07.484845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:83:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-25 22:12:07.484950: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-25 22:12:07.485125: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-10-25 22:12:07.485220: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-10-25 22:12:07.485355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-10-25 22:12:07.485501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-10-25 22:12:07.485644: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-25 22:12:07.485823: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-10-25 22:12:07.486045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-10-25 22:12:07.491806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2020-10-25 22:12:07.492050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-10-25 22:12:09.256059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-25 22:12:09.256148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \r\n2020-10-25 22:12:09.256180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \r\n2020-10-25 22:12:09.256399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \r\n2020-10-25 22:12:09.361347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 793 MB memory) -> physical GPU (device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:27:00.0, compute capability: 7.0)\r\n2020-10-25 22:12:09.366430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 29702 MB memory) -> physical GPU (device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:83:00.0, compute capability: 7.0)\r\nGoing into training now...\r\nTraceback (most recent call last):\r\n  File \"dlnn_brahms_restore_clean.py\", line 219, in <module>\r\n    main()\r\n  File \"dlnn_brahms_restore_clean.py\", line 216, in main\r\n    optimizer=optimizer, epsilon=epsilon)\r\n  File \"dlnn_brahms_restore_clean.py\", line 165, in evaluate_source_sep\r\n    callbacks=[EarlyStopping('val_loss', patience=patience, mode='min')])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1064, in fit\r\n    steps_per_execution=self._steps_per_execution)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\", line 1112, in __init__\r\n    model=model)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\", line 787, in __init__\r\n    lambda x: model(x, training=False), args=(concrete_x,))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1259, in run\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 2730, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 3417, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 572, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\", line 787, in <lambda>\r\n    lambda x: model(x, training=False), args=(concrete_x,))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1007, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"dlnn_brahms_restore_clean.py\", line 103, in call\r\n    return self.model(inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 993, in __call__\r\n    input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py\", line 207, in assert_input_compatibility\r\n    ' input tensors. Inputs received: ' + str(inputs))\r\nValueError: Layer model expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor: shape=(5, 1847, 2049), dtype=float32, numpy=\r\narray([[[0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        ...,\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.]],\r\n\r\n       [[0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        ...,\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.]],\r\n\r\n       [[0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        ...,\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.]],\r\n\r\n       [[0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        ...,\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.]],\r\n\r\n       [[0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        ...,\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.],\r\n        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>]\r\nException ignored in: <bound method Buckets.__del__ of <tensorflow.python.eager.monitoring.ExponentialBuckets object at 0x7f9c0bfbfee8>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/monitoring.py\", line 407, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'TFE_MonitoringDeleteBuckets'", "@QColeman97 It is working now. Do you meant with tf-nightly?", "For nightly you need to do other related changes like probably to `yield [x, y1, y2]` in your `my_dummy_generator`", "@bhack I meant with tf-nightly, yes.\r\n\r\n I see. This is my output with tf 2.3.1 with single value loss (do I just not have enough compute power?):\r\n\r\nMatplotlib created a temporary config/cache directory at /tmp/matplotlib-_8mmg1gr because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\r\n2020-10-25 23:07:01.505866: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTrain Input Stats:\r\nN Feat: 2049 Seq Len: 1847 Batch Size: 5\r\nMaking model...\r\n2020-10-25 23:07:03.082774: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-25 23:07:03.104389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-25 23:07:03.106598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:83:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-25 23:07:03.106682: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-25 23:07:03.110026: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-25 23:07:03.113600: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-25 23:07:03.114134: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-25 23:07:03.117347: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-25 23:07:03.118928: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-25 23:07:03.125340: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-25 23:07:03.131473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-10-25 23:07:03.132171: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-25 23:07:03.172811: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2245615000 Hz\r\n2020-10-25 23:07:03.202965: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e41c90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-25 23:07:03.203094: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-25 23:07:03.451461: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56dd930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-25 23:07:03.451568: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100S-PCIE-32GB, Compute Capability 7.0\r\n2020-10-25 23:07:03.451680: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100S-PCIE-32GB, Compute Capability 7.0\r\n2020-10-25 23:07:03.453587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:27:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-25 23:07:03.455757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: \r\npciBusID: 0000:83:00.0 name: Tesla V100S-PCIE-32GB computeCapability: 7.0\r\ncoreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s\r\n2020-10-25 23:07:03.455851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-25 23:07:03.456096: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-25 23:07:03.456342: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-25 23:07:03.456430: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-25 23:07:03.456675: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-25 23:07:03.456720: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-25 23:07:03.456774: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-25 23:07:03.462476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-10-25 23:07:03.462557: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-25 23:07:05.240595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-25 23:07:05.240691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 \r\n2020-10-25 23:07:05.240722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y \r\n2020-10-25 23:07:05.240898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N \r\n2020-10-25 23:07:05.245076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 797 MB memory) -> physical GPU (device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:27:00.0, compute capability: 7.0)\r\n2020-10-25 23:07:05.248527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 29706 MB memory) -> physical GPU (device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:83:00.0, compute capability: 7.0)\r\nGoing into training now...\r\n2020-10-25 23:07:05.881489: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\nEpoch 1/10\r\n2020-10-25 23:07:19.615180: W tensorflow/core/common_runtime/bfc_allocator.cc:431] Allocator (GPU_0_bfc) ran out of memory trying to allocate 72.18MiB (rounded to 75690240)requested by op gradient_tape/pow_1/mul\r\nCurrent allocation summary follows.\r\n2020-10-25 23:07:19.615968: I tensorflow/core/common_runtime/bfc_allocator.cc:970] BFCAllocator dump for GPU_0_bfc\r\n2020-10-25 23:07:19.616015: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (256): \tTotal Chunks: 5563, Chunks in use: 5563. 1.36MiB allocated for chunks. 1.36MiB in use in bin. 36.2KiB client-requested in use in bin.\r\n2020-10-25 23:07:19.616144: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-25 23:07:19.616264: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2020-10-25 23:07:19.616493: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-25 23:07:19.616821: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (4096): \tTotal Chunks: 4, Chunks in use: 4. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 16.0KiB client-requested in use in bin.\r\n2020-10-25 23:07:19.616862: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (8192): \tTotal Chunks: 7, Chunks in use: 6. 62.5KiB allocated for chunks. 49.5KiB in use in bin. 48.0KiB client-requested in use in bin.\r\n2020-10-25 23:07:19.616899: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (16384): \tTotal Chunks: 1849, Chunks in use: 1849. 36.11MiB allocated for chunks. 36.11MiB in use in bin. 36.11MiB client-requested in use in bin.\r\n2020-10-25 23:07:19.616941: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (32768): \tTotal Chunks: 1847, Chunks in use: 1847. 72.60MiB allocated for chunks. 72.60MiB in use in bin. 72.16MiB client-requested in use in bin.\r\n2020-10-25 23:07:19.617275: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (65536): \tTotal Chunks: 1, Chunks in use: 1. 64.5KiB allocated for chunks. 64.5KiB in use in bin. 40.0KiB client-requested in use in bin.\r\n2020-10-25 23:07:19.617321: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-25 23:07:19.617369: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-25 23:07:19.617418: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-25 23:07:19.617453: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-25 23:07:19.617594: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-25 23:07:19.617643: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (4194304): \tTotal Chunks: 4, Chunks in use: 4. 16.00MiB allocated for chunks. 16.00MiB in use in bin. 16.00MiB client-requested in use in bin.\r\n2020-10-25 23:07:19.617679: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (8388608): \tTotal Chunks: 10, Chunks in use: 10. 80.04MiB allocated for chunks. 80.04MiB in use in bin. 80.04MiB client-requested in use in bin.\r\n2020-10-25 23:07:19.617728: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-25 23:07:19.617785: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (33554432): \tTotal Chunks: 2, Chunks in use: 1. 72.13MiB allocated for chunks. 36.07MiB in use in bin. 36.07MiB client-requested in use in bin.\r\n2020-10-25 23:07:19.617832: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (67108864): \tTotal Chunks: 7, Chunks in use: 7. 519.12MiB allocated for chunks. 519.12MiB in use in bin. 505.29MiB client-requested in use in bin.\r\n2020-10-25 23:07:19.617917: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-25 23:07:19.617989: I tensorflow/core/common_runtime/bfc_allocator.cc:977] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2020-10-25 23:07:19.618034: I tensorflow/core/common_runtime/bfc_allocator.cc:993] Bin for 72.18MiB was 64.00MiB, Chunk State: \r\n2020-10-25 23:07:19.618097: I tensorflow/core/common_runtime/bfc_allocator.cc:1006] Next region of size 836239360\r\n2020-10-25 23:07:19.618123: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6000000 of size 1280 next 1\r\n2020-10-25 23:07:19.618137: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6000500 of size 4194304 next 5\r\n2020-10-25 23:07:19.618149: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400500 of size 256 next 8\r\n2020-10-25 23:07:19.618161: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400600 of size 256 next 13\r\n2020-10-25 23:07:19.618172: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400700 of size 256 next 20\r\n2020-10-25 23:07:19.618184: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400800 of size 256 next 21\r\n2020-10-25 23:07:19.618196: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400900 of size 256 next 22\r\n2020-10-25 23:07:19.618207: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400a00 of size 256 next 23\r\n2020-10-25 23:07:19.618219: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400b00 of size 256 next 24\r\n2020-10-25 23:07:19.618230: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400c00 of size 256 next 624\r\n2020-10-25 23:07:19.618241: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400d00 of size 256 next 1867\r\n2020-10-25 23:07:19.618252: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400e00 of size 256 next 1866\r\n2020-10-25 23:07:19.618263: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6400f00 of size 256 next 1868\r\n2020-10-25 23:07:19.618275: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6401000 of size 256 next 27\r\n2020-10-25 23:07:19.618287: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6401100 of size 256 next 28\r\n2020-10-25 23:07:19.618299: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6401200 of size 256 next 29\r\n2020-10-25 23:07:19.618310: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6401300 of size 256 next 30\r\n2020-10-25 23:07:19.618323: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6401400 of size 256 next 31\r\n2020-10-25 23:07:19.618334: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01b6401500 of size 256 next 11\r\n\r\n...\r\n\r\n2020-10-25 23:07:30.264760: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe26800 of size 20480 next 9204\r\n2020-10-25 23:07:30.264788: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe2b800 of size 20480 next 9208\r\n2020-10-25 23:07:30.264822: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe30800 of size 20480 next 9212\r\n2020-10-25 23:07:30.264850: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe35800 of size 20480 next 9216\r\n2020-10-25 23:07:30.264885: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe3a800 of size 20480 next 9220\r\n2020-10-25 23:07:30.264912: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe3f800 of size 20480 next 9224\r\n2020-10-25 23:07:30.264946: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe44800 of size 20480 next 9228\r\n2020-10-25 23:07:30.264978: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe49800 of size 20480 next 9232\r\n2020-10-25 23:07:30.265012: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe4e800 of size 20480 next 9236\r\n2020-10-25 23:07:30.265039: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe53800 of size 20480 next 9240\r\n2020-10-25 23:07:30.265067: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe58800 of size 20480 next 9244\r\n2020-10-25 23:07:30.265101: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe5d800 of size 20480 next 9247\r\n2020-10-25 23:07:30.265128: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe62800 of size 256 next 9248\r\n2020-10-25 23:07:30.265155: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe62900 of size 20480 next 9250\r\n2020-10-25 23:07:30.265190: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe67900 of size 20480 next 9252\r\n2020-10-25 23:07:30.265218: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe6c900 of size 20480 next 9256\r\n2020-10-25 23:07:30.265246: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe71900 of size 20480 next 9260\r\n2020-10-25 23:07:30.265273: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe76900 of size 20480 next 9264\r\n2020-10-25 23:07:30.265308: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe7b900 of size 20480 next 9268\r\n2020-10-25 23:07:30.265335: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe80900 of size 20480 next 9272\r\n2020-10-25 23:07:30.265362: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe85900 of size 20480 next 9276\r\n2020-10-25 23:07:30.265396: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe8a900 of size 20480 next 9280\r\n2020-10-25 23:07:30.265425: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe8f900 of size 20480 next 9284\r\n2020-10-25 23:07:30.265458: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe94900 of size 20480 next 9288\r\n2020-10-25 23:07:30.265486: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01cbe99900 of size 20480 next 9295\r\n2020-10-25 23:07:30.265523: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] Free  at 7f01cbe9e900 of size 37806080 next 9291\r\n2020-10-25 23:07:30.265557: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01ce2ac900 of size 37826560 next 9292\r\n2020-10-25 23:07:30.265585: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01d06bf900 of size 75690240 next 9293\r\n2020-10-25 23:07:30.265620: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01d4eeea00 of size 75690240 next 9294\r\n2020-10-25 23:07:30.265647: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01d971db00 of size 75690240 next 9296\r\n2020-10-25 23:07:30.265682: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01ddf4cc00 of size 75690240 next 9297\r\n2020-10-25 23:07:30.265710: I tensorflow/core/common_runtime/bfc_allocator.cc:1026] InUse at 7f01e277bd00 of size 90194688 next 18446744073709551615\r\n2020-10-25 23:07:30.265751: I tensorflow/core/common_runtime/bfc_allocator.cc:1031]      Summary of in-use Chunks by size: \r\n2020-10-25 23:07:30.265783: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 5567 Chunks of size 256 totalling 1.36MiB\r\n2020-10-25 23:07:30.265813: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 1280 totalling 1.2KiB\r\n2020-10-25 23:07:30.265850: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 4 Chunks of size 4096 totalling 16.0KiB\r\n2020-10-25 23:07:30.265885: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 6 Chunks of size 8448 totalling 49.5KiB\r\n2020-10-25 23:07:30.265914: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1849 Chunks of size 20480 totalling 36.11MiB\r\n2020-10-25 23:07:30.265949: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 38144 totalling 37.2KiB\r\n2020-10-25 23:07:30.265978: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1846 Chunks of size 41216 totalling 72.56MiB\r\n2020-10-25 23:07:30.266014: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 66048 totalling 64.5KiB\r\n2020-10-25 23:07:30.266042: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 3 Chunks of size 4194304 totalling 12.00MiB\r\n2020-10-25 23:07:30.266077: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 4198400 totalling 4.00MiB\r\n2020-10-25 23:07:30.266106: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 10 Chunks of size 8392704 totalling 80.04MiB\r\n2020-10-25 23:07:30.266134: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 37826560 totalling 36.07MiB\r\n2020-10-25 23:07:30.266163: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 6 Chunks of size 75690240 totalling 433.10MiB\r\n2020-10-25 23:07:30.266191: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] 1 Chunks of size 90194688 totalling 86.02MiB\r\n2020-10-25 23:07:30.266225: I tensorflow/core/common_runtime/bfc_allocator.cc:1038] Sum Total of in-use chunks: 761.43MiB\r\n2020-10-25 23:07:30.266257: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] total_region_allocated_bytes_: 836239360 memory_limit_: 836239360 available bytes: 0 curr_region_allocation_bytes_: 1672478720\r\n2020-10-25 23:07:30.266294: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Stats: \r\nLimit:                       836239360\r\nInUse:                       798420992\r\nMaxInUse:                    798434048\r\nNumAllocs:                       22334\r\nMaxAllocSize:                 90194688\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2020-10-25 23:07:30.266892: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ********************************************____***************************************************x\r\n2020-10-25 23:07:30.266948: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at tile_ops.cc:223 : Resource exhausted: OOM when allocating tensor with shape[5,3784503] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"dlnn_brahms_restore_clean.py\", line 219, in <module>\r\n    main()\r\n  File \"dlnn_brahms_restore_clean.py\", line 216, in main\r\n    optimizer=optimizer, epsilon=epsilon)\r\n  File \"dlnn_brahms_restore_clean.py\", line 165, in evaluate_source_sep\r\n    callbacks=[EarlyStopping('val_loss', patience=patience, mode='min')])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1098, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 840, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2829, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[5,3784503] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node gradient_tape/pow_1/mul (defined at dlnn_brahms_restore_clean.py:119) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n [Op:__inference_train_function_23861]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node gradient_tape/pow_1/mul:\r\n pow_1/y (defined at dlnn_brahms_restore_clean.py:36)\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\n2020-10-25 23:07:30.436665: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n\t [[{{node PyFunc}}]]\r\n", "Yes this is about GPU memory `OOM when allocating tensor with shape.`", "@bhack thanks for your help. Not to go down the rabbit-hole but, to validate this I lowered my batch size (for no OOM error), and ran this on my actual data (noisy audio mixes for source separation) and the verdict is that I see my loss and validation loss remain NaN over all epochs no matter how much I clip the gradient - trying clipvalues in range from [0.1, 20].\r\n\r\nI was able to achieve real-numbered loss in my previous implementation of this model with gradient clipping. Do you know why this model gives me bad results?\r\n\r\nFYI I only used the Keras functional API to make my previous model, which didn't let me put loss inside of model.compile() (because of an error saying the eager tensor my loss function returned was incompatible with the Keras DAG working with symbolic tensors) so I used model.add_loss() instead - this I believe hogged GPU memory and caused my OOM errors to begin with.\r\n\r\nThese are the two outputs of my model (clear source & noisy source) which are zero-valued from NaN loss:\r\n![clear_output_spgm](https://user-images.githubusercontent.com/25238854/97261351-44409e80-17e4-11eb-965e-3b83af24a3cd.png)\r\n![noise_output_spgm](https://user-images.githubusercontent.com/25238854/97261431-6f2af280-17e4-11eb-9b9c-0c2e30cfd467.png)", "I think this Is another problem. You can try on stackoverflow", "No problem, thanks for all the help.\r\n\r\n(edit - solution: https://github.com/tensorflow/tensorflow/issues/39892#issuecomment-765117453)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44185\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44185\">No</a>\n", "For all interested, solution to this problem: https://github.com/keras-team/keras/issues/14140#issuecomment-713660704"]}, {"number": 44184, "title": "Temporary testing cortex-m4", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44183, "title": "Switch conv, depthwise_conv and fully_connected kernels to flat namespace and build fixes.", "body": "This pull request applies flat tflite namespaces to conv, depthwise_conv and fully_connected kernels.\r\nAlso fixed some minor bugs which prevent building after [-Werror] flag was added.\r\n\r\nFixes #43686", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44181, "title": "clang: error: linker command failed with exit code 1 (use -v to see invocation)", "body": "hi, \r\nI was trying to run:\r\n\r\n**_make -f tensorflow/lite/micro/tools/make/makefile test_hello_world_test_**\r\n\r\nand had an error: **clang: error: linker command failed with exit code 1 (use -v to see invocation)**\r\n\r\nUsing make to build other source files gave me the same error. I'm using macOSX Catalina 10.15.7\r\n\r\nCheers\r\n\r\n\r\n\r\n@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["@ShuojinHang \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "https://github.com/tensorflow/tensorflow/pull/44196 should fix the issue, and will close this bug. Please reopen if that does not work.", "Yes, it worked. Thank you very much!"]}, {"number": 44180, "title": "[DOC] Removed extraneous period in TFLite code example", "body": "This code example in the TFLite documentation [found here](https://www.tensorflow.org/lite/convert/index#convert_a_savedmodel_recommended_) has an extraneous period, likely a typo. \r\n\r\nThis results in a syntax error if the reader copies and pastes the code.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44180) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 44179, "title": "File system scheme 'gs' not implemented", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: [2.3.1](https://github.com/tensorflow/tensorflow/releases/tag/v2.3.1)\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?: pip (in virtualenv)\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 10.2 / 7.6.5\r\n- GPU model and memory: Tesla P100, 16GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nUsing `tensorflow.io.gfile.GFile` with the GCS filesystem results in `tensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 'gs' not implemented`, but only if TF is compiled from source. Using the vanilla `pip install tensorflow==2.3.1` yields no errors.\r\n\r\nTo clarify how the TF wheel was built, in the `./configure` step I've used all default options except for CUDA support. When building the pip package, I ran `bazel build --config=cuda --config=monolithic --config=v2 //tensorflow/tools/pip_package:build_pip_package`.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nRunning\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.io.gfile.GFile('gs://foo', mode='w') as bar:\r\n\tbar.write('baz')\r\n```\r\n\r\nYields\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/home/joao/py_3_7/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 101, in write\r\n    self._prewrite_check()\r\n  File \"/home/joao/py_3_7/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 87, in _prewrite_check\r\n    compat.as_bytes(self.__name), compat.as_bytes(self.__mode))\r\ntensorflow.python.framework.errors_impl.UnimplementedError: File system scheme 'gs' not implemented (file: 'gs://foo')\r\n```\r\n\r\n**Any other info / logs**\r\n1. It seems like this issue is related to [this](https://github.com/tensorflow/tensorflow/issues/38477#issuecomment-657929334) comment. A few comments below it was [suggested](https://github.com/tensorflow/tensorflow/issues/38477#issuecomment-659069010) to build the gcs plugin separately, but I was unable to complete that step (no `gcs_file_system.dll` found);\r\n2. This issue was reproduced on a different machine, with everything equal except the GPUs (GTX1080 8GB and Titan X 12GB);\r\n3. The example above is a dummy GCS path, but the error is the same when a previously working path is given.\r\n\r\nThanks :)", "comments": ["I believe the option `--config=monolithic` is the issue here. I think this PR should fix the problem but it doesn't. However, this PR could give you more insight why the `--config=monolithic` failed. https://github.com/tensorflow/tensorflow/pull/43594\r\n\r\nI don't remember extractly but I think one of `tf.io` or `tf.io.gfile` should work with `--config=monolithic`. Could you try `tf.io.read_file` and tell me the result ?", "> I believe the option `--config=monolithic` is the issue here. I think this PR should fix the problem but it doesn't. However, this PR could give you more insight why the `--config=monolithic` failed. #43594\r\n> \r\n> I don't remember extractly but I think one of `tf.io` or `tf.io.gfile` should work with `--config=monolithic`. Could you try `tf.io.read_file` and tell me the result ?\r\n\r\nInteresting! Can confirm that `tf.io.read_file()` works fine. Similarly, `tf.io.write_file()` can also write into GCS. So it seems like the problem is restricted to `tf.io.gfile` -- I'm building without `--config=monolithic`, and I'll post results here soon.", "@vnvo2409 just as you wrote, building without `--config=monolithic` makes everything work \ud83d\udc4d \r\n\r\nPerhaps a noob question: what are the main advantages of using `--config=monolithic`?", "> what are the main advantages of using `--config=monolithic` ?\r\n\r\nI am not sure about it but we have to use `--config=monolithic` on Windows because building and linking `dll` on Windows is not trivial. `--config=monolithic` will build everything into one lib. On other OS, we don't enable this option. So I think the main advantages of using `--config=monolithic` is the same as the advantages of static linking over dynamic linking ( easier to link,  easier to distribute the binary, faster because we only have to link once, etc )", "@gante \r\nJust to confirm. You built TF with #43594 was merged or not yet ?\r\nAnd could you close the issue if everthing is ok, please ?", "> @gante\r\n> Just to confirm. You built TF with #43594 was merged or not yet ?\r\n> And could you close the issue if everthing is ok, please ?\r\n\r\nA working solution can be obtained by building without `--config=monolithic` on release tag v2.3.1. I haven't tested building with #43594.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44179\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44179\">No</a>\n"]}, {"number": 44178, "title": "Deprecation warnings Model.state_updates and layer.updates when saving model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): happens on 2.3.1 (v2.3.0-54-gfcc4b966f1) and 2.4.0-dev20201020 (v1.12.1-44160-g72c19e8880)\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nGetting two deprecation warnings when saving a model (with default parameters).\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2334: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\r\n  warnings.warn('`Model.state_updates` will be removed in a future version. '\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1397: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\r\n  warnings.warn('`layer.updates` will be removed in a future version. '\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo warnings!\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nmodel = keras.models.Sequential([\r\n  keras.layers.Dense(1, input_shape=(1,)),\r\n])\r\n\r\nmodel.save('my_model')\r\n```\r\n[Colab](https://colab.research.google.com/drive/1tAx3zIJ9fKzpjitXmIhebxL_UkeFLidM?usp=sharing)\r\n", "comments": ["@Molkree \r\nI ran the code shared, these are just depreciation warnings, nothing to worry about they will not affect your performance..\r\nTo hide the warnings please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/27023#issuecomment-475684266) and let us know.", "I know that they are \"just\" deprecation warnings, however calling non deprecated method `save` on model and getting these warnings is weird to say the least. It means that `save` calls deprecated functions somewhere, shouldn't it be corrected? These warnings are here exactly for this reason, to remind us to replace code with something else (or just remove calls as warnings suggest that it's called automatically now) instead of just silencing all warnings.", "I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/82be60a30e3631dff4a7dda8037626c6/untitled452.ipynb) on tf-nightly [2.4.0-dev20201022].", "@Molkree Agree with you. Thanks for raising this issue. We will work on removing those warnings. Thanks again. ", "@Molkree This issue has been fixed in the latest nightly and the fix has been cherrypicked into the r2.4 branch. I am goign to close this bug, feel free to re-open if you think this is not fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44178\">No</a>\n"]}, {"number": 44177, "title": "ctc_loss generate different value between sparse label and dense label", "body": "With tensorflow 1.15, I get different value when using sparse tensor and dense tensor as labels separately. Is there something wrong with my code? I also use tensorflow 2.x to verf this and get the same result. The two different usage would get different vaule.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# params\r\nbatch_size = 3\r\nseq_len = 15\r\nlabel_len = 12\r\nnum_class = 20\r\n\r\n# construct input\r\nindices = np.array([[0, 0], [0, 4], [1, 9], [2, 5]], dtype=np.int64)\r\nvalues = np.array([1, 1, 1, 1], dtype=np.int64)\r\nshape = np.array([batch_size, label_len], dtype=np.int64) \r\ntensor = tf.SparseTensor(indices, values, shape)\r\ntensor = tf.cast(tensor, tf.int32)\r\n# generate label\r\nlogit = tf.random.uniform([seq_len,batch_size,num_class], dtype=tf.float32)\r\n\r\n# two diffenret loss\r\nloss_1 = tf.reduce_mean(\r\n    tf.nn.ctc_loss(\r\n        labels=tensor, inputs=logit,\r\n        sequence_length=seq_len * np.ones(batch_size)\r\n    ),\r\n    name='ctc_loss1'\r\n)\r\nloss_2 = tf.reduce_mean(\r\n    tf.nn.ctc_loss_v2(\r\n        labels=tf.sparse_tensor_to_dense(tensor, default_value=num_class-1),\r\n        logits=logit,\r\n        logit_length=seq_len * np.ones(batch_size, dtype=np.int32),\r\n        label_length=label_len * np.ones(batch_size, dtype=np.int32),\r\n        blank_index=num_class-1\r\n    ),\r\n    name='ctc_loss2'\r\n)\r\n\r\n# run\r\nsess = tf.compat.v1.Session()\r\nprint(sess.run([loss_1, loss_2]))\r\n# output is : [39.27905, 740.8396]\r\n```\r\n", "comments": ["I have tried in colab with with TF 1.15 and was able to reproduce the issue. I am getting the below error message with TF 2.x. (`TypeError: ctc_loss_v3() got an unexpected keyword argument 'inputs'`)Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/3c87880e9c8f2ad7eb7dbf045a548844/untitled.ipynb).Thanks!", "> I have tried in colab with with TF 1.15 and was able to reproduce the issue. I am getting the below error message with TF 2.x. (`TypeError: ctc_loss_v3() got an unexpected keyword argument 'inputs'`)Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/3c87880e9c8f2ad7eb7dbf045a548844/untitled.ipynb).Thanks!\r\n\r\nThe api has changed the inputs to logits. So you shuold make some changes correspondingly.", "@wb-finalking \r\nPlease use 2.x and let us know if there are any issues as there is no support for 1.x.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44177\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44177\">No</a>\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "miss the same problem,exspecially in blank label=0"]}, {"number": 44176, "title": "Massive memory leaks due to data.Dataset.shuffle", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  18.04.1-Ubuntu\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 2080 Ti, ~11gb\r\n\r\n**Describe the current behavior**\r\nWhen a new iterator to a dataset containing a `shuffle()` iteration is opened after the old one became completely exhausted,\r\nthe memory held by the `ShuffleDataset` is not released / reused, resulting in massive memory leaks and ultimately in the process being killed by the OOM reaper.\r\n\r\nFor this purpose it does not matter whether we manually iterate over the dataset, use a Keras function like `Model.fit()` or chain a `Dataset.repeat()` operation at the end.\r\nThe original bug was found in production code and the condensed code below outlines roughly our original data pipeline\r\nbut perfectly reproduces the problem.\r\n\r\n**Describe the expected behavior**\r\nMemory usage should be constant when a new iterator to the Dataset is opened and there are no existing iterators anymore.\r\n\r\nTo be extra safe it might be desirable to immediately release any memory held by the `ShuffleDataset` when iteration is done,\r\nso that other components can use it. (maybe introduce parameter controlling the behaviour?). This could be very important in conjunction with `Dataset.interleave()`, e.g when we iterate 36 files with a `cycle_length` of four and only have enough memory to hold 4 shuffle buffers in memory. If memory is not immediately released, we would run out of memory after the first four files have been processed.\r\n\r\n**Standalone code to reproduce the issue**\r\nI run the code with the `memory-profiler` package (https://pypi.org/project/memory-profiler/) to generate plots of the memory usage. By default shuffle buffers are enabled but when any additional argv is passed, shuffle buffers will be disabled:\r\n\r\nExample usage: `mprof run --include-children test.py ` or `mprof run --include-children test.py no-shuffle` \r\n\r\nI recommend at least 32 GB of memory so that you can properly observe the behaviour. Otherwise feel free to tune down the memory usage in the code, for example by reducing the image size from 512x512 to 256x256.\r\n\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ndo_shuffle = len(sys.argv) <= 1\r\n\r\n# Simulate reading from files\r\nfilenames = tf.data.Dataset.from_tensor_slices(['{}.data'.format(i) for i in range(16)])\r\n\r\ndef read_files(files):\r\n    # In the original code we open TFRecordDatasets here\r\n    N = 8192 * 4\r\n\r\n    def gen():\r\n        for _ in range(N // 32):\r\n            yield tf.random.normal([32, 512, 512, 1])\r\n\r\n    rng_ds = tf.data.Dataset.from_generator(gen, tf.float32).unbatch()\r\n    return rng_ds\r\n\r\nreaders_ds = filenames.batch(4).map(read_files, num_parallel_calls=1, deterministic=True)\r\n\r\ndef process(ds):\r\n    # Create windows of 4 and add them as extra T dimension \r\n    window_size = 4\r\n    ds = ds.window(window_size, shift=1, drop_remainder=True).flat_map(lambda x: x).batch(window_size)\r\n    \r\n    # buffer size = 1.07 GB (256 * 4 * 512 * 512 * 4)\r\n    if do_shuffle:\r\n        ds = ds.shuffle(    \r\n            256, \r\n            reshuffle_each_iteration=True\r\n        )\r\n\r\n    return ds\r\n\r\n# interleave will result in 4 iterators being opened in parallel\r\n# which iterate the whole dataset (each iterates 4 files and there are 32 files in total)\r\nds = readers_ds.interleave(\r\n        process,\r\n        cycle_length=4,   # total buffer size: 1.07 GB * 4 = 4.29 GB\r\n        block_length=1,\r\n        num_parallel_calls=1,\r\n        deterministic=False\r\n    )\r\n\r\nds = ds.batch(32)\r\n\r\nfor e in range(30):\r\n    print('epoch: ', e)\r\n\r\n    # this creates a temporary iterator to the dataset\r\n    for x in ds:\r\n        pass\r\n```\r\n\r\n**Other info / logs**\r\nThe first run uses shuffling and we can clearly see the buffer filling up again after each epoch without the old memory being released (it appears that sometimes a small fraction is released though). I'm not sure why the buffers use 8gb in total opposed to the theoretical 4gb. After the fourth epoch the process is killed on my machine, because i run out of memory (32gb):\r\n\r\n![shuffle](https://user-images.githubusercontent.com/1858546/96578075-e6d4bb00-12d4-11eb-80bf-047a826e2e90.png)\r\n\r\nLog:\r\n```\r\nepoch:  0\r\nepoch:  1\r\nepoch:  2\r\nepoch:  3\r\nepoch:  4\r\n```\r\n\r\n-----------------------\r\n\r\nFor the second run I disabled shuffling and we can see that there is still some leakage yet much more irregularly. In previous test runs which used our original data-pipeline, I was able to achieve a flat memory usage by disabling the shuffling; I'm not sure why it doesn't work with the test script though. This might require further investigation. I manually terminated the script after a while.\r\n\r\n![no-shuffle](https://user-images.githubusercontent.com/1858546/96579856-bd695e80-12d7-11eb-941a-6f87677c418f.png)\r\n\r\nLog:\r\n```\r\nepoch:  0\r\nepoch:  1\r\nepoch:  2\r\nepoch:  3\r\nepoch:  4\r\nepoch:  5\r\n```", "comments": ["I cannot reproduce the issue when I ran your program using internal version of TensorFlow at HEAD and I am not aware of any issues that got fixed between TF 2.3 and now that could explain that.\r\n\r\nCould you set the `TF_CPP_VMODULE` environment variable to `dataset=2`? This will log which tf.data iterators are being constructed and destructed. The shuffle transformation buffer is owned by the iterator, which would allow us to establish whether the buffer memory is being released at the end of each epoch or not.\r\n\r\nLastly, how do you measure the memory consumption?", "@jsimsa Can you check this code. The memory usage keeps increasing.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport psutil\r\n\r\ndata = np.ones([int(1e7), 1], dtype=np.float32)\r\ndataset = tf.data.Dataset.from_tensor_slices(data)\r\niterator = dataset.shuffle(int(1e7)).batch(int(1e6)).repeat(10)\r\n\r\nfor it in iterator:\r\n  used_mem = psutil.virtual_memory().used\r\n  print(\"used memory: {} Mb\".format(used_mem / 1024 / 1024))\r\n```\r\n\r\nAm I using it correctly? \r\nThank you.", "@shauidu I cannot reproduce this using [public colab](https://colab.research.google.com/drive/1Ci07KJHbHW5HRQ5l28AScnlbLYvG2oHR?authuser=1)\r\n\r\n![Screen Shot 2020-10-29 at 10 30 43 AM](https://user-images.githubusercontent.com/1072079/97610376-04d7a500-19d2-11eb-9e30-5fea77d8fc1c.png)\r\n\r\n", "I'm having the same issue.\r\n\r\nSystem information(s) (I've tried two different configurations):\r\nTensorflow : 2.3.0 (gpu) ; 2.0 (gpu)\r\nGPU: 2x 2080ti\r\nCUDA: 10.1; 10.0\r\nPython: 3.5\r\n\r\nIssue:\r\nRunning the above test script with or without GPU leads to huge memory usage very quickly\r\n\r\nOutput:\r\nused memory: 5203.69140625 Mb\r\nused memory: 5211.5078125 Mb\r\nused memory: 5272.92578125 Mb\r\nused memory: 5308.12890625 Mb\r\nused memory: 5356.12890625 Mb\r\nused memory: 5404.41015625 Mb\r\nused memory: 5453.68359375 Mb\r\nused memory: 5502.7421875 Mb\r\nused memory: 5551.15625 Mb\r\nused memory: 5601.9296875 Mb\r\nused memory: 5569.28515625 Mb\r\nused memory: 5578.01953125 Mb\r\nused memory: 5627.60546875 Mb\r\nused memory: 5677.09375 Mb\r\nused memory: 5723.94140625 Mb\r\nused memory: 5772.9375 Mb\r\nused memory: 5829.57421875 Mb\r\nused memory: 5871.0703125 Mb\r\nused memory: 5920.375 Mb\r\nused memory: 5971.19921875 Mb\r\n\r\nEdit:\r\nI've checked the colab configuration and it uses tensorflow==2.3.0 (not tensorflow-gpu==2.3.0). Therefore, my initial guess was that maybe the issue was related to library version. However, also this setup shows the same issue. \r\n\r\nGenerally speaking, running the code in CPU mode shows some memory cleaning, but overall, memory usage increases as follows:\r\n\r\nused memory: 6375.6015625 Mb\r\nused memory: 6423.8203125 Mb\r\nused memory: 6473.92578125 Mb\r\nused memory: 6530.765625 Mb\r\nused memory: 6571.97265625 Mb\r\nused memory: 6623.296875 Mb\r\nused memory: 6673.5546875 Mb\r\nused memory: 6718.78515625 Mb\r\nused memory: 6769.36328125 Mb\r\nused memory: 6376.51171875 Mb  <-- here's some cleaning\r\nused memory: 6693.9921875 Mb    <-- ??\r\nused memory: 6703.70703125 Mb\r\nused memory: 6751.8125 Mb\r\nused memory: 6801.23046875 Mb\r\nused memory: 6850.26953125 Mb\r\nused memory: 6899.07421875 Mb\r\nused memory: 6948.76953125 Mb\r\nused memory: 6998.671875 Mb\r\nused memory: 7048.421875 Mb\r\nused memory: 7115.67578125 Mb\r\n", "@federicoruggeri what do the numbers look like when you remove the `shuffle` transformation?", "@jsimsa Disabling shuffle gives the following results. Memory usage seems stable.\r\n\r\nCPU:\r\nused memory: 6492.44921875 Mb\r\nused memory: 6537.33203125 Mb\r\nused memory: 6555.30859375 Mb\r\nused memory: 6563.078125 Mb\r\nused memory: 6566.87890625 Mb\r\nused memory: 6584.36328125 Mb\r\nused memory: 6567.640625 Mb\r\nused memory: 6567.97265625 Mb\r\nused memory: 6569.578125 Mb\r\nused memory: 6569.79296875 Mb\r\nused memory: 6569.296875 Mb\r\nused memory: 6504.59765625 Mb\r\nused memory: 6538.203125 Mb\r\nused memory: 6556.16796875 Mb\r\nused memory: 6563.2109375 Mb\r\nused memory: 6566.70703125 Mb\r\nused memory: 6566.70703125 Mb\r\nused memory: 6566.79296875 Mb\r\nused memory: 6576.90625 Mb\r\nused memory: 6568.70703125 Mb\r\n\r\nGPU:\r\nused memory: 7211.34375 Mb\r\nused memory: 7243.49609375 Mb\r\nused memory: 7255.85546875 Mb\r\nused memory: 7260.39453125 Mb\r\nused memory: 7264.484375 Mb\r\nused memory: 7213.5859375 Mb\r\nused memory: 7243.359375 Mb\r\nused memory: 7256.2890625 Mb\r\nused memory: 7276.30859375 Mb\r\nused memory: 7266.03515625 Mb\r\nused memory: 7265.80859375 Mb\r\nused memory: 7266.3125 Mb\r\nused memory: 7266.55078125 Mb\r\nused memory: 7266.390625 Mb\r\nused memory: 7272.328125 Mb\r\nused memory: 7265.828125 Mb\r\nused memory: 7265.41015625 Mb\r\nused memory: 7265.54296875 Mb\r\nused memory: 7265.46875 Mb\r\nused memory: 7265.32421875 Mb", "@federicoruggeri can you reproduce this issue in a colab? I cannot reproduce the issue in my environment.", "> @federicoruggeri can you reproduce this issue in a colab? I cannot reproduce the issue in my environment.\r\n\r\nI've tried the colab session that you've linked above, but I'm not able to reproduce the issue there. My initial guess was that the colab runtime was using tensorflow==2.3 (no gpu) but the same configuration does not work for me. Just to be sure about that, I've created a virtualenv from scratch with just tensorflow==2.3 and python 3.5.0. Running the same toy script does show the memory leak issue. \r\n\r\nTherefore, I don't know if the problem does not show in the colab session due to some additional package that is installed there. Could the python version mean something here? I'm a bit sceptic about this since @sehoffmann was using python 3.6.9 (yet another version).", "@federicoruggeri  can you try @jsimsa script in an official Tensorflow Docker image?", "Sorry for the late reply!\r\n\r\nI've tried running the script in the following docker tensorflow image: tensorflow/tensorflow:latest-gpu-jupyter\r\nI don't know if the selected image is good for testing (if it is not the case, which one should I try?). The selected docker image runs tensorflow-gpu==2.3.1. \r\nRunning the test script on CPU, gives the following results:\r\n\r\nCPU, no shuffle:\r\nused memory: 5913.8671875 Mb\r\nused memory: 5958.3046875 Mb\r\nused memory: 5976.640625 Mb\r\nused memory: 5983.40234375 Mb\r\nused memory: 5986.78125 Mb\r\nused memory: 5912.99609375 Mb\r\nused memory: 5957.484375 Mb\r\nused memory: 5976.9375 Mb\r\nused memory: 5984.3203125 Mb\r\nused memory: 6001.578125 Mb\r\nused memory: 5988.734375 Mb\r\nused memory: 5988.7265625 Mb\r\nused memory: 5988.8046875 Mb\r\nused memory: 5988.8046875 Mb\r\nused memory: 5988.8046875 Mb\r\nused memory: 5988.8046875 Mb\r\nused memory: 6007.3828125 Mb\r\nused memory: 5988.41796875 Mb\r\nused memory: 5988.41015625 Mb\r\nused memory: 5988.34765625 Mb\r\n\r\nCPU, shuffle:\r\nused memory: 7384.109375 Mb\r\nused memory: 7432.59765625 Mb\r\nused memory: 7482.33984375 Mb\r\nused memory: 7532.17578125 Mb\r\nused memory: 7583.8671875 Mb\r\nused memory: 7633.65234375 Mb\r\nused memory: 7681.5234375 Mb\r\nused memory: 7731.34375 Mb\r\nused memory: 7780.64453125 Mb\r\nused memory: 7830.98046875 Mb\r\nused memory: 7791.765625 Mb\r\nused memory: 7801.609375 Mb\r\nused memory: 7850.3828125 Mb\r\nused memory: 7899.046875 Mb\r\nused memory: 7948.2890625 Mb\r\nused memory: 7997.28515625 Mb\r\nused memory: 8047.5390625 Mb\r\nused memory: 8097.25390625 Mb\r\nused memory: 8145.54296875 Mb\r\nused memory: 8194.12109375 Mb\r\n\r\nAs you can see, enabling shuffles causes the same memory leak issue.", "Can you check also with `tensorflow/tensorflow:2.4.0rc3-gpu`?", "Here you are!\r\n\r\nCPU, no shuffle:\r\nused memory: 6489.3828125 Mb\r\nused memory: 6527.87109375 Mb\r\nused memory: 6556.4140625 Mb\r\nused memory: 6584.17578125 Mb\r\nused memory: 6490.6875 Mb\r\nused memory: 6534.18359375 Mb\r\nused memory: 6562.65234375 Mb\r\nused memory: 6590.45703125 Mb\r\nused memory: 6499.765625 Mb\r\nused memory: 6534.37109375 Mb\r\nused memory: 6562.42578125 Mb\r\nused memory: 6590.02734375 Mb\r\nused memory: 6559.4296875 Mb\r\nused memory: 6546.87890625 Mb\r\nused memory: 6563.83984375 Mb\r\nused memory: 6591.390625 Mb\r\nused memory: 6559.734375 Mb\r\nused memory: 6534.88671875 Mb\r\nused memory: 6564.5 Mb\r\nused memory: 6591.3359375 Mb\r\n\r\nCPU, shuffle:\r\nused memory: 8369.921875 Mb\r\nused memory: 8429.8828125 Mb\r\nused memory: 8466.77734375 Mb\r\nused memory: 8519.40625 Mb\r\nused memory: 8568.71484375 Mb\r\nused memory: 8616.8203125 Mb\r\nused memory: 8665.1015625 Mb\r\nused memory: 8723.90625 Mb\r\nused memory: 8763.6171875 Mb\r\nused memory: 8812.03125 Mb\r\nused memory: 8652.1171875 Mb\r\nused memory: 8692.453125 Mb\r\nused memory: 8741.24609375 Mb\r\nused memory: 8790.6875 Mb\r\nused memory: 8841.984375 Mb\r\nused memory: 8897.66796875 Mb\r\nused memory: 8939.43359375 Mb\r\nused memory: 8989.12109375 Mb\r\nused memory: 9038.546875 Mb\r\nused memory: 9088.08984375 Mb\r\n", "Do you have an updated Nvidia driver?", "Info:\r\nNVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1 \r\n\r\nI've tried also the 450 driver \r\n\r\nIs it ok?", "It Is strange that It Is GPU only. Can you try to give a run with https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth", "@bhack Actually the problem seems to not related to the device. I've tested tensorflow-gpu (different versions) with and without GPU device, as well as standard tensorflow.\r\n\r\nAs you suggested, I've also tried by enabling memory growth (that's what I usually do) but the memory leak still appears.\r\n\r\nThe strange thing is that the same code on the colab session works fine. I don't know if it is a combination of tensorflow packages (and their versions) that does the trick..", "Yes It was very strange if It was related to the GPU package. The problem is the Colab has [its own build from soruce](https://colab.research.google.com/notebooks/tensorflow_version.ipynb#scrollTo=8UvRkm1JGUrk). \nSo we are not sure that in Docker or Wheels we are the Colab build.\n\nCan you try on Colab with `pip install tf-nightly`?", "Here's the link to the colab session: https://colab.research.google.com/drive/1MKQmTbly7BxSJVxEjlgdO-ZpRsVK0RhJ?usp=sharing\r\n\r\nI'm not sure I've done eveything correctly (debug print messages seem ok to me): running `!pip install tf-nightly` for the first time requires restarting the runtime. After that, I think the update is up.\r\n\r\nFor what concerns the test script, it runs fine without any memory issue.", "I have recently investigated the memory growth observed for OSS version of TensorFlow when `shuffle` is used. The conclusion of my investigation is that the memory growth is because of poor performance of the memory allocator (TensorFlow OSS uses system malloc by default). In my experiments, switching to use TCMalloc (details below) resulted in constant memory usage (and program speedup).\r\n\r\nFor the evaluation, I used the following simple input pipeline:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport psutil\r\n\r\ndataset = tf.Dataset.range(int(1e7))\r\niterator = dataset.shuffle(int(1e7)).batch(int(1e6))\r\n\r\nfor _ in iterator:\r\n  used_mem = psutil.virtual_memory().used\r\n  print(\"used memory: {} Mb\".format(used_mem / 1024 / 1024))\r\n```\r\n\r\nWhen executed on workstation, it produces the following output:\r\n\r\n```\r\n$ python example.py\r\n\r\nused memory: 19853.52734375 Mb\r\nused memory: 19905.6484375 Mb\r\nused memory: 19958.109375 Mb\r\nused memory: 20014.796875 Mb\r\nused memory: 20064.8359375 Mb\r\nused memory: 20061.375 Mb\r\nused memory: 20117.23828125 Mb\r\nused memory: 20172.8515625 Mb\r\nused memory: 20228.18359375 Mb\r\nused memory: 20278.62890625 Mb\r\n```\r\n\r\nI then installed tcmalloc using `sudo apt-get install libtcmalloc-minimal4` and used it for the same program, as follows:\r\n\r\n```\r\n$ LD_PRELOAD=/path/to/libtcmalloc_minimal.so.4 python example.py\r\n\r\nused memory: 19291.0859375 Mb\r\nused memory: 19307.90234375 Mb\r\nused memory: 19315.859375 Mb\r\nused memory: 19315.859375 Mb\r\nused memory: 19315.875 Mb\r\nused memory: 19317.8671875 Mb\r\nused memory: 19311.14453125 Mb\r\nused memory: 19317.3515625 Mb\r\nused memory: 19317.34765625 Mb\r\nused memory: 19316.96484375 Mb\r\n```\r\n\r\nNot only the gradual memory growth disappeared, but the program also ran 2x faster.", "Is this about a specific glibc version?", "Also I don't know if we could pilot some specific tuning with [`mallopt`](https://man7.org/linux/man-pages/man3/mallopt.3.html) e.g. `M_ARENA_MAX`", "Independently arrived at a similar memory issue in a training situation I'm working on. In my case, we had a lot of RAM, and we noticed that after 3 or so epochs, it no longer continued incrementing.\r\n\r\nAfter swapping to the malloc library above, from the start we had it staying constant! Thanks @jsimsa  for posting your fix! Is this dependency issue even mentioned anywhere in the TF docs?", "> Independently arrived at a similar memory issue in a training situation I'm working on. In my case, we had a lot of RAM, and we noticed that after 3 or so epochs, it no longer continued incrementing.\n> \n> After swapping to the malloc library above, from the start we had it staying constant! Thanks @jsimsa  for posting your fix! Is this dependency issue even mentioned anywhere in the TF docs?\n\nWhat Is your glibc version?", "2.27-3ubuntu1", "> 2.27-3ubuntu1\n\nIt could be nice to test this with a more recent glibc version", "> > 2.27-3ubuntu1\r\n> \r\n> It could be nice to test this with a more recent glibc version\r\n\r\nMy experiments used `Debian GLIBC 2.31-9`.", "@jsimsa Can you reproduce the memory growing in your example prefix `python` with the env `M_CHECK_=1`?", "@bhack Unfortunately, I will not have cycles to investigate this further in the near future.", "Ok just to confirm that I cannot reproduce your example with `M_CHECK_=1`", "Hi @jsimsa, So I tried your solution which solved my memory problem but it's 2x slower (TF version: 2.4.0). is there any fix planned to this issue in the upcoming releases ?", "Any tips on what to do if the tcmalloc LD_PRELOAD only works occasionally? \r\n1. I ran the command with the mentioned library `LD_PRELOAD=\"/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4\" python script.py` and it worked. \r\n2. I interrupted the run with CTRL+C.\r\n3. I started the same run again, with the exact same command line, but this time it didn't work.\r\n\r\nIt only worked 3 or 4 times so far (out of maybe 50 runs).\r\n\r\nUsing Ubuntu 20.04 amd64."]}, {"number": 44175, "title": "Workaround for blasLt batch size limitation", "body": "An issue was identified in cublasLtMatmul where large batch sizes are sometimes not supported. This PR changes the plan and execution logic to break large batch sizes up into supported sizes.\r\n\r\nIt also fixes a minor potential issue with SetBiasPointer not being called inside the lock.\r\n\r\n(This is a follow-up to https://github.com/tensorflow/tensorflow/pull/43237)\r\n\r\ncc @nluehr @timshen91 @sanjoy ", "comments": []}]