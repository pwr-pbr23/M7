[{"number": 8450, "title": "Move bazelrc to the master location so that it concatenates both configurations.", "body": "", "comments": ["Mac timeouts are unrelated, but we do need to move this file on the macs. They're still colorless.", "Could you file a bug for the bazelrc changes for macs?\r\nWe will take care of those too as soon as possible."]}, {"number": 8449, "title": "sporadic system crash on evaluation", "body": "Problem:\r\nsystem crashes completely testing the net (not training) - black screen and instant reboot\r\n(tryed nvidia-375 and nvidia-378 driver)\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttp://stackoverflow.com/questions/37504470/tensorflow-crashes-when-using-sess-run\r\n\r\n### Environment info\r\nOperating System:\r\n\r\n`Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-66-generic x86_64)`\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n```\r\nwilli@Grafik14:~$ ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root   558720 Jan 27 14:40 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Jan 27 14:40 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Jan 27 14:40 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Jan 27 14:40 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Jan 27 14:40 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Jan 27 15:16 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 Jan 27 15:16 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 root root 79337624 Jan 27 15:16 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Jan 27 15:16 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n```\r\nwilli@Grafik14:~$ pip show tensorflow\r\nName: tensorflow\r\nVersion: 1.0.1\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python2.7/dist-packages\r\nRequires: mock, numpy, protobuf, wheel, six\r\n```\r\n\r\n```\r\nwilli@Grafik14:~$ pip show tensorflow-gpu\r\nName: tensorflow-gpu\r\nVersion: 1.0.1\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python2.7/dist-packages\r\nRequires: mock, numpy, protobuf, wheel, six\r\n\r\n\r\n```\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n```\r\nwilli@Grafik14:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.1\r\n```\r\n\r\n\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n[code to reproduce](https://github.com/TheTesla/tftest)\r\n\r\n### What other attempted solutions have you tried?\r\nmcelog was empty\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.55GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 11.90G (12772704256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nINFO:tensorflow:Saving checkpoints for 3201 into /tmp/mnist_convnet_model/model.ckpt.\r\nINFO:tensorflow:loss = 0.462657, step = 3201\r\nINFO:tensorflow:\r\nINFO:tensorflow:global_step/sec: 24.3908\r\nINFO:tensorflow:loss = 0.391431, step = 3301\r\nINFO:tensorflow:Saving checkpoints for 3400 into /tmp/mnist_convnet_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 0.372794.\r\nWARNING:tensorflow:From tfMnist.py:156: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From tfMnist.py:156: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nINFO:tensorflow:Starting evaluation at 2017-03-15-22:40:29\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)\r\nINFO:tensorflow:Finished evaluation at 2017-03-15-22:40:30\r\nINFO:tensorflow:Saving dict for global step 3400: accuracy = 0.9089, global_step = 3400, loss = 0.326106\r\nWARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\r\n{'loss': 0.32610574, 'global_step': 3400, 'accuracy': 0.90890002}\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nExtracting MNIST-data/train-images-idx3-ubyte.gz\r\nExtracting MNIST-data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST-data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST-data/t10k-labels-idx1-ubyte.gz\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd121224850>, '_tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1.0\r\n}\r\n, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\r\n  equality = a == b\r\nWARNING:tensorflow:From tfMnist.py:145: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From tfMnist.py:145: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From tfMnist.py:145: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.55GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 11.90G (12772704256 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nINFO:tensorflow:Saving checkpoints for 3401 into /tmp/mnist_convnet_model/model.ckpt.\r\nINFO:tensorflow:loss = 0.447333, step = 3401\r\nINFO:tensorflow:\r\nINFO:tensorflow:global_step/sec: 24.4126\r\nINFO:tensorflow:loss = 0.367684, step = 3501\r\nINFO:tensorflow:Saving checkpoints for 3600 into /tmp/mnist_convnet_model/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 0.330527.\r\nWARNING:tensorflow:From tfMnist.py:156: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From tfMnist.py:156: calling evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nINFO:tensorflow:Starting evaluation at 2017-03-15-22:40:41\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)\r\n\r\n```", "comments": ["Asked and answered on stackoverflow\r\nhttp://stackoverflow.com/questions/37504470/tensorflow-crashes-when-using-sess-run\r\n\r\n\r\n"]}, {"number": 8448, "title": "For loop over 'NoneType' TensorFlow Dimensions", "body": "I'm implementing a new type of NN in TensorFlow. The difference is in the evaluation function, so instead of calling `tf.matmul()`, I call my own function, which we'll call `My_Function(A)`.\r\n\r\nA snippet of the code can be seen below, where `A` is the tensor on the left being multiplied by this new NN implementation, which is on the right. The equivalent tensorflow code would be `tf.matmul(A, this_new_NN)`.\r\n\r\n    def My_Function(self, A):\r\n        dims = A.get_shape().as_list()\r\n        shape = [dims[0], self.m] # Defining shape of resulting tensor\r\n        X = tf.placeholder(tf.float32, shape=[shape[0], shape[1]])\r\n        result = tf.zeros(tf.shape(X), dtype=tf.float32)\r\n        for xyz in self.property:\r\n            # Do some computation between A and xyz, xyz is a property of this_new_NN\r\n            # resulting to temp_H with dimension [shape[0], xyz.m] of type tf.tensor\r\n            dims_H = temp_H.get_shape().as_list()\r\n            indices = [[i,j] for i in range(0, dims_H[0]) for j in range(xyz.k, xyz.k+dims_H[1])]\r\n            # indices is a list of indices to update in \"result\"\r\n            values = tf.reshape(temp_H, [-1]) # Values in temp_H as 1D list\r\n            delta = tf.SparseTensor(indices, values, shape)\r\n            result += tf.sparse_tensor_to_dense(delta)\r\n        return result\r\n\r\nNow the problem I'm having is in the line where I calculate the `indices`, where I'm getting the error\r\n\r\n`TypeError: 'NoneType' object cannot be interpreted as an integer`\r\n\r\nNow, I understand that this error means that you cannot iterate a for loop over a type of `None`, but the problem I have is the test set and the training set have different values for `batch_size`. This means that when I go to create `result`, the first dimension is unknown which is why it is of type `None`. \r\n\r\nBut, to get the indices that I have to update in `result`, I have to use a for loop to generate those values as a list to feed into `delta` which I'm creating as a `tf.SparseTensor` so it can be added to `result`. \r\n\r\nMy question is, what is the best way to get the indices? I have tried replacing `dims_H[0]` in the for loop with a `tf.placeholder(tf.int32)` object instead, where I would then just pass the size when I run the session, but I get the error of\r\n\r\n`TypeError: 'Tensor' object cannot be interpreted as an integer`\r\n\r\nAny help would be greatly appreciated.\r\n\r\nEdit:\r\n\r\nJust for reference, this code is called in the following way, where `M` is the pre-built new NN composed with `tf.Variable` values.\r\n\r\n`Y1 = tf.nn.relu(M.My_Function(A) + B1)`\r\n\r\nwhere `B1` is the offset for this layer, and `A` is the input layer.\r\n\r\nEdit2:\r\n\r\n`result` should be a zero tensor *every time* `My_Function` is called. However, I have a suspicion that it is preserving the values of `result` with each function call. If this is right, please let me know what I need to do to change that.\r\n\r\nEdit3:\r\n\r\nWhen defining `A` it is defined as\r\n\r\n    X1 = tf.placeholder(tf.float32, [None, 28, 28, 1])\r\n    A = tf.reshape(X1, [-1, 28*28])\r\n\r\nAs the dimensions of `A` change between the training data and test data.", "comments": ["The issue seems to broil down to replacing \r\n`indices = [[i,j] for i in range(0, dims_H[0]) for j in range(xyz.k, xyz.k+dims_H[1])]`\r\nwith a TensorFlow graph implementation.  I believe in principle this is expressible using existing ops; some of the relevant ones seem to be `tf.shape(...)[0]` (which will give you the correct batch_size at execution time) and `tf.range()`, etc.  Could you give those a try?", "Using `tf.shape(...)[0]` and `tf.range()` so it's of the form `tf.range(0, tf.shape(...)[0], 1)` for the first and the equivalent `tf.range()` function for the second gives the error of\r\n\r\n`TypeError: 'Tensor' object is not iterable.`\r\n\r\nSo it sees the results of `tf.shape` as a tensor object. In fact, if I take the exact same line that's giving me problems, and just change `range` to `tf.range`, I get the same error about it being a Tensor, even though it's given as a list to `dims_H`", "Hmm, `tf.range()` should be able to handle scalar Tensors.  I just tried:\r\n\r\n```python\r\nwith tf.Session() as sess:\r\n  v = tf.Variable(np.zeros((2, 2)))\r\n  print sess.run(tf.range(0, tf.shape(v)[1]))\r\n```\r\nwith no issues.", "First of all Zongheng, I'd like to thank you for your help so far. But I'm still not able to get it working. The problem is still dealing with the same line, however it is a different error... I have posted the majority of my code as I believe it might have something to do with `tf.matmul()` being involved right before this process, yet I'm not 100% sure.\r\n\r\n    def My_Function(self, A):\r\n        dims = A.get_shape().as_list()\r\n        shape = [dims[0], self.m] # Defining shape of resulting tensor\r\n        with tf.Session() as sess:\r\n            X = tf.placeholder(tf.float32, shape=[shape[0], shape[1]])\r\n            result = tf.zeros(tf.shape(X), dtype=tf.float32)\r\n            for xyz in self.property:\r\n                # Computation is of the following form, where xyz.M is a tf.Variable() tensor\r\n                a = A[:, xyz.i:xyz.i + xyz.n] # Slice of A (the input tensor)\r\n                temp_H = tf.matmul(a, sub.M)\r\n                indices = [[i,j] for i in sess.run(tf.range(0, tf.shape(temp_H)[0])) for j in tf.sess(tf.range(xyz.k, xyz.k+tf.shape(temp_H)[1]))] # same but with sess.run, tf.range, tf.shape\r\n                # indices is a list of indices to update in \"result\"\r\n                values = tf.reshape(temp_H, [-1]) # Values in temp_H as 1D list\r\n                delta = tf.SparseTensor(indices, values, shape)\r\n                result += tf.sparse_tensor_to_dense(delta)\r\n        return result\r\n\r\n\r\nI have attempted changing `tf.shape(temp_H)[0]` to `tf.shape(result)[0]` (as they have the same first dimension) in that line but I still get the same error. More than likely the same error is going to occur for `j` in the above for loop, but the solution should be similar. \r\n\r\nI did attempt removing the `j` to just trouble shoot the line, changing the indices line to `indices = [[i] for i in sess.run(tf.range(0, tf.shape(temp_H)[0]))]` but it still gives the same error, so the _current_ error is not with `j` but with `i`.\r\n\r\nThe traceback is the following, dealing with the `indices = ` line still:\r\n\r\n```\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float\r\n[[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op 'Placeholder', defined at:\r\n  File \"/home/christopher/anaconda3/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\r\n    app.launch_new_instance()\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\r\n    app.start()\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-4-8452266bec9e>\", line 3, in <module>\r\n    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1587, in placeholder\r\n    name=name)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2043, in _placeholder\r\n    name=name)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/christopher/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float\r\n[[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]```\r\n", "@cmertin can you provide a (reasonably) minimal repro of this problem? It'd be helpful if you could modify the code snippet you provided so it runs as-is and produces the error you're seeing in your full model.", "@skye here is a gist version of my code. It is a minimal version and I want to point something out. On the line defining `indices`, where the error occurs, for the value/range of `j`, in my actual code it will be a different range. For example, instead of going from `tf.range(0, tf.shape(temp)[1])`, it will go to something like `tf.range(self.k, self.k + tf.shape(temp)[1])`. Doing that wouldn't work in the example I uploaded, but it's required in my main program. However, the definition of `i` is based on the definition in the example, so it is `tf.range(0, tf.shape(A)[0])` in my code as well as the gist.\r\n\r\nhttps://gist.github.com/cmertin/84dda9294a596b238958627b6728d13d", "Thank you for providing the repro script, it's very helpful. The problem with the `indices =` line is that you're running a computation that depends on the placeholder `X` (`temp` is derived from `a` which is derived from the My_Function argument `A`, and you've passed in `X`). As the error message suggests, you'll need to somehow pass in a feed_dict argument to your sess.run() call that provides a value for `X`.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@cmertin Hi, did you fix this problem? I am also try to loop over 'NoneType' TensorFlow Dimensions and I have the same error messages as you.", "@zhaopku Hi, me too. When i set x = tf.placeholder(tf.float32, [None,  3] ), i want to loop over the first dimmension of the x.  If i use \"for  i in range(x.get_shape().as_list()[0])\", the error is   \" 'NoneType' object cannot be interpreted as an integer\",  if \" for i in range(tf.shape(x)[0])\", error is \" 'Tensor' object cannot be interpreted as an integer\". Did you find a way to fix this? Thanks.", "In same situation as the others here, also trying to do what @zaczou mentioned.\r\n\r\n@girving can we reopen the issue?", "i m having the same issue. has this been fixed? I can't loop thru the None dimension of a placeholder. ", "Hi, does anyone solve this problem to use for loop over 'NoneType' placeholder? ", "Hi, I uses tf.scan for loop over 'NoneType'. Note that None dimension should be dimension 0.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/scan", "Has anyone found a solution to the issue? @ktsumura could you please show us a working example?", "Is this issue resolved?", "@cmertin how do you solve this problem afterwards?", "Also hitting this problem rn, trying to map over indices in the first (None) dimension.", "try disabling eager execution before building the graph !\r\n\r\n`import tensorflow.compat.v1 as tf\r\n\r\ntf.disable_v2_behavior()`\r\n"]}, {"number": 8447, "title": "im2txt checkpoint issue", "body": "I have compiled tensorflow from source. I have followed steps at https://github.com/tensorflow/models/tree/master/im2txt. But when I run bazel-bin/im2txt/run_inference, it gives an error:\r\n\r\n`bazel-bin/im2txt/run_inference   --checkpoint_path=${CHECKPOINT_DIR}   --vocab_file=${VOCAB_FILE}   --input_files=${IMAGE_FILE} --checkpoint_path=\"/home/raspberry/models/im2txt/im2txt_pretrained/model.ckpt-2000000\"`\r\n\r\nAnd error:\r\n\r\n Traceback (most recent call last):\r\n  File \"/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py\", line 83, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py\", line 63, in main\r\n    restore_fn(sess)\r\n  File \"/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/inference_utils/inference_wrapper_base.py\", line 96, in _restore_fn\r\n    saver.restore(sess, checkpoint_path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1428, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Tensor name \"lstm/basic_lstm_cell/biases\" not found in checkpoint files /home/raspberry/models/im2txt/im2txt_pretrained/model.ckpt-2000000\r\n\t [[Node: save/RestoreV2_380 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_380/tensor_names, save/RestoreV2_380/shape_and_slices)]]\r\n\r\nCaused by op u'save/RestoreV2_380', defined at:\r\n  File \"/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py\", line 83, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py\", line 49, in main\r\n    FLAGS.checkpoint_path)\r\n  File \"/home/raspberry/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/inference_utils/inference_wrapper_base.py\", line 116, in build_graph_from_config\r\n    saver = tf.train.Saver()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1040, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1070, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 675, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): Tensor name \"lstm/basic_lstm_cell/biases\" not found in checkpoint files /home/raspberry/models/im2txt/im2txt_pretrained/model.ckpt-2000000\r\n\t [[Node: save/RestoreV2_380 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_380/tensor_names, save/RestoreV2_380/shape_and_slices)]]\r\n\r\n\r\n\r\n", "comments": ["I have downgraded to 0.12 . But it still gives errors\r\n`\r\nbazel-bin/im2txt/run_inference   --checkpoint_path=${CHECKPOINT_DIR}   --vocab_file=${VOCAB_FILE}   --input_files=${IMAGE_FILE} --checkpoint_path=\"/home/raspberry/models/im2txt/im2txt_pretrained/model.ckpt-2000000\"`\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/raspberry/workspace/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py\", line 83, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/home/raspberry/workspace/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py\", line 49, in main\r\n    FLAGS.checkpoint_path)\r\n  File \"/home/raspberry/workspace/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/inference_utils/inference_wrapper_base.py\", line 115, in build_graph_from_config\r\n    self.build_model(model_config)\r\n  File \"/home/raspberry/workspace/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/inference_wrapper.py\", line 36, in build_model\r\n    model.build()\r\n  File \"/home/raspberry/workspace/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/show_and_tell_model.py\", line 356, in build\r\n    self.build_model()\r\n  File \"/home/raspberry/workspace/models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/show_and_tell_model.py\", line 247, in build_model\r\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(\r\nAttributeError: 'module' object has no attribute 'BasicLSTMCell'\r\n", "@cshallue  @ebrevdo does this have something to do with the recent RNN refactors?", "@mustafaxfe @concretevitamin \r\n\r\nYes, this has to do with the RNN refactors. It looks like what's happened here is you trained your model with a previous version of TF, and now it won't open with the new version of TF. Downgrading the TF version on its own won't help, because this repository's code has been updated for TF 1.0.\r\n\r\nIt's quite simple to rename the offending Variables in your previous checkpoint. I believe this will work, although you may need to tweak it a bit.\r\n\r\n```\r\nvars_to_rename = {\r\n      \"lstm/BasicLSTMCell/Linear/Matrix\": \"lstm/basic_lstm_cell/weights\",\r\n      \"lstm/BasicLSTMCell/Linear/Bias\": \"lstm/basic_lstm_cell/biases\",\r\n}\r\nnew_checkpoint_vars = {}\r\nreader = tf.train.NewCheckpointReader(path_to_checkpoint)\r\nfor old_name in reader.get_variable_to_shape_map():\r\n  if old_name in vars_to_rename:\r\n    new_name = vars_to_rename[old_name]\r\n  else:\r\n    new_name = old_name\r\n  new_checkpoint_vars[new_name] = tf.Variable(reader.get_tensor(old_name))\r\n\r\ninit = tf.global_variables_initializer()\r\nsaver = tf.train.Saver(new_checkpoint_vars)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(init)\r\n  saver.save(sess, path_to_new_checkpoint)\r\n```", "What will I do with this code segment?\r\nI edited code, but it is not working:\r\n```\r\nimport tensorflow as tf\r\nvars_to_rename = {\r\n      \"rnn/BasicLSTMCell/Linear/Matrix\": \"lstm/basic_lstm_cell/weights\",\r\n      \"rnn/BasicLSTMCell/Linear/Bias\": \"lstm/basic_lstm_cell/biases\",\r\n}\r\nnew_checkpoint_vars = {}\r\nreader = tf.train.NewCheckpointReader(\"/home/workspace/models/im2txt\")\r\nfor old_name in reader.get_variable_to_shape_map():\r\n  if old_name in vars_to_rename:\r\n    new_name = vars_to_rename[old_name]\r\n  else:\r\n    new_name = old_name\r\n  new_checkpoint_vars[new_name] = tf.Variable(reader.get_tensor(old_name))\r\n\r\ninit = tf.global_variables_initializer()\r\nsaver = tf.train.Saver(new_checkpoint_vars)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(init)\r\n  saver.save(sess, \"/home/workspace/models/im2txt/train\")\r\n```", "I just posted explicit instructions [here](https://github.com/tensorflow/models/issues/466)", "Thanks @cshallue!  I'm closing this issue for now; feel free to follow up in the other issue if it does not work for you @mustafaxfe."]}, {"number": 8446, "title": "Scatter_nd bug", "body": "I'm getting strange bug with scatter_nd (TF 1.0v).\r\n\r\n    InvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape[:IXDIM] +     \r\n    params_shape[IXDIM:], got updates.shape [1,2,4], indices.shape [1,2,2], params_shape [1,9,4]\r\n\r\nI've read the docs for scatter_nd function and it seems to me that following set of shapes are perfectly viable:\r\nMaybe I'm tired but, Using terms from the docs:\r\n`indices` rank: Q = 3\r\n`shape` rank : P = 3\r\nlast dim length for indices K = 2\r\n`updates` rank:  = Q-1+P-K = 3-1+3-2=3\r\nAll seems to fit.\r\nAnd full shape of `update` should be: \r\n`[indices_shape[0], indicies_shape[1], shape[K]] `\r\nwhich gives: \r\n`[1, 2, 4]`\r\n\r\nWhere is the problem?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "It is a bug report - now I'm sure - also it is the same as  #7585. ", "@Arsakes  Hi, I try to repeat your error with the following example (with your values of P, Q, K, and shape):\r\n\r\n![test](https://cloud.githubusercontent.com/assets/8649007/25023199/cb4ed708-204d-11e7-88da-e3ca0e714c75.jpg)\r\n\r\nbut it seems to work fine to me.\r\n\r\nDo you mind sharing more details how you get the error?", "Which tf. version you use 1.0???\r\nHere are details:\r\nhttp://stackoverflow.com/questions/43007410/tensorflow-unexpected-tf-gather-nd-behaviour-bug\r\nIf it is night build or 1.1 It is very likely the issue was solved.\r\n"]}, {"number": 8445, "title": "Non-Docker Codelab", "body": "In this codelab link: [TensorFlow For Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#1), it states:\r\n\r\n> This codelab will cover using both Docker and not using Docker. \r\n\r\nYet I can't find the non-Docker instructions anywhere.\r\nAm I missing something, or are they simply not there?", "comments": ["Following the [instructions in this link](https://www.tensorflow.org/versions/r0.12/get_started/os_setup#clone_the_tensorflow_repository), I ran:\r\n`git clone https://github.com/tensorflow/tensorflow`\r\n\r\nAnd with that, was able to continue the [instructions as per this page](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#2).\r\n\r\n"]}, {"number": 8444, "title": "Some minor optimizations for speeding up nearest neighbor resize.", "body": "By removing loop over channels. Impact mainly visible for more than 3\r\nchannels. Numbers for benchmark with 6 channels:\r\n\r\n```\r\nBefore:\r\nBenchmark                                        Time(ns) Iterations\r\n--------------------------------------------------------------------\r\nBM_Resize_ResizeNearestNeighbor_cpu_10_499_499   53222200        100  280.7M items/s\r\n\r\nAfter:\r\nBenchmark                                        Time(ns) Iterations\r\n--------------------------------------------------------------------\r\nBM_Resize_ResizeNearestNeighbor_cpu_10_499_499   44504450        100  335.7M items/s\r\n```", "comments": ["Can one of the admins verify this patch?", "Alternatively, you can use std::copy_n there instead of memcpy too.", "Two other things: can you also benchmark on GPU, and can you do best of 5 runs?", "My change doesn't touch any code that runs on GPU. Nevertheless, I ran the full benchmark 5 times on master and with this patched, see https://gist.github.com/panmari/4b495af0c383c26c1c50435d055c05a5. New version is consistently about 10% faster.", "Rebased on master.", "There's a much simpler version that gets the performance benefit:\r\n```c++\r\n    for (int b = 0; b < st.batch_size; ++b) {\r\n      for (int y = 0; y < st.out_height; ++y) {\r\n        const int64 in_y =\r\n            std::min(static_cast<int64>(floorf(y * st.height_scale)),\r\n                     (st.in_height - 1));\r\n        for (int x = 0; x < st.out_width; ++x) {\r\n          const int64 in_x =\r\n              std::min(static_cast<int64>(floorf(x * st.width_scale)),\r\n                       (st.in_width - 1));\r\n          std::copy_n(&input_data(b, in_y, in_x, 0), st.channels, &output_data(b, y, x, 0));\r\n        }\r\n      }\r\n    }\r\n```", "Thanks for the hint! The simplified version you proposed gave very similar benchmark results, so I adapted my pull request accordingly.", "Jenkins, test this please"]}, {"number": 8443, "title": "No such package 'tensorflow/tensorflow/tools/docs': BUILD file not found on package path", "body": "I am trying to generate docs locally as g3doc now recommends after the documentation change from about 2 weeks ago, but the command it says to run is failing.\r\n\r\nI am running `bazel run -- tensorflow/tools/docs:generate \\ --src_dir=tensorflow/docs_src/ \\ --output_dir=/tmp/tfdocs/`\r\n\r\nBut it fails with the error:\r\n\r\n```\r\nERROR: no such package 'tensorflow/tensorflow/tools/docs': BUILD file not found on package path.\r\nINFO: Elapsed time: 0.112s\r\nERROR: Build failed. Not running target.\r\n```\r\n\r\nI have the latest version of bazel installed, and my repo is up to date, so I'm not sure why this would be failing.  The `tools/docs/BUILD` file definitely exists, so it seems strange that it would tell me 'BUILD file not found on package path'.\r\n\r\nI figured since this change to the docs is very recent, there could potentially be some problems here...although I'm not sure why this would impact bazel.  Or is there something I'm missing?", "comments": ["I just tried that command on the latest master checkout and it worked for me. Can you verify that you're running the bazel command from the latest master branch, at the root of the repo, etc.?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8442, "title": "Serialization error in freeze_graph and/or optimize_for_inference_lib", "body": "I haven't found any mention of this anywhere online.\r\nAfter applying a workaround to #8404 another error turns up:\r\n\r\n`E/TensorFlowInferenceInterface: Failed to load model from 'file:///android_asset/optimized_model.pb': java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: read/bn4/BatchNorm/cond/AssignMovingAvg_1/decay = Const[_class=[\"loc:@BatchNorm_3/moving_variance\"], dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0.999>](read/bn4/BatchNorm/cond/Switch:1)`\r\n\r\nI have no ideas on what this error message even tries to tell me nor how to resolve the issue.", "comments": ["@petewarden \r\nDo you think this is in some way coupled to the bug behind #8404?", "After switching to the new Graph Transforms Tool like in #8404, the error message has gone (/changed...)"]}, {"number": 8441, "title": "XLA Feature request: Support Variables for tfcompile", "body": "Is it feasible to extend tfcompile to support training?\r\n\r\nOur reinforcement learning pipelines sometimes spend significant time in Python side of session.run because networks are small, and need lots of session.run calls since each .run leads to an interaction with a simulator (ie, Mujoco, gym or universe)\r\n\r\nSo you could have have 30k session.run calls where actual computation takes 200 usec, and Python session.run overhead is another 150 usec.\r\n\r\nThis could be improved if the network was tfcompiled and we didn't have to use session.run, however, this would need tfcompile to be extended to support multiple sets of fetches for the compiled object. IE\r\n\r\n1. network initialization, input: weights, output: None\r\n2. network forward op: input: observation, output: action\r\n3. network train op: input: observations, actions output: None\r\n4. network variable read: input: None, output: weights", "comments": ["cc @tatatodd ", "@yaroslavvb : If the overhead is purely Python, could you use the C++/C/Java/Go APIs to run the training loop after creating and exporting the graph from Python (as a SavedModel)?", "+1 if you're going to go as far as tfcompile; consider just using C++ to interface with your training loop.\r\n\r\nalternatively, write a TF C++ op wrapper that communicates with a mujoco session via server/client calls; and then your multiple interactions can be subsumied within a tf.while_loop.\r\n\r\nanother alternative might be to use session.partial_run to avoid the cost of copying data back and forth between python/C++.", "Training is in Python because each step interacts with other Python libraries. Mujoco, [gym](https://gym.openai.com/) and [Universe](https://universe.openai.com/) have  Python bindings. C++ bindings are either not as well tested (people tend to use Mujoco through Python) or missing completely (gym + universe).\r\n\r\nPartial run would help if I needed to persist some Tensors between run calls. But here I'm injecting new data each time. I can see how Mujoco C++ op would be more efficient but going down that route seems a bit scary", "tfcompile will create a C++ binary for you; you'd have to swig-wrap it to make it work with python which is IMO more daunting. I've noticed folks are afraid of writing C++ ops but IMO this is the least painful way to design interfaces with other systems thanks to the static type checking (and with Google's address sanitizer you get the additional benefit of ensuring you're not corrupting memory).\r\n\r\nyou could probably also try the pywrap to make a python op that interacts with mujoco within a tf.while_loop and is more lightweight than many session.run calls.", "I was hoping to avoid SWIG and use `ctypes` with tfcompiled object. IE something like\r\n\r\n```\r\nimport ctypes\r\nnetwork = ctypes.cdll.LoadLibrary(\"compiled.so\")\r\nresult = ctypes.c_float()\r\nnetwork.runforward(result)\r\n```", "if you get that to work, please do report back!", "Fair enough, though you could write C++ code just to execute the graph (equivalent to what the generated `tfcompile` shared library would do) and use it in a way similar you described, no? Just that the library is produced by compiling a `.cc` file that you write instead of being produced by `tfcompile`.\r\n\r\nAnyway, just an idea, I am in no way suggesting that this feature request isn't a reasonable one. Just pointing out possible equivalents you can pursue today without the feature.\r\n\r\nI will stop derailing this thread now :)", "@asimshankar Hm, good point. For your proposed alternative I could also use it through ctypes and `compiled.so` would be a C++ binary that I created which uses TF C++ API and implements those 4 modes: variable in, forward, train op, variable out\r\n\r\nI was assuming that using XLA AOT for this is more efficient because it gets rid of unneeded TF runtime bits. (ie, tiny networks perform best when restricted to 1 core, so don't need parallel scheduling)\r\n\r\nImplementing this feature would require changes to `tfcompile` to support Variables and multiple fetch configurations. On other hand it would make XLA AOT compiler more useful/widely applicable.\r\n\r\nI'll leave it to XLA maintainers to decide if this feature fits into the XLA vision/design", "/cc @tatatodd @hawkinsp ", "Wrt multiple fetch configurations, an alternative is to compile separate functions, one for each configuration.  The only downside that I see of this strategy is that perhaps lots of generated code is duplicated in each function; i.e. the combined size of the *.o files is larger than strictly necessary.  But it doesn't sound like that would be a big issue.\r\n\r\nSupporting Variables is trickier.  In theory we could hoist the variables outside of the \"pure graph\", and add logic to read/write the variables at the edges.  But it's not clear this is worth doing for the AOT codepath; it's quite a bit of work.  You could always do this manually though, so that you end up with a \"pure graph\" that can be AOT compiled.", "That said there is some experimental support for compiling code that manipulates variables in the JIT use case:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/jit/kernels/xla_device_launch_op.cc\r\n\r\n(That code only works for ResourceVariables, which will replace Variables at some point in the future.)\r\n\r\nCode that reads and writes variables is rewritten into a pure computation that takes the initial values of the variables as inputs and returns updated values as outputs. So most of the code one would need is actually already there.", "OK, it seems like some of the\u00a0performance benefits needed be already by making binary blob with C++ TF API and accesing it through ctypes. Benchmarking is needed to decide if extra work of adding variables to XLA is justified. Will close this for now, will reopen if I come across some concrete numbers."]}, {"number": 8440, "title": "Tensorboard conda envs not working", "body": "Hi guys,\r\n\r\nI am having a rather weird problem, which I hope someone here can solve. I am currently using the newest version of anaconda to create a python 3.5 environment with Tensorflow r1.0 installed. Inside my script I am perfectly able to create event files that should contain my Tensorflow graph, the script opens a writer that uses the tf.session graph as the graph and I create namescopes for the layers, weights and biases (Neural Network).\r\n\r\nUpon running an event file gets generated that also can be used to open up tensorboard from my command prompt. This generates an localhost:6006 which can be opened perfectly fine but there is no graph where it should be. Additionally when I use the tensorboard inspect logdir command it tells me the event file is found. Now I have no idea if it is a tensorboard problem, an installation problem or a script problem. \r\n\r\nDoes this perhaps sound familiar to anyone? Or is anyone perhaps able to open my event file? Would love to figure this out.\r\n\r\n[events.out.tfevents.1489598042.EH39G4DC.zip](https://github.com/tensorflow/tensorflow/files/845479/events.out.tfevents.1489598042.EH39G4DC.zip)\r\n\r\nIf needed i can upload my script! Thanks alot!", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. In addition, include a simple reproducible test case. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I have the exact same problem! Does anyone know how to fix this problem?"]}, {"number": 8439, "title": "tf_kernel_library doesn't work from repo containing tensorflow as a submodule", "body": "`tf_kernel_library` and friends from `tensorflow.bzl` refer to deps like `//tensorflow/core:gpu_lib`.  These deps do not work if TensorFlow exists only under `@org_tensorflow`, which results in errors like\r\n\r\n    bazel build ...\r\n    WARNING: /usr/local/google/home/geoffreyi/.cache/bazel/_bazel_geoffreyi/f928d2088d4c5ef4e2f54ff2af6f8a71/external/org_tensorflow/tensorflow/workspace.bzl:72:5: tf_repo_name was specified to tf_workspace but is no longer used and will be removed in the future.\r\n    WARNING: /usr/local/google/home/geoffreyi/.cache/bazel/_bazel_geoffreyi/f928d2088d4c5ef4e2f54ff2af6f8a71/external/org_fold/WORKSPACE:1: Workspace name in /usr/local/google/home/geoffreyi/.cache/bazel/_bazel_geoffreyi/f928d2088d4c5ef4e2f54ff2af6f8a71/external/org_fold/WORKSPACE (@org_tensorflow_fold) does not match the name given in the repository's definition (@org_fold); this will cause a build error in future versions.\r\n    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.\r\n    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.\r\n    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.\r\n    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.\r\n    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.\r\n    ERROR: /usr/local/google/home/geoffreyi/deepmath/deepmath/guidance/BUILD:83:1: no such package 'tensorflow': Package crosses into repository @org_tensorflow and referenced by '//deepmath/guidance:clause_ops'.\r\n    ERROR: Analysis of target '//deepmath/guidance:clause_ops' failed; build aborted.\r\n    INFO: Elapsed time: 0.320s", "comments": ["Cc @jart, @damienmg.  I naively tried adding a `@%ws%` prefix similar to the third party BUILD files, but `ws` isn't defined inside tensorflow itself.\r\n\r\nHere's an example of a problematic line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L528", "@damienmg Are there plans to make Bazel more resilient to this kind of configuration error?  As is it seems very tricky to use in a scalable and correct way.", "PS, me + @scott-gray gave up on compiling TF as a submodule for a custom cuda op because of similar snags\r\n\r\n\r\nIE, bazel select isn't context sensitive to the workspace it's defined in, and TF cuda macros use  `select` --\r\nhttp://stackoverflow.com/questions/41153199/building-a-tensorflow-based-android-app-with-tensorflow-as-a-repository", "@yaroslavvb What did you do instead?", "I ended up just writing a simple script to copy my custom ops into tensorflow/core/user_ops and build things from there.  Then I just copy out the generated .so file and incorporate it my pip setup.py (in package_data).  This lets me distribute a nice clean pip package that's mostly independent of TF.", "I can make things work as expected if I add `@org_tensorflow` before every target in `tensorflow.bzl`.  @jart: I imagine that's not kosher, but if there's version that is I can prepare a PR.", "@scott-gray @yaroslavvb Custom ops with submodules should work now.  Not `tf_kernel_library`, though; only `tf_custom_op_library`.", "Or rather, `tf_kernel_library` probably works too, but it wouldn't be usable from Python (which defeats the purpose)."]}, {"number": 8438, "title": "How to broadcast", "body": "Dear all:\r\nI want to broadcast Thr from a single float value to (batchsize, 128,128, 1), I don't know how to do that, when I use the code below, the error occurred that shape do not match. I am using the newest version of tensorflow. Many thanks.\r\n\r\nThr  = tf.multiply(Thr, tf.ones([shape[1], shape[2]], tf.float32))\r\n   \r\n    ", "comments": ["Please send usage questions to StackOverflow"]}, {"number": 8437, "title": "fix issue #8325", "body": "fix issue #8325 ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 8436, "title": "Please provide a working android example with Android Studio 2.3", "body": "Obsolete gradle version of android example doesn't allow simply importing it as a directory to Android Studio and get the following error:\r\n\r\nA problem occurred evaluating root project 'android'.\r\n> java.lang.NullPointerException (no error message)\r\n\r\nStill trying to solve this, but reproduction is so trivial (simply try to import the project directory to latest Android Studio and build), I would expect it to be solved faster by the community...", "comments": ["You're probably running into an uninitialized bazel-tensorflow directory. This is due to a recent change meant to automatically fetch the app assets for you.\r\n\r\nIf you run:\r\n`bazel build -c opt tensorflow/examples/android:tensorflow_demo`\r\none time it should create the necessary directories for you. After that you can build from inside Android Studio.", "Thanks!\r\nI now see this is written in the README, I missed that part. However - maybe it can be emphasized better in README, and error messages could be improved for the next guys who just want to try out the project in AS and encounter this.", "This should be resolved by @ggfan's 360f449d95cf487fd35dbcbc548a6b65fa7ae64f, which removes the bazel dependency on downloading models (meaning it will also work for the makefile build now too)."]}, {"number": 8435, "title": "fork the tensorflow out and modify it for euroc.", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 8434, "title": "Expose DynamicRnnEstimator in contrib.learn", "body": "cc: @martinwicke ", "comments": []}, {"number": 8433, "title": "Cannot find attention functions", "body": "Hi, i am trying to implement attention rnn and want to use https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/attention_decoder_fn_inference and other attention methods from attention_decoder_fn.py . However i cannot find it here on github ...\r\n\r\nWhen this functionality will be available?\r\nWhy is it in api web page but not in the repo?", "comments": ["This function exists in the 1.0 branch: https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/seq2seq/python/ops/attention_decoder_fn.py#L134\r\n\r\nHowever, it was recently deleted from master: https://github.com/tensorflow/tensorflow/commit/d46937f3684d3be2285f7401c2f471f542d147c2\r\n\r\nI suggest either using 1.0, or looking at the code currently in master: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops\r\n\r\nIf you have further questions about using seq2seq, please ask on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) (we try to keep Github issues focused on bugs and feature requests)."]}, {"number": 8432, "title": "can't run the tensorflow/models/image/cifar10_train.py", "body": "2017-03-15 20:25:07.637177: step 0, loss = 4.68 (15.8 examples/sec; 8.118 sec/batch)\r\n2017-03-15 20:25:08.461579: step 10, loss = 4.64 (1899.2 examples/sec; 0.067 sec/batch)\r\n2017-03-15 20:25:09.127436: step 20, loss = 4.55 (1772.0 examples/sec; 0.072 sec/batch)\r\n2017-03-15 20:25:09.786581: step 30, loss = 4.43 (2158.3 examples/sec; 0.059 sec/batch)\r\n2017-03-15 20:25:10.453524: step 40, loss = 4.36 (2082.0 examples/sec; 0.061 sec/batch)\r\n2017-03-15 20:25:11.129617: step 50, loss = 4.30 (1850.5 examples/sec; 0.069 sec/batch)\r\n2017-03-15 20:25:11.791486: step 60, loss = 4.21 (2012.2 examples/sec; 0.064 sec/batch)\r\n2017-03-15 20:25:12.466547: step 70, loss = 4.16 (1905.1 examples/sec; 0.067 sec/batch)\r\n2017-03-15 20:25:13.120901: step 80, loss = 4.17 (1898.9 examples/sec; 0.067 sec/batch)\r\n2017-03-15 20:25:13.770472: step 90, loss = 4.14 (2087.0 examples/sec; 0.061 sec/batch)\r\n/home/fly/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\r\nwarn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\r\nAn exception has occurred, use %tb to see the full traceback.\r\nSystemExit\r\nthe information above was my problem.\r\nwhen i run the code and set the max_steps = 100,the screen print \"SystemExit\" at step 90,when i set the code and set the max_steps = 1000,the screen print \"SystemExit\" at step 990.", "comments": ["It is a normal problem?"]}, {"number": 8431, "title": "dilated convoluton uses a lot of memory", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n* https://github.com/tensorflow/tensorflow/issues/5083 \r\n\r\n### Environment info\r\nOperating System: `Linux hpclogin2 2.6.32-642.15.1.el6.x86_64 #1 SMP Thu Feb 23 11:19:57 CST 2017 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\nInstalled version of CUDA and cuDNN: 8.0 and 5.1\r\n<details>\r\n<summary>(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):</summary>\r\n```\r\nlrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas.so -> libcublas.so.8.0\r\nlrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas.so.8.0 -> libcublas.so.8.0.27\r\n-rwxr-xr-x 1 sebo root  38838688 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas.so.8.0.27\r\n-rw-r--r-- 1 sebo root  49345532 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas_device.a\r\n-rw-r--r-- 1 sebo root  45050574 Sep  1  2016 /appl/cuda/8.0/lib64/libcublas_static.a\r\n-rw-r--r-- 1 sebo root    560184 Sep  1  2016 /appl/cuda/8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\r\n-rwxr-xr-x 1 sebo root    394472 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart.so.8.0.27\r\n-rw-r--r-- 1 sebo root    737516 Sep  1  2016 /appl/cuda/8.0/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 sebo root        15 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft.so -> libcufft.so.8.0\r\nlrwxrwxrwx 1 sebo root        18 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft.so.8.0 -> libcufft.so.8.0.27\r\n-rwxr-xr-x 1 sebo root 146745600 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft.so.8.0.27\r\n-rw-r--r-- 1 sebo root 129655446 Sep  1  2016 /appl/cuda/8.0/lib64/libcufft_static.a\r\nlrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw.so -> libcufftw.so.8.0\r\nlrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw.so.8.0 -> libcufftw.so.8.0.27\r\n-rwxr-xr-x 1 sebo root    456424 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw.so.8.0.27\r\n-rw-r--r-- 1 sebo root     42134 Sep  1  2016 /appl/cuda/8.0/lib64/libcufftw_static.a\r\nlrwxrwxrwx 1 sebo root        17 Sep  1  2016 /appl/cuda/8.0/lib64/libcuinj64.so -> libcuinj64.so.8.0\r\nlrwxrwxrwx 1 sebo root        20 Sep  1  2016 /appl/cuda/8.0/lib64/libcuinj64.so.8.0 -> libcuinj64.so.8.0.27\r\n-rwxr-xr-x 1 sebo root   6459464 Sep  1  2016 /appl/cuda/8.0/lib64/libcuinj64.so.8.0.27\r\n-rw-r--r-- 1 sebo root   1649302 Sep  1  2016 /appl/cuda/8.0/lib64/libculibos.a\r\nlrwxrwxrwx 1 sebo root        16 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand.so -> libcurand.so.8.0\r\nlrwxrwxrwx 1 sebo root        19 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand.so.8.0 -> libcurand.so.8.0.27\r\n-rwxr-xr-x 1 sebo root  59057024 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand.so.8.0.27\r\n-rw-r--r-- 1 sebo root  59273876 Sep  1  2016 /appl/cuda/8.0/lib64/libcurand_static.a\r\nlrwxrwxrwx 1 sebo root        18 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver.so -> libcusolver.so.8.0\r\nlrwxrwxrwx 1 sebo root        21 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver.so.8.0 -> libcusolver.so.8.0.27\r\n-rwxr-xr-x 1 sebo root  52380368 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver.so.8.0.27\r\n-rw-r--r-- 1 sebo root  22313722 Sep  1  2016 /appl/cuda/8.0/lib64/libcusolver_static.a\r\nlrwxrwxrwx 1 sebo root        18 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse.so -> libcusparse.so.8.0\r\nlrwxrwxrwx 1 sebo root        21 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse.so.8.0 -> libcusparse.so.8.0.27\r\n-rwxr-xr-x 1 sebo root  42976296 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse.so.8.0.27\r\n-rw-r--r-- 1 sebo root  51604078 Sep  1  2016 /appl/cuda/8.0/lib64/libcusparse_static.a\r\n```\r\n</details>\r\n<br>\r\n\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`): 168d188168b30b204099f21e456151752d7fb718\r\n2. The output of `bazel version`: `0.4.3`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```py\r\nimport numpy as np\r\nimport sugartensor as stf\r\nimport tensorflow as tf\r\n\r\n\r\ndef get_variable(name, in_dim, out_dim, size=None):\r\n    if size is None:\r\n        size = 1\r\n        shape = (in_dim, out_dim)\r\n    else:\r\n        shape = (size, in_dim, out_dim)\r\n\r\n    w = tf.get_variable(name, shape, dtype=tf.float32,\r\n                        initializer=tf.random_uniform_initializer(\r\n                            minval=-np.sqrt(1 / (in_dim * size)),\r\n                            maxval=np.sqrt(1 / (in_dim * size))\r\n                        ))\r\n    return w\r\n\r\n# build forward pass\r\nembedding = get_variable('embed', 128, 892)\r\nembedding_inv = get_variable('embed-inv', 892, 128)\r\n\r\ndata = tf.placeholder(name='x', shape=(160, 200), dtype=tf.int32)\r\noutput = tf.nn.embedding_lookup(embedding, data)\r\n\r\nfor i in range(60):\r\n    Wi = get_variable(f'W{i}', 892, 892, 5)\r\n    output = tf.nn.convolution(input=output, filter=Wi,\r\n                               padding='SAME', dilation_rate=[16],\r\n                               name='aconv1d')\r\n    output = tf.nn.relu(output)\r\n\r\nlogits = tf.reshape(tf.matmul(tf.reshape(output, [-1, 892]), embedding_inv),\r\n                    [160, 200, 128])\r\n\r\n# optimize for the idendity function\r\nloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n    labels=data, logits=logits\r\n))\r\n\r\n# create update ops\r\noptimizer = tf.train.AdamOptimizer()\r\ngrad_and_vars = optimizer.compute_gradients(loss, tf.trainable_variables())\r\nupdate_ops = optimizer.apply_gradients(grad_and_vars)\r\n\r\nconfig = tf.ConfigProto(allow_soft_placement=True)\r\nwith tf.Session(config=config) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    for i in range(1000):\r\n        loss_result = sess.run([loss, update_ops], feed_dict={\r\n            data: np.random.randint(0, 128, size=(160, 200))\r\n        })[0]\r\n        print(f'iteration {i} complete: {loss_result}')\r\n```\r\n\r\nThis example is perhaps too theoretical to be discussed from a practical application perspective. The actual application is the [ByteNet](https://arxiv.org/abs/1610.10099) model, the implementation is very similar to https://github.com/buriburisuri/ByteNet. The ByteNet model stacks multiple one-dimensional-dilated-convolutions (30), because each of them uses `space_to_batch` they use a lot of memory.\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nI've implemented one-dimensional-masked-dilated-convolutions using `tf.scan`, this uses much less memory but is also slower.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n* The error log from the actual application: https://gist.github.com/AndreasMadsen/91e49e13f0085ececbef0f80c830c5af (note that this happens after 21334 iterations/14 hours, so there may also be a garbage collection issue)\r\n* The error log from the simplified example: https://gist.github.com/AndreasMadsen/94f5100aff697cdf5ff6c26f90a6dad7", "comments": ["@zheng-xq, do you have any insights into the performance of dilated conv's?", "What is a lot of memory? It looks like from the log the tensor it is allocating was 148MB? What GPU do you have? How much memory does it have? Does it work if you write checkpoints frequently and restart? That would verify if it is a memory leak. @zheng-xq ", "The Convolution kernel today doesn't support dilation natively. When the Cudnn support is generally available, we will add the native support.", "> What is a lot of memory? It looks like from the log the tensor it is allocating was 148MB? What GPU do you have? How much memory does it have?\r\n\r\nIt is a Titan X 2016 [GP102 (Pascal)], it has 11.90 GiB of memory.\r\n\r\n> Does it work if you write checkpoints frequently and restart? That would verify if it is a memory leak.\r\n\r\nIt was not a memory leak, but a long sequence that caused that particular issue. But the general issue still exists.", "I have very little experience with CuDNN, but is it possible that dilation is already supported?\r\n\r\n**in `5.1/include/cudnn.h`**\r\n\r\n```h\r\ncudnnStatus_t CUDNNWINAPI cudnnSetConvolution2dDescriptor_v5( cudnnConvolutionDescriptor_t convDesc,\r\n                                                             int pad_h,    // zero-padding height\r\n                                                             int pad_w,    // zero-padding width\r\n                                                             int u,   // vertical filter stride\r\n                                                             int v,   // horizontal filter stride\r\n                                                             int upscalex, // upscale the input in x-direction\r\n                                                             int upscaley, // upscale the input in y-direction\r\n                                                             cudnnConvolutionMode_t mode,\r\n                                                             cudnnDataType_t dataType\r\n                                                           );\r\n```\r\n\r\n`upscalex` kinda sounds like another word for dilation and in cuDNN v6 RC (http://blog.yannisassael.com/2017/02/cudnn-v6-0-rc/) it appears to have been renamed to `dilation_h`.\r\n\r\n**in `6.0/include/cudnn.h`**\r\n\r\n```h\r\ncudnnStatus_t CUDNNWINAPI cudnnGetConvolution2dDescriptor_v5(  const cudnnConvolutionDescriptor_t convDesc,\r\n                                                            int* pad_h,    // zero-padding height\r\n                                                            int* pad_w,    // zero-padding width\r\n                                                            int* u,        // vertical filter stride\r\n                                                            int* v,        // horizontal filter stride\r\n                                                            int* dilation_h, // filter dilation in the vertical dimension\r\n                                                            int* dilation_w, // filter dilation in the horizontal dimension\r\n                                                            cudnnConvolutionMode_t* mode,\r\n                                                            cudnnDataType_t *computeType\r\n                                                         );\r\n```\r\n\r\nedit: currently `upscale` is just set to 1, which would correspond to no dilation: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L545", "Dilated convolution is definitely supported in CuDNN 6.0 now (just released), see https://github.com/tensorflow/tensorflow/issues/8828\r\n\r\n```\r\nIn cudnnSetConvolutionNdDescriptor, previously unused \u201cupscale\u201d fields\r\nhave been repurposed to allow user specification of dilation factors along each\r\ndimension. This does not alter existing behavior (which required the upscale\r\nfields to be set to 1). However, these fields can now be set to a value greater than\r\n1, to perform dilated convolutions while using the algorithms specified in the\r\n\u201cNew Features\u201d section of these Release Notes.\r\n```", "@zheng-xq , could you please comment again.", "We are currently working on CuDNN6.0 support, cc @tfboyd,", "@tfboyd any progress on \"native\" cudnn dilated conv support? As referenced in [this comment](https://github.com/tensorflow/tensorflow/issues/8828#issuecomment-325129198) on issue #8828 ", "Assigning to myself to try and find out if these is any near-term planned work to add support for cuDNN dilated conv.  ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "dilated conv is now supported via cuDNN.  I have no idea if this resolves the memory issue.  I am going to marked this closed given the age.  If anyone test with the new cuDNN dilated conv and has an issue reopen or better open a new issue and reference this one if applicable.  ", "@yzhwang if you have an issue finding the API.  I took a quick look and did not see a link.", "Thanks @tfboyd ! Is there currently a release with this change and if not, which release is planned to contain the change? (I assume the next one but it's nice to have the number for folks who might come across this issue in the future and don't want to cross-ref release dates with comment dates.)", "@strubell and @tfboyd \r\nStarting from TensorFlow 1.5.0 release, the dilated convolution is supported through the existing tf.nn.conv2d() API with an additional argument: dilations. Please refer to the documentation for more details:\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/conv2d\r\nUnfortunately there is only conv2d support for that, for conv1d, you need to do some reshape, for conv3d, I will work on adding that dilation support to tf.nn.conv3d(). But it won't happen any time before the dev summit.", "Sounds good. Thanks!\n\nOn Mon, Mar 12, 2018 at 2:33 PM Yangzihao Wang <notifications@github.com>\nwrote:\n\n> @strubell <https://github.com/strubell> and @tfboyd\n> <https://github.com/tfboyd>\n> Starting from TensorFlow 1.5.0 release, the dilated convolution is\n> supported with older tf.nn.conv2d() API with an additional argument:\n> dilations. Please refer to the documentation for more details:\n> https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n> Unfortunately there is only conv2d support for that, for conv1d, you need\n> to do some reshape, for conv3d, I will work on adding that dilation support\n> to tf.nn.conv3d(). But it won't happen any time before the dev summit.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8431#issuecomment-372415681>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADHZtysm-RM4MInD_FgSVyAfgquHGV2Dks5tdr9pgaJpZM4Md26Z>\n> .\n>\n", "@yzhwang Has dilation support for the.nn.conv3d added as of now? I'm trying to run dilations with conv3d, but it says No Algorithm Worked! Is the problem fixed?"]}, {"number": 8430, "title": "Exif Orientation support tf.image.decode_jpeg()", "body": "Hi.\r\nI have large amounts of JPEG files uploaded by users on our service .\r\nThey use mainly smartphone camera, so most files contain EXIF orientation metadata.\r\n\r\nIt would seem that the `tf.image.decode_jpeg()` ignores this information,\r\nSo I cannot build pipeline using pure tensor operation.\r\nI thought it is not better to writing custom decoder depending external libraries, considering complying thread safety and future compatibility.\r\n\r\nMy Pipeline code is like below:\r\n\r\n```python\r\nfilename_queue = tf.train.string_input_producer(files)\r\nreader = tf.WholeFileReader()\r\nkey, supplyContent = reader.read(filename_queue)\r\ndecoding = tf.image.decode_jpeg(supplyContent, channels=3)\r\n#  source image may have orientation but dropped \r\nresizing = tf.image.resize_images(decoding, (IMAGE_SIZE, IMAGE_SIZE))\r\n# ...\r\n```\r\n\r\nTransformation of image can be done with simple tensor op, since tf.image has many convenience methods.\r\nHow about adding support for these meta informations ?\r\n\r\n\r\n", "comments": ["I doubt we'll working on this since we haven't seen much demand.  FYI, writing custom input-processing ops is totally _a_ valid use of TensorFlow.  In this case, I'd very much encourage it since it could deliver more functionalities (necessary) and better performance (good).", "@concretevitamin \r\nAll right, I understand.\r\nI'll write decoding logic with referring PIL .\r\n\r\nThank you for your reply.", "@concretevitamin \r\n> I doubt we'll working on this since we haven't seen much demand.\r\n\r\nFYI - the Flickr 100m image dataset (and by extension the multimedia creative commons image dataset) has many (~0.5% I think...) EXIF formatted JPG files, which currently break with tf.image.decode_image and tf.image.decode_jpeg. Also I find it surprising that there is not much demand for dealing with images taken on smart phones considering \"Tensorflow was designed with mobile and embedded platforms in mind.\" https://www.tensorflow.org/mobile/\r\n\r\nIt might be worth at least flagging this for contributions.", "+ @petewarden who will know more about mobile use cases.", "Any update on EXIF orientation support in the latest api ?  This actually prevent me to properly use the Dataset logic with smartphone pictures.... ", "Yeah, this is a big problem", "Please see https://github.com/tensorflow/io/pull/604 which adds the kernel ops in tensorflow/io to extract EXIF orientation (to be used in dataset).", "Someone made a package to resolve this problem.\r\n\r\nhttps://medium.com/@ageitgey/the-dumb-reason-your-fancy-computer-vision-app-isnt-working-exif-orientation-73166c7d39da"]}, {"number": 8429, "title": "Versions Web Page Needs Updating", "body": "This really is a small issue but I couldn't submit a PR as I couldn't find the file for it.\r\nAt https://www.tensorflow.org/versions/ the current version is stated as 0.12 when it should be 1.0!\r\n\r\n`The docs at root (i.e. in tensorflow.org/api_docs) refer to the most recent stable branch (in this case, r0.12)`\r\n\r\nThat's it. Sorry for raising an issue for such a trivial detail! Thanks", "comments": ["@yifeif: Yifei, could you take a look?  Thanks!", "Thanks for pointing it out @jubjamie.\r\n@wolffg could we add to our website generation to update according to root change? Thank you!", "This has been fixed. Now it should say \"most recent stable branch (in this case, r1.2)\". Thanks!"]}, {"number": 8428, "title": "HVX Acceleration support", "body": "I successfully built and ran the test application from `tensorflow/tree/master/tensorflow/contrib/hvx`. I'd now like to benchmark HVX against the CPU implementations of `tensorflow/tools/benchmark`, and the Android camera demo, but I wasn't able to find any documentation describing how to build said apps with HVX support (my builds run on the CPU). I'm testing on the Open-Q 820 development board with Android 7.0.\r\n\r\nIs it possible to utilize HVX acceleration outside the HVX test application, preferably with the benchmark and Android camera demos? If so, could someone please point me in the right direction?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8427, "title": "Fix small typo in seq2seq", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 8426, "title": "Windows: Fix TensorFlow Bazel build", "body": "- Remove explict reference to nodejs binary\r\n@org_nodejs//:bin/node doesn't work on Windows, since it's supposed to\r\nbe @org_nodejs//:node.exe\r\n@jart \r\n\r\n- Fix Bazel build after #8217\r\n1. Use COMPILER_MSVC instead of PLATFORM_WINDOWS, because the latter one\r\nis only defined in tf_copts. Some targets don't have tf_copts will fail to\r\nbuild, eg. //tensorflow/core/distributed_runtime/rpc:grpc_remote_master\r\n2. Add TF_COMPILE_LIBRARY in tf_copts\r\n@guschmue @mrry \r\n\r\n- Fix the pip package dependencies in Bazel build (due to eb8bb9e461f669f299aa031634530995bc43f92b)\r\n1. Remove some targets that doesn't build yet.\r\n2. After bazelbuild/bazel#2088 is fixed,\r\n   //tensorflow/tensorboard is buildable now.\r\n\r\n- Remove python tests than are already deleted from bazel_test_lib.sh\r\n@caisq \r\n", "comments": []}, {"number": 8425, "title": "Saving with monitored session", "body": "I get the following error when trying to use a saver with a Monitored session:\r\n\r\n```\r\nFile \"/users/spraak/spch/prog/spch/tensorflow-1.0.0/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1369, in save\r\n    raise TypeError(\"'sess' must be a Session; %s\" % sess)\r\nTypeError: 'sess' must be a Session; <tensorflow.python.training.monitored_session.MonitoredSession object at 0x2be710d0>\r\n```\r\n\r\nI save like this:\r\n\r\n```\r\nwith tf.train.MonitoredTrainingSession(...) as sess:\r\n    ...\r\n    saver.save(sess, 'model.ckpt')\r\n```\r\n\r\ninstead of sess I also tried `tf.get_default_session()`, but this returns None. In all other cases the monitored_session works just like a normal session, so I think it's a bug.", "comments": ["I found that the Session object is stored in sess._sess._sess._sess._sess and if I do \r\n\r\n```\r\nsaver.save(sess._sess._sess._sess._sess, 'model.ckpt')\r\n```\r\n\r\nIt actually works, but this is of course not desired. I wrote temporary method to circumvent the issue until it is solved:\r\n\r\n```\r\ndef get_session(sess):\r\n    session = sess\r\n    while type(session).__name__ != 'Session':\r\n        #pylint: disable=W0212\r\n        session = session._sess\r\n    return session\r\n```\r\n\r\nI can then save with:\r\n\r\n```\r\nsaver.save(get_session(sess), 'model.ckpt')\r\n```", "I don't think explicitly manipulating Saver inside a `MonitoredTrainingSession` is recommended (could you use a vanilla `Session`?), but @ispirmustafa must know more.", "The thing is I have a training graph that holds a bunch of variables that are relevant for training, but not for the model (e.g. global_step). I have 2 savers, one that stores all my variables, to checkpoint the training process, so I can resume training if necessary. And one trainer to save my model at the end of training. This is why I want to explicitly manipulate a saver.", "You can write a hook. something like as follows:\r\n```\r\nclass SaveAtEnd(tf.train.SessionRunHook):\r\n  def begin(self):\r\n    self._saver = # create your saver\r\n  def end(self, session):\r\n    saver.save(session, ...)\r\n```", "Why was this issue closed? I don't see @ispirmustafa 's answer solves the problem. ", "You have to create a hook that does the saving for you and put it into the session", "This is the hook I use:\r\n\r\n```\r\nclass SaveAtEnd(tf.train.SessionRunHook):\r\n    '''a training hook for saving the final variables'''\r\n\r\n    def __init__(self, filename, variables):\r\n        '''hook constructor\r\n\r\n        Args:\r\n            filename: where the model will be saved\r\n            variables: the variables that will be saved'''\r\n\r\n        self.filename = filename\r\n        self.variables = variables\r\n\r\n    def begin(self):\r\n        '''this will be run at session creation'''\r\n\r\n        #pylint: disable=W0201\r\n        self._saver = tf.train.Saver(self.variables, sharded=True)\r\n\r\n    def end(self, session):\r\n        '''this will be run at session closing'''\r\n\r\n        self._saver.save(session, self.filename)\r\n```", "@vrenkens it works smoothly thank you very much ", "> This is the hook I use:\r\n> \r\n> ```\r\n> class SaveAtEnd(tf.train.SessionRunHook):\r\n>     '''a training hook for saving the final variables'''\r\n> \r\n>     def __init__(self, filename, variables):\r\n>         '''hook constructor\r\n> \r\n>         Args:\r\n>             filename: where the model will be saved\r\n>             variables: the variables that will be saved'''\r\n> \r\n>         self.filename = filename\r\n>         self.variables = variables\r\n> \r\n>     def begin(self):\r\n>         '''this will be run at session creation'''\r\n> \r\n>         #pylint: disable=W0201\r\n>         self._saver = tf.train.Saver(self.variables, sharded=True)\r\n> \r\n>     def end(self, session):\r\n>         '''this will be run at session closing'''\r\n> \r\n>         self._saver.save(session, self.filename)\r\n> ```\r\n\r\nShould we need to pass the filename and required variables to save through the hook?\r\nWhat should I pass if I want to save all the variables?\r\n\r\nI tried to use **SavedModelBuilder**. But, upon constructing **signatureDef** for that, I used `tf.get_default_graph().get_tensor_by_name('model/att_seq2seq/strided_slice:0')`\r\n\r\nBut, it says, `KeyError: \"The name 'model/att_seq2seq/strided_slice:0' refers to a Tensor which does not exist. The operation, 'model/att_seq2seq/strided_slice', does not exist in the graph.\"`\r\n\r\nAt which point of time possibly after sess.run(), I could use **SavedModelBuilder** to save all variables(**including table values**)?"]}, {"number": 8424, "title": "mean_pairwise_squared_error", "body": "when i use the tf.losses.mean_pairwise_squared_error(labels, predictions, weights=1.0, scope=None, loss_collection=tf.GraphKeys.LOSSES) function, i am sure the data is right. but the loss on the tensorboard is always zero. I try hard but can know why?\r\nscore_a=tf.reshape(score,[-1])#shape: [1,39]\r\nys_a=tf.reshape(ys,[-1])#shape: [1,39]\r\nwith tf.name_scope('loss'):\r\n\tloss=tf.losses.mean_pairwise_squared_error(score_a,ys_a)", "comments": ["If this isn't a bug, it will be better/faster answered on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) \u2014 the correct place to ask for help, with a bigger community. If this is a bug, please follow guidelines for reporting problems. When you click on \"New issue\" there is a template to be filled.\r\n\r\nOn the topic: You have to call [tf.summary.scalar](https://www.tensorflow.org/api_docs/python/tf/summary/scalar) to send/print your loss to TensorBoard. Please take a look at the [docs of TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) for more information.", "I agree with the assessment of @MicaelCarvalho - the usage questions are best served via StackOverflow."]}, {"number": 8423, "title": "Android demo build Error", "body": "when i used android studio build the android demo, i got this error:\r\n\r\nWarning: ignoring http_proxy in environment.\r\n____Loading package: tensorflow/examples/android\r\n____Found 1 target...\r\n____Building...\r\nTarget //tensorflow/examples/android:external_assets up-to-date (nothing to build)\r\n____Elapsed time: 0.163s, Critical Path: 0.00s\r\n\r\n:copyExternalAssets UP-TO-DATE\r\n:buildNativeBazel\r\nWarning: ignoring http_proxy in environment.\r\n____Loading complete.  Analyzing...\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/zhaoming/tensorflow/tensorflow/tensorflow/core/BUILD:872:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\n____Found 1 target...\r\n____Building...\r\nERROR: /home/zhaoming/tensorflow/tensorflow/tensorflow/examples/android/BUILD:22:1: C++ compilation of rule '//tensorflow/examples/android:libtensorflow_demo.so' failed: false failed: error executing command /bin/false -MD -MF bazel-out/stub_armeabi-v7a-opt/bin/tensorflow/examples/android/_objs/libtensorflow_demo.so/tensorflow/examples/android/jni/object_tracking/tracked_object.pic.d ... (remaining 25 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nTarget //tensorflow/examples/android:tensorflow_native_libs failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n____Elapsed time: 0.248s, Critical Path: 0.01s\r\n\r\n FAILED\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nExecution failed for task ':buildNativeBazel'.\r\n> Process 'command '/home/zhaoming/bin/bazel'' finished with non-zero exit value 1\r\n\r\n* Try:\r\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.\r\n\r\nBUILD FAILED\r\n\r\nTotal time: 0.675 secs\r\n\r\nhow can i fix it? thx\r\n", "comments": ["Have you followed the instructions from [examples/android/README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md), including the NDK/SDK installation and WORKSPACE editing?\r\n\r\nIf so, when you build manually with:\r\n`bazel build -c opt tensorflow/examples/android:tensorflow_demo --stacktrace --info`\r\n\r\nDoes it give you any more info?", " i run bazel build -c opt tensorflow/examples/android:tensorflow_demo\r\nit passed , gradle build fixed too.  i dont know why..", "Ok, great :)  /usr/bin/false errors usually occur if Bazel isn't able to find an expected binary (e.g. g++). Possibly you installed something or updated your WORKSPACE file between attempts?"]}, {"number": 8422, "title": "Tensorflow_gpu crash on windows server 2012(Nightly build Feb 15, 2017 1:25:00 AM)", "body": "### Environment info\r\nOperating System:\r\nWindows Server 2012 R2\r\nGPU GeForce GTX 750 and 1080 in two windows server workstation.\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA 8.0 installed\r\nCuDNN download, and set path to system environment variable %PATH%\r\n\r\nIf installed from binary pip package, provide:\r\nI install the fallow whl: \"tensorflow_gpu-1.0.0rc2-cp35-cp35m-win_amd64.whl\"\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-win/85/DEVICE=gpu,OS=windows/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.0.0rc2-cp35-cp35m-win_amd64.whl\r\n\r\nI only import tensorflow, and it's crash.\r\n-------------------------------------------------\r\n```\r\n(venv64) C:\\work\\keras01>python\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\work\\keras01\\venv64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, i\r\nn swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\work\\keras01\\venv64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\work\\keras01\\venv64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module\r\n>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\work\\keras01\\venv64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, i\r\nn <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\work\\keras01\\venv64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, i\r\nn swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\work\\keras01\\venv64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\work\\keras01\\venv64\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\work\\keras01\\venv64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module\r\n>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\work\\keras01\\venv64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, i\r\nn swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\work\\keras01\\venv64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\work\\keras01\\venv64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module\r\n>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\work\\keras01\\venv64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, i\r\nn <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\work\\keras01\\venv64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, i\r\nn swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\work\\keras01\\venv64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#im\r\nport_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n", "comments": ["Have you followed the advice given regarding the _pywrap_tensorflow problem. Check out: [https://stackoverflow.com/q/42011070](https://stackoverflow.com/q/42011070)\r\n\r\nIt looks like you are missing a DLL. However, I had this problem not long ago even though I did have the DLL. Just ensure that your environment variables are set correctly. If playing around with that doesn't work i'll try and remember the other troubleshooting steps I did to fix it. (As it was a bit tricky to solve!)", "Thanks your reply, my computer has already install visual studio 2015 and also install 2015 Redistributable\r\nin previous version of tensorflow_gpu 0.12.x don't have this issue.", "Can you try installing via pip install tensorflow-gpu\r\n\r\nI seem to recall that the nightly build I used threw this error even though the DLL was there and set up correctly. Installing straight from pip solved this issue. Uninstall the current version first.", "Sometimes pip may misbehave on windows.\r\nCould you try this command out with an administrator terminal:\r\n```\r\npip install --ignore-installed --upgrade tensorflow-gpu\r\n```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Though I am able to import tensorflow, I am not able to create a session and test it.\r\nIt gives below error.\r\n\r\n\r\n(tensorflow-gpu) C:\\Users\\ssheshap>python\r\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  5 2016, 11:41:13) [MSC v\r\n.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.Session()\r\n2017-11-16 19:52:51.979025: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platfor\r\nm\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE ins\r\ntructions, but these are available on your machine and could speed up CPU comput\r\nations.\r\n2017-11-16 19:52:51.979894: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platfor\r\nm\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 in\r\nstructions, but these are available on your machine and could speed up CPU compu\r\ntations.\r\n2017-11-16 19:52:51.980766: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platfor\r\nm\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 in\r\nstructions, but these are available on your machine and could speed up CPU compu\r\ntations.\r\n2017-11-16 19:52:51.981665: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platfor\r\nm\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1\r\ninstructions, but these are available on your machine and could speed up CPU com\r\nputations.\r\n2017-11-16 19:52:51.982829: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platfor\r\nm\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2\r\ninstructions, but these are available on your machine and could speed up CPU com\r\nputations.\r\n2017-11-16 19:52:51.984912: W c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\core\\platfor\r\nm\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX ins\r\ntructions, but these are available on your machine and could speed up CPU comput\r\nations.\r\n2017-11-16 19:52:52.026790: E c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\stream_execu\r\ntor\\cuda\\cuda_driver.cc:405] failed call to cuInit: CUDA_ERROR_UNKNOWN\r\n2017-11-16 19:52:52.031638: I c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\stream_execu\r\ntor\\cuda\\cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for hos\r\nt: WIN-0LVFD08FG54\r\n2017-11-16 19:52:52.033113: I c:\\l\\work\\tensorflow-1.1.0\\tensorflow\\stream_execu\r\ntor\\cuda\\cuda_diagnostics.cc:165] hostname: WIN-0LVFD08FG54\r\n<tensorflow.python.client.session.Session object at 0x0000000C22A2B358>\r\n>>> exit()\r\n\r\nPlease help.\r\nThanks"]}, {"number": 8421, "title": "Bazel build failed in compiling TensorFlow from source. ", "body": "I am compiling TensorFlow from source on an Ubuntu machine. This is a verbose output of the error log generated in the ```bazel build //tensorflow/tools/pip_package:build_pip_package``` execution.\r\n\r\nError Log:\r\n\r\n    ERROR: /home/annanay/.cache/bazel/_bazel_root/9b62c7240de3d9136528cdded945b550/external/llvm/BUILD:418:5: Generating code from table: lib/Target/AArch64/AArch64.td @llvm//:aarch64_target_gen__gen_fast_isel_genrule failed: bash failed: \r\n    error executing command \r\n    (cd /home/annanay/.cache/bazel/_bazel_root/9b62c7240de3d9136528cdded945b550/execroot/tensorflow && \\\r\n    exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/llvm/llvm-tblgen -I external/llvm/include -I external/llvm/tools/clang/include -I $(dirname external/llvm/lib/Target/AArch64/AArch64.td) -gen-fast-isel external/llvm/lib/Target/AArch64/AArch64.td -o bazel-out/local-opt/genfiles/external/llvm/lib/Target/AArch64/AArch64GenFastISel.inc'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\r\n    bazel-out/host/bin/external/llvm/llvm-tblgen: relocation error: bazel-out/host/bin/external/llvm/llvm-tblgen: symbol _ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEE9_M_createERmm, version GLIBCXX_3.4.21 not defined in file libstdc++.so.6 with link time reference\r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n\r\n\r\n\r\nI built TensorFlow from source, and here is the relevant information:\r\n```\r\n$ git rev-parse HEAD\r\n4c3bb1aeb7bb46bea35036433742a720f39ce348\r\n\r\n$ bazel version\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n\r\n```\r\nSystem:\r\n```\r\nx86_64 GNU/Linux\r\n```\r\n", "comments": ["using gcc version 5.x? \r\nhttp://stackoverflow.com/questions/36816570/glibcxx-3-4-21-not-defined-in-file-libstdc-so-6-with-link-time-reference \r\nand https://www.tensorflow.org/install/install_sources -> \"NOTE on gcc version 5: the binary pip packages available on the TensorFlow website are built with gcc4 that uses the older ABI. To make the library compatible with the older abi you have to add -cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\"", "@truatpasteurdotfr but I'm not using the binary packages from the TensorFlow website. Building everything from source!", "libstdc++ was already in the latest version. \r\n\r\nIf I were to add the flag, do I have to manually enter it in the bazel build files or is there a way to pass it as a flag?\r\n\r\nAlso, my ```gcc -v``` is ```6.3.0```.\r\n\r\n", "We have instructions in the website for building from source, as @truatpasteurdotfr indicated:\r\nhttps://www.tensorflow.org/install/install_sources\r\n\r\nPlease go to this website and follow the instructions there.", "Please reopen if the official instructions don't work for you.", "I am getting a similar error, building from source. I have multiple gccs available on my system, but `$(which gcc) --version` says 4.8.5.", "this works for me: (cpu only version on CentOS-7) https://github.com/truatpasteurdotfr/docker-centos7-tensorflow", "That doesn't help me because 1) I'd like to compile with optimizations and 2) I can't use docker on my system.", "@vladfi1 you can get the ideas just by reading the Dockerfile, ymmv.", "I have kind of the same problem, installing from source gives the following error\r\n\r\n```\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 384.280s, Critical Path: 107.73s\r\nFAILED: Build did NOT complete successfully\r\n```\r\nDigging a bit dipper I found multiple errors:\r\n```\r\nERROR: /home/kirk/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: C++ compilation of rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so' failed (Exit 1)\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:76:18: error: 'se' does not name a type\r\n using StatusOr = se::port::StatusOr<T>;\r\n                  ^~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:139:28: error: 'StatusOr' was not declared in this scope\r\n typedef std::function<void(StatusOr<Tensor>)> CommunicationDoneCallback;\r\n                            ^~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:139:28: note: suggested alternatives:\r\nIn file included from ./tensorflow/stream_executor/dnn.h:33:0,\r\n                 from ./tensorflow/stream_executor/stream.h:30,\r\n                 from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:31:\r\n./tensorflow/stream_executor/lib/statusor.h:28:36: note:   'stream_executor::port::StatusOr'\r\n using StatusOr = ::xla::StatusOr<T>;\r\n                                    ^\r\nIn file included from ./tensorflow/stream_executor/lib/statusor.h:21:0,\r\n                 from ./tensorflow/stream_executor/dnn.h:33,\r\n                 from ./tensorflow/stream_executor/stream.h:30,\r\n                 from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:31:\r\n./tensorflow/compiler/xla/statusor.h:87:7: note:   'xla::StatusOr'\r\n class StatusOr : private internal_statusor::StatusOrData<T>,\r\n       ^~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:139:45: error: template argument 1 is invalid\r\n typedef std::function<void(StatusOr<Tensor>)> CommunicationDoneCallback;\r\n                                             ^\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In function 'void tensorflow::contrib::mpi_collectives::{anonymous}::PerformCollectiveOp(tensorflow::contrib::mpi_collectives::{anonymous}::TensorTable&, tensorflow::contrib::mpi_collectives::MPIResponse)':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:512:14: error: 'StatusOr' was not declared in this scope\r\n     callback(StatusOr<Tensor>(*output_tensor));\r\n              ^~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:512:14: note: suggested alternatives:\r\nIn file included from ./tensorflow/stream_executor/dnn.h:33:0,\r\n                 from ./tensorflow/stream_executor/stream.h:30,\r\n                 from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:31:\r\n./tensorflow/stream_executor/lib/statusor.h:28:36: note:   'stream_executor::port::StatusOr'\r\n using StatusOr = ::xla::StatusOr<T>;\r\n                                    ^\r\nIn file included from ./tensorflow/stream_executor/lib/statusor.h:21:0,\r\n                 from ./tensorflow/stream_executor/dnn.h:33,\r\n                 from ./tensorflow/stream_executor/stream.h:30,\r\n                 from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:31:\r\n./tensorflow/compiler/xla/statusor.h:87:7: note:   'xla::StatusOr'\r\n class StatusOr : private internal_statusor::StatusOrData<T>,\r\n       ^~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:512:29: error: expected primary-expression before '>' token\r\n     callback(StatusOr<Tensor>(*output_tensor));\r\n                             ^\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:512:46: error: 'callback' cannot be used as a function\r\n     callback(StatusOr<Tensor>(*output_tensor));\r\n                                              ^\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:514:14: error: 'StatusOr' was not declared in this scope\r\n     callback(StatusOr<Tensor>(status));\r\n              ^~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:514:14: note: suggested alternatives:\r\nIn file included from ./tensorflow/stream_executor/dnn.h:33:0,\r\n                 from ./tensorflow/stream_executor/stream.h:30,\r\n                 from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:31:\r\n./tensorflow/stream_executor/lib/statusor.h:28:36: note:   'stream_executor::port::StatusOr'\r\n using StatusOr = ::xla::StatusOr<T>;\r\n                                    ^\r\nIn file included from ./tensorflow/stream_executor/lib/statusor.h:21:0,\r\n                 from ./tensorflow/stream_executor/dnn.h:33,\r\n                 from ./tensorflow/stream_executor/stream.h:30,\r\n                 from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:31:\r\n./tensorflow/compiler/xla/statusor.h:87:7: note:   'xla::StatusOr'\r\n class StatusOr : private internal_statusor::StatusOrData<T>,\r\n       ^~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:514:29: error: expected primary-expression before '>' token\r\n     callback(StatusOr<Tensor>(status));\r\n                             ^\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:514:38: error: 'callback' cannot be used as a function\r\n     callback(StatusOr<Tensor>(status));\r\n                                      ^\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In member function 'void tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback)':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:997:52: error: 'StatusOr' has not been declared\r\n     auto allreduce_done_callback = [done, context](StatusOr<Tensor> status) {\r\n                                                    ^~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:997:60: error: expected ',' or '...' before '<' token\r\n     auto allreduce_done_callback = [done, context](StatusOr<Tensor> status) {\r\n                                                            ^\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In lambda function:\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:998:26: error: 'status' was not declared in this scope\r\n       context->SetStatus(status.status());\r\n                          ^~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In member function 'void tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback)':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1091:52: error: 'StatusOr' has not been declared\r\n     auto allgather_done_callback = [done, context](StatusOr<Tensor> status) {\r\n                                                    ^~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1091:60: error: expected ',' or '...' before '<' token\r\n     auto allgather_done_callback = [done, context](StatusOr<Tensor> status) {\r\n                                                            ^\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In lambda function:\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1092:26: error: 'status' was not declared in this scope\r\n       context->SetStatus(status.status());\r\n                          ^~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In instantiation of 'void tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1095:21: error: cannot convert 'tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda(int)>' to 'tensorflow::contrib::mpi_collectives::{anonymous}::CommunicationDoneCallback {aka int}' in assignment\r\n     record.callback = allgather_done_callback;\r\n     ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1108:32: error: invalid use of 'auto'\r\n       allgather_launch_callback();\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~^~\r\nIn file included from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/move.h:57:0,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/stl_pair.h:59,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/stl_algobase.h:64,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/deque:60,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/queue:60,\r\n                 from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:18:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits: In instantiation of 'class std::result_of<tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&()>':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/functional:1913:9:   required by substitution of 'template<class _Functor, class, class> std::function<_Res(_ArgTypes ...)>::function(_Functor) [with _Functor = tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>; <template-parameter-1-2> = void; <template-parameter-1-3> = <missing>]'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1111:7:   required from 'void tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:2496:12: error: invalid use of incomplete type 'std::__result_of_impl<false, false, tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&>::type'\r\n     struct result_of<_Functor(_ArgTypes...)>\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:2291:12: note: declaration of 'std::__result_of_impl<false, false, tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&>::type'\r\n     struct __result_of_success : __success_type<_Tp>\r\n            ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In instantiation of 'void tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1111:7: error: no matching function for call to 'stream_executor::Stream::ThenDoHostCallback(tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&)'\r\n       stream->ThenDoHostCallback(allgather_launch_callback);\r\n       ^~~~~~\r\nIn file included from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:31:0:\r\n./tensorflow/stream_executor/stream.h:1987:11: note: candidate: stream_executor::Stream& stream_executor::Stream::ThenDoHostCallback(std::function<void()>)\r\n   Stream &ThenDoHostCallback(std::function<void()> callback);\r\n           ^~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/stream.h:1987:11: note:   no known conversion for argument 1 from 'tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>' to 'std::function<void()>'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In instantiation of 'void tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1095:21: error: cannot convert 'tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda(int)>' to 'tensorflow::contrib::mpi_collectives::{anonymous}::CommunicationDoneCallback {aka int}' in assignment\r\n     record.callback = allgather_done_callback;\r\n     ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1108:32: error: invalid use of 'auto'\r\n       allgather_launch_callback();\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~^~\r\nIn file included from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/move.h:57:0,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/stl_pair.h:59,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/stl_algobase.h:64,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/deque:60,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/queue:60,\r\n                 from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:18:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits: In instantiation of 'class std::result_of<tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&()>':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/functional:1913:9:   required by substitution of 'template<class _Functor, class, class> std::function<_Res(_ArgTypes ...)>::function(_Functor) [with _Functor = tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>; <template-parameter-1-2> = void; <template-parameter-1-3> = <missing>]'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1111:7:   required from 'void tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:2496:12: error: invalid use of incomplete type 'std::__result_of_impl<false, false, tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&>::type'\r\n     struct result_of<_Functor(_ArgTypes...)>\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:2291:12: note: declaration of 'std::__result_of_impl<false, false, tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&>::type'\r\n     struct __result_of_success : __success_type<_Tp>\r\n            ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In instantiation of 'void tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1111:7: error: no matching function for call to 'stream_executor::Stream::ThenDoHostCallback(tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&)'\r\n       stream->ThenDoHostCallback(allgather_launch_callback);\r\n       ^~~~~~\r\nIn file included from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:31:0:\r\n./tensorflow/stream_executor/stream.h:1987:11: note: candidate: stream_executor::Stream& stream_executor::Stream::ThenDoHostCallback(std::function<void()>)\r\n   Stream &ThenDoHostCallback(std::function<void()> callback);\r\n           ^~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/stream.h:1987:11: note:   no known conversion for argument 1 from 'tensorflow::contrib::mpi_collectives::MPIAllgatherOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>' to 'std::function<void()>'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In instantiation of 'void tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1001:21: error: cannot convert 'tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda(int)>' to 'tensorflow::contrib::mpi_collectives::{anonymous}::CommunicationDoneCallback {aka int}' in assignment\r\n     record.callback = allreduce_done_callback;\r\n     ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1014:32: error: invalid use of 'auto'\r\n       allreduce_launch_callback();\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~^~\r\nIn file included from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/move.h:57:0,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/stl_pair.h:59,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/stl_algobase.h:64,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/deque:60,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/queue:60,\r\n                 from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:18:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits: In instantiation of 'class std::result_of<tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&()>':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/functional:1913:9:   required by substitution of 'template<class _Functor, class, class> std::function<_Res(_ArgTypes ...)>::function(_Functor) [with _Functor = tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>; <template-parameter-1-2> = void; <template-parameter-1-3> = <missing>]'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1017:7:   required from 'void tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:2496:12: error: invalid use of incomplete type 'std::__result_of_impl<false, false, tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&>::type'\r\n     struct result_of<_Functor(_ArgTypes...)>\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:2291:12: note: declaration of 'std::__result_of_impl<false, false, tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&>::type'\r\n     struct __result_of_success : __success_type<_Tp>\r\n            ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In instantiation of 'void tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1017:7: error: no matching function for call to 'stream_executor::Stream::ThenDoHostCallback(tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&)'\r\n       stream->ThenDoHostCallback(allreduce_launch_callback);\r\n       ^~~~~~\r\nIn file included from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:31:0:\r\n./tensorflow/stream_executor/stream.h:1987:11: note: candidate: stream_executor::Stream& stream_executor::Stream::ThenDoHostCallback(std::function<void()>)\r\n   Stream &ThenDoHostCallback(std::function<void()> callback);\r\n           ^~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/stream.h:1987:11: note:   no known conversion for argument 1 from 'tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::GpuDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>' to 'std::function<void()>'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In instantiation of 'void tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1001:21: error: cannot convert 'tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda(int)>' to 'tensorflow::contrib::mpi_collectives::{anonymous}::CommunicationDoneCallback {aka int}' in assignment\r\n     record.callback = allreduce_done_callback;\r\n     ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1014:32: error: invalid use of 'auto'\r\n       allreduce_launch_callback();\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~^~\r\nIn file included from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/move.h:57:0,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/stl_pair.h:59,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/bits/stl_algobase.h:64,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/deque:60,\r\n                 from /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/queue:60,\r\n                 from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:18:\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits: In instantiation of 'class std::result_of<tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&()>':\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/functional:1913:9:   required by substitution of 'template<class _Functor, class, class> std::function<_Res(_ArgTypes ...)>::function(_Functor) [with _Functor = tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>; <template-parameter-1-2> = void; <template-parameter-1-3> = <missing>]'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1017:7:   required from 'void tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]'\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:2496:12: error: invalid use of incomplete type 'std::__result_of_impl<false, false, tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&>::type'\r\n     struct result_of<_Functor(_ArgTypes...)>\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/type_traits:2291:12: note: declaration of 'std::__result_of_impl<false, false, tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&>::type'\r\n     struct __result_of_success : __success_type<_Tp>\r\n            ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc: In instantiation of 'void tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]':\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1130:1:   required from here\r\ntensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:1017:7: error: no matching function for call to 'stream_executor::Stream::ThenDoHostCallback(tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>&)'\r\n       stream->ThenDoHostCallback(allreduce_launch_callback);\r\n       ^~~~~~\r\nIn file included from tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:31:0:\r\n./tensorflow/stream_executor/stream.h:1987:11: note: candidate: stream_executor::Stream& stream_executor::Stream::ThenDoHostCallback(std::function<void()>)\r\n   Stream &ThenDoHostCallback(std::function<void()> callback);\r\n           ^~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/stream.h:1987:11: note:   no known conversion for argument 1 from 'tensorflow::contrib::mpi_collectives::MPIAllreduceOp<Device>::ComputeAsync(tensorflow::OpKernelContext*, tensorflow::AsyncOpKernel::DoneCallback) [with Device = Eigen::ThreadPoolDevice; tensorflow::AsyncOpKernel::DoneCallback = std::function<void()>]::<lambda()>' to 'std::function<void()>'\r\n```\r\n\r\nOn another note the installation instructions are not helpful if someone uses different distro other than ubuntu.\r\n\r\nAlso I've noticed that during the configuration if you set up option to compile with mpi support then there is a problem in linking the correct files. The `configure.py` sript looks at `/usr/lib/libmpi.so` but my installation is located at `/usr/lib/openmpi/libmpi.so` which I had to configure manually.\r\n\r\nNow for the errors that are occurring dose anyone have any idea why do we get them?\r\nMy baze version is 0.12 the latest release is 0.13? Does that have to do anything or is the gcc version. I have installed on my system 7 and version 6 but the error that I get are from compiling with version 6.\r\n\r\nI hope someone from the dev team can shed some light?", "I am afraid I do not have a very helpful response to your problem too, but MPI support and contrib folder are also community supported. If you can find a fix, we are happy to review and merge. But it is not likely for us to find the cycles and fix this ourselves.\r\n\r\nFor your comment about installation instructions, thanks for your feedback, it is very helpful. This has been mostly because we use ubuntu, and we have not tried many things on other distros. We are slowly trying to make things more distro independent, but as we are mostly ubuntu/debian users, it is usually difficult to see for us. We are happy to discuss any of your concerns, and if you have proposed fixes, I am happy to review them for documentation, too.", "@gunan Thank you for your prompt reply. If u don't mind me asking the binary wheels that u guys provide are they built with or without mpi support?", "As far as I know they are without MPI support.\r\nAdding @av8ramit and @case540 to confirm", "Also pretty sure they are without MPI support.", "@gunan & @case540 thank you! I believe that the error that I'm facing must have to do sth with older c/c++ syntax and newer compiler, at least that's the only thing that I can infer from the error output:\r\n```\r\nerror: 'se' does not name a type\r\n using StatusOr = se::port::StatusOr<T>;\r\n```\r\nBut requires further investigation...", "@kirk86 u just need to change se to stream_executor, and the mpiops.cc just complies successfully. Looks like someone shortened the name of stream_executor as se, and unfortunately the se was deleted in newer code.\r\ncheck the file //tensorflow/stream_executor/lib/statusor.h", "@ghostindex good catch mate. I confirm that indeed changing `se` to `stream_executor` did solve the problem, as you said someone apparently tried to use a shortened version but without linking that somehow to the original namespace."]}]