[{"number": 145, "title": "unable to install -Inside the virtualenv, install TensorFlow:", "body": "everytime i execute the line\n\n(tensorflow)$ pip install --upgrade <$url_to_binary.whl>\n\nI get below errors. Experts pls help. \n\nHemanths-MBP:tensorflow hemanthreganti$ (tensorflow)$ pip install --upgrade <$url_to_binary.whl>\n-bash: syntax error near unexpected token `$'\nHemanths-MBP:tensorflow hemanthreganti$  pip install --upgrade <$url_to_binary.whl>\n-bash: syntax error near unexpected token`newline'\nHemanths-MBP:tensorflow hemanthreganti$ pip install --upgrade <$url_to_binary.whl>\n-bash: syntax error near unexpected token `newline'\nHemanths-MBP:tensorflow hemanthreganti$  source bin/activate \n(tensorflow)Hemanths-MBP:tensorflow hemanthreganti$ pip install --upgrade <$url_to_binary.whl>\n-bash: syntax error near unexpected token`newline'\n(tensorflow)Hemanths-MBP:tensorflow hemanthreganti$ (tensorflow)$ pip install --upgrade <$url_to_binary.whl>\n-bash: syntax error near unexpected token `$'\n(tensorflow)Hemanths-MBP:tensorflow hemanthreganti$ (tensorflow)$ python tensorflow/models/image/mnist/convolutional.py\n-bash: syntax error near unexpected token`$'\n(tensorflow)Hemanths-MBP:tensorflow hemanthreganti$ python tensorflow/models/image/mnist/convolutional.py\npython: can't open file 'tensorflow/models/image/mnist/convolutional.py': [Errno 2] No such file or directory\n(tensorflow)Hemanths-MBP:tensorflow hemanthreganti$ \n", "comments": ["Hemanths-MBP:tensorflow hemanthreganti$ sudo pip install --upgrade virtualenv\nPassword:\nThe directory '/Users/hemanthreganti/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nThe directory '/Users/hemanthreganti/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nRequirement already up-to-date: virtualenv in /Library/Python/2.7/site-packages\n", "The string \"<$url_to_binary.whl>\" should be replaced with the correct URL; please see elsewhere in that document to see which version of the whl file is right for you.\n\nFor the second error, it looks like you already have virtualenv set up.\n", "Thanks ebrevdo. I am unable to find any link on the page http://tensorflow.org/get_started/os_setup.md .Would you be able to provide me with the right link.\n", "That page lists it under OS X section:\nhttps://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n"]}, {"number": 144, "title": "tutorial GPU issue", "body": "Hi guys,\nI suppose that you've got an error in tutorial, section \"Create the pip package and install\" ([link](http://www.tensorflow.org/get_started/os_setup.md#installing_from_sources)):\n\n1) this command should create pip package:\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \n\nIn order to use GPU in training you need to specify --config=cuda:\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \n", "comments": ["+1 I faced a similar issue and it took me a long time to realize this. It would be helpful if the docs are updated.\n", "Cool, will update our docs today if I can, thanks!\n", "Fixed in a recent check-in, website will be updated soon.\n"]}, {"number": 143, "title": "Truncated backdrop with max pooling over time", "body": "It seems like the way truncated backprop is done, at least in the tutorial on RNNs, is to \n- set up the data in batches where the time dimension is capped at the max `num_steps` \n- run forward/backward passes making sure to use the correct initial state (e.g final state of the previous batch if was part of the same observation but prior in time or the true initial state if the current batch starts at true `t=0`)\n\nThis works fine if each hidden state contributes directly to the loss but this approach fails if you want to do something like max pooling across time.  You can carry around the current max (like the correct initial state for truncated backprop) but it seems like the gradient won't flow from the loss, to the max'd elements.  \n\nTo summarize, truncated backprop won't work in situations when a given hidden state at time `t`'s contribution to the loss is unknown without knowing the hidden states for all `t`.  \n\nWould love to be wrong.  \n", "comments": ["No, you are correct. Truncated BP works well for when whatever you are optimizing happens (roughly) in the currently unrolled window. With partial unrolling (compared to complete unrolling) you can do more updates over time, which usually improves convergence speed. Obviously you don't want to unroll for too few steps, as that will increase the error during back-propagation too much. For your problem I would recommend to fully unroll (just increase the num_steps to your maximum sequence length).  \n", "Alright, makes sense.  Thanks for confirming.\n"]}, {"number": 142, "title": "TF not compatible with AWS GPU instances?", "body": "It seems like Google Compute Engine doesn't even have gpu instances, and AWS GPU instance isn't supported because it requires >= 3.5 cuda compute capability.  Is this correct?\n", "comments": ["This is correct.  We currently do not support AWS GPU instances because of their 3.0 cuda compute capability.  You can try disabling that requirement in the source code and seeing if it works for you.\n", "Is there any plan to support them, i.e. does the tensorflow source actually utilize the new features in cuda 3.5 capability?\n", "zheng-xq is working on adding configurable support to it.\n\nFor every compute capability you add, the compile time and binary size increases significantly, so we're trying to find a solution.\n\nIn any case, thanks for the report -- de-duping with https://github.com/tensorflow/tensorflow/issues/25.\n"]}, {"number": 141, "title": "typo in decaying the learning rate example", "body": "From example in [docs](http://tensorflow.org/api_docs/python/train.md#decaying_the_learning_rate):\n\n> learning_rate = tf.exponential_decay(starter_learning_rate, global_step,\n>                                      100000, 0.96, staircase=True)\n\nshould match function definiton above `tf.train.exponential_decay`.\n", "comments": ["Thanks, fixed this in our internal repo -- will push out the fix some time this week.\n"]}, {"number": 140, "title": "Questions about using LSTM ", "body": "Hi. I am a newbie on TensorFlow, just like most of the others here.\nI was trying to use the LSTM modules, and some questions arose.\n1. How can I cope with non-fixed length of sequences? For example, the length of longest sentence in each batch is different, but in RNN tutorial, it does like \n\nfor i in range(len(num_steps)):\n    # The value of state is updated after processing each batch of words.\n    output, state = lstm(words[:, i], state)\n\n....\n\nwhen this \"num_steps\" is not set, what kind of value or variable can I use?\n1. How can I use mask in LSTM? within a batch, all the sentences have different length, so it is essential to mask the lstm results according to the length of each sentence. I don't think there is masking function inside TensorFlor LSTM module. What options do I have?\n\nSorry for bothering you, and thank you for making public this great project.\n\n-Taeksoo\n", "comments": ["This is a good question.  Our current suggestion is to pad your inputs to a fixed length, then slice them into reasonable frame count chunks (e.g. 50); and use truncated BPTT.  The PTB tutorial has an example of truncated BPTT, but not of padding.  However this is easy enough to do in python for now.\n\nWe are considering other ways to work with dynamic length sequences, but nothing is in the release as of now.  But even with purely dynamic RNNs you'll still probably have to pad if you want to minibatch.\n\nA way of dealing with masking is to add a num_steps length list of [batch_size] weight vectors.  Then perform a distributed multiply this with the loss output, aggregate and make that the new loss.  Set weights past your sequence length for each minibatch entry to zero; these particular gradients won't be backpropagated.\n", "Thanks for the question!  These types of questions should be posted to the groups list (github issues are for bugs / installation issues).  https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss\n"]}, {"number": 139, "title": "unable to use nn.moments when the dimension of the axis is None", "body": "calling `nn.moments` with an axis of dimension `None` produces this error:\n\n```\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py\", line 536, in moments\n    divisor *= x.get_shape()[d].value\nTypeError: unsupported operand type(s) for *=: 'float' and 'NoneType'\n```\n\nis it possible for the `nn.moments` implementation to use the inferred dimension in it's calculation?\n", "comments": ["It looks like the problem arises because (i) `tf.nn.moments` relies on knowing the fully-defined shape for its argument, and (ii) the `x` argument that you passed to `tf.nn.moments` has one or more undefined dimensions.\n\nThis can arise when shape inference doesn't have enough information to infer all of the dimensions in your tensor. In this case, the best thing to do is the following:\n\n``` python\nx = ...\naxes = [...]\nx.set_shape([size_in_dim_0, ...])  # Each `size_in_dim_i` must be an integer.\nx_moments = tf.nn.moments(x, axes)\n```\n\nIf the size in any of those dimensions is dynamic, the current implementation of `tf.nn.moments()` will not work, but it is possible to add a custom version that does the trick:\n\n``` python\ndef moments(x, axes, name=None):\n  \"\"\"Version of tf.nn.moments that supports variable-sized tensors.\n\n  N.B. The rank must be known statically for this version to work.\n  \"\"\"\n  with tf.op_scope([x], name, \"moments\"):\n    x = tf.convert_to_tensor(x, name=\"x\")\n    divisor = tf.constant(1.0, dtype=x.dtype)\n    x_dynamic_shape = tf.shape(x)\n    for d in xrange(x.get_shape().ndims):\n      if d in axes:\n        divisor = divisor * x_dynamic_shape[d]\n    divisor = tf.inv(divisor)\n    mean = math_ops.mul(math_ops.reduce_sum(x, axes), divisor, name=\"mean\")\n    var = math_ops.mul(math_ops.reduce_sum(math_ops.square(x - mean), axes),\n                       divisor, name=\"variance\")\n    return mean, var\n```\n\nA version that works with dynamic rank tensors would also be possible, but it would be quite a bit more complicated.\n\nIt would also be possible to replace the `divisor` computation with a call to `tf.mean()`, but as the comment in [the implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn.py#L539) mentions, there are sum performance issues when using `tf.mean()` on GPU at present.\n", "I'm using this custom function right now:\n\n``` python\ndef moments(x, axes, name=None):\n  with tf.op_scope([x, axes], name, \"moments\"):\n    x = tf.convert_to_tensor(x, name=\"x\")\n    divisor = tf.constant(1.0)\n    for d in xrange(len(x.get_shape())):\n      if d in axes:\n        divisor *= tf.to_float(tf.shape(x)[d])\n    divisor = tf.inv(divisor, name=\"divisor\")\n    axes = tf.constant(axes, name=\"axes\")\n    mean = tf.mul(tf.reduce_sum(x, axes), divisor, name=\"mean\")\n    var = tf.mul(tf.reduce_sum(tf.square(x - mean), axes),\n                       divisor, name=\"variance\")\n    return mean, var\n```\n\nI guess it is not trivial to access the function tf.shape within the `nn.moments` implementation\n", "Thanks for sharing your workaround! We're working on a fix that should address this in the core library.\n", "Should be fixed by @mrry in 1d76583411038767f673a0c96174c80eaf9ff42f, which I just pushed to HEAD.  Will be in the next binary release.\n", "Thanks for this version of moments as it's still an issue of tf.nn.moments! \n"]}, {"number": 138, "title": "Out of Memory in mnist?", "body": "ubuntu@slave1:/media/slave1temp/tensorflow_full$ python tensorflow/tensorflow/models/image/mnist/convolutional.py\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:888] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties: \nname: GeForce GTX 750 Ti\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.189\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.96GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 1896685568\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8\nInitialized!\nEpoch 0.00\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 256 (256B) Pool: chunks: 64 free: 35 cumulative malloc: 93 cumulative freed: 64\nNumber of chunks: 64, in_use chunks: 29\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 2048 (2.0KiB) Pool: chunks: 8 free: 4 cumulative malloc: 7 cumulative freed: 3\nNumber of chunks: 8, in_use chunks: 4\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 4096 (4.0KiB) Pool: chunks: 16 free: 13 cumulative malloc: 18 cumulative freed: 15\nNumber of chunks: 16, in_use chunks: 3\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 32768 (32.0KiB) Pool: chunks: 8 free: 5 cumulative malloc: 9 cumulative freed: 6\nNumber of chunks: 8, in_use chunks: 3\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 139264 (136.0KiB) Pool: chunks: 15 free: 15 cumulative malloc: 16 cumulative freed: 16\nNumber of chunks: 15, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 212992 (208.0KiB) Pool: chunks: 12 free: 9 cumulative malloc: 14 cumulative freed: 11\nNumber of chunks: 12, in_use chunks: 3\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 278528 (272.0KiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1\nNumber of chunks: 1, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 851968 (832.0KiB) Pool: chunks: 4 free: 4 cumulative malloc: 4 cumulative freed: 4\nNumber of chunks: 4, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 1703936 (1.62MiB) Pool: chunks: 6 free: 6 cumulative malloc: 6 cumulative freed: 6\nNumber of chunks: 6, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 2883584 (2.75MiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1\nNumber of chunks: 1, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 3407872 (3.25MiB) Pool: chunks: 8 free: 8 cumulative malloc: 9 cumulative freed: 9\nNumber of chunks: 8, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 6815744 (6.50MiB) Pool: chunks: 12 free: 9 cumulative malloc: 18 cumulative freed: 15\nNumber of chunks: 12, in_use chunks: 3\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 15728640 (15.00MiB) Pool: chunks: 1 free: 0 cumulative malloc: 1 cumulative freed: 0\nNumber of chunks: 1, in_use chunks: 1\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 20971520 (20.00MiB) Pool: chunks: 1 free: 1 cumulative malloc: 1 cumulative freed: 1\nNumber of chunks: 1, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 125829120 (120.00MiB) Pool: chunks: 1 free: 0 cumulative malloc: 1 cumulative freed: 0\nNumber of chunks: 1, in_use chunks: 1\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 251658240 (240.00MiB) Pool: chunks: 0 free: 0 cumulative malloc: 0 cumulative freed: 0\nNumber of chunks: 0, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:339] Chunk size: 503316480 (480.00MiB) Pool: chunks: 3 free: 3 cumulative malloc: 3 cumulative freed: 3\nNumber of chunks: 3, in_use chunks: 0\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:345] Aggregate Region Memory: 1896685568 (1.77GiB)\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:347] Aggregate Chunk Memory: 1803329536 (1.68GiB)\nW tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:89] Out of GPU memory, see memory state dump above\nW tensorflow/core/kernels/conv_ops.cc:162] Resource exhausted: OOM when allocating tensor with shapedim { size: 5000 } dim { size: 14 } dim { size: 14 } dim { size: 64 }\nW tensorflow/core/common_runtime/executor.cc:1027] 0x57b6090 Compute status: Resource exhausted: OOM when allocating tensor with shapedim { size: 5000 } dim { size: 14 } dim { size: 14 } dim { size: 64 }\n     [[Node: Conv2D_3 = Conv2D[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](MaxPool_2, Variable_2)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x50bc640 Compute status: Resource exhausted: OOM when allocating tensor with shapedim { size: 5000 } dim { size: 14 } dim { size: 14 } dim { size: 64 }\n     [[Node: Conv2D_3 = Conv2D[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](MaxPool_2, Variable_2)]]\n     [[Node: Softmax_1/_45 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_735_Softmax_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nTraceback (most recent call last):\n  File \"tensorflow/tensorflow/models/image/mnist/convolutional.py\", line 270, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 11, in run\n    sys.exit(main(sys.argv))\n  File \"tensorflow/tensorflow/models/image/mnist/convolutional.py\", line 258, in main\n    validation_prediction.eval(), validation_labels)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 405, in eval\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2728, in _eval_using_default_session\n    return session.run(tensors, feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 345, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 419, in _do_run\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 5000 } dim { size: 14 } dim { size: 14 } dim { size: 64 }\n     [[Node: Conv2D_3 = Conv2D[T=DT_FLOAT, padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](MaxPool_2, Variable_2)]]\n     [[Node: Softmax_1/_45 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_735_Softmax_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Conv2D_3', defined at:\n  File \"tensorflow/tensorflow/models/image/mnist/convolutional.py\", line 270, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 11, in run\n    sys.exit(main(sys.argv))\n  File \"tensorflow/tensorflow/models/image/mnist/convolutional.py\", line 229, in main\n    validation_prediction = tf.nn.softmax(model(validation_data_node))\n  File \"tensorflow/tensorflow/models/image/mnist/convolutional.py\", line 179, in model\n    padding='SAME')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 207, in conv2d\n    use_cudnn_on_gpu=use_cudnn_on_gpu, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 633, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1710, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 988, in __init__\n    self._traceback = _extract_stack()\n\nubuntu@slave1:/media/slave1temp/tensorflow_full$ \n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/136\n"]}, {"number": 137, "title": "bazel always re-downloaded the dependency libraries", "body": "I use `bazel build -c opt //tensorflow/tools/pip_package:build_pip_package` to build the package. But it always re-downloaded the dependency git repositories. \n\nHi, I'm not familiar with `bazel`. Is it reasonable?\n", "comments": []}, {"number": 136, "title": "ResourceExhaustedError in CNN/MNIST example (with GPU)", "body": "(I'm using GPU(GTX 980) with CUDA-7.0&cuDNNv2, on Ubuntu 14.04)\nI have gone through MNIST tutorial: http://tensorflow.org/tutorials/mnist/pros/index.md\n\nEverything was going well except for the last two lines:\n\n``` python\nprint \"test accuracy %g\"%accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n```\n\nExecuting these lines, I got an error: \n\n``` python\nResourceExhaustedError: OOM when allocating tensor with shapedim { size: 10000 } dim { size: 18 } dim { size: 18 } dim { size: 32 }\n```\n\nI think basic reason for this error is that test data can not be allocated to GPU device.\nIs this a bug or not? Are there good way to avoid this issue?\n", "comments": ["I am having the same issue on OS X Yosemite with Docker container.\nIn my case it should be using the CPU, I tried bumping Docker memory limit to 8GB but still getting OOM  .\nLooking at Docker Stats, during the final accuracy.eval() memory goes from 300MB to 900MB then it crashes.\n", "If you don't have enough memory on your GPU to fit the whole test data, you could feed it in small batches to the eval graph using feed_dict like the example does with the training data.\n", "I solved my case. Being on OS X, I use Docker via the docker-machine VM.\nBoth Docker and docker-machine VM have a default memory limit of 1GB, increasing both memory limits I was able to get the results without OOM errors. \n@ywatanabex ,  if it may help you, using Docker stats I saw memory peak around 1.9GB \n", "I have this same problem on this same example with Ubuntu 14.04 with Nvidia GTX 970. Hits 3.35 GB usage and then crashes.\n", "Same problem when running from a Dockers terminal on a Windows machine. I ran the example convolutional.py as suggested in their tutorials and for six hours it was still running chugging out the gradually decreasing error rates before crashing out!\nAnything unusual about it taking 6 hours or is it normal since I am using CPU and not the recommended GPU for DNNs?\n", "Submitted a PR to that effect : https://github.com/tensorflow/tensorflow/pull/157\n\nDid just as @vincentvanhoucke suggested and transformed the test set to use batches.\n", "I had the same issue with Ubuntu 14.04LTS & GTX970, but now solved with the code by @mtourne (#157). Thanks!\n", "@mtourne Thanks a lot for the fix. It works for my GPU (previously I also had Resource Exhausted error).\n\nAnother way following @vrv 's advice from [this discussion](https://github.com/soumith/convnet-benchmarks/issues/66) is to install tensorflow from the latest source and configure your session to use BFC allocator before running it like this:\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allocator_type = 'BFC'\nwith tf.Session(config = config) as s:\n\nThe original example from @vrv  is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/alexnet/alexnet_benchmark.py#L201). My GPU has 6 GB though. This allocator seems to dynamically allocate memory according to GPU's memory so it will probably work with cards having less memory as well.\n", "@mondatu: just to be clear, you're saying that just using BFC alone is enough to solve the mnist tutorial out-of-memory?  No need to do the batching in mnist?\n\n(The default allocator and the BFC allocator do dynamic memory allocation but the default one is more prone to unrecoverable fragmentation -- BFC behaves better for more models).\n", "@vrv yes, I didn't use the batch version of @mtourne. I updated tensorflow source code, compiled and reinstalled. Then I modified the original file convolutional.py with the above code I mentioned, thanks to your advice. It worked with my card (Titan Z) with 6 GB (obviously it runs only on one single GPU inside that duo card).\n\nI also run the original file convolutional.py (with no modification) with the updated tensorflow source code and I got the same old Resource Exhaustion error\n", "Cool, glad to know it worked!  We made the BFC allocator the default yesterday, so if you sync to head you probably don't need your additional modifications.  This will make it into the next binary release.  Thanks again!\n", "Has anyone tried BFC allocator with \"only\" 4GB of gpu memory ?\n", "You're welcome. Yeah, I updated and now I can run the original file without modification.\nThanks a lot to you guys for developing and releasing this wonderful framework.\n", "Sounds like this has been resolved.  Let us know if this issue comes back.\n", "I'm using the BFC allocator on a GTX 750 (1GB memory). I see a bunch of lines for the `bfc_allocator` creating bins (from chunk 1KB to 1GB), like \"Creating bin of max chunk size 1.00GiB\". The BFC allocator also says it's \"Allocating 741.23MiB bytes\", which should be in the range. Also I run `nvidia-smi` during the run to monitor the memory, and I see it stabilize at 871MiB before crashing.\n\nIt crashes with (during the first epoch):\n\n```\nBin for 19.53MiB was 32.00MiB, Chunk State:\nRan out of memory trying to allocate 19.53MiB.  See logs for memory state\n```\n\nAny ideas? Anyone else run with around the same GPU memory? I can supply more trace info if needed, thanks!\n", "Incase anyone else is having this issue running on docker I solved it by creating a docker machine as follows: `docker-machine create --driver virtualbox --virtualbox-memory 4096 --virtualbox-cpu-count 4 default`\n", "@skimmer: see PR : #157 from @mtourne, I needed to apply that patch for the script to run properly on my machine. FWIW, my graphics card is the [NVIDIA GeForce GTX 780 Ti](http://www.nvidia.com/gtx-700-graphics-cards/gtx-780ti).\n", "Same issue with the latest stable release of Tensorflow, Quadro 970M with 2GB mem. \n\nAccording the logging output, BFC allocator was being used. \n\n```\nW tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:211] Ran out of memory trying to allocate 29.91MiB.  See logs for memory state\n```\n\n29.9 MB doesn't seem like an awful lot, but I'm assuming it has to be allocated contiguously. \n", "Me too, exactly the same as @orodbhen with 2GB GeForce GTX 960M\n", "As an additional data point, I'm getting the same as @lookfwd and @orodbhen with a GeForce GTX 750 (1 GB). The manual batching from @mtourne is fairly out of date (relative to the git master branch). Just FYI.\n", "FYI: I had the same error (Ram 32GB, Titan X 12GB). **Restarting iPython notebook helped**.\n", "I was trying the Deep MNIST from the tutorial on the site. Using the entire test set throws the ran out of memory trying to allocate 78.1KiB (So close!)\r\nChanging the batches of the test images works, But pushing it to batches of 5000, just so I could see where exactly the program breaks, gave me a warning that it ran out of memory. The strange thing is that, while the previous attempt, without batching, completely broke the program, this didn't. I still got the result, but with a lot of warning that says:\r\n\r\n> W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:217] Ran out of memory trying to allocate 2.92GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n\r\nI am using a 2GB 960M and I can confirm that the BFC option is enabled, since there were related errors with chunks when it completely crashed the first time. ", "Would running this in a terminal console avoid memory problems compared to the Python notebook?", "The problem is that one of the layers needs 2GB of RAM for the activations if you use batch size 10k. So running in terminal would not avoid memory problems.\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/6019#issuecomment-267881864 for memory analysis", "I'm using a 2GB 860M.\r\nmnist.test.image.shape is (10000,784).\r\nRestricting shape_size to 7000, I could do it.\r\n\r\nbatch size\r\n`batch_tx, batch_ty = mnist.test.next_batch(10)`    \r\n`print(\"test accuracy %g\"%accuracy.eval(feed_dict={x:batch_tx, y_: batch_ty, keep_prob: 1.0}))`\r\n-> test accuracy 0.992\r\n\r\nor\r\n\r\nslice\r\n`test_image = mnist.test.images[0:7000, :]`      \r\n`test_label = mnist.test.labels[0:7000, :]`            \r\n`print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: test_image, y_: test_label, keep_prob: 1.0}))`\r\n->test accuracy 0.992", "I am doing similar things as @Shuto050505 did here, but I compute the mean of accuracy for each batch in all test data. Replace the following line:\r\n\r\n```\r\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={ \r\n      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\r\n```\r\n\r\nby this:\r\n\r\n```\r\nbatch_size = 50\r\nbatch_num = int(mnist.test.num_examples / batch_size)\r\ntest_accuracy = 0\r\n    \r\nfor i in range(batch_num):\r\n    batch = mnist.test.next_batch(batch_size)\r\n    test_accuracy += accuracy.eval(feed_dict={x: batch[0],\r\n                                              y_: batch[1],\r\n                                              keep_prob: 1.0})\r\n\r\ntest_accuracy /= batch_num\r\nprint(\"test accuracy %g\"%test_accuracy)\r\n````\r\n\r\nI will get the mean of test accuracy `test accuracy 0.9922`.\r\n\r\n_EDIT: Updated accuracy with corrected training process_", "Encountered the same problem with a 2GB GeForce GT 710(btw it worked well the first time) on Linux Mint KDE 18(Sarah). @ChinChangYang 's solution works flawlessly and finally saved me after two days of failing to fix this.", "Encountered the same issue on my Windows 10, 2GB GTX1050. Using @ChinChangYang 's solution fixed it for me. (Make sure to place the other code at the right indentation, so it falls within the `with` block", "Just restarting Pycharm resolved this problem for me as well", "I am new to tensorflow and Machine Learning. Recently I am working on a model. My model is like below,\r\n\r\n1. Character level Embedding Vector -> Embedding lookup -> LSTM1\r\n\r\n2. Word level Embedding Vector->Embedding lookup -> LSTM2 \r\n\r\n3. [LSTM1+LSTM2] -> single layer MLP-> softmax layer\r\n\r\n4. [LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator\r\n\r\nwhile I'm working on this model I got the following error. I thought My batch is too big. Thus I tried to reduce the batch size from 20 to 10 but it doesn't work. \r\n\r\n> ResourceExhaustedError (see above for traceback): OOM when allocating\r\n> tensor with shape[24760,100] \t [[Node:\r\n> chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split =\r\n> Split[T=DT_FLOAT, num_split=4,\r\n> _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y,\r\n> chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] \t [[Node:\r\n> bi-lstm/bidirectional_rnn/bw/bw/stack/_167 =\r\n> _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\",\r\n> send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\",\r\n> send_device_incarnation=1,\r\n> tensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\",\r\n> tensor_type=DT_INT32,\r\n> _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\ntensor with ***shape[24760,100]*** means 2476000*32/1024*1024 = 75.*** MB memory. I am running the code on a titan X(11 GB) gpu. What could go wrong. Why this type of error occured?\r\n\r\nExtra info: the size of the LSTM1 is 100. for bidirectional LSTM it becomes 200.\r\nThe size of the LSTM2 is 300. For Bidirectional LSTM it becomes 600.\r\n\r\n***Note*** : The error occurred after 32 epoch. My question is why after 32 epoch there is an error. Why not at the initial epoch.", "I was faced with the same \"ResourceExhaustedError\" issue, so I changed the code as follows. I have 2GB of GPU memory, but I failed to run this simple code. I don't know how much memory should my video card has. Anyway, I changed the code to do not use the GPU and fix the issue.\r\n\r\n...\r\nconfig = tf.ConfigProto(\r\n\tdevice_count = {'GPU':0}\r\n)\r\nsess = tf.Session(config=config)\r\n...\r\n\r\n", "Just reduce the batch size while feeding the test data to GPU", "\"Just reduce the batch size while feeding the test data to GPU\"\r\nbatch_size=1\r\n\r\nNow what? 0.5, 0?\r\n\r\nHave 2 1080ti's, and sill running out of memory.", "@shaunstoltz, what does `nvidia-smi` display before and after `ResourceExhaustedError`?", "I have had a similar problem just now.\r\nBatch of images had to be analysed and I, by mistake, created a function, that was loading a model into the memory for every image to be recognised.\r\n\r\n\r\n```\r\ndef predict(num_class, weights_path, img_path):    \r\n\tbase_model = VGG16.VGG16(include_top=False, weights=None)\r\n\tx = base_model.output\r\n\tx = Dense(128)(x)\r\n\tx = GlobalAveragePooling2D()(x)\r\n\tpredictions = Dense(num_class, activation='softmax')(x)\r\n\r\n\tmodel = Model(inputs=base_model.input, outputs=predictions)\r\n\tmodel.load_weights(weights_path)\r\n\t....\r\n\t....\r\n```\r\n\r\n\r\nHopefully, it will help someone!", "I met the same problem in GPU, but when i change it to the cpu, it works well. the gpu is too samll! ", "I came across the same problem. I shut down all the anaconda prompt windows and cleared all the python tasks. Reopened an Anaconda prompt window and executed the train.py file with GPU. It worked for me the next time. The Anaconda and Python terminals were taking up the memory which doesn't leave space for the training process. Hope this helps \ud83d\udc4d", "> I was trying the Deep MNIST from the tutorial on the site. Using the entire test set throws the ran out of memory trying to allocate 78.1KiB (So close!)\r\n> Changing the batches of the test images works, But pushing it to batches of 5000, just so I could see where exactly the program breaks, gave me a warning that it ran out of memory. The strange thing is that, while the previous attempt, without batching, completely broke the program, this didn't. I still got the result, but with a lot of warning that says:\r\n> \r\n> > W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:217] Ran out of memory trying to allocate 2.92GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n> \r\n> I am using a 2GB 960M and I can confirm that the BFC option is enabled, since there were related errors with chunks when it completely crashed the first time.\r\n\r\nDo you know what happens when this message occurs now? I am confused and have no idea why we can get a result when we run out of memory.", "> I am new to tensorflow and Machine Learning. Recently I am working on a model. My model is like below,\r\n> \r\n> 1. Character level Embedding Vector -> Embedding lookup -> LSTM1\r\n> 2. Word level Embedding Vector->Embedding lookup -> LSTM2\r\n> 3. [LSTM1+LSTM2] -> single layer MLP-> softmax layer\r\n> 4. [LSTM1+LSTM2] -> Single layer MLP-> WGAN discriminator\r\n> \r\n> while I'm working on this model I got the following error. I thought My batch is too big. Thus I tried to reduce the batch size from 20 to 10 but it doesn't work.\r\n> \r\n> > ResourceExhaustedError (see above for traceback): OOM when allocating\r\n> > tensor with shape[24760,100] \t [[Node:\r\n> > chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/split =\r\n> > Split[T=DT_FLOAT, num_split=4,\r\n> > _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](gradients_2/Add_3/y,\r\n> > chars/bidirectional_rnn/bw/bw/while/bw/lstm_cell/BiasAdd)]] \t [[Node:\r\n> > bi-lstm/bidirectional_rnn/bw/bw/stack/_167 =\r\n> > _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\",\r\n> > send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\",\r\n> > send_device_incarnation=1,\r\n> > tensor_name=\"edge_636_bi-lstm/bidirectional_rnn/bw/bw/stack\",\r\n> > tensor_type=DT_INT32,\r\n> > _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]]\r\n> \r\n> tensor with _**shape[24760,100]**_ means 2476000_32/1024_1024 = 75.*** MB memory. I am running the code on a titan X(11 GB) gpu. What could go wrong. Why this type of error occured?\r\n> \r\n> Extra info: the size of the LSTM1 is 100. for bidirectional LSTM it becomes 200.\r\n> The size of the LSTM2 is 300. For Bidirectional LSTM it becomes 600.\r\n> \r\n> _**Note**_ : The error occurred after 32 epoch. My question is why after 32 epoch there is an error. Why not at the initial epoch.\r\n\r\nHello, have you solved it? I have also encountered the same problem as you.  ", "@vaibhavsood 6GB 1080GTX is not enough?", "Guys anybody knows how to flush nvidia gpu cache memory (tell how to both in windows and in ubuntu).... ! sometimes even that might work", "> \r\n> \r\n> Guys anybody knows how to flush nvidia gpu cache memory (tell how to both in windows and in ubuntu).... ! sometimes even that might work\r\n\r\nimport gc;    gc.collect()", "use small patches, this the solution for this issue.", "For the people stuck with this in models other than mnist.\r\nthe reason for this is the high amount of parameters (please check your `model.summary()`).\r\n\r\nA good method to drastically lower these parameters is to add:\r\n`subsample=(2, 2)` (careful it lowers the resolution of images/data) in all the Convolutional layers above that Flatten layer,\r\nif subsample doesn't work then it is `stride=(2, 2)`.", "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,3,8,128,8,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[node GPU0/D_loss/G/G_synthesis/cond/cond_1/Upscale2D_2/Tile (defined at D:\\jiateng\\project\\stylegan\\stylegan\\training\\networks_stylegan.py:66) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\u8fd9\u4e2a\u600e\u4e48\u89e3\u51b3\u554a\r\n\u914d\u7f6e\u4fe1\u606f\uff1a\r\n{   'ask_confirmation': False,\r\n    'host_name': 'localhost',\r\n    'num_gpus': 1,\r\n    'print_info': False,\r\n    'run_desc': 'sgan-pictrue-1gpu',\r\n    'run_dir': 'results\\\\00000-sgan-pictrue-1gpu',\r\n    'run_dir_extra_files': None,\r\n    'run_dir_ignore': ['__pycache__', '*.pyproj', '*.sln', '*.suo', '.cache', '.idea', '.vs', '.vscode', 'results', 'datasets', 'cache'],\r\n    'run_dir_root': 'results',\r\n    'run_func_kwargs': {   'D_args': {'func_name': 'training.networks_stylegan.D_basic'},\r\n                           'D_loss_args': {'func_name': 'training.loss.D_logistic_simplegp', 'r1_gamma': 10.0},\r\n                           'D_opt_args': {'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08},\r\n                           'G_args': {'func_name': 'training.networks_stylegan.G_style'},\r\n                           'G_loss_args': {'func_name': 'training.loss.G_logistic_nonsaturating'},\r\n                           'G_opt_args': {'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08},\r\n                           'dataset_args': {'tfrecord_dir': 'pictrue'},\r\n                           'grid_args': {'layout': 'random', 'size': '4k'},\r\n                           'metric_arg_list': [{'func_name': 'metrics.frechet_inception_distance.FID', 'minibatch_per_gpu': 1, 'name': 'fid50k', 'num_images': 50000}],\r\n                           'mirror_augment': True,\r\n                           'sched_args': {   'D_lrate_dict': {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003},\r\n                                             'G_lrate_dict': {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003},\r\n                                             'lod_initial_resolution': 8,\r\n                                             'minibatch_base': 4,\r\n                                             'minibatch_dict': {4: 128, 8: 128, 16: 128, 32: 64, 64: 32, 128: 16, 256: 8, 512: 4}},\r\n                           'tf_config': {'rnd.np_random_seed': 1000},\r\n                           'total_kimg': 25000},\r\n    'run_func_name': 'training.training_loop.training_loop',\r\n    'run_id': 0,\r\n    'run_name': '00000-sgan-pictrue-1gpu',\r\n    'submit_target': <SubmitTarget.LOCAL: 1>,\r\n    'task_name': 'Administrator-00000-sgan-pictrue-1gpu',\r\n    'user_name': 'Administrator'}\r\n"]}, {"number": 135, "title": "osx 10.11 installation issues", "body": "Trying to install it with `pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.wh`. Same issue if doing it with virutalenv. \n\nException:\nTraceback (most recent call last):\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/basecommand.py\", line 211, in main\n    status = self.run(options, args)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/commands/install.py\", line 305, in run\n    wb.build(autobuilding=True)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/wheel.py\", line 705, in build\n    self.requirement_set.prepare_files(self.finder)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/req/req_set.py\", line 334, in prepare_files\n    functools.partial(self._prepare_file, finder))\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/req/req_set.py\", line 321, in _walk_req_to_install\n    more_reqs = handler(req_to_install)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/req/req_set.py\", line 491, in _prepare_file\n    session=self.session)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/download.py\", line 825, in unpack_url\n    session,\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/download.py\", line 673, in unpack_http_url\n    from_path, content_type = _download_http_url(link, session, temp_dir)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/download.py\", line 857, in _download_http_url\n    stream=True,\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 477, in get\n    return self.request('GET', url, *_kwargs)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/download.py\", line 373, in request\n    return super(PipSession, self).request(method, url, *args, *_kwargs)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 465, in request\n    resp = self.send(prep, *_send_kwargs)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py\", line 573, in send\n    r = adapter.send(request, *_kwargs)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py\", line 46, in send\n    resp = super(CacheControlAdapter, self).send(request, **kw)\n  File \"/Users/Schmidt/anaconda/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py\", line 431, in send\n    raise SSLError(e, request=request)\nSSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)\n\n I've installed openssl (1.0.2d) through homebrew.\n\nTried with bazel: `bazel build -c opt //tensorflow/tools/pip_package:build_pip_package` and the following error pops up:\n`ERROR: no such package 'tensorflow/tools/pip_package': BUILD file not found on package path.`\nI've got tensorflow running with docker but I want it in the host machine.\n", "comments": ["This person seems to have had the same problem: https://github.com/tensorflow/tensorflow/issues/64#issuecomment-155270240\n\nThey solved it by downloading .whl file manually ( https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl ) and run $pip install tensorflow-0.5.0-py2-none-any.whl\n\nFor other common OS X installation issues and troubleshooting: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#on-macosx-\n", "Thanks for posting the problem and the solution.  I had the exact same problem and the solution given worked for me as well.  \n", "Still works. Thanks\n", "Installing easy_install and upgrading setup-tools also solves the issue. \nUse command: `curl https://bootstrap.pypa.io/ez_setup.py -o - | python` (https://pythonhosted.org/setuptools/easy_install.html#installing-easy-install). \n", "@arustagi\u2019s solution worked for me. This is with TensorFlow version `tensorflow-0.7.1`.\n", "@iRapha This worked perfectly thanks.\n", "@iRapha Worked for me perfectly with tensorflow-0.9.0.\n", "Solved the same problem for me with TF 0.9.0 on OSX 10.10.5\n", "@arustagi\u2019s solution worked for me. This is with TensorFlow version tensorflow-0.10.0rc0\n", "Has anyone experienced this issue in Linux? I'm having this issue in a linux box.\n", "@brando90 I had a similar issue on a Linux box - fixed it by running\n\n```\npip install --upgrade $TF_BINARY_URL\npip install -I --upgrade setuptools\npip install --upgrade $TF_BINARY_URL\n\n```\n", "Just had this issue with Conda 4.2.7/Python 3.5.2/pip 8.1.2./Tensorflow 0.10 (CPU) on a Mac (OS X 10.11.4) in a fresh conda env. The solution @wingated posted for Linux worked for me.\n", "Had the same issue with conda virtual environment. Solved it using suggestion by https://github.com/ContinuumIO/anaconda-issues/issues/542\nIn short, next line before upgrading the tensorflow does the trick:\n`pip install --upgrade --ignore-installed setuptools`\n", "@yselivonchyk worked for me ! Thanks ! \ud83d\udc4d \n", "@yselivonchyk awesome - I came here with a similar problem with conda, but when trying to install the google cloud pubsub client lib.\n", "@yselivonchyk solution worked for me. \r\nInstalling tensorflow-0.12 on windows.", "@arustagi: your solution worked for me. Thanks!", "@yselivonchyk finally... thank you", "@yselivonchyk thank you, now it works =)", "@yselivonchyk   works great!", "@yselivonchyk, solution worked for me thank you.", "@yselivonchyk, finally worked!! thanks!!!!!", "@yselivonchyk Thanks! Now the error was solved. TensorFlow 1.0.0(CPU) on Anaconda 4.2(Python 3.5.2) with Win8.1 64bit.", "@yselivonchyk Thanks! Error problem was solved. Anaconda 4.2.12 Python 2.7 with Win7 64bit", "@yselivonchyk works for me"]}, {"number": 134, "title": "Linux installation issue for GPU-enabled version", "body": "Hi,\n\nI ran the command \"$ pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\" and I got:\n\n```\nDownloading/unpacking https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n  Downloading tensorflow-0.5.0-cp27-none-linux_x86_64.whl (50.5Mb): 50.5Mb downloaded\n  Running setup.py egg_info for package from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n    Traceback (most recent call last):\n      File \"<string>\", line 14, in <module>\n    IOError: [Errno 2] No such file or directory: '/tmp/pip-o6Tpui-build/setup.py'\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n\n  File \"<string>\", line 14, in <module>\n\nIOError: [Errno 2] No such file or directory: '/tmp/pip-o6Tpui-build/setup.py'\n\n----------------------------------------\nCommand python setup.py egg_info failed with error code 1 in /tmp/pip-o6Tpui-build\nStoring complete log in /home/shubhanshu/.pip/pip.log\nTraceback (most recent call last):\n  File \"/usr/bin/pip\", line 9, in <module>\n    load_entry_point('pip==1.1', 'console_scripts', 'pip-2.7')()\n  File \"/usr/lib/python2.7/dist-packages/pip/__init__.py\", line 116, in main\n    return command.main(args[1:], options)\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 141, in main\n    log_fp = open_logfile(log_fn, 'w')\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 168, in open_logfile\n    log_fp = open(filename, mode)\nIOError: [Errno 13] Permission denied: '/home/shubhanshu/.pip/pip.log'\n```\n\nAny directions, how to proceed ahead?\n", "comments": ["I've also encountered the missing setup.py error with an older version of pip. After upgrading pip it went through fine.\n", "```\nsudo pip install pip --upgrade\n```\n", "Great!  We added these instructions to our installation docs.  Hopefully it addresses most people's issues.\n"]}, {"number": 133, "title": "Linux installation problem with VirtualEnv", "body": "I'm using virtual-env to install tensorflow. I've successfully created a virtual-env and when trying to install tensorflow using pip, i'm getting this error.\nAny suggestions ?? Thanks.\n\n```\nUnpacking /home/ayan/Desktop/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n  Running setup.py egg_info for package from file:///home/ayan/Desktop/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n    Traceback (most recent call last):\n      File \"<string>\", line 16, in <module>\n    IOError: [Errno 2] No such file or directory: '/tmp/pip-ULRUTP-build/setup.py'\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n  File \"<string>\", line 16, in <module>\nIOError: [Errno 2] No such file or directory: '/tmp/pip-ULRUTP-build/setup.py'\n----------------------------------------\nCommand python setup.py egg_info failed with error code 1 in /tmp/pip-ULRUTP-build\nStoring complete log in /home/ayan/.pip/pip.log\n```\n", "comments": ["Also happens on Mac: https://github.com/tensorflow/tensorflow/issues/46\n Also reported on stackexchange: http://stackoverflow.com/questions/33623453/pip-installation-error-no-such-file-or-directory-setup-py where they claim that \"with python 2.7.10, python-dev and pip 1.5.6, tensorflow will be successfully installed using the pip method.\"\n\nBut, as seen below, updating pip did _not_ work for **spiderhacker**, so I am striking out the rest of this comment. -2015 Nov 13 23:45 UTC\n\n~~What version of pip are you using (type \"pip --version\")~~\n~~If you see a version less than 1.5.6, type \"pip install --upgrade pip\" within your virtualenv to get it to update itself.~~\n~~I did this in my system and it worked (but I'm on Mac, so you still need to try it and comment here to tell if it worked)~~\n", "My pip is up-to-date (\"pip --version\" gives 7.1.2). But my python version is 2.7.4. Is that the problem ?\n", "That's kind of odd, since I got it to work in Python 2.7.3. (And it fixed [my version of the bug](https://github.com/tensorflow/tensorflow/issues/46) )\n\nWas virtualenv still activated in your shell session? To be clear, the full command sequence would be:\n\n```\n# assuming we want to be working within ~/tensorflow\nvirtualenv --system-site-packages ~/tensorflow\ncd ~/tensorflow\nsource bin/activate\n# we are now \"inside the virtual environment\", and ./bin is at the beginning of $PATH\npip --version    # Here I had version 1.1\npip install --upgrade pip\npip --version    # ...and now it says 7.1.2\n# In the following use \"/cpu/\" or \"/gpu\" as desired\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n# (we would continue with tensorflow setup/testing here)\n# when done, we could exit the shell completely, or just leave virtualenv with this command:\ndeactivate\n# Now the prompt and $PATH are back to normal\n```\n", "Its still the same after upgrading everything. Python is now 2.7.10, pip is 7.1.2.\nVirtualEnv is still activated in shell. And the sequence is absolutely same as you showed.\nAfter downloading the .whl file, I'm getting the same error.\n", "Darn, I'm out of ideas. But I will edit my comment above to remove the suggestion that \"this worked for me so it should work for you\".\n", "Did this issue get resolved? A lot of things have changed with 0.6.0.\n"]}, {"number": 132, "title": "Communicating channels: gitter.im + discourse", "body": "If we're in for the openess and effectiveness of the whole thing...\n\n(1) Why not using gitter.im ?\n\nI also believe we should have a discourse forums, which I'm more than willing to help / set up if you really need (just can't pay for it or move people from the mailing list without being official).\n\n(2) Yes, discourse would be a **perfect** replacement for the mailing list **and** to this Issue tracker. :)\n", "comments": ["Can you give example of a large OSS project that are using discourse effectively? thanks. \n", "@anshzinc other than discourse.org itself: http://discourse.ubuntu.com/ , http://discuss.atom.io/ and https://forums.docker.com/ - [the list](https://meta.discourse.org/t/please-visit-our-discourse-forum-directory/3102?u=cawas) is actually pretty huge, so maybe it's better to just see [a few of their customers](http://www.discourse.org/faq/customers/)\n", ":+1:\n", "Should probably split this into 2 separate issues, shouldn't we?\n", "since the TF community is growing so rapidly, I'm not sure if the TF devs at Google will have bandwidth to scale with this growth. Just an idea: How about we start something unofficial where anyone interested in TF can log on and ask questions to the room? This will also cut down the number of duplicate issues we are seeing here.\n", "@delip I've tried doing such stuff before, a few times, including going with official routes, and I've always failed. I got a good grasp on how it should be done next to prevent my previous failures and I while I still think just _starting something unofficial_ **might** work I would bet it won't. Because there is already this official channel that people already think \"it's good enough\".\n\nThis needs to be closed in favor of the better one. That's how it would surely work.\n", "No plans to use these by the Google team at this point. We'll definitely evaluate how our current communication channels are working over time and consider alternatives.\n"]}, {"number": 131, "title": "CUDA 7.0 is hard-coded in `configure` script for Linux", "body": "I am running Ubuntu 15.04 with CUDA 7.5 installed. However, CUDA 7.0 is hard-coded in the `configure` script, and I am unable to configure to use CUDA 7.5:\n\n```\n\u276f\u276f\u276f ./configure\nDo you wish to build TensorFlow with GPU support? [y/n] y\nGPU support will be enabled for TensorFlow\n\nPlease specify the location where CUDA 7.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\nInvalid path to CUDA 7.0 toolkit. /usr/local/cuda/lib64/libcudart.so.7.0 cannot be found\n```\n\nThis is because CUDA 7.5's libraries have the `.7.5` suffix rather than `.7.0`:\n\n```\n\u276f\u276f\u276f ls -l /usr/local/cuda/lib64/libcudart.so*\nlrwxrwxrwx 1 root root     16 Aug 15 08:55 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Aug 15 08:55 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug 15 08:55 /usr/local/cuda/lib64/libcudart.so.7.5.18\n```\n", "comments": ["see https://github.com/tensorflow/tensorflow/issues/54 or https://github.com/tensorflow/tensorflow/issues/20 \n", "Ah, thanks for pointing out those bugs. Feel free to de-dup this issue.\n", "Hi, i had the same problem, and if you take a look about the configure script:\n\nif [ -z \"$CUDA_TOOLKIT_PATH\" ]; then\n    default_cuda_path=/usr/local/cuda\n    read -p \"Please specify the location where CUDA $TF_CUDA_VERSION toolkit is installed. Refer to README.md for more details. [Default is $default_cuda_path]: \" CUDA_TOOLKIT_PATH\n\nSo you have nothingt to do, just find your nvidia toolkit, mine was located in:\n/usr/lib32/nvidia-364\n\nso just:\n\nexport CUDA_TOOLKIT_PATH=/usr/lib32/nvidia-364\n\nAnd you are done\n"]}, {"number": 130, "title": "Use Bazel 0.1.1", "body": "Currently, the [TensorFlow Getting Started](http://tensorflow.org/get_started/os_setup.md#installing_from_sources) doc recommends using Bazel 0.1.0. This is a pretty old release, and a number of bugs, such as bazelbuild/bazel#586, has been fixed since then.\n\nThis is a tracking bug to switch to a newer version of Bazel. I believe we are in the process of cutting a new release. I will ping this bug once we are ready to update the docs to recommend using a newer Bazel release.\n\nRelated: https://github.com/tensorflow/tensorflow/issues/129\n", "comments": [":+1: we use `0.1.1` in the Docker builds, so I can confirm TF will build with it.\n", "Let's go ahead and update the docs to recommend 0.1.1 since this release contains a number of fixes and improvements over 0.1.0 and is known to work for building TF. I'll combine this with #129.\n"]}, {"number": 129, "title": "Update docs to instruct to use Bazel 0.1.1 installer", "body": "I noticed that the [TensorFlow Getting Started](http://tensorflow.org/get_started/os_setup.md#installing_from_sources) doc says to build Bazel 0.1.0 from source. It would be easier for users to use the Bazel installer to install Bazel instead as documented on the [Installing Bazel](http://bazel.io/docs/install.html) docs. The installers are available on the [Bazel releases](https://github.com/bazelbuild/bazel/releases) page.\n\nRelated: https://github.com/tensorflow/tensorflow/issues/130\n", "comments": ["David: should we wait for 0.1.1 before updating the docs, or just do this now?\n", "@vrv Bazel 0.1.1 is already available, though I think we might be cutting 0.1.2 soon. I will check with the team to confirm whether we should wait for a new release or recommend using the 0.1.1 installer now.\n", "@vrv I've checked with the Bazel team. There are some backwards incompatible changes at Bazel HEAD. Since 0.1.1 contains the fix for a number of bugs, such as bazelbuild/bazel#586, let's go ahead and update the TensorFlow docs to tell users to install Bazel 0.1.1 using the Bazel installer.\n", "Okay, updated in https://github.com/tensorflow/tensorflow/commit/72a5a60dd4664a7caa4611344364ac7851464a60  -- thanks!\n"]}, {"number": 128, "title": "Lots of C++ compile-time warnings", "body": "I noticed a large number of compile-time warnings while compiling TensorFlow on Ubuntu 15.04.\n\nI am building without GPU support using Bazel 0.1.1 (to work around bazelbuild/bazel#586) with the following command: `bazel build //tensorflow/cc:tutorials_example_trainer`\n\nMany of the errors look like the following:\n\n```\nINFO: From Compiling tensorflow/cc/ops/nn_grad.cc:                                                                                                                                                                                                                                                                 [142/18386]\nIn file included from ./tensorflow/core/framework/op.h:10:0,\n                 from ./tensorflow/core/framework/function.h:10,\n                 from tensorflow/cc/ops/nn_grad.cc:1:\n./tensorflow/core/lib/strings/str_util.h: In instantiation of 'std::vector<std::basic_string<char> > tensorflow::str_util::Split(tensorflow::StringPiece, char, Predicate) [with Predicate = tensorflow::str_util::AllowEmpty]':\n./tensorflow/core/lib/strings/str_util.h:125:41:   required from here\n./tensorflow/core/lib/strings/str_util.h:133:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     for (int i = 0; i < text.size() + 1; i++) {\n                       ^\n./tensorflow/core/lib/strings/str_util.h:134:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n       if ((i == text.size()) || (text[i] == delim)) {\n              ^\nINFO: From Compiling tensorflow/cc/ops/const_op.cc:\nIn file included from ./tensorflow/core/framework/op.h:10:0,\n                 from ./tensorflow/core/graph/graph_def_builder.h:6,\n                 from ./tensorflow/cc/ops/const_op.h:5,\n                 from tensorflow/cc/ops/const_op.cc:1:\n./tensorflow/core/lib/strings/str_util.h: In instantiation of 'std::vector<std::basic_string<char> > tensorflow::str_util::Split(tensorflow::StringPiece, char, Predicate) [with Predicate = tensorflow::str_util::AllowEmpty]':\n./tensorflow/core/lib/strings/str_util.h:125:41:   required from here\n./tensorflow/core/lib/strings/str_util.h:133:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n     for (int i = 0; i < text.size() + 1; i++) {\n                       ^\n./tensorflow/core/lib/strings/str_util.h:134:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n       if ((i == text.size()) || (text[i] == delim)) {\n              ^\nIn file included from ./tensorflow/core/framework/attr_value_util.h:7:0,\n                 from ./tensorflow/core/framework/node_def_builder.h:5,\n                 from ./tensorflow/core/graph/node_builder.h:5,\n                 from ./tensorflow/core/graph/graph_def_builder.h:8,\n                 from ./tensorflow/cc/ops/const_op.h:5,\n                 from tensorflow/cc/ops/const_op.cc:1:\n./tensorflow/core/public/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = float; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<float, 1ul, 1, lon\ng int>, 1>]':\n./tensorflow/core/public/tensor.h:190:40:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = float; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<float, 1ul, 1, long int>, 1>]'\ntensorflow/cc/ops/const_op.cc:63:1:   required from here\n./tensorflow/core/public/tensor.h:411:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n   for (int d = 0; d < NDIMS; d++) {\n                     ^\n./tensorflow/core/public/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = double; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<double, 1ul, 1, l\nong int>, 1>]':\n./tensorflow/core/public/tensor.h:190:40:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = double; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<double, 1ul, 1, long int>, 1>]'\ntensorflow/cc/ops/const_op.cc:64:1:   required from here\n./tensorflow/core/public/tensor.h:411:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n./tensorflow/core/public/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = int; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<int, 1ul, 1, long in\nt>, 1>]':\n./tensorflow/core/public/tensor.h:190:40:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = int; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<int, 1ul, 1, long int>, 1>]'\ntensorflow/cc/ops/const_op.cc:65:1:   required from here\n./tensorflow/core/public/tensor.h:411:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n./tensorflow/core/public/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = unsigned char; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<unsigned c\nhar, 1ul, 1, long int>, 1>]':\n./tensorflow/core/public/tensor.h:190:40:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = unsigned char; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<unsigned char, 1ul, 1, long int>, 1>]'\ntensorflow/cc/ops/const_op.cc:66:1:   required from here\n./tensorflow/core/public/tensor.h:411:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n./tensorflow/core/public/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = short int; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<short int, 1ul\n, 1, long int>, 1>]':\n./tensorflow/core/public/tensor.h:190:40:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = short int; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<short int, 1ul, 1, long int>, 1>]'\ntensorflow/cc/ops/const_op.cc:67:1:   required from here\n./tensorflow/core/public/tensor.h:411:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n./tensorflow/core/public/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = signed char; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<signed char,\n 1ul, 1, long int>, 1>]':\n./tensorflow/core/public/tensor.h:190:40:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = signed char; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<signed char, 1ul, 1, long int>, 1>]'\ntensorflow/cc/ops/const_op.cc:68:1:   required from here\n./tensorflow/core/public/tensor.h:411:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n./tensorflow/core/public/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = long long int; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<long long\nint, 1ul, 1, long int>, 1>]':\n./tensorflow/core/public/tensor.h:190:40:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = long long int; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<long long int, 1ul, 1, long int>, 1>]'\ntensorflow/cc/ops/const_op.cc:69:1:   required from here\n./tensorflow/core/public/tensor.h:411:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n./tensorflow/core/public/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = bool; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<bool, 1ul, 1, long\nint>, 1>]':\n./tensorflow/core/public/tensor.h:190:40:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = bool; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<bool, 1ul, 1, long int>, 1>]'\ntensorflow/cc/ops/const_op.cc:70:1:   required from here\n./tensorflow/core/public/tensor.h:411:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n./tensorflow/core/public/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = std::complex<float>; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<std:\n:complex<float>, 1ul, 1, long int>, 1>]':\n./tensorflow/core/public/tensor.h:190:40:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = std::complex<float>; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1ul, 1, long int>, 1>]'\ntensorflow/cc/ops/const_op.cc:72:1:   required from here\n./tensorflow/core/public/tensor.h:411:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n./tensorflow/core/public/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = std::basic_string<char>; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<\nstd::basic_string<char>, 1ul, 1, long int>, 1>]':\n./tensorflow/core/public/tensor.h:190:40:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = std::basic_string<char>; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<std::basic_string<char>, 1ul, 1, long int>, 1>]'\ntensorflow/cc/ops/const_op.cc:88:1:   required from here\n./tensorflow/core/public/tensor.h:411:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n```\n", "comments": ["There are unfortunately a lot more when compiling with --config=cuda -- we'll use this bug to track eliminating most of these warnings.\n", "We've eliminated most of these warnings.  signed/unsigned comparisons are always annoying and will likely creep up, but we'll address them individually as needed.\n"]}, {"number": 127, "title": "cannot use bazel to compile tensorflow example codes", "body": "tensorflow was successfully installed on my OSX10.11 system.\nusing \"import tensorflow\" in python donot report any error, and \"hello world\" application can be carried out.\nbut when i want to run the examples(http://tensorflow.org/tutorials/recurrent/index.md) in the tensorflow root directory, \ni typed \"bazel build -c opt tensorflow/models/rnn/ptb:ptb_word_lm\"\nit showed that:\nERROR: /Users/aaa/Develop/tools/tensorflow/tensorflow/python/BUILD:1: Extension file not found: 'google/protobuf/protobuf.bzl'.\nERROR: error loading package 'tensorflow/python': Extension file not found: 'google/protobuf/protobuf.bzl'.\nINFO: Elapsed time: 0.057s\n\ni don't know what is wrong.\nany comment is appreciated.\n", "comments": ["Seems that you might not have cloned the `protobuf` submodule? When you cloned the tensorflow repository, did you add the `--recurse-submodules` flag?\n\nTry running `git submodule update  --init` in the repository then building again.\n\nEdit: typo in git submodule command\n", "Likely the culprit -- reopen if this was not the case!\n", "Thanks, @davidzchen. For anyone Googling and copy pasting, it's\n`git submodule update --init`\n", "Sorry, there was a typo in the git submodule command in my comment. Thanks, @lionleaf!\n"]}, {"number": 126, "title": "cpu version Installed successfully, but cannot import tensorflow in python.", "body": "My env is like this:\n\n```\nRed Hat Enterprise Linux Server release 5.5 (Tikanga)\n\npython 2.7.3\n```\n\nI successfully installed tensorflow, using `pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl`\n\nthen I goes into `python`, then `import tensorflow as trf`, I got this error:\n\n```\n\n$ python\nPython 2.7.3 (default, Nov  1 2013, 09:47:57) \n[GCC 4.1.2 20080704 (Red Hat 4.1.2-48)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/users/jackson/pyenvs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/users/jackson/pyenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 22, in <module>\n    from tensorflow.python.client.client_lib import *\n  File \"/users/jackson/pyenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py\", line 35, in <module>\n    from tensorflow.python.client.session import InteractiveSession\n  File \"/users/jackson/pyenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 11, in <module>\n    from tensorflow.python import pywrap_tensorflow as tf_session\n  File \"/users/jackson/pyenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/users/jackson/pyenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: /users/jackson/pyenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: ELF file OS ABI invalid\n\n\n```\n\nCan I get some help on this pls?\n", "comments": ["Tensorflow pip packages were compiled on Ubuntu machines. Can you try building the pip package from source and installing it?\n", "did pip packages from source, worked. thanks\n", "Yes ty\n\ngeorge\n", "hi guys, can you tell me how I can \" build the pip package from source and install it\" in windows?\n\nSo far I did: \npip2.7.exe install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n\nand got the following error when I run a program with \"import tensorflow\":\nImportError: No module named _pywrap_tensorflow\n\nThanks ahead!\n"]}, {"number": 125, "title": "Can anyone install it with cuda7.5 and cudnn 7.0?", "body": "I installed it from source code. Everything is OK. \n  But when I  run '\"\"\"bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\"\"\" . \n  The errors are   1)Unable to load cuBLAS DSO,2)Unable to load cuDNN DSO,3)Unable to load cuFFT DSO,4)Unable to load cuRAND DSO.\n", "comments": ["see https://github.com/tensorflow/tensorflow/issues/54 or https://github.com/tensorflow/tensorflow/issues/20 \n", "This is vexing...plus looking at the other issues, this does not lend faith to this highly publicized software release...Forcing a dependency version prior to the latest version is just silly. Hope Google fixes this soon.\n", "Yup, we're working on it.\n", "I got it to run. just manually modified all references to 7.5 (7.0 for cudnn).\n\nJust do `grep -r -n libcu *` from inside the tensorflow directory and then open another window to manually adjust the source files. It appears to compile and run the sample training. \n\nMy question to the tensorflow team would be whether the decision to not include R3 was based on incompatibilities in the API  (I had thought that nvidia's cuDNN libraries were meant to be drop-in upgradeable), or the lack of testing with R3.\n", "@zhongliangong, TensorFlow in its current form is extensively tested with Cudnn R2. You are welcome to modify the source and try with R3 locally. The official upgrade will happen when it is ready. \n", "Another (hacky) way to fix this:  on linux, in your cuda directory there are already symbolic links from the *.7.5 named file to the non-suffixed files.  Just create another symbolic link for each of those that is *.7.0 and it works like a charm!\n", "@mtanana  I tried your solution. It seems to be working, but there is some warmings, such as \n**\ntensorflow/stream_executor/cuda/cuda_fft.cc:327] Unable to load cuFFT DSO.\nI tensorflow/stream_executor/dso_loader.cc:77] LD_LIBRARY_PATH: /usr/local/cuda-7.5/lib64:/usr/local/cuda-7.5/lib64:/usr/local/cuda-7.5/lib64:\nI tensorflow/stream_executor/cuda/cuda_rng.cc:317] Unable to load cuRAND DSO.\n**\n\ndoes these warming matters or we can ignore it?\n", "@helxsz \nI think these warning means that it did not work.  (These were the warnings that went away after I made the symbolic links).  Did you make 6ish links in your /usr/local/cuda-7.5/lib64 folder?  For example, what did you name the one for cuRAND?  And what did it link to?\n"]}, {"number": 124, "title": "tools/jdk: BUILD file not found on package path.", "body": "While installing from source, I got this error\n\n`ERROR: Loading of target '//tools/jdk:SingleJar_deploy.jar' failed; build aborted: no such package 'tools/jdk': BUILD file not found on package path.`\n`ERROR: Loading failed; build aborted`\n\nI already installed java8 as per bazel's instruction page. \n", "comments": ["Experiencing the same problem. Havn't found a solution yet.\n", "Having the same issue. Not resolved yet. Please help.\n", "What version of bazel are you using?\n", "Blaze version info: Build label: 0.1.0\nIs this the right version?\n", "Try using 0.1.1 -- our bazel install instructions have been updated here: http://www.tensorflow.org/get_started/os_setup.html#installation-for-linux\n\n(re-open if this is not sufficient -- this is most likely a bazel issue from what I understand)\n", "Hi,\n\nI have this same problem trying to install Tensor Flow from source on a CentOS 6.7 machine. I've tried with both bazel 0.1.1 and 0.1.3. With 0.1.1, I get:\n\n```\n$ pwd\n$HOME/code/tensorflow\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures --ignore_unsupported_sandboxing\nWARNING: Output base '$HOME/.cache/bazel/_bazel_mpost/891d341da8754b366638c3a40c9b09f5' is on NFS. This may lead to surprising failures and undetermined behavior.\nERROR: Loading of target '//tools/jdk:GenClass_deploy.jar' failed; build aborted: no such target '//tools/jdk:GenClass_deploy.jar': target 'GenClass_deploy.jar' not declared in package 'tools/jdk' defined by $HOME/.bazel/base_workspace/tools/jdk/BUILD.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.034s\n```\n\nThe documentation doesn't seem to address this. Do you have any suggestions?\n", "Same issue as mjpost here.\n", "Update: I think this occurred because we did not have a complete build of bazel. We switched to a CentOS 7.1 machine, and everything (both bazel and TF) built fine.\n", "Hello, \n\nI have a similar problem when testing bazel with \"bazel test\": \n\nERROR: /share/sw/free/zyzhang/bazel.Feb04/bazel-0.1.4.Feb012016/tools/defaults/BUILD:13:1: no such target '@bazel_tools//tools/jdk:GenClass_deploy.jar': target 'GenClass_deploy.jar' not declared in package 'tools/jdk' defined by /home/zyzhang/.cache/bazel/_bazel_zyzhang/3ac78c7468df3709b4ef704638238acf/external/bazel_tools/tools/jdk/BUILD and referenced by '//tools/defaults:genclass'.\n\nTo begin with, how do I make sure the bazel build was \"complete\", as mjpost encountered? What could I be missing in the bazel build? The bazel build was successful and I am doing \"bazel test\" with the bazel built from \"bazel build\" \n"]}, {"number": 123, "title": "Cant install, Mac - El Capitan - not a supported wheel", "body": "I try using this command in terminal \n$ pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\nbut i get \"tensorflow-0.5.0-py2-none-any.whl is not a supported wheel on this platform.\"\n\nI dont know what I did wrong, i am pretty sure pip is up to date, and I just downloaded homebrew for this...\n\nI am using python with the anaconda package if that effects anything?\n\nI have no idea what I am doing here, help please.\n", "comments": ["I had the same error and I fixed it. Please output \"pip --version\" and print it here.\n\nIf it's similar to \"pip 7.1.2 from /usr/local/lib/python3.4/site-packages (python 3.4)\" then you are using pip3, run:\n\nsudo easy_install pip\n\nIf after you install pip using that command you get a traceback error involving something called six, run:\n\nsudo easy_install -U six\n\nIf after you install that (or pip if you didn't get a \"six error\") and you get a traceback error involving numpy run:\n\nsudo easy_install -U numpy\n\nIf all the requirements are met but you get an error saying at the end:\n\nOSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/SOMEPACKAGE'\n\nthen simply rerun the pip install command with sudo.\n\nThose are all the problems and solutions that I ran across.\n", "whatever you managed to do doesnt seem to want to work for me\n\nthe result of running \"pip --version\" was \n\"pip 7.1.2 from /Users/act65/anaconda/lib/python3.4/site-packages (python 3.4)\"\n\ni dont get any errors when reinstalling pip\nrunning \n\"$ pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\"\ni get the same error, not a supported wheel...\ni tried running sudo pip ... and got\n\"The directory '/Users/act65/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\"\n\nI have tried a few different things but now have run out of ideas, I tried using docker instead and I think it worked, but how do i get the docker environment over into anaconda and thus sypder, my IDE???\n\nI also tried using the virtualenv thing, but that doesnt work either - \ni run \n\"$ virtualenv --system-site-packages ~/tensorflow)\nand get\n\"ERROR: The executable /Users/act65/tensorflow/bin/python3 is not functioning\nERROR: It thinks sys.prefix is '/Users/act65' (should be '/Users/act65/tensorflow')\nERROR: virtualenv is not compatible with this system or executable\"\n", "pip --version ought output something similar to \"pip 7.1.2 from /Library/Python/2.7/site-packages/pip-7.1.2-py2.7.egg (python 2.7)\". \n\notherwise it will not download. easy_install will provide you with the correct version of pip to use in this case.\n", "ok, so that could/would mean that the problem is with using anaconda?\n\nalso, pip is in the anaconda directory so easy_install doesnt really do anything. it just says, pip is already installed in /Users/act65/anaconda/bin.\n\nshould i just start from scratch with by downloading a seperate version of python?\n", "ok, i tried something else and I might be getting somewhere? i downloaded the source files and tried just running this\n\n$ pip install tensorflow\n\nwhich returned\n\n\"Requirement already satisfied (use --upgrade to upgrade): tensorflow in ./anaconda/lib/python3.4/site-packages\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.9.2 in ./anaconda/lib/python3.4/site-packages (from tensorflow)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in ./anaconda/lib/python3.4/site-packages/six-1.10.0-py3.4.egg (from tensorflow)\"\n\nand i am a little confused as to what it is trying to tell me\n", "You should be able download python 2.7 concurrently with python 3.4.\n\nLook at the requirements on tensorflow.org. Python 3.4 is not supported.\n", "ah...\nmy bad\nthanks (it worked...)\n", "Thanks @yyttr3 You help me put\n", "Hey, I had the same issue when trying to install Tensorflow in the Anaconda distribution of Python. I could fix it by installing tensorflow via this site: https://conda.anaconda.org/jjhelmus\n\nI found this in this thread: https://github.com/tensorflow/tensorflow/issues/1097\n\nCheers and keep on deep learning :)\n"]}, {"number": 122, "title": "Windows Installation?", "body": "Is it possible to install TF in windows environment?\nI checked \"pip install\" is not supported in windows. \n\nAny plan for it?\n", "comments": ["I have the same question.\n", "Please refer #17.\n", "Closing this as a duplicate of #17 - please feel free to continue the discussion there.\n", "C:\\Users\\Desktop>docker run -it b.gcr.io/tensorflow/tensorflow\n\nUnable to find image 'b.gcr.io/tensorflow/tensorflow:latest' locally\n\ndocker: Error response from daemon: unable to ping registry endpoint https://b.g\ncr.io/v0/\nv2 ping attempt failed with error: Get https://b.gcr.io/v2/: dial tcp 64.233.188\n.82:443: i/o timeout\n"]}, {"number": 121, "title": "ImportError: undefined symbol: clock_gettime", "body": "I ran these successfully:\n\n```\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n$ pip install --user /tmp/tensorflow_pkg/tensorflow-0.5.0-py2-none-any.whl\n```\n\nBut the `convolutional.py` Python script gives an error:\n\n```\n$ python2.7 tensorflow/models/image/mnist/convolutional.py\nTraceback (most recent call last):\n  File \"tensorflow/models/image/mnist/convolutional.py\", line 13, in <module>\n    import tensorflow.python.platform\n  File \"/home/name/.local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/home/name/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 22, in <module>\n    from tensorflow.python.client.client_lib import *\n  File \"/home/name/.local/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py\", line 35, in <module>\n    from tensorflow.python.client.session import InteractiveSession\n  File \"/home/name/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 11, in <module>\n    from tensorflow.python import pywrap_tensorflow as tf_session\n  File \"/home/name/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 26, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/name/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 22, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: /home/name/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: \nundefined symbol: clock_gettime\n```\n", "comments": ["Which OS version and glibc version are you using? Based the answers to [a similar stackoverflow question](http://stackoverflow.com/questions/2418157/ubuntu-linux-c-error-undefined-reference-to-clock-gettime-and-clock-settim), my guess is that you have a glibc version earlier than 2.17, which is needed for the binary distribution to work.\n\nThe easiest workaround may be to install a Docker container containing TensorFlow, by following [the instructions here](http://tensorflow.org/get_started/os_setup.md#docker-based_installation).\n", "Docker seems like a good option, though I don't have root/sudo access on my current machine, so it's harder to install.\n\nI did just solve the import error `undefined symbol: clock_gettime` error, by adding -lrt to the build like this:\n\n```\n--- a/tensorflow/tensorflow.bzl\n+++ b/tensorflow/tensorflow.bzl\n@@ -284,7 +284,7 @@ _py_wrap_cc = rule(attrs={\n\n def tf_extension_linkopts():\n-  return []  # No extension link opts\n+  return [\"-lrt\"]\n```\n", "Thanks for checking that that works!\n", "might be a bit off topic but I'm getting the same error on another openai package:\r\n\r\n```\r\n>>> import fastzbarlight\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib64/python3.4/site-packages/fastzbarlight/__init__.py\", line 7, in <module>\r\n    from ._zbarlight import zbar_code_scanner\r\nImportError: /usr/lib64/python3.4/site-packages/fastzbarlight/_zbarlight.cpython-34m.so: undefined symbol: clock_gettime\r\n```\r\n"]}, {"number": 120, "title": "TensorFlow session.run() overhead for graphs with few flops", "body": "The following code will take 10 seconds to run:\n\n``` python\nwith tf.Session():\n    for _ in range(1000):\n        tf.constant(0).eval()\n```\n\nNow, this might not be an issue for large-scale machine learning, but it makes running the graph in real-time very hard. I was trying to port a forward simulation model from Theano to TensorFlow, where 1000 runs of forward simulation took 0.2s in Theano vs. 17s in TensorFlow, and over half of the time was taken by this `session.run()` overhead.\n", "comments": ["There's a subtle issue that crops up when performing these microbenchmarks on TensorFlow. I ran a version of your code on my laptop (a late-2014 MacBook Air):\n\n``` python\n>>> with tf.Graph().as_default():\n...   with tf.Session():\n...     start = time.time()\n...     for _ in xrange(1000):\n...       _ = tf.constant(0).eval()\n...     end = time.time()\n>>> print end - start\n4.721920967102051\n```\n\nThe perhaps surprising thing to note about this code is that it is actually the `tf.constant(0)` method that is the expensive part. If I run this slightly different program, it completes much faster:\n\n``` python\n>>> with tf.Graph().as_default():\n...   c = tf.constant(0)\n...   with tf.Session():\n...     start = time.time()\n...     for _ in xrange(1000):\n...       _ = c.eval()\n...     end = time.time()\n>>> print end-start\n0.0893728733063\n```\n\nIn the first version (like in your snippet), I created 1000 identical constant nodes in the graph, and evaluated each of them: this took several seconds. In the second version, I created the constant node once, and evaluated it 1000 times: this took less than 100 milliseconds.\n\nIn summary, it's important to try to reuse the existing graph as much as possible for each step of your model. Adding nodes to the graph isn't free, and we could probably optimize that further. Feel free to share more of your simulation code, and we can look into any major performance issues that it might have.\n", "Thanks - I guess this means that the overhead is somewhere else then. I'll post more information once I can narrow down the cause of the slowdown.\n", "@mrry \n\nI've put up some simple benchmark here:\n\nhttps://github.com/dementrock/tensorfuse#running-benchmark\n\nThis is a small library that bridges the API between Theano and TensorFlow, and another library called CGT. I just wrote 3 simple tests, and both Theano and CGT beat TensorFlow on these:\n\n```\nUsing Theano for TensorFuse\nfunc:'time_sin' 10000 times took: 0.1418 sec\nfunc:'time_matmul' 10000 times took: 0.0786 sec\nfunc:'time_slicing' 10000 times took: 0.1508 sec\nUsing CGT for TensorFuse\nfunc:'time_sin' 10000 times took: 0.0516 sec\nfunc:'time_matmul' 10000 times took: 0.0619 sec\nfunc:'time_slicing' 10000 times took: 0.0529 sec\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8\nUsing TensorFlow for TensorFuse\nfunc:'time_sin' 10000 times took: 0.8568 sec\nfunc:'time_matmul' 10000 times took: 1.1223 sec\nfunc:'time_slicing' 10000 times took: 1.1388 sec\n```\n\nIt might be the way I'm bridging TensorFlow to `theano.function`, since I'm pretty new to TensorFlow. The implementation is at \nhttps://github.com/dementrock/tensorfuse/blob/master/tensorfuse/__init__.py#L33. I'd really appreciate it if you could take a look. Thanks!\n", "Your benchmarks seem to align with what I can measure on my laptop. For example:\n\n``` python\n>>> with tf.Graph().as_default():\n...   x = tf.constant(np.random.rand(3, 3).astype(np.float32))\n...   y = tf.constant(np.random.rand(3, 3).astype(np.float32))\n...   z = tf.matmul(x, y)\n...   with tf.Session():\n...     start = time.time()\n...     for _ in xrange(10000):\n...       _ = z.eval()\n...     end = time.time()\n>>> end - start\n1.6025779247283936\n```\n\nI should add that the operations in your benchmark are very small - computable in a time that is on the order of microseconds or nanoseconds - compared to the size of computation for which TensorFlow is designed. While it would be nice to reduce any unnecessary overhead from step and op dispatch, it is unlikely that doing so would dramatically reduce the time taken to run an inference or training step in a realistic neural network. With that said, thanks for looking into this, and if you have any suggestions for how to reduce this overhead, we would be glad to hear them!\n", "Is the size defined in terms of the dimension of the data, or the complexity of the computation?\n", "I'd define it in terms of the number of floating-point operations needed to compute the result of the step. A 3 x 3 matrix multiplication (to take the `time_matmul` benchmark as an example) uses very few floating-point operations compared to the constant framework overhead. It would be quicker to do the computation than dispatch it to another framework. Similarly, it would almost certainly not be worth offloading that computation to a GPU, because of the overheads in dispatching and fetching the results of a kernel. By contrast, larger matrix multiplications an convolutions have a high flop count, and will tend to benefit from this approach.\n\nIt would still be very informative to learn how the overhead changes with the size of the data. Would you consider adding different sizes of input for each of the workloads?\n", "The overhead does diminish as the size of the matrix increases. Some results below (all tests including the ones before were on CPU):\n\n```\nUsing Theano for TensorFuse\nfunc:'time_matmul_3x3' 10000 times took: 0.0804 sec\nfunc:'time_matmul_64x64' 10000 times took: 0.2886 sec\nfunc:'time_matmul_256x256' 1000 times took: 0.3078 sec\nfunc:'time_matmul_512x512' 1000 times took: 2.0757 sec\nfunc:'time_matmul_1024x1024' 100 times took: 1.6689 sec\nfunc:'time_matmul_2048x2048' 10 times took: 1.3290 sec\nfunc:'time_matmul_4096x4096' 10 times took: 10.4791 sec\nfunc:'time_matmul_8192x8192' 10 times took: 82.8672 sec\nUsing CGT for TensorFuse\nfunc:'time_matmul_3x3' 10000 times took: 0.0608 sec\nfunc:'time_matmul_64x64' 10000 times took: 0.2964 sec\nfunc:'time_matmul_256x256' 1000 times took: 0.3348 sec\nfunc:'time_matmul_512x512' 1000 times took: 2.4141 sec\nfunc:'time_matmul_1024x1024' 100 times took: 1.9360 sec\nfunc:'time_matmul_2048x2048' 10 times took: 1.4082 sec\nfunc:'time_matmul_4096x4096' 10 times took: 10.7015 sec\nfunc:'time_matmul_8192x8192' 10 times took: 83.5396 sec\nUsing TensorFlow for TensorFuse\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8\nfunc:'time_matmul_3x3' 10000 times took: 1.2752 sec\nfunc:'time_matmul_64x64' 10000 times took: 2.5342 sec\nfunc:'time_matmul_256x256' 1000 times took: 0.8030 sec\nfunc:'time_matmul_512x512' 1000 times took: 4.4840 sec\nfunc:'time_matmul_1024x1024' 100 times took: 2.1287 sec\nfunc:'time_matmul_2048x2048' 10 times took: 1.5108 sec\nfunc:'time_matmul_4096x4096' 10 times took: 11.0894 sec\nfunc:'time_matmul_8192x8192' 10 times took: 84.9977 sec\n```\n\nThe overhead is unnoticeable when size >= 2048.\n\nHowever, I suspect there's some overhead with each operation added to the graph, even if they are all executed in a single `session.run()`, which might be the reason why tensorflow is so much slower for the forward physics simulation stuff I was doing.\n", "Closing due to inactivity -- it would be great to get the overhead lower for compute-light graphs, but it's probably not a major focus.\n", "I ran Derek's experiment again:\r\n```\r\nwith tf.Graph().as_default():\r\n  c = tf.constant(0)\r\n  with tf.Session():\r\n    start = time()\r\n    for _ in xrange(1000):\r\n      _ = c.eval()\r\n    end = time()\r\nprint(end-start)\r\n```\r\n`>>> 0.215318918228`\r\n\r\nThat's around 5000 steps per second. His results were 2.5x faster back in 2015. Seems that the graph execution overhead is immense!", "@TimZaman Just speculating, but I think that a large fraction of the cost might come from the first call to `c.eval()`, which performs various one-time startup activities (and has generally grown in responsibility since 2015). I'd hope that the subsequent steps are faster than 200us per call. You might be interested in looking here to see some of the ways to reduce the overhead of invoking a graph:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/session_benchmark.py\r\n\r\nHowever, the recent engineering focus has been on making single-op execution fast in eager mode, and using TensorFlow functions as a replacement for `sess.run()`.", "Ran some tests.. just a small improvement:\r\n## {cold, warm, warm} start:\r\n```\r\nwith tf.Graph().as_default():\r\n  c = tf.constant(0)\r\n  with tf.Session():\r\n    for e in range(3):\r\n      start = time()\r\n      for _ in xrange(1000):\r\n        _ = c.eval()\r\n      end = time()\r\n      print(end-start)\r\n```\r\n```\r\n>>> 0.240370988846\r\n>>> 0.179282188416\r\n>>> 0.208700180054\r\n```\r\n\r\n## evaluating just the .op\r\nmakes it twice as fast (still meh).\r\n```\r\nwith tf.Graph().as_default():\r\n  c = tf.constant(0)\r\n  with tf.Session() as sess:\r\n    for e in range(3):\r\n      start = time()\r\n      for _ in xrange(1000):\r\n        sess.run(c.op)\r\n      end = time()\r\n      print(end-start)\r\n```\r\n```\r\n>>> 0.167293071747\r\n>>> 0.106302022934\r\n>>> 0.102854967117\r\n```\r\n\r\n## eager\r\nAround 40k per second. The same code in torch is 300k/s, numpy 800k/s, python(int) 17M/s.\r\n```\r\ntf.enable_eager_execution()\r\nc = tf.constant(0)\r\nc = tf.convert_to_tensor(c)\r\nfor e in range(3):\r\n    start = time()\r\n    for _ in xrange(1000):\r\n        c += 1\r\n    end = time()\r\n    print(end-start)\r\n```\r\n```\r\nc= tf.Tensor(1000, shape=(), dtype=int32)\r\n0.0253150463104\r\nc= tf.Tensor(2000, shape=(), dtype=int32)\r\n0.0205891132355\r\nc= tf.Tensor(3000, shape=(), dtype=int32)\r\n0.0187749862671\r\n```\r\n", "CC @asimshankar for the Eager numbers, to see if there's any low-hanging fruit there.", "In graph mode I think the constant is being placed on GPU (which you probably have given where you work :P) and so maybe you are partially timing GPU->CPU copy time.  Things get a little faster (2x on my machine) if you don't use GPU, FWIW.   Still not where it needs to be though...", "Nope GPUs masked out with cuda visible devices.\n\nOn Mon, Nov 5, 2018, 18:42 Vijay Vasudevan <notifications@github.com wrote:\n\n> In graph mode I think the constant is being placed on GPU (which you\n> probably have given where you work :P) and so maybe you are partially\n> timing GPU->CPU copy time. Things get a little faster (2x on my machine) if\n> you don't use GPU, FWIW. Still not where it needs to be though...\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/120#issuecomment-436111678>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHXSRGElQVxfe7IbSDFvkQnz6tgIyDVpks5usPczgaJpZM4GgAfn>\n> .\n>\n", "RE: eager - There are two annoying things right now - the operator overloading for `+=` and the conversion from the Python `1` to a `Tensor`. Those need to be improved (CC @akshaym), but in the mean time these trivial changes make a big improvement (number from my machine running 1.12.0):\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom time import time\r\n\r\ntf.enable_eager_execution()\r\nc = tf.constant(0)\r\no = tf.constant(1)\r\nfor e in range(3):\r\n    start = time()\r\n    for _ in xrange(1000):\r\n        c = tf.add(c, o)\r\n    end = time()\r\n    print(end-start)\r\n```\r\n\r\nResults in:\r\n\r\n```\r\n0.00489807128906\r\n0.00518703460693\r\n0.00459504127502\r\n```\r\n\r\nAnd just for thrills, compiling into a graph (using [`tf.contrib.eager.defun`](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun) right now, which will morph into [`tf.function` in 2.0](https://github.com/tensorflow/community/pull/20)):\r\n\r\n```python\r\n@tf.contrib.eager.defun\r\ndef f(c):\r\n  o = tf.constant(1)\r\n  for _ in xrange(1000):\r\n    c = tf.add(c, 0)\r\n  return c\r\n\r\nc = tf.constant(0)\r\n# Discard the first run, since that includes graph building time\r\n_ = f(c)\r\nfor e in range(3):\r\n  start = time()\r\n  c = f(c)\r\n  end = time()\r\n  print(end - start)\r\n```\r\n\r\nResults in:\r\n\r\n```\r\n0.00160002708435\r\n0.00185298919678\r\n0.00164079666138\r\n```", "I should add that using the graph function, the overheads of the operator overload and the Python->Tensor conversion are paid for only at graph construction time. So this should work too:\r\n\r\n```python\r\n@tf.contrib.eager.defun\r\ndef f(c):\r\n  for _ in xrange(1000):\r\n    c += 1\r\n  return c\r\n\r\nc = tf.constant(0)\r\n# Discard the first run, since that includes graph building time\r\n_ = f(c)\r\nfor e in range(3):\r\n  start = time()\r\n  c = f(c)\r\n  end = time()\r\n  print(end - start)\r\n```", "Nice! Nearing pytorch territory! Indeed nasty issues. I wonder how to\naddress those.\n\nOn Mon, Nov 5, 2018, 19:40 Asim Shankar <notifications@github.com wrote:\n\n> RE: eager - There are two annoying things right now - the operator\n> overloading for += and the conversion from the Python 1 to a Tensor.\n> Those need to be improved (CC @akshaym <https://github.com/akshaym>), but\n> in the mean time these trivial changes make a big improvement (number from\n> my machine running 1.12.0):\n>\n> import tensorflow as tffrom time import time\n>\n> tf.enable_eager_execution()\n> c = tf.constant(0)\n> o = tf.constant(1)for e in range(3):\n>     start = time()\n>     for _ in xrange(1000):\n>         c = tf.add(c, o)\n>     end = time()\n>     print(end-start)\n>\n> Results in:\n>\n> 0.00489807128906\n> 0.00518703460693\n> 0.00459504127502\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/120#issuecomment-436120637>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHXSRPIC0HBYiUWLvnnB--4sku6B8Oftks5usQS8gaJpZM4GgAfn>\n> .\n>\n", "We also ran into this issue, significant call overhead compared to Theano (~10x slower for small graphs, even with defun). Could we maybe re-open this issue to give it more significance?", "@twiecki - re:defun, would you mind filling a new issue specifically for that, along with details of what exactly was being measured?", "@asimshankar https://github.com/tensorflow/tensorflow/issues/24684"]}, {"number": 119, "title": "Bazel can't build protobuf", "body": "The errors of  `bazel build -c opt //tensorflow/tools/pip_package:build_pip_package` are always some header files of protobuf are not found. Related to \nhttps://github.com/google/protobuf/issues/925. I was wondering why nobody reported this yesterday.\n\n```\nINFO: Found 1 target...\nINFO: From Compiling google/protobuf/src/google/protobuf/compiler/code_generator.cc [for host]:\ngoogle/protobuf/src/google/protobuf/compiler/code_generator.cc:39:43: fatal error: google/protobuf/stubs/strutil.h: No such file or directory\n #include <google/protobuf/stubs/strutil.h>\n                                           ^\ncompilation terminated.\nERROR: /home/user/Codes/DeepLearning/tensorflow/google/protobuf/BUILD:166:1: C++ compilation of rule '//google/protobuf:protoc_lib' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 32 argument(s) skipped).\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 1.205s, Critical Path: 0.88s\n```\n", "comments": ["`git checkout tags/0.1.0`\n", "And also `sudo pip install wheel` which is not documented.\n"]}, {"number": 118, "title": "\"help wanted\" I cannot get the TensorBoard working, I am following the given tutorial", "body": "*_I am having trouble writing the log file to be able to visualize it in TensorBoard. Can somebody point me in the right direction? I have followed the code given in the tutorial. When run I get the error: *_ \n-  \"expected %s got %s.\" % (cls.**name**, type(msg).**name**))\n  TypeError: Parameter to MergeFrom() must be instance of same class: expected GraphDef got Graph. for field Event.graph_def*\n\nThe error is said to be on line:\n\n``` python\nsummary_writer = tf.train.SummaryWriter('/Users/Cristian/Desktop/log', sess.graph)\n```\n\n**My code:**\n\n``` Python\nimport input_data\nimport tensorflow as tf\n\n\nprint \"Start program\"\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\nprint \"Done loading images\"\n\nx = tf.placeholder(\"float\", [None, 784])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\n\ny_ = tf.placeholder(\"float\", [None,10])\n\nprint \"Done init\"\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n\n\nprint \"Done loading vars\"\n\ninit = tf.initialize_all_variables()\nprint \"Done init vars\"\n\nsess = tf.Session()\nsess.run(init)\n\nprint '------'\nprint type(sess.graph_def)\n\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\nacc = tf.scalar_summary(\"Accuracy:\", accuracy)\n\nmerged_summary_op = tf.merge_all_summaries()\nsummary_writer = tf.train.SummaryWriter('/Users/Cristian/Desktop/log', sess.graph)\n\nprint \"Session started\"\n\n\nfor i in range(2000):\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n  if i % 100 == 0:\n    print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n    summary_str = sess.run(merged_summary_op)\n    summary_writer.add_summary(summary_str, i)\n\n\nprint \"Done training\"\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n\nprint sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n\n```\n", "comments": ["sess.graph is a python tf.Graph object, whereas SummaryWriter takes a tf.GraphDef (proto).  I think you can get further if you use sess.graph_def rather than sess.graph.  Let us know if that helps!\n", "Also, consider using StackOverflow for these kinds of help questions -- you'll probably get better community support there.\n\n(See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/index.md#help--support--how-do-i- )\n"]}, {"number": 117, "title": "TensorBoard logdir path, if relative, is relative to $HOME", "body": "It took me a while to figure this out and I'm not entirely sure that my interpretation is correct, but even if I'm not in $HOME, running tensorboard --logdir=. will give a list of directories in my home, and passing an absolute path appears to pick things up correctly. It is somewhat counterintuitive to me that relative paths are not relative to the current directory, so maybe that's something to document, disallow, or change?\n\nThanks, this library is amazing!\n", "comments": ["Thanks for reporting this, I'm looking into it.\n", "@danmane Filed ~~[PR-337](https://github.com/tensorflow/tensorflow/pull/337)~~ [#1161](https://tensorflow-review.googlesource.com/#/c/1161/) for this, should be a simple fix.  I don't have access to a Windows machine to run the test suite at the moment, but I doubt it'll break anything.\n", "Thanks @vrv for reviewing, we should be able to close this out now.\n", "Indeed, should be fixed by https://github.com/tensorflow/tensorflow/pull/337\n\nThanks for the fix!\n"]}, {"number": 116, "title": "When will you have a version of TensorFlow for Win10/8/7?", "body": "When will that happen?  Should I do it myself?\n", "comments": ["De-duping with https://github.com/tensorflow/tensorflow/issues/17\n", "What about Win10 to support a Maxell or earlier CUDA GPUs, OpenCL, and Close to Metal for ATI?\n"]}]