[{"number": 36621, "title": "Expected int for argument 'ksizes' not tf.Tensor", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.7\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\ntf.image.extract_patches only accepts static integers as ksize arguments in data.Dataset API pipeline.\r\n\r\n**Describe the expected behavior**\r\nAccept variable data dimension as size argument\r\n\r\n**Code to reproduce the issue**\r\nI'm trying to create sliding window feature across all rows in tensor simultaneously. Each tensor can have different number of rows.\r\n```\r\nimage=tf.constant([[[[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]],\r\n  [[11], [12], [13], [14], [15], [16], [17], [18], [19], [20]],\r\n  [[21], [22], [23], [24], [25], [26], [27], [28], [29], [30]],\r\n  [[31], [32], [33], [34], [35], [36], [37], [38], [39], [40]],\r\n  [[41], [42], [43], [44], [45], [46], [47], [48], [49], [50]],\r\n  [[51], [52], [53], [54], [55], [56], [57], [58], [59], [60]],\r\n  [[61], [62], [63], [64], [65], [66], [67], [68], [69], [70]],\r\n  [[71], [72], [73], [74], [75], [76], [77], [78], [79], [80]],\r\n  [[81], [82], [83], [84], [85], [86], [87], [88], [89], [90]],\r\n  [[91], [92], [93], [94], [95], [96], [97], [98], [99], [100]]]])\r\n\r\nds=tf.data.Dataset.from_tensor_slices(image)\r\n\r\ndef create_patches(image):\r\n    return tf.image.extract_patches(image[None,...],\r\n                           [1, tf.shape(image)[1], 3, 1],\r\n                           [1, 1, 1, 1],\r\n                           [1, 1, 1, 1],\r\n                           padding='VALID')\r\nds=ds.map(create_patches)\r\n\r\nfor i in ds:\r\n    print(i)\r\n```\r\n\r\n**Other info / logs**\r\n`TypeError: Expected int for argument 'ksizes' not <tf.Tensor 'strided_slice_1:0' shape=() dtype=int32>.`\r\n", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/ff6fff58e8904211c6c218e121bf100e/36621.ipynb). Thanks!", "@mindis The method `tf.image.extract_patches` was expecting an int where as you are passing a tensor. When I replaced `tf.shape(image)[1]` with `image.shape[1]`, everything worked as expected. Please check the [gist here]( https://colab.sandbox.google.com/gist/jvishnuvardhan/4a7da2707c133cfc96a9cbdc77392f73/36621.ipynb). Thanks!", "thanks @jvishnuvardhan but unfortunately in my use case, data shape is dynamic and I can only get shape with tf.shape(), but then resulting Tensor is not accepted by  tf.image.extract_patches. \r\n\r\nsee example with dynamic shape\r\n\r\n```\r\nds=tf.data.Dataset.from_tensor_slices([1,2,3])\r\ndef generate_random_shape_data(t):\r\n    random_dim=tf.random.uniform([1],minval=2,maxval=10,dtype=tf.int32)[0]\r\n    values=tf.range(count)\r\n    index=tf.stack([val,val], axis=1)\r\n    t=tf.scatter_nd(index,values,[random_dim,random_dim])\r\n    t=tf.reshape(t,[1,random_dim,random_dim,1])\r\n    return t\r\n\r\ndef create_patches(t):\r\n    return tf.image.extract_patches(t,\r\n                           [1, t.shape[1], 3, 1],\r\n                           [1, 1, 1, 1],\r\n                           [1, 1, 1, 1],\r\n                           padding='VALID')\r\nds=ds.map(generate_random_shape_data)\r\nds=ds.map(create_patches)\r\n\r\n\r\nfor i in ds:\r\n    print(i)\r\n```\r\n\r\n`TypeError: Expected int for argument 'ksizes' not None.`", "@mindis The following code works for me with tf.shape(). Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/8138234984669dfe28f92e760633e8c9/36621.ipynb).\r\n```\r\nimage=tf.constant([[[[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]],\r\n  [[11], [12], [13], [14], [15], [16], [17], [18], [19], [20]],\r\n  [[21], [22], [23], [24], [25], [26], [27], [28], [29], [30]],\r\n  [[31], [32], [33], [34], [35], [36], [37], [38], [39], [40]],\r\n  [[41], [42], [43], [44], [45], [46], [47], [48], [49], [50]],\r\n  [[51], [52], [53], [54], [55], [56], [57], [58], [59], [60]],\r\n  [[61], [62], [63], [64], [65], [66], [67], [68], [69], [70]],\r\n  [[71], [72], [73], [74], [75], [76], [77], [78], [79], [80]],\r\n  [[81], [82], [83], [84], [85], [86], [87], [88], [89], [90]],\r\n  [[91], [92], [93], [94], [95], [96], [97], [98], [99], [100]]]])\r\n\r\nds=tf.data.Dataset.from_tensor_slices(image)\r\nimage_shape =tf.shape(image)[1].numpy()\r\n  \r\ndef create_patches(image):\r\n  return tf.image.extract_patches(image[None,...],\r\n                           [1, image_shape, 3, 1],\r\n                           [1, 1, 1, 1],\r\n                           [1, 1, 1, 1],\r\n                           padding='VALID')\r\nds=ds.map(create_patches)\r\n\r\nfor i in ds:\r\n    print(i)\r\n```", "Hi @jvishnuvardhan, thanks for looking into this!\r\n\r\nUnfortunately `.numpy()` doesn't work with **dynamic** shapes. See error below. \r\n\r\n```\r\ncount=3\r\nds=tf.data.Dataset.from_tensor_slices([1,2,3])\r\ndef generate_random_shape_data(t):\r\n    random_dim=tf.random.uniform([1],minval=2,maxval=10,dtype=tf.int32)[0]\r\n    values=tf.range(count)\r\n    index=tf.stack([values,values], axis=1)\r\n    t=tf.scatter_nd(index,values,[random_dim,random_dim])\r\n    t=tf.reshape(t,[1,random_dim,random_dim,1])\r\n    return t\r\n\r\ndef create_patches(t):\r\n    return tf.image.extract_patches(t,\r\n                           [1, t.shape[1].numpy(), 3, 1],\r\n                           [1, 1, 1, 1],\r\n                           [1, 1, 1, 1],\r\n                           padding='VALID')\r\nds=ds.map(generate_random_shape_data)\r\nds=ds.map(create_patches)\r\n\r\n\r\nfor i in ds:\r\n    print(i)\r\n```\r\n\r\nError:\r\n` AttributeError: 'NoneType' object has no attribute 'numpy'`\r\n\r\n", "@jvishnuvardhan it seems to me that you have no idea what are TF2 eager mode and graph mode. in tf.data everything runs in graph mode (unless you use py function i guess)", "I have the same question. How to deal with the dynamic kernel size?", "Same question here about the dynamic size.", "I was facing the same issue. Fortunately i was able to manage as shown here: <https://stackoverflow.com/questions/63207282/how-to-access-tensor-shape-inside-map-function/63211231#63211231>, thanks to @ethanyanjiali and @jvishnuvardhan. @mindis, let me know if this resolved your problem.", "@mindis Is this still an issue for you. Please close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36621\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36621\">No</a>\n", "Facing similar issue in tf 2.4 with tf data and map functions.", "> Facing similar issue in tf 2.4 with tf data and map functions.\r\n\r\n@anilsathyan7,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "I think this is not an issue and rather a limitation, the parameters for the operation are static. The real problem is not tf.data but the extract_image_patches operation that does not support a tensor input neither for ksizes nor strides parameters when running in graph mode.\r\n\r\nNote that the inner code (gen_array_ops) checks that these parameters are of type list or tuple:\r\n\r\n```python\r\n  # Add nodes to the TensorFlow graph.\r\n  if not isinstance(ksizes, (list, tuple)):\r\n    raise TypeError(\r\n        \"Expected list for 'ksizes' argument to \"\r\n        \"'extract_volume_patches' Op, not %r.\" % ksizes)\r\n  ksizes = [_execute.make_int(_i, \"ksizes\") for _i in ksizes]\r\n  if not isinstance(strides, (list, tuple)):\r\n    raise TypeError(\r\n        \"Expected list for 'strides' argument to \"\r\n        \"'extract_volume_patches' Op, not %r.\" % strides)\r\n```\r\n"]}, {"number": 36620, "title": "GPU performance of TensorflowLite in C++", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): b44fb933866191b94bed36a80e2b1cfd94589d6d\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): bazel 2.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm running TFLite using C++ api through JNI wrapper on Android. Two libraries are built: `libtensorflowlite.so` and `libtensorflowlite_gpu_gl.so` for GPU delegate. It's expected that GPU inference should be faster than CPU inference, as mentioned in https://www.tensorflow.org/lite/performance/benchmarks\r\n\r\n**Describe the expected behavior**\r\nWe found that GPU delegate has the same performance as CPU, even though invoke() returns `kTfLiteOk`\r\n```\r\ninterpreter->Invoke() != kTfLiteOk\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nvoid ImageAPI::loadModel(const std::string &model_path) {\r\n\r\n    LOGD(\"initializing Image\\n\");\r\n\r\n    model = tflite::FlatBufferModel::BuildFromFile(model_path.c_str());\r\n    if(!model){\r\n        LOGD(\"Failed loading model\\n\");\r\n        printf(\"Failed to mmap model\\n\");\r\n        exit(0);\r\n    }\r\n\r\n    LOGD(\"DONE Image\\n\");\r\n\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    tflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\n}\r\n\r\nvoid ImageAPI::runImage(const unsigned char *rgb, unsigned char *Image) {\r\n\r\n    if (use_gpu) {\r\n        if (!delegate) {\r\n            const TfLiteGpuDelegateOptions options = {\r\n                    .metadata = NULL,\r\n                    .compile_options = {\r\n                            .precision_loss_allowed = 1,  // FP16\r\n                            .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST\r\n                    },\r\n            };\r\n\r\n            delegate = TfLiteGpuDelegateCreate(&options);\r\n            if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {\r\n                exit(0);\r\n            }\r\n        }\r\n    }\r\n\r\n    interpreter->AllocateTensors();\r\n\r\n    float *image_640 = interpreter->typed_input_tensor<float>(1);\r\n    float *image_320 = interpreter->typed_input_tensor<float>(0);\r\n\r\n    for (int i = 0; i < H_LARGE * W_LARGE * CHANNEL3; i++) {\r\n        image_640[i] = rgb[i]/255.;\r\n    }\r\n\r\n    for (int i = 0; i < H_SMALL; i++) {\r\n        for (int j = 0; j < W_SMALL; j++) {\r\n            for (int c = 0; c < CHANNEL3; c++) {\r\n                image_320[i * CHANNEL3 * W_SMALL + j * CHANNEL3 + c] = image_640[2 * i * CHANNEL3 * W_LARGE + 2 * j * CHANNEL3  + c];\r\n            }\r\n        }\r\n    }\r\n\r\n    if (interpreter->Invoke() != kTfLiteOk) return;\r\n\r\n    float *output = interpreter->typed_output_tensor<float>(0);\r\n\r\n    for (int i = 0; i < H_LARGE * W_LARGE * CHANNEL3; i++) {\r\n        Image[i] = output[i]*255;\r\n    }\r\n}\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I did some tracing with SnapDragon Profiler, and found that GPU are not used.", "GPU delegate only works on Android and iOS for now.\r\nhttps://www.tensorflow.org/lite/performance/gpu_advanced", "@terryheo This is actually running on an Android phone (NDK r18b). We found that GPU latency is much slower than the one reported on benchmark tool."]}, {"number": 36619, "title": "unable to save the tensorflow hub model of universal lite with tensorflow 1.15", "body": "I'm unable to save the model, after trained my data by using hub model of universal lite 2( https://tfhub.dev/google/universal-sentence-encoder-lite/2).\r\n\r\ntensorflow== 1.15 tensorflow_hub=0.7.0 python =3.7.6 OS = Window 10 64 bit\r\n\r\n** model local**\r\n\r\npath =\"C:/Users/771556/Downloads/ModelLiteUSE\" #url =\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\" liteModule = hub.Module(path)\r\n\r\ninput_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None]) encodings = liteModule( inputs=dict( values=input_placeholder.values, indices=input_placeholder.indices, dense_shape=input_placeholder.dense_shape))\r\n\r\nwith tf.Session() as sess: spm_path = sess.run(liteModule(signature=\"spm_path\"))\r\n\r\nsp = spm.SentencePieceProcessor() sp.Load(spm_path) print(\"SentencePiece model loaded at {}.\".format(spm_path))\r\n\r\ndef process_to_IDs_in_sparse_format(sp, sentences): ids = [sp.EncodeAsIds(x) for x in sentences] max_len = max(len(x) for x in ids) dense_shape=(len(ids), max_len) values=[item for sublist in ids for item in sublist] indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))] return (values, indices, dense_shape)\r\n\r\ndef embed(input): values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, input) # Reduce logging output. logging.set_verbosity(logging.ERROR) with tf.Session() as session: session.run([tf.global_variables_initializer(), tf.tables_initializer()]) message_embeddings = session.run( encodings, feed_dict={input_placeholder.values: values, input_placeholder.indices: indices, input_placeholder.dense_shape: dense_shape}) return message_embeddings **Training the data**\r\n\r\nTrainModel= embed(file_data.text)\r\n\r\n**Saving model**\r\n\r\ntf.saved_model.save(TrainModel,'D:/liteTrainmodel')\r\n\r\n**Error**\r\n![Capture](https://user-images.githubusercontent.com/16205110/74142686-67f49200-4c1f-11ea-8b2f-07720ffc0f28.JPG)\r\n", "comments": ["@zayedrais \r\n\r\nCan you please provide the simple standalone code with proper indentation or colab link to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "> @ravikyram \r\n\r\nkindly find the colab url\r\nhttps://colab.research.google.com/drive/1r7nPKTL4SWhkBvXQ3mr42iXm8cA17xJ0", "@ravikyram \r\n\r\nkindly find the colab url\r\nhttps://colab.research.google.com/drive/1r7nPKTL4SWhkBvXQ3mr42iXm8cA17xJ0\r\n", "@zayedrais As per saving the train data, you cannot save it as there is no graph associated with it.\r\nand that is the reason why `tf.saved_model.save(traindata,'/tmp/tfhub_modules')` throws an error.\r\n\r\nAlso looking at this code\r\n ```\r\nexported.f = tf.function(\r\n  lambda  x: exported.v * x)\r\n```\r\n`exported.v` comes from a different graph and hence the ValueError.\r\n`ValueError: Tensor(\"Variable_4:0\", shape=(3, 512), dtype=float32_ref) must be from the same graph as Tensor(\"saver_filename:0\", shape=(), dtype=string)`\r\n\r\nThis is the expected behavior in Tensorflow. For faster response please post this question in stackoverflow as there is a wider community to respond. Thanks!"]}, {"number": 36618, "title": "Import issue tensorflow 2.1.0 using Windows 10 AMD Radeon processor with GPU", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): bin\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.5.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: AMD Radeon 1 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nCannot import tensorflow after installation\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\npip install tensorflow\r\n\r\n\r\n**Any other info / logs**\r\nimport tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\kumar.shivam\\AppData\\Local\\Programs\\Python\\Python35\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n", "comments": ["@shivamspj,\r\nCould you please check [this](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) comment from a similar issue and let us know if it helps. Thanks!", "@amahendrakar\r\nThanks for the prompt reply. I could solve the issue by reinstalling python version 3.5. MSVC reinstallation. And corresponding libraries(tensorflow).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36618\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36618\">No</a>\n"]}, {"number": 36617, "title": "Remove Meaningless Reversed()", "body": "The existing code will reverse the list containing the constant value. Reversed list is always the same list, so overhead occurs.\r\n\r\nThank you for your efforts to create the best project, TensorFlow.", "comments": ["@marload Can you please fix the build failures? Thanks!", "@gbaned \r\nI solved build failed! I know you are busy, but please review it again."]}, {"number": 36616, "title": "tf.signal.inverse_stft does not reconstructs the original signal", "body": "**System information**\r\n- I have used an example from tf doc\r\n- Ubuntu 18.04.4\r\n- TensorFlow 2.1.0 installed via pip\r\n- Python version: 3.7.3\r\n\r\nFollowing the official example from inverse_stft code to reconstruct signal I got either totally different signal or when frame_step>=frame_length I got nan's every frame_step elements starting at position 0. For clarity, the lengths from the official example were divided by 10.\r\n```\r\nimport tensorflow as tf\r\nframe_length = 40\r\nframe_step = 16\r\nwaveform = tf.random.normal(dtype=tf.float32, shape=[100])\r\nstft = tf.signal.stft(waveform, frame_length, frame_step)\r\ninverse_stft = tf.signal.inverse_stft(\r\n    stft, frame_length, frame_step,\r\n    window_fn=tf.signal.inverse_stft_window_fn(frame_step))\r\nprint(inverse_stft)\r\nprint(waveform)\r\ntf.Tensor(\r\n[ 0.00000000e+00  3.99959536e-05 -2.40500085e-04 -1.59982394e-03\r\n  5.19763958e-03 -4.16988507e-03  2.64100209e-02 -9.08877850e-02\r\n  3.40112969e-02  3.86196673e-01  5.13033606e-02 -2.86520272e-01\r\n  6.11472845e-01  4.98466581e-01  1.13349378e+00  1.09166965e-01\r\n -9.97731745e-01 -5.43629766e-01 -2.73563933e+00  7.85581648e-01\r\n -8.19584429e-01  1.79670918e+00  9.02783334e-01 -1.48530865e+00\r\n -2.47491312e+00 -1.24023890e+00  1.15495408e+00  1.42492950e+00\r\n -7.37325311e-01 -1.20595813e+00 -3.62768292e-01  1.35240197e+00\r\n -3.68225783e-01 -7.91088283e-01  1.89298892e+00  5.07700205e-01\r\n  6.15092158e-01 -1.92212605e+00 -8.31669629e-01  7.79488802e-01\r\n -3.24547052e-01 -5.50925493e-01 -7.81439543e-01  8.75341117e-01\r\n -6.82320118e-01  7.01450050e-01 -4.50847566e-01  6.66174769e-01\r\n -1.23529518e+00 -1.24680352e+00  3.43645632e-01  1.09950686e+00\r\n  5.97586334e-01 -3.48894447e-01  6.68327957e-02 -1.43280745e+00\r\n -6.64948940e-01 -7.94504046e-01  1.14265656e+00 -9.80610073e-01\r\n -4.60786790e-01 -3.97007465e-01 -9.27713037e-01  1.07594097e+00\r\n  5.27661502e-01 -1.61613131e+00 -2.59635901e+00 -3.46627653e-01\r\n  3.27228665e-01 -1.07391989e+00  9.76892650e-01 -2.26863116e-01\r\n -1.79977849e-01 -7.51911581e-01 -1.52214825e-01 -2.73942560e-01\r\n -5.64359844e-01  5.01259208e-01  4.91654649e-02 -1.36436284e-01\r\n  1.09956011e-01  7.98072666e-02 -1.27519906e-01  1.10613126e-02\r\n -3.04599851e-03  1.31062159e-04 -4.15140385e-05  3.23910135e-05], shape=(88,), dtype=float32)\r\ntf.Tensor(\r\n[ 1.4038084   1.0222757  -0.39947665 -0.54522014  0.5803976  -0.19679223\r\n  0.61842763 -1.1810968   0.26712915  1.9677812   0.18064204 -0.73945624\r\n  1.2229462   0.81378937  1.5831046   0.13582362 -1.1432989  -0.58895135\r\n -2.8576777   0.8025885  -0.8269907   1.8019977   0.9033268  -1.4853681\r\n -2.4749134  -1.2402394   1.1549553   1.4249299  -0.73732626 -1.205959\r\n -0.36276835  1.3524033  -0.36822623 -0.7910883   1.8929895   0.50770015\r\n  0.615092   -1.922127   -0.8316696   0.77948934 -0.32454705 -0.5509257\r\n -0.7814399   0.87534165 -0.6823204   0.7014507  -0.45084816  0.6661754\r\n -1.2352954  -1.2468035   0.34364572  1.0995075   0.59758615 -0.34889427\r\n  0.06683233 -1.4328083  -0.6649485  -0.7945043   1.1426575  -0.98061115\r\n -0.46078658 -0.39700815 -0.9277131   1.0759422   0.527661   -1.616195\r\n -2.5979242  -0.34764734  0.33018574 -1.0971678   1.0204725  -0.24577658\r\n -0.2062361  -0.9355164  -0.21259227 -0.44723493 -1.1287199   1.293658\r\n  0.1731153  -0.6951827   0.8636062   1.0371045  -2.9860606   0.5220258\r\n -0.34013367  0.04466997 -0.06895867  0.82789123  0.43297967  0.7684806\r\n -0.6908023   0.57721597  0.9199321   1.3768114   1.2351048  -0.9184835\r\n  0.5188774   0.88726586  1.4416382  -0.40344936], shape=(100,), dtype=float32)\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\nframe_length = 40\r\nframe_step = 40\r\nwaveform = tf.random.normal(dtype=tf.float32, shape=[100])\r\nstft = tf.signal.stft(waveform, frame_length, frame_step)\r\ninverse_stft = tf.signal.inverse_stft(\r\n    stft, frame_length, frame_step,\r\n    window_fn=tf.signal.inverse_stft_window_fn(frame_step))\r\nprint(inverse_stft)\r\nprint(waveform)\r\ntf.Tensor(\r\n[        nan -0.5289995  -0.42322898  1.7522525   0.34771994 -0.7660091\r\n  1.6534201  -0.23190129 -0.64468837 -1.1712008  -2.6483274   1.3840579\r\n -1.0050658   0.87601507 -0.5170545   0.6623281  -1.0479059  -0.5475797\r\n -1.2517245  -1.1959579   0.58061975 -0.1989309   0.18141915 -1.7858055\r\n -0.7680144  -0.21199739  0.2686664   0.06079084 -0.68268234  0.9945366\r\n  2.2236376  -0.56432855  1.281768    0.8328386  -0.6435259   0.85617316\r\n -0.09510866  0.7118371   2.0645704   0.63955176         nan  0.15469487\r\n  1.0494236   0.3661145  -1.4089363  -1.3391382   1.0011977  -0.07683202\r\n -1.4500219  -0.0392501   1.1920362   1.8719866   0.7821921  -0.45498323\r\n -1.890665    0.67334163 -2.023159   -0.29750296 -0.2908026   0.38893116\r\n -0.10589531  0.7326803   1.2091097  -0.34784627  1.311726    0.79103297\r\n -0.91987884  0.51282054 -0.20457014  0.50253487 -0.46842676  2.5228522\r\n -0.07464705  1.7176666  -0.40536413 -0.06539667  1.0447518  -0.42340115\r\n  0.1290549   0.53456116], shape=(80,), dtype=float32)\r\ntf.Tensor(\r\n[-2.005882   -0.5290042  -0.42322478  1.7522544   0.34772    -0.76600945\r\n  1.6534219  -0.23190151 -0.64468867 -1.1712015  -2.6483283   1.3840592\r\n -1.0050665   0.8760156  -0.5170555   0.66232944 -1.0479063  -0.54757965\r\n -1.2517256  -1.1959581   0.5806202  -0.19893074  0.1814189  -1.7858069\r\n -0.76801383 -0.21199743  0.2686672   0.06079032 -0.6826823   0.9945371\r\n  2.2236395  -0.56433004  1.2817682   0.8328381  -0.64352524  0.8561739\r\n -0.09510913  0.7118365   2.064576    0.6395979   0.7826336   0.15469465\r\n  1.0494274   0.36611095 -1.4089363  -1.339139    1.0011992  -0.07683332\r\n -1.4500217  -0.03925065  1.1920377   1.8719873   0.78219193 -0.4549838\r\n -1.8906664   0.67334247 -2.0231593  -0.2975028  -0.2908029   0.3889313\r\n -0.1058953   0.7326806   1.2091101  -0.34784687  1.3117266   0.79103285\r\n -0.91987914  0.5128205  -0.20457047  0.5025352  -0.46842635  2.5228539\r\n -0.07464796  1.7176673  -0.40536594 -0.06539585  1.0447516  -0.42340097\r\n  0.12904675  0.5345574  -0.12362233 -1.9486434   0.53776556 -0.32944998\r\n  0.48096088 -1.0506314  -1.2964804  -0.34489468 -0.38139907  0.49959585\r\n -0.25842932 -0.8132113   1.0629432  -0.5591399   0.11014029 -0.4576707\r\n -0.15766484 -0.07155921  0.2819425   0.40777263], shape=(100,), dtype=float32)\r\n```\r\n\r\n", "comments": ["Was able to reproduce the issue with Tf 2.1 and tf-nightly.\r\nPlease find the gist [here](https://colab.research.google.com/gist/gadagashwini/1e329d12c63809e0e0fc5fe5e098e8a7/untitled386.ipynb). Thanks!", "@jvishnuvardhan any news on this issue?", "Same issue for me", "Same here tested on 2.2.0 and the recent 2.3.0", "Can I ping @rryan if it's relevant? ", "@grzjab  You're seeing the effects of overlap add with the window function. I changed waveform to 88 samples so it has a integer number of frames with your frame length and step and got this (with tf2.1):\r\n`np.isclose(waveform, inverse_stft)\r\nOut[9]:\r\narray([False, False, False, False, False, False, False, False, False,\r\n       False, False, False, False, False, False, False, False, False,\r\n       False, False, False, False, False, False,  True,  True,  True,\r\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\r\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\r\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\r\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\r\n        True,  True, False, False, False, False, False, False, False,\r\n       False, False, False, False, False, False, False, False, False,\r\n       False, False, False, False, False, False, False])`\r\n\r\nYou can see that it's only the framed samples at the ends of the waveform that differ from the original, which is what you'd expect with the default Hann window the stft func is applying. Similarly, the Hann window goes to zero at the ends so you can't do an stft with frame_step=frame_length because the inverse window_fn will have NaN's.\r\n", "Was able to reproduce issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/87c9c6bf653365fed310b13b9dd57524/35650.ipynb). Thanks!", "Is there any solution?", "Hello!\r\n\r\nI  don't know if the bug has been solved but I did a small hack that does the job.\r\n\r\nFirst the original behavior: \r\n\r\n```python\r\nimport tensorflow as tf\r\nframe_length = 40\r\nframe_step = 16\r\nx_length = 100\r\nx = tf.random.normal(dtype=tf.float32, shape=[x_length])\r\n\r\n# original \r\nstft = tf.signal.stft(x, frame_length, frame_step, pad_end=True)\r\ninverse_stft = tf.signal.inverse_stft(stft, frame_length, frame_step, window_fn=tf.signal.inverse_stft_window_fn(frame_step))\r\ny = tf.slice(inverse_stft, begin=(0, ), size=(x_length, ))\r\ntf.experimental.numpy.allclose(x, y, atol=1e-7)\r\n>> False\r\n```\r\nHere's the overlap between the original (blue) and the output (orange). Note the problem at the beginning. \r\n\r\n![image](https://user-images.githubusercontent.com/11062535/129184330-08d830c6-d140-4c14-b109-b280c8670d5c.png)\r\n\r\nThe hack: \r\n\r\n```python\r\n# hack \r\n\r\n# zero padding the beginning of the signal\r\nzero_pad_init = round(frame_length - frame_step)\r\npaddings = tf.constant([[zero_pad_init, 0]])\r\nx_pad = tf.pad(x, paddings)\r\nstft_pad = tf.signal.stft(x_pad, frame_length, frame_step, pad_end=True)\r\ninverse_stft_pad = tf.signal.inverse_stft(stft_pad, frame_length, frame_step, window_fn=tf.signal.inverse_stft_window_fn(frame_step))\r\ny = tf.slice(inverse_stft_pad, begin=(zero_pad_init, ), size=(x_length, ))\r\ntf.experimental.numpy.allclose(x, y, atol=1e-7)\r\n>> True\r\n```\r\nNow the output is as expected:\r\n\r\n![image](https://user-images.githubusercontent.com/11062535/129184846-87fb5051-640f-4acb-bd97-6bf492c0c853.png)\r\n\r\nIdeally I'd expect something like [librosa](https://librosa.org/doc/0.8.0/generated/librosa.stft.html) that handles properly the center of the window and does not need require you to zero pad at the beginning, nor to slice after the inverse_stft. But at least with this hack: ```inverse_stft(stft(x)) == x```. \r\n\r\nI hope this helps!\r\n\r\nP.S: this code was generated using `tf 2.5.0`", "Has anyone found a solution to this?", "@grzjab \r\nCould you please check comment by @gabolsgabs if it helps also let us know if the latest version helps.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36616\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36616\">No</a>\n"]}, {"number": 36615, "title": "tf.saved_model.save can not save a model containing DenseHashTable", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below):  tf-nightly(2.2.0.dev20200208)\r\n\r\n**Describe the current behavior**\r\nA keras model which contains DenseHashTable cannot be saved by tf.saved_model.save function.\r\nGot a \"RuntimeError: Attempting to capture an EagerTensor without building a function.\"\r\n\r\n**Describe the expected behavior**\r\nSave the model correctly.\r\n\r\n**Code to reproduce the issue**\r\nHere is the simplest situation.\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Model(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.table = tf.lookup.experimental.DenseHashTable(\r\n        key_dtype=tf.int64,\r\n        value_dtype=tf.int64,\r\n        default_value=-1,\r\n        empty_key=0,\r\n        deleted_key=-1)\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.int64)])\r\n  def call(self, input):\r\n    return self.table.lookup(input)\r\n\r\nm = Model()\r\ntf.saved_model.save(m, '/tmp/test')\r\n```\r\n\r\n[Here](https://colab.research.google.com/drive/1ENfP-3vp1c7hoRQU-0egTngatcMMYsMp) is the code gist.\r\n", "comments": ["i am able to replicate the issue, please find the gist [here](https://colab.research.google.com/gist/Saduf2019/d63cdb41d69ba7e65c91698ce15a0434/36615.ipynb).", "@doldre As error mentioned model was not built. saving custom_objects (subclassed models, layers) require special attention when saving and loading. Please check more details on saving custom_objects [here](https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects). Please check even more detailed description on [saving and loading of subclassed models](https://www.tensorflow.org/guide/keras/save_and_serialize#part_ii_saving_and_loading_of_subclassed_models). There is one important line mentioned in the second resource I shared here is `First of all, a subclassed model that has never been used cannot be saved.`\r\n\r\nPlease close the issue if this was resolved for you. Thanks!\r\n\r\n", "When I tried m.summary(), it throws an error as follows. The error trace describes that model has not yet been built.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-0db5d921a66f> in <module>()\r\n----> 1 m.summary()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in summary(self, line_length, positions, print_fn)\r\n   1348     \"\"\"\r\n   1349     if not self.built:\r\n-> 1350       raise ValueError('This model has not yet been built. '\r\n   1351                        'Build the model first by calling `build()` or calling '\r\n   1352                        '`fit()` with some data, or specify '\r\n\r\nValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.\r\n```", "I tried to modify the code to below\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass Model(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.table = tf.lookup.experimental.DenseHashTable(\r\n        key_dtype=tf.int64,\r\n        value_dtype=tf.int64,\r\n        default_value=-1,\r\n        empty_key=0,\r\n        deleted_key=-1)\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.int64)])\r\n  def call(self, input):\r\n    return self.table.lookup(input)\r\n  \r\n  def insert(self, keys, values):\r\n    return self.table.insert(keys, values)\r\n\r\nm = Model()\r\nm.insert([1,2,3,4], [4,3,2,1])\r\nprint(m([1,2,3,4]))\r\nm.summary()\r\ntf.saved_model.save(m, '/tmp/test') \r\n```\r\nm.summary() worked, but another exception occured.\r\n```\r\nTensor(\"model_16/StatefulPartitionedCall:0\", shape=(4,), dtype=int64)\r\nModel: \"model_16\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nWARNING:tensorflow:Skipping full serialization of Keras model <__main__.Model object at 0x7f8b139ef940>, because its inputs are not defined.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-18-3c22b98a08cb> in <module>()\r\n     23 print(m([1,2,3,4]))\r\n     24 m.summary()\r\n---> 25 tf.saved_model.save(m, '/tmp/test')\r\n\r\n10 frames\r\n/tensorflow-1.15.0/python3.6/tensorflow_core/python/framework/ops.py in _assert_same_graph(original_item, item)\r\n   5912   if original_item.graph is not item.graph:\r\n   5913     raise ValueError(\"%s must be from the same graph as %s.\" %\r\n-> 5914                      (item, original_item))\r\n   5915 \r\n   5916 \r\n\r\nValueError: Tensor(\"MutableDenseHashTable_17:0\", shape=(), dtype=resource) must be from the same graph as Tensor(\"serving_default_input:0\", shape=(?,), dtype=int64).\r\n```", "@doldre,\r\nThe error seems to be resolved in the **`Latest Version of Tensorflow (2.4.1)`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/d63ec7e7de7a272dabaeb511b272c047/36615.ipynb) of the **`Working Code`**. Thanks! ", "> @doldre,\r\n> The error seems to be resolved in the **`Latest Version of Tensorflow (2.4.1)`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/d63ec7e7de7a272dabaeb511b272c047/36615.ipynb) of the **`Working Code`**. Thanks!\r\n\r\nYes, It is resolved!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36615\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36615\">No</a>\n"]}, {"number": 36614, "title": "tensorflow GPU is not installed properly", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Laptop\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.1.0\r\n- Python version:3.7.4\r\n- Installed using virtualenv? pip? conda?: using pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA: 10.2 , cuDNN: 7.6.5\r\n- GPU model and memory: GeForce GTX 1650 , 4GB GPU memory\r\n- System Configuration: intel i7 processor, 1 TB memory, 8 GB RAM.\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nExecuted below code than the eror occured which is pasted in the file \"tensorflow installation error\":\r\n\"from tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\"\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[tensorflow installation error.txt](https://github.com/tensorflow/tensorflow/files/4179294/tensorflow.installation.error.txt)\r\n\r\n", "comments": ["@Kaushikudaya \r\nCould you please check [this](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) link on a similar issue and let us know if it helps. Thanks!", "@Kaushikudaya \r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the latest [microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\nMake sure you update environment path for cuda(please refer this https://www.tensorflow.org/install/gpu#windows_setup ). Make sure if there is a library that is in a different location/not installed on your system that cannot be loaded.Also, please follow the instructions from [Tensorflow website](https://www.tensorflow.org/install/gpu#windows_setup).\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you. Thanks!", "Hi Team,\n\nThat error was solved after updating the tensorflow to 2.0\nbut i am getting other error:\n[image: image.png]\n\n\nPlease check\n\nOn Tue, Feb 11, 2020 at 5:27 PM ravikyram <notifications@github.com> wrote:\n\n> @Kaushikudaya <https://github.com/Kaushikudaya>\n> What is make/model of your cpu? -HP OMEN , 4GB GEForce GTX 1650 GPU, 8GB\n> RAM, 1TB memory, i7 intel processor.\n> I suspect your cpu model does not support AVX instructions sets.See hardware\n> requirements\n> <https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements>\n> -It is a brand new CPU so it supports AVX\n> Make sure to download the latest microsoft visual c++ redistributable\n> from here\n> <https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads>.\n> - I have downloaded 2015 VS from the site,\n> Make sure you update environment path for cuda(please refer this\n> https://www.tensorflow.org/install/gpu#windows_setup ). Make sure if\n> there is a library that is in a different location/not installed on your\n> system that cannot be loaded.Also, please follow the instructions from\n> Tensorflow website.\n> Please, check Your CPU/Python is on 32 bits?Please, refer #36167\n> <https://github.com/tensorflow/tensorflow/issues/36167> and see if it\n> helps you. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36614?email_source=notifications&email_token=AL7RQ7YJ5GJFC2R7FCPY2LLRCKHB3A5CNFSM4KSIVGAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELME52I#issuecomment-584601321>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AL7RQ752WLURGDTF6LF67R3RCKHB3ANCNFSM4KSIVGAA>\n> .\n>\n", "@Kaushikudaya Please share complete executable code and the error **log** faced for us to replicate the issue and help", "@Kaushikudaya Could you please respond to the above comment", "@Kaushikudaya Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36614\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36614\">No</a>\n"]}, {"number": 36613, "title": "cannot resolve external symbol __imp_TF_Version", "body": "I dowload tensorflow c libs from: https://tensorflow.google.cn/install/lang_c.\r\nwhen I try to use these libs with vs2019, it turn out an error message: cannot resolve external symbol __imp_TF_Version", "comments": ["The codes are from https://tensorflow.google.cn/install/lang_c too.\r\n#include <stdio.h>\r\n#include <tensorflow/c/c_api.h>\r\n\r\nint main() {\r\n\tprintf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n\treturn 0;\r\n}", "@ccl-private, Provide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "@ccl-private, Any update?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36613\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36613\">No</a>\n"]}, {"number": 36612, "title": "ReduceLROnPlateau callback crashed when learning rate is of type tf.keras.optimizers.schedules", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using tf.keras.callbacks.ReduceLROnPlateau in callbacks in model.fit(), it fails with an error if model.optimizer.lr is of type  tf.keras.optimizers.schedules. *. \r\n\r\nFor example, one can use tf.keras.optimizers.schedules.ExponentialDecay in an optimizer. This would cause the following line to fail:\r\n\r\n[https://github.com/tensorflow/tensorflow/blob/6dd3403a89f2f0f9a6219ba926091e203693ff9f/tensorflow/python/keras/callbacks.py#L2021](https://github.com/tensorflow/tensorflow/blob/6dd3403a89f2f0f9a6219ba926091e203693ff9f/tensorflow/python/keras/callbacks.py#L2021)\r\n\r\nAbove line assumes lr to be a float.\r\n\r\nThe error it gives in this case is :\r\n\r\nTypeError: float() argument must be a string or a number, not 'ExponentialDecay' \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nReduceLROnPlateau should also work with learning rate whose type is of tf.keras.optimizers.schedules\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dd1923 \r\n\r\nWill it be possible to share simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ravikyram Here is the [colab link](https://colab.research.google.com/drive/1bULTBS7TYFrSOtYbLoqaHj0zM0Tr2zU3).", "I have tried on colab with TF version 2.1.0, 2.2.0-dev20200211 and was able to reproduce the issue. Thanks!", "Any updates?", "@jvishnuvardhan ? @rchao ?", "Same here. ReduceLRonPlateau does not play well with ExponentialDecay. Providing `lr` as a simple constant works. \r\n\r\n```python\r\ntf.__version__\r\n2.2.0\r\n\r\ntf.keras.__version__\r\n2.3.0-tf\r\n```", "Doesn't look like anyone from tf team is interested in fixing any bugs. I have had a few others open for a long time now.", "@dd1923 it's open-source, so you can only complain if you have an MR fixing the bug, but no one reviews it. And even then, you have the liberty to fork the repo and maintain a better fork. \r\n\r\nEspecially, this issue seems to be easily fixable with just python knowledge. ", "@dd1923 Extremely sorry for missing this issue. I could reproduce the issue with `TF2.2`. However, I ran your code with `tf-nightly` and I cannot reproduce the issue. The callback is working without any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/f8a79e46abbbcb2b5aa25636d25f0433/reducelronplateau-bug.ipynb). Thanks!\r\n\r\n", "I am closing this issue as this was resolved in `tf-nightly`. If you want to use stable version, then please wait for some time. Stable `TF2.3` will be released in near future. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36612\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36612\">No</a>\n", "I updated my Tensorflow version to `2.3.0`, However, the behavior still persists on my machine. I am trying to combine `tf.keras.optimizers.schedules.ExponentialDecay`with `tf.keras.callbacks.ReduceLROnPlateau` with no luck. Thank you for your help. \r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.6\r\nTensorFlow installed from (source or binary): pip3\r\nTensorFlow version (use command below): 2.3.0\r\nPython version: 3.7", "@dcleres Can you please create a new issue with a simple standalone code to reproduce the issue? Thanks!"]}, {"number": 36611, "title": "Buid failed in redhat system..", "body": "ERROR: /root/.cache/bazel/_bazel_daixiangzi/7c0ccbbd924a8f891a09b57c04646026/external/boringssl/BUILD:130:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1)\r\ngcc: error trying to exec 'cc1': execvp: No such file or directory\r\nTarget //tensorflow/tools/graph_transforms:transform_graph failed to build\r\nINFO: Elapsed time: 29.467s, Critical Path: 4.82s\r\nINFO: 9 processes: 9 local.\r\nFAILED: Build did NOT complete successfully\r\nGcc and g++ version is 4.8.5\r\nNO CUDA", "comments": ["tensorflow version is master", "i run  bazel build --config=opt tensorflow/tools/graph_transforms:transform_graph --verbose_failures\r\n", "bazel version is 1.2.1\r\n", "@daixiangzi Could you  provide the exact sequence of commands / steps that you executed before running into the problem, also please check this [link](https://stackoverflow.com/questions/11912878/gcc-error-gcc-error-trying-to-exec-cc1-execvp-no-such-file-or-directory) if it helps. ", "> @daixiangzi Could you provide the exact sequence of commands / steps that you executed before running into the problem, also please check this [link](https://stackoverflow.com/questions/11912878/gcc-error-gcc-error-trying-to-exec-cc1-execvp-no-such-file-or-directory) if it helps.\r\n\r\n1.git clone https://github.com/tensorflow/tensorflow.git\r\n2 cd tensorflow\r\n3 ./configure\r\n\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nFound possible Python library paths:\r\n  /usr/lib/python2.7/site-packages\r\n  /usr/lib64/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python2.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n4.bazel build --config=opt tensorflow/tools/graph_transforms:transform_graph --verbose_failures\r\n\r\nERROR:\r\nWARNING: /home/daixiangzi/github/tensorflow/tensorflow/core/BUILD:1973:1: in srcs attribute of cc_library rule //tensorflow/core:gif_internal: please do not import '//tensorflow/core/platform:gif.h' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nINFO: Analyzed target //tensorflow/tools/graph_transforms:transform_graph (155 packages loaded, 10240 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base /root/.cache/bazel/_bazel_daixiangzi/7c0ccbbd924a8f891a09b57c04646026/sandbox\r\nERROR: /root/.cache/bazel/_bazel_daixiangzi/7c0ccbbd924a8f891a09b57c04646026/external/boringssl/BUILD:130:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1)\r\ngcc: error trying to exec 'cc1': execvp: No such file or directory\r\nTarget //tensorflow/tools/graph_transforms:transform_graph failed to build\r\nINFO: Elapsed time: 47.152s, Critical Path: 4.85s\r\nINFO: 9 processes: 9 local.\r\n\r\n", "> > @daixiangzi Could you provide the exact sequence of commands / steps that you executed before running into the problem, also please check this [link](https://stackoverflow.com/questions/11912878/gcc-error-gcc-error-trying-to-exec-cc1-execvp-no-such-file-or-directory) if it helps.\r\n> \r\n> 1.git clone https://github.com/tensorflow/tensorflow.git\r\n> 2 cd tensorflow\r\n> 3 ./configure\r\n> \r\n> Please specify the location of python. [Default is /usr/bin/python]:\r\n> Found possible Python library paths:\r\n> /usr/lib/python2.7/site-packages\r\n> /usr/lib64/python2.7/site-packages\r\n> Please input the desired Python library path to use. Default is [/usr/lib/python2.7/site-packages]\r\n> \r\n> Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\n> No XLA JIT support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\n> No OpenCL SYCL support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: n\r\n> No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: n\r\n> No CUDA support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\n> Clang will not be downloaded.\r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n> \r\n> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\n> Not configuring the WORKSPACE for Android builds.\r\n> \r\n> Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n> --config=mkl # Build with MKL support.\r\n> --config=monolithic # Config for mostly static monolithic build.\r\n> --config=ngraph # Build with Intel nGraph support.\r\n> --config=numa # Build with NUMA support.\r\n> --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.\r\n> --config=v2 # Build TensorFlow 2.x instead of 1.x.\r\n> Preconfigured Bazel build configs to DISABLE default on features:\r\n> --config=noaws # Disable AWS S3 filesystem support.\r\n> --config=nogcp # Disable GCP support.\r\n> --config=nohdfs # Disable HDFS support.\r\n> --config=nonccl # Disable NVIDIA NCCL support.\r\n> Configuration finished\r\n> \r\n> 4.bazel build --config=opt tensorflow/tools/graph_transforms:transform_graph --verbose_failures\r\n> \r\n> ERROR:\r\n> WARNING: /home/daixiangzi/github/tensorflow/tensorflow/core/BUILD:1973:1: in srcs attribute of cc_library rule //tensorflow/core:gif_internal: please do not import '//tensorflow/core/platform:gif.h' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\n> INFO: Analyzed target //tensorflow/tools/graph_transforms:transform_graph (155 packages loaded, 10240 targets configured).\r\n> INFO: Found 1 target...\r\n> INFO: Deleting stale sandbox base /root/.cache/bazel/_bazel_daixiangzi/7c0ccbbd924a8f891a09b57c04646026/sandbox\r\n> ERROR: /root/.cache/bazel/_bazel_daixiangzi/7c0ccbbd924a8f891a09b57c04646026/external/boringssl/BUILD:130:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1)\r\n> gcc: error trying to exec 'cc1': execvp: No such file or directory\r\n> Target //tensorflow/tools/graph_transforms:transform_graph failed to build\r\n> INFO: Elapsed time: 47.152s, Critical Path: 4.85s\r\n> INFO: 9 processes: 9 local.\r\n\r\nand then i run sudo find / -name \"cc1\",no find and cat /usr/libexec/gcc/x86_64-redhat-linux/4.8.5/ no find cc1.i think if gcc version relationship\uff1f\uff1f\r\n", "@daixiangzi  Please share the GCC and tensorflow version used.", "> @daixiangzi Please share the GCC and tensorflow version used.\r\n\r\nGCC version is 4.8.5", "> @daixiangzi Please share the GCC and tensorflow version used.\r\n\r\ntensorflow version is master", "@daixiangzi  you please try with \"GCC 7.3.1\" and let us know if it helps.", "@daixiangzi Please confirm if the above comment helps and your issue is resolved.", "> @daixiangzi Please confirm if the above comment helps and your issue is resolved.\r\n\r\ni also meet new problem :\r\nERROR: /home/github/tensorflow_source/tensorflow/core/lib/monitoring/BUILD:191:1: C++ compilation of rule '//tensorflow/core/lib/monitoring:percentile_sampler' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -MD -MF ... (remaining 73 argument(s) skipped)\r\n", "> > @daixiangzi Please confirm if the above comment helps and your issue is resolved.\r\n> \r\n> i also meet new problem :\r\n> ERROR: /home/github/tensorflow_source/tensorflow/core/lib/monitoring/BUILD:191:1: C++ compilation of rule '//tensorflow/core/lib/monitoring:percentile_sampler' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer '-std=c++0x' -MD -MF ... (remaining 73 argument(s) skipped)\r\n\r\n@daixiangzi Hi. I meet the same problem:\r\n`external/org_tensorflow/tensorflow/core/lib/monitoring/BUILD:176:1: C++ compilation of rule '@org_tensorflow//tensorflow/core/lib/monitoring:percentile_sampler' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 80 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nexternal/org_tensorflow/tensorflow/core/lib/monitoring/percentile_sampler.cc: In member function 'void tensorflow::monitoring::PercentileSamplerCell::Add(double)':\r\nexternal/org_tensorflow/tensorflow/core/lib/monitoring/percentile_sampler.cc:32:45: error: no match for 'operator=' (operand types are '__gnu_cxx::__alloc_traits<std::allocator<tensorflow::monitoring::PercentileSamplerCell::Sample> >::value_type {aka tensorflow::monitoring::PercentileSamplerCell::Sample}' and '<brace-enclosed initializer list>')\r\n   samples_[next_position_] = {nstime, sample};\r\n                                             ^\r\nIn file included from external/org_tensorflow/tensorflow/core/lib/monitoring/percentile_sampler.cc:16:0:\r\nexternal/org_tensorflow/tensorflow/core/lib/monitoring/percentile_sampler.h:66:10: note: candidate: tensorflow::monitoring::PercentileSamplerCell::Sample& tensorflow::monitoring::PercentileSamplerCell::Sample::operator=(const tensorflow::monitoring::PercentileSamplerCell::Sample&)\r\n   struct Sample {\r\n          ^~~~~~\r\nexternal/org_tensorflow/tensorflow/core/lib/monitoring/percentile_sampler.h:66:10: note:   no known conversion for argument 1 from '<brace-enclosed initializer list>' to 'const tensorflow::monitoring::PercentileSamplerCell::Sample&'\r\nexternal/org_tensorflow/tensorflow/core/lib/monitoring/percentile_sampler.h:66:10: note: candidate: tensorflow::monitoring::PercentileSamplerCell::Sample& tensorflow::monitoring::PercentileSamplerCell::Sample::operator=(tensorflow::monitoring::PercentileSamplerCell::Sample&&)\r\nexternal/org_tensorflow/tensorflow/core/lib/monitoring/percentile_sampler.h:66:10: note:   no known conversion for argument 1 from '<brace-enclosed initializer list>' to 'tensorflow::monitoring::PercentileSamplerCell::Sample&&'\r\nexternal/org_tensorflow/tensorflow/core/lib/monitoring/percentile_sampler.cc: In member function 'tensorflow::monitoring::Percentiles tensorflow::monitoring::PercentileSamplerCell::value() const':\r\nexternal/org_tensorflow/tensorflow/core/lib/monitoring/percentile_sampler.cc:76:62: error: could not convert '{percentile, samples.std::vector<tensorflow::monitoring::PercentileSamplerCell::Sample>::operator[](index).tensorflow::monitoring::PercentileSamplerCell::Sample::value}' from '<brace-enclosed initializer list>' to 'tensorflow::monitoring::PercentilePoint'\r\n       PercentilePoint pct = {percentile, samples[index].value};\r\n`\r\n\r\nDid you solve it?  \r\nFor me: bazel-2.0.0, gcc-7.5.0, TF2.2.0\uff0cubuntu-16.04\r\nAny help will be appreciated. thanks!", "Also, just hit the same problem with same config as @cxyanhk - anybody been able to solve it?", "I also meet this issue  @cxyanhk and I'm using Ubuntu-18.05, gcc-7.5.0, bazel-2.0.0, TF2.2.0. Anyone can help?", "@daixiangzi,\r\n\r\nWe are checking to see if you still need help on this issue. You can try building the latest stable version of TF i.e `2.6` using this [guide](https://www.tensorflow.org/install/source) and make sure to use this [link](https://www.tensorflow.org/install/source#tested_build_configurations) to know about tested build configurations. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36611\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36611\">No</a>\n"]}, {"number": 36610, "title": "Successfully convert model through full-integer, but leave nodes with int8 instead of uint8", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-55-generic x86_64)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: would run on Raspberry Pi4 +edge TPU or Dev-board\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.15.0-0-g590d6eef7e 1.15.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): GCC 7.3.0\r\n- CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.243\r\n- GPU model and memory: 2080Ti\r\n**(but TensorFlow was not compiled with CUDA support)**\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI convert my model from pytorch->onnx->tensorflow->tensorflow lite. And I would run on Edge TPU, thus my tensorflow version should be 1.15.0 as suggestion. However, due to lack of \"split\", and incorrect of \"min and max\". I rebuild from source based on the two links below.\r\n\r\n[https://github.com/tensorflow/tensorflow/commit/3c2be61e68acea533030b8f70fa0db3339b19f93](url)\r\n[https://github.com/tensorflow/tensorflow/pull/32582/files?file-filters%5B%5D=.cc&file-filters%5B%5D=.h&file-filters%5B%5D=No+extension#diff-71a783b5a7624430897d44e42f816444](url)\r\n\r\nI use full-integer quantization, and all the conversion is successful. The tflite file works fine on interpreter. Input and Output datatype are tf.uint8 correctly. But almost every node in the model except input and output is tf.int8 (quantization).\r\n\r\n**Describe the expected behavior**\r\nI think these nodes should be tf.uint8. And I wonder that whether I could just load the tflite file via \"JSON\" format and change the model[\"subgraphs\"][0][\"tensors\"][\"type\"] \"INT8\" to \"UINT8\".\r\n\r\nThis idea based on the link below.\r\n[https://towardsdatascience.com/hacking-google-coral-edge-tpu-motion-blur-and-lanczos-resize-9b60ebfaa552](url)\r\nActually, it failed. I want to know the reasons.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n\r\nimport tensorflow as tf\r\nimport torch\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport pathlib\r\n\r\n#tf.compat.v1.enable_eager_execution()\r\n#tf.logging.set_verbosity(tf.logging.DEBUG)\r\n\r\n#import matplotlib.pyplot as plt\r\nprint(tf.__version__)\r\nprint(torch.__version__)\r\n\r\n\r\nval = \"./ILSVRC2012_img_val/\"\r\nval=pathlib.Path(val)\r\nitem=np.array([i.name for i in val.glob(\"*\")])\r\n\r\nimage_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,data_format=\"channels_first\")\r\n\r\ntrain_data_gen = image_generator.flow_from_directory(directory=str(val),\r\n                                                     batch_size=100,\r\n                                                     shuffle=True,\r\n                                                     target_size=(320,320),\r\n                                                     classes = list(item))\r\n\r\nimage_batch, label_batch = next(train_data_gen)\r\nimage_batch=np.expand_dims(image_batch,1)\r\n\r\nprint(((image_batch).dtype))\r\n\r\nimage_batch, label_batch = next(train_data_gen)\r\nmean=([0.485, 0.456, 0.406])\r\nstd=([0.229, 0.224, 0.225])\r\nfor i in range(3):\r\n    image_batch[:,i,:,:]=(image_batch[:,i,:,:]-mean[i])/(std[i])\r\nprint(image_batch.shape)\r\nimage_batch=(np.expand_dims(image_batch,1))\r\n\r\n\r\ndef representative_data_gen():\r\n    for i in range(100):\r\n        yield [image_batch[i]]\r\n\r\ngraph_def_file = \"test_320.pb\"\r\ninputs=['input.1']\r\noutputs=['655']\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file,inputs, outputs)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n#converter.inference_type=tf.uint8\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n#converter.experimental_new_quantizer= True\r\n#converter.experimental_enable_mlir_converter = True\r\ntflite_model_op= converter.convert()\r\nopen(\"cvted_model_320_quant_op.tflite\", \"wb\").write(tflite_model_op)\r\n\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI tried compile with Edge TPU Compiler, and find out \"unsupported data type\". Then I use netron find that most of them are tf.int8.\r\n\r\nHere is the Edge TPU compiler log, result of Netron and my model. \r\n\r\n```\r\nEdge TPU Compiler version 2.0.291256449\r\n\r\nModel compiled successfully in 45 ms.\r\n\r\nInput model: /home/xxxxx01/39dstest/cvted_model_320_quant_op.tflite\r\nInput size: 6.28MiB\r\nOutput model: cvted_model_320_quant_op_edgetpu.tflite\r\nOutput size: 6.03MiB\r\nOn-chip memory available for caching model parameters: 0.00B\r\nOn-chip memory used for caching model parameters: 0.00B\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 0\r\nTotal number of operations: 7950\r\nOperation log: cvted_model_320_quant_op_edgetpu.log\r\n\r\nModel successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\r\nNumber of operations that will run on Edge TPU: 0\r\nNumber of operations that will run on CPU: 7950\r\n\r\nOperator                       Count      Status\r\n\r\nSPLIT                          36         Operation is working on an unsupported data type\r\nQUANTIZE                       3729       Operation is otherwise supported, but not mapped due to some unspecified limitation\r\nMUL                            74         Operation is working on an unsupported data type\r\nMAXIMUM                        38         Operation is working on an unsupported data type\r\nMINIMUM                        38         Operation is working on an unsupported data type\r\nADD                            74         Operation is working on an unsupported data type\r\nCONCATENATION                  56         Operation is working on an unsupported data type\r\nPAD                            37         Operation is working on an unsupported data type\r\nCONV_2D                        38         Operation is working on an unsupported data type\r\nDEPTHWISE_CONV_2D              3680       Operation is working on an unsupported data type\r\nTRANSPOSE                      148        Operation is working on an unsupported data type\r\nMEAN                           1          Operation is working on an unsupported data type\r\nFULLY_CONNECTED                1          Operation is working on an unsupported data type\r\n\r\n```\r\n**Result of Netron**\r\n![Results of Netron](https://user-images.githubusercontent.com/60849247/74120385-1be52580-4bfe-11ea-8909-7d636b97be14.PNG)\r\n\r\n**My model**\r\n[test_320.zip](https://github.com/tensorflow/tensorflow/files/4178930/test_320.zip)\r\n\r\n\r\nNew to Github, Thx ><", "comments": ["So looks like the TFLite converter does the right job and you are asking for a good approach to post-processing all the nodes to uint8 for edge TPU execution? As far as I know, the current edge TPU runtime will automatically convert int8 activation to uint8, so you don't need to handle them by yourself.", "Can you set both the `converter.inference_input_type = tf.int8`, `converter.inference_output_type = tf.int8` and `converter.inference_type=tf.int8` and then give it another try? I noticed the extra Quatize op is converting uint8 to int8. So if you set all all the inference types to int8, the nodes of the model will be in int8 and the edge tpu converter should be able to do the right job.", "Hello I found the bug is hard to reproduce since it is required to rebuild tensorflow.\r\nCan you attach the post-training quantized tflite model if possible? Thanks!", "@liufengdb @nicci1771 Thanks for your reply.\r\nSure, here are all the tflite model with different inference input/output type.\r\nhttps://drive.google.com/file/d/1cQ2vZ1pwyGh9__huOIXZFrxF_-_QgdMV/view\r\nYes, you got it. All the inference on cpu was fine. However, in edge TPU failed.\r\n\"the current edge TPU runtime will automatically convert int8 activation to uint8\"\r\nDid you mean Edge TPU runtime 13? I think edgetpu compiler maybe is another problem. \r\nFrom their official website\r\n\"* **These models do not compile properly with Edge TPU Compiler version 2.0.2xx (from our September update). You can instead use the previous compiler version and run them with the latest runtime**.\"\r\n Unfortunately, I don't know where to get older version.\r\n\r\nAnd, I have tried your suggestions. I set the input/output type to tf.int8, but I cannot set \"converter.inference_type=tf.int8\". It said that it should be float32.\r\nDoes this error relate to my representative dataset?\r\n\r\nThanks a lot.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36610\">No</a>\n"]}, {"number": 36609, "title": "ImportError:_pywrap_checkpoint_reader", "body": "When I tried to run freeze_graph.py, the following import Error comes out.\r\n\r\nfrom tensorflow.python.training import py_checkpoint_reader\r\nImportError: cannot import name 'py_checkpoint_reader'\r\n \r\nI found 'py_checkpoint_reader.py' file in tensorflow/python/training folder.\r\nBut inside of 'py_checkpoint_reader.py' file,\r\n\r\nfrom tensorflow.python._pywrap_checkpoint_reader import CheckpointReader\r\n\r\nI cannot find the file '_pywrap_checkpoint_reader. \r\n\r\n", "comments": ["@TaeheeJeong Please provide the exact sequence of commands / steps that you executed before running into the problem along with the tensorflow version.", "    inception_v3.pb and   inception_v3.ckpt are copied into this folder.\ntf.__version__\n'1.15.0'\n\ncommands:\ntensorflow/tensorflow/python/tools$ python freeze_graph.py\n--input_graph=inception_v3.pb --input_binary=true\n--input_checkpoint=inception_v3.ckpt --output_graph=inception_v3_frozen.pb\n--output_node_name=InceptionV3/Predictions/Reshape_1\nTraceback (most recent call last):\n  File \"freeze_graph.py\", line 58, in <module>\n    from tensorflow.python.training import py_checkpoint_reader\nImportError: cannot import name 'py_checkpoint_reader'\n\n\n\nOn Sun, Feb 9, 2020 at 10:30 PM Saduf2019 <notifications@github.com> wrote:\n\n> @TaeheeJeong <https://github.com/TaeheeJeong> Please provide the exact\n> sequence of commands / steps that you executed before running into the\n> problem along with the tensorflow version.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36609?email_source=notifications&email_token=AEFAWISH3XQGYWHI2IWPGR3RCDYA3A5CNFSM4KSG3LK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELHMGNY#issuecomment-583975735>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AEFAWIQO3NF3I57COSR6EZ3RCDYA3ANCNFSM4KSG3LKQ>\n> .\n>\n", "I just tried in tf.1.12.0. the same issue is existed.\n\nOn Mon, Feb 10, 2020 at 9:32 AM Taehee Jeong <taehee77@gmail.com> wrote:\n\n>     inception_v3.pb and   inception_v3.ckpt are copied into this folder.\n> tf.__version__\n> '1.15.0'\n>\n> commands:\n> tensorflow/tensorflow/python/tools$ python freeze_graph.py\n> --input_graph=inception_v3.pb --input_binary=true\n> --input_checkpoint=inception_v3.ckpt --output_graph=inception_v3_frozen.pb\n> --output_node_name=InceptionV3/Predictions/Reshape_1\n> Traceback (most recent call last):\n>   File \"freeze_graph.py\", line 58, in <module>\n>     from tensorflow.python.training import py_checkpoint_reader\n> ImportError: cannot import name 'py_checkpoint_reader'\n>\n>\n>\n> On Sun, Feb 9, 2020 at 10:30 PM Saduf2019 <notifications@github.com>\n> wrote:\n>\n>> @TaeheeJeong <https://github.com/TaeheeJeong> Please provide the exact\n>> sequence of commands / steps that you executed before running into the\n>> problem along with the tensorflow version.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/36609?email_source=notifications&email_token=AEFAWISH3XQGYWHI2IWPGR3RCDYA3A5CNFSM4KSG3LK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELHMGNY#issuecomment-583975735>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AEFAWIQO3NF3I57COSR6EZ3RCDYA3ANCNFSM4KSG3LKQ>\n>> .\n>>\n>\n", "@TaeheeJeong Could you please share the code for \"freeze_graph.py\" so we could help you resolve the issue", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py\n\n\n\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/py_checkpoint_reader.py\n\n\n\n\nOn Tue, Feb 11, 2020 at 3:17 AM Saduf2019 <notifications@github.com> wrote:\n\n> @TaeheeJeong <https://github.com/TaeheeJeong> Could you please share the\n> code for \"freeze_graph.py\" so we could help you resolve the issue\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36609?email_source=notifications&email_token=AEFAWISZXHCPXIQVMQ4B6RDRCKCKZA5CNFSM4KSG3LK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELMBRKI#issuecomment-584587433>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AEFAWIQKC3NZSU2QHOJNJXLRCKCKZANCNFSM4KSG3LKQ>\n> .\n>\n", "@TaeheeJeong \r\nI have tried the same on nightly version and there is no issue, [gist](https://colab.sandbox.google.com/gist/Saduf2019/6d390784507dbb0c6c92f07fef842c88/latest.ipynb#scrollTo=sb7Mr-WqEWg8) for the same.\r\nAs there are dependencies you will have to clone Tensorflow as shown in the gist.", "I tried to run it on gist and got the same error.\n\n[image: image.png]\n\nOn Wed, Feb 12, 2020 at 12:41 AM Saduf2019 <notifications@github.com> wrote:\n\n> @TaeheeJeong <https://github.com/TaeheeJeong>\n> I have tried the same on nightly version and there is no issue, gist\n> <https://colab.sandbox.google.com/gist/Saduf2019/6d390784507dbb0c6c92f07fef842c88/latest.ipynb#scrollTo=sb7Mr-WqEWg8>\n> for the same.\n> As there are dependencies you will have to clone Tensorflow as shown in\n> the gist.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36609?email_source=notifications&email_token=AEFAWIQP34BTABECI3ZN7E3RCOY3BA5CNFSM4KSG3LK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELP5FNI#issuecomment-585093813>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AEFAWIT6PPIS7J45KUB27MLRCOY3BANCNFSM4KSG3LKQ>\n> .\n>\n", "@TaeheeJeong You have check out r.15 branch since you are using TF 1.15\r\n`!git clone -b r1.15 --single-branch https://github.com/tensorflow/tensorflow.git`\r\nOr you may upgrade your TF version to 2.1 since the import\r\n`from tensorflow.python.training import py_checkpoint_reader` was added then.\r\nSee https://github.com/tensorflow/tensorflow/commit/b66e4e833c5aacc31d0feaa629f2d064766a7a0b", "Hi,\r\n\r\nI'm running TF version 1.15.2 and I still get the import error from the same place as @TaeheeJeong\r\n\r\nShould I upgrade to TF 2.1 just to be able run `freeze_graph.py` ? Or is there a way to stay within 1.15.2 and still be able to use the script.\r\n\r\nThanks!\r\n", "same error happens \r\ni've tried tf1.14,  error message is exactly same \r\n\r\ni downloaded several version of tensorflows from git and found 'py_checkpoint_reader.py' somewhere. but here comes another error message, \"No Module named 'tensorflow.python._pywrap_checkpoint_reader;'\" \r\n\r\ni couldn't find the file from anywhere. Is this from bazel or something? not from tensorflow?\r\n(i'm not using bazel now)"]}, {"number": 36608, "title": "Update eigen archive download path of TFLite r2.1 branch", "body": "This PR fixes issue  #36425", "comments": []}, {"number": 36607, "title": "Error when installing: ERROR: tensorflow-2.1.0-cp27-cp27m-macosx_10_15_intel.whl is not a supported wheel on this platform.", "body": "Running Mojave 10.14.6\r\nTensorflow installed from source (via https://www.tensorflow.org/install/source from cloning the repo)\r\nPython 2.7.15\r\nInstalled using pip\r\nBazel version 1.2.1\r\nClang version 11.0.0\r\nIntel Iris 1536 MB\r\n\r\nWhen trying to build tensorflow from this [link](https://www.tensorflow.org/install/source) I was able to get through all steps until actually installing using pip. Everything went relatively smoothly although it took a while (although it did take about 5 hours).\r\n\r\nI was excited to finally get to the end but was disappointed because pip couldn't install the package giving me the following error:\r\n\r\n`ERROR: tensorflow-2.1.0-cp27-cp27m-macosx_10_15_intel.whl is not a supported wheel on this platform.`\r\n\r\nI googled the error and searched through the list of other similar issues but most of the issues were either old or didn't apply to the same os.\r\n\r\nHow do I fix this?", "comments": ["@namedtoaster Could you please provide the exact sequence of commands / steps that you executed before running into the problem [the pip command where you faced the issue]", "@Saduf2019 \r\nThe command I ran was `pip install /tmp/tensorflow_pkg/tensorflow-2.1.0-cp27-cp27m-macosx_10_15_intel.whl\r\n`\r\n\r\nThe commands I ran up to that were:\r\n`./configure` - I selected the default options for the first few steps then selected \"N\" for the follow on Y/N questions\r\n`bazel build //tensorflow/tools/pip_package:build_pip_package`\r\n`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n\r\nAnd then the pip command.", "@namedtoaster According to your message, you are running macOS 10.14 and trying to install a 10.15 pip wheel. Either upgrade your OS (`Mojave 10.14.6` --> `10.15`) or use a `macosx_10_14` pip wheel.", "@namedtoaster  Please let us know if the above comment helps resolve the issue.", "@freedomtan @Saduf2019 thanks I just got the update on my Mac book working on installing it now. Once I can test again I\u2019ll let you know", "Updating to macos 10.5 Catalina fixed my problem. Thanks for the help", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36607\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36607\">No</a>\n", "For your reference, I change the file name to tensorflow-2.1.0-cp37-cp37m-macosx_10_14_x86_64.whl from tensorflow-2.1.0-cp37-cp37m-macosx_10_15_x86_64.whl, and the whl can be installed in OSX 10.14 without any issues.\r\n\r\n[PR 748: BUG: Xcode 11.0 with default -macosx-version-min=10.15 seg faults](http://manao.inria.fr/eigen_tmp/pullrequests/748/)\r\n[Disable AVX on broken xcode versions. See PR 748.](https://gitlab.stce.rwth-aachen.de/stce/eigen-ad/commit/71aa53dd6dfdc497324d9e87f59c4ba820191856)\r\n\r\nPer the above bug reports, the Eigen library works well in OSX 10.14, and AVX of the Eigen library is disabled in OSX 10.15.\r\n\r\nIt could not be recommended to upgrade the OSX to 10.15 for this issue, because the Eigen library in the TensorFlow package could get problems due to a Clang bug about AVX instructions in OSX 10.15.\r\n\r\nThe best practice for AVX enabled in OSX could be as the following.\r\n1. staying in OSX 10.14.\r\n2. adding  '-mmacosx-version-min=10.14' to '--config=opt', when running './configure' to setup the building environment.\r\n3. using 'bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package' to rebuild the source code of TensorFlow.\r\n\r\n```\r\n~/tensorflow/ $ ./configure\r\n.....\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: -mmacosx-version-min=10.14\r\n......\r\n~/tensorflow/ $ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n~/tensorflow/ $ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n~/tensorflow/ $ mv /tmp/tensorflow_pkg/tensorflow-2.1.0-cp37-cp37m-macosx_10_14_x86_64.whl /tmp/tensorflow_pkg/tensorflow-2.1.0-cp37-cp37m-macosx_10_14_x86_64.whl\r\n~/tensorflow/ $ pip install --force-reinstall /tmp/tensorflow_pkg/tensorflow-2.1.0-cp37-cp37m-macosx_10_14_x86_64.whl\r\n```"]}, {"number": 36606, "title": "Terryheo master", "body": "", "comments": []}, {"number": 36605, "title": "fft2d different results between tensorflow and numpy", "body": "Hi,\r\n\r\nI have one question about the fft2d and fftshift. \r\nThe following code shows the similar process (fft2d > fftshift > * filter(LPF) > ifftshift > ifft2d) with numpy and tensorflow.\r\nHowever the result is mush different. And the result of tensorflow is so weird.\r\nCould anyone help me figure out what's wrong in the code?\r\nThanks a lot.\r\n\r\n`    \r\n    img = cv2.imread(\"xxx.png\")\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)\r\n    width = img.shape[1]\r\n    height = img.shape[0]\r\n\r\n    filter = LPF_Butterworth(width, height, 33, 4)\r\n\r\n    # use numpy fft2 ===========\r\n    f = np.fft.fft2(img)\r\n    f = np.fft.fftshift(f)\r\n    f_l = f * filter\r\n    f_l = np.fft.ifftshift(f_l)\r\n    f_l = np.fft.ifft2(f_l)\r\n    f_np = np.real(f_l)\r\n    #===========================\r\n\r\n    input_placeholder = tf.compat.v1.placeholder(tf.float32,\r\n                                       shape=[height, width, 1],\r\n                                       name='input')\r\n\r\n    # use tensorflow fft2d ===========\r\n    tf_filter = tf.convert_to_tensor(filter, dtype=tf.float32)\r\n    tf_filter_comx = tf.expand_dims(tf.complex(tf_filter, tf.zeros(tf_filter.shape)),2)\r\n    fft_org = tf.fft2d(tf.cast(input_placeholder, tf.complex64))\r\n    fft_org = tf.signal.fftshift(fft_org, axes=[0,1])\r\n    fft_filter = fft_org * tf_filter_comx\r\n    fft_filter = tf.ifft2d(tf.signal.ifftshift(fft_filter, axes=[0,1]))\r\n    fft_filter = tf.real(fft_filter)\r\n    #================================\r\n\r\n    sess = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(tf.local_variables_initializer())\r\n\r\n    with sess.as_default():\r\n        feed_dict = {input_placeholder: np.expand_dims(img, axis=2)}\r\n        f_tf = sess.run(fft_filter, feed_dict=feed_dict)\r\n\r\n    f_np = cv2.normalize(f_np, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\r\n    f_tf = cv2.normalize(f_tf, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\r\n    cv2.imshow(\"numpy\", f_np)\r\n    cv2.imshow(\"tensorflow\", f_tf)\r\n    cv2.waitKey()\r\n`\r\nresult: https://i.stack.imgur.com/0vLSd.png\r\n\r\n[](https://i.stack.imgur.com/0vLSd.png)\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 2080Ti 11G\r\n", "comments": ["@why702, I tried replicating the issue but getting different error, looks like code is incomplete. Can you provide complete code. Please see the gist [here](https://colab.research.google.com/gist/gadagashwini/f257c390760219303aabf242c6f5299c/untitled383.ipynb). Thanks!", "@gadagashwini  Sorry, I can't edit your gist.\r\nAnd so here is the lost code.\r\nThanks.\r\n\r\n```\r\nimport numpy as np\r\nimport cv2\r\nimport math\r\nimport tensorflow as tf\r\n\r\ndef LPF_Butterworth(width, height, kRadius, kOrder):\r\n    fltDst = np.empty([height, width])\r\n    cx = width / 2\r\n    cy = height / 2\r\n    for row in range(height):\r\n        for col in range(width):\r\n            kDistance = math.sqrt((col - cx) ** 2 + (row - cy) ** 2)\r\n            fltDst[row][col] = 1 / (1 + pow((kDistance / kRadius),\r\n                                            (2 * kOrder)))\r\n    return fltDst\r\n```\r\n", "Was able to replicate the issue with Tf1.15. \r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/5b034e9985b110e692fbc4695ac70b35/untitled383.ipynb). Thanks!", "\r\n\r\n\r\n> Was able to replicate the issue with Tf1.15.\r\n> Please find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/5b034e9985b110e692fbc4695ac70b35/untitled383.ipynb). Thanks!\r\n\r\nHi @jvishnuvardhan,\r\nIs something wrong in my code?\r\n", "I have been trying to implement the [Mosse ](https://www.cs.colostate.edu/~vision/publications/bolme_cvpr10.pdf) tracker in Tensorflow and have run into the same issue. Any updates? ", "[fft2d](https://www.tensorflow.org/api_docs/python/tf/signal/fft2d) operates on the innermost 2 dimensions. In your case, `input_placeholder` has shape `[width, height, 1]`, which is probably not what you want (it's also inconsistent with img). If I change the shape to `[width, height]` and remove the calls to `expand_dims` throughout, then the two images become visually indistinguishable.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36605\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36605\">No</a>\n"]}, {"number": 36604, "title": "Small docstring fix. This make the example work.", "body": "The case is important.", "comments": []}, {"number": 36602, "title": "GPU Ram exhaustion TF 2.1 CUDA 10.1", "body": "**System information**\r\n- Have I written custom code : yes, although very minimal\r\n- OS Platform and Distribution: Linux Ubuntu 19.10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0  (also tested 2.0.0)\r\n- Python version: 3.6.10 anaconda\r\n- CUDA/cuDNN version: 10.1.243\r\n- GPU model and memory: Nvidia RTX 2080 7979MiB\r\n\r\n**Describe the current behavior**\r\nSince updating to 2.1.0, the following simple commands cause memory exhaustion and\r\nall further inputs to fail with cuDNN errors:\r\n\r\n```python\r\nX = tf.random.normal((16, 128, 16), dtype=tf.float32)\r\nlayer_norm = tf.keras.layers.LayerNormalization()\r\nXn = layer_norm(X)\r\n```\r\nwhich causes\r\ncuDNN launch failure : input shape ([1,2048,16,1]) [Op:FusedBatchNormV3]\r\n(BatchNormalization has the same issue, unsurprisingly)\r\n\r\nInitially, I thought this was a CUDA/CuDNN installation issue, but if I add\r\n\r\n```python\r\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\r\n    tf.config.experimental.set_memory_growth(gpu, False)\r\n```\r\n\r\nthe problem does not occur.\r\n\r\nThis issue does not happen if I change TF or CUDA versions TF 2.0.0 or CUDA 10.0 \r\n-- it is specific to 2.1.0 + 10.1.  (It is still present in the current nightly).\r\n\r\nI have not been able to replicate this problem in, say, colab, but I don't know what version\r\nof Cuda is in use there.\r\n\r\nTest cases:\r\n  * TF 2.1.0 Cuda 10.1.243 CuDNN(-Dev) 7.6.5.42-1+cuda10.1 TensorRT 6.0.1.5+cuda10.1\r\n     * Memory Growth: True ->  Failure with CuDNN start error\r\n     * Memory Growth: False -> Success\r\n  * TF 2.0.0 Cuda 10.1.243 CuDNN(-Dev) 7.6.5.42-1+cuda10.1 TensorRT 6.0.1.5+cuda10.1\r\n     * Memory Growth: True ->  Success\r\n     * Memory Growth: False -> Success\r\n  * TF 2.1.0 Cuda 10.0.130 CuDNN(-Dev) 7.6.5.32-1+cuda10.0 TensorRT 7.0.0.11\r\n     * Memory Growth: True ->  Success\r\n     * Memory Growth: False -> Success\r\n  *  TF 2.0.0 Cuda 10.0.130 CuDNN(-Dev) 7.6.5.32-1+cuda10.0 TensorRT 7.0.0.11\r\n     * Memory Growth: True ->  Success\r\n     * Memory Growth: False -> Success\r\n  *  TF 2.0.0 Cuda 10.0.130 CuDNN(-Dev) 7.6.5.32-1+cuda10.0 TensorRT 7.0.0.11\r\n     * Memory Growth: True ->  Success\r\n     * Memory Growth: False -> Success\r\n  *  TF 2.1.0 Cuda 10.0.130 CuDNN(-Dev) 7.6.5.32-1+cuda10.0 TensorRT 6.0.1.5\r\n     * Memory Growth: True ->  Success\r\n     * Memory Growth: False -> Success\r\n\r\nNote TensorRT 7 is not available for CUDA 10.1, hence the last test with v6 of TensorRT.\r\nI tried to test on CUDA 10.2 by building from source, but got bogged down with blaze\r\ninstallation conflicts.  I have reinstalled the failing set of libraries + TF from scratch more than\r\nonce as a sanity check, and it has re-occurred each time.\r\n\r\n**Describe the expected behavior**\r\n  It should be possible to execute LayerNorm/BatchNorm.", "comments": ["Sorry, I flipped True and False for memory growth in the above.  Aagh.\r\n\r\nThat should be:\r\n\r\n```python\r\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n```\r\nfixes the problem for 2.1.0/10.1, and\r\n\r\n* TF 2.1.0 Cuda 10.1.243 CuDNN(-Dev) 7.6.5.42-1+cuda10.1 TensorRT 6.0.1.5+cuda10.1\r\n    * Memory Growth: False -> Failure with CuDNN start error\r\n    * Memory Growth: True -> Success\r\n", "Yes. Limiting gpu growth by setting flag as `true` is correct approach. Closing this issue since its resolved. Thanks!", "Verified that the TF 2.1 with CUDA 10.0 was only passing because it was using the CPU.\r\n\r\nHowever: also verified that TF 2.0 + CUDA 10.0 does work without needing to limit GPU growth.  So needing to limit memory *is* new behavior in 2.1 + CUDA 10.1."]}, {"number": 36601, "title": "ERROR:root: Internal Python error in the inspect module while installing Tensorflow", "body": "<em>I have installed TensorFlow 2.0 with CUDA 10.1, cuDNN SDK = 7.6 and all the different libraries such as pandas, sci-kit learn, Keras, NumPy, Gensim etc. This error is probably due to new TensorFlow version. When I try to confirm it by running this confirmation code. I am getting an error. Kindly help.\r\n\r\n**System information**\r\n- Window 10\r\n- TensorFlow GPU version: 2.0\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: Geforce 940M and 12 GB\r\n\r\n\r\n**Command I tried to confirm installation**\r\nimport sys\r\nimport tensorflow.keras\r\nimport pandas as pd\r\nimport sklearn as sk\r\nimport tensorflow as tf\r\nprint(f\"Tensor Flow Version: {tf.__version__}\")\r\nprint(f\"Keras Version: {tensorflow.keras.__version__}\")\r\nprint()\r\nprint(f\"Python {sys.version}\")\r\nprint(f\"Pandas {pd.__version__}\")\r\nprint(f\"Scikit-Learn {sk.__version__}\")\r\nprint(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\r\n\r\n**ERROR:root: Internal Python error in the inspect module while installing Tensorflow**\r\n\r\n`Traceback (most recent call last):\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-5-a5e23077879c>\", line 8, in <module>\r\n    print(f\"Tensor Flow Version: {tf.__version__}\")\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'AttributeError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 45, in <module>\r\n    from . _api.v2 import compat\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\", line 23, in <module>\r\n    from . import v1\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\", line 40, in <module>\r\n    from . import experimental\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\experimental\\__init__.py\", line 11, in <module>\r\n    from tensorflow.python.ops.control_flow_v2_toggles import output_all_intermediates\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_v2_toggles.py\", line 24, in <module>\r\n    from tensorflow.python.ops import control_flow_util_v2\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_util_v2.py\", line 28, in <module>\r\n    from tensorflow.python.keras.engine import base_layer_utils\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\__init__.py\", line 27, in <module>\r\n    from tensorflow.python.keras import applications\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\applications\\__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras import engine\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\__init__.py\", line 23, in <module>\r\n    from tensorflow.python.keras.engine.base_layer import Layer\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 56, in <module>\r\n    from tensorflow.python.keras.saving.saved_model import layer_serialization\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\__init__.py\", line 20, in <module>\r\n    from tensorflow.python.keras.saving.hdf5_format import load_attributes_from_hdf5_group\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\", line 32, in <module>\r\n    from tensorflow.python.keras.utils import conv_utils\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\__init__.py\", line 38, in <module>\r\n    from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\multi_gpu_utils.py\", line 22, in <module>\r\n    from tensorflow.python.keras.engine.training import Model\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 42, in <module>\r\n    from tensorflow.python.keras import metrics as metrics_module\r\n  File \"C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 34, in <module>\r\n    from tensorflow.python.keras.engine.base_layer import Layer\r\nImportError: cannot import name 'Layer' from 'tensorflow.python.keras.engine.base_layer' (C:\\Users\\Abdul\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py)\r\n---------------------------------------------------------------------------`", "comments": ["@abdulwahabqurashi, Suspecting Tensorflow installed incorrectly. Can you please uninstall Tensorflow and Python. and reinstall Tensorflow and Python. Let us know how it progresses. Thanks!", "@abdulwahabqurashi, Is this still an issue?", "Issue fixed after uninstall and install an older version. Thanks for your help", "@abdulwahabqurashi, Glad that its working now. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36601\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36601\">No</a>\n", "@gadagashwini  which version you install ?", "@khaledxdelahkk , can you share your error log and system information. Thanks", "@gadagashwini , i'm so sory for i can't reply early i'm still have this problem\r\n\r\n(tensorflow_cpu) C:\\Users\\mvenv\\Desktop\\object detection>python\r\nPython 3.7.7 (default, Apr 15 2020, 05:09:04) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\mvenv\\anaconda3\\envs\\tensorflow_cpu\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine faile\r\n\r\nMy system information:\r\nprocessor: Intel(R)Core(TM) i3 CPU M370 @2.40Ghz 2.40 GHz\r\nRam: 8GB\r\nsystem type: 64-bit Operting System,x64-based processor\r\n\r\n\r\n\r\n\r\n\r\n\r\n"]}, {"number": 36600, "title": "Tensorflow 2.1 BERt embeddings - Variable is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key", "body": "I'm trying to load the [following](https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1) BERt presaved model from tensorflow hub. As per the previously mentioned page, this is the code that I'm trying to execute\r\n\r\n```\r\nmax_seq_length = 128  # Your choice here.\r\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\r\n                                       name=\"input_word_ids\")\r\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\r\n                                   name=\"input_mask\")\r\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\r\n                                    name=\"segment_ids\")\r\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1\",\r\n                            trainable=True)\r\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\r\n```\r\nHowever, as soon as I get to execute the hub.KerasLayer, I'm getting the following exception.\r\n\r\n\r\n```\r\nIn [10]: bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1\", \r\n    ...:                             trainable=True)                                                                                                                                                        \r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-10-f1fd8e265590> in <module>\r\n      1 bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1\",\r\n----> 2                             trainable=True)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py in __init__(self, handle, trainable, arguments, **kwargs)\r\n    111       for v in self._func.trainable_variables:\r\n    112         self._add_existing_weight(v, trainable=True)\r\n--> 113       trainable_variables = set(self._func.trainable_variables)\r\n    114     else:\r\n    115       trainable_variables = set()\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in __hash__(self)\r\n   1087   def __hash__(self):\r\n   1088     if ops.Tensor._USE_EQUALITY and ops.executing_eagerly_outside_functions():  # pylint: disable=protected-access\r\n-> 1089       raise TypeError(\"Variable is unhashable if Tensor equality is enabled. \"\r\n   1090                       \"Instead, use tensor.experimental_ref() as the key.\")\r\n   1091     else:\r\n\r\nTypeError: Variable is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\r\n```\r\n\r\nWhat am I missing? I'm running all of the above on macOS 10.15 and TF 2.1\r\n\r\n", "comments": ["The above code runs without any problem on a Win 10 machine", "@TheGlobalist \r\nI have tried on colab with TF version 2.1.0 and I am not seeing any error message. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d6f1c53c38aa2f12077ecfa39a0223da/untitled633.ipynb).Thanks!", "> @TheGlobalist\r\n> I have tried on colab with TF version 2.1.0 and I am not seeing any error message. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d6f1c53c38aa2f12077ecfa39a0223da/untitled633.ipynb).Thanks!\r\n\r\nI'll try this in the evening as I don't have the macOS machine with me now. Thank you! \ud83d\udc4d ", "> @TheGlobalist\r\n> I have tried on colab with TF version 2.1.0 and I am not seeing any error message. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d6f1c53c38aa2f12077ecfa39a0223da/untitled633.ipynb).Thanks!\r\n\r\nI confirm that this on my local macOS machine doesn't work and gives the above error", "Also: if I downgrade to 1.x, the error doesn't happen", "@TheGlobalist I am not running into any error if I am running the code in [colab](https://colab.research.google.com/gist/gowthamkpr/340ef323a3236ca0b3ef188d8ccc7d97/untitled633.ipynb). I am using `hub 0.7` and `Tensorflow 2.1\r\n`\r\n", "> @TheGlobalist I am not running into any error if I am running the code in [colab](https://colab.research.google.com/gist/gowthamkpr/340ef323a3236ca0b3ef188d8ccc7d97/untitled633.ipynb). I am using `hub 0.7` and `Tensorflow 2.1 `\r\n\r\nSorry, maybe I'm missing the point... shouldn't TF be able to run everywhere?\r\nIf so, why are we talking about Colab if I'm asking for macOS?\r\n", "Can you try it with tensorflow 2.2.0rc1 and let me know if the issue still persists. I tried to reproduce it on ubuntu 18.04 and am not running into any error. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue as it has been inactive for more than 2 weeks. Please add additional comments for us to open this issue again. Thanks!"]}, {"number": 36599, "title": "NMT with attention", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@arshren  Could you please update the template and share the issue faced,", "@arshren Please respond to the above comment", "@arshren Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 36598, "title": "installation issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2.0\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-d6579f534729>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hello\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n \r\n**Describe the problem**\r\n\r\n**Provide the exac\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Jyothif\r\nCould you please check this [link](https://github.com/tensorflow/tensorflow/issues/36167) on a similar issue and let us know if it helps. Thanks!", "@Saduf2019 \r\nThank you for quick response, It got solved and I got another error, while running the code, Please help me in this \r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-29-297258a82b16> in <module>\r\n----> 1 classifier.fit(X_train, y_train, batch_size=10, epochs=100)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n    950             sample_weight=sample_weight,\r\n    951             class_weight=class_weight,\r\n--> 952             batch_size=batch_size)\r\n    953         # Prepare validation data.\r\n    954         do_validation = False\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\r\n    787                 feed_output_shapes,\r\n    788                 check_batch_axis=False,  # Don't enforce the batch size.\r\n--> 789                 exception_prefix='target')\r\n    790 \r\n    791             # Generate sample-wise weight values given the `sample_weight` and\r\n\r\n~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    136                             ': expected ' + names[i] + ' to have shape ' +\r\n    137                             str(shape) + ' but got array with shape ' +\r\n--> 138                             str(data_shape))\r\n    139     return data\r\n    140 \r\n\r\nValueError: Error when checking target: expected dense_3 to have shape (6,) but got array with shape (1,)\r\n\r\n\u200b\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36598\">No</a>\n", "@Jyothif As the issue with installation is resolved, please create a new issue for the new error faced by you."]}, {"number": 36597, "title": "raise error in tf.math.polyval for non-list coeffs", "body": "resolves #34947 ", "comments": ["@rushabh-v Can you please fix the build failures? Thanks!", "Didn't know the system of TF of using `test.Testcase` for tests. corrected the tests. I think it's okay now. @mihaimaruseac Can you please review it again? "]}, {"number": 36596, "title": "tf.keras GradientTape: get gradient with respect to input", "body": "Tensorflow 2.1\r\n\r\nI want to get the gradients with respect to the input instead of the gradient with respect to the trainable weights. I adjust the example from https://www.tensorflow.org/guide/keras/train_and_evaluate to\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nassert len(physical_devices) > 0, 'Not enough GPU hardware devices available'\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\ndef loss_fun(y_true, y_pred):\r\n    loss = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\r\n    return loss\r\n\r\n# Create a dataset\r\nx = np.random.rand(10, 180, 320, 3).astype(np.float32)\r\ny = np.random.rand(10, 1).astype(np.float32)\r\ndataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(1)\r\n\r\n# Create a model\r\nbase_model = tf.keras.applications.MobileNet(input_shape=(180, 320, 3), weights=None, include_top=False)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\r\noutput = tf.keras.layers.Dense(1)(x)\r\nmodel = tf.keras.models.Model(inputs=base_model.input, outputs=output)\r\n\r\nfor input, target in dataset:\r\n\r\n    for iteration in range(400):\r\n        with tf.GradientTape() as tape:\r\n            # Run the forward pass of the layer.\r\n            # The operations that the layer applies\r\n            # to its inputs are going to be recorded\r\n            # on the GradientTape.\r\n            prediction = model(input, training=False)  # Logits for this minibatch\r\n\r\n            # Compute the loss value for this minibatch.\r\n            loss_value = loss_fun(target, prediction)\r\n\r\n        # Use the gradient tape to automatically retrieve\r\n        # the gradients of the trainable variables with respect to the loss.\r\n        grads = tape.gradient(loss_value, model.inputs)\r\n        print(grads)  # output: [None]\r\n        # Run one step of gradient descent by updating\r\n        # the value of the variables to minimize the loss.\r\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n        optimizer.apply_gradients(zip(grads, model.inputs))\r\n\r\n        print('Iteration {}'.format(iteration))\r\n```\r\n\r\n\r\nHowever, this doesnot work, because grads = tape.gradient(loss_value, model.inputs) returns [None]. Is this intended behaviour or not? If yes, what is the recommended way to get the gradients with respect to the input?", "comments": ["@dmus \r\nI tried to reproduce the issue with TF-GPU  2.1.0 ,2.2.0-dev20200210 .It returns [None] and also I am seeing the error message `ValueError: No gradients provided for any variable: ['input_1:0'].Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a60963d39257237126a70cb305e73662/untitled632.ipynb). Is this the expected behavior?. Thanks!", "@ravikyram I updated the gist. It turns out for one input this can be solved by converting the input explicitly to a tf.Variable and then use tape.watch() explicitly for the case with one input. However, it does not work when using multiple inputs\r\n\r\n```\r\nfor input, target in dataset:\r\n    image = tf.Variable(input[0])\r\n    for iteration in range(400):\r\n        with tf.GradientTape() as tape:\r\n            tape.watch(image)\r\n            # Run the forward pass of the layer.\r\n            # The operations that the layer applies\r\n            # to its inputs are going to be recorded\r\n            # on the GradientTape.\r\n            prediction = model(input, training=False)  # Logits for this minibatch\r\n\r\n            # Compute the loss value for this minibatch.\r\n            loss_value = loss_fun(target, prediction)\r\n\r\n        # Use the gradient tape to automatically retrieve\r\n        # the gradients of the trainable variables with respect to the loss.\r\n        grads = tape.gradient(loss_value, image)\r\n        print(grads)  # output: [None]\r\n        # Run one step of gradient descent by updating\r\n        # the value of the variables to minimize the loss.\r\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n        optimizer.apply_gradients(zip([grads], [image]))\r\n\r\n        print('Iteration {}'.format(iteration))\r\n```\r\n\r\nSo the problem is there when using multiple inputs, seems like this is a bug?", "As a workaround: I can get this example working when I convert the image to a tensor for computing the gradient and to a tf.Variable for updating. \r\n\r\n```\r\nfor input, target in dataset:\r\n    image0 = tf.convert_to_tensor(input[0])\r\n    image1 = tf.convert_to_tensor(input[1])\r\n\r\n    image0_var = tf.Variable(image0)\r\n\r\n    for iteration in range(400):\r\n        with tf.GradientTape() as tape:\r\n            tape.watch(image0)\r\n            # Run the forward pass of the layer.\r\n            # The operations that the layer applies\r\n            # to its inputs are going to be recorded\r\n            # on the GradientTape.\r\n            # prediction = model(input, training=False)  # Logits for this minibatch\r\n            prediction = model([input[0], input[1]])\r\n            print('prediction: {}'.format(prediction))\r\n            # Compute the loss value for this minibatch.\r\n            # loss_value = loss_fun(target, prediction)\r\n\r\n        # Use the gradient tape to automatically retrieve\r\n        # the gradients of the trainable variables with respect to the loss.\r\n        grads = tape.gradient(prediction, [input[0]])\r\n        print(grads)  # output: [None]\r\n        # Run one step of gradient descent by updating\r\n        # the value of the variables to minimize the loss.\r\n\r\n        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n        optimizer.apply_gradients(zip(grads, [image0_var]))\r\n        #optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n\r\n        print('Iteration {}'.format(iteration))\r\n\r\n```", "I recently ran into this issue and was able to confirm that @dmus 's workaround seems to solve it for me (TF 2.1.0, stable, compiled from source, GPU).  Going to give my thoughts for the benefit of the next person to come across the issue of wanting to compute gradients with respect to inputs.\r\n\r\nIn my testing, I'm not sure if this is bug, but it feels unintuitive.  Suppose we have a Model `model` and Variable `v` and we execute  \r\n```\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(model.input)\r\n    model_vals = model(v)\r\n```\r\nThe problem is that the call `tape.gradient(model_vals, model.variables)` generally works as expected to get the gradients with respect to the variables of the model, but `tape.gradient(model_vals, model.input)` does not work like one would expect.  The sources parameter of `GradentTape.gradient` seems to expect variables, but the attribute `model.input` is a symbolic Tensor.  For whatever reason, feeding this variable into either `tape.watch` or `tape.gradient` fails silently.  The default behavior of `tape.gradient` if it can't find a path of differentiation is for it to return `None`, so this is why this value is returned.\r\n\r\nThinking deeper about this, it seems unreasonable to assume that we can use `model.input` as the object to differentiate against because the same model can be used multiple times within the same `GradientTape` session.  If composed together (`model_vals = model(model(v))`), there would be multiple paths from the target to the input tensors, making the gradient ill-defined.\r\n\r\nI think it would be nice for one of those methods to throw a warning if called in this way, considering that it's easy to conflate the idea of input with the variable fed into the input.  But perhaps this would be harder than it seems.\r\n\r\nBelow I have a simple example that illustrates what's happening.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.layers as layers\r\n\r\ntf.random.set_seed(0)\r\n\r\nmodel_in = keras.Input(shape=(2,))\r\nmodel_d1 = layers.Dense(64, activation='relu')(model_in)\r\nmodel_d2 = layers.Dense(64, activation='relu')(model_d1)\r\nmodel_d3 = layers.Dense(1)(model_d2)\r\nmodel = keras.Model(inputs=[model_in], outputs=[model_d3])\r\nmodel.summary()\r\n\r\nv = tf.Variable([[0.1, 0.2]], name='v_var')\r\n\r\n####################################################\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(model.input)\r\n    model_vals = model(v)\r\nprint(len(tape.watched_variables())) \r\n# 7      ( = (1 kernel + 1 bias) * (3 dense layers) + (1 v_var) )\r\nprint(model.input)\r\n# Tensor(\"input_1:0\", shape=(None, 2), dtype=float32)\r\nmodel_grad = tape.gradient(model_vals, model.input)\r\nprint(model_grad)\r\n# None\r\n####################################################\r\n\r\n####################################################\r\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\r\n    tape.watch(model.input)\r\n    model_vals = model(v)\r\nprint(len(tape.watched_variables())) \r\n# 0      (model.input isn't a variable so it is ignored)\r\n####################################################\r\n\r\n####################################################\r\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\r\n    tape.watch(v)\r\n    model_vals = model(v)\r\nprint(len(tape.watched_variables())) \r\n# 1\r\nprint(v)\r\n# <tf.Variable 'v_var:0' shape=(1, 2) dtype=float32, numpy=array([[0.1, 0.2]], dtype=float32)>\r\nmodel_grad = tape.gradient(model_vals, v)\r\nprint(model_grad)\r\n# tf.Tensor([[-0.04392226 -0.06807809]], shape=(1, 2), dtype=float32)\r\n####################################################\r\n```", "any update on this? I am also not getting any gradients even after converting input as a Variable and watching that variable using tape.watch\r\n\r\nI am trying to get the gradient with respect to the input but I am getting None. tried using https://stackoverflow.com/questions/57759635/get-gradients-with-keras-tensorflow-2-0 this answer but it is giving None. I have given total code below. You can check and execute the same code in https://colab.research.google.com/drive/1pHzLA7kHB1AGgaILCnv012ZlgNVPIQRW colab notebook. \r\n\r\n```import numpy as np\r\nimport pandas as pd\r\nimport random as rn\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Input, Embedding\r\nfrom tensorflow.keras.models import Model\r\n\r\ndef get_model():\r\n    input_layer = Input(shape=(24,), name=\"input_layer\")\r\n    ##i am initilizing randomly. But you can use predefined embeddings. \r\n    x_embedd = Embedding(input_dim=13732, output_dim=100, input_length=24, mask_zero=True, \r\n                        embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23),\r\n                         name=\"Embedding_layer\")(input_layer)\r\n    \r\n    x_lstm = LSTM(units=20, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \r\n                 kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\r\n                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\r\n                 bias_initializer=tf.keras.initializers.zeros(), name=\"LSTM_layer\")(x_embedd)\r\n    \r\n    x_out = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),\r\n                  name=\"output_layer\")(x_lstm)\r\n    \r\n    basic_lstm_model = Model(inputs=input_layer, outputs=x_out, name=\"basic_lstm_model\")\r\n    \r\n    return basic_lstm_model\r\n\r\nbasic_lstm_model = get_model()\r\n\r\ntemp_features = np.random.randint(low=1, high=13732, size=(10,24))\r\n\r\ndef get_gradient(model, x):\r\n    x_tensor = tf.Variable(tf.convert_to_tensor(x, dtype=tf.float32))\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(x_tensor)\r\n        loss = model(x_tensor)\r\n    #print(tape.watched_variables())\r\n    grads = tape.gradient(loss, x_tensor)\r\n    return grads\r\n\r\nprint(get_gradient(basic_lstm_model, temp_features))```", "Got it. It is because of the Embedding layer. The gradient is not flowing through the embedding layer. ", "Can you get the gradient of the input by calling:\r\n```\r\ntape.gradient(loss, input)\r\n```\r\ninstead of:\r\n```\r\ntape.gradient(loss, model.input)\r\n```\r\n?", "I have a similar issue trying to compute gradients with tf.gradients of the loss wrt the inputs. Just returns None in all cases.\r\n\r\nInterestingly this works perfectly in older versions of tensorflow (< 2.0) - so they've broken something. This is an extremely annoying issue if trying to work within tf.keras's train functions, but from reading above it seems that even if you were to write your own this issue would persist.\r\n\r\nCan't this behaviour be reverted to how it was pre-2.0, where you could calculate the gradients however you liked, without arbitrary restrictions? \r\n\r\nEDIT:\r\n\r\nDuh, obviously this is because 2.0 enforced eager execution, if you disable this the old-style gradient calculations seem to work fine. (Though apparently this is meant to issue a runtime warning, which it did not.)\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "this simple example fails as well\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128),\r\n  tf.keras.layers.ReLU(),\r\n  tf.keras.layers.Dense(10)\r\n])\r\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\nimages, labels = next(iter(train_ds))\r\n\r\nwith tf.GradientTape() as tape:\r\n  tape.watch(model.layers[0].output)\r\n  predictions = model(image)\r\n  loss = loss_object(labels, predictions)\r\n\r\ngrads = tape.gradient(loss, model.layers[0].output)\r\nprint(grads)\r\n```", "@UdiBhaskar Did you find a work-around to make the gradient flow through the embedding layer?", "I found a solution to my problem. For future references and in hope that it would help someone in a similar need as me - \r\n\r\nThe error that I was committing - Converting the loss value and the predictions from the model to numpy array. **It is imperative that you don't convert them to numpy arrays at any point in time before computing the gradients.** In my case, I was using a custom loss function which converted the predicted values to numpy arrays for computations.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36596\">No</a>\n", "> Got it. It is because of the Embedding layer. The gradient is not flowing through the embedding layer.\r\n\r\nI am \r\n\r\n> I recently ran into this issue and was able to confirm that @dmus 's workaround seems to solve it for me (TF 2.1.0, stable, compiled from source, GPU). Going to give my thoughts for the benefit of the next person to come across the issue of wanting to compute gradients with respect to inputs.\r\n> \r\n> In my testing, I'm not sure if this is bug, but it feels unintuitive. Suppose we have a Model `model` and Variable `v` and we execute\r\n> \r\n> ```\r\n> with tf.GradientTape() as tape:\r\n>     tape.watch(model.input)\r\n>     model_vals = model(v)\r\n> ```\r\n> \r\n> The problem is that the call `tape.gradient(model_vals, model.variables)` generally works as expected to get the gradients with respect to the variables of the model, but `tape.gradient(model_vals, model.input)` does not work like one would expect. The sources parameter of `GradentTape.gradient` seems to expect variables, but the attribute `model.input` is a symbolic Tensor. For whatever reason, feeding this variable into either `tape.watch` or `tape.gradient` fails silently. The default behavior of `tape.gradient` if it can't find a path of differentiation is for it to return `None`, so this is why this value is returned.\r\n> \r\n> Thinking deeper about this, it seems unreasonable to assume that we can use `model.input` as the object to differentiate against because the same model can be used multiple times within the same `GradientTape` session. If composed together (`model_vals = model(model(v))`), there would be multiple paths from the target to the input tensors, making the gradient ill-defined.\r\n> \r\n> I think it would be nice for one of those methods to throw a warning if called in this way, considering that it's easy to conflate the idea of input with the variable fed into the input. But perhaps this would be harder than it seems.\r\n> \r\n> Below I have a simple example that illustrates what's happening.\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> import tensorflow.keras as keras\r\n> import tensorflow.keras.layers as layers\r\n> \r\n> tf.random.set_seed(0)\r\n> \r\n> model_in = keras.Input(shape=(2,))\r\n> model_d1 = layers.Dense(64, activation='relu')(model_in)\r\n> model_d2 = layers.Dense(64, activation='relu')(model_d1)\r\n> model_d3 = layers.Dense(1)(model_d2)\r\n> model = keras.Model(inputs=[model_in], outputs=[model_d3])\r\n> model.summary()\r\n> \r\n> v = tf.Variable([[0.1, 0.2]], name='v_var')\r\n> \r\n> ####################################################\r\n> with tf.GradientTape() as tape:\r\n>     tape.watch(model.input)\r\n>     model_vals = model(v)\r\n> print(len(tape.watched_variables())) \r\n> # 7      ( = (1 kernel + 1 bias) * (3 dense layers) + (1 v_var) )\r\n> print(model.input)\r\n> # Tensor(\"input_1:0\", shape=(None, 2), dtype=float32)\r\n> model_grad = tape.gradient(model_vals, model.input)\r\n> print(model_grad)\r\n> # None\r\n> ####################################################\r\n> \r\n> ####################################################\r\n> with tf.GradientTape(watch_accessed_variables=False) as tape:\r\n>     tape.watch(model.input)\r\n>     model_vals = model(v)\r\n> print(len(tape.watched_variables())) \r\n> # 0      (model.input isn't a variable so it is ignored)\r\n> ####################################################\r\n> \r\n> ####################################################\r\n> with tf.GradientTape(watch_accessed_variables=False) as tape:\r\n>     tape.watch(v)\r\n>     model_vals = model(v)\r\n> print(len(tape.watched_variables())) \r\n> # 1\r\n> print(v)\r\n> # <tf.Variable 'v_var:0' shape=(1, 2) dtype=float32, numpy=array([[0.1, 0.2]], dtype=float32)>\r\n> model_grad = tape.gradient(model_vals, v)\r\n> print(model_grad)\r\n> # tf.Tensor([[-0.04392226 -0.06807809]], shape=(1, 2), dtype=float32)\r\n> ####################################################\r\n> ```\r\n@kphawkins \r\nI am trying to find out the gradient of the cross entropy loss and some custom loss functions with respect to the input image .\r\nI followed the same procedure you mentioned ,still the gradient value returning None.Please go through the code  below.\r\n\r\nfrom keras.applications.vgg19 import preprocess_input\r\nfrom keras.applications.vgg19 import decode_predictions\r\nfrom keras.preprocessing.image import img_to_array\r\nfrom keras.preprocessing.image import load_img\r\nmodel = vgg19.VGG19(weights=\"imagenet\", include_top=True)\r\nimage = load_img('/content/dog.jpg', target_size=(224, 224))\r\nimage = img_to_array(image)\r\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\r\n# prepare the image for the VGG model\r\nimage = preprocess_input(image)\r\nyhat = model.predict(image)\r\n#print(yhat)\r\nlabel = decode_predictions(yhat)\r\nlabel = label[0][0]\r\n# print the classification\r\ncross_entropy_loss=-(math.log(label[2]))\r\nprint(cross_entropy_loss)\r\n# Cross_entropy loss returnining  0.035864354427793455\r\nimage=tf.convert_to_tensor(image)\r\ncross_entropy_loss=tf.convert_to_tensor(cross_entropy_loss)\r\nimage=tf.Variable(image)\r\nce=tf.Variable(cross_entropy_loss)\r\nwith tf.GradientTape() as tape:\r\n   tape.watch(image)\r\n   loss=ce\r\n   #print(loss)\r\nprint(len(tape.watched_variables()))\r\nmodel_grad = tape.gradient(loss,image)\r\nprint(model_grad)\r\nIt is still returning 1 None \r\nIs there any way to solve this issue?", "Does this being solved? I trying to implement a GradCAM which the model serve on tensorflow serving outputs a prediction and final_conv outputs, however i got None when use the tape.gradient, is it possible to compute the gradients which the model serving on the Tensorflow Serving/TritonInferenceServer?", "> I found a solution to my problem. For future references and in hope that it would help someone in a similar need as me -\r\n> \r\n> The error that I was committing - Converting the loss value and the predictions from the model to numpy array. **It is imperative that you don't convert them to numpy arrays at any point in time before computing the gradients.** In my case, I was using a custom loss function which converted the predicted values to numpy arrays for computations.\r\n\r\nMay I ask one question about the issue of converting the loss value and the predictions to numpy arrays. First, I would like to explain my problem, I have created a step function in which I pass one batch at a time of the data to my NN model. After applying the prediction I compute the jacobian of a part of the output and here I do get a non-null jacobian but when I compute the gradient of the loss w.r.t. the model trainable parameters I get none values.\r\n\r\nI tried to print the predictions output values before applying the gradient on the loss function and I get this for the prediction output \"pred\"\r\n\r\ntf.Tensor[... values...], shape=(10, 35), dtype=float64)\r\n\r\nI tried to make sure that my prediction output is not in numpy array format, therefore I wrote this command after the prediction\r\n\r\n`pred_T = tf.Variable(tf.convert_to_tensor(pred))`\r\n\r\nwhen I print pred_T, I get this:\r\n\r\n<tf.Variable 'Variable:0' shape=(10, 35) dtype=float64, numpy=\r\narray([...values...])><dtype: 'float64'>\r\n\r\nI don't understand why pred_T is still in numpy format !!\r\n\r\nOf course the loss function is computed using pred or pred_T therefore it is important to make sure it is not in numpy format but I am not able to figure out the way of doing that. Does anybody has a solution for that.\r\n\r\nIn case this my step function code\r\n\r\n`\r\ndef step(self,t,y):\r\n        \r\n        Nu = self.Nu\r\n        Np = self.Np\r\n        a_tf = y[:,0:Nu]\r\n        b_tf = y[:,Nu:]\r\n        x_in = tf.Variable(tf.convert_to_tensor(t))\r\n        y_out = tf.Variable(tf.convert_to_tensor(y))\r\n        with tf.GradientTape(persistent=True) as tape:\r\n            pred = self.model(x_in)\r\n            pred_T = tf.Variable(tf.convert_to_tensor(pred))\r\n            a_pred = pred[:,0:Nu]\r\n            b_pred = pred[:,Nu:]\r\n        da_dt = tape.jacobian(a_pred,x_in) // gives non-null values\r\n        loss = y_out-pred_T\r\n        print(pred)\r\n        print(pred_T)\r\n        grads = tape.gradient(loss, self.model.trainable_variables) // gives NULL values\r\n        print(grads)\r\n        aDot = np.zeros((da_dt.shape[0], da_dt.shape[1]))\r\n        for i in range(da_dt.shape[0]):\r\n            aDot[i,:] = da_dt[i,:,i,0].numpy()\r\n        aDot_T = tf.Variable(aDot)\r\n        Prod_T = self.TensorProduct(a_pred,C)\r\n        f_1 = tf.tensordot(M,tf.transpose(aDot_T),1) - nu * tf.tensordot(B,tf.transpose(a_pred),1) + \\\r\n        tf.transpose(Prod_T) + tf.tensordot(K,tf.transpose(b_pred),1) \r\n        f_2 = tf.tensordot(P,tf.transpose(a_pred),1)\r\n        \r\n        #loss = tf.reduce_sum(tf.square(a_tf - a_pred)) + \\\r\n        #            tf.reduce_sum(tf.square(b_tf - b_pred)) + \\\r\n        #            tf.reduce_sum(tf.square(tf.transpose(f_1))) + \\\r\n        #            tf.reduce_sum(tf.square(tf.transpose(f_2)))\r\n        \r\n        print(\"Q4\")\r\n        #loss_a = tf.square(a_tf - a_pred)\r\n        #lossF1 = tf.square(tf.transpose(f_1))\r\n        #print(loss_a.shape)\r\n        #print(lossF1.shape)\r\n        #print(tape.watched_variables())\r\n\r\n        \r\n        print(\"Q5\")\r\n        print(grads)\r\n\r\n        self.loss = loss\r\n        print(\"Q6\")\r\n        #print(self.model.trainable_variables)\r\n        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\r\n        print(\"Q7\")\r\n\r\n`"]}, {"number": 36595, "title": "'Tensor' object has no attribute 'numpy'", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/quickstart/beginner\r\n\r\n\r\n## Description of issue (what needs changing):\r\nThe instructions state, in the fourth block of sample code, to use a command which returns a fatal error.\r\n\r\n`predictions = model(x_train[:1]).numpy()`\r\n\r\n\r\n### Clear description\r\nWhen running the example, as written, on Debian Stable (python 3.7.3)\r\n\r\n```\r\n>>> predictions = model(x_train[:1]).numpy()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n\r\n```", "comments": ["@TheUnixDude ,\r\n`import tensorflow `has attribute `numpy`, hence when tried running the tutorials, I was able to [execute](https://colab.research.google.com/gist/oanush/d32dbdcf29a4561af82b450790d53911/beginner.ipynb) without any issues. Please try running the code again and let us know your feedback.Thank you!", "Just a straight-up copy and paste of the commands given does not work with Debian Stable; this is after a install of tensorflow via pip3. Here's the full output.\r\n\r\n```\r\n$ python3\r\nPython 3.7.3 (default, Dec 20 2019, 18:57:59) \r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from __future__ import absolute_import, division, print_function, unicode_literals\r\n>>> import tensorflow as tf\r\n/home/me/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/me/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n>>> mnist = tf.keras.datasets.mnist\r\n>>> (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n11493376/11490434 [==============================] - 6s 1us/step\r\n>>> x_train, x_test = x_train / 255.0, x_test / 255.0\r\n>>> model = tf.keras.models.Sequential([\r\n...   tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n...   tf.keras.layers.Dense(128, activation='relu'),\r\n...   tf.keras.layers.Dropout(0.2),\r\n...   tf.keras.layers.Dense(10)\r\n... ])\r\nWARNING:tensorflow:From /home/me/.local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n>>> predictions = model(x_train[:1]).numpy()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n>>> \r\n\r\n```", "Ok, I figured out the issue.\r\n\r\nI'm hesitant to close this, however, since there a documentation issue in that the commands given in the tutorial are not the same as what is opened in Colab. \r\n\r\nThe Colab code contains a Colab-specific version check on the tensorflow module. And, by default, pip3 will not install tensorflow version 2 - you get 1.14 as of today.\r\n\r\nSince the new module is not called \"tensorflow2\", anyone looking at the Beginner's Tutorial might assume that we need to \"pip3 install tensorflow\", which is a good and valid assumption normally for python when trying to figure out modules you need, but that assumption is wrong in this case.", "Hi @rushabh-v \r\n\r\n> tensorflow version 2 - you get 1.14 as of today.\r\n\r\nI don't think that's correct.\r\nYou probably have tensorflow 1.14 installed, and \"pip install tensorflow\" says \"tensorflow is already installed\".\r\n\r\nTry \"pip install -U tensorflow\".", "Okay then, should I close the PR?", "I think so.", "Just to note, I did not have Tensorflow installed when I began this. Mark's suggestion is incorrect. \r\n\r\nTrying to update via \"pip install -U tensorflow\" does not install the new version.\r\n\r\nExample of how broken this is There's clearly a newer version, and pip refuses to update when told to. Note these versions have moved forward since I opened this BR.\r\n\r\n```\r\n$ pip3 search tensorflow\r\ntensorflow (2.1.0)                                - TensorFlow is an open source machine learning framework for everyone.\r\n  INSTALLED: 2.0.0b1\r\n  LATEST:    2.1.0\r\n\r\n$ pip3 install -U tensorflow\r\nRequirement already up-to-date: tensorflow in ./.local/lib/python3.7/site-packages (2.0.0b1)\r\n```", "Hmmm... \r\n\r\nOther than TensorFlow already being installed, the only way I can think of for pip install to get you an old version is if you're running a system or python version that we no longer support, pip might still find all the old bersions that are supported on that system. \r\n\r\ncould that be it?\r\nAny other ideas?", "Thanks, @TheUnixDude. Confirmed Debian stable is installing 1.14\r\nIgnoring that we say [in the docs](https://www.tensorflow.org/install/pip) that we officially support \"Ubuntu 16.04 or later (64-bit)\", this is odd.\r\n\r\nRepro in Docker:\r\n```\r\n$ docker run -it --rm debian:stable bash\r\n\r\n# apt update\r\n# apt install python3-dev python3-pip\r\n\r\n# python3 --version\r\nPython 3.7.3\r\n\r\n# pip3 install tensorflow\r\n\r\nCollecting tensorflow\r\n  Downloading https://files.pythonhosted.org/packages/f4/28/96efba1a516cdacc2e2d6d081f699c001d414cc8ca3250e6d59ae657eb2b/tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3MB)\r\n...\r\n\r\n## Trying known bad version to see available packages:\r\n# pip3 install tensorflow==7.8.7\r\nCollecting tensorflow==7.8.7\r\n  Could not find a version that satisfies the requirement tensorflow==7.8.7 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\nNo matching distribution found for tensorflow==7.8.7\r\n```\r\n\r\nSo it doesn't seem a stable version of 2.x is available.", "It is probably because the pip version is just too old.\r\ncould you check the pip version?\r\nOur minimum required pip version is listed on our installation page\r\n", "Yes, `pip install --upgrade pip setuptools` is needed", "Ah, @gunan, you are right . ANd we do mention this throughout the docs.\r\n\r\n```\r\n# pip3 --version\r\npip 18.1 from /usr/lib/python3/dist-packages/pip (python 3.7)\r\n\r\n# pip3 install -U pip\r\n...\r\n# pip3 --version\r\npip 20.0.2 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\r\n\r\n# pip3 install tensorflow\r\nDownloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\r\n```", "```\r\n$ apt search python3-pip\r\nSorting... Done\r\nFull Text Search... Done\r\npython3-pip/stable,now 18.1-5 all [installed]\r\n  Python package installer\r\n```\r\n\r\nSo Debian Stable is not a supported release for Tensorflow version 2. In fact, pip3 version 20 isn't even available in sid. But, yes, upgrading pip3 outside of the package manager does solve the problem even though pip3 now throws a warning when used.\r\n\r\nMaybe call this out in the Beginner's page? I would not expect a beginner to know the multiple layers needed to navigate this failure.\r\n\r\n```\r\n$ apt search python3-pip\r\nSorting... Done\r\nFull Text Search... Done\r\npython3-pip/stable,now 18.1-5 all [installed]\r\n  Python package installer\r\n\r\n\r\n$ pip3 install -U pip\r\nCollecting pip\r\n  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4MB 770kB/s \r\nInstalling collected packages: pip\r\nSuccessfully installed pip-20.0.2\r\n\r\n$ pip3 install -U tensorflow\r\nWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\r\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\r\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\r\nDefaulting to user installation because normal site-packages is not writeable\r\nCollecting tensorflow\r\n  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\r\n\r\n<snip remainder of upgrade output>\r\n```", "Debian's package manager installs very old versions for a lot of packages. `pip`, `cabal` and others all need to be updated outside of it.\r\n\r\nI don't know if we can document all the quirks of all the operating systems, but letting @lamberta decide what can be done.", "> Maybe call this out in the Beginner's page?\r\n\r\nI think it's worth an additional note in the first quickstart guides because it is confusing.\r\n\r\nMy ideal solution is a patch to 1.14 with a warning for folks to upgrade their pip to install the latest version. Typically these kinds of patches are reserved for security fixes, but this is a major usability flaw, IMHO", "I made a patch & pull request to the doc adding a link to the install page with a note to follow it closely - but I cannot agree to the Google CLA so I closed the PR. Sorry I can't be of more assistance there.", "No worries. I'll make a separate update to the docs. Thanks"]}, {"number": 36594, "title": "ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type NAType).", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64-bit\r\n- TensorFlow installed from (source or binary): PyCharm\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.6\r\n\r\n**Describe the current behavior**\r\nI more or less tried to follow the instructions to load pandas.DataFrame (https://www.tensorflow.org/tutorials/load_data/pandas_dataframe) using my own dataset. Unfortunately the tf.data.Dataset.from_tensor_slices() function returns the following error:\r\n\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type NAType).\r\n\r\nThe only difference in comparision with the tutorial is, that I used Strings as input in my pandas dataframe.\r\n\r\n**Describe the expected behavior**\r\nThe Tensorflow dataset is created correctly.\r\n\r\n**Code to reproduce the issue**\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport pandas as pd\r\n\r\n\r\ndef data_preprocessing():\r\n    print(tf.__version__)\r\n\r\n    train_file_path = 'Learning Data Input/TrainingData.csv'\r\n\r\n    pandas_data = pd.read_csv(train_file_path, delimiter=';', dtype='string')\r\n    for column in pandas_data:\r\n        if pandas_data[column].dtype != 'string':\r\n            print(column)\r\n    print(pandas_data.head())\r\n    print(pandas_data.dtypes)\r\n    \r\n    # Feature to be predicted\r\n    label_column = pandas_data.pop('PARTSET0')  \r\n\r\n    raw_training_data = get_dataset(pandas_data, label_column)\r\n\r\ndef get_dataset(pandas_data, label_column):\r\n    dataset = tf.data.Dataset.from_tensor_slices((pandas_data.values, label_column.values))\r\n\r\n    return dataset\r\n\r\n**Other info / logs**\r\nMy data set looks as follows:\r\n CC_ENDCUSTOMERNAME CC_ENDCUSTOMERCOUNTRY  ...     PARTSET7     PARTSET8\r\n0  xxx                  THA  ...  123456  123456\r\n1  yyy                   GER  ...  123456  123456\r\n2  zzz                   US ...  123456  123456\r\n\r\nand has round about 1000 rows and 190 columns. Please note that the numeric values in the csv are intentionally impported as Strings and this should be persisted.\r\n\r\nThanks for your help!\r\n", "comments": ["@MaUt89 Please provide stand alone implementable code with the csv file used.", "The code is already stand alone implementable, the def data_preprocessing() is only called in my main function and nothing else happens. The csv includes company sensitive data and I'm not able to provide it. Here is the complete error message for further information:\r\n\r\n2020-02-10 13:25:05.197725: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Z002P84D\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\structure.py\", line 111, in normalize_element\r\n    ops.convert_to_tensor(t, name=\"component_%d\" % i))\r\n  File \"C:\\Users\\Z002P84D\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1184, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"C:\\Users\\Z002P84D\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1242, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"C:\\Users\\Z002P84D\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\Z002P84D\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_conversion_registry.py\", line 52, in _default_conversion_function\r\n    return constant_op.constant(value, dtype, name=name)\r\n  File \"C:\\Users\\Z002P84D\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\Z002P84D\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 235, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\Z002P84D\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type NAType).\r\n\r\nAnd here some debug information of one column of the pandas DataFrame:\r\n![image](https://user-images.githubusercontent.com/53083678/74149947-45f01500-4c09-11ea-944f-14f94f104c98.png)\r\n", "@MaUt89 Since the tutorial you are referring works fine we don't suspect it to be a bug at this point. If you can provide a dummy csv file which gives you same error as you see in your actual case will help us validate the issue and work on its fix. Apologies we cannot help you debug/fix the error without complete minimal repro code. \r\nYou can also try posting this question on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow). \r\nFeel free to reopen when have more information.\r\nThanks!"]}, {"number": 36593, "title": "Fix Invalid description FileWriter", "body": "After Tensorflow 2.0,eager is used by default.\r\n\r\nSo, FileWriter class generates a runtime error because of eager context.\r\ntf.compat.v1.Session() \r\n\r\nEven though you use tf.compat.v1.Session (), the session works as an eager, so you can see the following error code.\r\n\r\n`RuntimeError: tf.summary.FileWriter is not compatible with eager execution. Use tf.contrib.summary instead.`\r\n\r\nBut as we all know, `tf.contrib` no longer exists in tensorflow 2.0 world.\r\n\r\nSo I added the following guide:  Use `tf.compat.v1.disable_eager_execution()` before the code.\r\n\r\nAnd I touched the document a bit to suit him.\r\n\r\nThanks for your working.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36593) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36593) for more info**.\n\n<!-- ok -->", "Thanks @ryujaehun,\r\n\r\nI'll try merging this.", "@ryujaehun Can you please address Ubuntu Sanity errors? Thanks!", "> @ryujaehun Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\nI fixed it, thanks!", "@ryujaehun Still, Ubuntu Sanity checks failing. Could you please check again?  Thanks!", "@ryujaehun Still, Ubuntu Sanity checks failing. Could you please check again? Thanks!", "@ryujaehun Still, Ubuntu Sanity checks failing. Could you please check again? Thanks!", "Here are the internal errors, @ryujaehun can you please verify ?\r\n\r\n`RuntimeError: v1.summary.FileWriter is not compatible with eager execution. Use tf.summary.create_file_writer,or a with v1.Graph().as_default(): context\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<embedded stdlib>/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"<embedded stdlib>/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \"google3/third_party/tensorflow/contrib/eager/python/tfe_test.py\", line 137, in testClassicSummaryFileWriterErrorsOut\r\n    writer.FileWriter(tempfile.mkdtemp())\r\n  File \"<embedded stdlib>/unittest/case.py\", line 217, in __exit__\r\n    expected_regex.pattern, str(exc_value)))\r\n  File \"<embedded stdlib>/unittest/case.py\", line 135, in _raiseFailure\r\n    raise self.test_case.failureException(msg)\r\nAssertionError: \"tf\\.summary\\.FileWriter is not compatible with eager execution\" does not match \"v1.summary.FileWriter is not compatible with eager execution. Use tf.summary.create_file_writer,or a with v1.Graph().as_default(): context\" `", "Yeah, it got stuck on an internal test. I've fixed it (cl/299164574).", "This is only waiting for an internal approval, it will be merged soon."]}, {"number": 36592, "title": "Heatmap function", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.0(I use colab so the latest probably)\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nPresently I have to write lots of code to create a heatmap for images. I would like to have a function that automatically generates one for you\r\n**Will this change the current api? How?**\r\nIt will add another function to the API\r\n**Who will benefit with this feature?**\r\nBeginners at ML and others who don't want to write lots of code to do this\r\n**Any Other info.**\r\n", "comments": ["@generationzcode Could you please share your code snippet, or any use case where assistance with this new feature request is required.", "https://colab.research.google.com/drive/1E6RZpXsQtRi9Sp4wOQ39mvKht3NMhX_F heres the code on colab. This was done in a google code in task\r\n\r\n", "There are also some others related features about heatmaps for keypoints/object detection tasks with heatmaps.\r\nSee https://github.com/tensorflow/addons/issues/1366 and https://github.com/tensorflow/addons/issues/1364", "Instead If your scope it is limited to network interpretability check:\r\nhttps://github.com/sicara/tf-explain\r\nhttps://github.com/tensorflow/lucid", "@generationzcode,\r\nCan you please refer the [Grad-CAM Algorithm](https://keras.io/examples/vision/grad_cam/#the-gradcam-algorithm) mentioned in the **`Keras Documentation`** and let us know if that is what you are looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36591, "title": "Nit tutorial inconsistency in text generation with RNN", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/text/text_generation#the_prediction_loop\r\n\r\n## Description of issue (what needs changing):\r\nConsider the previously generated letters for subsequent generations. Right now only the lastly generated letter is considered to generate the immediate next letter. The result reads very far from natural as you can imagine.\r\n\r\n### Clear description\r\n\r\nThe `generate_text()` method is supposed to generate text, letter by letter, by taking into account a starting string and any letter that has been generated so far. Right now, it's generating the first letter by taking into account the starting string, then from then on, it only considers the last generated letter. \r\n\r\nSo in this:\r\n```\r\ndef generate_text(model, start_string):\r\n  # Evaluation step (generating text using the learned model)\r\n\r\n  # Number of characters to generate\r\n  num_generate = 1000\r\n\r\n  # Converting our start string to numbers (vectorizing)\r\n  input_eval = [char2idx[s] for s in start_string]\r\n  input_eval = tf.expand_dims(input_eval, 0)\r\n\r\n  # Empty string to store our results\r\n  text_generated = []\r\n\r\n  # Low temperatures results in more predictable text.\r\n  # Higher temperatures results in more surprising text.\r\n  # Experiment to find the best setting.\r\n  temperature = 1.0\r\n\r\n  # Here batch size == 1\r\n  model.reset_states()\r\n  for i in range(num_generate):\r\n      predictions = model(input_eval)\r\n      # remove the batch dimension\r\n      predictions = tf.squeeze(predictions, 0)\r\n\r\n      # using a categorical distribution to predict the character returned by the model\r\n      predictions = predictions / temperature\r\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n\r\n      # We pass the predicted character as the next input to the model\r\n      # along with the previous hidden state\r\n      input_eval = tf.expand_dims([predicted_id], 0)\r\n\r\n      text_generated.append(idx2char[predicted_id])\r\n\r\n  return (start_string + ''.join(text_generated))\r\n```\r\n\r\nThe end of the for-loop needs to be updated to match the comment description, with something like this:\r\n```\r\n# We pass the predicted character as the next input to the model\r\n# along with the previous hidden state\r\ninput_eval = tf.concat([input_eval, tf.expand_dims([predicted_id], 0)], -1)\r\n```\r\nWhere we add the newly predicted ID to the end of what is currently `input_eval`\r\n\r\n### Submit a pull request?\r\n\r\nI plan on submitting a pull request with the quick fix.\r\n", "comments": ["This problem actually wouldn't exist for stateful models, i.e as long as the RNN layer has been specified to be stateful, it will keep track of what it's seen previously (since the last state reset) and we therefore only need to give it the last prediction.\r\nThe original implementation is correct!"]}]