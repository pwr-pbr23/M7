[{"number": 12123, "title": "Cannot compile on Mac OS X due to BoringSSL", "body": "\r\n------------------------\r\n\r\n### System information\r\n\r\n```\r\n@:~/projects/tensorflow <master>$ cat tf_env.txt\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin mn-mortutay 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.5.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin mn-mortutay 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named pywrap_tensorflow_internal\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n\r\nBuilding from commit:\r\n```\r\n* d74f65bac (Yun Peng, 3 hours ago) Make Windows Bazel GPU build work again (#11901)\r\n```\r\n\r\n### Describe the problem\r\nI'm following the instructions for Mac source installation (https://www.tensorflow.org/install/install_sources, \"Prepare environment for Mac OS\" and then \"Build the pip package\") but I'm running into issues with BoringSSL.\r\n\r\nIt looks like the `-Wunused-but-set-parameter` and `-Wno-free-nonheap-object` flags are causing the compilation to fail, since `clang` does not support them. \r\n\r\nExpected behavior: The build system should detect this and handle it correctly, eg. by not using the un-available flags\r\n\r\nActual behavior: Error / fails to build\r\n\r\n### Source code / logs\r\n\r\n\r\n```\r\n@:~/projects/tensorflow <master>$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: /Users/mortutay/projects/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /Users/mortutay/projects/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/protobuf_archive/python/google/protobuf/internal/api_implementation.cc:\r\nwarning: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Wunknown-warning-option]\r\nwarning: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Wunknown-warning-option]\r\n2 warnings generated.\r\nINFO: From Compiling external/swig/Source/Swig/typemap.c [for host]:\r\nwarning: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Wunknown-warning-option]\r\nwarning: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Wunknown-warning-option]\r\n2 warnings generated.\r\nINFO: From Compiling external/grpc/src/core/lib/debug/trace.c:\r\nwarning: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Wunknown-warning-option]\r\nwarning: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Wunknown-warning-option]\r\n2 warnings generated.\r\nINFO: From Compiling external/grpc/src/cpp/codegen/codegen_init.cc:\r\nwarning: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Wunknown-warning-option]\r\nwarning: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Wunknown-warning-option]\r\n2 warnings generated.\r\nINFO: From Compiling external/grpc/src/core/ext/transport/chttp2/alpn/alpn.c:\r\nwarning: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Wunknown-warning-option]\r\nwarning: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Wunknown-warning-option]\r\n2 warnings generated.\r\nERROR: /private/var/tmp/_bazel_mortutay/dacb21c644505cd819865fa365d2b69e/external/boringssl/BUILD:116:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1).\r\nerror: unknown warning option '-Wunused-but-set-parameter'; did you mean '-Wunused-parameter'? [-Werror,-Wunknown-warning-option]\r\nerror: unknown warning option '-Wno-free-nonheap-object'; did you mean '-Wno-sequence-point'? [-Werror,-Wunknown-warning-option]\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 2.829s, Critical Path: 0.91s\r\n```\r\n\r\n", "comments": ["Hi,\r\n\r\nI recently had issues compiling BoringSSL because the -Werror complier flag was set in the BoringSSL build. If your error log is complete, it looks like the same warnings that were tolerated in building other parts of the code, were treated as fatal errors in the BoringSSL build. \r\n\r\nIf the -Werror flag is your problem, it would be best to find where it is set. However, I used a brute force solution to get the job done.\r\n\r\n```bash\r\nfind . -type f -not -regex \".*\\.\\(o\\|h\\|c\\|a\\|t\\|cc\\|so\\)\" -exec sed -i -e \"s/-Werror/-Wno-excessive-errors/g\" {} \\;\r\n```\r\n\r\nYou are compiling on a Mac, so you may want to add \"bundle\" to the exclusions regex.\r\n\r\nThe fact you are getting these warnings may indicate that you have a new version of the compiler that is now complaining about things that previous versions did not (that was my case), or an older version of the tool chain that truly might not know how to do what the flags are requesting be done. Particularly, in the newer toolchain case, getting rid of -Werror is a solution. \r\n \r\nHope this helps.\r\n", "@mountaintom Thanks for the analysis!  Your explanation looks spot-on to me.\r\n\r\n@av8ramit Do you know who might be a good candidate to follow-up on Mac builds?", "Experienced the same issue with boringSSL, dug into it from and found Werror is being set in [add_boringssl_s390x.patch](https://github.com/tensorflow/tensorflow/blob/master/third_party/boringssl/add_boringssl_s390x.patch) on line 71, which is causing the build failure.\r\n\r\n```+boringssl_copts = select({\r\n+    \":windows\": [\r\n+        \"-DWIN32_LEAN_AND_MEAN\",\r\n+    ],\r\n+    \"//conditions:default\": [\r\n+        # Assembler option --noexecstack adds .note.GNU-stack to each object to\r\n+        # ensure that binaries can be built with non-executable stack.\r\n+        \"-Wa,--noexecstack\",\r\n+\r\n+        # This is needed on Linux systems (at least) to get rwlock in pthread.\r\n+        \"-D_XOPEN_SOURCE=700\",\r\n+\r\n+        # This list of warnings should match those in the top-level CMakeLists.txt.\r\n+        \"-Wall\",\r\n+        \"-Werror\",\r\n+        \"-Wformat=2\",\r\n+        \"-Wsign-compare\",\r\n+        \"-Wmissing-field-initializers\",\r\n+        \"-Wwrite-strings\",\r\n+        \"-Wshadow\",\r\n+        \"-fno-common\",\r\n+    ],\r\n+})\r\n```\r\n\r\n`-Werror` can just be replaced with `Wno-excessive-errors`, as suggested by @mountaintom , and TF will successfully build. Not sure if this is a sufficient fix as I'm not familiar with the library, but it does bypass the issue for now. ", "+1, the issue still there when compiling from source in master branch, after used the way of @lissahyacinth , as below:\r\n\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nWARNING: /Users/sunkai/work/src/tensorflow/tensorflow/core/BUILD:1632:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /Users/sunkai/work/src/tensorflow/tensorflow/core/BUILD:1632:1.\r\nERROR: /Users/sunkai/work/src/tensorflow/tensorflow/tools/pip_package/BUILD:100:1: no such package '@boringssl//': Traceback (most recent call last):\r\n\tFile \"/Users/sunkai/work/src/tensorflow/tensorflow/workspace.bzl\", line 119\r\n\t\t_apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n\tFile \"/Users/sunkai/work/src/tensorflow/tensorflow/workspace.bzl\", line 110, in _apply_patch\r\n\t\t_execute_and_check_ret_code(repo_ctx, cmd)\r\n\tFile \"/Users/sunkai/work/src/tensorflow/tensorflow/workspace.bzl\", line 94, in _execute_and_check_ret_code\r\n\t\tfail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(2) when executing 'patch -p1 -d /private/var/tmp/_bazel_sunkai/f32adcfaaeaeed39ed3812ab214ba306/external/boringssl -i /Users/sunkai/work/src/tensorflow/third_party/boringssl/add_boringssl_s390x.patch':\r\nStdout: patching file src/include/openssl/base.h\r\npatching file BUILD\r\n\r\nStderr: patch: **** malformed patch at line 88:      #\"//conditions:default\": [\"-DOPENSSL_NO_ASM\"],\r\n\r\n and referenced by '//tensorflow/tools/pip_package:licenses'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\nINFO: Elapsed time: 3.261s\r\n\r\nIs there any idea? Thanks.\r\n", "Is there some update? Looking forward to the response, thanks.", "I don't have problem with the way @lissahyacinth provided.\r\n\r\nPlease take a look at this error:\r\nStderr: patch: **** malformed patch at line 88: #\"//conditions:default\": [\"-DOPENSSL_NO_ASM\"],\r\nLooks like your patch file is not correct.\r\n", "This seems to be a bug with newer Bazel releases (maybe Bazel 0.5.3? not sure). I started having this with all my C++ projects that use Bazel. They all show these two warnings when compiling C++ files.\r\n\r\nIt looks like it adds these two flags when compiling with clang (I'm on Linux), even though it shouldn't.\r\n\r\n@gunan marking you as you pointed these warnings on the duplicate bug.", "@quaeler Could you please paste your version of bazel which works well on mac #12266? As reported by @RenatoUtsch on bazelbuild/bazel#3622 , the warning seems to be brought by new version of bazel.", "Yep:\r\n```\r\nautogenic:tensorflow loki$ bazel version\r\nBuild label: 0.5.3-homebrew\r\nBuild target: bazel-out/darwin_x86_64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sun Aug 6 12:54:10 2017 (1502024050)\r\nBuild timestamp: 1502024050\r\nBuild timestamp as int: 1502024050\r\nautogenic:tensorflow loki$\r\n```", "Interesting, Bazel does not add these unrecognized flags on my Mac OS either, while it adds them to the exact same rule in Linux. Maybe this is related to something else, and not only Bazel's version.", "Okay, upon digging a but more this does not seem to be related to bazel's version, but still seems to be a bazel bug. Follow bazelbuild/bazel#3622 if interested.", "Thanks @RenatoUtsch . I have put my env info on [bazelbuild/bazel#3622](https://github.com/bazelbuild/bazel/issues/3622).", "@keithhans thanks for the point. \r\nBut, I didn't change the line 88(should be 87 in original add_boringssl_s390x.patch) of `#\"//conditions:default\": [\"-DOPENSSL_NO_ASM\"]`.", "I find the reason why it didn't take effect by \"replacing\" `-Werror` with `-Wno-excessive-errors` since I wrongly added `#` to try to comment `-Werror` instead of replacing. Now it works and thanks.", "Where does \"-Wunused-but-set-parameter\" come from? What's your bazel version? If it is not 0.5.4, could you do an upgrade?\r\nBazel shouldn't add this arg to a compiler that doesn't support it.", "Yeah, removing -Werror is just a hack and doesn't solve the real problem.\n\nI could definitely reproduce bazel adding those two flags wrongly to clang\non 0.5.3. In a few hours I can update and try it again with 0.5.4.\n\nOn Mon, Aug 28, 2017, 06:17 Changming Sun <notifications@github.com> wrote:\n\n> Where does \"-Wunused-but-set-parameter\" come from? What's your bazel\n> version? If it is not 0.5.4, could you do an upgrade?\n> Bazel shouldn't add this arg to a compiler that doesn't support it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12123#issuecomment-325303387>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAzC7dzgjiAyFdwVS-QJqG3gCEay3zu3ks5scoWugaJpZM4Oxbpe>\n> .\n>\n", "This is fixed with Bazel 0.5.4.\r\n\r\nJust upgrade and these warnings won't happen anymore. No need for the `-Werror` hack too.", "Closing based on the latest comments."]}, {"number": 12122, "title": "Add name field for tf.contrib.data.Iterator.make_initializer operation", "body": "This fix adds name fields for tf.contrib.data.Iterator.make_initializer operation\r\n\r\nThis fix fixes #12083.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @tensorflower-gardener and @saeta to be potential reviewers.", "Thanks @mrry for the review. The PR has been updated.", "@tensorflow-jenkins test this please."]}, {"number": 12121, "title": "tf.random_normal_initializer produces inconsistent results with fixed seed on GPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.0.0, 1.1.0, 1.2.1\r\n- **Python version**: 2.7.10, 3.5.2\r\n- **Bazel version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: CUDA release 8.0, V8.0.46; cuDNN 8.0\r\n- **GPU model and memory**: GeForce GTX TITAN X 12GB, GK210GL [Tesla K80] 12GB\r\n- **Exact command to reproduce**: below\r\n\r\n### Describe the problem\r\nSetting both graph-level and op-level random seeds to fixed values, I still get inconsistent results initializing variables with `tf.random_normal_initializer`. Here's the code to reproduce the problem\r\n\r\n\timport numpy as np\r\n\timport tensorflow as tf\r\n\t\r\n\ttf.set_random_seed(0)\r\n\tnp.random.seed(0)\r\n\tshape = (100, 2048)\r\n\tseed = 0\r\n\tfor i in range(100):\r\n\t    seed += 1\r\n\t    init = tf.random_normal_initializer(stddev=0.02, seed=seed)\r\n\t    tf.get_variable(str(i), shape, initializer=init)\r\n\t\r\n\tsession = tf.Session()\r\n\tsession.run(tf.global_variables_initializer())\r\n\tvar_dict = {}\r\n\tfor var in tf.trainable_variables():\r\n\t    var_dict[var.name] = session.run(var.name)\r\n\t\r\n\tnp.savez_compressed(\"weights.npz\", **var_dict)\r\n\tsession.close()\r\n\r\nHere I initialize 100 variables with `tf.random_normal_initializer` with a fixed op-level seed and with a fixed graph-level seed.\r\n\r\nRunning this two times I get different results saved in `weights.npz`. Interestingly, this happens only when using GPU and the difference between the saved weights is very slight: usually only some of the variables are different, and they only differ in some small number of positions. \r\n\r\nI experience this problem on Tensorflow 1.0.0 (Python2), 1.1.0, 1.2.1 (Python3) and on two different GPUs. ", "comments": ["@zheng-xq Do you have any insights here, or can you suggest someone to investigate?  Thanks!", "Nagging Assignee @zheng-xq: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ymodak: It has been 89 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this still an issue with the latest version of TensorFlow for you?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 12120, "title": "Fixed channel mismatch if num_outputs=None", "body": "Addresses feature request from issue #10432. Continuation of feature implementation--initial changes did not address the case where `num_outputs = None`, in which case strides and the input channels must be reformulated according to data format. The previous commit is b7e2f03008e2eec28c26d3abf881ddc91b07fcda.\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@jcastagneri, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @zhangyaobit and @lukaszkaiser to be potential reviewers.", "I signed it!", "@jcastagneri Thanks for the fix.\r\n@tensorflow-jenkins test this please", "@jcastagneri the CLA bot didn't accept your signature. did you perhaps use a different email in the PR?", "@rmlarsen You're exactly right--the initial commit was with the wrong email. I rebased that commit, and locally it does show the email which is linked to the CLA, but this doesn't appear to have remedied the block. Do you have a suggestion for what I should do?", "@vrv do you have any advice? I have not dealt with this before.", "@tensorflow-jenkins test this please", "If the email shown in https://patch-diff.githubusercontent.com/raw/tensorflow/tensorflow/pull/12120.patch does not match the CLA, you'll have to force push your local commit to github.", "Several PRs fixed most of the problems in this section of the code, but not all. See PR #12273 for updates. Closing since the `num_outputs` bug was fixed.", "@jcastagneri thanks for the update."]}, {"number": 12119, "title": "Please add predict_proba to estimators class not in TF1.3 DNN", "body": "### Describe the problem\r\n\r\nFeature Request:\r\nThe TF.learn library in TF1.2 has perdict_proba for getting probability estimates for predictions it is particularly helpful for setting probability levels up or down or just measuring them.. it is a super helpful function for analysis as well as correlations can be drawn to input variables...\r\n\r\nThere are many use cases where you may want to accept a prediction with less than a 50/50 (think unbalanced classs)\r\nand there are other cases where you want greater certainty,  for some medical decisions or credit scoring..\r\n\r\nThe DNN class in TF 1.3 with the estimator does not seem to have this function. (while tf.learn dnn in TF1.2 does...)\r\n\r\nEstimators seem to be the way of the future, but at least IMHO we need this to make that move..\r\n\r\n\r\n", "comments": ["You can use tf.estimator now with e.g. predict(..., predict_keys=\"probabilities\")", "ahh!!! Doc pointers?"]}, {"number": 12118, "title": "C API Functions Support", "body": "Hi, \r\n\r\nI was wondering what the status is for supporting the creation of functions in the C API. Couldn't this be done in a similar manner to how while loops are currently constructed?\r\n\r\nThanks,\r\nAnthony\r\n\r\nP.S. @skye This could also be useful for defining gradient functions for ops not supported by the C API gradients.", "comments": ["Igor is working on this! I don't know Igor's github handle but I pinged him and will update this issue when I find out what it is.", "@iganichev is working on function support in the C API", "I am working on it right now. Interface for basic functionality has been reviewed and implementation is almost ready for review. The core interface just takes a TF_Graph and converts it to TF_Function. You can then add this TF_Function definition to other graphs and create TF_Operations that \"invoke\" it. ", "@iganichev That's great to hear! Do you have any updates on when that functionality might be available publicly?", "@eaplatanios Sorry for the delay. The interface and impl had to be significantly changed. I am going through some final stages of reviews. If we don't find major issues, it should be in this week. Internal changes get synced to github at least weekly, sometimes more often. So, the core functionality should hit github at most within two weeks. ", "@iganichev Thanks a lot Igor! I have already started using your API and it looks great. :) I only have a few questions:\r\n1. When converting a graph to a GraphDef using the C API, does it take into account functions that have been added to that graph? If not, I could always add them on my side after creating a GraphDef using the C API.\r\n2. If I close a `TF_Function` object, the function is still usable in graphs that it was previously added in, right?\r\n3. It would be nice to have an `ImportFunctionDef` function.\r\n4. It would be nice to have a `GraphGetFunctions` function that returns all functions defined in a graph.\r\n\r\n@skye It seems like this API can serve as the interface for yet unimplemented gradients in the C++ side. What do you think?\r\n", "@eaplatanios:\r\n1. Yes it does\r\n2. Assuming by \"close\" you mean calling TF_DeleteFunction, yes. Deleting the TF_Function struct does not do anything with the copies of the function that were added to graphs.\r\n3. I will add ImportFunctionDef. It should be pretty easy and I can see it being useful.\r\n4. GraphGetFunctions is a bit tricky. Functions are currently added to graphs as FunctionDef objects (plus (function_name, gradient_name) pairs in pending changes). We can reconstruct an equivalent TF_Function object from a graph, but it won't be the same TF_Function object that was added to the graph. I can see this causing confusion. If you need this functionality, I would suggest the following approach. Get GraphDef from graph, get FunctionDefs from this GraphDef, use ImportFunctionDef to create TF_Function objects. This approach is not super efficient, but hopefully does not need to run often and in critical paths. If performance of this functionality is critical, we can think about adding a TF_GraphGetFunctionDefinitionLibrary function. This will be as efficient as we can get with the current design.", "@iganichev That's great, thanks for all the answers! :)\r\n\r\nRegarding 4, that solution is totally fine. The only use case I have in my mind is for when one imports a graph from a GraphDef. In that case, I want to populate my Scala-side function registry with the functions that are defined in that graph. When manipulating graphs within Scala, no C-side functionality is needed as I can do the book-keeping there. And this use case should not be critical for efficiency, given that you're importing a GraphDef anyway.", "@iganichev This may be pretty straightforward, but when an op has a `func`-valued attribute (such as the `MapDataset` op), how should I set that attribute using the C API? Are we currently missing a `SetAttrFunc` method or is there another way? Thanks! :)", "@asimshankar I just noticed you address the `func`-valued attribute case for the eager API. How long do you think it might take to add support for it in the graph construction API too? I could look into implementing that if you direct me in what changes are required (e.g., what exactly defines a function attribute value?).", "@eaplatanios You can use TF_SetAttrValueProto to set any attribute by providing the AttrValue protobuf.", "@iganichev In that case, should I add the function to the graph first, then obtain its `FunctionDef`, and set the attribute for new ops using `TF_SetAttrValueProto`?", "@eaplatanios It might happen work in the other order, but adding the function to graph first is definitely the logical one.\r\n\r\nI believe you only need the function's name. You can definitely get it from the FunctionDef, but if you already have the name, you don't need to retrieve the FunctionDef.", "@iganichev how would I pass it using only the function name? Doesn't the SetAttrProto method expect a proto-serialized form of the function? If I only pass the name it wouldn't know it's referring to a function, right? Or does it infer that from the attribute type?\n\nAlso, I noticed that for the eager API, a complete AttrValueMap is created for the function (i.e., passed along with its name).\n\nOn Sep 15, 2017, 10:28 AM -0400, Igor Ganichev <notifications@github.com>, wrote:\n> @eaplatanios It might happen work in the other order, but adding the function to graph first is definitely the logical one.\n> I believe you only need the function's name. You can definitely get it from the FunctionDef, but if you already have the name, you don't need to retrieve the FunctionDef.\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "@iganichev @asimshankar Actually, when I try to use `TF_SetAttrValueProto` using a `FunctionDef`, I get the following error:\r\n\r\n`AttrValue had value with type 'list(string)' when 'func' expected`", "After browsing through the code, I figure out that I need to create an `AttrValue` proto message. This works:\r\n```\r\nval attrValue = AttrValue.newBuilder()\r\n  attrValue.setFunc(\r\n    NameAttrList.newBuilder()\r\n        .setName(value.name)\r\n        .build())\r\n// Then call the `TF_SetAttrValueProto` function with `attrValue.build().toByteArray`.\r\n```\r\nHow come the attr name-value list is not needed? Could ignoring it have any implications?", "@eaplatanios The \"name-value list\" you are referring to might be needed. As the comment in [here](https://github.com/tensorflow/tensorflow/blob/7534e5b0e8d796173416f91b29b79ad715afc913/tensorflow/core/framework/attr_value.proto#L44) is saying, this field holds whatever attributes the function or (a basic operation) needs. Just like you add attributes to basic operations (data type being the most common) you can add attributes to the function or operation referenced in here. Regular TF_Functions can have attributes as well, but all the ones used now are optional (except some cases/bugs like when you have constants in functions, enable constant folding, and are running on GPU - you need to set \"noinline\" to True). I will be committing a change that adds support for setting attributes on TF_Functions soon (it has been reviewed but there are some issues in earlier changes).\r\n\r\nYou would probably get these attributes from the user and can set them next to setting the name.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 12117, "title": "Bazel Windows Build: '//tensorflow/tools/proto_text:gen_proto_text_functions' failed to link", "body": "This was first caught by Bazel CI: http://ci.bazel.io/blue/organizations/jenkins/TensorFlow/detail/TensorFlow/1030/pipeline\r\nReported at https://github.com/bazelbuild/bazel/issues/3524\r\n```\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /OUT:bazel-out/msvc_x64-py3-opt/bin/tensorflow/tools/proto_text/gen_proto_text_functions.exe /MACHINE:X64 /SUBSYSTEM:CONSOLE @bazel-out/msvc_x64-py3-opt/bin/tensorflow/tools/proto_text/gen_proto_text_functions.exe-2.params /DEFAULTLIB:libcmt.lib.\r\nliblib_proto_parsing.a(logging.o) : error LNK2019: unresolved external symbol \"public: unsigned __int64 __cdecl tensorflow::StringPiece::Hasher::operator()(class tensorflow::StringPiece)const \" (??RHasher@StringPiece@tensorflow@@QEBA_KV12@@Z) referenced in function \"protected: struct std::pair<class std::_List_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class tensorflow::StringPiece const ,int> > > >,bool> __cdecl std::_Hash<class std::_Umap_traits<class tensorflow::StringPiece,int,class std::_Uhash_compare<class tensorflow::StringPiece,struct tensorflow::StringPiece::Hasher,struct std::equal_to<class tensorflow::StringPiece> >,class std::allocator<struct std::pair<class tensorflow::StringPiece const ,int> >,0> >::_Insert<struct std::pair<class tensorflow::StringPiece const ,int> &,class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class tensorflow::StringPiece const ,int> > > > >(struct std::pair<class tensorflow::StringPiece const ,int> &,class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class tensorflow::StringPiece const ,int> > > >)\" (??$_Insert@AEAU?$pair@$$CBVStringPiece@tensorflow@@H@std@@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBVStringPiece@tensorflow@@H@std@@@std@@@std@@@2@@?$_Hash@V?$_Umap_traits@VStringPiece@tensorflow@@HV?$_Uhash_compare@VStringPiece@tensorflow@@UHasher@12@U?$equal_to@VStringPiece@tensorflow@@@std@@@std@@V?$allocator@U?$pair@$$CBVStringPiece@tensorflow@@H@std@@@4@$0A@@std@@@std@@IEAA?AU?$pair@V?$_List_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBVStringPiece@tensorflow@@H@std@@@std@@@std@@@std@@_N@1@AEAU?$pair@$$CBVStringPiece@tensorflow@@H@1@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBVStringPiece@tensorflow@@H@std@@@std@@@std@@@1@@Z)\r\nbazel-out/msvc_x64-py3-opt/bin/tensorflow/tools/proto_text/gen_proto_text_functions.exe : fatal error LNK1120: 1 unresolved externals\r\nTarget //tensorflow/tools/proto_text:gen_proto_text_functions failed to build\r\n```\r\n\r\nCulprit is found by bisecting: tensorflow/tensorflow@072b0c9.\r\nThe unresolved symbol `tensorflow::StringPiece::Hasher::operator()` is implemented in `stringpiece.cc`. This change should have introduced a dependency on stringpieces.cc from logging.cc.\r\n\r\nHowever, `//tensorflow/tools/proto_text:gen_proto_text_functions` doesn't contain `stringpiece.cc`\r\n```\r\n$ bazel query 'somepath(//tensorflow/tools/proto_text:gen_proto_text_functions, tensorflow/core/lib/core/stringpiece.h)'\r\n//tensorflow/tools/proto_text:gen_proto_text_functions\r\n//tensorflow/core:lib_proto_parsing\r\n//tensorflow/core:lib/core/stringpiece.h\r\n\r\n$ bazel query 'somepath(//tensorflow/tools/proto_text:gen_proto_text_functions, tensorflow/core/lib/core/stringpiece.cc)'\r\nINFO: Empty results\r\n```\r\n\r\nThe strange thing is this didn't break the Linux build, one possible explanation is that there is an implementation of stringpiece.cc in protobuf, which is a dependency of `//tensorflow/tools/proto_text:gen_proto_text_functions`\r\n```\r\n$ bazel query 'deps(//tensorflow/tools/proto_text:gen_proto_text_functions)' | grep stringpiece.cc\r\n@protobuf_archive//:src/google/protobuf/stubs/stringpiece.cc\r\n```\r\n@learyg @gunan \r\n\r\n", "comments": ["@meteorcloudy Your analysis and fix look reasonable to me.  Marking as contributions welcome, and we can close this out when your PR gets merged.  Thanks!"]}, {"number": 12116, "title": "Branch 164623298", "body": "", "comments": ["@tensorflow-jenkins test this please", "benoitsteiner@ submitted an internal change to fix the constant folding errors in grappler. I'll abandon this PR and do a new push once his CL is submitted."]}, {"number": 12115, "title": "XLA feature fix: \"INVALID ARGUMENTS: Unsupported type in DataTypeToPrimitiveType complex64\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: tfcompile ...\r\n\r\nCurrently, the XLA compiler seems to not support complex dtypes. I ran into this when trying to use the spectral ops (ff2d) in an AOT compilation. Is it possible to fix this? Alternatively, is there a workaround that succeeds now? I thought I could maybe bitcast a float64 to a complex64, but it seems bitcast is also not implemented for XLA. ", "comments": ["Indeed there is currently no support for complex types in XLA.  Unfortunately there isn't a workaround at the moment.\r\n\r\nThis feature request is reasonable, but unfortunately I can't say exactly when we'll add this support.", ":+1: for complex types support. It would also be necessary for supporting `tf.fft` (https://github.com/tensorflow/tensorflow/issues/12314)", "https://github.com/tensorflow/tensorflow/commit/f226eb3717a0df815579178f4393d4e68cbe08fc\r\nand\r\nhttps://github.com/tensorflow/tensorflow/commit/4198e27be8115585ad6b5b141383fb7dc7856c24\r\nadd support for C64 as a type.\r\n\r\nFuture work will add at least an XLA bridge for FFT, but that can be tracked in #12314.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "This issue is fixed.\n\n\nsent from mobile\n\nOn Tue, Dec 19, 2017, 8:19 PM Alfred <notifications@github.com> wrote:\n\n> It has been 14 days with no activity and this issue has an assignee.Please\n> update the label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12115#issuecomment-352935697>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AVJZIyCBP6ID66V3bkErRX5cUw_mhx3Wks5tCGB3gaJpZM4OxHNM>\n> .\n>\n"]}, {"number": 12114, "title": "How can train my own model in tensorflow using Flickr  image dataset, How can i convert data into train, val, test modules", "body": "", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12113, "title": "bug with tf.unique where index output is int64", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX El Capitan 10.11.6 \r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc1-27-g2784b1c 1.3.0-rc2\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: y, idx = tf.unique(x, out_idx=tf.int64)\r\n\r\n### Describe the problem\r\nThe API says `tf.unique` supports `tf.int64` index output. However, no OpKernel supports such attributes.\r\n\r\n### Source code / logs\r\nTo reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.convert_to_tensor(np.array([0,1,2,0,1,2], dtype=np.int64))\r\ny, idx = tf.unique(x, out_idx=tf.int64)\r\nsess = tf.Session()\r\nprint(sess.run(idx))\r\n```\r\n\r\nOutput on Macbook Air:\r\n```\r\n2017-08-08 16:43:20.546840: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-08 16:43:20.546883: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-08 16:43:20.546895: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-08 16:43:20.546905: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nTraceback (most recent call last):\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1297, in _run_fn\r\n    self._extend_graph()\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1358, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Unique' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n\r\n\t [[Node: Unique = Unique[T=DT_INT64, out_idx=DT_INT64](Const)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"unique.py\", line 7, in <module>\r\n    print(sess.run(idx))\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'Unique' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n\r\n\t [[Node: Unique = Unique[T=DT_INT64, out_idx=DT_INT64](Const)]]\r\n\r\nCaused by op 'Unique', defined at:\r\n  File \"unique.py\", line 5, in <module>\r\n    y, idx = tf.unique(x, out_idx=tf.int64)\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3927, in unique\r\n    result = _op_def_lib.apply_op(\"Unique\", x=x, out_idx=out_idx, name=name)\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/dturmukh/anaconda/envs/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Unique' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n\r\n\t [[Node: Unique = Unique[T=DT_INT64, out_idx=DT_INT64](Const)]]\r\n```", "comments": ["Created a PR #12129 to address the int64 support for `tf.unique`.", "Thanks @yongtang for the quick PR!  Marking this as community support; it'll get closed when the PR gets merged."]}, {"number": 12112, "title": "tf.contrib.slim.nets moved?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0.44 / 5.1\r\n- **GPU model and memory**: Nvidia Tesla P100\r\n- **Exact command to reproduce**: \r\nimport tensorflow as tf\r\nvgg = tf.contrib.slim.nets.vgg\r\n\r\n### Describe the problem\r\nThe Problem is that slim does not seem to contain the directory nets (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim/python/slim/nets) anymore even though in the repo it's still there.\r\nIt's also still in the slim docs (https://github.com/tensorflow/tensorflow/issues/12112).\r\n\r\n### Source code / logs\r\nimport tensorflow as tf\r\nvgg = tf.contrib.slim.nets.vgg\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\nAttributeError: module 'tensorflow.contrib.slim' has no attribute 'nets'\r\n", "comments": ["Duplicate https://github.com/tensorflow/tensorflow/issues/6064"]}, {"number": 12111, "title": "Added a bias term in Bahdanau attention alignments", "body": "The original paper by Bahdanau (https://arxiv.org/abs/1409.0473) lists a bias term (Eq. 18) and Tensorflow's normalized Bahdanau implementation uses one, so I don't see why we shouldn't use one in the original implementation.\r\n\r\nEdit: Err, I was wrong, staring at papers too much all day, and quoted an equation from the paper that introduces Bahdanau energy normalization (http://arxiv.org/abs/1704.00784). Still, the original may have used it too as they state they \"omit bias terms to make the equations less cluttered\". My bad - the reviewer decides what to do with this I guess. ", "comments": ["Can one of the admins verify this patch?", "@lmthang Mind taking a look?", "@lmthang any luck with this?", "@lmthang ping", "Hi guys,\r\n\r\nSorry for a delay on this. \r\n\r\nThere was no bias term in the original paper by Baudanau https://arxiv.org/pdf/1409.0473.pdf (see page 14). This change will break existing models that uses \"bahdanau\" attention, so I will oppose this change.\r\n(The author of this PR also acknowledged that he/she misread the paper).\r\n\r\nIf people would like to have bias in the attention, I'd recommend using normed_bahdanau instead. See https://github.com/tensorflow/nmt#hyperparameters.\r\n\r\n-Thang\r\n\r\n", "Closing..."]}, {"number": 12110, "title": "Android example broken with nativeBuildSystem = none", "body": "This commit breaks the android example when nativeBuildSystem = none since it directly uses the native image conversion functions rather than calling the java ones. The native conversions are not implemented when nativeBuildSystem = none.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/003deb88b7fb015db86089c2a87b3044cad2c714\r\n\r\nTo reproduce:\r\n\r\n1. Set nativeBuildSystem = none\r\n2. Build example\r\n3. Run on a phone\r\n4. Observe crash on startup and errors about unimplemented methods\r\n", "comments": []}, {"number": 12109, "title": "Compiling Tensorflow Crashes", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\n1.3-rc2\r\n- Exact command to reproduce:\r\ncpu=armeabi-v7a\r\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=$cpu --verbose_failures\r\n\r\nI have a VM setup with Ubuntu 16 64bit.\r\nI tried compiling the tensorflow library with Bazel to generate the .so and .jar files.\r\n\r\nI have modified the register_types.h file according to \r\nhttps://github.com/bmount/tensorflow/commit/24b59a4a7797623a9da9311ee1214ac334478ed3#diff-76e272a58ca1535b3e0ec93499779a14\r\n\r\nBut I don't think this issue is because of that change.\r\n\r\nThe Error is\r\n\r\n`ERROR: /home/anand/TensorflowAndroidPort/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:4539:1: Linking of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 1): arm-linux-androideabi-ar failed: error executing command`\r\n`Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 2134.608s, Critical Path: 54.46s\r\nFAILED: Build did NOT complete successfully`\r\n\r\nWith Verbose Failure Option:\r\n`ERROR: /home/anand/TensorflowAndroidPort/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:4539:1: Linking of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 1): arm-linux-androideabi-ar failed: error executing command \r\n  (exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-ar rcsD bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/aggregate_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/bias_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_bfloat.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_bool.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_complex128.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_complex64.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_double.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_float.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_half.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_int16.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_int32.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_int64.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_int8.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_uint16.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cast_op_impl_uint8.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/concat_lib_cpu.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/concat_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/constant_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_ops_common.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/dense_update_functor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/dense_update_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/example_parsing_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/fill_functor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/function_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/gather_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/identity_n_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/identity_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/immutable_constant_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/matmul_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/no_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/non_max_suppression_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/one_hot_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/pack_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reshape_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reverse_sequence_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/sendrecv_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/sequence_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/shape_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_3.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_4.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_5.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_6.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/slice_op_cpu_impl_7.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/softmax_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/split_lib_cpu.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/split_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/split_v_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_0.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_3.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_4.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_5.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_6.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/strided_slice_op_inst_7.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/unpack_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/variable_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/argmax_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/avgpooling_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/batch_matmul_op_real.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/batch_norm_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/bcast_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/check_numerics_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/control_flow_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_grad_filter_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_grad_input_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_grad_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_ops_fused.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/conv_ops_using_gemm.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/crop_and_resize_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_abs.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_add_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_add_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_bitwise_and.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_bitwise_or.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_bitwise_xor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_div.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_equal_to_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_equal_to_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_exp.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_floor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_floor_div.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_greater.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_greater_equal.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_invert.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_isfinite.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_less.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_less_equal.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_log.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_logical_and.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_logical_not.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_logical_or.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_maximum.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_minimum.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_mul_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_mul_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_neg.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_pow.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_reciprocal.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_rsqrt.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_select.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_sigmoid.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_sign.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_sqrt.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_square.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_squared_difference.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_sub.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_tanh.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/deep_conv2d.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/depthwise_conv_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/dynamic_partition_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/fake_quant_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/fifo_queue.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/fused_batch_norm_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/population_count_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/batchtospace_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/ctc_decoder_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/decode_bmp_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/depthtospace_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/dynamic_stitch_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/in_topk_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/logging_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/lrn_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/maxpooling_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op_cpu_impl_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op_cpu_impl_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op_cpu_impl_3.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op_cpu_impl_4.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/mirror_pad_op_cpu_impl_5.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/pad_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/padding_fifo_queue.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/padding_fifo_queue_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/queue_base.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/queue_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/random_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_all.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_any.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_common.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_max.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_mean.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_min.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_prod.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reduction_ops_sum.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/relu_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/resize_bilinear_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/resize_nearest_neighbor_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/restore_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/reverse_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/save_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/save_restore_tensor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/save_restore_v2_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/session_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/softplus_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/softsign_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/spacetobatch_functor.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/spacetobatch_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/spacetodepth_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/sparse_to_dense_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/stack_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/string_join_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/summary_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tensor_array.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tensor_array_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_functor_cpu.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_1.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_2.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_3.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_4.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_5.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_6.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/tile_ops_cpu_impl_7.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/topk_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/training_op_helpers.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/training_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/transpose_functor_cpu.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/transpose_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/warn_about_ints.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/where_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/xent_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/dequantize_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/meta_support.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantization_utils.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantize_down_and_shrink_range.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantize_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_activation_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_add_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_batch_norm_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_bias_add_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_concat_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_conv_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_instance_norm.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_matmul_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_mul_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_pooling_ops.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_reshape_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/quantized_resize_bilinear_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/requantization_range_op.o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/requantize.o)\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 2134.608s, Critical Path: 54.46s\r\nFAILED: Build did NOT complete successfully`\r\n\r\nBazel Version info:\r\n\r\n`Build label: 0.5.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 28 08:34:59 2017 (1501230899)\r\nBuild timestamp: 1501230899\r\nBuild timestamp as int: 1501230899`\r\n\r\nI'm using NDK version 12b with API version 23 and SDK 23 with Build Tools version 25.0.2 (SDK was installed through android studio)\r\n", "comments": ["Also see #11592 which looks related.  @andrewharp might have some suggestions.", "Hmmm, not sure I see the actual error message there :/ \r\n\r\nQuestions:\r\n- What happens if you try to execute `external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-ar` by itself?\r\n- Have you attempted to compile without the native code change?\r\n- Do you have enough disk space/memory for the VM?\r\n\r\nFWIW, we have a similar internal change that will hopefully be out in the next push or two to change the default detector to a multi-class model using the TF Object Detection api, so you should be able to use the prebuilt .so binaries then without any edits.", "This looks similar to the cases with TensorFlow Serving build failures due to insufficient RAM. If it fails in the link stage there usually is no specific message. If the failure is I the compile stage sometimes the text \u201cgcc: internal compiler error: Killed (program cc1plus)\u201d will be found in the error log.  \r\n\r\nYou are cross compiling and I do not know how much that will change things, but for i386 builds this has helped.\r\n\r\nTry adding this to your build command: \"bazel build -c opt --jobs 1 --local_resources 5000,1.0,1.0 --verbose_failures\"\r\n\r\nBecause your failure is in the link phase the --jobs 1 may have the most effect. For TensorFlow Serving the least amount of free RAM anyone, giving feedback, has has success with is 4GB. ", "Compiled correctly after I did a clean and reboot. Likely that it was a memory issue. Thanks for the help all.."]}, {"number": 12108, "title": "tf.string_split didn't behave the same as that split in python", "body": "In python, if we split the following string `'a#'.split('#')`, we will get a list of two elements: `['a', '']`.\r\nWhile in tensorflow, if we use `tf.string_split`, we will get an sparsetensor with only one elements. \r\nFor reproduction, you can run the following piece of code\r\n```\r\na = tf.constant(\"#2\")\r\nb = tf.string_split([a], \"#\")\r\nwith tf.Session() as ss:\r\n    ss.run(tf.local_variables_initializer())\r\n    print(ss.run(b))\r\n```\r\nAs a result, when we use `tf.string_split`, we don't know whether the `2` is before or after the `#`. I think such behavior is undesirable since we usually have missing values when decoding csv files.", "comments": ["In current implementation, by default empty results are skipped:\r\nhttps://github.com/tensorflow/tensorflow/blob/7280dafca161eb3413ea120d3dd07c63e5254e72/tensorflow/core/kernels/string_split_op.cc#L33\r\n\r\nI think it probably makes sense to add an additional argument in `tf.string_split` with `skip_empty`:\r\n```\r\ndef string_split(source, delimiter=\" \", skip_empty=True):\r\n...\r\n```\r\n\r\nI can submit a PR if tensorflow team is OK with that.", "It seems reasonable to add a skip_empty additional argument to string_split to enable this behavior. Thanks @yongtang for volunteering to create a PR with this. ", "Added a PR #12125 for this issue.", "@yongtang Thanks. So must I wait for the next release to use this feature after it's merged?"]}, {"number": 12107, "title": "Corrected dimension notes in attention_wrapper.py", "body": "I was following along your Bahdanau implementation while doing my own and noticed a bug in the documentation. While it's true you can project memory and query to `attention.num_inputs`, the context is of shape `[B, memory_size]`.\r\n\r\nTo illustrate the lines around the ones I edited:\r\n```\r\n# D_attention: num_units\r\n# D_encoded: memory size, encoder dimensionality, ...\r\n# T: time\r\n\r\nalignments # B x T x D_attention\r\nalignments_reduced # B x T\r\nmemory     # B x T x D_encoded\r\n\r\nalignments_reduced x memory # B x D_encoded\r\n```\r\n\r\nThe `alignments_reduced` is computed in [attention_wrapper.py#L447](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L447). ~~Additionally, I do not see mention of this step in the original paper but can see how it might be needed to make calculation of `context` possible.~~\r\n\r\n~~Maybe this is an incorrect interpretation of the original paper? It does not seem to mention a projection to a common dimensionality of both memory and query (`D_attention` above), denoted by `num_inputs` in the `BahdanauAttention` constructor. Say now we leave `D_attention = D_encoded`. The result would be that `alignments` and `memory` are of the same dimensionality, can safely be multiplied component-wise and then sum-reduced along the time dimension. This is an alternative interpretation of the paper, an approach that does not introduce `num_inputs` for attention and does not compute the sum along the last `num_inputs` axis (`alignments_reduced`).~~\r\n", "comments": ["Can one of the admins verify this patch?", "I would like to mention @lmthang who has once mentioned he is the developer of the early version of this file. ", "@oahziur says this looks good!", "@tensorflow-jenkins test this please."]}, {"number": 12106, "title": "Build fails for certain GCC paths", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.3-rc2\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n5.2\r\n- **CUDA/cuDNN version**:\r\n8.0 / 5.1.10\r\n- **GPU model and memory**:\r\nNvidia GTX 1080 Ti\r\n- **Exact command to reproduce**:\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --spawn_strategy=standalone\r\n\r\n### Describe the problem\r\nBuild will fail if compiler is not located in specific paths (like `/usr/bin`). Also will happen by compiling with a symbolic link to compiler if the link reside there.\r\n\r\n**Steps to reproduce**\r\nMake a symbolic link to GCC and store it somewhere like `/etc/gcc`. run `./configure` and set compiler path to `/etc/gcc` then run bazel build. This is probably why builds failing on certain many Linux distributions and is related to issues like #3550 and many other abandoned ones.\r\n\r\n### Source code / logs\r\n\r\n`$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --spawn_strategy=standalone`\r\n```\r\nWARNING: /opt/tensorflow-1.3.0-rc2/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /opt/tensorflow-1.3.0-rc2/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Found 1 target...\r\nERROR: /root/.cache/bazel/_bazel_root/84ac956a7b3384a65a68aa2a845ef1a1/external/lmdb/BUILD.bazel:8:1: C++ compilation of rule '@lmdb//:lmdb' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/84ac956a7b3384a65a68aa2a845ef1a1/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/opt/tensorflow-1.3.0-rc2/cuda-plat-sym \\\r\n    CUDNN_INSTALL_PATH=/opt/tensorflow-1.3.0-rc2/cuda-plat-sym \\\r\n    GCC_HOST_COMPILER_PATH=/etc/some-gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=5.1.10 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' -MD -MF bazel-out/local_linux-opt/bin/external/lmdb/_objs/lmdb/external/lmdb/mdb.pic.d -fPIC -iquote external/lmdb -iquote bazel-out/local_linux-opt/genfiles/external/lmdb -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -w -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/lmdb/mdb.c -o bazel-out/local_linux-opt/bin/external/lmdb/_objs/lmdb/external/lmdb/mdb.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nsome-gcc: error trying to exec 'cc1': execvp: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "comments": ["I was able to successfully reproduce this issue (by creating a symlink to gcc and alluding to that in ./configure)\r\n\r\nlooks like somehow the code in https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl can't access cc1plus which is usually installed with g++\r\n\r\n$locate cc1plus\r\n/usr/lib/gcc/x86_64-linux-gnu/4.8/cc1plus\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@reith Have you tried solution (https://github.com/tensorflow/tensorflow/issues/336#issuecomment-173633780) provided in https://github.com/tensorflow/tensorflow/issues/336 ? ", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@yzhwang no, sorry. I'm terribly busy and am not able to follow this.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Close due to inactivity. Feel free to reopen it if the solution I suggested in earlier comment doesn't work. Thanks!"]}, {"number": 12105, "title": "show proper error message when run `saved_model_cli` without arguments instead of an error said `AttributeError: 'Namespace' object has no attribute 'func'`", "body": "before:\r\n```bash\r\n$ saved_model_cli\r\nTraceback (most recent call last):\r\n  File \"/home/yjmade/Envs/tf1.3/bin/saved_model_cli\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/yjmade/Envs/tf1.3/lib/python3.5/site-packages/tensorflow/python/tools/saved_model_cli.py\", line 649, in main\r\n    args.func(args)\r\nAttributeError: 'Namespace' object has no attribute 'func'\r\n```\r\n\r\nafter:\r\n```bash\r\n$ saved_model_cli\r\nusage: saved_model_cli [-h] [-v] {show,run} ...\r\nsaved_model_cli: error: too few parameters\r\n```\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Thanks for the contribution."]}, {"number": 12104, "title": "Ensure that bazel versions of the form X.Y.Z-mmmmm work correctly", "body": "The code in configure.py doesn't allow for bazel versions which identify themselves as X.Y.Z-mmmmm.  This is true for bazel on OS/X installed through homebrew.\r\n\r\nRecently some code that checks the version number has appeared, and this is causing configurations built with OS/X homebrew bazel to fail.\r\n\r\n", "comments": ["@DavidNorman, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener and @vrv to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 12103, "title": "Add framework for disabling C++ tests", "body": "This adds a small amount of scaffolding needed to get the disabled tests manifest into the runfiles for each test, and set a string to the name of the file in the test_macros rule.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@hawkinsp Done\r\n", "@tensorflow-jenkins test this please", "Looks good, thanks for the PR!", "cheers\r\n"]}, {"number": 12102, "title": "Speed up safe_strtod and safe_strtof functions by using double-conversion library", "body": "#11713", "comments": ["Can one of the admins verify this patch?", "Thanks for the contribution. It doesn't seem desirable to add a new dependency to TensorFlow for string-double conversion alone. Do you have benchmarks showing what the gain is on existing TensorFlow benchmark models?\r\n\r\n@cwhipkey what is your opinion?\r\n", "@rmlarsen, performance improvement is so attractive :)\r\nThere is benchmark: https://gist.github.com/AKindyakov/2c528beccb11a24c1e6ec02ec12df47e\r\nDC is significantly faster.", "@AKindyakov thanks. That is indeed a nice speedup. \r\n@martinwicke @vrv what are your opinions? Should we introduce this new dependency or try to speed up decode_csv in another way?", "We will investigate and see what direction to take.", "@rmlarsen any decision on this?\r\n\r\nI think we might be open sourcing the google library, which is probably faster than the TF re-implementation.", "@drpngx I have not had time to pursue this. ", "JFYI patch about case insensitivity for special values was accepted and merged into the master branch.\r\nhttps://github.com/google/double-conversion/pull/47\r\n", "Oh, I hadn't realized this was from google. @martinwicke wdyt about taking on this additional dependency?", "I have no problem with it. Seems lightweight enough. We should double-check whether it supports all platforms, but I see MSVC files there, so I presume the answer is yes.", "@AKindyakov can you look at the merge conflict and the PR comments", "Yes, sure, I'll do it this night.", "I honestly don't have time to look at this right now, unassigning myself. @sb2nov could you re-assign, please?", "@AKindyakov there is a merge conflict. Can you take a look.", "@sb2nov, it's done", "Jenkins, test this please.", "Jenkins, test this, please.", "@sb2nov, I've fixed build and test problems. May I ask you to run full tests set for it?", "Jenkins, test this please.\n\nOn Oct 4, 2017 2:21 AM, \"Alexander\" <notifications@github.com> wrote:\n\n> @sb2nov <https://github.com/sb2nov>, I've fixed build and test problems.\n> May I ask you to run full tests set for it?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12102#issuecomment-334097757>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbfXFxLiqu791N0g8cUS1PTNIHweHks5so04RgaJpZM4Owazw>\n> .\n>\n", "I've fixed makefile build and have fixed up dependencies lists in ``tensorflow/core``.\r\nMay I ask you to run full tests on it?", "Fixed problems with sanity check", "Can I ask someone to run the full test set for it?", "jenkins, test this please", "@cwhipkey I fixed up ```CMake``` build, remove octal number support and add test cases with trailing and leading characters. Could you please run tests on it?\r\n\r\nI was looking for a code in double conversion library which we can throw out in the android build. But this library is very solid and there is nothing to cut off.", "+ @petewarden  - pete, this will increase binary size some on  mobile (maybe not the full 77k, but it will probably be an increase).  What do you think we should do?   Some options are:\r\n\r\n1. use ifdef to use the old code on mobile.  This clearly would cause no binary increase on mobile, but makes another code path that will be less tested and it's a bit unfortunate to have 2 code paths.\r\n\r\n2. similar to 1., but do it in a way that can be tested off android.\r\n\r\n3. add in the performant library and code only use by the kernels, but not in the string library.  This seems not great, since you would need to call the fast library from the kernels, instead of just using the string library.\r\n\r\n4. test more fully and maybe it won't be a big change, or we can find savings elsewhere.", "Is there any update on this? This issue seems to be a current bottleneck in my code, it adds a huge overhead when reading data.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "There are still problems with the Windows build, relevant error:\r\n\r\n```\r\n17:22:02 LINK : fatal error LNK1181: cannot open input file 'double_conversion\\src\\double_conversion\\double-conversion\\double-conversion.lib' [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\proto_text.vcxproj]\r\n```", "@mrry, do you know how to fix this?", "At a guess, maybe the static library has a different filename on Windows? Maybe there's a missing `lib/` in the definition of `double_conversion_STATIC_LIBRARIES`?", "Is it me suppose to fix this?\r\nI don't have any WIN machine for the test.", "@AKindyakov yes usually the committer ensures that everything builds, with our help. For this one, I pushed a commit that I think should fix it.", "@drpngx thanks a lot!", "Looks like there's a sycl breakage at `master`. Let's wait.", "Jenkins, test this please.", "@martinwicke, @drpngx have you any idea what is wrong with the build? There are no errors in build log only warnings which are not related to my changes at all.", "The mac test failed with a timeout.\r\n\r\nThe CMake failed with the same error as before: \r\n```\r\n22:31:05 LINK : fatal error LNK1181: cannot open input file 'double_conversion\\src\\double_conversion\\double-conversion\\lib\\double-conversion.lib' [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\proto_text.vcxproj]\r\n```\r\nThe other errors don't appear to be related. Let's try again.", "@martinwicke, @drpngx, I believe I fixed up windows build. May I ask you to test it, please? (sorry for the delay - busy time)", "Jenkins. Test this please.", "@martinwicke is there a way to fix the CLA bot here. It was approved yes but the check seems to be pending.", "It looks to me like it is stuck. I will override. I guess the XLA tests are known to be broken?\r\n\r\n@willnorris FYI, not sure how to get it out of its funk.", "A Googler has manually verified that the CLAs look good, thanks! They commented:\n\n&gt; manually verified\n\n<!-- manual_ok -->", "There's no obvious reason that I can see that would have caused this to get stuck, but I've manually approved it and will look into the cause.", "Thanks!", "Just as a heads-up: This contribution is causing difficulties when the code is sync'ed to Google internal. We are considering reverting it first to unblock the code sync'ing, and maybe finding a way to make a similar change in internal.\r\n\r\ncc @gunan "]}, {"number": 12101, "title": "Build the latest source code will fail under Linux platform", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10 (Artful Aardvark)\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.3\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nBuild the the TF from the latest version of source code will fail.\r\n\r\n### Source code / logs\r\nWARNING: /home/kevin/research/openSource/tensorflow-fork/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/kevin/research/openSource/tensorflow-fork/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (177 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: no such package '@protobuf//src/google/protobuf': Could not find handler for bind rule //external:protobuf\r\nINFO: Elapsed time: 10.961s, Critical Path: 0.12s\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["You need:\r\n`bazel clean`\r\nbefore build/rebuild", "It has happened to me too and the `bazel build`(`bazel build -c opt --copt=-mavx --copt=-mfpmath=both --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package`) failed. However, after I did `bazel clean` the build was successful, but some warnings still appeared (showed below). Also, there were a huge amount of `INFO` messages (I was not able to copy all and paste here). I was with the version `1.1.0` installed and I am updating to `v1.3` (`git checkout v1.3` before `./configure` and `bazel build `.\r\n```\r\nWARNING: /home/victor/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\n\r\nWARNING: /home/victor/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\n\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (176 packages loaded).\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/snappy/snappy-c.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++ [enabled by default]\r\ncc1plus: warning: unrecognized command line option \"-Wno-shift-negative-value\" [enabled by default]\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++ [enabled by default]\r\ncc1plus: warning: unrecognized command line option \"-Wno-shift-negative-value\" [enabled by default]\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++ [enabled by default]\r\ncc1plus: warning: unrecognized command line option \"-Wno-shift-negative-value\" [enabled by default]\r\nINFO: From Compiling external/snappy/snappy-c.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++ [enabled by default]\r\ncc1plus: warning: unrecognized command line option \"-Wno-shift-negative-value\" [enabled by default]\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++ [enabled by default]\r\ncc1plus: warning: unrecognized command line option \"-Wno-shift-negative-value\" [enabled by default]\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++ [enabled by default]\r\ncc1plus: warning: unrecognized command line option \"-Wno-shift-negative-value\" [enabled by default]\r\nINFO: From Compiling external/snappy/snappy.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++ [enabled by default]\r\nexternal/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':\r\nexternal/snappy/snappy.cc:1403:38: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < blocks_.size(); ++i) {\r\n                                      ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option \"-Wno-shift-negative-value\" [enabled by default]\r\nINFO: From Compiling external/snappy/snappy.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++ [enabled by default]\r\nexternal/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':\r\nexternal/snappy/snappy.cc:1403:38: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < blocks_.size(); ++i) {\r\n                                      ^\r\n\r\n\r\n```", "Fails for me under ubuntu 17.10  running cuda 9 and cudnn 7.0.4\r\nused gcc 5.4 compiler\r\nGot following errors:\r\n\r\nchris@Rearden:~/tensorflow$ bazel clean\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nchris@Rearden:~/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\nWARNING: /home/chris/tensorflow/tensorflow/core/BUILD:1781:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/chris/tensorflow/tensorflow/tensorflow.bzl:1044:30\r\nWARNING: /home/chris/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/chris/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (210 packages loaded).\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy-c.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nINFO: From Compiling external/snappy/snappy.cc [for host]:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nexternal/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':\r\nexternal/snappy/snappy.cc:1403:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < blocks_.size(); ++i) {\r\n                       ^\r\nIn file included from external/snappy/snappy-internal.h:34:0,\r\n                 from external/snappy/snappy.cc:30:\r\nexternal/snappy/snappy.cc: In instantiation of 'bool snappy::SnappyScatteredWriter<Allocator>::AppendFromSelf(size_t, size_t) [with Allocator = snappy::SnappySinkAllocator; size_t = long unsigned int]':\r\nexternal/snappy/snappy.cc:715:13:   required from 'void snappy::SnappyDecompressor::DecompressAllTags(Writer*) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>]'\r\nexternal/snappy/snappy.cc:799:3:   required from 'bool snappy::InternalUncompressAllTags(snappy::SnappyDecompressor*, Writer*, snappy::uint32) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>; snappy::uint32 = unsigned int]'\r\nexternal/snappy/snappy.cc:1460:78:   required from here\r\nexternal/snappy/snappy.cc:1316:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (PREDICT_TRUE(offset - 1u < op_ptr_ - op_base_ && op_end <= op_limit_)) {\r\n                                  ^\r\nexternal/snappy/snappy-stubs-internal.h:80:25: note: in definition of macro 'PREDICT_TRUE'\r\n #define PREDICT_TRUE(x) x\r\n                         ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-shift-negative-value'\r\nERROR: /home/chris/.cache/bazel/_bazel_chris/a819f6789d392dd47d3107e5934b6ad1/external/com_googlesource_code_re2/BUILD:11:1: C++ compilation of rule '@com_googlesource_code_re2//:re2' failed (Exit 1)\r\nexternal/com_googlesource_code_re2/re2/prefilter_tree.cc: In member function 'std::__cxx11::string re2::PrefilterTree::NodeString(re2::Prefilter*) const':\r\nexternal/com_googlesource_code_re2/re2/prefilter_tree.cc:116:14: error: 'to_string' is not a member of 'std'\r\n   string s = std::to_string(node->op()) + \":\";\r\n              ^\r\nexternal/com_googlesource_code_re2/re2/prefilter_tree.cc:123:12: error: 'to_string' is not a member of 'std'\r\n       s += std::to_string((*node->subs())[i]->unique_id());\r\n            ^\r\nexternal/com_googlesource_code_re2/re2/prefilter_tree.cc: In member function 'std::__cxx11::string re2::PrefilterTree::DebugNodeString(re2::Prefilter*) const':\r\nexternal/com_googlesource_code_re2/re2/prefilter_tree.cc:392:22: error: 'to_string' is not a member of 'std'\r\n       node_string += std::to_string((*node->subs())[i]->unique_id());\r\n                      ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 10.937s, Critical Path: 3.14s\r\nFAILED: Build did NOT complete successfully\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "I gave up. Back on 16.04", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "See comment. I gave up on this. I have no idea how to change label. ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "How?\n\nGet Yahoo Mail for Mobile \n \n  On Thu, Jan 4, 2018 at 2:13 PM, Alfred<notifications@github.com> wrote:   \nIt has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue? Please update the label and/or status accordingly.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n    \n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@CD1010  do you solve your problem? I get the same problem", "@wwb20030901 No.  I think we have to wait till they support 17.10 Ubuntu.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closed", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 12100, "title": "Update head.py", "body": "Replace scalar_summary for tf.summary.scalar", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@alanyee please address the remaining test failures in:\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/6219/consoleFull", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@alanyee @ispirmustafa It looks like there are some non-trivial changes in the tags causing a failure in the unit tests. Is this expected?\r\n\r\n```\r\n======================================================================\r\nFAIL: testBinaryClassificationEval (__main__.BinaryClassificationHeadTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 814, in testBinaryClassificationEval\r\n    _assert_summary_tags(self, [\"loss\"])\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 90, in _assert_summary_tags\r\n    test_case.assertItemsEqual(expected_tags or [], actual_tags)\r\nAssertionError: Element counts were not equal:\r\nFirst has 1, Second has 0:  'loss'\r\nFirst has 0, Second has 1:  u'binary_logistic_head/loss'\r\n```", "Ah, I have been trying to get around that problem which has occurred for another PR: \r\n```----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 1444, in testMultiClassWithLabelKeysEvalAccuracy1\r\n    _assert_summary_tags(self, [\"loss\"])\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 90, in _assert_summary_tags\r\n    test_case.assertItemsEqual(expected_tags or [], actual_tags)\r\nAssertionError: Element counts were not equal:\r\nFirst has 1, Second has 0:  'loss'\r\nFirst has 0, Second has 1:  u'multi_class_head/loss'\r\n```\r\n\r\nThe problem is essentially that the training and eval losses show up in different graphs which is why the deprecated API was used in the first place.", "Yes we intentionally keep deprecated summary API in head.py under contrib.\r\nThe heads we moved to core (python/estimator/canned/head.py) we use the new summary API.\r\nClosing this since we'll not change deprecated api usages in contrib version of head.", "@ispirmustafa okay but i wanted to test out one more change to see if it would work"]}, {"number": 12099, "title": "Cannot import tensorflow 1.0.1 after compiled from source", "body": "\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.0.1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.3\r\n- **CUDA/cuDNN version**: x\r\n- **GPU model and memory**: x\r\n- **Exact command to reproduce**: import tensorflow\r\n\r\n### Describe the problem\r\nI successfully built tensorflow 1.0.1 from source using optimization flags with the command `bazel build -c opt --copt=-mmmx --copt=-msse --copt=-msse2  -k //tensorflow/tools/pip_package:build_pip_package`. After that, I activated my virtualenv and issued a `pip install /tmp/tensorflow_pkg/tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl` to install in the virtual env this compiled from source version. When I try to import tensorflow in the python code, I receive a error (showing it with -v flag from python interpreter activated) showed below in the source code / logs session.\r\n\r\n### Source code / logs\r\n```import numpy.ma.core # precompiled from /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/numpy/ma/core.pyc\r\n# /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/numpy/ma/extras.pyc matches /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/numpy/ma/extras.py\r\nimport numpy.ma.extras # precompiled from /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/numpy/ma/extras.pyc\r\n# /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.pyc matches /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\r\nimport tensorflow.python.pywrap_tensorflow # precompiled from /home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.pyc\r\ndlopen(\"/home/ubuntu/miniconda3/envs/tf_exp/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\", 102);\r\nIllegal instruction (core dumped)\r\n```\r\n", "comments": ["@flavioschuindt Can you retry from the latest sources?", "@tatatodd Ok, I had some progress here. I tried all the stpes above in a VirtualBox ubuntu machine. It seems that VirtualBox is not fully activating all of these CPU instructions, so in runtime (`import tensorflow`) I got this error. Do you know how to fully activate it in VirtualBox?", "@flavioschuindt In your description, I do not see that if you ran this command to build the pip package:\r\n```\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp\r\n```\r\nWithout this, it is possible you have a stale pip package and not building the latest one.\r\n\r\n", "Yes, I ran this command. The `whl` package was properly built, but when tried to run it in vagrant I received the error that I mentioned previously.", "I think you are right, you are not emulating the right cpu in virtualbox. Try building w/o the --copt=-mmmx --copt=-msse --copt=-msse2 and make sure that works.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12098, "title": "Tensorflow GPU import error", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform**: Windows 10 Pro x64\r\n- **TensorFlow installed from**: pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.5.3\r\n- **CUDA/cuDNN version**: CUDA8.0 / cuDNN v7\r\n- **GPU model and memory**: GTX 1050 \r\n- **Exact command to reproduce**: ```import tensorflow```\r\n\r\n### Describe the problem\r\nIt is impossible to import tensorflow in my present environment. I have checked for a compatible GPU and the neccessary dll's. Even though it hasn't been able to import the tensorflow modules.\r\n\r\n### Source code / logs\r\n```python\r\nTraceback (most recent call last):\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: No se puede encontrar el m\u00f3dulo especificado.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "comments": ["I was able to fix the issue by downgrading cuDNN to v5."]}, {"number": 12097, "title": "I got a type error without rhyme or reason\uff0cwhen i rewrite a _linear function in the rnn_cell_impl.py", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win64\r\n- **TensorFlow installed from (source or binary)**: pip \r\n- **TensorFlow version (use command below)**:1.2\r\n- **Python version**:  3.6\r\n- **CUDA/cuDNN version**:ONLY CPU\r\n- **GPU model and memory**:ONLY CPU\r\n- **Exact command to reproduce**: No\r\n\r\n### Describe the problem\r\nI want to write a lstm with batch normalization . After , i read the code of BasicLSTMCell , i find i only need to wirte a _linear function acording to this paper https://arxiv.org/pdf/1603.09025.pdf section 3\r\nand the new _batchlinear function is  below here , the only difference between _batchlinear function  and \r\n_linear function is  the arg mul it's weights separately and do  it's batch normalization .when i build a  multi layer rnn like this \r\n```\r\ncells       = [BatchLSTMCell(rnn_numhidden,forget_bias=0.,activation=tf.tanh) for _ in range(num_rnn)]\r\nstack       = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\r\nnet, _      = tf.nn.dynamic_rnn(stack, net, seqlen_batch, dtype=tf.float32)\r\n```\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'\r\n### Source code / logs\r\nsource code are here\r\n```\r\ndef _batchlinear(   args,\r\n                    output_size,\r\n                    bias,\r\n                    xh_epsilon = 1e-3,\r\n                    hh_epsilon = 1e-3,\r\n                    bias_initializer=None,\r\n                    kernel_initializer= None):           \r\n  if args is None or (nest.is_sequence(args) and not args):\r\n    raise ValueError(\"`args` must be specified\")\r\n  if not nest.is_sequence(args):\r\n    args = [args]\r\n\r\n  # Calculate the total size of arguments on dimension 1.\r\n  total_arg_size = 0\r\n  shapes = [a.get_shape() for a in args]\r\n  for shape in shapes:\r\n    if shape.ndims != 2:\r\n      raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\r\n    if shape[1].value is None:\r\n      raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\r\n                       \"but saw %s\" % (shape, shape[1]))\r\n    else:\r\n      total_arg_size += shape[1].value\r\n\r\n  dtype = [a.dtype for a in args][0]\r\n\r\n  # Now the computation.\r\n  scope = vs.get_variable_scope()\r\n  with vs.variable_scope(scope) as outer_scope:\r\n    if len(args) == 1:\r\n        weights_xh = vs.get_variable('W_xh',\r\n            [shapes[0], output_size],\r\n            dtype = dtype,\r\n            initializer=kernel_initializer)\r\n        res = math_ops.matmul(args[0], weights_xh)\r\n    else:\r\n        weights_xh = vs.get_variable('W_xh',\r\n            [shapes[0], output_size],\r\n            dtype = dtype,\r\n            initializer=kernel_initializer)\r\n        xh = math_ops.matmul(args[0], weights_xh)\r\n        xh_scale = vs.get_variable('xh_scale', [output_size], initializer=init_ops.constant_initializer(0.1, dtype=dtype))\r\n        xh_offset = vs.get_variable('xh_offset', [output_size])\r\n        xh_batch_mean, xh_batch_var = nn_impl.moments(xh, [0])\r\n        xh = (xh - xh_batch_mean) / math_ops.sqrt(xh_batch_var + xh_epsilon)\r\n        xh = xh_scale*xh + xh_offset\r\n        if kernel_initializer is None:\r\n            weights_hh = vs.get_variable('W_hh',\r\n                [shapes[0], output_size],\r\n                dtype = dtype)\r\n        hh = math_ops.matmul(args[0],weights_hh)\r\n        hh_scale = vs.get_variable('hh_scale', [output_size], initializer=init_ops.constant_initializer(0.1, dtype=dtype))\r\n        hh_offset = vs.get_variable('hh_offset', [output_size])\r\n        hh_batch_mean, hh_batch_var = nn_impl.moments(hh, [0])\r\n        hh = (hh - hh_batch_mean) / math_ops.sqrt(hh_batch_var + hh_epsilon)\r\n        xh = hh_scale*hh + hh_offset\r\n        res = xh+hh\r\n    if not bias:\r\n      return res\r\n    with vs.variable_scope(outer_scope) as inner_scope:\r\n      inner_scope.set_partitioner(None)\r\n      if bias_initializer is None:\r\n        bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\r\n      biases = vs.get_variable(\r\n          'bias', [output_size],\r\n          dtype=dtype,\r\n          initializer=bias_initializer)\r\n    return nn_ops.bias_add(res, biases)\r\n```\r\n```\r\nclass BatchLSTMCell(RNNCell):\r\n  \"\"\"Basic LSTM recurrent network cell.\r\n\r\n  The implementation is based on: http://arxiv.org/abs/1409.2329.\r\n\r\n  We add forget_bias (default: 1) to the biases of the forget gate in order to\r\n  reduce the scale of forgetting in the beginning of the training.\r\n\r\n  It does not allow cell clipping, a projection layer, and does not\r\n  use peep-hole connections: it is the basic baseline.\r\n\r\n  For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell}\r\n  that follows.\r\n  \"\"\"\r\n\r\n  def __init__(self, num_units, forget_bias=1.0,\r\n               state_is_tuple=True, activation=None, reuse=None):\r\n    \"\"\"Initialize the basic LSTM cell.\r\n\r\n    Args:\r\n      num_units: int, The number of units in the LSTM cell.\r\n      forget_bias: float, The bias added to forget gates (see above).\r\n      state_is_tuple: If True, accepted and returned states are 2-tuples of\r\n        the `c_state` and `m_state`.  If False, they are concatenated\r\n        along the column axis.  The latter behavior will soon be deprecated.\r\n      activation: Activation function of the inner states.  Default: `tanh`.\r\n      reuse: (optional) Python boolean describing whether to reuse variables\r\n        in an existing scope.  If not `True`, and the existing scope already has\r\n        the given variables, an error is raised.\r\n    \"\"\"\r\n    super(BatchLSTMCell, self).__init__(_reuse=reuse)\r\n    if not state_is_tuple:\r\n      logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\r\n                   \"deprecated.  Use state_is_tuple=True.\", self)\r\n    self._num_units = num_units\r\n    self._forget_bias = forget_bias\r\n    self._state_is_tuple = state_is_tuple\r\n    self._activation = activation or math_ops.tanh\r\n\r\n  @property\r\n  def state_size(self):\r\n    return (LSTMStateTuple(self._num_units, self._num_units)\r\n            if self._state_is_tuple else 2 * self._num_units)\r\n\r\n  @property\r\n  def output_size(self):\r\n    return self._num_units\r\n\r\n  def call(self, inputs, state):\r\n    \"\"\"Long short-term memory cell (LSTM).\"\"\"\r\n    sigmoid = math_ops.sigmoid\r\n    # Parameters of gates are concatenated into one multiply for efficiency.\r\n    if self._state_is_tuple:\r\n      c, h = state\r\n    else:\r\n      c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\r\n\r\n      concat = _batchlinear(args=[inputs, h], output_size=4 * self._num_units, bias=True)\r\n\r\n    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\r\n    i, j, f, o = array_ops.split(value=concat, num_or_size_splits=4, axis=1)\r\n\r\n    new_c = (\r\n        c * sigmoid(f + self._forget_bias) + sigmoid(i) * self._activation(j))\r\n    new_h = self._activation(new_c) * sigmoid(o)\r\n\r\n    if self._state_is_tuple:\r\n      new_state = LSTMStateTuple(new_c, new_h)\r\n    else:\r\n      new_state = array_ops.concat([new_c, new_h], 1)\r\n    return new_h, new_state\r\n```\r\n\r\nlogs are here \r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-21f874a460ec>\", line 1, in <module>\r\n    runfile('C:/Users/Administrator/Test/char_rnn.py', wdir='C:/Users/Administrator/Test')\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 688, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 101, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/Administrator/Test/char_rnn.py\", line 25, in <module>\r\n    final_logits = charRnn(char_seq_batch,seqlen_batch,charsize=charsize,num_rnn=num_layers,rnn_numhidden = rnn_numhidden)\r\n\r\n  File \"C:\\Users\\Administrator\\Test\\networks.py\", line 221, in charRnn\r\n    net, _      = tf.nn.dynamic_rnn(stack, net, seqlen_batch, dtype=tf.float32)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 574, in dynamic_rnn\r\n    dtype=dtype)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 737, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2770, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2599, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2549, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 720, in _time_step\r\n    skip_conditionals=True)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 206, in _rnn_step\r\n    new_output, new_state = call_cell()\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 708, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 181, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 441, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 917, in call\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 181, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 441, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\Administrator\\Test\\networks.py\", line 167, in call\r\n    concat = _batchlinear(args=[inputs, h], output_size=4 * self._num_units, bias=True)\r\n\r\n  File \"C:\\Users\\Administrator\\Test\\networks.py\", line 80, in _batchlinear\r\n    initializer=kernel_initializer)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1065, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 962, in get_variable\r\n    use_resource=use_resource, custom_getter=custom_getter)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 360, in get_variable\r\n    validate_shape=validate_shape, use_resource=use_resource)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1405, in wrapped_custom_getter\r\n    *args, **kwargs)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 184, in _rnn_get_variable\r\n    variable = getter(*args, **kwargs)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 184, in _rnn_get_variable\r\n    variable = getter(*args, **kwargs)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 352, in _true_getter\r\n    use_resource=use_resource)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 653, in _get_single_variable\r\n    shape = tensor_shape.as_shape(shape)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 798, in as_shape\r\n    return TensorShape(shape)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 434, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 434, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 376, in as_dimension\r\n    return Dimension(value)\r\n\r\n  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\", line 32, in __init__\r\n    self._value = int(value)\r\n\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'", "comments": []}, {"number": 12096, "title": "I configured ops_to_register.h and set #define SHOULD_REGISTER_OP_GRADIENT false, but unexpectedly the generated libtensorflow_inference.so file get larger.", "body": "### Describe the problem\r\n I  configured ops_to_register.h as below:\r\n\r\n#define SHOULD_REGISTER_OP(op) true\r\n#define SHOULD_REGISTER_OP_GRADIENT false\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) true\r\n\r\nset  SHOULD_REGISTER_OP_GRADIENT false. And then I used command below to generate libtensorflow_inference.so file. \r\n\r\n`bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\                                         \r\n            --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\r\n            //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n            --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n            --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a`\r\n\r\nI expected that this configuration will reduce gradient operations and make the genereted libtensorflow_inference.so file smaller. But unexpectedly, the generated file is 16M ,while  the .so file generate by command below which will include all operators is 9.7M.\r\n\r\n` bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n         --crosstool_top=//external:android/crosstool \\\r\n         --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n         --cpu=armeabi-v7a`\r\n\r\n### Question\r\nI can't understand why the .so file get larger when  I set SHOULD_REGISTER_OP_GRADIENT false than the original .so file which include all operators. Could you please explain it to me ? Thank you very much and hope for your answer.", "comments": ["Selective registration draws from a larger set of potential ops than the regular TF build. It appears what you have done with your macro definitions is to set it to include all these ops, excluding only the gradients which are used for training. Gradients are already excluded by default, and you've added more ops, therefore it makes sense you'll get a larger resulting library.\r\n\r\nAs explained in #11936, you'll want to used [print_selective_registration_header.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/print_selective_registration_header.py) to generate a custom header which only causes the ops you actually use to be linked in.\r\n", "Yeah , I can reduce the .so size when using method of print_selective_registration_header.py. But I want to make it available for different algorithm models.  \r\n\r\nSo the default configuration doesn't include all SHOULD_REGISTER_OP_KERNEL and SHOULD_REGISTER_OP opretions ? But I see the code of selective_registration.h id like this:\r\n![image](https://user-images.githubusercontent.com/10495849/29102454-d011a392-7ceb-11e7-98c4-baa55c0e1d7d.png)\r\n#define SHOULD_REGISTER_OP(op) true\r\n#define SHOULD_REGISTER_OP_GRADIENT true\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) true\r\n\r\nThey are all set as true in selective_registration.h \r\n\r\nI get confused. Which file configured the default  included operations ? Is it selective_registration.h ?  Can I get all available operations somewhere ? ", "Those defines you are referencing are only active if you _have not_ defined [SELECTIVE_REGISTRATION](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/selective_registration.h#L21) in your build command. Otherwise they come from ops_to_register.h.\r\n\r\nAs described in the comments, print_selective_registration.py will give you an ops_to_register.h file.\r\n\r\nHowever if you want a general-purpose library capable of handling multiple models, you should probably just stick to the default builds. The entire point of selective registration is to remove unnecessary ops, and if you want to support different models you need to know ahead of time which ones are safe to remove.\r\n\r\nFor further assistance on this I would recommend posting on StackOverflow, as there is a larger community for support there, thanks.", "Thank you very much~"]}, {"number": 12095, "title": "WARNING:tensorflow:The default stddev value of initializer will change from \u201c1/sqrt(vocab_size)\u201d to \u201c1/sqrt(dimension)\u201d after 2017/02/25", "body": "I got this warning message when set Deep Model.\r\n\r\nWarning message\r\n\r\n```\r\nWARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\r\n```\r\n\r\nMy code\r\n\r\n```\r\n    deep_columns = [\r\n      tf.contrib.layers.embedding_column(workclass, dimension=8),\r\n      tf.contrib.layers.embedding_column(education, dimension=8),\r\n      tf.contrib.layers.embedding_column(gender, dimension=8),\r\n      tf.contrib.layers.embedding_column(relationship, dimension=8),\r\n      tf.contrib.layers.embedding_column(native_country, dimension=8),\r\n      tf.contrib.layers.embedding_column(occupation, dimension=8),\r\n      age, education_num, capital_gain, capital_loss, hours_per_week\r\n    ]\r\n```\r\n\r\nPlease advice. Thank you.", "comments": ["The `tf.contrib.layers.embedding_column` has no `initializer` value set. If you don't set it then defaults to `tf.truncated_normal_initializer` with mean 0.0 and `standard deviation 1/sqrt(sparse_id_column.length).`\r\n\r\nSo,the warning states that A new changed implementation for default of standard deviation  will be `1/sqrt(dimension)` after 2017/02/25 instead of `1/sqrt(vocab_size)`", "@printdhruv Thanks for the explanation!\r\n\r\n@datomnurdin For future reference, note that this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12094, "title": "Add `.idea/**` into .gitignore for JetBrains IDE", "body": "We usually use `PyCharm` for reading the source code of `Tensorflow`, so it is necessary for most developers to add `.idea/**` into .gitignore for `JetBrains` IDE to ignore those non-project files.", "comments": ["@asdf2014, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @benoitsteiner and @keveman to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "I think is generally not best practice to add IDE specific rules to a project gitignore. Rather have the developer use a global gitignore on his/her system. See for example: https://github.com/joeblau/gitignore.io/issues/4"]}]