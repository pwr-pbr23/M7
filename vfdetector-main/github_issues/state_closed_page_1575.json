[{"number": 5664, "title": "one function in tensorflow tutorials should be replaced", "body": "I'm learning tutorials of tf.contrib.learn Quickstart in this [website](https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html) and found that the function **tf.contrib.learn.datasets.base.load_csv**  isn't existed.So the following codes\r\n`training_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TRAINING,\r\n                                                       target_dtype=np.int)`\r\n`test_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TEST,\r\n                                                   target_dtype=np.int)`\r\n should be replaced by\r\n`training_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TRAINING,\r\n                                                       target_dtype=np.int,features_dtype=np.float32)`\r\n`test_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TEST,\r\n                                                   target_dtype=np.int,features_dtype=np.float32)\r\n`\r\n\r\nand my tensorflow version is  (0.11.0rc1)", "comments": ["@BoyuanJiang Can you try with 0.11?  0.11.0rc1 has 2d4267507e312007a062a90df37997bca8019cfb.\n", "yeah,I know that def load_csv has been replaced by def load_csv_with_header.I mean that the tutorials of this [website](https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html)should be update,because in the tutorials still use the function load_csv and may confuse the beginners.\n\n(It's my first time use github and not sure where should I issue the problem 0_0\n", "@BoyuanJiang It looks like the tutorials have also been updated (if you look at the master tutorials), so I think we're good.\n"]}, {"number": 5663, "title": "slim.batch_norm used with slim.conv2d problem", "body": "Dear All,\r\nI have met some problems when using the batch norm layer of slim. I trained the same model structure but with different ways to use batch_norm layer, shown below. It seems the output of the two ways to use batch_norm is different to me. I have look inside the code to see. It seems the two way no different at all. But the result is actually not the same. Anyone met it before?\r\n\r\n1. net = slim.conv2d(net, 64, [4, 4], 2, normalizer_fn=None, activation_fn=None, biases_initializer=None, reuse=reuse)\r\n   output1 = slim.batch_norm(net)\r\n\r\n2. output2 = slim.conv2d(net, 64 ,[4, 4], 2, normalizer_fn=slim.batch_norm, activation_fn=None, reuse=reuse)\r\n\r\n\r\n", "comments": ["I believe one of those ways normalizes before the activation.  The other does not.\n", "There is no activation function because it is set to None.\n\nVariable reuse isn't set to `reuse` in the batch_norm of example 1.  Otherwise I would expect the results to match.\n", "Can you give a reproducible example?  Even better, if you have a link to the code you think is wrong, can you provide it?  It sounds like you may already know what the problem is.\n", "@mikowals @girving Actually, I read the [code](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/contrib/layers/python/layers/layers.py), from line 439 to 455, it describes how the conv2d layer uses the batch_norm and activation. What I understand from this is that the `reuse` parameter only influnence the `conv2d` layer but not the function used inside.\nTo make the two ways same, I also set `biases_initializer` None.\n\n@girving Sorry, I don't know what is the exact problem. But I guess it ways to use `batch_norm`.  I just try one [tutorial](https://github.com/awjuliani/TF-Tutorials/blob/master/DCGAN.ipynb) written by @awjuliani. Here are the two ways I used in the code which I use Ipython notebook to test. What I just change is the `dis3` of the `discriminator`. The first way ipython notebook is shown [here.](https://github.com/ColaWithIce/TF-test/blob/master/DCGAN-Copy1.ipynb) The second is [here.](https://github.com/ColaWithIce/TF-test/blob/master/DCGAN.ipynb) The output of the training message is very different. The two way's result should be similar but now as you can see the Disc Loss are not in same scale. I just wonder why this will happen. Is there something wrong with the variable updating or the way I use the code is wrong. \n", "I made a simpler reproduction and it shows both examples are equivalent.  There could still be a problem with variable reuse if you are not placing example 1 above into a variable_scope with reuse set to True in validation.  \n\n```\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\ndata = tf.random_normal(shape=(64, 16, 16, 3), seed=10)\n\nconv_bn = slim.conv2d(data, 16, (3,3), activation_fn=None,\n                        normalizer_fn=slim.batch_norm, normalizer_params={'is_training': True},\n                        scope='conv')\n\nconv_no_bn = slim.conv2d(data, 16, (3,3), activation_fn=None, biases_initializer=None,\n                        reuse=True, scope='conv')\nalt_conv_bn = slim.batch_norm(conv_no_bn, is_training=True)\n\nis_equal = tf.reduce_all(tf.equal(conv_bn, alt_conv_bn))\n\nmoving_variances = tf.contrib.framework.get_variables_by_suffix('moving_variance')\nvariance_is_equal = tf.reduce_all(tf.equal(*moving_variances))\n\nmoving_means = tf.contrib.framework.get_variables_by_suffix('moving_mean')\nmean_is_equal = tf.reduce_all(tf.equal(*moving_means))\n\nwith tf.Session() as sess:\n  sess.run(tf.initialize_all_variables())\n  print(sess.run(is_equal)) # True\n  print(sess.run(is_equal)) # True\n  print([v.name for v in moving_variances]) # ['conv/BatchNorm/moving_variance:0', 'BatchNorm/moving_variance:0']\n  print([v.name for v in moving_means]) # ['conv/BatchNorm/moving_mean:0', 'BatchNorm/moving_mean:0']\n  print(sess.run([variance_is_equal, mean_is_equal])) # [True, True]\n```\n", "@mikowals  Thank you for your comment. I think I find the problem. The trainable variable numbers are different between different ways. That means if I use `reuse` in `conv2d` layer, all the variable created in this layer INCLUDE the variables created in `batch_norm` layer will be reused when `reuse` is `True`\nWhat I test is shown below.\nFor example 1:\n\n```\nfor v in tf.trainable_variables():\n    print 'name = {}'.format(v.value())\n    print len(tf.trainable_variables())\n```\n\nOUTPUT:\n\n```\nname = Tensor(\"g_project/weights/read:0\", shape=(100, 4096), dtype=float32)\nname = Tensor(\"g_project/BatchNorm/beta/read:0\", shape=(4096,), dtype=float32)\nname = Tensor(\"g_conv1/weights/read:0\", shape=(5, 5, 64, 256), dtype=float32)\nname = Tensor(\"g_conv1/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"g_conv2/weights/read:0\", shape=(5, 5, 32, 64), dtype=float32)\nname = Tensor(\"g_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"g_conv3/weights/read:0\", shape=(5, 5, 16, 32), dtype=float32)\nname = Tensor(\"g_conv3/BatchNorm/beta/read:0\", shape=(16,), dtype=float32)\nname = Tensor(\"g_out/weights/read:0\", shape=(32, 32, 1, 16), dtype=float32)\nname = Tensor(\"d_conv1/weights/read:0\", shape=(4, 4, 1, 16), dtype=float32)\nname = Tensor(\"d_conv2/weights/read:0\", shape=(4, 4, 16, 32), dtype=float32)\nname = Tensor(\"d_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"d_conv3/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\nname = Tensor(\"BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"d_out/weights/read:0\", shape=(1024, 1), dtype=float32)\nname = Tensor(\"d_out/biases/read:0\", shape=(1,), dtype=float32)\nname = Tensor(\"BatchNorm_1/beta/read:0\", shape=(64,), dtype=float32)\n17\n```\n\nFor example 2:\n\n```\nfor v in tf.all_variables():\n    print 'name = {}'.format(v.value())\n    print len(tf.all_variables())\n\n```\n\nOUTPUT:\n\n```\nname = Tensor(\"g_project/weights/read:0\", shape=(100, 4096), dtype=float32)\nname = Tensor(\"g_project/BatchNorm/beta/read:0\", shape=(4096,), dtype=float32)\nname = Tensor(\"g_conv1/weights/read:0\", shape=(5, 5, 64, 256), dtype=float32)\nname = Tensor(\"g_conv1/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"g_conv2/weights/read:0\", shape=(5, 5, 32, 64), dtype=float32)\nname = Tensor(\"g_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"g_conv3/weights/read:0\", shape=(5, 5, 16, 32), dtype=float32)\nname = Tensor(\"g_conv3/BatchNorm/beta/read:0\", shape=(16,), dtype=float32)\nname = Tensor(\"g_out/weights/read:0\", shape=(32, 32, 1, 16), dtype=float32)\nname = Tensor(\"d_conv1/weights/read:0\", shape=(4, 4, 1, 16), dtype=float32)\nname = Tensor(\"d_conv2/weights/read:0\", shape=(4, 4, 16, 32), dtype=float32)\nname = Tensor(\"d_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"d_conv3/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\nname = Tensor(\"d_conv3/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"d_out/weights/read:0\", shape=(1024, 1), dtype=float32)\nname = Tensor(\"d_out/biases/read:0\", shape=(1,), dtype=float32)\n16\n```\n\n@mikowals By the way, when I use `slim.batch_norm` alone. Is it the right way to set parameters of normalization {is_training=True} in Training phase and {is_training=False, reuse=True} in Testing and validation phase? My point is do I need to set `reuse` to True in the Testing phase. Or just set the `reuse` as default `None.` Thank you in advance.\n", "Yes, setting `reuse=True` in `slim.conv2d` will also insure the batch norm variables are reused.   So in general your example 2 is the simple way.  \n\nYou are also correct with how `{'is_training': True}` is used with batch norm.  Setting `reuse=True` for testing is also probably correct though often it is best just to save the model and have the evaluation code create its own model and load the variables from a checkpoint.   See examples in [slim.evaluation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/evaluation.py).\n\nIf your issue is resolved you should close it.\n"]}, {"number": 5662, "title": "Windows support for tf.contrib libraries in the PIP package.", "body": "Is there any documentation or tutorial on how to include tf.contrib libraries in the PIP package built with cmake on Windows?\r\n\r\nOr to install tf.contrib libraries as a separate step?", "comments": ["Pull request #5634 re-enables tf.contrib in the Windows/CMake PIP package (it's currently broken because a new package was added and the CMake rules don't reflect it yet). If that doesn't work, please let us know on this issue!\n", "I can confirm the PR successfully includes tf.contrib in the PIP package on Windows. @mrry Thank you, sir!\n"]}, {"number": 5661, "title": "Why does encoder not have mask option in Seq2Seq model ?", "body": "Hi, all,\r\n\r\nThere is a confusion that tf's seq2seq models don't have mask option for sequence padding.\r\n\r\nAs I know, mask and padding always are together. If without mask, the final result of padding is imprecise. And tf's rnns have implemented mask.\r\n\r\nSo why does seq2seq not have mask option?\r\n\r\n```python\r\n#rnn\r\ndef rnn(cell, inputs, initial_state=None, dtype=None,\r\n        sequence_length=None, scope=None):\r\n\r\n# seq2seq\r\ndef embedding_rnn_seq2seq(encoder_inputs,\r\n                          decoder_inputs,\r\n                          cell,\r\n                          num_encoder_symbols,\r\n                          num_decoder_symbols,\r\n                          embedding_size,\r\n                          output_projection=None,\r\n                          feed_previous=False,\r\n                          dtype=None,\r\n                          scope=None):\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["Questions like this are better asked on StackOverflow.  Github issues are for bug reports and feature requests.\n"]}, {"number": 5660, "title": "How to applicate tensorflow on KNL? any suggestions?thanks very much.", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["I don't think this is enough detail for us to help you.\n"]}, {"number": 5659, "title": "How to trought cuda/lib path", "body": "Hi \r\nI am very Struggling without going through the path of cuda/lib.\r\n\r\n```\r\nbash-3.2$ python\r\nPython 3.5.2 |Anaconda custom (x86_64)| (default, Jul  2 2016, 17:52:12) \r\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nSegmentation fault: 11\r\n```\r\n\r\n~/.bash_profile\r\n```\r\nexport CUDA_HOME=/usr/local/cuda\r\nexport DYLD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/lib\r\nexport PATH=\"$CUDA_HOME/bin:$PATH\"\r\n```\r\n\r\n```\r\nbash-3.2$ ls -la /usr/local/cuda/lib/libcud*\r\nlrwxr-xr-x  1 root  admin     33 11 17 11:39 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib\r\n-rwxr-xr-x  1 root  wheel  13504  9 27 06:59 /usr/local/cuda/lib/libcuda.dylib\r\nlrwxr-xr-x  1 root  wheel     45  9 27 07:00 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\nlrwxr-xr-x  1 root  wheel     50  9 27 07:00 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x  1 root  wheel     46  9 27 07:00 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\nlrwxr-xr-x  1 root  wheel     49  9 27 07:00 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\nlrwxr-xr-x  1 root  admin     47 11 15 17:38 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib\r\nlrwxr-xr-x  1 root  admin     45 11 15 17:38 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib\r\nlrwxr-xr-x  1 root  admin     48 11 15 17:38 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a\r\n```\r\n\r\nbut, In the following case I Successful \"import tensorflow\" \r\n```\r\nbash-3.2$ cd /usr/local/cuda/lib\r\nbash-3.2$ python\r\nPython 3.5.2 |Anaconda custom (x86_64)| (default, Jul  2 2016, 17:52:12) \r\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.8.0.dylib locally\r\n>>> \r\n```\r\n\r\n", "comments": ["It sounds like you have two different versions of cuda installed, and it segfaults because you are building against one and linking against another at runtime.\n", "Hi\none version of cuda installed my pc.\nI tried uninstall/install cuda. and rebuild tensorflow.\n\n```\nPlease specify the location of python. [Default is /XXXX/.pyenv/shims/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\nNo Hadoop File System support will be enabled for TensorFlow\nFound possible Python library paths:\n  /XXXX/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages\nPlease input the desired Python library path to use.  Default is [/XXXX/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages]\n\nUsing python library path: /XXXX/.pyenv/versions/anaconda3-4.0.0/lib/python3.5/site-packages\nDo you wish to build TensorFlow with OpenCL support? [y/N] N\nNo OpenCL support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 3.0,3.5,5.2                                          \n................\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n........\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n```\n\nbut, same error.\n", "@TatsujiNakayama Unfortunately I don't think this is enough information for us to debug.  Can you run `otool -L` on the TensorFlow library to see what it's linking against?\r\n\r\n@zhanghan328 That isn't the same problem, so please don't post it in this thread.", "@girving \r\nfollowing otool command is right?\r\n`otool -L  ./lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so`\r\n\r\nis Result:\r\n```\r\n./lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so:`\r\n\tbazel-out/local_darwin-py3-opt/bin/tensorflow/python/_pywrap_tensorflow.so (compatibility version 0.0.0, current version 0.0.0)\r\n\t@rpath/libcudart.8.0.dylib (compatibility version 0.0.0, current version 8.0.47)\r\n\t/System/Library/Frameworks/IOKit.framework/Versions/A/IOKit (compatibility version 1.0.0, current version 275.0.0)\r\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1226.10.1)\r\n\t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 120.1.0)\r\n```\r\nThanks", "@TatsujiNakayama Yeah, that's what I meant, but I do not remember how to interpret the `@rpath` result.\r\n\r\n@jart Any ideas?", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 5658, "title": "Produce binary release tarballs for the TensorFlow C API", "body": "These scripts are intended to be run with every release to\r\nproduce libtensorflow.tar.gz for CPU and GPU on Linux and OS X\r\nfor x86_64 architecture machines.\r\n(Eventually there will be other operating systems and architectures).\r\n\r\nThese binary releases are then intended to make use of other language\r\nbindings (such as [Rust](https://github.com/tensorflow/rust), [Haskell](https://github.com/tensorflow/haskell), [Go](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go)) easier\r\nas the common case would be to download the binary C-library release and\r\navoid the need to build TensorFlow from source (and all the time and\r\nexternal dependencies doing so entails).\r\n\r\nFiles:\r\n- tensorflow/tools/ci_build/builds/libtensorflow.sh - Baseline common script to build a tarball\r\n- tensorflow/osx/libtensorflow_{cpu,gpu}.sh - Build tarballs for OS X with and without GPU support\r\n- tensorflow/linux: Has similar top level scripts, but the build happens in a docker container so it contains 4 files - the two top level builds, one shared libtensorflow_docker.sh that is used to build and execute the docker container and libtensorflow.sh which is the script run inside the container.", "comments": ["Jenkins, test this please\n\nThe test failures seemed unrelated, retrying.\n", "Jenkins, test this please\n", "both failures are known issues. Merging.\n"]}, {"number": 5657, "title": "0.11.0rc2 -> 0.11.0, fix broken windows cmake ci.", "body": "", "comments": ["@Oneplus, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan to be a potential reviewer.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Thanks for the change, however I just prepared #5656 which should be a more generic update.\nIf the tests pass, I will merge that and abandon this one.\n"]}, {"number": 5656, "title": "Make windows test script version independent.", "body": "This should fix the windows cmake build issues.", "comments": ["I like this! Is it responsible for the flakes?\n", "I think it is possible because of this we had a bunch of breakages today with the version update.\nI will wait for cmake python tests and then merge.\n", "OK now it is working with /p and no space between = and <\n"]}, {"number": 5655, "title": "[Windows/CMake] Add more error checking to the CI script.", "body": "", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan to be a potential reviewer.\n", "Not sure if this works just yet, but I wanted to run it through the CI....\n", "Hmm, this didn't seem to help... I got a passing build by rerunning manually (http://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/139/) but it looks like it didn't run `ctest` :(.\n", "Maybe it will work after #5656\n", "Derek, could you rebase and merge the latest changes for version independence?\n", "@gunan Looks like it's still not working the way I expected... I'll table this one for now (because hopefully your other fix will make the tests green).\n", "@gunan This seems to fix the Windows CMake tests, by applying one of the known workarounds for the `easy-install.pth` bug. All the other failures seem to be caused by some unrelated bug in `tf.learn`.\n", "thank you!!!!!\n", "only dnn_test failure on the way.\nmerging right away!\n"]}, {"number": 5654, "title": "[Windows] Fix GetMatchingPaths for use on Windows.", "body": "The existing implementation is not compatible with paths containing\r\nbackslashes. Use a workaround on Windows only (converting backslashes\r\nto forward slashes).\r\n\r\nAlso enable several tests in python/training, and fix io_ops_test.py\r\nand saver_test.py to work on Windows.\r\n\r\n(saver_test.py and some other tests are disabled until #5634 is merged.)", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @jhseu and @malzantot to be potential reviewers.\n", "/cc @guschmue \n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "Test failures look like flakes, but for good measure:\nJenkins, test this please.\n", "Not sure its the same cause but after running tests well all of last week, my jenkins is acting up today on the tests as well. Not clean failures.\nSomething that came in yesterday I think (I pull once an hour so I can't pinpoint the exact commit). \nFor some reason gpu builds seem to be fine. \n", "@tensorflow-jenkins test this please.\n\n(Hoping it's a flake, because the tests pass locally for me.)\n", "yeah, my bad: the 0.11.0 was my problem I think. \nbut I found @gunan change to your ci script - hope Jenkins will be all happy again.\n", "@gunan All checks have passed! Looks like your fix did it :).\n", "wohooo!\nIt took a few tries, but it finally passed :)\n"]}, {"number": 5653, "title": "Fixed several bugs in the OpenCL code", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @lukeiwanski, @keveman and @tensorflower-gardener to be potential reviewers.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n"]}, {"number": 5652, "title": "cifar10_multi_gpu_train.py breaks with more than 1 GPU", "body": "### Environment info\r\nOperating System: Ubuntu \r\n\r\nInstalled version of CUDA and cuDNN: 8.0 and 5\r\n\r\n1. The commit hash (`git rev-parse HEAD`): 3d41cf77d624aeee0482f92121a9300b29db2809\r\n2. The output of `bazel version`: \r\nBuild label: 0.3.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\r\nBuild timestamp: 1475861110\r\nBuild timestamp as int: 1475861110\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\npython cifar10_multi_gpu_train.py --num_gpus=2\r\n\r\nBoth cifar10_train.py and cifar10_multi_gpu_train.py (without specifying num_gpus, so running on a single GPU) work.\r\n\r\n### Logs or other output that would be helpful\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\nTraceback (most recent call last):\r\n  File \"cifar10_multi_gpu_train.py\", line 280, in <module>\r\n    tf.app.run()\r\n  File \"/data/github/tensorflow/_python_build/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"cifar10_multi_gpu_train.py\", line 276, in main\r\n    train()\r\n  File \"cifar10_multi_gpu_train.py\", line 180, in train\r\n    loss = tower_loss(scope)\r\n  File \"cifar10_multi_gpu_train.py\", line 92, in tower_loss\r\n    loss_averages_op = loss_averages.apply(losses + [total_loss])\r\n  File \"/data/github/tensorflow/_python_build/tensorflow/python/training/moving_averages.py\", line 391, in apply\r\n    self._averages[var], var, decay, zero_debias=zero_debias))\r\n  File \"/data/github/tensorflow/_python_build/tensorflow/python/training/moving_averages.py\", line 70, in assign_moving_average\r\n    update_delta = _zero_debias(variable, value, decay)\r\n  File \"/data/github/tensorflow/_python_build/tensorflow/python/training/moving_averages.py\", line 177, in _zero_debias\r\n    trainable=False)\r\n  File \"/data/github/tensorflow/_python_build/tensorflow/python/ops/variable_scope.py\", line 1024, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/data/github/tensorflow/_python_build/tensorflow/python/ops/variable_scope.py\", line 850, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/data/github/tensorflow/_python_build/tensorflow/python/ops/variable_scope.py\", line 346, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/data/github/tensorflow/_python_build/tensorflow/python/ops/variable_scope.py\", line 331, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/data/github/tensorflow/_python_build/tensorflow/python/ops/variable_scope.py\", line 650, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable tower_1/tower_1/conv1/weight_loss/avg/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?", "comments": ["@shlens Could you take a look?  Scopes have been fiddled with recently, and maybe this file didn't get updated?\n", "This was fixed in November."]}, {"number": 5651, "title": "Disable scatter_nd_ops_test in OSS python tests until the bug is fixed", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @drpngx, @tensorflower-gardener and @ebrevdo to be potential reviewers.\n"]}, {"number": 5650, "title": "Problem running word2vec_optimized on Amazon GPU machine", "body": "I've not found anyone reporting this specific issue. I'm trying to run tensorflow word2vec script on a aws\r\ngpu machine through docker-nvidia container and getting this error:\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/595666/log.txt)\r\n\r\n\r\n\r\n### Environment info\r\nOperating System: ubuntu 16.10 on a g2.2xlarge aws machine. Nvidia driver version 367.57\r\n\r\nInstalled version of CUDA and cuDNN:  \r\nrunning on docker-nvidia tensorflow-0.11.0-gpu machine\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n./usr/local/nvidia/lib64/libcuda.so\r\n./usr/local/nvidia/lib64/libcuda.so.1\r\n./usr/local/nvidia/lib64/libcuda.so.367.57\r\n./usr/local/cuda-7.5/targets/x86_64-linux/lib/stubs/libcuda.so\r\n./usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudadevrt.a\r\n./usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart_static.a\r\n./usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so.7.5\r\n./usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so.7.5.18\r\n./usr/local/cuda-7.5/targets/x86_64-linux/lib/libcudart.so\r\n./usr/local/cuda-7.5/extras/Debugger/lib64/libcudacore.a\r\n./usr/local/cuda-7.5/extras/Debugger/include/libcudacore.h\r\n./usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\r\n./usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.3\r\n./usr/lib/x86_64-linux-gnu/libcudnn_static.a\r\n./usr/lib/x86_64-linux-gnu/libcudnn.so.5\r\n./usr/lib/x86_64-linux-gnu/libcudnn.so\r\n\r\n\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nthis is the command I executed and first lines of output:\r\n\r\n\u00b4python word2vec_optimized.py --train_data text8 --eval_data questions-words.txt --save_path train\u00b4\r\n\r\n### What other attempted solutions have you tried?\r\nTryed to run the same code on a docker tensorflow-0.10.0 machine (without gpu libraries) and obtained\r\nsimilar error.\r\n\r\n### Logs or other output that would be helpful\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \r\nname: GRID K520\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\r\npciBusID 0000:00:03.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.91GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\r\nI tensorflow/models/embedding/word2vec_kernels.cc:200] Data file: text8 contains 100000000 bytes, 17005207 words, 253854 unique words, 71290 unique frequent words.\r\nData file:  text8\r\nVocab size:  71290  + UNK\r\nWords per epoch:  17005207\r\nEval analogy file:  questions-words.txt\r\nQuestions:  17827\r\nSkipped:  1717\r\nW tensorflow/core/framework/op_kernel.cc:940] Failed precondition: Attempting to use uninitialized value global_step\r\n\t [[Node: AssignAdd = AssignAdd[T=DT_INT32, _class=[\"loc:@global_step\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](global_step, AssignAdd/value)]]\r\n\r\n", "comments": ["What version of tensorflow is this?  That error typically means variables haven't been initialized, but that's surprising if you haven't modified the script.\n", "Hello Geoffrey,\nI've just realized that I do changed a line of the code. The original error was that the object doesn't have a method global_variables_initializer. I changed it for initialize_all_variables and got the error I reported. That was in version 0.11.0. Now I just checked between versions and with 0.10.0 code runs ok.I'm sorry about the confusion. You can delete the issue if you want or do I need to do so?\nGreetings\u00a0 Jos\u00e9 Antonio Ram\u00edrez\n\n```\n  De: Geoffrey Irving <notifications@github.com>\n```\n\n Para: tensorflow/tensorflow tensorflow@noreply.github.com \nCC: Jos\u00e9 Antonio Ram\u00edrez Moguel rmja_99@yahoo.com; Author author@noreply.github.com\n Enviado: Jueves, 17 de noviembre, 2016 11:21:38\n Asunto: Re: [tensorflow/tensorflow] Problem running word2vec_optimized on Amazon GPU machine (#5650)\n\nWhat version of tensorflow is this? That error typically means variables haven't been initialized, but that's surprising if you haven't modified the script.\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.  \n", "@ramja Glad it's resolved!\n"]}, {"number": 5649, "title": "ContextManager Error on tf.name_scope: RuntimeError(\"generator didn't yield\")", "body": "I have this minimal code that shows the error:\r\n\r\n```\r\nscope = tf.name_scope('loss')\r\nwith scope as s:\r\n    print s\r\n    \r\nwith scope as s:\r\n    print s\r\n```\r\n\r\nIs this a bug or done by design?", "comments": ["@lukaszkaiser Any reason our scopes shouldn't be re-enterable?\n", "I think name scope is not meant to be used sa object this way, and I don't see why it should. Use variable_scope if you want to capture scope as object and yield.\n", "@lukaszkaiser The issue is not about if I should use this or that, its if this should be considered a bug or not because solving this is easy for users, but a behaviour like this should be by design given certain criteria or fixed because it can cause un expected errors.\n"]}, {"number": 5648, "title": "Branch 139339828", "body": "Internal push.", "comments": ["@rmlarsen, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ispirmustafa and @keveman to be potential reviewers.\n"]}, {"number": 5647, "title": "Improved support for OpenCL devices", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @lukeiwanski, @keveman and @ebrevdo to be potential reviewers.\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "I signed it !\n", "Will have to override CLA check.\nCLA check ok, but it wont accept as PR creator does not own all commits, or something in those lines.\n"]}, {"number": 5646, "title": "Maunally setting TF_NEED_CUDA=0 in build_tf_windows.sh", "body": "Soon TF_NEED_CUDA won't be False by default on Windows, we need this to make sure [tf-master-win-bzl ](http://ci.tensorflow.org/job/tf-master-win-bzl/) won't fail. @gunan ", "comments": ["@meteorcloudy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @benoitsteiner to be a potential reviewer.\n", "I would like to create new scripts for those under windows/**gpu**/\nI would like to make what we execute in CI visible to developers.\n", "And we will have a separate GPU build for windows, as in linux.\n", "Oh, I didn't mean to use this script for GPU build, I just want keep the CPU build work after #5644.\nBecause after that, if we don't set `TF_NEED_CUDA=0`, `echo \"\" | ./configure` will just fail.\nSorry I didn't make that clear.\n", "OK, now I understand :)\nJenkins, test this please.\n", "Jenkins, test this please.\n"]}, {"number": 5645, "title": "contrib/layers: Make integer check Python 3 compatible", "body": "Fixes `NameError: name 'long' is not defined` error thrown in Python 3.", "comments": ["@gokceneraslan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @lukaszkaiser, @tensorflower-gardener and @zhangyaobit to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Fixes the Python 3 compatibility bug introduced by Cassandra Xia in https://github.com/tensorflow/tensorflow/commit/b294ca32134e99f3b038577851ecc72e52bbf23a.\n", "@tensorflow-jenkins test this please\n", "The failures in MacOS test and Window cmake test are unrelated. Merging the PR now.\n", "PR merged. Thanks, @gokceneraslan \n"]}, {"number": 5644, "title": "Add Windows GPU support with Bazel", "body": "@mrry @dslomov @damienmg \r\n\r\nThe change needed for Bazel is also ready to review:\r\nhttps://bazel-review.googlesource.com/?polygerrit=0#/c/7351/\r\nOnce these changes are checked in, we should be able to build TensorFlow with GPU support on Windows by\r\n`bazel build --confg=win-cuda -c opt --cpu=x64_windows_msvc --cpu=x64_windows tensorflow/tools/pip_package:build_pip_package`\r\n\r\nI excluded `depthwise_conv_grad_op` which doesn't seem to be excluded in CMake build.\r\nIt caused a linking error like this, and only happens in GPU build:\r\n```\r\ndepthwise_conv_grad_op.o : error LNK2019: unresolved external symbol \"public: static void __cdecl tensorflow::DepthwiseConv2dBackpropInputGPULaunch<float>::Run(struct Eigen::GpuDevice const &,struct tensorflow::DepthwiseArgs,float const *,float const *,float *)\" (?Run@?$DepthwiseConv2dBackpropInputGPULaunch@M@tensorflow@@SAXAEBUGpuDevice@Eigen@@UDepthwiseArgs@2@PEBM2PEAM@Z) referenced in function \"public: static void __cdecl tensorflow::LaunchDepthwiseConvBackpropInputOp<struct Eigen::GpuDevice,float>::launch(class tensorflow::OpKernelContext *,struct tensorflow::DepthwiseArgs,float const *,float const *,float *)\" (?launch@?$LaunchDepthwiseConvBackpropInputOp@UGpuDevice@Eigen@@M@tensorflow@@SAXPEAVOpKernelContext@2@UDepthwiseArgs@2@PEBM2PEAM@Z)\r\ndepthwise_conv_grad_op.o : error LNK2019: unresolved external symbol \"public: static void __cdecl tensorflow::DepthwiseConv2dBackpropInputGPULaunch<double>::Run(struct Eigen::GpuDevice const &,struct tensorflow::DepthwiseArgs,double const *,double const *,double *)\" (?Run@?$DepthwiseConv2dBackpropInputGPULaunch@N@tensorflow@@SAXAEBUGpuDevice@Eigen@@UDepthwiseArgs@2@PEBN2PEAN@Z) referenced in function \"public: static void __cdecl tensorflow::LaunchDepthwiseConvBackpropInputOp<struct Eigen::GpuDevice,double>::launch(class tensorflow::OpKernelContext *,struct tensorflow::DepthwiseArgs,double const *,double const *,double *)\" (?launch@?$LaunchDepthwiseConvBackpropInputOp@UGpuDevice@Eigen@@N@tensorflow@@SAXPEAVOpKernelContext@2@UDepthwiseArgs@2@PEBN2PEAN@Z)\r\ndepthwise_conv_grad_op.o : error LNK2019: unresolved external symbol \"public: static void __cdecl tensorflow::DepthwiseConv2dBackpropFilterGPULaunch<float>::Run(struct Eigen::GpuDevice const &,struct tensorflow::DepthwiseArgs,float const *,float const *,float *)\" (?Run@?$DepthwiseConv2dBackpropFilterGPULaunch@M@tensorflow@@SAXAEBUGpuDevice@Eigen@@UDepthwiseArgs@2@PEBM2PEAM@Z) referenced in function \"public: static void __cdecl tensorflow::LaunchDepthwiseConvBackpropFilterOp<struct Eigen::GpuDevice,float>::launch(class tensorflow::OpKernelContext *,struct tensorflow::DepthwiseArgs,float const *,float const *,float *)\" (?launch@?$LaunchDepthwiseConvBackpropFilterOp@UGpuDevice@Eigen@@M@tensorflow@@SAXPEAVOpKernelContext@2@UDepthwiseArgs@2@PEBM2PEAM@Z)\r\ndepthwise_conv_grad_op.o : error LNK2019: unresolved external symbol \"public: static void __cdecl tensorflow::DepthwiseConv2dBackpropFilterGPULaunch<double>::Run(struct Eigen::GpuDevice const &,struct tensorflow::DepthwiseArgs,double const *,double const *,double *)\" (?Run@?$DepthwiseConv2dBackpropFilterGPULaunch@N@tensorflow@@SAXAEBUGpuDevice@Eigen@@UDepthwiseArgs@2@PEBN2PEAN@Z) referenced in function \"public: static void __cdecl tensorflow::LaunchDepthwiseConvBackpropFilterOp<struct Eigen::GpuDevice,double>::launch(class tensorflow::OpKernelContext *,struct tensorflow::DepthwiseArgs,double const *,double const *,double *)\" (?launch@?$LaunchDepthwiseConvBackpropFilterOp@UGpuDevice@Eigen@@N@tensorflow@@SAXPEAVOpKernelContext@2@UDepthwiseArgs@2@PEBN2PEAN@Z)\r\nbazel-out/vc_14_0_x64-py3-opt/bin/tensorflow/cc/tutorials_example_trainer.exe : fatal error LNK1120: 4 unresolved externals\r\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\r\nINFO: Elapsed time: 2180.664s, Critical Path: 1792.27s\r\n```\r\n\r\nThe source file [depthwise_conv_grad_op.cc](https://github.com/tensorflow/tensorflow/blob/beb10ceb086fe94a6b1247b45397aafddd47e05d/tensorflow/core/kernels/depthwise_conv_grad_op.cc#L113) looks weird to me, because the syntax highlight starts go wrong from line 113. Might need some help to resolve this.\r\n\r\n\r\n", "comments": ["@meteorcloudy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @davidzchen to be potential reviewers.\n", "There is still an issue that the pip package is trying to load dll libraries with the wrong name on Windows.\nMentioned here: https://github.com/bazelbuild/bazel/issues/2075\nI'll fix it after PR #5494 is merged, need to resolve some conflict anyway.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n"]}, {"number": 5643, "title": "TypeError: run() got an unexpected keyword argument 'argv'", "body": "When I followed the TensorFlow Mechanics 101 tutorial  `python fully_connected_feed.py` in `examples/tutorials/mnist` directory, I came across this problem: \r\n```\r\nTraceback (most recent call last):\r\n  File \"fully_connected_feed.py\", line 277, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\nTypeError: run() got an unexpected keyword argument 'argv'\r\n```\r\nI am using tensorflow 0.11 version in Anaconda environment.", "comments": ["The `argv` argument was added recently: https://github.com/tensorflow/tensorflow/commit/6812d46b957e32eba37c67384cc2136908d7a1ff.  You are comparing a new version of a tutorial with an old version of tensorflow.\n", "I'm having the same problem. And i'm really using new example with old TF because the pip installation is not placing the examples files, so i've manually copied from sources and placed inside the pip installation. \n\nI'm using the fallowing:\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\n\nwhat should i use to get TF with examples?\n\nthanks in advance.\n", "I've meet the same problem with lastest TF, does sb know how to fix this problem\uff1fIs there a new tutorial docuement that i can follow, thanks a lot !\n\nTraceback (most recent call last):\n  File \"convolutional.py\", line 339, in <module>\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\nTypeError: run() got an unexpected keyword argument 'argv'\n", "Checkout the r0.11 branch instead of the master branch.\nUse the tutorial code in that branch.\n", "Thanks a lot, i git the r0.11 banch eventually, and it works ! \n", "Thanks, it worked.  =D\n", "How to checkout the r0.11branch? my system is Ubuntu 16.04 LTS. Thanks.", "Use \"-b r0.11\" with Git clone!", "@marteiro Thanks,it worked.\r\nCan solve this problem by Update Tensorflow to new version? How to update Tensorflow to new version?", "@girving \r\nCan solve this problem by Update Tensorflow to new version? How to update Tensorflow to new version?", "@ttytt1978\r\nIf your are not contributing to the master branch, just use the r0.11. Not much else to hope for.", "I have the same error running the example in the `learn` folder:\r\n\r\non latest version 0.12:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/examples/learn/text_classification.py\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"text_classification.py\", line 136, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\nTypeError: run() got an unexpected keyword argument 'argv'\r\n```", "@loretoparisi You are not running 0.12 tensorflow: https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/python/platform/app.py", "@girving sorry, didn't get. I'm using the `text_classification.py` example from the `r0.12` blob, where I'm wrong?", "@loretoparisi You are running `text_classification.py` from `0.12` against an older version of TensorFlow, which includes an older version of `app.py` which does not have the flag.", "@girving ok this makes sense, in fact my `requirements.txt` looks like\r\n\r\n```\r\n$ cat requirements.txt \r\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0-py2-none-any.whl\r\nmatplotlib\r\nnltk\r\nsklearn\r\nscipy\r\npandas\r\n```\r\n\r\nThanks going to move to 0.12 nightly, if there is one?", "I am also getting the same error, how can I upgrade my tensorflow?", "@jubins for my understanding, the fix is in `0.12` but there is not nightly build in Jenkins already, the latest nightly build is [here](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=mac-slave/), but it is still on `0.11`, so the only option I see so far is to move to branch [r0.12](https://github.com/tensorflow/tensorflow/tree/r0.12) and compile from the scratch.\r\n\r\n", "@loretoparisi I recompiled my code using `r0.12` but it now gives another error 'run() got an unexpected keyword argument argv'\r\n\r\n![screenshot from 2016-11-23 19-45-47](https://cloud.githubusercontent.com/assets/18559677/20583320/e4fc1b26-b1b6-11e6-89c6-2b480a29509f.png)\r\n", "Prior to 1.0, always make sure the tensorflow example code you run is from the same branch as the binary version you have installed.\r\n\r\nSo your options are:\r\n1) Upgrade your tensorflow installation (via nightly or source built from the same master branch) to use files in the master branch\r\n2) Check out the copy of the example code from the branch you have installed (For example: r0.11 if you have 0.11 installed, r0.10 if you have 0.10 installed).\r\n\r\nMixing and matching source code and binary install will likely cause you pain.\r\n", "I got it and it works,thanks!", "@ttytt1978 how you achieved that?\r\n@vrv it could be useful a step by step guide, thanks.", "@loretoparisi \r\n![branche](https://cloud.githubusercontent.com/assets/8791765/20658433/c00eaa8a-b577-11e6-963c-9326a86b8666.png)\r\nSelect the branch match you tensorflow version and download it, run the file in the directory you download.", "Please Dude, read the topic:\r\n\r\n**git clone https://github.com/tensorflow/tensorflow -b r0.11**\r\n", "Thx a lot every one, it works!", "THX all! It helps a lot", "@girving Hi, im running tensorflow wheel version 0.10.0 with gpu support (cuda-7.5) and getting the same error. When I tried to branch to r11.0, it successfully installs wheel version 0.11.0 but prompts errors saying that it requires cuda-8.0 libraries. Whereas 0.10.0 only requires cuda-7.5. Do you know if there's another way to fix this error with cuda-7.5 libraries? Thanks!", "Thanks, i try \"-b r0.11\" with Git clone and it works", "where to clone it\r\n", "Is this problem relevant to version 1.7.0? I installed tensorflow through pycharm and according to print(tf.__version__) my version is 1.7.0. However I faced the same problem"]}, {"number": 5642, "title": "Tensorflow built from sources in Docker doesn't recognize GPUs", "body": "I need to build TensorFlow from sources in a Dockerfile (because of our architecture's constraints). Unfortunately, TensorFlow does not recognize the gpus.\r\nI use a Tesla K80, with Nvidia driver's version 367.55\r\n\r\nHere is the DockerFile related to Tensorflow:\r\n```\r\n#####       BAZEL        #####\r\nRUN apt-get update \\\r\n&& apt-get install -y software-properties-common curl\r\nRUN apt-get update\r\nRUN echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | debconf-set-selections\r\n\r\nRUN add-apt-repository ppa:webupd8team/java \r\nRUN apt-get update \r\nRUN apt-get install -y oracle-java8-installer\r\n\r\nRUN echo \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | tee /etc/apt/sources.list.d/bazel.list\r\nRUN curl https://storage.googleapis.com/bazel-apt/doc/apt-key.pub.gpg | apt-key add - \r\nRUN apt-get update\r\nRUN apt-get install --yes --force-yes bazel\r\nRUN apt-get upgrade -y --force-yes bazel\r\nRUN apt-get install -y swig\r\n\r\n#####       TENSORFLOW        #####\r\nWORKDIR /Programs\r\nRUN git clone https://github.com/tensorflow/tensorflow.git\r\nWORKDIR /Programs/tensorflow\r\n\r\nENV PYTHON_BIN_PATH /usr/bin/python3.5\r\nENV TF_NEED_GCP 0\r\nENV TF_NEED_HDFS 1\r\nENV TF_NEED_CUDA 1\r\nENV TF_NEED_OPENCL 0\r\nENV TF_CUDNN_VERSION 5\r\nENV TF_CUDA_VERSION 8.0\r\nENV TF_CUDA_COMPUTE_CAPABILITIES 3.7\r\nENV GCC_HOST_COMPILER_PATH /usr/bin/gcc\r\nENV CUDA_TOOLKIT_PATH /usr/local/cuda\r\nENV CUDNN_INSTALL_PATH /usr/local/cuda\r\n\r\nRUN echo /usr/local/lib/python3.5/dist-packages | ./configure && \\\r\n    bazel build -c opt --config=cuda tensorflow/tools/pip_package:build_pip_package\r\nRUN bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\nRUN pip3 install /tmp/tensorflow_pkg/*.whl --upgrade\r\n```\r\nI run the docker like this : \r\n```\r\ndocker run -it --device /dev/nvidiactl --device /dev/nvidia-uvm --device /dev/nvidia0 --device /dev/nvidia1 tensorflow bash\r\n```\r\nWhen I am in the Docker, I run this little script to see if it sees gpu's work:\r\n```\r\nimport tensorflow as tf\r\nif __name__ == \"__main__\":\r\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\r\n    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n    print(\"Tf version :\",tf.__version__)\r\n```\r\nWhat it prints : \r\n```\r\nroot@fae4ae4d9fee:/home# CUDA_VISIBLE_DEVICES=0 python3 gpu.py \r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.8.0 locally\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_UNKNOWN\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: fae4ae4d9fee\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: fae4ae4d9fee\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.55.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.55  Tue Sep 27 10:17:05 PDT 2016\r\nGCC version:  gcc version 4.9.2 (Debian 4.9.2-10) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.55.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.55.0\r\nTf version : 0.11.0rc2\r\n```\r\nNote that I obtain the same output when I run this script without the `CUDA_VISIBLE_DEVICES=0` flag\r\n\r\noutput of `ls -l /path/to/cuda/lib/libcud*`\r\n```\r\nroot@fae4ae4d9fee:/home# ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root    558720 Nov 15 11:29 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root        16 Nov 15 11:29 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root        19 Nov 15 11:29 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root    415432 Nov 15 11:29 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root    775162 Nov 15 11:29 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 1000 users       13 Jul 27 05:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 1000 users       17 Jul 27 05:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxrwxr-x 1 1000 users 79337624 Jul 27 05:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-rw-r-- 1 1000 users 69756172 Jul 27 05:53 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\nThe commit hash (`git rev-parse HEAD`):\r\n`de6bbda2353b50944bd06d5b04c86f8c0a62792a`\r\n\r\nThe output of `bazel version`:\r\n```\r\nroot@fae4ae4d9fee:/Programs/tensorflow# bazel version\r\n.\r\nBuild label: 0.4.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n```\r\nNote that the nvidia drivers in the Docker are very aware of the GPU's:\r\n```\r\nroot@fae4ae4d9fee:~# nvidia-smi\r\nWed Nov 16 13:33:16 2016       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.55                 Driver Version: 367.55                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:83:00.0     Off |                    0 |\r\n| N/A   36C    P0    60W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 0000:84:00.0     Off |                    0 |\r\n| N/A   27C    P0    74W / 149W |      0MiB / 11439MiB |     98%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nI am not very familiar with Docker, so maybe am I doing something wrong in the Dockerfile. Thank's to anyone who could help me with this !", "comments": ["@caisq Any ideas?\n", "Will take a look. I'm traveling today and tomorrow. Will try to find some time to debug this later this week.\n", "That's not how you're supposed to start the container, look at the [official instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker#running-the-container)\n", "You have to use `nvidia-docker`  instead of `docker` when starting the container.\r\nOtherwise the GPU will not be accessible.", "Closing the issue as there has not been an update for a long time, and using nvidia-docker should solve the problem."]}, {"number": 5641, "title": "Fix projector_api_test, ensuring it runs.", "body": "This test was missing a call to tf.test.main(), so the test wasn't being\r\nrun.\r\n\r\nThis commit also fixes the test with s/embedding/embeddings/.", "comments": ["@darrengarvey, thanks for your PR! By analyzing the history of the files in this pull request, we identified @dsmilkov to be a potential reviewer.\n", "Can one of the admins verify this patch?\n", "Thanks for the fix!\n", "Jenkins, test this please.\n", "@benoitsteiner Merge this please. Thank you!\n", "PR merged. Thanks, @darrengarvey and @dsmilkov !"]}, {"number": 5640, "title": "Simplify CMake thirdparty library target names", "body": "In order to build platform independent static libraries, we can use CMakes\r\nbuiltin `CMAKE_STATIC_LIBRARY_PREFIX` and `CMAKE_STATIC_LIBRARY_SUFFIX`\r\nvariables to separate the naming convention for Windows and Unix host\r\nsystems.", "comments": ["@NewProggie, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @guschmue and @lilac to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@NewProggie It looks like `libjsoncpp.a` is the wrong name for the Linux CPU CMake build. Can you fix that up and we'll run the tests again? Thanks!\n", "@tensorflow-jenkins test this please.", "It looks like there's still a problem with the gRPC library name on Linux. My guess is that it only contains `Release/` on Windows.", "Ah, shoot. I'll return to this one time in the future and fix all the cross references, as there's no technical reason for all the thirdparty libs to build in different subfolders (as far as I can see).", "OK - thanks for the effort here though. I think the inconsistency stems from the fact that some of the modules are built in tree (then we link the libraries directly from wherever the build output put them), whereas others are also installed (which puts them in a more predictable location).\r\n\r\nIt'd be great to sort this out at some point!", "Looks like this PR is dead at the moment, so I'm going to close it. Feel free to re-open or create a new one when its time to move forward. "]}, {"number": 5639, "title": "KeyError: u'SaveV2' when loading exported model", "body": "I have a TensorFLow model trained on a GPU machine. Next, I need to export it and deploy on CPU only production machine.\r\n\r\nI have trained and exported a model from a GPU machine as described in [MNIST export example](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_export.py). Saver object was initialized above.\r\n\r\n```\r\nwith graph.as_default():\r\n    saver = tf.train.Saver(tf.all_variables(), sharded=True)\r\n...\r\n\r\nexport_path =  'resnet34_rmsprop_wd1e-1/saves/'\r\nprint('Exporting trained model to %s' % export_path)\r\ninit_op = tf.group(tf.initialize_all_tables(), name='init_op')\r\nmodel_exporter = exporter.Exporter(saver)\r\nmodel_exporter.init(sess.graph.as_graph_def(),\r\n                            init_op=init_op,\r\n                            default_graph_signature=exporter.classification_signature(input_tensor=inference_images,\r\n                                                                                      classes_tensor=inference_class,\r\n                                                                                      scores_tensor=inference_predictions),\r\n                            named_graph_signatures={'inputs': exporter.generic_signature({'images': inference_images}),\r\n                                                    'outputs': exporter.generic_signature({'class': inference_class, 'predictions': inference_predictions})})\r\nmodel_exporter.export(export_path, tf.constant(1), sess)\r\nprint('Done exporting!')\r\n```\r\n\r\nNext, I am trying to load saved model to CPU machine with:\r\n\r\n`new_saver = tf.train.import_meta_graph('assets/saved_model/export.meta')\r\nnew_saver.restore(sess, 'assets/saved_model/export')`\r\n\r\nAnd what I am getting is:\r\n\r\n```\r\nTraceback (most recent call last):\r\nFile \"script_test_classifier.py\", line 4, in <module>\r\n...\r\nline 33, in __initialize_session__\r\nnew_saver = tf.train.import_meta_graph('assets/saved_model/export.meta')\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1711, in import_meta_graph\r\nread_meta_graph_file(meta_graph_or_file), clear_devices)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1598, in _import_meta_graph_def\r\ninput_graph_def, name=\"\", producer_op_list=producer_op_list)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 258, in import_graph_def\r\nop_def = op_dict[node.op]\r\nKeyError: u'SaveV2'\r\n```\r\n\r\nWhat is the reason of the error and how it could be fixed?\r\n\r\n**Production (CPU machine) environment info:**\r\nAWS instance type: m4.xlarge\r\nOperating System: Ubuntu 14.04 x64\r\nInstalled version of CUDA and cuDNN: None, I am using CPU version of TF for the production environment\r\nA link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\nThe output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`: 0.11.0\r\n", "comments": ["@shelpuk I believe you are using a newer version of TensorFlow to make the graph, which then isn't readable by the older version of TensorFlow on the production machine.  TensorFlow is not guaranteed to be forward compatible across versions.\n\n@petewarden Looks like this may be your importer code.  Throwing an exception is correct, but `KeyError` isn't a very informative exception.  You up for cleaning that up?\n", "Closing due to inactivity. Most likely due to needing a newer version of TensorFlow installed. Also see related commit improving error message.", "/home/pr/PycharmProjects/TensorVoice/venv/bin/python /home/pr/Documents/projects/speech_recognition/Speech_Commands/test.py\r\n2018-03-20 16:39:54.772431: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/home/pr/Documents/projects/speech_recognition/Speech_Commands/test.py\", line 41, in <module>\r\n    new_saver = tf.train.import_meta_graph('/home/pr/Documents/projects/speech_recognition/Speech_Commands/trains/conv.ckpt-8.meta')\r\n  File \"/home/pr/PycharmProjects/TensorVoice/venv/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1909, in import_meta_graph\r\n    **kwargs)\r\n  File \"/home/pr/PycharmProjects/TensorVoice/venv/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\", line 737, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/home/pr/PycharmProjects/TensorVoice/venv/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/pr/PycharmProjects/TensorVoice/venv/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 431, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"/home/pr/PycharmProjects/TensorVoice/venv/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 211, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: # 'DecodeWav'\r\n\r\n\r\n\r\n> Even i am getting the same problem, Do i have to degrade my Tensorflow \r\n\r\n ??\r\n\r\n", "KeyErrors in loading a Tensorflow graph can be because of missing explicit import statements.  I ran into the same error for `no op named GatherTree`. Apparently Tensorflow does some dynamic loading of imports, thus causing such errors. In my opinion, this is more of a bug and at least the error messages should suggest an appropriate action. Anyway, I fixed my issue using the following:\r\n\r\n`from tensorflow.contrib.seq2seq.python.ops import beam_search_ops`\r\n\r\nYou can try a similar solution by looking up the relevant op that should be imported.", "KeyError: u'SSTableReaderV2'\r\n", "I also experienced \"KeyError: 'CSVDataset'\" while loading protobuf (saved_model.pb) thru tf.saved_model.loader.load.\r\nAfter declaring 'from tensorflow.contrib.data import CsvDataset', my app works like a charm!\r\nThx @iamgroot42 :) \ud83d\udc4d "]}, {"number": 5638, "title": "train inception from scratch: imagenet_train not a target", "body": "\r\n\r\nI successfully compiled tensorflow and I was able to run the retrain binary with my own images. \r\n\r\nI'd like to retrain inception from scratch, as described [here](https://github.com/tensorflow/models/tree/master/inception), but the package is not available:\r\n\r\n```\r\n$ bazel build inception/imagenet_train\r\nERROR: no such target '//:inception/imagenet_train': target 'inception/imagenet_train' not declared in package\r\n```", "comments": ["Note that inception is in a different `models` repository.\n", "This is what I see under models\n\n```\n$ bazel build tensorflow/models/\ntensorflow/models/embedding:  tensorflow/models/embedding/  tensorflow/models/image/      tensorflow/models/rnn:        tensorflow/models/rnn/  \n```\n\nI don't see `imagenet_train`. Is `image` the same thing?\n", "@zzzrpagliari If you want to find inception in the models repo, please click on your own link from the original comment.\n"]}, {"number": 5637, "title": "word2vec_basic global_variables_initializer error", "body": "I'm running Tensorflow version 0.11.0 which has [this](https://github.com/tensorflow/tensorflow/commit/4cbdead95f22de74bcbc72a68c9a38d465202db9#diff-ae1a8f7b66539f000615a4ab7e4b2151) commit included and yet I get the following error when running the ``word2vec_basic.py`` script:\r\n```\r\n##################################################\r\n0.11.0\r\n##################################################\r\nFound and verified text8.zip\r\nData size 17005207\r\nMost common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\r\nSample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\r\n3084 originated -> 12 as\r\n3084 originated -> 5239 anarchism\r\n12 as -> 6 a\r\n12 as -> 3084 originated\r\n6 a -> 195 term\r\n6 a -> 12 as\r\n195 term -> 6 a\r\n195 term -> 2 of\r\nTraceback (most recent call last):\r\n  File \"word2vec_basic.py\", line 183, in <module>\r\n    init = tf.global_variables_initializer()\r\nAttributeError: 'module' object has no attribute 'global_variables_initializer'\r\n```\r\n\r\nThe part at the top with hashtags is something I added to print out the tf version that is used.\r\nWhen I rename [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L178) line back to ``initialize_all_variables`` it works.", "comments": ["@vkaracic I just checked, and it doesn't look like 0.11.0 has that commit.  Are you sure?\n", "I'm sorry, I was looking at the wrong file. Nevertheless `word2vec_basic` script breaks, I guess there is no point in opening a PR reverting that one line back to `initialize_all_variables` if that commit is going to be in the next release?\n", "@vkaracic The 0.11.0 version of `word2vec_basic` already uses `initialize_all_variables`, so I think all is well.  You're trying to run a new version of a tutorial with an old version of tensorflow.\n"]}, {"number": 5636, "title": "Official download links still point to 0.11.0rc2 instead of 0.11.0", "body": "Hi!\r\n\r\nOn https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md  the URLs still point to 0.11.0rc2 instead of 0.11.0.\r\n\r\n", "comments": ["#5629 just got merged!\n"]}, {"number": 5635, "title": "R0.11", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@sueyllam, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @keveman to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}]