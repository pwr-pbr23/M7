[{"number": 175, "title": "einsum-like function?", "body": "Numpy has einsum which is very useful for formulating and computing tensor operations efficiently. Any plans to include an equivalent in tensorflow?\n", "comments": ["We are not working on it right now -- would be happy to have contributions though!\n", "any updates on this issue?\n", "Would also like to see this. One tensor op to rule them all.\n\nIf anyone wants to have a go at this, note that the gradient of einsum can be implemented neatly as another einsum with swapped arguments / indices. See the autograd implementation make_grad_einsum: https://github.com/HIPS/autograd/blob/master/autograd/numpy/numpy_grads.py#L447 \n", "Just a minor comment: NumPy einsum supports arbitrary number of input arrays, but I guess it would suffice to support only two input tensors in TensorFlow.\n", "einsum would subsume tensordot which is tracked in https://github.com/tensorflow/tensorflow/issues/216\n", "I'm taking a stab at this, but running into the following issue:\n\nI want to call the templated method tensorflow::Tensor::tensor<T, NDIMS>  and then use Eigen's contract (https://bitbucket.org/eigen/eigen/src/default/unsupported/Eigen/CXX11/src/Tensor/README.md?fileviewer=file-view-default#markdown-header-contraction),\n but the number of dimensions will not be known at compile time.\n\nI noticed that in other parts of the codebase, the following pattern is used:\n\nTensor input_a = ....\nswitch (input_a.dims()) {\ncase 1: <do stuff with input_a.tensor<T, 1>() >\ncase 2: <do stuff with input_a.tensor<T, 2>() >\n...\n\nBut in this case there would be 3 tensors (2 inputs and the output) whose number of dimensions are not known at compile time, so doing something like this would be rather messy.\n\nThoughts on the preferred / most elegant way to do this?\n", "@yaroslavvb thanks Bulatov, It did work for high dimentional tensors inner product!"]}, {"number": 174, "title": "run from script only instead of compiling with bazel?", "body": "I posted in the google discussion group but I think the turn-over rate is faster here. Hopefully I get a faster response.\n\nIn the RNN example tutorial, we're compiling the python script with bazel before executing it, instead of executing the script directory as in the simple MNIST example. Is it possible to do the RNN example purely in python script instead of using bazel? I used a binary installation instead of cloning the repo, so many of the file structures are different. I tried to hack it so I can run it from script instead of compiling with bazel, but I'm getting this error:\n\n```\n$ python ptb_word_lm.py --data_path=simple-examples/data --model small\n   File \"ptb_word_lm.py\", line 128, in __init__\n   self._cost = cost = tf.reduce_sum(loss) / batch_size\n   TypeError: unsupported operand type(s) for /: 'Tensor' and 'int'\n```\n\nOr is learning bazel the only way (which I don't mind but would like to avoid)? \n", "comments": ["I think some of those models aren't in our pip package yet -- building from source works for those.  We'll try to get around to fixing this soon, but for now building from source should work.\n", "By that you mean I'd be using bazel to build but I can still run my experiment with a script alone right? I.e. bazel is not used for launching experiments \n", "The reason this won't using just the binary installed version of pip is because it relies on some additional includes not in the pip package (like reader.py in the same directory).  We didn't include the tutorials in the pip package because the package is mainly meant for modeling code.\n\nAs a hack, you could try setting your PYTHONPATH to the tensorflow/python/ subdirectory but that will probably break other things.\n\nYou can also move the python files in the ptb/ directory somewhere else, keep only the 'import tensorflow as tf' import and change the import of reader.py to 'import reader'.  That'll get you part of the way there.\n", "that last bit of your suggestion is exactly what I did to get the error I'm reporting. I copied the /ptb directory and adjusted the reader.py location in the import\n\nBut I guess that's not important now. I could run and past the test scripts:\nrnn_cell_test.py \ntf_rnn_test.py\n\ndoes that mean at least the rnn and rnn_cell module is included in the pip package and is functional (I've noticed they're not in the documentation on tensorflow.org page under NeuralNetworks, Are they stable)?\n\nAlso, just for reference, is there a tutorial to install tensorflow from source w/o GPU support?\n", "Yes, rnn and rnn_cell _are_ included in the pip package.\n\nTo build tensorflow from source, use:\n  bazel build -c opt\nwithout the --config=cuda flag.\n\nYou can use this to build and run the build_pip_package target.  The build_pip_package will then create a binary wheel without the cuda.\n\nThis is in the tutorials.\n", "I'm still having some trouble:\n\n```\ntensorflow$ bazel build -c opt\nINFO: Found 0 targets...\nINFO: Elapsed time: 0.189s, Critical Path: 0.00s\n```\n\nsorry I don't code much at all, this is fairly challenging. \n", "Follow the instructions [here](http://www.tensorflow.org/get_started/os_setup.md) under the section \"Create the pip package and install\".  However, also pass in the flag \"--config=cuda\" as well as the current \"-c opt\" in order to build with CUDA.  Remember to run the configure script before running the bazel build.\n", "Ah sorry - you were not interested in having a cuda build.  Ignore my suggestion of adding \"--config=cuda\" and just follow the directions as they are.\n", "thanks! \nIt had not occured to me to look under installation direction under MacOS as I'm running ubuntu.\nI think we're good for now. Thanks for being patient with me.\n"]}, {"number": 173, "title": "Unable to restore trained models on the en-fr translate model:  tensorflow.python.framework.errors.NotFoundError: Tensor name \"embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix\" not found", "body": "Hi, when I try the command:\n\nSince ultimately I want to use a model similar to translate for POS tagging, I just copies translate.py to pos.py and trained a model.\n\n`/home/vvkulkarni/bazel/output/bazel run -- //tensorflow/models/rnn/translate:pos --data_dir /data/vivek/pos_tags/ --train_dir /data/vivek/pos_tags/models/ --decode 1`\n\nI get the error:\n\n`File \"/home/vvkulkarni/.cache/bazel/_bazel_vvkulkarni/4fc9a6787896f8b64890985f604431f2/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorfl\now/models/rnn/translate/pos.runfiles/tensorflow/python/platform/default/_app.py\", line 15, in run\n    sys.exit(main(sys.argv))\n  File \"/home/vvkulkarni/.cache/bazel/_bazel_vvkulkarni/4fc9a6787896f8b64890985f604431f2/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorfl\now/models/rnn/translate/pos.runfiles/tensorflow/models/rnn/translate/pos.py\", line 262, in main\n    decode()\n  File \"/home/vvkulkarni/.cache/bazel/_bazel_vvkulkarni/4fc9a6787896f8b64890985f604431f2/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorfl\now/models/rnn/translate/pos.runfiles/tensorflow/models/rnn/translate/pos.py\", line 196, in decode\n    model = create_model(sess, True)\n  File \"/home/vvkulkarni/.cache/bazel/_bazel_vvkulkarni/4fc9a6787896f8b64890985f604431f2/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorfl\now/models/rnn/translate/pos.runfiles/tensorflow/models/rnn/translate/pos.py\", line 112, in create_model\n    model.saver.restore(session, \"/data/vivek/pos_tags/models/translate.ckpt-13450\")\n  File \"/home/vvkulkarni/.cache/bazel/_bazel_vvkulkarni/4fc9a6787896f8b64890985f604431f2/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorfl\now/models/rnn/translate/pos.runfiles/tensorflow/python/training/saver.py\", line 867, in restore\n    sess.run([self._restore_op_name], {self._filename_tensor_name: save_path})\n  File \"/home/vvkulkarni/.cache/bazel/_bazel_vvkulkarni/4fc9a6787896f8b64890985f604431f2/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorfl\now/models/rnn/translate/pos.runfiles/tensorflow/python/client/session.py\", line 349, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/home/vvkulkarni/.cache/bazel/_bazel_vvkulkarni/4fc9a6787896f8b64890985f604431f2/tensorflow/bazel-out/local_linux-fastbuild/bin/tensorfl\now/models/rnn/translate/pos.runfiles/tensorflow/python/client/session.py\", line 423, in _do_run\n    e.code)\ntensorflow.python.framework.errors.NotFoundError: Tensor name \"embedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix\" n\not found in checkpoint files /data/vivek/pos_tags/models/translate.ckpt-13450\n         [[Node: save/restore_slice_14 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_sa\nve/Const_0, save/restore_slice_14/tensor_name, save/restore_slice_14/shape_and_slice)]]\nCaused by op u'save/restore_slice_14', defined at`\n", "comments": ["Can you upload the checkpoint file for inspection?\n", "Attaching the file. Because github does not allow attachments of certain filetypes, I had ro rename the file to .txt.\n\n[translate.ckpt-12950.txt](https://github.com/tensorflow/tensorflow/files/33528/translate.ckpt-12950.txt)\n", "It is looking for:\n\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell2/GRUCell/Gates/Linear/Matrix\n\nbut your checkpoint contains:\n\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix\nand\nembedding_attention_seq2seq/RNN/MultiRNNCell/Cell1/GRUCell/Gates/Linear/Matrix\n\nbasically the graph you loaded from has 3 stacked RNNs (looking for Cell2) but the checkpoint file was created with a graph trained with only 2 stacked RNNs (it has Cell0 and Cell1).\n", "If it helps, this is the command used to train the model:\n`/home/vvkulkarni/bazel/output/bazel run -- //tensorflow/models/rnn/translate:pos --data_dir /data/vivek/pos_tags/ --train_dir /data/vivek/pos_tags/models_new/ --size=64 --num_layers=2 --steps_per_checkpoint=50 --fr_vocab_size 48 --batch_size 100`\n\nand this is the command I used to decode:\n`/home/vvkulkarni/bazel/output/bazel run -- //tensorflow/models/rnn/translate:pos --data_dir /data/vivek/pos_tags/ --train_dir /data/vivek/pos_tags/models_new/ --decode 1 --fr_vocab_size 48`\n", "Try padding the flag \"--num_layers=2\" to the second call.  My guess is the default value is 3.\n", "I tried that, but that does not work. I get more errors: W tensorflow/core/common_runtime/executor.cc:1027] 0x8e51560 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1024] rhs shape= [64]\n\nSee below for details\n\nINFO: Running command line: bazel-bin/tensorflow/models/rnn/translate/pos --data_dir /data/vivek/pos_tags/ --train_dir /data/vivek/pos_tags/models_new/ --decode 1 --fr_vocab_size 48 --num_layers 2\n\n`I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 24\nI tensorflow/core/common_runtime/direct_session.cc:45] Direct session inter op parallelism threads: 24\nW tensorflow/core/common_runtime/executor.cc:1027] 0x8e51560 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1024] rhs shape= [64]\n         [[Node: save/Assign_11 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attentio\nn_decoder/Attention_0/Linear/Bias, save/restore_slice_11)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x8e51560 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,1,1024,1024] rhs shape= [1,1,64,64]\n         [[Node: save/Assign_16 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attentio\nn_decoder/AttnW_0, save/restore_slice_16)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x8e51560 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [2048,1024] rhs shape= [128,64]\n         [[Node: save/Assign_4 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/RNN/MultiRNNCell/Cell0/GRUCell/Candid\nate/Linear/Matrix, save/restore_slice_4)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x8e51560 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [2048,1024] rhs shape= [128,64]\n         [[Node: save/Assign_12 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attentio\nn_decoder/Attention_0/Linear/Matrix, save/restore_slice_12)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x8e51560 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1024] rhs shape= [64]\n         [[Node: save/Assign_23 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attentio\nn_decoder/MultiRNNCell/Cell1/GRUCell/Candidate/Linear/Bias, save/restore_slice_23)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x8e51560 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1024] rhs shape= [64]\n         [[Node: save/Assign_15 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attentio\nn_decoder/AttnV_0, save/restore_slice_15)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x8e51560 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [2048] rhs shape= [128]\n         [[Node: save/Assign_25 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attentio\nn_decoder/MultiRNNCell/Cell1/GRUCell/Gates/Linear/Bias, save/restore_slice_25)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x8e51560 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [2048,1024] rhs shape= [128,64]\n         [[Node: save/Assign_18 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attentio\nn_decoder/Linear/Matrix, save/restore_slice_18)]]\nW tensorflow/core/common_runtime/executor.cc:1027] 0x8e51560 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [2048,2048] rhs shape= [128,128]\n         [[Node: save/Assign_22 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/attentio\nn_decoder/MultiRNNCell/Cell0/GRUCell/Gates/Linear/Matrix, save/restore_slice_22)]]`\n", "When you run decoding, you basically have to have the same parameters as when you trained.  For example, you forgot to add --size=64 to the second command, and the number 64 is one of the incompatible dimensions.  I bet if you look at the code, you'll see that the default value for size is 1024.\n", "Yup :) Thanks that seems to work, although it seems counter-intuitive to me, that I need to restate all the parameters I used when I train the model. Ideally it should have picked up the parameters from the serialized model.  I am not sure if this is a bug or as intended. Please feel free to close the bug if you think appropriate.\n\nPasting what worked, so that others who may have hit the isseue may benefit,\n\n /home/vvkulkarni/bazel/output/bazel run -- //tensorflow/models/rnn/translate:pos --data_dir /data/vivek/pos_tags/ --train_dir /data/vivek/pos_tags/models_new/ --decode 1 --fr_vocab_size 48 --num_layers 2 --size 64\n", "A program _could_ try to pick up the parameters from the serialized model -- but for the tutorial we'll keep it as is for now.  Please re-open if there are any other issues here!\n", "Yes, I had the same problem, and when I gave the same params like \"--size=256 --num_layers=2\" while decode too, it worked. I think defaults are different during decode.\n", "Hello everyone,\r\nI executed the code translate.py from sequence to sequence models tutorial of tensorflow for 20 hours \r\n![seqgoo](https://cloud.githubusercontent.com/assets/20130992/23177149/ca80dc96-f833-11e6-99c7-61b2150c7261.png)\r\nand did not get output.\r\nCan anyone tell me how much time will it take to display the output\r\nSystem specifications:\r\nI installed Ubuntu 16.04 on VMware with a disk space of 100"]}, {"number": 172, "title": "Wrong Logistic Loss", "body": "In tensorflow/python/ops/nn.py, the logistic loss is described as\n\nx - x \\* z + log(1 + exp(-x))\n\nbut this is not what's commonly known as the logistic loss.\nI see that you're trying to apply a trick here to avoid overflow, but it should be \n\nx - x \\* z + log(exp(0 - (x - x \\* z)) + exp(-x))\n\ninstead of what you have.\n\nOf course, the correct version is now prone to underflow, so it would make most sense to add a native robust log-sum-exp function to tensorflow and then rely on that to avoid over- and underflow.\n", "comments": ["Just to clarify what we do here:\n\nSigmoid input is x, output is y, target is z:\n\ny = 1 / (1 + exp(-x))\n1 - y = exp(-x) / (1 + exp(-x))\n\nLogistic loss \n= -[z \\* log(y) + (1-z) \\* log(1-y)]\n= z \\* log(1 + exp(-x)) - (1-z) \\* [-x - log(1 + exp(-x))]\n= z \\* log(1 + exp(-x)) + x + log(1 + exp(-x)) - z_x - z \\* log(1 + exp(-x))\n= x - z_x + log(1 + exp(-x))\n\nTo avoid overflow and branching in TensorFlow this is calculated as:\n\nLogisticLoss = max(x, 0) - z*x + log(1 + exp(-abs(x)))\n", "What is the purpose of replacing -x with -abs(x)?\n", "@jey-ivanov 0 < exp(-abs(x)) <= 1 to avoid overflow. Without abs, the smaller the x value (negative), the greater the value of exp(-x).\n", "I see, you're assuming your targets to be 0/1, where as usually, people are using -1/+1 labels when speaking of the logistic loss.\nThanks for clarifying! Using that, I'm able to achieve the same test errors as with my naive log-sum-exp implementation.\n\nAre you planning to add a robust log-sum-exp in the future?\n", "It would be nice for Tensor Flow to have a native log-sum-exp, that'll make things faster, and help people have less numerical issues.\n", "It would be useful. Here's a  no-frills CL which adds a numeric op (squared difference of two tensors) - https://github.com/tensorflow/tensorflow/commit/bf536bcc888768b586440579b3fdeecf80a48acf It looks complicated because it works for multiple backends (CPU and GPU) and multiple front-ends (Python/Go/etc)\n", "I have a decent log-sum-exp implemented:\n\n```\ndef log_sum_exp(xs):\n  maxes = tf.reduce_max(xs, keep_dims=True)\n  xs -= maxes\n  return tf.squeeze(maxes, [-1]) + tf.log(tf.reduce_sum(tf.exp(xs), -1))\n```\n\nI think (?) this should avoid any major numerical issues...\n"]}, {"number": 171, "title": "Tensorboard creates unecessary loops in graph", "body": "I've followed the mnist expert tutorial and wanted to see the graph on the tensorboard. The dashboard looks great, except that in the graph unrelated nodes are grouped together, creating weird loops and making it harder to follow what's happening. Especially with Relu_1 & Relu_2, the merging of the arrows are quite confusing.\n\n![screenshot tensorboard](https://cloud.githubusercontent.com/assets/725737/11115434/c009582e-892b-11e5-8d55-89666555b14e.png)\n\nCode:\n\n``` python\nimport input_data\n\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\nimport tensorflow as tf\n\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding='SAME')\n\n\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(\"float\", shape=[None, 784])\ny_ = tf.placeholder(\"float\", shape=[None, 10])\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1, 28, 28, 1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\nkeep_prob = tf.placeholder(\"float\")\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n\ntf.scalar_summary('Training error', cross_entropy)\ntf.scalar_summary('Training accuracy', accuracy)\ntf.scalar_summary('sparsity', tf.nn.zero_fraction(h_fc1))\n\nsess.run(tf.initialize_all_variables())\n\n\nmerged_summary_op = tf.merge_all_summaries()\nprint merged_summary_op\nsummary_writer = tf.train.SummaryWriter('/tmp/mnist_logs', sess.graph_def)\n\nfor i in range(20000):\n    batch = mnist.train.next_batch(50)\n    sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n    if i % 100 == 0:\n        train_accuracy = accuracy.eval(feed_dict={\n            x: batch[0], y_: batch[1], keep_prob: 1.0})\n        print \"step %d, training accuracy %g\" % (i, train_accuracy)\n        summary_str = sess.run(merged_summary_op, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n        summary_writer.add_summary(summary_str, i)\n\n\n\nprint \"test accuracy %g\" % accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n```\n", "comments": ["Thanks for pointing this out. The looping was designed for LSTMs, where we may have >10 instances of the same section and displaying all of them is messy. In this case though, it's just confusing! I think we'll both disable looping for small numbers of repetitions, and also work on making the looped layout a bit easier to understand in general.\n", "This should be closed now. Won't link the commit since it was an early commit that bundled many commits together. Let us know if you run into this issue again. Thanks!\n"]}, {"number": 170, "title": "Not a gzipped file", "body": "When I get started with 'Train your first TensorFlow neural net model':\n`$ python tensorflow/models/image/mnist/convolutional.py`\n\nThere is an error:\n\n```\nFile \"tensorflow/models/image/mnist/convolutional.py\", line 59, in extract_data\n    bytestream.read(16)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 268, in read\n    self._read(readsize)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 303, in _read\n    self._read_gzip_header()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/gzip.py\", line 197, in _read_gzip_header\n    raise IOError, 'Not a gzipped file'\n```\n", "comments": ["The code attempts to download the data files from the MNIST web site, and assumes it's properly downloaded if the file is present locally on your system. You might have a corrupted file, in which case deleting it and retrying might help. Otherwise, try to get the data via your browser directly from:\nhttp://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nhttp://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nhttp://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nhttp://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n", "I downloaded these four data files from the link.But I don't know where these data file should be put.Is there anyone who can help me?  @vincentvanhoucke \r\n", "@SunsetWan \r\ndefault path is `/tmp/tensorflow/mnist/input_data/`, you can find it in `mnist_softmax.py`.", "@wogong thank you very much!", " File \"C:\\Users\\Ahmed\\Anaconda3\\lib\\gzip.py\", line 409, in _read_gzip_header\r\n    raise OSError('Not a gzipped file (%r)' % magic)\r\n\r\nOSError: Not a gzipped file (b'<h')", "The data exists locally on your machine, by default this data set located at `/tmp/cifar10_data`.\r\nremove this dir and run the training step again.", "For windows I found the corrupted files in  `C:\\Users\\username\\.keras\\datasets\\fashion-mnist`", "I tried your method and it succeed @vincentvanhoucke"]}, {"number": 169, "title": "Small typo in Deep MNIST Tutorial", "body": "There is a small typo on the [Deep MNIST for Experts](http://tensorflow.org/tutorials/mnist/pros/index.md) page. It's the first sentence of the second paragraph under the Start TensorFlow InteractiveSession header.\n\n> Here we instead use the convenience InteractiveSession class, ...\n\nshould maybe  be\n\n> Here we instead use the convenient InteractiveSession class,...\n\nor maybe\n\n> Here we instead use the convenience of the InteractiveSession class, ...\n", "comments": ["Pushed to git in https://github.com/tensorflow/tensorflow/commit/6b12d081d54b89869e26d9d99828f13de381761e  -- thanks!\n"]}, {"number": 168, "title": "tensor flow does not support operator.__truediv__", "body": "tensorflow type does not support operation.**truediv** as in the basic example in https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py\nIn line 153,  ' normalized_embeddings = embeddings / norm'  will cause the attribute error as followed\n\n\"AttributeError: type object 'Tensor' has no attribute '**truediv**\"\n\nTo solve the problem, we can replace this line as 'normalized_embeddings = tf.div(embeddings, norm)'\n", "comments": ["Fixed by 1d76583411038767f673a0c96174c80eaf9ff42f.  Also see #1 .\n", "Thankyou for your help\n"]}, {"number": 167, "title": "g3doc is not installed when using pip ", "body": "Hi All,\n\nAt least under Mac OS X the following command:\n\npip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\n\nwill not install g3doc (and probably some other modules). \n\nJust a minor issue :)\n", "comments": ["I'm seeing the same issue on ubuntu 14.04 \n", "How important is it to be installed for you? That documentation is more for our websites / github.  We could include it if it was important...\n", "In general we avoid installing g3doc as part of the package because it is not part of the core functionality.  Similarly with the tutorials; they're kept out so that people don't have to dig around in dist-packages & site-packages for the files.  They are instead made available in the git repo.\n", "Sounds good, closing for now then.\n"]}, {"number": 166, "title": "Does TensorFlow support temporal convolution", "body": "like [Torch does](https://github.com/torch/nn/blob/master/doc/convolution.md)? I assume not, hence asking here.\n\nTemporal convolution is useful for encoding a sentence. I find it work better than RNN/LSTM.\n", "comments": ["+1\n", "Could you tell me what's the difference between the temporal convolution and using tf.nn.conv_2d with a very specific kernel, e.g. [height=kW, width=embedding_size, in=inputFrameSize, out=outputFrameSize]?\n", "Related discussion on reddit: https://www.reddit.com/r/MachineLearning/comments/3sjl7o/tensorflow_vs_theano_vs_torch_comparison/cwxsq08\n", "In Theano's [conv2d](http://deeplearning.net/software/theano/library/tensor/nnet/conv.html#theano.tensor.nnet.conv.conv2d), you can specify the `image_shape` to be `None`, which signals that it's not a constant.\n\nIt's unclear if non-const input size is supported in TensorFlow's `conv2d`.\n", "Only strides / padding are constants, input and filter can be dynamic sizes: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/nn.md#conv2d\n", "Thanks.\n"]}, {"number": 165, "title": "in_top_k op does not work with int64 labels", "body": "The `tf.nn.in_top_k` op does not work with `int64` labels. This is problematic when computing accuracy from a one-hot encoding of labels as the output of `argmax` is `int64`:\n\n``` python\ny = tf.placeholder(\"float\")\nyhat = tf.placeholder(\"float\")\ntf.nn.in_top_k(yhat, tf.argmax(y, 1), 1)\n```\n\nyields the error:\n\n```\nTypeError: Input 'targets' of 'InTopK' Op has type int64 that does not match expected type of int32.\n```\n\nThe simple workaround is to cast the labels before calling the op:\n\n```\ntf.nn.in_top_k(y, tf.cast(tf.argmax(y_, 1), \"int32\"), 1)\n```\n\nbut it'd be nice if `int64` labels were supported out of the box.\n", "comments": ["I added this some time last week to the source -- let me know if this still doesn't work at HEAD.  (will be in our next binary release)\n"]}, {"number": 164, "title": "Direct Native Code / LLVM IR generation / JIT Compilation / Staging / Incremental Computing", "body": "If it possible to have a feature to bypass C++ and go directly to Native / LLVM IR\n", "comments": ["This can be useful when you have a defined DSL: https://github.com/tensorflow/tensorflow/issues/70\n", "This is something we are actively working on, although it is in the early stages.  It is discussed briefly in the second-to-last paragraph of the Future Work section of the white paper at http://tensorflow.org/whitepaper2015.pdf:\n\n\"We also have a number of concrete directions to improve the performance of TensorFlow. One such direction is our initial work on a just-in-time compiler that can take a subgraph of a TensorFlow execution, perhaps with some runtime profiling information about the typical sizes and shapes of tensors, and can generate an optimized routine for this subgraph. This compiler will understand the semantics of perform a number of optimizations such as loop fusion, blocking and tiling for locality, specialization for particular shapes and sizes, etc.\"\n", "Sounds jolly good to me. Also how about having a high level language / DSL around this.\n", "I am not a fan of Python but an following: https://github.com/Maratyszcza/PeachPy is an interesting approach used in http://www.yeppp.info/\n", "@jeffreyadean When you talk of Halide in the White paper you conclude with \"In future work we are hoping to extend\nTensorFlow with a similar cross-operation dynamic com-\npilation framework.\" Can you argument a little bit more on this?\n", "@sirinath OpenCV is working on something similar with [Numl](http://code.opencv.org/projects/opencv/wiki/2015#2015-11-10)\n", "Some optimisations may not be apparent when static set up. Perhaps TensorFlow can do some staging and optimisation of the execution while running based on usage patterns, i.e., JIT compilation / Runtime Code Re Optimisation. This options need to be opt in where you have to option of full static optimisation and ability to specify a graph or sub graph for JIT / staging where this might help.\n\nPerhaps you can borrow some ideas from projects related to: https://scala-lms.github.io/\n", "This issue is more of a large scale road map item, and is already handled elsewhere. This issue is not individually actionable, so closing for now.\n", "Can you point me to the relevant project?\n", "We are working on it, there's no separate project for this.\n"]}, {"number": 163, "title": "Published Roadmap", "body": "Can you publish a road map to get an idea on the direction which project is going?\n", "comments": ["Same as: https://github.com/tensorflow/tensorflow/issues/162\n"]}, {"number": 162, "title": "Any Roadmap Available?", "body": "As stated on http://tensorflow.org,\n\n> TensorFlow is not complete; it is intended to be built upon and extended.\n\nI would therefore like to know if a roadmap exists.\n", "comments": ["Yes something like [OpenCV weekly meetings](http://code.opencv.org/projects/opencv/wiki/2015), Torch roadmap at https://github.com/torch/torch7/issues/326 or [Gitlab direction](https://about.gitlab.com/2016/01/05/future-direction-gitlab/) could be really helpful for better planning PR activities by the community and to not overlap too much with Google Tensorflow team internal work.\n", "See also https://about.gitlab.com/2016/01/11/being-a-good-open-source-steward/\n", "@vrv Thank you. Can you add a section \"we are on...\"  where you can expose what is already in work by Google team. It seems that you don't use WIP PR for internals so could be useful to update a section like this to avoid contributors to start to works on things already allocated internally.\n", "These are all things we are actively working on. We don't yet have a more aspirational longer term timeline yet, when we do, we will publish that as well. That may include things we know we want but are not yet working on. \n\nWhere there are things that are useful for the community to take on, we have marked them with the [contributions welcome](https://github.com/tensorflow/tensorflow/labels/contributions%20welcome) tag. For instance, many issues relating to performance improvements are marked this way.\n", "@martinwicke Ok thank you for the clarification. But e.g. https://github.com/tensorflow/tensorflow/issues/22 is tagged contributions welcome but it is also in the Roadmap. Is it tagged contributions welcome because is \"long term\" or do you have already started to design/code on it? I'm asking you this because the mean of my previous comment was to clarify, for a contributor, what make sense to start in his own fork to not conflict further with an emerging internal code developed on same target.\n", "In case of #22, We are coordinating with the people on that thread (and\nothers). It hasn't gotten beyond initial planning yet. If you are\nconsidering contributing for anything, do comment on the associated issue\nto find out the current state of affairs and to avoid duplicating effort.\nThe issues will be updated with major developments, but usually the actual\ndevelopment happens outside the issue thread.\n\nOn Thu, Jan 14, 2016 at 9:27 AM bhack notifications@github.com wrote:\n\n> @martinwicke https://github.com/martinwicke Ok thank you for the\n> clarification. But e.g. #22\n> https://github.com/tensorflow/tensorflow/issues/22 is tagged\n> contributions welcome but it is also in the Roadmap. Is it tagged\n> contributions welcome because is \"long term\" or do you have already started\n> to design/code on it? I'm asking you this because the mean of my previous\n> comment was to clarify, for a contributor, what make sense to start in his\n> own fork to not conflict further with an emerging internal code developed\n> on same target.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/162#issuecomment-171714012\n> .\n"]}, {"number": 161, "title": "Can't install tensorflow on OS X - problem with virtualenv", "body": "I try to install tensorflow, but run into this message, what should I do?\n\nJohans-MacBook-Pro:~ johanbjorck$ pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl\nCould not find an activated virtualenv (required).\nJohans-MacBook-Pro:~ johanbjorck$ \n", "comments": ["try running \nexport PIP_REQUIRE_VIRTUALENV=false\nfrom the terminal and reinstalling. \n", "It worked, thanks a lot :D\n\n2015-11-11 22:28 GMT-05:00 Christian Beasley notifications@github.com:\n\n> try running\n> export PIP_REQUIRE_VIRTUALENV=false\n> from the terminal and reinstalling.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "You're welcome :D\n", "Thanks for the help @yyttr3!\n"]}, {"number": 160, "title": "doc for install from source of pip+gpu missing --config=cuda and --use_gpu", "body": "I was able to use GPU from python on AWS g2.2xlarge only after adding `--config=cuda` and `--use_gpu`\nto the the instructions for creating a pip at http://tensorflow.org/get_started/os_setup.md#installing_from_sources\n\n```\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg --use_gpu\n```\n", "comments": ["--use_gpu seems unnecessary\n--config=cuda is necessary: we've updated our git docs, our website hasn't been updated.  should be updated tomorrow though!\n"]}, {"number": 159, "title": "tensorflow binary image for ARM architecture", "body": "Any chance that an ARM compatible tensorflow whl binary image is available for Linux? \n", "comments": ["Not at the moment -- you might try building from source for arm.  We don't really have a test infrastructure yet to make sure we keep it working, which is why we don't build the binary package for it.\n", "@petewarden: What's the status of this now? \n", "I'm hoping to work on this over the next couple of weeks.\n", "I actually misread this - I didn't realize it was discussing an ARM binary for Linux. I'm not likely to be working on that, the closest I have is the makefile support for the Raspberry Pi. I'm going to close this since it's an enhancement and is unlikely to be worked on soon.\n"]}, {"number": 158, "title": "Switch int to uint to remove some warnings", "body": "eliminate some compiler warnings.\n", "comments": ["As noted in our contributing.md, currently our changes have to go through gerrit (which I see you've done, and we'll try to integrate soon).  Closing for now, thanks!\n"]}, {"number": 157, "title": "Change test set in mnist demo to use batches to avoid being OOM (>4GB) on gpu.", "body": "I feel like most potential enthusiasts don't really have access to >4GB gpu.\n\nThis change aims at making the examples that ship with tensorflow easier to play with for amateurs.\n", "comments": ["With the latest BFC allocator I think you mentioned this is no longer necessary.  We want to keep convolutional.py pretty simple -- if this is required for even smaller GPUs, we might want to just mention that in the file and point to an external example.  Thanks for looking into this and helping to verify the fix!\n", "Needed to apply this patch to run in my environment.\n\nRunning on Ubuntu 14.04.3 LTS (Trusty Tahr) with NVIDIA GeForce GTX 780 Ti, for which NVidia Compute Capability == 3.5, using python2.7.\n\nPulled from git repo Sunday December 13, top of git log:\n\n```\ncommit 6936918ad8abd7ade445674d7c912e69157a1017\nMerge: 8de955d 10e62dc\nAuthor: Vijay Vasudevan <vrv@google.com>\nDate:   Fri Dec 11 23:17:16 2015 -0800\n\n    TensorFlow: Merging changes from internal\n```\n", "I encountered this error while running a p2 instance on AWS. I used this command to see how the GPU was doing:\n\n> watch nvidia-smi\n\nBeside the OOM woes, I also saw no GPU utilization.\nOne thing that worked for me, was to make sure that I didn't have any extra notebooks running. I stopped all other running notebooks in jupyter, and restarted the kernel. After that, the GPU utilization came back and the OOM error went away.\n", "Thanks for the patch!!", "I also get the OOM issue while with this reason:\r\nHere is the logs outputs from the terminal and the GPU usage status. My device is Ubuntu 14.04, GCC 4.8.4, Linux 2, python 2.7.6, GTX 1080(8G memory,total 4 GPUS in my linux server). Could anyone help me?\r\n\r\n```\r\ntensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 256.0KiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:993] Resource exhausted: OOM when allocating tensor with shape[128,512]\r\nTraceback (most recent call last):\r\n  File \"mnist_mlp.py\", line 64, in <module>\r\n    validation_data=(x_test, y_test))\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 845, in fit\r\n    initial_epoch=initial_epoch)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training.py\", line 1485, in fit\r\n    initial_epoch=initial_epoch)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/training.py\", line 1140, in _fit_loop\r\n    outs = f(ins_batch)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 2073, in __call__\r\n    feed_dict=feed_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[128,512]\r\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=4432543, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dropout_1/cond/dropout/Shape)]]\r\n\t [[Node: Mean_3/_33 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_546_Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'dropout_1/cond/dropout/random_uniform/RandomUniform', defined at:\r\n  File \"mnist_mlp.py\", line 49, in <module>\r\n    model.add(Dropout(0.2))\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 455, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 554, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/core.py\", line 111, in call\r\n    training=training)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 2421, in in_train_phase\r\n    x = switch(training, x, alt)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 2380, in switch\r\n    else_expression_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1738, in cond\r\n    orig_res, res_t = context_t.BuildCondBranch(fn1)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1639, in BuildCondBranch\r\n    r = fn()\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/core.py\", line 109, in dropped_inputs\r\n    seed=self.seed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 2681, in dropout\r\n    return tf.nn.dropout(x * 1., retain_prob, noise_shape, seed=seed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1936, in dropout\r\n    dtype=x.dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/random_ops.py\", line 244, in random_uniform\r\n    seed2=seed2)\r\nEvery 2.0s: nvidia-smi                    878cb24ce8b6: Wed Oct 18 07:26:58 2017\r\n\r\nWed Oct 18 07:26:58 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 0000:05:00.0     Off |                  N/A |\r\n| 86%   86C    P2   156W / 200W |   7789MiB /  8112MiB |     94%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1080    Off  | 0000:06:00.0     Off |                  N/A |\r\n| 97%   93C    P2   129W / 200W |   7791MiB /  8113MiB |     98%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 1080    Off  | 0000:09:00.0     Off |                  N/A |\r\n| 86%   85C    P2   184W / 200W |   7791MiB /  8113MiB |     98%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 1080    Off  | 0000:0A:00.0     Off |                  N/A |\r\n| 47%   63C    P2   145W / 200W |   7791MiB /  8113MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\nEvery 2.0s: nvidia-smi                     878cb24ce8b6: Wed Oct 18 07:26:58 2017\r\n\r\nWed Oct 18 07:26:58 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 0000:05:00.0     Off |                  N/A |\r\n| 86%   87C    P2   131W / 200W |   7789MiB /  8112MiB |     98%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1080    Off  | 0000:06:00.0     Off |                  N/A |\r\n| 97%   93C    P2   124W / 200W |   7791MiB /  8113MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 1080    Off  | 0000:09:00.0     Off |                  N/A |\r\n| 86%   85C    P2   102W / 200W |   7791MiB /  8113MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 1080    Off  | 0000:0A:00.0     Off |                  N/A |\r\n| 47%   64C    P2   123W / 200W |   7791MiB /  8113MiB |     92%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\nEvery 2.0s: nvidia-smi                                             878cb24ce8b6: Wed Oct 18 07:26:59 2017\r\n\r\nWed Oct 18 07:26:59 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 0000:05:00.0     Off |                  N/A |\r\n| 86%   86C    P2   138W / 200W |   7789MiB /  8112MiB |     96%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1080    Off  | 0000:06:00.0     Off |                  N/A |\r\n| 97%   92C    P2   135W / 200W |   7791MiB /  8113MiB |     75%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 1080    Off  | 0000:09:00.0     Off |                  N/A |\r\n| 86%   85C    P2   100W / 200W |   7791MiB /  8113MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 1080    Off  | 0000:0A:00.0     Off |                  N/A |\r\n| 47%   63C    P2   120W / 200W |   7791MiB /  8113MiB |     93%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\nEvery 2.0s: nvidia-smi                                                           878cb24ce8b6: Wed Oct 18 07:29:25 2017\r\n\r\nWed Oct 18 07:29:25 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 0000:05:00.0     Off |                  N/A |\r\n| 85%   84C    P2   118W / 200W |   7789MiB /  8112MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1080    Off  | 0000:06:00.0     Off |                  N/A |\r\n| 96%   90C    P2   121W / 200W |   7791MiB /  8113MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 1080    Off  | 0000:09:00.0     Off |                  N/A |\r\n| 86%   87C    P2   132W / 200W |   7791MiB /  8113MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 1080    Off  | 0000:0A:00.0     Off |                  N/A |\r\n| 46%   62C    P2   136W / 200W |   7791MiB /  8113MiB |     99%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n```"]}, {"number": 156, "title": "RuntimeError: Broken toolchain: cannot link a simple C program", "body": "Trying to install on Ubuntu 14.04.03 using pip, i have this error\n\n```\npip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\n```\n\n```\nRuntimeError: Broken toolchain: cannot link a simple C program\nCommand /usr/bin/python -c \"import setuptools, tokenize;__file__='/tmp/pip_build_root/numpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-hnmAZh-record/install-record.txt --single-version-externally-managed --compile failed with error code 1 in /tmp/pip_build_root/numpy\nStoring debug log for failure in /root/.pip/pip.log\n```\n", "comments": ["Installing numpy, swig and python-dev fixed the issue.\n\n```\nsudo apt-get install python-numpy swig python-dev\n```\n"]}, {"number": 155, "title": "Complete the loop before returning the words", "body": "The loop now completes before returning the words, 'with' is used to make sure the file closes and the description of the function now describes that it returns a list of words, not a string.\n", "comments": ["As noted in our contributing.md, currently our changes have to go through gerrit.  Closing for now, thanks!\n"]}, {"number": 154, "title": "Changes to word2vec_basic.py", "body": "The routine returns now after performing the full loop, and uses with to ensure the file is properly closed. \n", "comments": []}, {"number": 153, "title": "Library not loaded: /usr/lib/libc++.1.dylib", "body": "When trying to run an example, as given in the [instructions](http://tensorflow.org/get_started/os_setup.md#virtualenv_install), the command \"python tensorflow/models/image/mnist/convolutional.py\" does not work because that is not the proper path to convolutional.py.\n\nIf I use the proper path, I get this error:\n\n```\n $ source bin/activate\n(tensorflow) $ python lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\nTraceback (most recent call last):\n  File \"lib/python2.7/site-packages/tensorflow/models/image/mnist/convolutional.py\", line 12, in <module>\n    import tensorflow.python.platform\n  File \"/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py\", line 4, in <module>\n    from tensorflow.python import *\n  File \"/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 22, in <module>\n    from tensorflow.python.client.client_lib import *\n  File \"/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/client_lib.py\", line 35, in <module>\n    from tensorflow.python.client.session import InteractiveSession\n  File \"/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 11, in <module>\n    from tensorflow.python import pywrap_tensorflow as tf_session\n  File \"/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: dlopen(/Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 2): Library not loaded: /usr/lib/libc++.1.dylib\n  Referenced from: /Users/munafo/devt/tensorflow/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n  Reason: image not found\n(tensorflow) $ \n```\n\nI have a complete index of all files on my machine, and there is no \"libc++.1.dylib\" anywhere. I do have a \"/usr/lib/libstdc++.6.dylib\".\n\nMacOS 10.6.8, Python 2.7.3, virtualenv 1.7.1.2\n", "comments": ["Can you try to build the pip package from source following the instructions on tensorflow.org, install that, and report back?\n", "[Here](http://tensorflow.org/get_started/os_setup.md#installing_from_sources) are the instructions. As you can see, the first step is to install [Bazel](http://bazel.io/docs/install.html), and their fist step is to install JDK 8. The JDK 8 installers are [here](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html). I got \"JDK 8 Update 65.pkg\", dated 2015 Oct 6. It claims it needs \"Mac OS X 10.7.3 or later\" to install. Even JDK 7 [makes the same claim](https://docs.oracle.com/javase/7/docs/webnotes/install/mac/mac-jdk.html).\n\nIt seems I might be better off building the missing **libc++.1.dylib** library itself, as detailed [here](http://libcxx.llvm.org/) and [here](http://stackoverflow.com/questions/18033255/install-libc-on-mac-10-6-8).\n", "Closing due to inactivity -- reopen if still interested and the problem hasn't been fixed in the past few months.\n", "I am trying to set up TensorFlow and am receiving this same error both with a standard pip install and also using virtual env. Has any progress been made @vrv @mrob27 @ebrevdo ? Any tips much appreciated to get started with the appealing library. \n", "I got stuck at a later stage of the process, and switched to non-TF\napproach to deep learning.\n\nOn Monday, 19 September 2016, Stephen Grinich notifications@github.com\nwrote:\n\n> I am trying to set up TensorFlow and am receiving this same error both\n> with a standard pip install and also using virtual env. Has any progress\n> been made @vrv https://github.com/vrv @mrob27\n> https://github.com/mrob27 @ebrevdo https://github.com/ebrevdo ? Any\n> tips much appreciated to get started with the appealing library.\n\n## \n\n  Robert Munafo  --  mrob.com\n  Follow me at: gplus.to/mrob - fb.com/mrob27 - twitter.com/mrob_27 -\nmrob27.wordpress.com - youtube.com/user/mrob143 - rilybot.blogspot.com\n", "I am closing this bug due to lack of recent activity. For future searchwes, the original poster posted that they were using Mac OS 10.6. which was subsumed by Mac OS Lion in 2011. libc++ was not included in Mac OS Snow Leopward because Apple had not switched to Clang and libc++ yet. Our binaries are for a single version of Mac OS (El Capitan currently). For the subsequent posters, please file a new bug with your relevant system details.\n"]}, {"number": 152, "title": "Segmentation fault when GPUs are already used", "body": "When I set the nvidia driver in exclusive mode and one of the GPU is already used by another process, I get a segmentation fault:\n\n```\n$ python -c \"import tensorflow as tf;tf.InteractiveSession()\"\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 12\nSegmentation fault (core dumped)\n```\n\nIf I limit the visible GPUs to only GPUs that have nothing running on them, it don't segfault:\n\n```\n$CUDA_VISIBLE_DEVICES=1 python -c \"import tensorflow as tf;tf.InteractiveSession()\"\nI tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 12\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:09:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 12105628263\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 12\n```\n\nHere is my output of nvidia-smi:\n\n```\n$ nvidia-smi \nWed Nov 11 16:48:27 2015       \n+------------------------------------------------------+                       \n| NVIDIA-SMI 352.39     Driver Version: 352.39         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 750     Off  | 0000:05:00.0      On |                  N/A |\n| N/A   48C    P8     0W /  38W |     25MiB /  2047MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX TIT...  Off  | 0000:06:00.0     Off |                  N/A |\n| 42%   82C    P2   127W / 250W |    262MiB / 12287MiB |     39%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\n| 22%   46C    P8    17W / 250W |     23MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX TIT...  Off  | 0000:0A:00.0     Off |                  N/A |\n| 22%   37C    P8    15W / 250W |    361MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1171    G   /usr/bin/X                                      17MiB |\n|    1     32740    C   python                                         209MiB |\n|    3      9429    C   python                                         336MiB |\n+-----------------------------------------------------------------------------+\n\n```\n\nI suppose that in the code that check the available GPUs, it don't handle correctly a case when one of the GPUs can't be used.\n", "comments": ["@nouiz, you are correct in your analysis. TensorFlow tries to grab all the GPU it sees from the system that passes its criteria. CUDA_VISIBLE_DEVICES is the solution I would suggest. Please let us know if that is not enough.\n", "@zheng-xq +1 on CUDA_VISIBLE_DEVICES; this fixes the issue for me.\n", "Re-open if CUDA_VISIBLE_DEVICES isn't good enough\n", "@vrv This is still a problem when there are multiple GPUs (in exclusive mode) on the machine and some of them have processes running on them. Setting CUDA_VISIBLE_DEVICES is not ideal if you do not actually know which GPU is available. nvidia-smi will only show the current state of the GPUs and there's no way of ensuring that while you spawn your process, no other process occupied the GPU you set for use. Is there a way of polling GPUs and assigning the process to the first available, compatible GPU on the machine instead of having to manually specify the GPU id?\n\nPS: I cannot reopen this issue.\n", "@zheng-xq: is it possible to query the status of the exclusive mode bit?\n", "@noisychannel, even if we can query which GPU is available, and by the time we actually try to assign the GPU, it could be taken by other processes. This is not very different from having a script query GPU status through nvidia-smi, and set the GPU id accordingly. \n\nThe only way I can think of that makes your case easier is actually to create the context, and if it fails, ignore that failure and move on to the next. However, this is undesirable for many more common cases, where such context creation errors are signs of bigger problems. \n", "The established way to do this seems to be: \n1. Set the GPUs to operate in exclusive mode.\n2. Do not call `cudaSetDevice()`.\n3. When you run your program, it will try and create the context on the first GPU and fail if the GPU is busy.  Assuming you're using CUDART, it should _silently_ try and create the context on the next GPUs and fail if all GPUs are busy. \n\nAs for exception handling while creating the context, it seems like the this scenario can be handled with the exceptions returned by `cuCtxCreate()`. \n\nA solution like this is optimal when you delegate the scheduling of your jobs to something like SGE and cannot trust the script based querying approach which I think is common for large clusters.  \n", "This would work fine if the framework only wants to use a single GPU. TensorFlow is designed to use multiple GPU seamlessly at the same time. So at the initialization stage, it grabs all the visible devices that are compatible, since the client program might use all of them. The actual device used is only known at the graph construction/execution time, which is at a much later stage.\n\nFor most of our existing users, the GPUs that are available to a particular job is known when it starts. So it is okay for TensorFlow to take all of them. A few options to think about: \n1. Is it possible for the job scheduler to reserve the GPUs in your clusters? That would work best with TensorFlow. \n2. If you only know the list of candidate GPUs, and the number of GPUs to use, we can add a special mode where we try to create context in the candidate list, and stops if desired number of GPUs are taken. This requires more plumbing to both TensorFlow and the underlying Stream-Executor that actually manages GPUs. We would much prefer #1 if that is possible for you. \n", "@zheng-xq For 1: AFAIK, the only responsibility of most job schedulers is to ensure that you will have access to the number of GPUs you have asked for on the machine which has been assigned to you. It leaves the actual CPU/GPU scheduling of the processes on that machine to the OS/Cuda driver. Also, hypothetically, if the scheduler was to attempt something of this sort, it would not be able to block the GPU without creating something like a context. \n\nFor 2 : I understand that this is probably painful. However, it does add a more general set of features. The case where I want `k` out of `n` GPUs on the machine is most prominent. Eg,, it would be able to handle the case where the candidate list is the set of all GPUs on the machine and I want only 2 available and compatible GPUs from that list. Having the ability to specify how many GPUs you want to use will be a great addition and this will tie in perfectly with most job schedulers. So to tensorflow, I specify, I want to run this thing on 2 GPUs and I tell the job scheduler that I need 2 GPUs. \n", "Drive-by comment: the sysadmin for my local SGE cluster was able to have an environment variable set that tells me which GPUs my jobs have been assigned. I then use this variable to set CUDA_VISIBLE_DEVICES. I would guess that most common schedulers have similar capabilities, although they may require root-level tweaking.\n", "This is useful to know. However, something like this would require all jobs on the cluster to respect that environment variable when using GPUs. And it takes away some of the scheduling responsibility that should ideally belong to the CUDA driver, adding another layer of complexity. In fact, most other toolkits that are GPU capable will not require something like this and as long as the GPUs are set in exclusive mode, they work just fine. \n", "Another drive-by comment: when the allocation fails, the app actually segfaults on our machines. That does not provide any possibility for the user to recover from it at the python level. \nA good-enough solution would be\na) throw an exception so that user can it catch and try (for example) another device\nb) ideally, adding /gpu:auto which would just iterate through the devices and allocate the one that is available.\n", "@noisychannel, please see if you can work around this issue at the moment. Meanwhile, I will investigate if we can officially support the soft-try-and-device-limit approach in TensorFlow. \n", "Okay. Thanks. Keep me posted. \n", "@zheng-xq: Is this still an issue?\n", "I'm not aware of a \"soft-try-and-assign-device\" implementation in Tensorflow. \n", "Closing this automatically due to lack of recent activity. Please reopen when new information becomes available. @zheng-xq, please re-open if you plan to be still working on this.\n", "@aselle I'm on a shared multi GPU machine and this doesn't appear to have been fixed.\n", "@AndreasMadsen, what is your exact hardware/software configuration and steps to reproduce (see the new issue template).\n", "The steps are the same @nouiz describe. `python -c \"import tensorflow as tf;tf.InteractiveSession()\"` fails with a Segmentation fault. Adding `CUDA_VISIBLE_DEVICES=1` fixes it.\n\nI have a hunch that it might be related to mixing GPUs with diffrent CUDA capabilities. But I have no way to test that.\n\n```\nSat Oct 22 16:07:37 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 352.39     Driver Version: 352.39         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K40c          On   | 0000:02:00.0     Off |                    0 |\n| 23%   23C    P8    18W / 235W |     22MiB / 11519MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K20c          On   | 0000:03:00.0     Off |                    0 |\n| 30%   20C    P8    16W / 225W |     13MiB /  4799MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K20c          On   | 0000:83:00.0     Off |                    0 |\n| 30%   22C    P8    16W / 225W |     13MiB /  4799MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX TIT...  On   | 0000:84:00.0     Off |                  N/A |\n| 22%   31C    P8    29W / 250W |    161MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n\n```\n", "A backtrace can be useful here. Either run under pdb directly, or examine\ncore file under pdb (ie, set ulimit -c unlimited, get core file, open core\nfile with pdb and using pywraptensorflow.so as binary)\n\nOn Sat, Oct 22, 2016 at 7:10 AM, Andreas Madsen notifications@github.com\nwrote:\n\n> The steps are the same @nouiz https://github.com/nouiz describe. python\n> -c \"import tensorflow as tf;tf.InteractiveSession()\" fails with a\n> Segmentation fault. Adding CUDA_VISIBLE_DEVICES=1 fixes it.\n> \n> I have a hunch that it might be related to mixing GPUs with diffrent CUDA\n> capabilities. But I have no way to test that.\n> \n> Sat Oct 22 16:07:37 2016\n> +------------------------------------------------------+\n> | NVIDIA-SMI 352.39     Driver Version: 352.39         |\n> |-------------------------------+----------------------+----------------------+\n> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n> |===============================+======================+======================|\n> |   0  Tesla K40c          On   | 0000:02:00.0     Off |                    0 |\n> | 23%   23C    P8    18W / 235W |     22MiB / 11519MiB |      0%   E. Process |\n> +-------------------------------+----------------------+----------------------+\n> |   1  Tesla K20c          On   | 0000:03:00.0     Off |                    0 |\n> | 30%   20C    P8    16W / 225W |     13MiB /  4799MiB |      0%   E. Process |\n> +-------------------------------+----------------------+----------------------+\n> |   2  Tesla K20c          On   | 0000:83:00.0     Off |                    0 |\n> | 30%   22C    P8    16W / 225W |     13MiB /  4799MiB |      0%   E. Process |\n> +-------------------------------+----------------------+----------------------+\n> |   3  GeForce GTX TIT...  On   | 0000:84:00.0     Off |                  N/A |\n> | 22%   31C    P8    29W / 250W |    161MiB / 12287MiB |      0%   E. Process |\n> +-------------------------------+----------------------+----------------------+\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/152#issuecomment-255530572,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHIWJPL03E-PFT3I-1py8mtlPhJ9qks5q2hlZgaJpZM4GglfQ\n> .\n", "Hi,\r\nI am still observing problems w/ GPU reservation in a 16 GPU node. For some GPUs, when reserving the GPU with CUDA_VISIBLE_DEVICES, it fails to reserver the node. In TF 0.10 it generated a coredump. In TF 0.12 it complains about not being able to start the tf.Session.\r\n\r\nThe node status is this:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0     46690    C   /usr/bin/python2.7                            2326MiB |\r\n|    2     13781    C   python                                        3456MiB |\r\n|    3     26767    C   python                                         282MiB |\r\n|    4     53525    C   /usr/bin/python2.7                             740MiB |\r\n|    5     13782    C   python                                        3456MiB |\r\n|    6      9427    C   /usr/bin/python2.7                            3455MiB |\r\n|    9     53527    C   /usr/bin/python2.7                             740MiB |\r\n|   10     53222    C   /usr/bin/python2.7                             740MiB |\r\n|   11     53526    C   /usr/bin/python2.7                             740MiB |\r\n|   12     54930    C   /usr/bin/python2.7                            1085MiB |\r\n|   13     54926    C   /usr/bin/python2.7                             785MiB |\r\n|   14     54927    C   /usr/bin/python2.7                             785MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nAnd when I use CUDA_VISIBLE_DEVICES=1 , which is free:\r\n```\r\nBefore reservation the free GPUs  [1, 7, 8, 15]\r\nSetting CUDA_VISIBLE_DEVICES=1\r\nE tensorflow/core/common_runtime/direct_session.cc:135] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE\r\nTraceback (most recent call last):\r\n...\r\n File \"PATH/run.py\", line 138, in reserveGPUs\r\n    with  tf.Session() as session:\r\n  File \"PATH/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1186, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"PATH/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 551, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"PATH/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/amr/tools/tensorflow/0.12-cuda7.5/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.", "Actually, this also fails in the above example:\r\n```\r\n CUDA_VISIBLE_DEVICES=1 python2.7 -c \"import tensorflow as tf;tf.InteractiveSession()\"\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.7.5 locally\r\nE tensorflow/core/common_runtime/direct_session.cc:135] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE\r\n...\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n```\r\nbut\r\n```\r\nCUDA_VISIBLE_DEVICES=8 python2.7 -c \"import tensorflow as tf;tf.InteractiveSession()\"\r\n```\r\nand\r\n```\r\nCUDA_VISIBLE_DEVICES=15 python2.7 -c \"import tensorflow as tf;tf.InteractiveSession()\"\r\n```\r\nwork.\r\n\r\nNote that the same happens with the new ConfigProto options: https://github.com/tensorflow/tensorflow/issues/1888\r\n```\r\npython2.7 -c \"import tensorflow as tf; config_proto = tf.ConfigProto(); config_proto.gpu_options.visible_device_list=\\\"1\\\"; tf.Session(config=config_proto)\"\r\n```\r\ndoesn't work but\r\n```\r\npython2.7 -c \"import tensorflow as tf; config_proto = tf.ConfigProto(); config_proto.gpu_options.visible_device_list=\\\"8\\\"; tf.Session(config=config_proto)\"\r\n``` \r\nworks.", "One problem that I ran into with using both CUDA_VISIBLE_DEVICES and gpu_options.visible_device_list is that GPU numbering can be different between Nvidia NVML (and nvidia-smi for example) and what you get from cuda if you do cuDeviceGet().  On some systems, NVML and nvidia-smi apparently order the card numbers by sorting the PCI id.  <b>Cuda doesn't necessarily use the same order.</b>  This can happen on a machine where all the GPU cards are identical.\r\n\r\nThe result is a crash when CUDA_VISIBILE_DEVICES is used to select a GPU.  With gpu_options.visible_device_list you get a failure opening the session for a mismatched GPU that isn't actually available.\r\n\r\nHere's the mapping I get on a particular system at work:\r\nNVML id 0 maps to Cuda index 4\r\nNVML id 1 maps to Cuda index 5\r\nNVML id 2 maps to Cuda index 6\r\nNVML id 3 maps to Cuda index 7\r\nNVML id 4 maps to Cuda index 0\r\nNVML id 5 maps to Cuda index 1\r\nNVML id 6 maps to Cuda index 2\r\nNVML id 7 maps to Cuda index 3\r\nNVML id 8 maps to Cuda index 8\r\nNVML id 9 maps to Cuda index 9\r\nNVML id 10 maps to Cuda index 10\r\nNVML id 11 maps to Cuda index 11\r\nNVML id 12 maps to Cuda index 12\r\nNVML id 13 maps to Cuda index 13\r\nNVML id 14 maps to Cuda index 14\r\nNVML id 15 maps to Cuda index 15\r\n\r\nHere is the python that generated this:\r\n\r\n```\r\n#!/usr/bin/env python\r\n\r\nimport pynvml\r\nimport os\r\nimport ctypes\r\n\r\ncuda = ctypes.CDLL('libcuda.so')\r\ncuda_device = ctypes.c_int32(0)\r\ncuda_pci_bus = ctypes.c_int32(0)\r\ncuda_pci_slot = ctypes.c_int32(0)\r\n\r\nif cuda.cuInit(ctypes.c_int32(0)) != 0:\r\n    raise Exception(\"Cuda failure\")\r\ntotal_gpus = ctypes.c_int32(0)\r\nif cuda.cuDeviceGetCount(ctypes.byref(total_gpus)) != 0:\r\n    raise Exception(\"Cuda failure\")\r\n\r\npci_map = {}\r\nfor gpu in range(total_gpus.value):\r\n    if cuda.cuDeviceGet(ctypes.byref(cuda_device), ctypes.c_int32(gpu)) != 0:\r\n        raise Exception(\"Cuda failure\")\r\n    if cuda.cuDeviceGetAttribute(ctypes.byref(cuda_pci_bus), ctypes.c_int32(33), cuda_device) != 0:\r\n        raise Exception(\"Cuda failure\")\r\n    if cuda.cuDeviceGetAttribute(ctypes.byref(cuda_pci_slot), ctypes.c_int32(34), cuda_device) !=0:\r\n        raise Exception(\"Cuda failure\")\r\n    # The below is correct on my system but...\r\n    pci_string = \"0000:%02X:%02X.0\" % (cuda_pci_bus.value, cuda_pci_slot.value)\r\n    pci_map[pci_string] = gpu\r\n\r\npynvml.nvmlInit()\r\nassert pynvml.nvmlDeviceGetCount() == total_gpus.value\r\n\r\nfor gpu in xrange(total_gpus.value):\r\n    h = pynvml.nvmlDeviceGetHandleByIndex(gpu)\r\n    bus_id = pynvml.nvmlDeviceGetPciInfo(h).busId\r\n    print \"NVML id {} maps to Cuda index {}\".format(gpu, pci_map[bus_id])\r\n\r\n```\r\n\r\nOn systems like this, if nvidia-smi says a particular GPU id <i>N</i> is free, you should reserve it e.g. using gpu_options.visible_device_list using the corresponding Cuda index <i>f(N)</i> which may be different.\r\n\r\nWhether this is a bug in the way Tensorflow implements gpu_options.visible_device_list or not is debatable.\r\n", "@cbquillen do I understand it correctly that the problem arises only when you use `CUDA_VISIBLE_DEVICES` mechanism to select the GPU device used by TensorFlow, and some other mechanism (which?) to select GPU device used by another program?", "Indeed, visible_device_list deals with what is provided at runtime via the cuGetDevice API -- how external drivers and processes remap the physical ID to the 'visible id' is out of TensorFlow's control, and it sounds like you agree, that visible_device_list must use f(N), not N, since f is not something TF controls.\r\n\r\nI believe our documentation is pretty clear on this point: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L48 . -- let us know if there's any further documentation that might help?\r\n", "@yaroslavvb \r\n\r\nI am running into this in the case where exclusive GPU access is turned on and there are other processes using some GPUs on the machine.  If I find a free GPU subset using nvidia-smi (or the NVML library) and try to limit Tensorflow to using this subset via CUDA_VISIBLE_DEVICES or gpu_options.visible_device_list in the Tensorflow config_proto, the result is a request for busy GPUs unless I use the mapping derived by looking at PCI-ids.   ", "Hm, that's kind of annoying actually, I've also been parsing `nvidia-smi` to generate `export CUDA_VISIBLE_DEVICES=` command on shared computers (along the lines of [setup_one_gpu](https://gist.github.com/yaroslavvb/3de518e0912e21a150c55c0eb5cfadeb)). Haven't run into problems yet, but it would be nice to have some solution that works reliably.", "@vrv \r\n\r\nThe problem is that users will see the ordering provided by nvidia-smi (sorted by PCI-Id) and try to use it, not realizing it actually has nothing to do with what CUDA_VISIBLE_DEVICES actually selects.  Now a lot of people think this is an NVidia bug, and it's possible that they agree and fix it in newer drivers.  However it might be worth adding a prominent warning in the documentation to the effect that this problem exists.  Otherwise you will continue to get queries about it.\r\n", "I agree that this is a problem.  I'm not sure if NVidia would prefer people to not use the cuGetDevice API in favor of whatever nvidia-smi is using.  Perhaps @benbarsdell or @cliffwoolley might know?\r\n\r\nIn lieu of that, would you like to add the warning to the documentation somewhere?  You probably know best where others like you would go looking for it :)\r\n\r\n", "@vrv\r\nA short warning in tensorflow.Session about using CUDA_VISIBLE_DEVICES with a link would probably help.  The visible_device_list doc config.proto would be a place where the a complete  problem is described and where you would link to.\r\n\r\nstracing nvidia-smi reveals that it loads libnvidia-ml.so.  So it is using NVML and so could Tensorflow.\r\n", "I was thinking documentation rather than source code might be better: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/using_gpu/index.md#using-gpus as a possible area to write a short section on it?\r\n\r\ncc @zheng-xq: I know he had reservations about adding visible_device_list for this exact reason of confusion -- perhaps he can weigh in if he wants.", "This may be helpful to some users who are on a cluster. Often, you don't have information about which GPU has been allocated to you or which one to occupy if free, with CUDA_VISIBLE_DEVICES. \r\n\r\nI use this script to (https://gist.github.com/noisychannel/cdf57e2f177e98ae653230323a093d1e) to return the first free GPU and then use something like : \r\n```\r\nCUDA_VISIBLE_DEVICES=`scripts/free-gpu` python \r\n```\r\n\r\nEdit : This is the parsed nvidia-smi output that @yaroslavvb mentioned earlier in this thread. ", "@yaroslavvb\r\nLooking at the NVIDIA [docs](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars) more, I think you can get your setup_one_gpu script to work reliably if you set the CUDA_DEVICE_ORDER environment variable to 'PCI_BUS_ID'.\r\n\r\n@zheng-xq A cheezy solution that would probably reduce the likelihood of complaints (but create some less-likely new problems) would be to force set CUDA_DEVICE_ORDER='PCI_BUS_ID' within Tensorflow.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "If someone is interested in adding CUDA_DEVICE_ORDER='PCI_BUS_ID' support in TensorFlow, a contribution is welcome. But note that we can only override it if it is not already set. And it has to been before any potential Cuda calls are made. ", "I don't think I've actually seen an application that sets CUDA_DEVICE_ORDER on its own.  Usually this would be set in the environment before launching the application, perhaps in combination with CUDA_VISIBLE_DEVICES.", "You can certainly set environment variables within an application before the first call to tf.Session().", "Of course it's _possible_.  Just as in C you'd use setenv().  I was just saying that for those particular environment variables, the choice is a bit specific to the combination of application and machine the application is run on.  So usually the user (or script) invoking the application would set those env vars rather than having the application set them unilaterally.  I'd argue for example that it's unwise for TF itself to set CUDA_DEVICE_ORDER, because it would undo the CUDA feature that this variable sits on top of, which is to, by default, have the \"best\" device be device 0; this is handy when you don't know much about the system you're running on and are just trying things out.  More sophisticated users just pick a GPU or, if really fancy, pick one based on NVML output as we're discussing here.  But we shouldn't, IMO, assume all users are so sophisticated.", "`export CUDA_VISIBLE_DEVICES=0 ` solves my issue!", "Closing as this is resolved, free to reopen if problem persists.", "Python 3.6.9\r\nTensorflow 2.1.0 gpu supported \r\nCuda 10.0\r\nTensorRT 6.0\r\nGPU - 1\r\nIs still get the error even after `export CUDA_VISIBLE_DEVICES=0`  or `export CUDA_VISIBLE_DEVICES=1` \r\n\r\n**Error log :**\r\n```\r\nCan't identify the cuda device. Running on device 0\r\nSegmentation fault (core dumped)\r\n\r\n```\r\n"]}, {"number": 151, "title": "Typo in reshape documentation", "body": "Small typo at \nhttp://tensorflow.org/api_docs/python/array_ops.md#AUTOGENERATED-shapes-and-shaping \nUnder the reshape example it says: \n\n> tensor 't' is [[[1, 1], [2, 2]]\n>                [[3, 3], [4, 4]]]\n> tensor 't' has shape [2, 2]\n\nBut I think t initially has the shape [2, 2, 2] not [2, 2].\n", "comments": []}, {"number": 150, "title": "Is there 3D ConvNets support ? ", "body": "(Is there) **or** (will there be) a **3D ConvNets** support in **tensorflow** ?  \nWhich helps for spatiotemporal features. \n", "comments": ["We don't have a conv3d op yet, but there will eventually be!   We'll use this issue to track this feature request.\n", ":+1:\n", "Theano offers serveral 3D Convolutional Implementations on CUDA. https://github.com/Theano/Theano/blob/master/theano/sandbox/cuda/GpuConv3D.py\nhttps://github.com/Theano/Theano/blob/master/theano/sandbox/cuda/GpuConvGrad3D.py\nhttps://github.com/Theano/Theano/blob/master/theano/sandbox/cuda/fftconv.py \nI would really appreciate, if you guys can port this stuff to tensorflow.\n", "Any word on this? Would be very useful capability. \n", ":+1:\n", "+1 on this request.\n", "Please, support 3D convnets. Thanks for help.\n", "Ops are arriving with https://github.com/tensorflow/tensorflow/pull/1571/commits/784778e76d3f5fd75b9ec99138770795907f1616\n", "Great!\n\nI guess a problem with 3 (or higher) D convnets will be the cost of computations. Do you think it would be possible / useful (or maybe it is already possible in your ops?) to implement a tetrahedral instead of square grid convnet as suggested in this article? Interesting also for later generalization to higher dimension: if I understand well, this approach (generalized if necessary to higher dimension) will make the number of nodes in the kernel linear instead of exponential in the dimension of the initial tensor.\n\nhttp://arxiv.org/pdf/1505.02890v2.pdf\n", "Can the people at the origin of the present enhancement look at the following one:\n\nhttps://github.com/tensorflow/tensorflow/issues/1661\n\nThe generalization asked may (?) be easier to implement by the same people who have already done it in 3D...\n", "Is it just the missing conv3d operation? Or do _all_ operations for ConvNets need to be modified? Because when looking through the API documentation, the input tensor almost always seems to be fixed at 4D with [batch, height, width, channels].\nWhat I can spot now but is probably incomplete:\n- avg_pool\n- max_pool\n- moments\n", "https://github.com/tensorflow/tensorflow/commit/6a187ccddaebb741ea77fc3201c6e36625f0aadb added 3d convolution support, including the pooling ops.\n", "is there also a 3d \"Deconvolution\" (conv3d_transpose) available?\nThanks!\n", "@rogertrullo `conv3d_backprop_input` is available e.g. #2467\n", "@rogertrullo if you manage to get conv3d_backprop_input to work as a deconvolution, would you publish some example code?\n", "@el3ment sure!, @daeyun I am wondering though, if I define the operation like in #2467 , if I use the function as a layer, will the backward pass do something? (since we are actually using the backward pass of the convolution as forwards pass for the \"deconvolution\" )\n", "@rogertrullo I misunderstood, although for 2D, conv2d_transpose is actually `conv2d_backprop_input`, it looks like its 3d counterpart is incomplete and the backward pass of conv3d_backprop_input is not implemented. I haven't gotten around it either. (Edited)\n", "According to the doc (r0.9 and master), `max_pool3d` doesn't support pooling along the depth axis (`ksize[1] = 1`). Is there a way around this? Is this a missing feature?\n"]}, {"number": 149, "title": "Official Tensorflow Docker Image", "body": "Hello Tensorflow Community,\n\nI just wanted to kick start a discussions on creating an official docker image for Tensorflow. So going in line with creating common framework for machine learning related researchers and developers to rally around, and given the onslaught on software containers, I think creating a common Tensorflow image would also help in the same regard. \n\nI'd normally make a more detailed proposal as I did for a cuda docker image [here](https://devtalk.nvidia.com/default/topic/858201/official-docker-image-for-cuda-/?), but my goal right now is just to facilitate a discussion. I know there are many technical issues thanks in part to heavy use of GPUs and driver dependencies, but it looks like Nvidia is making some progress on that front: [NVIDIA/nvidia-docker](https://github.com/NVIDIA/nvidia-docker).\n\nSo if you like the idea or have some ideas/drafts, please chime in. \n", "comments": ["Hi @ruffsl -- just to ask, by \"official\" do you mean having an org on Docker hub?\n\nIf so, it's definitely something we want to do; in fact, I registered the Tensorflow org on docker hub -- I just didn't know how to experiment with moving over pre-release without accidentally making the org public. :wink: \n\nIf you've done this before, I'd love help/pointers.\n", "Hey @craigcitro, Yes, I think making a official Docker Hub image would be a great goal!\n\nIn my haste, I didn't notice the [tensorflow/tools/docker](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker) folder already in the repo. This great progress, I'll have better sift through what already exists, as I'm just starting with this project my self.\n\nI've created and submitted official Docker Hub images before, this including one for [ROS](https://hub.docker.com/_/ros/) and [Gazebo](https://hub.docker.com/_/gazebo/) projects, and if you'd like any assistance, I'd love to help. If you want to PM me, I could elaborate on the processes and my experience in greater detail.\n", "Relevant issue on starting cuda image that GPU enabled tag of tensorflow could build from:\nhttps://github.com/NVIDIA/nvidia-docker/issues/7\n", "More than having a docker image, I'd add, would be having it [ready for development](http://stackoverflow.com/questions/33702064/tensorflow-docker-dev-workflow-on-mac) and, eventually, even promoting it as the default way to go #203 ! :)\n\nAs a newcomer to docker myself, I'm struggling quite a lot to get off the ground here, even after studying it and better understanding it quite well. It isn't as simple or easy as it may seem at first - but it could and should be!\n", "@craigcitro , could there be a separate repo (tensorflow-docker?) under the tensorflow github org for just the Dockerfiles? Many [other](https://github.com/nodejs/docker-node) [projects](https://github.com/docker-library/golang) keep theses files separate to make attaching web hooks, CI and other things simpler. Its a common practice among most official images. Then we'd have a place to build up PRs, and link back to during the submission process. \n\nAlso what are the origins of the `b.gcr.io/tensorflow/tensorflow` base image?\nI'm not clear on how it was built:\n\n``` shell\n:~$ docker history b.gcr.io/tensorflow/tensorflow:latest \nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n217daf2537d2        7 days ago          /bin/sh -c #(nop) CMD [\"/bin/bash\"]             0 B                 \nda34eb7f1273        7 days ago          /bin/sh -c #(nop) EXPOSE 8888/tcp               0 B                 \ne9bc6354df37        7 days ago          /bin/sh -c #(nop) EXPOSE 6006/tcp               0 B                 \n55b545f9baa4        7 days ago          /bin/sh -c #(nop) COPY file:a7af486c3e6a1a7cf   35 B                \n957340752397        7 days ago          /bin/sh -c #(nop) COPY file:617470c4514ec5022   137 B               \nc906b2184874        8 days ago          /bin/sh -c pip --no-cache-dir install ipykern   3.413 kB            \nf67a15164dd5        8 days ago          /bin/sh -c curl -O https://bootstrap.pypa.io/   7.079 MB            \nbcb5994d8a18        8 days ago          /bin/sh -c pip install https://storage.google   60.33 MB            \n934fbda38a19        8 days ago          /bin/sh -c pip install         jupyter          125.2 MB            \n51666ff792cc        8 days ago          /bin/sh -c apt-get update && apt-get install    272.1 MB            \nfc3b69d5428a        8 days ago          /bin/sh -c #(nop) MAINTAINER Craig Citro <cra   0 B                 \n1d073211c498        3 weeks ago         /bin/sh -c #(nop) CMD [\"/bin/bash\"]             0 B                 \n5a4526e952f0        3 weeks ago         /bin/sh -c sed -i 's/^#\\s*\\(deb.*universe\\)$/   1.895 kB            \n99fcaefe76ef        3 weeks ago         /bin/sh -c echo '#!/bin/sh' > /usr/sbin/polic   194.5 kB            \nc63fb41c2213        3 weeks ago         /bin/sh -c #(nop) ADD file:e97fe9bddafcfac4ca   187.7 MB \n```\n", "- instructions for building the docker containers are in [the README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/README.md).\n- we can move the dockerfiles into their own repo if/when it's a problem -- right now, we're still making sure we've got syncing and external contribution flows in order, so no need to add extra complexity yet.\n- sadly, the CUDA image still doesn't have all the libraries we need (cudnn isn't there).\n", "Ok, so once a cudnn supported tag image comes around (see https://github.com/NVIDIA/nvidia-docker/issues/10), how do you envision the build image dependency structured? For some given major release of tensorflow, the tags would follow something roughly like this:\n\n| Tag | From |\n| --- | --- |\n| tensorflow:`1.0`/`latest` | ubuntu:`14.04` |\n| tensorflow:`1.0-gpu` | cuda:`7.0-cudnn` |\n\ni.e. what tags are you expecting to host, and where would each build from?\n", "@ruffsl whoa, that cudnn issue being resolved is fantastic! thanks for doing the legwork there.\n\ncurrently, i was thinking we'd do three collections of containers:\n\n| Name | From | Contents |\n| --- | --- | --- |\n| tensorflow | ubuntu | base TF install, no deps (bazel, java, etc) |\n| tensorflow-full | tensorflow | full source install, with all deps |\n| tensorflow-full-gpu | tensorflow-full | same as tensorflow-full, but built with GPU support |\n\nit's actually possible that we'd tweak the from lines, depending on how we build the containers (eg script vs. `Dockerfile`), but the net effect would be the same.\n\nthat said, I'm now curious: is multiple collections of tags the more common approach in the docker world?  in particular, would we also maintain `latest`, `latest-full`, and `latest-gpu`? (i feel like `latest` gets special treatment from docker, but `latest-gpu` and `latest-full` wouldn't, right?)\n", "Hmm, normally its a [best practice](https://docs.docker.com/v1.8/articles/dockerfile_best-practices/) to keep a nice linear sequential hierarchy in the tag structure to leverage the storage savings and reduce duplication of binaries by reusing image layers on disk. But from what I'm seeing currently, building the project over itself (once for CPU, then for GPU) is leading to some large image sizes:\n\n``` console\n$ docker images\nREPOSITORY                                TAG                    IMAGE ID            CREATED             VIRTUAL SIZE\ntensorflow                                gpu                    1ca458346ab2        16 hours ago        4.847 GB\ntensorflow                                cpu                    f092a7f6f122        16 hours ago        3.967 GB\ntensorflow                                latest                 6711b8288898        16 hours ago        2.406 GB\ncudnn                                     v2-7.0                 80a8e0efea8d        21 hours ago        2.041 GB\ncuda                                      7.0                    f8abb195d52b        21 hours ago        2.012 GB\nubuntu                                    14.04                  e9ae3c220b23        9 days ago          187.9 MB\n```\n\n(The above isn't really optimized yet, its just built in cascaded order with nothing flattened like in the README.md. Also, perhaps the cuda image could [shed some weight](https://github.com/NVIDIA/nvidia-docker/issues/12), I suspect it might be rolling in a lot of odd packages)\n\nBy forking the `tensorflow:full` and `tensorflow:full-gpu` tags we could keep the disk size of each individual tag reasonable at the cost of making the sum of all tags larger. However, my expectation is that users will just deploy with one or the other so bandwidth in moving images would be the limitation. Whereas if someone has both tags locally, ether for testing or experimentation, I don't think storage would be their primary concern. The same could be done I suppose for the `latest` image, so the trade off is redundant looking dockerfiles and layers for smaller individual tags. \n\n| Name | From |\n| --- | --- |\n| tensorflow:`latest` | ubuntu:`14.04` |\n| tensorflow:`full` | tensorflow:`latest` (OR ubuntu:`14.04`) |\n| tensorflow:`full-gpu` | cuda:`7.0-cudnn` |\n\nWe could play some tricks and keep the heads of each dockerfile similar as long as we could to [preserve the build catch](http://thenewstack.io/understanding-the-docker-cache-for-faster-builds/). I'm not sure how much the Docker Hub's build processes respects this though if an entire repo is churned with independent tags.\n\nTo get to your last question: Yes, `latest` holds a special [default](https://docs.docker.com/v1.8/reference/builder/#from) meaning. I haven't seen the use of `:latest-foo` anywhere, as the user would have to spell it out fully to use it anyway, so for brevity most just do `:foo` or `:<release>-foo`. By the way, what release is tensorflow at? Is this all still version < 1.0?\n", "I've been tinkering with variants of the Dockerfiles for each tag, and with my latest modifications I'm getting virtual image sizes shown bellow given the `FROM` structure I tabled above:\n\n``` console\n$ docker images \nREPOSITORY           TAG                   IMAGE ID            CREATED             VIRTUAL SIZE\ntensorflow           full-gpu              73ec812422ad        21 minutes ago      3.444 GB\ntensorflow           full                  3c90422c6a96        About an hour ago   2.125 GB\ntensorflow           latest                098cb21442b1        3 days ago          1.606 GB\n```\n\nGiven the dependencies for a source build, I don't immediately see much more I could cut. Are there any plans for shipping a binary, built against a release of cuda? Then we could swap for the lighter [`cudnn-runtime`](https://github.com/NVIDIA/nvidia-docker#cudnn) tag.\n", "Sorry for the lull here -- was waiting for the NVidia images to get pushed publicly, and now they're live! Adding in @ruffsl @ebrevdo @jendap for discussion as well.\n\nI now see what you mean about multiple tags in the repo, which I think is what we want to go with. For better or worse, I think we want to go with four tags:\n\n| Tag | From | Contents |\n| --- | --- | --- |\n| `latest`/`<release>` | `ubuntu:14.04` | minimal container with TF, no GPU support |\n| `latest-gpu`/`<release>-gpu` | `cuda:cudnn-runtime` | minimal container with TF and GPU support |\n| `devel`/`<release>-devel` | `ubuntu:14.04` | full image with all TF deps + source code, no GPU |\n| `devel-gpu`/`<release>-devel-gpu` | `cuda:cudnn-devel` | full image with all TF deps + source + GPU support |\n- We'll update these as new versions of TF get released, so that over time we'll end up with a number of older tags.\n- My main motivation for the two sets of images is just size: I think the `latest` image can be slimmed down a bit, so it'll end up ~1/2 the size of the image with GPU support. For the \"I just want to kick the tires\" use case, this seems valuable.\n- The naming scheme is also similar to the one for the CUDA images, except that we elide the `runtime` suffix.\n\nThoughts?\n", "If you still have strong requirements on the CUDA and cuDNN versions, you should make your gpu image use the full version name like `cuda:7.0-cudnn2-runtime`. The tag `cuda:cudnn-runtime` is currently cuDNN v2 but will be bumped to cuDNN v4 once it's available (soon!).\n", "I concur with @flx42 , specifically for official images, as you'd most likely want everything nailed down so it doesn't shift underneath you unless explicitly noted through updating the Dockerfiles. A compromise of specificity is why we normally see `FROM ubuntu:14.04` and not just `ubuntu` nor `ubuntu:14.04.3`, as updates in minor distros releases are normally quite stable routine security patches. \n\nFor cudnn, we should use the full name `cuda:7.0-cudnn2-runtime` and `cuda:7.0-cudnn2-devel` to lock down the version of cuda, as even cuda 7.5 support still seems to be an open issue [https://github.com/tensorflow/tensorflow/issues/20].\n", "OK -- I was waiting for the new release, which is imminent, so new images are pushed.\n\nWe now have `b.gcr.io/tensorflow/tensorflow:<tag>`, for `tag` all four variations mentioned above, as `latest` and `0.6.0` versions. I'm also busy uploading new containers to dockerhub.\n\nI think we can close this for now, with a caveat that we need to come back in ~2 months and figure out if there are enough downloads that we want to submit a PR to become an official repo?\n", "Hmm, I suppose we could also come back to this once https://github.com/NVIDIA/nvidia-docker/issues/7 is finished, as this would be a prerequisite for official repo GPU support. I take it this the current docker hub repo: https://hub.docker.com/r/tensorflow/tensorflow\n", "@ruffsl yep, that's the one. \n\nso closing for now, but we can either reopen or create a new issue later.\n"]}, {"number": 148, "title": "Wrong link in the \"common problems\" docs", "body": "On the \"Getting started page\" in the following part:\n\n```\nCommon Problems\nGPU-related issues\n\nIf you encounter the following when trying to run a TensorFlow program:\n\nImportError: libcudart.so.7.0: cannot open shared object file: No such file or directory\n\nMake sure you followed the the GPU installation instructions.\n```\n\nThe last word \"instructions\" leads to http://www.tensorflow.org/#install_cuda, however the main page it leads to does not have an anchor \"install_cuda\", so it just ends up leading to the top of main page.\n\nPresumably, the correct link is http://www.tensorflow.org/get_started/os_setup.md#install_cuda\n", "comments": ["Thanks for the report -- our website generation is undergoing fixes / changes this week -- it is also quite behind our github documentation.  We'll close this bug once we've fixed these issues, thanks!\n"]}, {"number": 147, "title": "add multiple-machine support", "body": "Many projects require more resources than are available in a single machine. **Does the current opensource release of Tensorflow support processing that spans multiple-machines?**\n\nI ask because the main webpage (says single machine) seems to conflict[1] with the Tensorflow whitepaper pdf (says multiple machines).\n\nIf the current opensource tensorflow does not support multiple-machine execution but the closed-sourced version does. **Is there any plan to release an opensource version that supports multiple machines? Is there an approximate timeframe?**\n\n[1]\n- The [tensorflow.org main webpage](http://tensorflow.org/) says \"This open source release supports single machines\". Also the [Resources/FAQ says \"single computer\"](http://tensorflow.org/resources/faq.md).\n- But then when you [read the Tensorflow whitepaper](http://download.tensorflow.org/paper/whitepaper2015.pdf), the whitepaper indicates in many places that Tensorflow can run across multiple machines. See these quotes from the whitepaper:\n  - \"large-scale  distributed  systems  of  hundreds of machines\" and \"using\n    many  hundreds  of  machines\"\n  - \" extends  it with  support  for  an environment where the client, the master, and the workers can all be in different processes on different machines\"\n  - \"use remote communication mechanisms such as TCP or RDMA to move data across machine boundaries\"\n", "comments": ["The current release doesn't support multiple machines, but we're working on it. Please see #23 for more details. \n\nClosing as duplicate of #23.\n"]}, {"number": 146, "title": "Wrong multiplication in MNIST beginner tutorial", "body": "(Not sure if this is the right place to report a problem with [this tutorial](http://tensorflow.org/tutorials/mnist/beginners/index.md).)\n\nI believe that the explanation of matrix multiplication in [this image](http://api.tensorflow.org/system/image/body/1707/softmax-regression-scalarequation.png) is wrong.\n\nIt should be W_1,1 x_1 + W_1,2 x_2 + W_1,3 x_3... (i.e., x should increase per row, not per column).\n", "comments": ["This is fixed at HEAD, hasn't yet been pushed to the website.  Thanks for the report!\n\nDe-duping with https://github.com/tensorflow/tensorflow/issues/78\n"]}]