[{"number": 34423, "title": "Issue in building TFLITE static library for linux environment ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution \r\nLinux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): v1.15.0-rc-2\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: From source\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): gcc5.5\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nCannot compile static library for TFLITE inference engine using the provided Makefile. When run, generated_scheme.h will not be link with flatbuffers/flatbuffers.h and will flag variable not declared in this scope for everything \r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ngit checkout tags/v1.15.0-rc-2\r\ncd tensorflow\r\n./tensorflow/lite/tools/make/download_dependencies.sh \r\n ./tensorflow/lite/tools/make/build_lib.sh\r\n\r\n**Any other info / logs**\r\n\r\n", "comments": ["issue can be resolved by removing line 33 and 34 from the Makefile in ./tensorflow/lite/tools/make/Makefile\r\n`INCLUDES := \\\r\n-I. \\\r\n-I$(MAKEFILE_DIR)/../../../../../ \\\r\n-I$(MAKEFILE_DIR)/../../../../../../ \\\r\n-I$(MAKEFILE_DIR)/downloads/ \\\r\n-I$(MAKEFILE_DIR)/downloads/eigen \\\r\n-I$(MAKEFILE_DIR)/downloads/absl \\\r\n-I$(MAKEFILE_DIR)/downloads/gemmlowp \\\r\n-I$(MAKEFILE_DIR)/downloads/neon_2_sse \\\r\n-I$(MAKEFILE_DIR)/downloads/farmhash/src \\\r\n-I$(MAKEFILE_DIR)/downloads/flatbuffers/include \\\r\n-I$(OBJDIR)`\r\n\r\nTo \r\n`-I. \\\r\n-I$(MAKEFILE_DIR)/downloads/ \\\r\n-I$(MAKEFILE_DIR)/downloads/eigen \\\r\n-I$(MAKEFILE_DIR)/downloads/absl \\\r\n-I$(MAKEFILE_DIR)/downloads/gemmlowp \\\r\n-I$(MAKEFILE_DIR)/downloads/neon_2_sse \\\r\n-I$(MAKEFILE_DIR)/downloads/farmhash/src \\\r\n-I$(MAKEFILE_DIR)/downloads/flatbuffers/include \\\r\n-I$(OBJDIR)`", "So, the issue is resolved here right @SidGATOR ", "> So, the issue is resolved here right @SidGATOR\r\n\r\nYes it is. You can add the changes to the branch", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34423\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34423\">No</a>\n"]}, {"number": 34422, "title": "Fixes issue #34398, equation from BTRS paper", "body": "Fixed a equation of the TF implementation of BTRS [paper](https://epub.wu.ac.at/1242/1/document.pdf). Equation is available on page 7 of the paper", "comments": ["Can you also add a test case please?", "@mihaimaruseac Thank you for the prompt response. Really appreciate it! Can you please direct me to the documentation for test case in tensorflow? I can't find them here: https://github.com/tensorflow/tensorflow ", "Fixes https://github.com/tensorflow/tensorflow/issues/34398", "Thanks @mihaimaruseac . Hopefully, I'll get to work on some test cases for TF soon."]}, {"number": 34421, "title": "Expanded the description of tf.gfile.", "body": "", "comments": ["I'm going to go ahead and close this PR, because it seems to have stalled. please feel free to reopen!"]}, {"number": 34420, "title": "Equality testing instead of identity", "body": "Identity comparing here will result in errors when the input string is created in non trivial ways. for example:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\nstandardize = ''.join(['lower', '_and_strip_punctuation'])\r\nlayer = keras.layers.experimental.preprocessing.TextVectorization(standardize=standardize)\r\nlayer(tf.constant([['hello'], ['hello world']]))\r\n\r\n# ValueError: lower_and_strip_punctuation is not a supported standardization. TextVectorization supports the following options for `standardize`: None, 'lower_and_strip_punctuation', or a Callable.\r\n\r\nsplit = ''.join('whitespace')\r\nlayer = keras.layers.experimental.preprocessing.TextVectorization(split=split)\r\nlayer(tf.constant([['hello'], ['hello world']]))\r\n\r\n# ValueError: whitespace is not a supported splitting.TextVectorization supports the following options for `split`: None, 'whitespace', or a Callable.\r\n```\r\n\r\nThis was tested with \r\n\r\n```\r\n>>> tf.__version__\r\n'2.1.0-dev20191119'\r\n```\r\n\r\nAnd Python3", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34420) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34420) for more info**.\n\n<!-- ok -->", "Looks good to me, but can you also write a test please? The one example you mentioned at the start of the issue should be good.", "@mihaimaruseac just added the tests.", "```\r\nFAIL: Found 2 non-whitelisted pylint errors:\r\ntensorflow/python/keras/layers/preprocessing/text_vectorization_test.py:475: [C0326(bad-whitespace), ] Exactly one space required after comma\r\n\r\ntensorflow/python/keras/layers/preprocessing/text_vectorization_test.py:485: [C0326(bad-whitespace), ] Exactly one space required after comma\r\n```", "@mihaimaruseac Fixed the lining issues. I'm new to Python, running:\r\n\r\n```\r\npylint --rcfile=/tmp/pylintrc tensorflow/python/keras/layers/preprocessing/text_vectorization_test.py\r\n```\r\n still returns a lot of warnings but they don't seem related to this PR.", "@dfalbel Could you please check reviewer comments and keep us posted. Thanks!", "yes, I'll do it "]}, {"number": 34419, "title": "Switch to NDK API level 21", "body": "This resolves an issue where the arm64 build was incorrectly targeting API 28, leading to unexpected behavior on pre-API 28 devices.\r\n\r\nFixes #31114 \r\n\r\nPiperOrigin-RevId: 280766624\r\nChange-Id: I8500b69a5f6bebbeb0aafcf5744f5be5944738b9", "comments": []}, {"number": 34418, "title": "TensorFlow-gpu 2.0 - Forward features in estimators prediction output", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIn tensorflow-gpu 1.14, we were passing the ID key(s) as a forward feature so that it appears in prediction outputs\r\n\r\nestimator = tf.contrib.estimator.forward_features(\r\n        estimator,\r\n        keys=[\r\n            feature_forward_key.name\r\n            for feature_forward_key in metadata.FEATURE_FORWARD_KEYS\r\n        ])\r\n\r\nOn migrating the code to Tensorflow-gpu 2.0, tf.contrib package is removed and we couldn't find forward-features functionality in tensorflow core for an estimator.\r\n\r\n**Will this change the current api? How?** No\r\n\r\n**Who will benefit with this feature?** Everyone\r\n\r\n**Any Other info.**\r\n", "comments": ["Contrib.estimator has been deprecated for quite some time. If you still need this functionality, I would recommend forking it. The code as-is is a relatively short function, and you could likely clean it up even further if it only needs to support your use-case.", "I had the same issue. The \"bare-bones\" solution is quite easy:\r\n```python\r\ndef forward_features(estimator, key):\r\n    def new_model_fn(features, labels, mode, config):\r\n        spec = estimator.model_fn(features, labels, mode, config)\r\n        predictions = spec.predictions\r\n        predictions[key] = features[key]\r\n        spec = spec._replace(predictions=predictions)\r\n        return spec\r\n    return tf.estimator.Estimator(model_fn=new_model_fn, model_dir=estimator.model_dir, config=estimator.config)\r\n```"]}, {"number": 34417, "title": "Have the count mode passed to the callbacks", "body": "**System information**\r\n- TensorFlow version: 2.0.0\r\n- Are you willing to contribute it: Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRight now when training on a generator (I don't know about the other cases), when the callbacks are configured, the number of samples passed in might actually be the number of steps (i.e. divided by batch size). To cope with that for the progress bar, a `count_mode` variable is used to know how to update it.\r\n\r\nI would like the `count_mode` variable to also be passed to the callbacks in order for them to also adapt their behaviour based on it.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nThe [`keras-tqdm`](https://github.com/bstriner/keras-tqdm) library is currently [not working properly because of that lack](https://github.com/bstriner/keras-tqdm/issues/37). It could implement a new way to deal with the count, once this variable is provided in the callbacks parameters.\r\n", "comments": ["Hi @zaccharieramzi,\r\n\r\nI am the maintainer for [tqdm progress bar](https://github.com/tensorflow/addons/blob/master/docs/tutorials/tqdm_progress_bar.ipynb) in tensorflow addons, does this problem persists with `tfa`'s implementation of tqdm progress bar? Thanks!", "Hi @shun-lin ,\r\n\r\nThanks for the pointer. I have installed `tensorflow-addons` using `pip install tensorflow-addons`. However, when testing with the following code:\r\n```python\r\nimport tensorflow_addons as tfa\r\nprint(tfa.__version__)\r\ntfa.callbacks.TQDMProgressBar\r\n```\r\n\r\nI get the following output:\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-0c7c9299f697> in <module>\r\n      1 import tensorflow_addons as tfa\r\n      2 print(tfa.__version__)\r\n----> 3 tfa.callbacks.TQDMProgressBar\r\n\r\nAttributeError: module 'tensorflow_addons.callbacks' has no attribute 'TQDMProgressBar'\r\n```\r\n\r\nHave you had this problem before?\r\n\r\nAlso, I noticed in the source code that you d[idn't allow a custom metrics formatting](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/callbacks/tqdm_progress_bar.py#L199). [This is available in `keras-tqdm`](https://github.com/bstriner/keras-tqdm/blob/master/keras_tqdm/tqdm_callback.py#L13) and was quite handy for me, I can open an issue and a PR for it if you want.", "Hi!\r\n\r\nCan you pip install using this following command?\r\n\r\n`!pip install -q --no-deps tensorflow-addons~=0.6`\r\n\r\n(without the `!` if in terminal or with `!` if in notebook)\r\nSince I just added the tqdm progress bar callback recently, it will be available with regular `pip install tensorflow addons` in the next release, so now we need to add a bit more flag to as above to install. \r\nI did not include a custom metrics formatting because I first want to get a minimum working product out to tensorflow addons as their code style is more strict, but if it is useful for you and feel like it will be for others. Feel free to opens an issue in TF Addons and a pull request, thanks for the contribution.\r\n\r\nIf you still have problem using the pip install command line above, please let me know!\r\n\r\nEdit: If the command line above doesn't work then you would have to [install from source](https://github.com/tensorflow/addons#installing-from-source) as tqdm progress bar should be available on pip in the next release (0.7).", "@zaccharieramzi,\r\nThe code:\r\n\r\n```python\r\nimport tensorflow_addons as tfa\r\nprint(tfa.__version__)\r\ntfa.callbacks.TQDMProgressBar\r\n``` \r\nworks fine now. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/283bbb9237dd959f18922a956f26751e/gh_34417.ipynb) of the working code. Thanks!", "@rmothukuru yes sorry I should have closed the issue a long time ago.\r\nI am using it on a regular basis now without problems.\r\nI think we can close this."]}, {"number": 34416, "title": "Error trying to convert a model using full integer quantization", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.3:\r\n- TensorFlow installed from source: pip3 install --upgrade tensorflow==1.15\r\n- TensorFlow version: 1.15\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\r\n```\r\n\r\n**Model:**\r\n```\r\nimport pathlib\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  # tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\n\r\nmodel.evaluate(x_test,  y_test, verbose=2)\r\n\r\n\r\n# Save the model into SaveModel format\r\n\r\nsaved_model_dir = pathlib.Path(\"./saved_model/\")\r\ntf.saved_model.save(model, str(saved_model_dir))\r\n```\r\n\r\n**Convert the model:**\r\n```\r\nimport pathlib\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nmnist = tf.keras.datasets.mnist\r\nx_train = mnist.load_data()[0][0] / 255.0\r\n\r\nsaved_model_dir = pathlib.Path(\"./saved_model/\")\r\n\r\n\r\n# Convert the model from saved model\r\n\r\nimages = tf.cast(x_train, tf.float32)\r\nmnist_ds = tf.data.Dataset.from_tensor_slices(images).batch(1)\r\n\r\n\r\ndef representative_dataset_gen():\r\n    for input_value in mnist_ds.take(100):\r\n        yield [input_value]\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_dir))\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_dataset_gen\r\n\r\ntflite_quant_model = converter.convert()\r\n\r\ntflite_quant_model_file = saved_model_dir/\"mnist_post_quant_model_io.tflite\"\r\ntflite_quant_model_file.write_bytes(tflite_quant_model)\r\n```\r\n\r\n**Any other info / logs**\r\n\r\n```\r\n/usr/bin/python3.6 /home/mapeima/Code/python/edgeTPU/model_converter.py\r\n2019-11-19 13:34:02.791346: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-11-19 13:34:02.791364: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-11-19 13:34:02.791397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jihr): /proc/driver/nvidia/version does not exist\r\n2019-11-19 13:34:02.791614: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-19 13:34:02.814669: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2712000000 Hz\r\n2019-11-19 13:34:02.815094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3b64140 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-19 13:34:02.815124: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\n2019-11-19 13:34:02.937627: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-11-19 13:34:02.937702: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-19 13:34:02.948371: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2019-11-19 13:34:02.948393: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: Graph size after: 189 nodes (144), 355 edges (290), time = 4.452ms.\r\n2019-11-19 13:34:02.948398: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.08ms.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py:854: UserWarning: Property target_ops is deprecated, please use target_spec.supported_ops instead.\r\n  \"target_spec.supported_ops instead.\" % name)\r\n2019-11-19 13:34:02.971121: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-11-19 13:34:02.971187: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-19 13:34:02.977074: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2019-11-19 13:34:02.977098: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 34 nodes (-8), 57 edges (-12), time = 3.248ms.\r\n2019-11-19 13:34:02.977102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 34 nodes (0), 57 edges (0), time = 0.73ms.\r\nUse '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Use '@tf.function' or '@defun' to decorate the function.Traceback (most recent call last):\r\n  File \"/home/mapeima/Code/python/edgeTPU/model_converter.py\", line 30, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 983, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py\", line 449, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py\", line 200, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-11-19 13:34:04.120420: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: IdentityN\r\n2019-11-19 13:34:04.120589: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 17 operators, 26 arrays (0 quantized)\r\n2019-11-19 13:34:04.120707: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 17 operators, 26 arrays (0 quantized)\r\n2019-11-19 13:34:04.120849: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 6 operators, 12 arrays (0 quantized)\r\n2019-11-19 13:34:04.121155: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 5 operators, 11 arrays (0 quantized)\r\n2019-11-19 13:34:04.121196: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 4 operators, 9 arrays (0 quantized)\r\n2019-11-19 13:34:04.121224: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 4 operators, 9 arrays (0 quantized)\r\n2019-11-19 13:34:04.121241: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 4 operators, 9 arrays (0 quantized)\r\n2019-11-19 13:34:04.121273: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 576 bytes, theoretical optimal value: 576 bytes.\r\n2019-11-19 13:34:04.121289: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 204042 ops, equivalently 102021 MACs\r\n2019-11-19 13:34:04.121293: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 101770\r\n2019-11-19 13:34:04.121470: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.\r\n\r\n\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["I tried the conversion with tf 2.0 and the your conversion code worked fine without any errors. Can you try this with 2.0 again?", "Yes, I know that with tf 2.0 works perfectly but the point is that I need to use tf 1.15 because I want to run that model on the Google edgeTPU and to do that I have to perform a full integer quantization. As they say [here](https://coral.withgoogle.com/docs/edgetpu/models-intro/#quantization), I must use tf 1.15:\r\n\r\n> Note: To use post-training quantization, you must use TensorFlow 1.15 and set both the input and output type to uint8. (Currently, TensorFlow 2.0 does not support uint8 input/output with post-training quantization.) For instructions, see the TensorFlow Lite guide to full integer post-training quantization.", "Hi @mapeima \r\n\r\nWe have code that is about to be submitted to enable the uint8 parameters in the 2.0 converter. The fastest path for you will be to wait until that is in the nightly, and then upgrade to using the new version of the converter that has support for IdentityN. Will let you know when that is ready.\r\n\r\nThanks!\r\n-Suharsh", "I meet the same problem\uff0ceven when I  post-quant full integer with MobilenetV2", "Any updates with tf 1.15 ?", "@vinorth05 It looks like you can now use TF 2.x (preferably  > TF 2.3 so that you can have a fully int8 quantized model - including uint8 model input and output). Here is the colab that you can use as a reference: [https://colab.sandbox.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb]\r\n\r\nDoes this work for you? \r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34416\">No</a>\n"]}, {"number": 34415, "title": "How to limit the cpu core number in Tensorflow 2.0", "body": "I am using tf.keras to train a model with cpu and I want to limit the cpu usage of this program. I found this article(https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43383):\r\n\r\n```\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\n\r\nconfig = tf.ConfigProto(intra_op_parallelism_threads=args.jobs, \\ \r\n                        inter_op_parallelism_threads=args.jobs, \\\r\n                        allow_soft_placement=True, \\\r\n                        device_count = {'CPU': args.jobs})\r\nsession = tf.Session(config=config)\r\nK.set_session(session)\r\n```\r\n\r\nBut in Tensorflow 2.0, ConfigProto is deprecated and Session is gone. What's the correct method to limit cpu usage?", "comments": ["```\r\ntf.config.threading.set_intra_op_parallelism_threads(1)\r\ntf.config.threading.set_inter_op_parallelism_threads(112)\r\n```\r\nHave a try of this setting @fancyerii ?", "@fancyerii, Please take a look at this [link](https://www.tensorflow.org/api_docs/python/tf/config/threading). Thanks!", "I tried \r\n```\r\ntf.config.threading.set_intra_op_parallelism_threads(1)\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\n```\r\nusing `tensorflow.keras`. It doesn't work on Windows 10, Windows 10 WSL and Ubuntu. On Ubuntu and WSL it uses all CPUs, no matter the number I set. On Windows 10 it seems to regulate something, but not exactly the number of CPUs. And what is the difference between intra and inter op?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "It's apparently a bug discussed in #29968", "@benni93 on the difference between intra and inter op, may have a look at this [StackOverflow answer](https://stackoverflow.com/a/41233901/4360557).", "This issue still persists. Please **Re-Open** this issue.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/29968#issuecomment-572492815\r\n", "# Tensorflow 2.0 (Not Working)\r\n\r\n```from tensorflow.compat.v1.keras import backend as K\r\nconfig = tf.compat.v1.ConfigProto(device_count={\"CPU\": 2})\r\nK.set_session(tf.compat.v1.Session(config=config))", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/c20f28f27cfe7e5624d55f9c8bef16a8/untitled84.ipynb)..Thanks !", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/74cd676f801692042889258380ba8c54/untitled247.ipynb) ..Thanks!", "hello all,\r\n\r\nfor me it seems to work with: \r\n```\r\ntf.config.threading.set_intra_op_parallelism_threads(1)\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\n```\r\nusing CentOs and tensorflow 2.5.0 and python 3.6.\r\n\r\nI think one crucial factor is finding a commonly agreed way of testing this. I was using: `htop -p <pid>` and I was validating that without the settings I've got >100% CPU usage and with the settings <=100%, which was my goal, albeit I agree that this test might not be the most sophisticated one :)\r\n\r\nIn https://github.com/tensorflow/tensorflow/issues/29968, it's discussed that there are many threads and context switches. Which I can confirm, however for my use-case was not relevant and from the CPU utilization point of view - I think - is irrelevant. \r\nIf there are 100 threads but only one is running or ~1core can serve all active threads, I'm fine.\r\n\r\n@sushreebarsa both of your notebooks have syntax error which is independent of tensorflow. At the same time it does not really matter, because what you try to do is anyhow not supported in tensorflow 2, session is not really supported anymore: https://www.tensorflow.org/guide/effective_tf2#functions_not_sessions", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> In #29968, it's discussed that there are many threads and context switches. Which I can confirm, however for my use-case was not relevant and from the CPU utilization point of view - I think - is irrelevant.\r\n\r\nMany threads and context switches cause cache trashing and inefficiencies, i.e. 100 % CPU load does not translate into 100 % efficient CPU use. You can analyze this further with performance analysis tools such as `perf` to estimate how much actual CPU cycles are used and how many are lost due to the threading overhead. ", "I had the same problem. Make sure the first lines in your code are:\r\n\r\n```\r\nimport os\r\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\r\n```\r\n\r\nAnd then import everything else. If you import tensorflow or even numpy before that, it won't work. Hope that solves the issue!", "@fancyerii please refer to the above comment https://github.com/tensorflow/tensorflow/issues/34415#issuecomment-909201783 and let us know if you still need any help here. If it is resolved then please feel free to move this issue to close status ? Thanks!   ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34415\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34415\">No</a>\n", "Seems TF2 still does not respect these options for simple linear algebra tasks. For example, \r\n```\r\ninputA = tf.random.uniform([batch_size, problem_size, problem_size])\r\noutput = tf.linalg.inv(\r\n            inputA, adjoint=False, name=None\r\n        )\r\noutput = tf.reduce_sum(output)\r\n```\r\nwould still use all cores it can find no matter what is set in config.\r\nIt seems the only thing that actually works as intended is \r\n```\r\nos.environ[\"OMP_NUM_THREADS\"] = f\"{N}\"\r\nos.environ['TF_NUM_INTEROP_THREADS'] = f\"{N}\"\r\nos.environ['TF_NUM_INTRAOP_THREADS'] = f\"{N}\"\r\n```\r\n\r\nMaybe would be nice if programmatic change would work too, starting new processes not always convenient.", "Tried above solutions, still all CPU cores were used by Tensorflow."]}, {"number": 34414, "title": "Added new hlo to lhlo xla dialect emitters.", "body": "Added missing `hlo` to `lhlo` code generation functionality.", "comments": []}, {"number": 34413, "title": "tf.12 to tf2.0", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\n```\r\ndef compile(self, learning_rate, momentum):\r\n    \"\"\"Gets the model ready for training. Adds losses, regularization, and\r\n    metrics. Then calls the Keras compile() function.\r\n    \"\"\"\r\n    # Optimizer object\r\n    optimizer = keras.optimizers.SGD(\r\n        lr=learning_rate, momentum=momentum,\r\n        clipnorm=self.config.GRADIENT_CLIP_NORM)\r\n    # Add Losses\r\n    # First, clear previously set losses to avoid duplication\r\n    self.keras_model._losses = []\r\n    self.keras_model._per_input_losses = {}\r\n    loss_names = [\r\n        \"rpn_class_loss\",  \"rpn_bbox_loss\",\r\n        \"mrcnn_class_loss\", \"mrcnn_bbox_loss\", \"mrcnn_mask_loss\"]\r\n    for name in loss_names:\r\n        layer = self.keras_model.get_layer(name)\r\n        if layer.output in self.keras_model.losses:\r\n            continue\r\n        loss = (\r\n            tf.reduce_mean(layer.output, keepdims=True)\r\n            * self.config.LOSS_WEIGHTS.get(name, 1.))\r\n        self.keras_model.add_loss(loss)\r\n\r\n    # Add L2 Regularization\r\n    # Skip gamma and beta weights of batch normalization layers.\r\n    reg_losses = [\r\n        keras.regularizers.l2(self.config.WEIGHT_DECAY)(w) / tf.cast(tf.size(w), tf.float32)\r\n        for w in self.keras_model.trainable_weights\r\n        if 'gamma' not in w.name and 'beta' not in w.name]\r\n    self.keras_model.add_loss(tf.add_n(reg_losses))\r\n\r\n    # Compile\r\n    self.keras_model.compile(\r\n        optimizer=optimizer,\r\n        loss=[None] * len(self.keras_model.outputs))\r\n\r\n    # Add metrics for losses\r\n    for name in loss_names:\r\n        if name in self.keras_model.metrics_names:\r\n            continue\r\n        layer = self.keras_model.get_layer(name)\r\n        self.keras_model.metrics_names.append(name)\r\n        loss = (\r\n            tf.reduce_mean(layer.output, keepdims=True)\r\n            * self.config.LOSS_WEIGHTS.get(name, 1.))\r\n        self.keras_model.metrics_tensors.append(loss)\r\n```\r\nThis code works in tensorflow1.12 with tf.keras, while it does not work in tensorflow 2.0 with tf.keras, where should i change ?  When i turn off eager mode by using `tf.compat.v1.disable_eager_execution()`, it can start to train while there is only loss displayed (without \"rpn_class_loss\",  \"rpn_bbox_loss\", \"mrcnn_class_loss\", \"mrcnn_bbox_loss\", \"mrcnn_mask_loss\").\r\n\r\n In eager mode, it raises error using a tf.Tensor as a Python bool is not allowed in Graph execution in `        if layer.output in self.keras_model.losses:\r\n            continue` ; x and y must have the same dtype, got tf.float64!=tf.float32 in `    reg_losses = [\r\n        keras.regularizers.l2(self.config.WEIGHT_DECAY)(w) / tf.cast(tf.size(w), tf.float32)\r\n        for w in self.keras_model.trainable_weights\r\n        if 'gamma' not in w.name and 'beta' not in w.name]`, If i delete those lines, it would have error No gradients provided for any variable.", "comments": ["Looks like code is incomplete. Request you to provide minimal stand alone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@xuchengggg if you use tf2.0 you have to change: \r\n`model.metrics_tensors.append(loss)`\r\nto\r\n`model.metrics.append(loss)`\r\n\r\nMore info: https://github.com/tensorflow/tensorflow/issues/33789", "@xuchengggg \r\n\r\nCan you please confirm if @kiflowb777 's workaround is working for you.Thanks!", "I am very sorry that the reply is late. I have changed the metrics_tensors to metrics before his @kiflowb777  workaround, but forgot to change the code in the issue and there are still same problems as i mentioned in the issue.\r\nI saw in some tutorials that the source of conda is often updated more slowly and I use `conda install tensorflow-gpu==2.0`, so will it be a problem with the version of conda installed?", "@xuchengggg \r\n\r\nRequest you to provide minimal stand alone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!"]}, {"number": 34412, "title": "Added missing xla lowering operations.", "body": "Added some additional `xla` lowering operations to the `mlir` tensorflow part.", "comments": []}, {"number": 34411, "title": "Fix compile error in Tensorflow Lite Micro.", "body": "Fix missing std:: in activation_utils.h, as highlighted in this [comment](https://github.com/tensorflow/tensorflow/issues/34219#issuecomment-554614495) by @jomoengineer.", "comments": ["Ready to merge"]}, {"number": 34410, "title": "How to limit tensorflow CPU and memory usage in c_api?", "body": "**In python:**\r\n`sess = tf.Session(config=\r\n    tf.ConfigProto(inter_op_parallelism_threads=1,\r\n                   intra_op_parallelism_threads=1))`\r\n**c_api:**\r\nHow?", "comments": ["Use TF_SessionOptions https://github.com/tensorflow/tensorflow/blob/f7c570c53cee6bf140c7900ae01f857871f74d26/tensorflow/c/c_api.h#L147 to set the config proto, which has those fields."]}, {"number": 34409, "title": "C++ compilation of rule '//tensorflow/cc:cc_op_gen_main' failed (Ex it 1)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  CentOS 6.10\r\n- TensorFlow installed from (source or binary):\r\n  source\r\n- TensorFlow version:\r\n  Branch r2.0\r\n- Python version:\r\n 3.7.5\r\n- Bazel version (if compiling from source):\r\n  0.26.1\r\n- GCC/Compiler version (if compiling from source):\r\n gcc version 4.9.1 20140922 (Red Hat 4.9.1-10) (GCC)\r\n- CUDA/cuDNN version:\r\n  CUDA 10.1  cuDNN 7.6.5\r\n- GPU model and memory:\r\n  Tesla P4 8G\r\n\r\n**Describe the problem**\r\n  While building TensorFlow from r2.0 branch the build fails.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n  ``` bash\r\n  bazel build --config=opt --config=cuda --config=nonccl --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n ```\r\n\r\n**Any other info / logs**\r\n\r\n**configure output**\r\n```\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.1 installed.\r\nPlease specify the location of python. [Default is /usr/local/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/python3/lib/python3.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/python3/lib/python3.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: Y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: N\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: \r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: N\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /opt/rh/devtoolset-3/root/usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: N\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n```\r\n\r\n**build output**\r\n```\r\nERROR: /root/tensorflow/tensorflow/cc/BUILD:636:1: C++ compilation of rule '//tensorflow/cc:cc_op_gen_main' failed (Ex\r\nit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/opt/rh/devtoolset-3/root/usr/lib64:/opt/rh/devtoolset-3/root/usr/lib:/usr/local/cuda-10.1/lib64 \\\r\n    PATH=/usr/local/python3/bin/:/usr/local/git/bin:/opt/rh/devtoolset-3/root/usr/bin:/usr/lib/jvm/jdk1.8.0_212/bin:/u\r\nsr/local/bin:/usr/local/cuda-10.1/bin:/usr/local/cuda-10.1/NsightCompute-2019.1:/usr/local/openresty/nginx/sbin:/usr/l\r\nocal/php/bin:/usr/local/openssl/bin:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/bin:/bin:/usr/sbin:/sbin:/root/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/cc/_objs/cc_op_gen_main/cc_op_gen_main.d '-frandom-seed=bazel-out/host/bin/tensorflow/cc/_objs/cc_op_gen_main/cc_op_gen_main.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif_archive -isystem bazel-out/host/bin/external/gif_archive -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread -c tensorflow/cc/framework/cc_op_gen_main.cc -o bazel-out/host/bin/tensorflow/cc/_objs/cc_op_gen_main/cc_op_gen_main.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from tensorflow/cc/framework/cc_op_gen_main.cc:21:0:\r\n./tensorflow/core/lib/io/path.h: In instantiation of 'std::string tensorflow::io::JoinPath(const T& ...) [with T = {st\r\nd::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::basic_string<char, std::char_traits<char>, \r\nstd::allocator<char> >}; std::string = std::basic_string<char>]':\r\ntensorflow/cc/framework/cc_op_gen_main.cc:42:72:   required from here\r\n./tensorflow/core/lib/io/path.h:47:42: error: could not convert '{args#0, args#1}' from '<brace-enclosed initializer l\r\nist>' to 'std::initializer_list<absl::string_view>'\r\n   return internal::JoinPathImpl({args...});\r\n                                          ^\r\n./tensorflow/core/lib/io/path.h: In function 'std::string tensorflow::io::JoinPath(const T& ...) [with T = {std::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::basic_string<char, std::char_traits<char>, std::allocator<char> >}; std::string = std::basic_string<char>]':\r\n./tensorflow/core/lib/io/path.h:48:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 27.597s, Critical Path: 19.68s\r\nINFO: 145 processes: 145 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@hectorqin,\r\nIs this still an issue?\r\n\r\nCould you please follow the official [build form source guide](https://www.tensorflow.org/install/source) and check if you are facing the same issue with TF v2.4 as well? Thanks!", "Sorry this is a long time ago, i can't confirm now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34409\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34409\">No</a>\n"]}, {"number": 34408, "title": "tensorflow.python.keras.testing_utils.layer_test breaks when a (custom) layer is returning a list/tuple of tensor ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip3 installed\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nWhen we create a custom `tensorflow.keras.layer.Layer`, said `MyCustomLayer`, that returns a list/tuple of tensors in it's `call` method, calling `layer_test(MyCustomLayer, input_shape=expected_input_shape)` will break as `layer_test` does not expect list / tuple as outputs of the custom layer's `call` method, and by tensorflow's keras layer documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#call), list / tuple of tensors are valid output. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.testing_utils import layer_test\r\n\r\n# a dummy layer that just returns a list of input\r\nclass MyCustomLayer(tf.keras.layers.Layer):\r\n  def call(self, input):\r\n    return [input, input]\r\n\r\nlayer_test(MyCustomLayer, input_shape=(1,2))\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior is that the above snippet will not return errors (log attached below)\r\n\r\nPotential Solution: Check the input `x` in `dtype()` method in `tensorflow_core/python/keras/backend.py`.\r\n\r\nHappy to contribute if this is helpful :)\r\n\r\n**Other info / logs**\r\n\r\nError Log:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-0eea06e136e5> in <module>()\r\n----> 1 layer_test(MyCustomLayer, input_shape=(1,2))\r\n\r\n2 frames\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/test_util.py in decorated(self, *args, **kwargs)\r\n   1583       original_var = os.environ.get(\"TF_CUDNN_DETERMINISTIC\", \"\")\r\n   1584       os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"true\"\r\n-> 1585       result = f(self, *args, **kwargs)\r\n   1586       os.environ[\"TF_CUDNN_DETERMINISTIC\"] = original_var\r\n   1587       return result\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/testing_utils.py in layer_test(layer_cls, kwargs, input_shape, input_dtype, input_data, expected_output, expected_output_dtype, expected_output_shape, validate_training, adapt_data)\r\n    140   x = keras.layers.Input(shape=input_shape[1:], dtype=input_dtype)\r\n    141   y = layer(x)\r\n--> 142   if keras.backend.dtype(y) != expected_output_dtype:\r\n    143     raise AssertionError('When testing layer %s, for input %s, found output '\r\n    144                          'dtype=%s but expected to find %s.\\nFull kwargs: %s' %\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/backend.py in dtype(x)\r\n   1247   ```\r\n   1248   \"\"\"\r\n-> 1249   return x.dtype.base_dtype.name\r\n   1250 \r\n   1251 \r\n\r\nAttributeError: 'list' object has no attribute 'dtype'\r\n```\r\n", "comments": ["Issue is replicating on colab with Tf 2.0.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/d58b838b5dc4b2c91997e4c3d54a4700/untitled260.ipynb). Thanks!", "I have similar problem with custom layers in tf.keras: https://github.com/tensorflow/tensorflow/issues/33785\r\nIt seems like tf.keras cannot compute custom layer's shape.", "@kiflowb777 I think the problem is different from [#33785](https://github.com/tensorflow/tensorflow/issues/33785) as this issue is due to the `test_layers` in `test_utils` not expecting a list/tuples of tensors.", "Chatted with @shun-lin and Shun is happy to contribute. Thanks Shun!", "@shun-lin Any update on adding multi-input/multi-output support? ", "I can take up this issue if you're busy. Need it for a personal project.", "@Squadrick yes may you pick this one up? I didn't have too much time recently to dive deeper into good solution, thanks!", "@Squadrick Are you working on this issue? Thanks!", "@jvishnuvardhan Hey, sorry, completely forgot about it. I've added to my high priority TODO list. With the quarantine in place, I'll get to it ASAP. ", "@Squadrick stay safe :)", "@Squadrick\r\nCan you please update on this\r\nThanks!", "Was able to replicate the issue in TF 2.6.0-dev20210529,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/7dc0ce8fb04d3334ddb9d89579a6c344/untitled83.ipynb#scrollTo=jy7mryJ6Tw2D)..Thanks !", "I could reproduce the issue with TF 2.6 .Please, find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/819bdd15fb3cc1b711e7bbe130d9b640/untitled83.ipynb#scrollTo=OoCgztDsViOy).Thanks!", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34408\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34408\">No</a>\n"]}, {"number": 34407, "title": "How to access the context menu in pywinauto", "body": "Hello All,\r\n\r\nI could not able to access the menus(tool bars in the left side of an application). Can you please help how to acsess thos menus using pywinauto ?\r\nI could not able to attach the screenshot because of some issue in the attachment process.\r\n\r\n\r\nThanks in Advance!!\r\n\r\n\r\n", "comments": ["@Sathis89 ,\r\nCan you please open an issue in the [pywinauto](https://github.com/pywinauto/pywinauto/issues/new) repo as this is not related to TensorFlow.\r\nThanks!"]}, {"number": 34406, "title": "Mixed precision training with tf.keras 2.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to incorporate mixed-precision training in my `tf.keras` models. After going through [the instructions on the NVIDIA docs](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html), I used the following code snippet (full code is provided in the end):\r\n\r\n```python\r\nopt = tf.keras.optimizers.Adam()\r\nopt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\r\nmodel.compile(loss=loss, optimizer=opt)\r\nmodel.fit(...)\r\n```\r\nI do not see any speed up after mixed precision is enabled. I tried out `tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")`. Still there is no significant improvement to note. On the other hand, I tried to experiment with `tf.keras.mixed_precision.experimental.set_policy('mixed_float16')` but I am kind of confused about how to appropriately use it. I am trying to train the following model on the FashionMNIST dataset:\r\n\r\n```python\r\ntf.keras.mixed_precision.experimental.set_policy('mixed_float16')\r\nmodel = Sequential([\r\n            Input((28,28,1)),\r\n            Conv2D(32, (3, 3), activation='relu'),\r\n            MaxPooling2D((2,2)),\r\n            Conv2D(64, (3, 3), activation='relu'),\r\n            MaxPooling2D((2,2)),\r\n            Conv2D(64, (3, 3), activation='relu'),\r\n            GlobalAveragePooling2D(),\r\n            Dense(64, activation='relu'),\r\n            Dense(10, activation='softmax')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n```\r\n\r\nBut when I call `model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=128)`, I get the following:\r\n\r\n```\r\n/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    561                   \"%s type %s of argument '%s'.\" %\r\n    562                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\r\n--> 563                    inferred_from[input_arg.type_attr]))\r\n    564 \r\n    565           types = [values.dtype]\r\n\r\nTypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.\r\n```\r\n\r\nThe entire error trace is there in the attached notebook. The second experiment in this line I did was with `policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')`. In that, I just set the `dtype` of the penultimate dense layer (of the above-mentioned model) to `policy`. The works fine but throws this warning:\r\n\r\n```\r\nWARNING:tensorflow:Layer dense_1 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n```\r\n\r\nAny recommended way of using these policies along with simple and more complex models? \r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n[Mixed Precision Training in tf.keras 2.0.zip](https://github.com/tensorflow/tensorflow/files/3862764/Mixed.Precision.Training.in.tf.keras.2.0.zip)\r\n\r\n", "comments": ["I had the same problem with float16 training. Please refer to [this issue](https://github.com/tensorflow/tensorflow/issues/33484) for an explanation on why the TypeError is thrown. Apparently, it will be fixed in TF2.1 and is already fixed in the current `tf-nightly-gpu`.", "@verrannt thank you very much :)", "@sayakpaul ,\r\nLooks like the issue is fixed in `tf-nightly-gpu`, can you confirm the same and close the issue ?Thanks!", "@oanush I can definitely check that out but there were some other issues I mentioned which I believe need some more attention. For example, the speedup not happening when using `tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")`. So, I would wait for a few days to get some pointers on that.  ", "When using `tf.keras.mixed_precision.experimental.set_policy('mixed_float16')`, the last layer (softmax layer) must currently be set to have `dtype=tf.float32` and it will work:\r\n\r\n```python\r\nDense(10, activation='softmax', dtype=tf.float32)\r\n```\r\n\r\nHowever, you are not seeing speed-up because of a few reasons:\r\n\r\n- The model is too small (only 60554 params), so other overheads (casting, padding) have greater impact than speeding up the layers\r\n- Many shapes in the model are not fully efficient on Tensor Cores. Conv filters/channels are multiples of 8, so that's good. But the input/output sizes are not in multiples of 8 (I see 26, 13 etc), and this requires padding in the software. This is much less significant on larger models.\r\n\r\nFor some \"non-official\" examples demonstrating speed-up on some real-world models, check out:\r\nhttps://github.com/NVAITC/pycon-sg19-tensorflow-tutorial\r\n\r\nFor some additional background information, also check out:\r\n\r\n* https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/experimental/enable_mixed_precision_graph_rewrite\r\n* https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html\r\n\r\nP.S. I am using TensorFlow 2.0 stable", "@tlkh thanks much for your pointers. I experimented today with ResNet50V2 as well but could not observe any significant improvements. You can find the notebook [here](https://gist.github.com/sayakpaul/e595b12b09dbb40e452db0289a6aa76c). \r\n\r\nAlso, did you check out experiments with `tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)` and `tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")` (notebooks `1 - MP in tf.keras 2.0.ipynb` and `2 - MP in tf.keras 2.0.ipynb` respectively)? They include a larger model. ", "**2 - MP in tf.keras 2.0.ipynb**\r\n\r\n- **Your training is CPU bottlenecked due to the data augmentation**.  Just adding `use_multiprocessing=True` and `workers=8` to the fit function cuts each epoch from 30+s to 20+s on my workstation (20 core Xeon processor). Using tf.data (in particular the `prefetch` function) would likely yield much better results. \r\n- In any case, I don't think you're using the mixed precision API correctly. `tf.keras.mixed_precision.experimental.LossScaleOptimizer` does not activate mixed precision on its own. You need to call `tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})`. Note that this is set on a per-session basis, so you need to call `tf.keras.backend.clear_session` to remove that setting. The same goes for when using `enable_mixed_precision_graph_rewrite`, it is a per-session config. You can't enable it half-way in a notebook for the **same** model (like in your first notebook!). \r\n- GPU utilisation is oddly low (<20%) and I'm actually not quite sure why actually.\r\n- The model is still \"too small\". The model's convolution layers only have 32 filters each. As a rule of thumb, Conv layers under 256 filters are likely to be memory, not compute bottlenecked. \r\n\r\nHere are my results for mixed precision vs FP32 (V100) after some modifications:\r\n\r\n* Modifications: 256 filters per conv layer; 40 workers; batch_size 320; enabled XLA compiler\r\n* FP32: 74.54s for first 3 epochs\r\n* mixed precision: 59.97s for first 3 epochs\r\n\r\nFor what it's worth, smaller models aren't currently a good showcase for mixed precision. I typically work with larger transformer models, and mixed precision is really a godsend for those! \r\n", "@tlkh thank you for providing the information. I could not find this on the documentations I had studied. Moving forward, I will consider these and will report back accordingly. ", "Thank you @sayakpaul for filing this issue, and @tlkh for the responses! As mentioned, TF2.1 will fix the TypeError, but you can pass `dtype=tf.float32` to make it work in 2.0.\r\n\r\nHowever, even in 2.1, it is recommended to do softmax in float32 for numeric stability. @tlkh fix of passing `dtype=tf.float32` will cause the softmax to be done in float32, solving the issue. For slightly better performance, you can also just do the softmax in float32 while keeping the Dense layer in mixed_float16, by splitting the Dense layer from the activation as follows:\r\n\r\n```\r\nmodel = Sequential([\r\n            ... # All the previous layers\r\n            Dense(10),\r\n            Activation('softmax', dtype='float32')\r\n])\r\n```\r\n\r\nI am trying to find a way to automatically do Softmax in float32, but as of 2.1, you still must explicitly pass `dtype='float32'`.\r\n\r\nAlso, you can safely ignore the warning you received:\r\n\r\n```\r\nWARNING:tensorflow:Layer dense_1 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n```\r\n\r\nIn this particular case, the warning is a bug and should not appear. This has been fixed in the nightlies and TF 2.1, and does not appear with `tf-nightly-gpu`.\r\n\r\nI will have a better tutorial for using mixed precision in the future. For now, I recommend using the following: https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/Policy", "Thank you very much for passing this along @reedwm. I think the notebooks @tlkh shared are quite nice and definitely provide good insights. I am also working on more experiments to make it easier for the community to try mixed-precision training in `tf.keras` 2.0. ", "@tlkh I experimented with your codes today but on a [different dataset](https://datahack.analyticsvidhya.com/teams/game-of-thrones). Mixed Precision indeed showed it's magic and that too big time. I have pushed the files [here](https://github.com/sayakpaul/Mixed-Precision-Training-in-tf.keras-2.0/tree/master/With_Policy). The GPU utility has significantly increased as I am now using utilities from `tf.data`. I did not see any stark difference in the memory footprint though (you can check those information [here](https://app.wandb.ai/sayakpaul/mixed-precision-tf-keras/runs)). \r\n\r\nI am going to experiment with the other methods now and will share the results here. On a side note, if you see the performance of ResNet got worse on the dataset I used, it can be for a number of reasons, though. Have you tried other networks like VGG16 for example? \r\n\r\n**Note** that the repository is not complete yet. I am going to add all the acknowledgments, README a bit later. \r\n\r\n**Edit**:\r\nAfter tweaking the learning rate, I was able to get a steady decrease in the loss as well as a steady increase in the accuracy:\r\n\r\n```\r\nTrain for 78 steps\r\nEpoch 1/5\r\n78/78 [==============================] - 70s 898ms/step - loss: 0.3721 - accuracy: 0.7402\r\nEpoch 2/5\r\n78/78 [==============================] - 19s 238ms/step - loss: 0.2111 - accuracy: 0.7772\r\nEpoch 3/5\r\n78/78 [==============================] - 18s 234ms/step - loss: 0.1474 - accuracy: 0.7811\r\nEpoch 4/5\r\n78/78 [==============================] - 18s 233ms/step - loss: 0.1194 - accuracy: 0.7873\r\nEpoch 5/5\r\n78/78 [==============================] - 18s 235ms/step - loss: 0.1027 - accuracy: 0.7888\r\n```\r\n\r\nI used the Policy method here. However, when I used the `tf.keras.mixed_precision.experimental.LossScaleOptimizer`, there was a steady increase in the accuracy but the loss increased with each and every epoch. Could this because the loss values are scaled in this method? I have updated the notebooks in the above-mentioned repository as well. ", "When using the Policy method, the loss is also scaled. I'm not sure why the tf.config + LossScaleOptimizer would not be working when the Policy method does.\r\nI'm currently a bit tied up with work, but I'll take a close look at this when I have some time. ", "Sure. Thanks, @tlkh :)", "@sayakpaul, in [the Jupyter notebook where you have the LossScaleOptimizer](https://github.com/sayakpaul/Mixed-Precision-Training-in-tf.keras-2.0/blob/master/With_Loss_Scale_Optimizer/Mixed_Precision.ipynb), you have the line\r\n\r\n```\r\ntf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,  \r\n                                                       \"dynamic\")\r\n```\r\n\r\nThe issue is LossScaleOptimizer does not modify the passed in optimizer, but instead returns a new optimizer that performs loss scaling. Try setting `opt` to the LossScaleOptimizer instead:\r\n\r\n```\r\nopt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,  \r\n                                                               \"dynamic\")\r\n```\r\nI think fixing this will cause tf.config + LossScaleOptimizer to work just as well as the Policy. In the future, I'll try to raise an error or warning if this issue occurs.\r\n\r\nAlso note currently I cannot run the Jupyter notebooks as I do not have the `X_train.npy` and `y_train.npy` datasets.\r\n\r\n", "@reedwm extremely sorry about that silly mistake. I have updated the notebooks accordingly but still, the loss increases along with the accuracy. \r\n\r\nNote that I intentionally did not pass the `.npy` files. You would need to run the `Data_gathering_and_prep.ipynb` notebook to generate them. ", "Also, for the LossScaleOptimizer notebook, try passing `learning_rate=1e-4` to the Adam optimizer:\r\n\r\n```\r\nopt = Adam(learning_rate=1e-4)\r\n```\r\n\r\nThis is already done in the Policy notebook. After doing this, the loss decreased for me instead of increasing.", "Hi @reedwm.\r\n\r\nIn the [Mixed_Precision_Data_Augmentation notebook](https://github.com/sayakpaul/Mixed-Precision-Training-in-tf.keras-2.0/blob/master/With_Loss_Scale_Optimizer/Mixed_Precision_Data_Augmentation.ipynb) I am already passing it:\r\n\r\n```\r\nopt = Adam(learning_rate=1e-4)\r\nopt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt,  \r\n                                                       \"dynamic\")\r\nmodel.compile(loss=\"categorical_crossentropy\",\r\n              optimizer=opt,\r\n              metrics=[\"accuracy\"])\r\n```\r\n\r\nThe loss is not decreasing. "]}, {"number": 34405, "title": "tf.keras.utils.get_file error in Extracting .zip format", "body": "Hi, \r\n**Issue**: tf.keras.utils.get_filecannot extract .zip file and returns a list\r\nBasicallly I have to convert the entire data set into tar.gz format since for some reason the above module is unable to extract the .zip folder. Another bug is that the .tar folder name and the fname have to be compulsorily same as per present module. I hope these two issues get fixed by the Tensorflow community :)\r\nAny help will be appreciate...\r\nThanks in Advance...", "comments": ["I think this API is able to extract the .zip file. Is there something special in your modules?", "@vinayapathak : From https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file \r\n\r\n> files in tar, tar.gz, tar.bz, and zip formats can also be extracted. Passing a hash will verify the file after download. The command line programs shasum and sha256sum can compute the hash.\r\n\r\nCould you post your code snippet for us to analyze the issue better?", "@vinayapathak, Specify the `archive_format` as .zip or .tar. \r\n`cache_subdir: Subdirectory under the Keras cache dir where the file is saved. If an absolute path /path/to/folder is specified the file will be saved at that location.`\r\nFor more info see https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file", "Thanks all!", "@vinayapathak Hi\r\ncan you help me to solve this\r\ni want to replace my files in this block of lines\r\nimage_path = tf.keras.utils.get_file(\r\n      'flower_photos',\r\n      'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\r\n      untar=True)\r\n\r\n", "Extremely sorry for late reply. Use !wget for best performance"]}, {"number": 34404, "title": " Distributed training resnet50 using 4 nodes 32  TeslaV100 cards", "body": "\r\nI checked a lot of literature, but I didn't find the results. The questions are as follows:\r\nHow many hours can it converge\uff1f\uff08Distributed training resnet50 using 4 nodes 32 TeslaV100 cards\uff09\r\n\r\nDo you have internal test results that can be displayed to better understand the performance of your distributed training.", "comments": ["There are lot of resources like [this](https://www.leadergpu.com/articles/429-tensorflow-resnet-50-benchmark) on benchmark performance of tensorflow. \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 34403, "title": "Fix 2 failing tests", "body": "", "comments": []}, {"number": 34402, "title": "Fix typos in distribute_lib.py code examples", "body": "", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 34401, "title": "Building from source \"undefined reference to `cublasGemmStridedBatchedEx' \"", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source \r\n- TensorFlow version:1.15\r\n- Python version:3.5.12\r\n- Installed using virtualenv? pip? conda?: pip install\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): GCC-5.4.0\r\n- CUDA/cuDNN version: CUDA 9.1.85 cuDNN 7.0.5\r\n- GPU model and memory: Tesla M4 4Gb\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nYou have bazel 0.29.1- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.5/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n/usr/local/lib/python3.5/dist-packages\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 9.1 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 5.2]: 5.2\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n$bazel build --verbose_failures --action_env=LD_LIBRARY_PATH=/usr/local/cuda/lib64 --config=cuda --config=opt //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=nonccl --config=nohdfs --config=mkl --config=monolithic --config=noaws --config=nogcp --config=monolithic\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR: /home/tensorflow/tensorflow/tensorflow/lite/toco/BUILD:439:1: Linking of rule '//tensorflow/lite/toco:toco' failed (Exit 1)\r\n[19,054 / 21,282] 46 actions running\r\n    Compiling tensorflow/core/kernels/slice_op_gpu.cu.cc [for host]; 181s local\r\n    Compiling tensorflow/core/kernels/pad_op_gpu.cu.cc [for host]; 154s local\r\n    Compiling tensorflow/core/kernels/tile_functor_cpu.cc [for host]; 123s local\r\n    Compiling tensorflow/core/kernels/resource_variable_ops.cc [for host]; 106s local\r\n    Compiling tensorflow/core/kernels/conv_ops_fused_float.cc [for host]; 94s local\r\n    Compiling tensorflow/core/kernels/conv_ops_fused_double.cc [for host]; 91s local\r\n    Compiling tensorflow/core/kernels/conv_ops.cc [for host]; 91s local\r\n    Compiling tensorflow/core/kernels/conv_grad_ops_3d.cc [for host]; 90s local ...\r\nbazel-out/host/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.o): In function `tensorflow::Status stream_executor::gpu::CUDABlas::DoBlasGemmBatchedInternal<Eigen::half, float, cublasStatus_t (*)(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, float const*, float const**, int, float const**, int, float const*, float**, int, int)>(cublasStatus_t (*)(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, float const*, float const**, int, float const**, int, float const*, float**, int, int), stream_executor::Stream*, stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, float, absl::Span<stream_executor::DeviceMemory<Eigen::half>* const> const&, int, absl::Span<stream_executor::DeviceMemory<Eigen::half>* const> const&, int, float, absl::Span<stream_executor::DeviceMemory<Eigen::half>* const> const&, int, int, stream_executor::ScratchAllocator*)':\r\ncuda_blas.cc:(.text._ZN15stream_executor3gpu8CUDABlas25DoBlasGemmBatchedInternalIN5Eigen4halfEfPF14cublasStatus_tP13cublasContext17cublasOperation_tS8_iiiPKfPSA_iSB_iSA_PPfiiEEEN10tensorflow6StatusET1_PNS_6StreamENS_4blas9TransposeESM_yyyT0_RKN4absl4SpanIKPNS_12DeviceMemoryIT_EEEEiSX_iSN_SX_iiPNS_16ScratchAllocatorE[_ZN15stream_executor3gpu8CUDABlas25DoBlasGemmBatchedInternalIN5Eigen4halfEfPF14cublasStatus_tP13cublasContext17cublasOperation_tS8_iiiPKfPSA_iSB_iSA_PPfiiEEEN10tensorflow6StatusET1_PNS_6StreamENS_4blas9TransposeESM_yyyT0_RKN4absl4SpanIKPNS_12DeviceMemoryIT_EEEEiSX_iSN_SX_iiPNS_16ScratchAllocatorE]+0x8c2): undefined reference to `cublasGemmBatchedEx'\r\nbazel-out/host/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.o): In function `tensorflow::Status stream_executor::gpu::CUDABlas::DoBlasGemmBatchedInternal<float, float, cublasStatus_t (*)(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, float const*, float const**, int, float const**, int, float const*, float**, int, int)>(cublasStatus_t (*)(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, float const*, float const**, int, float const**, int, float const*, float**, int, int), stream_executor::Stream*, stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, float, absl::Span<stream_executor::DeviceMemory<float>* const> const&, int, absl::Span<stream_executor::DeviceMemory<float>* const> const&, int, float, absl::Span<stream_executor::DeviceMemory<float>* const> const&, int, int, stream_executor::ScratchAllocator*)':\r\ncuda_blas.cc:(.text._ZN15stream_executor3gpu8CUDABlas25DoBlasGemmBatchedInternalIffPF14cublasStatus_tP13cublasContext17cublasOperation_tS6_iiiPKfPS8_iS9_iS8_PPfiiEEEN10tensorflow6StatusET1_PNS_6StreamENS_4blas9TransposeESK_yyyT0_RKN4absl4SpanIKPNS_12DeviceMemoryIT_EEEEiSV_iSL_SV_iiPNS_16ScratchAllocatorE[_ZN15stream_executor3gpu8CUDABlas25DoBlasGemmBatchedInternalIffPF14cublasStatus_tP13cublasContext17cublasOperation_tS6_iiiPKfPS8_iS9_iS8_PPfiiEEEN10tensorflow6StatusET1_PNS_6StreamENS_4blas9TransposeESK_yyyT0_RKN4absl4SpanIKPNS_12DeviceMemoryIT_EEEEiSV_iSL_SV_iiPNS_16ScratchAllocatorE]+0x9c7): undefined reference to `cublasGemmBatchedEx'\r\nbazel-out/host/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.o): In function `tensorflow::Status stream_executor::gpu::CUDABlas::DoBlasGemmBatchedInternal<double, double, cublasStatus_t (*)(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, double const*, double const**, int, double const**, int, double const*, double**, int, int)>(cublasStatus_t (*)(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, double const*, double const**, int, double const**, int, double const*, double**, int, int), stream_executor::Stream*, stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, double, absl::Span<stream_executor::DeviceMemory<double>* const> const&, int, absl::Span<stream_executor::DeviceMemory<double>* const> const&, int, double, absl::Span<stream_executor::DeviceMemory<double>* const> const&, int, int, stream_executor::ScratchAllocator*)':\r\ncuda_blas.cc:(.text._ZN15stream_executor3gpu8CUDABlas25DoBlasGemmBatchedInternalIddPF14cublasStatus_tP13cublasContext17cublasOperation_tS6_iiiPKdPS8_iS9_iS8_PPdiiEEEN10tensorflow6StatusET1_PNS_6StreamENS_4blas9TransposeESK_yyyT0_RKN4absl4SpanIKPNS_12DeviceMemoryIT_EEEEiSV_iSL_SV_iiPNS_16ScratchAllocatorE[_ZN15stream_executor3gpu8CUDABlas25DoBlasGemmBatchedInternalIddPF14cublasStatus_tP13cublasContext17cublasOperation_tS6_iiiPKdPS8_iS9_iS8_PPdiiEEEN10tensorflow6StatusET1_PNS_6StreamENS_4blas9TransposeESK_yyyT0_RKN4absl4SpanIKPNS_12DeviceMemoryIT_EEEEiSV_iSL_SV_iiPNS_16ScratchAllocatorE]+0xceb): undefined reference to `cublasGemmBatchedEx'\r\nbazel-out/host/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.o): In function `tensorflow::Status stream_executor::gpu::CUDABlas::DoBlasGemmBatchedInternal<std::complex<float>, std::complex<float>, cublasStatus_t (*)(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, float2 const*, float2 const**, int, float2 const**, int, float2 const*, float2**, int, int)>(cublasStatus_t (*)(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, float2 const*, float2 const**, int, float2 const**, int, float2 const*, float2**, int, int), stream_executor::Stream*, stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, std::complex<float>, absl::Span<stream_executor::DeviceMemory<std::complex<float> >* const> const&, int, absl::Span<stream_executor::DeviceMemory<std::complex<float> >* const> const&, int, std::complex<float>, absl::Span<stream_executor::DeviceMemory<std::complex<float> >* const> const&, int, int, stream_executor::ScratchAllocator*)':\r\ncuda_blas.cc:(.text._ZN15stream_executor3gpu8CUDABlas25DoBlasGemmBatchedInternalISt7complexIfES4_PF14cublasStatus_tP13cublasContext17cublasOperation_tS8_iiiPK6float2PSB_iSC_iSB_PPS9_iiEEEN10tensorflow6StatusET1_PNS_6StreamENS_4blas9TransposeESN_yyyT0_RKN4absl4SpanIKPNS_12DeviceMemoryIT_EEEEiSY_iSO_SY_iiPNS_16ScratchAllocatorE[_ZN15stream_executor3gpu8CUDABlas25DoBlasGemmBatchedInternalISt7complexIfES4_PF14cublasStatus_tP13cublasContext17cublasOperation_tS8_iiiPK6float2PSB_iSC_iSB_PPS9_iiEEEN10tensorflow6StatusET1_PNS_6StreamENS_4blas9TransposeESN_yyyT0_RKN4absl4SpanIKPNS_12DeviceMemoryIT_EEEEiSY_iSO_SY_iiPNS_16ScratchAllocatorE]+0xd2b): undefined reference to `cublasGemmBatchedEx'\r\nbazel-out/host/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.o): In function `tensorflow::Status stream_executor::gpu::CUDABlas::DoBlasGemmBatchedInternal<std::complex<double>, std::complex<double>, cublasStatus_t (*)(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, double2 const*, double2 const**, int, double2 const**, int, double2 const*, double2**, int, int)>(cublasStatus_t (*)(cublasContext*, cublasOperation_t, cublasOperation_t, int, int, int, double2 const*, double2 const**, int, double2 const**, int, double2 const*, double2**, int, int), stream_executor::Stream*, stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, std::complex<double>, absl::Span<stream_executor::DeviceMemory<std::complex<double> >* const> const&, int, absl::Span<stream_executor::DeviceMemory<std::complex<double> >* const> const&, int, std::complex<double>, absl::Span<stream_executor::DeviceMemory<std::complex<double> >* const> const&, int, int, stream_executor::ScratchAllocator*)':\r\ncuda_blas.cc:(.text._ZN15stream_executor3gpu8CUDABlas25DoBlasGemmBatchedInternalISt7complexIdES4_PF14cublasStatus_tP13cublasContext17cublasOperation_tS8_iiiPK7double2PSB_iSC_iSB_PPS9_iiEEEN10tensorflow6StatusET1_PNS_6StreamENS_4blas9TransposeESN_yyyT0_RKN4absl4SpanIKPNS_12DeviceMemoryIT_EEEEiSY_iSO_SY_iiPNS_16ScratchAllocatorE[_ZN15stream_executor3gpu8CUDABlas25DoBlasGemmBatchedInternalISt7complexIdES4_PF14cublasStatus_tP13cublasContext17cublasOperation_tS8_iiiPK7double2PSB_iSC_iSB_PPS9_iiEEEN10tensorflow6StatusET1_PNS_6StreamENS_4blas9TransposeESN_yyyT0_RKN4absl4SpanIKPNS_12DeviceMemoryIT_EEEEiSY_iSO_SY_iiPNS_16ScratchAllocatorE]+0xcfb): undefined reference to `cublasGemmBatchedEx'\r\nbazel-out/host/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.o): In function `stream_executor::gpu::CUDABlas::DoBlasGemmStridedBatched(stream_executor::Stream*, stream_executor::blas::Transpose, stream_executor::blas::Transpose, unsigned long long, unsigned long long, unsigned long long, float, stream_executor::DeviceMemory<Eigen::half> const&, int, long long, stream_executor::DeviceMemory<Eigen::half> const&, int, long long, float, stream_executor::DeviceMemory<Eigen::half>*, int, long long, int)':\r\ncuda_blas.cc:(.text._ZN15stream_executor3gpu8CUDABlas24DoBlasGemmStridedBatchedEPNS_6StreamENS_4blas9TransposeES5_yyyfRKNS_12DeviceMemoryIN5Eigen4halfEEEixSB_ixfPS9_ixi+0x2e3): undefined reference to `cublasGemmStridedBatchedEx'\r\ncollect2: error: ld returned 1 exit status\r\n[19,054 / 21,282] 46 actions running\r\n    Compiling tensorflow/core/kernels/slice_op_gpu.cu.cc [for host]; 181s local\r\n    Compiling tensorflow/core/kernels/pad_op_gpu.cu.cc [for host]; 154s local\r\n    Compiling tensorflow/core/kernels/tile_functor_cpu.cc [for host]; 123s local\r\n    Compiling tensorflow/core/kernels/resource_variable_ops.cc [for host]; 106s local\r\n    Compiling tensorflow/core/kernels/conv_ops_fused_float.cc [for host]; 94s local\r\n    Compiling tensorflow/core/kernels/conv_ops_fused_double.cc [for host]; 91s local\r\n    Compiling tensorflow/core/kernels/conv_ops.cc [for host]; 91s local\r\n    Compiling tensorflow/core/kernels/conv_grad_ops_3d.cc [for host]; 90s local ...\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n[19,101 / 21,282] checking cached actions\r\nINFO: Elapsed time: 1270.016s, Critical Path: 336.18s\r\n[19,101 / 21,282] checking cached actions\r\nINFO: 10667 processes: 10667 local.\r\n[19,101 / 21,282] checking cached actions\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n \r\nI just search all the issues about \"undefined reference to `cublasGemmStridedBatchedEx'\", but I believe mine is different.", "comments": ["hello, I think the reason of this issue has already been figured out. The CUDA version of my system is 9.1, however tensorflow v1.15 requires at least CUDA 10.  More details could be found in https://www.tensorflow.org/install/source#common_installation_problems\r\n\r\nNow I wonder why I cannot build tensorflow 1.15 from source code in my system environment, yet I could use pip to install tensorflow 1.15 or 2.0 version?", "Today I tried to build tensorflow 1.14 on ubuntu 16.04 with CUDA 9.1 and CUDNN 7 (using nvidia/cuda:9.1-cudnn7-devel-ubuntu16.04 docker image) and my build failed with same error:\r\n\r\n```\r\nERROR: /opt/tensorflow/tensorflow/tensorflow/contrib/reduce_slice_ops/BUILD:50:1: Linking of rule '//tensorflow/contrib/reduce_slice_ops:gen_reduce_slice_ops_py_wrappers_cc' failed (Exit 1)\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sreduce_Uslice_Uops_Cgen_Ureduce_Uslice_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined reference to `cublasGemmBatchedEx'\r\nbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sreduce_Uslice_Uops_Cgen_Ureduce_Uslice_Uops_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so.1: undefined reference to `cublasGemmStridedBatchedEx'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 813.405s, Critical Path: 96.34s\r\nINFO: 4817 processes: 4817 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "For anyone facing the same issue - I was using nvidia docker container with CUDA 9.1 and CUDNN 7. I had to add following step before building tensorflow:\r\n\r\n```bash\r\necho \"/usr/local/cuda-9.1/targets/x86_64-linux/lib\" > /etc/ld.so.conf.d/cuda_targets.conf\r\nldconfig\r\n```\r\nFor whatever reason `/usr/local/cuda-9.1/targets/x86_64-linux/lib` was not there.", "Hello,\r\n\r\nI can confirm the same behavior while attempting to build 1.14 with CUDA coming from either Ubuntu 18.04 or Debian 9 with backports.\r\n\r\nAny help would be greatly appreciated,\r\n\r\nRegards, Adam.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34401\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34401\">No</a>\n"]}, {"number": 34400, "title": "Add tests for UniqueDataset", "body": "This pull request adds test cases for the `UniqueDataset` op. I also refactored the op's source code from a single `.cc` file into separate header and source files.", "comments": []}, {"number": 34399, "title": "Fix invalid shape issue in random.uniform", "body": "This PR tries to address the issue raised in #34363 where\r\ninvalid shape passed to minval/maxval (expected to be 0-D)\r\ndoes not raise an error.\r\n\r\nThe issue was that in most of the scenarios the shape was\r\nchecked inside the C++ kernel ops.\r\n\r\nHowever, in one condition math_ops.add was used which will\r\nimplicitly do broadcast when necessarily.\r\nThis results in maxval/minval's shape getting carried.\r\n\r\nThis PR adds the shape check before math_ops.add, to make\r\nsure the shape is guaranteed.\r\n\r\nThis PR fixes #34363.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Could you please check failed build errors? Thanks!", "Thanks @alextp @gbaned for the review. The PR has been updated to fix the failed tests.", "@yongtang Here is the internal error we are getting , can you please check this.\r\n\r\n> google3/third_party/py/six/__init__.py\", line 737, in raise_from\r\n    raise value\r\nInvalidArgumentError: Function invoked by the following node is not compilable: name: \"__forward_call_512\" op: \"__forward_call_512\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" input: \"dummy_input\" attr { key: \"_XlaCompile\" value { b: true } } attr { key: \"config_proto\" value { s: \"\\n\\007\\n\\003CPU\\020\\001\\n\\016\\n\\nTPU_SYSTEM\\020\\001\\n\\007\\n\\003TPU\\020\\002\\n\\007\\n\\003GPU\\020\\0002\\002J\\0008\\001\" } } attr { key: \"executor_type\" value { s: \"\" } }.\r\nUncompilable nodes:", "Thanks @gbaned, the log is quite hard to interpret, I am wondering if there are any other information I could see? For example, can I get the code snippet that related to this error?", "> Thanks @gbaned, the log is quite hard to interpret, I am wondering if there are any other information I could see? For example, can I get the code snippet that related to this error?\r\n\r\nThe test breaks because `EnsureShape` is not implemented for XLA, and with your change we generate `EnsureShape` in graphs where we wouldn't before.  Can you please add an XLA kernel for `EnsureShape` (preferably in a separate PR)?  Let me know if you need help.", "Thanks @sanjoy. I will take a look and add XLA EnsureShape kernel first.", "@yongtang Any update on this PR, please. Thanks!", "Sorry for the late reply. Was out of the office during the year end and was only back last week. I will update and work on XLA EnsureShape shortly. \r\n", "@yongtang Any update on this PR, please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@yongtang Any update on this PR and can you please resolve conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "Sorry for the long wait. I have created #38544 to fix the ensure_shape issue. Once the #38544 is merged, this PR will be re-opened for a final fix."]}, {"number": 34398, "title": "Possible bug (wrong sign) in BTRS algorithm for binomial sampling?", "body": "This is a very esoteric issue: I was reading the [BTRS paper](https://www.tandfonline.com/doi/abs/10.1080/00949659308811496) and trying to understand the algorithm by reading the [TF implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_binomial_op.cc), and I think I found a bug on line 99, in the Stirling approximation for `log(k!)`:\r\n\r\n```\r\nreturn (1.0 / 12 - (1.0 / 360 + 1.0 / 1260 / kp1sq) / kp1sq) / (k + 1);\r\n```\r\n\r\nIf I'm reading the paper correctly, the sign on the third term is wrong, because it should be positive in the final sum (see page 106-106 of H\u00f6rmann et al):\r\n\r\n```\r\nreturn (1.0 / 12 - (1.0 / 360 - 1.0 / 1260 / kp1sq) / kp1sq) / (k + 1);\r\n```\r\n\r\nOr maybe there's an error in the paper? Anyway just thought I'd mention it.\r\n", "comments": ["@jamestwebber  It appears you are correct. I verified the equation from the original paper which can found here: https://epub.wu.ac.at/1242/1/document.pdf (Page 7)\r\n\r\n@lamberta I have sent a PR request for this issue. Thank you!", "This is merged and will be updated on the site the next docs push/release.\r\nThank you, all"]}, {"number": 34397, "title": "Fix typos in gradients_impl.py code examples", "body": "", "comments": []}, {"number": 34396, "title": "Error converting universal sentence encoder to TFLite with new converter. Failed to find function '__inference_pruned_1633'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos 10.14.6\r\n- TensorFlow installed from (source or binary): pip tf-nightly\r\n- TensorFlow version (or github SHA if from source): tf-nightly==2.1.0.dev20191113\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\ntflite_convert --experimental_new_converter --saved_model_dir . --output_file use.tflite\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2019-11-18 16:34:10.641404: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-18 16:34:10.653375: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbe5ebb5250 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-18 16:34:10.653433: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-11-18 16:34:13.643728: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-11-18 16:34:13.643858: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-18 16:34:13.717700: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2019-11-18 16:34:13.717743: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 183 nodes (0), 183 edges (0), time = 19.817ms.\r\n2019-11-18 16:34:13.717754: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 183 nodes (0), 183 edges (0), time = 19.384ms.\r\n2019-11-18 16:34:13.717762: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_pruned_1633\r\n2019-11-18 16:34:13.717770: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2019-11-18 16:34:13.717778: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2019-11-18 16:34:14.576811: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-11-18 16:34:14.576933: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-11-18 16:34:14.587930: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2019-11-18 16:34:14.587975: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 183 nodes (0), 183 edges (0), time = 3.225ms.\r\n2019-11-18 16:34:14.587986: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 183 nodes (0), 183 edges (0), time = 3.633ms.\r\nTraceback (most recent call last):\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/bin/tflite_convert\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 594, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 577, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 235, in _convert_tf2_model\r\n    tflite_model = converter.convert()\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 474, in convert\r\n    **converter_kwargs)\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 457, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py\", line 203, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-11-18 16:34:16.190068: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:106] Ignored output_format.\r\n2019-11-18 16:34:16.190086: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:112] Ignored drop_control_dependency.\r\nTraceback (most recent call last):\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/caleb.p/Development/tflite-hub/.venv/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Failed to find function '__inference_pruned_1633'. The imported TensorFlow GraphDef is ill-formed.\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://tfhub.dev/google/universal-sentence-encoder/3\r\n```\r\nSince [this](https://groups.google.com/a/tensorflow.org/forum/#!topic/tflite/C7Ag0sUrLYg) announcement I thought that converting the universal sentence encoder to TFLite might be supported. Could anyone explain the reason for failure here? Thanks a lot!\r\n", "comments": ["Exact same error with this model:\r\n\r\nhttps://tfhub.dev/google/universal-sentence-encoder-large/4", "Have you tried with latest tf-nightly? Is the issue resolved?", "Same problem here as well with https://tfhub.dev/google/universal-sentence-encoder-large/5\r\n![image](https://user-images.githubusercontent.com/51969305/78123084-eeeb0d00-740d-11ea-958e-93d0ef8856bc.png)\r\n", "I want to convert universal sentence encoder to TfLite for building a semantic search engine. Can anyone help me? ", "Python code to reproduce :\r\n`import tensorflow as tf`\r\n`import tensorflow_hub as hub` \r\n`model = tf.keras.Sequential()`\r\n`model.add(tf.keras.layers.InputLayer(dtype=tf.string, input_shape=()))`\r\n`model.add(hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\"))`\r\n`converter = tf.lite.TFLiteConverter.from_keras_model(model)`\r\n`converter.experimental_new_converter = True`\r\n`tflite_model = converter.convert()`\r\n", "I am getting this same error with tensorflow 2.2.rc3\r\n\r\n```\r\nimport tensorflow_hub as hub\r\nimport tensorflow as tf \r\n\r\nmax_seq_length = 128  # Your choice here.\r\n\r\ninput_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\r\n                                       name=\"input_ids\")\r\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\r\n                                   name=\"input_mask\")\r\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\r\n                                    name=\"segment_ids\")\r\nbert_layer =  hub.KerasLayer(\"https://tfhub.dev/tensorflow/albert_lite_base/1\",\r\n                            signature=\"tokens\",\r\n                            output_key=\"pooled_output\")\r\n\r\nalbert_inputs = dict(\r\n    input_ids=input_ids,\r\n    input_mask=input_mask,\r\n    segment_ids=segment_ids)\r\n\r\npooled_output = bert_layer(albert_inputs)\r\n\r\nmodel = tf.keras.Model(inputs=[input_ids, input_mask, segment_ids], outputs=[pooled_output])\r\nmodel.compile()\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)                                \r\ntflite_model = converter.convert()\r\n```", "@r-wheeler I have the same error too, so far TFLiteConverter only works with this model for me: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1", "Update, I managed to convert some ALBERT modules using `hub.load` instead of `hub.KerasLayer`, but not sure how to use the module to create a tf.keras.Model object?\r\n\r\n```\r\nalbert_module = hub.load(\"https://tfhub.dev/tensorflow/albert_lite_base/1\")\r\n# also works with https://tfhub.dev/google/albert_base/3 and https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-128_A-2/1\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([albert_module.signatures[\"tokens\"]])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\nwith tf.io.gfile.GFile(os.path.join(\"../\", \"model.tflite\"), 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\n> @r-wheeler I have the same error too, so far TFLiteConverter only works with this model for me: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\r\n\r\n", "@wing-yiu \r\n\r\nI have some old code I think I found from a github issue closed on tensorflow_hubs github page \r\n\r\n```\r\ndef load_module(module_url, signature='tokens'):\r\n    \"\"\"Load a module in tensorflow 2\"\"\"\r\n    module = hub.load(module_url, tags=[])\r\n    log.info(f\"found signatures {module.signatures}\")\r\n    return module.signatures[signature]\r\n\r\nembedder = load_module('https://tfhub.dev/google/albert_xlarge/2')\r\n```\r\n\r\nI dont recall if a keras model actually saved the weights correctly, it may have just been a work around right when tf2 came out. \r\n\r\nThat is cool it works just as a hub layer like that", "Thanks for the reply.\r\n\r\n> Update, I managed to convert some ALBERT modules using `hub.load` instead of `hub.KerasLayer`, but not sure how to use the module to create a tf.keras.Model object?\r\n\r\nYour code didn't work for me. Failed with: `ValueError: None is only supported in the 1st dimension. Tensor 'segment_ids' has invalid shape '[None, None]'`\r\n\r\nCould you share your environment information including Tensorflow version?\r\n", "Nevermind. Forgot to use `tf-nightly`. Can confirm these models all work.", "Thanks @r-wheeler for the snippet. Does anyone know how to use this to build a keras model for text classification?", "@wing-yiu To create the model in normal `tensorflow`, see https://stackoverflow.com/questions/61595909/tensorflow-hub-load-model-to-tflite/61601166#61601166. It also tries to generate the `tflite` model, but that generates the error described in this title.   ", "If anyone is still looking to convert albert_lite_base to tflite, there's a solution here: https://github.com/tensorflow/tensorflow/issues/41119. I've also updated the [colab notebook](https://colab.research.google.com/drive/16eEDreq7F2DZLSFrQJJrrwPQ2MubKrh3) with the working solution. ", "Any update on this?\r\nI get:\r\n`ValueError: Input 1 of node functional_1/model/USE/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall was passed float from Func/functional_1/model/USE/StatefulPartitionedCall/StatefulPartitionedCall/input/_186:0 incompatible with expected resource.`", "Does the issue still exist? ", "> Nevermind. Forgot to use `tf-nightly`. Can confirm these models all work.\r\n\r\nI am closing this as this was resolved with `tf-nightly`. \r\n\r\nIf anyone still facing issue with recent `tf-nightly`, please feel free to create a new issue with a simple standalone code to reproduce the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34396\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34396\">No</a>\n", "Has anyone tried exporting https://tfhub.dev/google/universal-sentence-encoder-large/3 to tflite?"]}, {"number": 34395, "title": "TFLite GL Delegate build is leaking Foundation libraries", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.14.6 Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.15.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source): NDK 18\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n\r\nThe Foundation Library is leaking into the GL delegate on v1.15.0.  This is related to [this bug](https://github.com/abseil/abseil-cpp/issues/326), but it didn't happen on v1.14.0\r\n\r\n**Any other info / logs**\r\n```\r\nJoes-MacBook-Pro-3:gpu jbowser$ bazel build -c opt --config android_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=hidden --linkopt -s --strip always :libtensorflowlite_gpu_gl.so\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=238\r\nINFO: Reading rc options for 'build' from /Users/jbowser/tensorflow_source/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from /Users/jbowser/tensorflow_source/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python3/bin/python3.7 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages --python_path=/usr/local/opt/python3/bin/python3.7 --config=xla --action_env ANDROID_NDK_HOME=/Users/jbowser/ndk_builds/android-ndk-r18 --action_env ANDROID_NDK_API_LEVEL=18 --action_env ANDROID_BUILD_TOOLS_VERSION=29.0.2 --action_env ANDROID_SDK_API_LEVEL=29 --action_env ANDROID_SDK_HOME=/Users/jbowser/library/Android/Sdk --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:xla in file /Users/jbowser/tensorflow_source/tensorflow/.tf_configure.bazelrc: --define with_xla_support=true\r\nINFO: Found applicable config definition build:android_arm64 in file /Users/jbowser/tensorflow_source/tensorflow/.bazelrc: --config=android --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nINFO: Found applicable config definition build:android in file /Users/jbowser/tensorflow_source/tensorflow/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nINFO: Analyzed target //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so (45 packages loaded, 3483 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base /private/var/tmp/_bazel_jbowser/e70d978bebad2fab57f73474f3b8c22a/sandbox\r\nERROR: /Users/jbowser/tensorflow_source/tensorflow/tensorflow/lite/delegates/gpu/BUILD:110:1: Linking of rule '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so' failed (Exit 1)\r\nexternal/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/darwin-x86_64/lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin/ld: cannot find Foundation: No such file or directory\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 68.598s, Critical Path: 26.36s\r\nINFO: 227 processes: 227 local.\r\nFAILED: Build did NOT complete successfully```", "comments": ["Looks like the fix for this issue (04e169ac74a81b00507f1bb571513a5c2f61547f) has not been included in v1.15.0 tag. If you want to build it from v1.15.0, you could cherry-pick that patch to your local repository and build successfully.\r\n\r\n```sh\r\n$ git checkout v1.15.0\r\n$ git cherry-pick 04e169ac74a81b00507f1bb571513a5c2f61547f\r\n$ bazel build ...\r\n```\r\n\r\nCould you try this and see if this works?", "@yyoon It does work.  So...a couple of questions:\r\n\r\n1. Does this just build on Linux? \r\n2. Will there be a patch release with this fix? Or should I just create a custom tag of v1.15.0+select_ops_fix to build off of? \r\n\r\n", "1. Yes, it does work on Linux without patching that commit.\r\n2. I guess it depends on your use case. The patch commit is already released in master branch, so if you could build from master, that should be an option for you. If not, I think locally creating a new tag for yourself should also work fine.", "I can create a custom tag for this.  The reason I want to build off of v1.15.0 and a custom tag is because I want to avoid issues with v2.0.0: https://github.com/tensorflow/tensorflow/issues/34393 ", "Yes, please use a custom tag if it suits your needs. Let me close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34395\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34395\">No</a>\n"]}, {"number": 34394, "title": "Google upstream r21 with required fixes", "body": "At the time when Tensorflow 2.1 release branched out, the ROCm support was broken on the master branch.\r\nIn this PR, we cherry-pick the minimum set of PRs from what we've filed against the Tensorflow master branch onto the r2.1 release branch, to fix the ROCm CSB release builds and to unblock the further testings. \r\n\r\nCC @deven-amd , @whchung", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34394) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34394) for more info**.\n\n<!-- ok -->", "@alextp , I've rebased the previous commits to a single one with a force push, in order to pass the CLA check. \r\nCan you take a look and re-approve this PR?  "]}]