[{"number": 7236, "title": "Gradient is incorrect for log pdf of Normal distribution", "body": "Am I missing something basic here?\r\n\r\nGradients of the log pdf of normal distributions are zero. I might be going crazy, it is late at night.\r\n\r\n```\r\nIn [12]: mu = tf.get_variable('mu', [], 'float32')\r\nIn [13]: q = dist.Normal(mu, 1.)\r\nIn [14]: log_q = q.log_pdf(q.sample())\r\nIn [15]: grad = tf.gradients(log_q, mu)\r\nIn [16]: sess.run(tf.global_variables_initializer())\r\nIn [17]: sess.run(grad)\r\nOut[17]: [0.0]\r\nIn [18]: tf.__version__\r\nOut[18]: '0.12.1'\r\n```", "comments": ["Thanks to @poolio - it's because of reparameterization:\r\n\r\n```\r\neps ~ N(0, 1)\r\nz = mu + sigma * eps\r\nq.log_pdf(z) ~= -(z - mu)^2 / 2 = - (mu + sigma * eps - mu)^2 / 2 = - (sigma * eps)^2 / 2\r\n```\r\n\r\nso it makes sense that the gradient is zero. \r\n\r\nThis is very unexpected behavior and broke my regression model. \r\n\r\nDistributions should abstract parameters: a sample should be treated as a new tensor. \r\n\r\nIs there a discussion of this anywhere?", "there is some in the reparameterization type doc. generally when calculating score functions, you have to perform\r\n```python\r\ntf.gradients(q.log_prob(tf.stop_gradients(z)), mu)\r\n```\r\ni agree having to be aware of how the sampling mechanism is implemented can be unintuitive. both use cases\u2014taking gradients through samples or not through samples\u2014are useful though.", "Thanks Dustin! \r\n\r\nI think it's weird to have to remember this.\r\n\r\nFor example, if you have a Bernoulli distribution, you don't need the tf.stop_gradients. \r\n\r\nSamples from distributions behaving differently in various contexts adds a lot of complexity to my code.\r\n\r\n", "so is this expected behavior, or is it a bug in TensorFlow?", "On Fri, Feb 3, 2017 at 1:21 PM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> so is this expected behavior, or is it a bug in TensorFlow?\n>\n\nArguably yes/yes, yes/no, no/yes, no/no. :)\n\nMany of the samplers don't have analytic gradients due to being\ndynamic-loop based, eg rejection samplers.  We have a reparameterization\nannotation for each class.  However, we should consider at least triggering\na warning if backpropping into a non-reparameterizable sampler.  Noteworthy\nexample: gradient of Gamma.sample is None wrt to shape param but -x/scale\nwrt to scale param (where x~Gamma).\n\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7236#issuecomment-277365262>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABFZtiejLStlbeIAPPU7ivyEHAY-ujlhks5rY5pEgaJpZM4L17vy>\n> .\n>\n", "ok, I'll go the least effort route and interpret it as \"working as intended\" :) If this is a bug/feature request, feel free to provide more concrete details of what should be changed, and I'll reopen"]}, {"number": 7235, "title": "undefined symbol: clock_gettime on Ubuntu 12.04", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nAlmost every possible workarounds and resolutions that are existing for all of the Tensorflow-related issues that have been discussed on GitHub and StackOverflow have been tried, but the issue still persists. Since, I could not find any discussions or fixes with respect to the particular issue that I am encountering, I thought reaching out to you folks would be the best option that is available, as it would help me learn from the developers themselves.\r\n\r\n\r\n### Environment info\r\nOperating System: Ubuntu 12.04 LTS 64-bit \r\nPython : 2.7.3 built on GCC 4.6.3\r\nTensorflow : 0.12.1, CPU-only version\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): No such file or directory\r\n\r\nSince installing from the binary pip package did not work, I tried installing Tensorflow from source, the details of which are provided below :\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`) - b5854872a49172e0502856b60cbaedf0df4df087\r\n2. The output of `bazel version` - 0.4.3\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n~ $ cd /etc\r\n/etc$ python\r\nPython 2.7.3 (default, Oct 26 2016, 21:01:49) \r\n[GCC 4.6.3] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: clock_gettime\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n\r\n\r\n### What other attempted solutions have you tried?\r\nThe initial point of error was with respect to GLIBC_2.x not being found on my filesystem. With that I started troubleshooting and debugging Tensorflow configuration with the following list of attempted solutions -\r\n1. Upgrade of GCC to v4.9\r\n2. Upgrade of Bazel to v0.4.3\r\n3. Upgrade of protobuf to v\r\n3. Bazel installation via binary installer\r\n4. Bazel installation via Git Repository clone (https://github.com/bazelbuild/bazel.git), followed by a hard reset to a specific HEAD\r\n5. Tensorflow installation via the pip package directly using the .whl file\r\n6. Tensorflow installation via Git Repository clone (https://github.com/tensorflow/tensorflow), followed by a hard reset to a specific HEAD\r\n7. Bazel build with additional options such as --linkopt '-lrt' --copt '-DGPR_BACKWARDS_COMPATIBILITY_MODE' --conlyopt='-std=c99'\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n1. Execution of ./configure fetches all external dependencies successfully\r\n2. During bazel build, massive number of warning messages regarding deprecation show up on the console. However, build completes successfully with building the pip package\r\n3. No errors during bazel-bin execution\r\n4. No errors during the .whl file execution either. Successfully installs Tensorflow 0.12.1 with most of the additional requirements like numpy, protobuf, wheel, werkzeug etc., being satisfied (the .whl file is named 'tensorflow-0.12.1-cp27-cp27mu-linux_x86_64.whl')\r\n\r\n\r\nI may have missed out on mentioning slight details, but I have tried my best to cover the wide range of fixes that I have been trying on. If in case, you guys have any questions for me, please feel free to reach out as those details might help you better understand the issue being described above.\r\n\r\nJust out of curiosity as it caught my eye, I upgraded my GCC to 4.9 as was mentioned in one of the StackOverflow discussion and updated the symlinks to point to the new version. After which, I performed all of the mentioned solutions that I tried. However, I see that my Python interpreter seems to have been compiled with GCC v4.6.3. Would this be causing any kind of issues with the native runtime not being loaded?\r\n\r\nThank you so much for helping us out! Looking forward to hear from your end with your inputs on this.\r\n\r\n\r\n\r\nAdding a fellow colleague @nishithhaa to this discussion as we are facing issues during the configuration and setup of Tensorflow on our machines.\r\n", "comments": ["https://github.com/benoitsteiner/tensorflow/commit/d82328afcde4c9c8eac5ea0199f0349d209a6f51 has added dependency on time libraries. I was able to build it on Ubuntu 14.04 and MacOS Sierra. Could it be that Ubuntu 12.04 is missing this function? @aselle  -- what is the official status of support of 12.04?", "@Carmezim @yaroslavvb \r\n\r\nI did see the comment by @yaroslavvb regarding my issue. But could not see any new answers posted for his comment/question. Is there anything else that you guys would want me to try from my end? If yes, please do let me know. Or, do I just wait to hear from any of you again?\r\n\r\nI may be a little on my toes needing help. But, just getting used to GitHub and how you all work with it. Pardon my naivety here, if found or felt.\r\n\r\nThanks for your help, as always! ", "@b0noI  in case he has some insight about clock_gettime portability", "Thanks for reaching out @namrathaurs. The TensorFlow team is committed to supporting Ubuntu Linux 14 LTE to 16. Ubuntu 12.04 isn't in our support matrix. That release is actually going to be EOL in two months. The Ubuntu team is going to stop creating security updates for your system. Please consider upgrading as soon as possible.", "FYI this occurs with the GPU version on ubuntu 18.04"]}, {"number": 7234, "title": "Update release version string.", "body": "", "comments": ["Let's hold this until cherrypicks are done, there may be some awful merge conflicts\r\n"]}, {"number": 7233, "title": "Fix by Josh Levenberg for cmake build", "body": "", "comments": ["Should have no effect on py3 build. Merging."]}, {"number": 7232, "title": "Cherrypicks for 1.0 release.", "body": "", "comments": ["Looks like a flake in cmake build.\r\nJenkins, test this please."]}, {"number": 7231, "title": "Fix by Josh Levenberg for cmake build.", "body": "", "comments": []}, {"number": 7230, "title": "Branch 146408773", "body": "", "comments": ["@tensorflow-jenkins test this please.", " Failure looks like a flake caused by not using `testCase.get_temp_dir`\r\nMerging."]}, {"number": 7229, "title": "Improve upgrade script to handle list comprehensions as arguments.", "body": "python's ast module does not return the correct location, so we\r\nhave to do our best to scan backwards to find where the [ token\r\nthat trully started the list comprehension occurs.", "comments": ["I've added tests now."]}, {"number": 7228, "title": "Branch 146389949", "body": "Push internal changes.", "comments": []}, {"number": 7227, "title": "ld: file not found: -lcudart.8.0 when building CUDA on MacOS and bazel 0.4.4", "body": "In today's head https://github.com/tensorflow/tensorflow/commit/b47dc70e548e6958919f87864b83866919473a92, and bazel 0.4.4, bazel building with XLA + CUDA on MacOS fails because it can't find cuda library when linking. I was able to build with the same setup from head from Jan 24 .\r\n\r\n```\r\nbazel build --config=cuda --config=opt -k //tensorflow/tools/pip_package:build_pip_package\r\n...\r\nld: file not found: -lcudart.8.0\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 0.513s, Critical Path: 0.11s\r\n\r\n```\r\n\r\nI see `libcudart_static.a` in my `/usr/local/cuda/lib` and similar issue [suggests](http://stackoverflow.com/questions/9633881/usr-bin-ld-cannot-find-lcudart) that linking needs to be done with -L$CUDA_HOME/lib flag, but I can't figure out how to massage that option into bazel config\r\n\r\nI also tried building with flags ` --action_env PATH --action_env DYLD_LIBRARY_PATH --action_env LD_LIBRARY_PATH` and copying `libcudart_static.a` to `libcudart.8.0.a`\r\n\r\nNote that libcudart dependency dependency exists in CPU-only build as well because of https://github.com/tensorflow/tensorflow/issues/7216", "comments": ["I've seen a nearly identical issue. The major difference is that I didn't build with XLA+CUDA; I only built with CUDA.\r\n\r\nOSX 10.12.3\r\nRetina MacBookPro 10,1 mid 2012.\r\nNvidia GT 650M\r\nPython 2.7.13 build in a virtual environment from brew with the latest requirements from pip\r\nLatest build requirements from brew (wheel, six, etc)\r\nLatest CUDA 8.0.63\r\nLatest cuDNN 5.1.10\r\n\r\nThe issue only started after I brew upgraded bazel to 0.4.4 a few days ago.\r\nReverting my brew install of bazel to 0.4.3 has allowed me to complete compilations and create a pip install package. \r\n\r\nFixed as follows:\r\n`brew uninstall bazel`\r\nCopy bazel 0.4.3 brew formula from:\r\nhttps://github.com/Homebrew/homebrew-core/blob/f4c0bf580fc94d31dbe09ddeab35b1ee3ac17c9e/Formula/bazel.rb\r\nPaste this old formula into brew config for bazel:\r\n`brew edit bazel`\r\nInstall the older version of bazel\r\n`brew install bazel`\r\nPin this currently installed \"older version\" of bazel\r\n`brew pin bazel`\r\n\r\nThe only other issue was that I needed to massage bazel with the following fix:\r\nhttps://github.com/tensorflow/tensorflow/issues/4187#issuecomment-244722024\r\n\r\nFor quick reference here is the changelog of bazel:\r\nhttps://github.com/bazelbuild/bazel/blob/master/CHANGELOG.md", "I confirmed that this issue is unrelated to XLA --- it's a CUDA on Mac OS X problem:\r\nERROR: .../tensorflow/tensorflow/python/BUILD:2278:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o ... (remaining 432 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nclang: warning: argument unused during compilation: '-pthread'\r\nld: file not found: -lcublas.8.0\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n", "This problem has been possibly resolved at head, although I won't know for sure until https://github.com/tensorflow/tensorflow/issues/7364 is fixed", "I have just checked at head still getting `ld: file not found: -lcudart.8.0` error. when trying to build pip package. The main thing that changed from the last time this build worked was upgrading Bazel to 0.4.4\r\n\r\nI notice that your continuous build also [fails](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/239/console) at head with similar error\r\n\r\n```\r\nld: file not found: -lcublas.8.0\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n\r\n```", "This is a Bazel bug. Reported with a simple reproducible test case at https://github.com/bazelbuild/bazel/issues/2526\r\n\r\nBuilding works with Bazel 0.4.3 or below.", "Fixed at Bazel head."]}, {"number": 7226, "title": "conv2d gives NaN gradients with float16 input", "body": "### Environment info\r\nOperating System: Ubuntu 16 LTS\r\nbreaks already on CPU\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: recent nightly build\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\n>tf.__version__\r\n'0.12.head'\r\n>tf.__git_version__\r\n'0.12.1-2263-g4cc0d1e-dirty'\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nslim = tf.contrib.slim\r\n\r\ndtype = tf.float16\r\nshape = (4, 16, 16, 3)\r\n\r\ninpt = tf.placeholder(dtype, shape, name='input')\r\nnet = slim.conv2d(inpt, 16, [3, 3], scope='conv')\r\nloss = tf.reduce_mean(net)\r\nopt = tf.train.AdamOptimizer(1e-3)\r\ntrain_op = slim.learning.create_train_op(loss, opt)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for i in range(2):\r\n        val = np.random.randn(*shape)\r\n        print(sess.run(train_op, feed_dict={inpt: val}))\r\n```\r\n\r\nSo basically it breaks on the second step of SGD because loss is NaN. If I change dtype in float32, it works. It should have nothing to do with CUDA, I tested it on CPU version as well as on GPU with CUDA8, CuDNN5.1.\r\n\r\n### What other attempted solutions have you tried?\r\nI have no idea what to try here. Now I continue with float32.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\n-0.00072765\r\nTraceback (most recent call last):\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n    return fn(*args)\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: LossTensor is inf or nan : Tensor had NaN values\r\n         [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_HALF, message=\"LossTensor is inf or nan\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](control_dependency)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test_bn.py\", line 22, in <module>\r\n    print(sess.run(train_op, feed_dict={inpt: val}))\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: LossTensor is inf or nan : Tensor had NaN values\r\n         [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_HALF, message=\"LossTensor is inf or nan\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](control_dependency)]]\r\n\r\nCaused by op 'train_op/CheckNumerics', defined at:\r\n  File \"test_bn.py\", line 16, in <module>\r\n    train_op = slim.learning.create_train_op(loss, opt)\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 472, in create_train_op\r\n    'LossTensor is inf or nan')\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 433, in check_numerics\r\n    message=message, name=name)\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2402, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/lear/kshmelko/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): LossTensor is inf or nan : Tensor had NaN values\r\n         [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_HALF, message=\"LossTensor is inf or nan\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](control_dependency)]]\r\n```", "comments": ["It is normal to see that float16 doesn't have enough range especially in the beginning of the training. So this is an intended behavior instead of a bug...\r\n\r\nIf you want to ask around about how to train with float 16, please go to stackoverflow... Thanks.", "Are you kidding me or what? How can it not have enough capacity when we start we just one convolution?\r\n\r\nFine, let's modify an example. Choose conv layer with zero weights\r\n```\r\nnet = slim.conv2d(inpt, 16, [3, 3], scope='conv', weights_initializer=tf.zeros_initializer())\r\n```\r\nand as well zero batch:\r\n```\r\n        val = np.zeros(shape)\r\n```\r\nIt still fails. Do you imply that float16 does not have enough capacity to backpropagate on zero batch through all zero convolution? It is clearly a bug somewhere in native code. Please reopen this issue, it can't be intended behaviour.", "For others with this issue, see here:\r\nhttp://stackoverflow.com/questions/42064941/tensorflow-float16-support-is-broken\r\n\r\nSetting the Adam epsilon to 1e-4 works for me."]}, {"number": 7225, "title": "Feature request: equivalent of np.clip", "body": "`tf.clip_by_value` is missing an important property of `np.clip` which lets upper/lower bounds to be tensors, whereas `tf.clip_by_value` only takes scalars\r\n\r\nA work-around is `tf.minimum(upper, tf.maximum(lower, x))` but that uses 2x memory for gradients. Another potential work-around\u00a0is to use `tf.map_fn` + `tf.clip_by_value`, but it is orders of magnitude slower. Some profiling: https://github.com/yaroslavvb/notebooks/blob/master/clipping-profile.ipynb", "comments": ["+1", "Assigned to @vrv mainly because you wrote the original clip_by_value(). Feel free to re-assign. Thanks.", "@yaroslavvb if you look at the implementation of clip_by_value, it is exactly doing ```tf.minimum(upper, tf.maximum(lower, x))```, so presumably we could change the documentation to not mention it's only scalar arguments.\r\n\r\nYour gradient argument seems reasonable, I'm going to assume that XLA will help address these issues instead of writing custom kernels.\r\n\r\nHowever, if someone wants to write a custom CPU and GPU kernel for clip_by_value that fuses it in the short term, that would be a nice intermediate contribution.\r\n\r\n(p.s., I actually didn't write this op).", "Added a PR #13998 to add the customized kernel for `tf.clip_by_value`. Please take a look."]}, {"number": 7224, "title": "dropout does not take in dynamic shape", "body": "I realize dropout `noise_shape` does not allow dynamic shape which becomes useless in batch processing whereby the batchsize can varies\r\n```python\r\nX = tf.placeholder('float', [None, 5, 10])\r\ntf.dropout(X, keep_prob=0.5, noise_shape=[-1, 5, 1]) \r\n```\r\nthrows Exception. Whereby it requires dimension to be >= 0\r\n\r\nhowever if we pass noise_shape as the shape of X which is dynamic, dropout can understand the shape.\r\n```python\r\ntf.dropout(X, keep_prob=0.5, noise_shape=tf.shape(X))\r\n```\r\nTherefore there is inconsistency in noise_shape inputs. How do we give dynamic shapes to `noise_shape` such as `noise_shape=[-1, 5, 1]` where the batchsize dimension can be variable just like how `tf.reshape` works\r\n\r\n```python\r\ntf.reshape(X, [-1, 5, 1])\r\n```", "comments": ["There are only a few functions that understand negative arguments to shape, such as `tf.reshape`.  Generalizing every function that takes a shape to support them wherever possible would overly complicate the code (currently, `tf.dropout` just passes the shape to `tf.random_uniform`.  Since dropout already does support dynamic shape, as you note, I'll close this.", "@girving So in this case, how do we create a dropout mask on an axis (dropout the entire dimension) for batch training of variable batch size, for example in a sequence of text embeddings of dimension `(batchsize, seqlen, embed_dim)` where we want to randomly dropout some of the word embedding in the sequence, which will require  `noise_shape=[-1, seqlen, 1]`, then how we do it with the current dropout? This is a very valid question that we probably need to find a solution.", "`noise_shape=[tf.shape(...)[0], seqlen, 1]`", "Thanks,  I can make the noise_shape able to take in -1 by simply converting a -1 at i position to a tf.shape(X)[i], i can make a pull request if that works.", "@hycis There are at least two problems with your suggestion already: you're off if the noise shape is lower rank, and your solution can't handle inputs that are already tensors.  There's also the larger question that I don't think this trick is standard numpy, and I'm leery of settling on an API that isn't standardized."]}, {"number": 7223, "title": "OS/X doesn't compile Tensorflow", "body": "Not sure of the exact circumstances of this, but I get an undefined symbol when trying to load the _pywrap_tensorflow.so.\r\n\r\nThe symbol is __cpu_model.  \r\n\r\nThere is a discussion here about it: https://github.com/numpy/numpy/issues/8530\r\n\r\nIt seems that the file crc32c_accelerate.cc calls the compiler function `__builtin_cpu_supports`.  While there is a test for the presence of this at the top of the file, there might be a bug in llvm/clang where this function references a symbol called __cpu_model, which isn't compiled into the final .so file.\r\n\r\nAs a test, I compiled:\r\n```\r\n#include <stdio.h>\r\n\r\nmain() {\r\n  if (__builtin_cpu_supports(\"sse4.2\")) {\r\n    puts(\"a\");\r\n  }\r\n}\r\n```\r\n\r\nand I received:\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"___cpu_model\", referenced from:\r\n      _main in t-2e30e2.o\r\nld: symbol(s) not found for architecture x86_64\r\n```\r\n\r\nMy version of clang is:\r\n```\r\nApple LLVM version 7.3.0 (clang-703.0.31)\r\nTarget: x86_64-apple-darwin15.6.0\r\n```\r\n\r\n", "comments": ["I wonder this is related https://github.com/tensorflow/tensorflow/commit/51e5197b , it was added in latest merge", "Yes - the change occurred in commit 51e5197bb73653609f4100439664123b80126447\r\n", "cc @martinwicke  to triage to owner of 51e5197b", "Not sure what the right thing to do is.  Maybe there is a clang inbuilt preprocessor symbol which can be used to disable this on OS/X before some known working version.  or something like that.", "This is the llvm bug and fix - was a long time ago though...\r\n\r\nhttps://llvm.org/bugs/show_bug.cgi?id=25510\r\n", "In the absence of a good way to test this, we probably won't be able to suggest a fix. If you find a good way to disable it, feel free to send a PR.", "Thanks - i will.  I think a test for APPLE and a test for the particular compiler major/minor version strings.", "As noted: I made a fix.  I don't have Sierra, so I don't know if my expression catching the apple compiler is too broad.  however, I will check on that today.", "Ok - i think the fix was basically right - however it isn't fixed in the system clang 8.0.0 (which is in Sierra). \r\n\r\nSee the code around line 370 in https://github.com/darktable-org/darktable/blob/master/src/CMakeLists.txt\r\n\r\nThis references https://llvm.org/bugs/show_bug.cgi?id=25510\r\n\r\nApparently it is something of an incompatibility between libgcc and clang compiler-rt.  It is mentioned as fixed (Alina Sbirlea 2016-07-18 16:39:15 CDT This should be resolved by r275484.) but I guess it has not made it into apple clang 8.0.0.\r\n\r\nI've hunted through the compiler-rt libraries and cannot find the symbol, so I conclude that it isn't in the Apple clang yet.   I think I'll update the patch to rule out apple compilers of major 8 and below.\r\n\r\n", "@rmlarsen Assigning to you mainly because you commented in the PR. Feel free to re-assign. Thanks.", "hi - i notice that you have taken my pull request and the problem is solved.  thanks :)\r\n"]}, {"number": 7222, "title": "Add missing zero padding to microsecond field added in PR#7197", "body": "", "comments": ["Test failures are unrelated. Merging."]}, {"number": 7221, "title": "Linking of rule '@protobuf//:internal/_api_implementation.so' failed:", "body": "I am trying to build tensorflow v0.12.0 from source with bazel 0.4.3 and failed with this error:\r\n\r\n```\r\nERROR: /ui/ncsa/qiyuelu1/.cache/bazel/_bazel_qiyuelu1/ce8d1fb9ff1b7a9dbbd225fe7f0c6f52/external/protobuf/BUILD:579:1: Linking of rule '@protobuf//:internal/_api_implementation.so' failed: link_dynamic_library.sh failed: error executing command \r\n  (cd /ui/ncsa/qiyuelu1/.cache/bazel/_bazel_qiyuelu1/ce8d1fb9ff1b7a9dbbd225fe7f0c6f52/execroot/tensorflow-0.12.0 && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH= .......\r\n\r\n```\r\nI searched online and found github bazelbuild/bazel #1867 and tensorflow/tensorflow issue #2266, #5617 and #5616, but it seems the changes suggested in these issue have already been implemented in the source code of version 0.12.0. I am not sure why this problem still exists.  \r\n\r\nEnvironment info\r\nOperating System: RHEL 6.8\r\nCUDA: 8.0\r\ncuDNN: 5.1\r\nJava: 1.8.0_112\r\ngcc: 4.9.2\r\nbazel: 0.4.3\r\ntensorflow: v0.12.0 (Latest commit c62a66b on Dec 19, 2016)\r\n\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nI tried tensorflow version 0.12.head and v1.0.1-rc0, both of them can be built successfully. However, we do need this specific version 0.12.0.\r\n\r\n", "comments": ["Have you tried changing the TF_BINARY_URL link to the version 0.12.0? Or do you need that specific commit?\r\n\r\nExample:\r\n\r\n> \\# Ubuntu/Linux 64-bit, GPU enabled, Python 3.4\r\n> \\# Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see \"Installing from sources\" below.\r\n> $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp34-cp34m-linux_x86_64.whl\r\n\r\nThe variable becomes:\r\n> $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0-cp34-cp34m-linux_x86_64.whl\r\n\r\nAnd then you just have to follow the installation with pip. ;-)\r\n\r\nEDIT: This is not a solution to the problem, just a workaround.", "@MicaelCarvalho Thanks for your response. \r\nUnfortunately, we need to build this specific version from source because, as we tested, keras, using tensorflow as backend, has some problem with newer (0.12.head) versions. I have to roll back older version like 0.12.0. ", "@qiyuelu1 the proposed approach will install version 0.12.0... if you only need \"older version like 0.12.0\" it should work; the only situation for which it won't do the job is if you really need _that specific commit_.", "@MicaelCarvalho Thanks for your response. Actually, installing binary directly doesn't work for me. I am porting tensorflow on a cluster which is running RHEL 6.8, its default gcc is 4.4.7 (I use 4.9.2 in my build as a module). I have to build from source and make modifications on CROSSTOOL.tpl etc. files, otherwise, will get glibc version complaints.    ", "@girving Assign to you mainly because you have comments in the mentioned issues. Feel free to re-assign. Thanks.", "@jmchen-g Please use the who-do-I-notify sheet.  I do not know about build issues.  @martinwicke.", "@qiyuelu1 - can you post the full link error you get if you build with -s?", "@r4nt Thanks for your response. This problem has been bypassed through updating keras+tensorflow to newer versions. \r\nThanks all for your help. I'm gonna close this session."]}, {"number": 7220, "title": "tf.pow(x,y) doesn't compute complex results", "body": "Slightly related to #7170 \r\n\r\n**Operating System:** Debian 4.8.15-2\r\n**Installed version of CUDA:** 8.0\r\n**Installed version of cuDNN:** 5.1.5\r\n**The output of `ls -l /path/to/cuda/lib/libcud*`:**\r\n> myuser@mymachine:/mypath$ ls -l /usr/local/cuda-8.0/lib64/libcud*\r\n> -rw-r--r-- 1 root root 558720 sept. 15 01:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\n> lrwxrwxrwx 1 root root     16 sept. 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\n> lrwxrwxrwx 1 root root     19 sept. 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n> -rw-r--r-- 1 root root 415432 sept. 15 01:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n> -rw-r--r-- 1 root root 775162 sept. 15 01:02 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\n\r\n**A link to the pip package you installed:** Lost in history. I don't think this is the problem, so I'm going to skip it.\r\n\r\n**The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`:**\r\n> myuser@mymachine:/mypath$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n> 0.12.1\r\n\r\n**Minimal reproducible example:** (using python3)\r\n```\r\nimport tensorflow as tf\r\nsession = tf.InteractiveSession()\r\ntf.pow(-83.56,2.0).eval() # this one works fine\r\ntf.pow(-83.56,1.0).eval() # this one works fine\r\ntf.pow(-83.56,1.5).eval() # nan\r\n```\r\n\r\nThe docs aren't clear whether it should compute complex results or not \u2014 but it states the function accepts complex inputs, so I assume returning \"nan\" is not expected.", "comments": ["Note that definition of `Pow` op means that all types are the same\r\n\r\n```\r\nREGISTER_OP(\"Pow\")\r\n    .Input(\"x: T\")\r\n    .Input(\"y: T\")\r\n    .Output(\"z: T\")\r\n```\r\n\r\n`_op_def_lib.apply_op` is not smart enough to figure out that -83.56^1.5 is complex, so that all inputs must also be treated as complex. This is similar https://github.com/tensorflow/tensorflow/issues/7170 and the lesson is that except for a few common special cases, you must specify `dtype` explicitly.\r\n\r\nThis works\r\n\r\n```\r\na = tf.constant(-83.56, dtype=tf.complex64)\r\nb = tf.constant(1.5, dtype=tf.complex64)\r\ntf.pow(a, b).eval()   #=> (9.1085985e-06-763.83142j)\r\n```\r\n"]}, {"number": 7219, "title": "Branch 146347092", "body": "", "comments": ["@martinwicke Any thoughts on the test failure of parser_test under Python 3?\r\n\r\n> FAIL: //tensorflow/tools/docs:parser_test (see /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/tensorflow/tools/docs/parser_test/test.log).\r\n> INFO: From Testing //tensorflow/tools/docs:parser_test:\r\n> ==================== Test output for //tensorflow/tools/docs:parser_test:\r\n> ....FF.....\r\n> ======================================================================\r\n> FAIL: test_generate_index (__main__.ParserTest)\r\n> ----------------------------------------------------------------------\r\n> Traceback (most recent call last):\r\n>   File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/parser_test.runfiles/org_tensorflow/tensorflow/tools/docs/parser_test.py\", line 287, in test_generate_index\r\n>     self.assertTrue('a_method' not in docs)\r\n> AssertionError: False is not true\r\n> \r\n> ======================================================================\r\n> FAIL: test_generate_markdown_for_class (__main__.ParserTest)\r\n> ----------------------------------------------------------------------\r\n> Traceback (most recent call last):\r\n>   File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/tensorflow/tools/docs/parser_test.runfiles/org_tensorflow/tensorflow/tools/docs/parser_test.py\", line 120, in test_generate_markdown_for_class\r\n>     self.assertTrue('a_method(arg=\\'default\\')' in docs)\r\n> AssertionError: False is not true\r\n> \r\n> ----------------------------------------------------------------------\r\n> Ran 11 tests in 0.004s\r\n> \r\n> FAILED (failures=2)\r\n> ================================================================================", "Tests are re-running on the other push. Sorry, I couldn't finish it last night."]}, {"number": 7218, "title": "get GMM weight", "body": "save GMM model mixture weight and read out from checkpoint file", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please\r\n", "The filing saver_test is unrelated. Merging."]}, {"number": 7217, "title": "Error while creating quantized graph using bazel tool", "body": "Hi, \r\nI am trying to quantize the already stripped and retrained inception graph using the following bazel commands, also I am using docker environment in ubuntu 16.10:\r\n\r\nbazel build tensorflow/contrib/quantization/tools:quantize_graph\r\nbazel-bin/tensorflow/contrib/quantization/tools/quantize_graph \\    \r\n  --input=/face_tf_train/new_retrained_graph.pb \\\r\n  --output_node_names=final_result \\\r\n  --output=/face_tf_train/new_stripped_quantized_graph.pb \\\r\n  --mode=eightbit  \r\nI am bit new in python and tensorflow, I am not sure what is happening. I want to use the quantized graph for ios mobile app which I am able to compile and run. \r\nAny help could be very much appriciated. \r\n\r\nI am getting folowing error in the command promt : \r\nroot@f098a9662116:/tensorflow# bazel build tensorflow/contrib/quantization/tools:quantize_graph\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nINFO: Found 1 target...\r\nTarget //tensorflow/contrib/quantization/tools:quantize_graph up-to-date:\r\n  bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph\r\nINFO: Elapsed time: 37.731s, Critical Path: 24.75s\r\nroot@f098a9662116:/tensorflow# bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph \\    \r\nTraceback (most recent call last):\r\n  File \"/tensorflow/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/contrib/quantization/tools/quantize_graph.py\", line 1003, in <module>\r\n    tf.app.run()\r\n  File \"/tensorflow/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/tensorflow/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/contrib/quantization/tools/quantize_graph.py\", line 985, in main\r\n    data = f.read()\r\n  File \"/tensorflow/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 101, in read\r\n    compat.as_bytes(self.__name), status)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/tensorflow/bazel-bin/tensorflow/contrib/quantization/tools/quantize_graph.runfiles/org_tensorflow/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors.FailedPreconditionError: .\r\n\r\n\r\nRegards,\r\nPankaj Wasnik\r\n", "comments": ["Hi, \r\nCan you please look into the issue ?\r\n\r\nRegards,\r\nPankaj", "@petewarden please take a look, this error comes up trying to quantize any model at the moment. ", "im facing the similar issue , is there any fix", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "I am facing the very same"]}, {"number": 7216, "title": "Cannot build Tensorflow with XLA, but without GPU support", "body": "I believe it is probably commit 191658d54f90ac03c15b339326129cd52d1f56a3\r\n\r\nTo be clear, this is when I build without configuring CUDA at the ./configure stage.   I have not tried to configure with CUDA but then build without the --config=cuda.\r\n\r\nHere is the final few lines of the build output:\r\n```\r\nout/local-opt/bin/external/png_archive/libpng.pic.a bazel-out/local-opt/bin/external/zlib_archive/libzlib.pic.a -lcublas -lcuda -lcudnn -lcufft -lcurand '' -Wl,-exported_symbols_list tensorflow/tf_exported_symbols.lds -ldl -lm -Wl,-rpath,../local_config_cuda/cuda/lib -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib -ldl -lpthread -pthread -lm -framework IOKit -lstdc++ -lm -undefined dynamic_lookup -headerpad_max_install_names): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nclang: warning: argument unused during compilation: '-pthread'\r\nld: file too small (length=0) file 'bazel-out/local-opt/bin/_solib_darwin/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudart.dylib' for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nYou can see that the cuda libraries are included in the build, and that the linker is complaining that the .so (.dylib) files are empty, which is true.\r\n\r\nMaybe this is a problem with the OS/X linker  which might object to empty .so files where other operating systems' linkers do not.\r\n\r\nFor me,  this is is a high priority issue.", "comments": ["Further to the earlier comment, the problem occurs on Linux too. \r\n\r\n```\r\nbazel-out/local-opt/bin/_solib_k8/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib/libcudart.so: file not recognized: File truncated\r\n```\r\n", "This is the target that I am building\r\n\r\n```\r\nbazel build --verbose_failures --config=opt tensorflow/tools/pip_package:build_pip_package\r\n```", "I can reproduce the problem on Linux, and I asked the author of that change to take a look.\r\n\r\n(We don't have any continuous builds for the combination of XLA enabled and GPU disabled, which is why no-one noticed.)\r\n\r\nThe easiest workaround would be to install the CUDA libraries and build with GPU enabled. You don't actually need to have a GPU to do this.", "I've confirmed that undoing the changes to the dependencies in 191658d54f90ac03c15b339326129cd52d1f56a3 will make the build succeed again.\r\n\r\nI think that the problem is the target :cuda_platform is referred to in a target called :stream_executor_cuda without being tested for CUDA config.\r\n\r\nThis is as opposed to its use in the macro tf_cuda_library, where it isn't included when cuda isn't enabled.\r\n\r\nThe stream_executor_cuda is always included in the gpu_executable and gpu_plugin targets in XLA, which are in turn always included in various other targets that are included in XLA independent of whether CUDA is enabled.\r\n\r\nI suspect that either the stream_executor_cuda should be a tf_cuda_library, or something like that.\r\nOr maybe it is the higher targets, like gpu_plugin which should be tf_cuda_library type targets.", "I don't really want to have to install the CUDA libraries.  However, I can temporarily remove the offending dependencies from stream_executor_cuda while the problem is fixed.  \r\n\r\nthanks for the response\r\n", "Sure. I just wanted to make sure you weren't blocked by this issue; it might take a few days to get the fix checked in.", "no problem.  all ok here.", "Without those deps, I can build the system, but it is failing to run the unit tests because of a lack of a symbol (__cpu_model) - i'll treat this as a separate problem though", "btw, building with XLA *with* GPU support is currently blocked by https://github.com/tensorflow/tensorflow/issues/7227", "Assigned to Peter fow now. Feel free to re-assign. Thanks.", "This issue was fixed by commit  https://github.com/tensorflow/tensorflow/commit/edc1bf117b88059b92d7500953615f417122950a .\r\n"]}, {"number": 7215, "title": "Incorrect documentation on log_loss", "body": "1) I noticed that [1] states that tf.contrib.losses.log_loss is deprecated and that tf.losses.log_loss should be used instead. However in v0.12 that function does not exist and the documentation in [2] does not talk about this. So which one is correct?\r\n2) Furthermore it seems that tf.contrib.losses.log_loss only returns a scalar for the summed loss of all dimensions. Is there a function more like tf.nn.softmax_cross_entropy_with_logits (but without the softmax), that returns the loss for each element separately?\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard6/tf.contrib.losses.log_loss.md\r\n[2] https://www.tensorflow.org/api_docs/python/contrib.losses/other_functions_and_classes#log_loss", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. In particular, we do not intend the documentation at `master` to describe the differences with previous versions of TensorFlow `contrib.  Thanks!"]}, {"number": 7214, "title": "tf_upgrade do not update concat, tf.image.resize_images", "body": "There are 2 issues about `concat`:\r\n- used `concat_dim` instead of `axis`.\r\n- Incorrect position of the explicit signature if the parameter is `[...]`\r\ne.g.\r\n```\r\ntf.concat(0, [tf.concat(0, tiles[y]) for y in range(4)])\r\n```\r\nbecomes\r\n```\r\ntf.concat(concat_dim=0, [values=tf.concat(concat_dim=0, values=tiles[y]) for y in range(4)])\r\n```\r\n\r\n<br/><br/>\r\n\r\nThe `height` and `width` signature of `tf.image.resize_images` has been changed to `size`. But it is not updated the current converter.\r\n\r\n", "comments": ["After a little investigation, I found that the second issue of `concat` is related to the `col_offset` of the `ast` nodes. When the parameter is a list, the `col_offset` does not include the `[`.\r\n\r\nApparently, using `ast` with `col_offset` is not the best way to calculate the position of the arguments.", "Looking...", "Good catch. While col_offset isn't perfect, I think using ast is already more reliable than using a regex or other simple search.  In particular, as you discovered, it looks like a bug in col_offset for list comprehensions. Generators, tuples, (even plain lists) work fine.  I can work around this by scanning backward to find the opening [ if it is a list comprehension, but that may be difficult to do in the presence of comments and multilines...\r\n```python\r\ntf.concat([       # cool [ ]\r\nx for x in blah], b)\r\n```\r\nso i'll probably limit it to a sole whitespace allowing scan backwards and throw an error otherwise.\r\n\r\nAs far as the `tf.image.resize_images` that happened between 0.10 and 0.11, so it is currently outside the scope of what we targeted with the script. We would welcome a contribution to improve the converter to handle it. ", "Hi @aselle , thanks for the reply. You are right. Using the `ast` would have the limitation about the position of a list-like object and problem about preserving the comments. I think you have tried to preserve the comment as much as possible. \r\n\r\nI have tried other methods to convert an inline method -> ast -> transform -> unparsing -> replace the original method by using `ast. NodeTransformer`. The transformation could be much easier especially working on the cases with changing signature, like `tf.image.resize_images` and `tf.reverse`. But apparently, this will lost all the inline comments like what you have mentioned.\r\n\r\nAt the moment, I am looking for some `ast` alternative methods like using Redboran (`pip install redbaron`). Is there any requirement of what kind of library can be used in the Tensorflow project?\r\n", "Hi @aselle , based on the thinking I have mentioned. I have created a demo in [#7254](https://github.com/tensorflow/tensorflow/pull/7254) that use RedBaron to parse the source into \"Full Syntax Tree\" with comments and formatting preserved. Then transform the tree nodes based on the rules and finally unparse the tree back to source code.\r\n\r\nThe `tf_update2.py` is a working prototype. It can already convert a single file with all the rules defined in `tf_update.py`. It has also fixed the issue I raised here. It also support converting `tf.reverse` and `tf.image.resize_images`.\r\n\r\nIf you think this method I am using is suitable for the project. I will continue to work on the remaining features like handling folder tree and create `report.txt`.\r\n", "Thanks for looking at this in detail. Sorry I missed your comments from before. In any case, I was able to fix the handling the list comprehensions in #7229.  A few thoughts on RedBaron vs ast.\r\n\r\n* One of the goals of the upgrade script was to have minimal extra dependencies. I'd ideally like people not to have to install extra python packages to use it. That's one reason why I used ast instead of an external library. \r\n\r\n* We've tested the ast approach a bunch, and so I'd rather not have a complete rewrite go out at this point (since our RC0 is coming out today!)\r\n\r\n* There is a disclaimer in RedBaron docs  \"**Disclamer**: RedBaron (and baron) is **working** with python3 but it NOT fully parsing it yet.\"\r\n\r\n* There are definitely transformations like the image update that is hard to do with the ast approach.\r\n\r\nAll that being said, I'm happy to insert in our README.md to point users to the new script in your repo.\r\n\r\n(cc @martinwicke.)\r\n", "Hi @aselle, I totally understand and that is exactly why I asked you if it is allowed to use RedBaron. I have finished all the coding to convert the original `tf_update.py` to use the new method. It can now support all the original command line parameters and will generate `report.txt` after the conversion.\r\n\r\nI have uploaded it to a standalone repository [tf0to1](https://github.com/machrisaa/tf0to1). May you insert this into your README.md? I would be really appreciated if you can. Thanks.\r\n", "I have a PR out to add it into README.md. Thanks for your contribution.\n-A\n\n\nOn Sat, Feb 4, 2017 at 8:58 AM Chris <notifications@github.com> wrote:\n\n> Hi @aselle <https://github.com/aselle>, I totally understand and that is\n> exactly why I asked you if it is allowed to use RedBaron. I have finished\n> all the coding to convert the original tf_update.py to use the new\n> method. It can now support all the original command line parameters and\n> will generate report.txt after the conversion.\n>\n> I have uploaded it to a standalone repository tf0to1\n> <https://github.com/machrisaa/tf0to1>. May you insert this into your\n> README.md? I would be really appreciated if you can. Thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7214#issuecomment-277459239>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAT52sa6fofRcV224yZPWFVPOXooqaNRks5rZK4kgaJpZM4L079y>\n> .\n>\n", "Thank you very much!", "Looks like the readme has been updated in https://github.com/tensorflow/tensorflow/pull/7334. Closing this issue. Thanks!"]}, {"number": 7213, "title": "TensorFlow Linear Model Tutorial code", "body": "Not sure this is right place for my question...\r\n\r\nAt any rate, I'm trying to follow the Linear model [tutorial ](https://www.tensorflow.org/tutorials/wide/), there is a code link there but leads to the wide and deep code , what am I missing here?", "comments": ["OK, now I see it's the same code"]}, {"number": 7212, "title": "tf.import_graph_def prepends 'import' to tensor names but graph_util.convert_variables_to_constants can't find the tensors", "body": "I am trying to merge two graph and encountering this issue.\r\n```\r\n\r\n# This is graph from session till now\r\ng_current_def = sess.graph.as_graph_def()\r\nwith tf.Graph().as_default() as g_2:\r\n    # Add another tensor which will be new input\r\n    prepend_input = tf.placeholder(tf.string, name='prepend_input')\r\n    # pass this new input tensor as input to imported graph\r\n    output_tensor , = tf.import_graph_def(g_default_def, input_map={\"input_tensor\": prepend_input}, return_elements=['output_tensor']) #This operation prepends 'import' to all tensor names in default graph\r\n    g2_default_def = g_2.as_graph_def()\r\n    # Convert merged graph into graph_def\r\n    output_graph_def = graph_util.convert_variables_to_constants(sess, g2_default_def, [\"import/\" + FLAGS.final_tensor_name])\r\n```\r\n\r\nThis throws error\r\n\r\n```\r\nValueError: Fetch argument u'import/final_training_ops/weights/final_weights:0' cannot be interpreted \r\nas a Tensor. (\"The name 'import/final_training_ops/weights/final_weights:0' refers to a Tensor which d\r\noes not exist. The operation, 'import/final_training_ops/weights/final_weights', does not exist in the\r\n graph.\")\r\n```\r\n\r\nwhereas tensor 'final_training_ops/weights/final_weights:0' is declared in original graph.", "comments": ["@nickj-google Looks like you touched the `graph_util.convert_variables_to_constants` function recently.  Do you know what's going on here?", "Sorry, no insight into what's going on here. I don't think my changes should result in the error @prats226 is seeing.", "@martinwicke Who's the go-to person for graph_util?  I can't find it in the doc.", "\u200bmaybe @petewarden?\n", "Any thoughts @petewarden?", "You could try setting the name argument of import_graph_def explicitly to \"\", for example:\r\n```\r\noutput_tensor , = tf.import_graph_def(g_default_def, input_map={\"input_tensor\": prepend_input}, return_elements=['output_tensor'], name=\"\")\r\n```", "No response after a few weeks, so I'm closing this for now. Please reopen with more information if this is incorrect."]}, {"number": 7211, "title": "Docfix: Document usage of --config=opt for SIMD instruction sets.", "body": "", "comments": ["btw, --march=native doesn't seem to enable all optimizations, so there's some confusion as to what flags need to be set (someone's getting 30% speedup by adding fma/avx/avx2/mpmath flags in addition to march=native) http://stackoverflow.com/questions/41996006/should-i-enable-simd-extensions-together-with-cuda-support-when-building-tensorf)", "See comment on SO thread -- It's all good."]}, {"number": 7210, "title": "Tensorflow android does not recognize imported networks(.pb files) with python operations ", "body": "tensorflow android does not recognize operations from an imported network(.pb files) defined/created with python operations  like  recogonize tf.python.ops.FIFOQueue . Works when the network is built/defined with tf.FIFOQueue. Similarly the tensorflow android implementation cribs about unrecognized operation RECIPROCAL when running inference for a network described [here](https://github.com/davidsandberg/facenet/blob/master/src/facenet_train.py)\r\n\r\n", "comments": ["@cvmlarun Since code size is precious on Android, we strip out a lot of ops by default.  \r\n\r\n@petewarden Is there a clean way of reenabling the ops he wants?  `tf.reciprocal` might be something we want to turn on always.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7209, "title": "Branch 146325723", "body": "", "comments": ["The errors related to parser_test look weird. They stack trace do not reflect the state of the parser.py in the staging master branch. Maybe there's some branch/commit mess up here. I've created another push:\r\nhttps://github.com/tensorflow/tensorflow/pull/7219", "I added some commits to this to fix some of these failures, that's why it's out of sync with staging."]}, {"number": 7208, "title": "Minimal changes in typo applied", "body": "Added minimal typo changes in README.md", "comments": ["Can one of the admins verify this patch?", "I dunno, that kind of makes it more verbose when it's clear already"]}, {"number": 7207, "title": "libcupti.so missing from Docker.gpu", "body": "### Environment info:\r\nThe latest docker GPU image.\r\n`gcr.io/tensorflow/tensorflow:latest-gpu`\r\n\r\n\r\n\r\n### Issue:\r\nWhile running [mnist_with_summaries.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py), you get \r\n`tensorflow/stream_executor/dso_loader.cc:119] Couldn't open CUDA library libcupti.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:`\r\n\r\n### Temporary Solution:\r\nAdding the path to CUPTI to the LD_LIBRARY_PATH environment variable. \r\n\r\n`export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\"`\r\n\r\n### Solution:\r\nAdd `ENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH` to [Dockerfile.gpu](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.gpu), as in #7206 \r\n\r\n", "comments": ["The fix is already merged so closing this..."]}]