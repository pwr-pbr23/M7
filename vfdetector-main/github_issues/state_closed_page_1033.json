[{"number": 22334, "title": "[tflite] fix calculating of output pixels", "body": "fix an issue reported by issue #22310", "comments": ["Can u quickly fix the clang format?\r\n\r\nhttps://source.cloud.google.com/results/invocations/2f645f3a-19b6-4ef3-a028-6629e673cae8/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_clang_format_out/log\r\n\r\ndiff --git a/tensorflow/contrib/lite/examples/label_image/bitmap_helpers_impl.h b/tensorflow/contrib/lite/examples/label_image/bitmap_helpers_impl.h\r\nindex 7e09d4bc79..21ad39a6bf 100644\r\n--- a/tensorflow/contrib/lite/examples/label_image/bitmap_helpers_impl.h\r\n+++ b/tensorflow/contrib/lite/examples/label_image/bitmap_helpers_impl.h\r\n@@ -80,8 +80,7 @@ void resize(T* out, uint8_t* in, int image_height, int image_width,\r\n   interpreter->Invoke();\r\n   auto output = interpreter->typed_tensor<float>(2);\r\n-  auto output_number_of_pixels =\r\n-      wanted_height * wanted_width * wanted_channels;\r\n+  auto output_number_of_pixels = wanted_height * wanted_width * wanted_channels;\r\n   for (int i = 0; i < output_number_of_pixels; i++) {\r\n     if (s->input_floating)", "@qlzh727 thanks. clang-format done"]}, {"number": 22333, "title": "TensorFlowlite  \u52a0\u8f7d\u6a21\u578b\u62a5 it is probably compressed", "body": "android\u7248\u672c5.1\r\n\u7cfb\u7edf\u7248\u672cWindows\r\n\u8f6f\u4ef6\u7248\u672candroidstudio3.0.1\r\n\u6a21\u578b yolo_v2_7_448.tflite\r\n\u7a0b\u5e8f\u8c03\u8bd5\u5230AssetFileDescriptor fileDescriptor = assets.openFd(modelFilename)\u65f6\u62a5\u9519\uff0c\u201dThis file can not be opened as a file descriptor; it is probably compressed\u201d", "comments": ["I am facing the same issue\r\nI referred [TFLite Poets 2](https://github.com/googlecodelabs/tensorflow-for-poets-2) example and [TFLite using Keras](https://www.youtube.com/watch?v=MZx1fhbL2q4)\r\n\r\nI have _**linear.tflite**_ file in **android assets** directory\r\nI am able to generate linear.tflite model but while accessing it from loadModel() method I am getting\r\n```\r\njava.io.FileNotFoundException: This file can not be opened as a file descriptor; it is probably compressed\r\n```\r\n\r\nat line\r\n\r\n```\r\nAssetFileDescriptor fileDescriptor = activity.getAssets().openFd(\"linear.tflite\"); // <- This Line\r\n```\r\n\r\n```\r\nprivate MappedByteBuffer loadModelFile(Activity activity) throws IOException {\r\n        AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(\"linear.tflite\"); // <- This Line\r\n        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n        FileChannel fileChannel = inputStream.getChannel();\r\n        long startOffset = fileDescriptor.getStartOffset();\r\n        long declaredLength = fileDescriptor.getDeclaredLength();\r\n        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n    }\r\n```\r\n\r\nIs there any other way to load **_tflite_** model to run prediction ?", "@adityakamble49 Hi, can you try loading your model using tf.contrib.lite.Interpreter.", "@sss0636 try adding the following in build.gradle in Android Studio then sync gradle:\r\nandroid{\r\n    aaptOptions {\r\n        noCompress \"tflite\"\r\n        noCompress \"lite\"\r\n}", "@adityakamble49 : Try wt-huang's suggestion, I don't think this is related to #21245 ", "> @ sss0636\u5c1d\u8bd5\u5728Android Studio\u4e2d\u7684build.gradle\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u5185\u5bb9\u7136\u540e\u540c\u6b65gradle\uff1a\r\n> android { \r\n> aaptOptions { \r\n> noCompress\u201ctflite\u201d \r\n> noCompress\u201clite\u201d \r\n> }\r\n\r\n\u95ee\u9898\u5df2\u89e3\u51b3", "Thanks @wt-huang , @shashishekhar. It worked. able to load model now\r\n```\r\nandroid{\r\naaptOptions {\r\nnoCompress \"tflite\"\r\nnoCompress \"lite\"\r\n}\r\n```"]}, {"number": 22332, "title": "Alternative to http://www.image-net.org/challenges/LSVRC/ for tflite accuracy tool?", "body": "I was looking into using the tflite accuracy tool provided here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/accuracy/ilsvrc. The documentation mentions that we need to download the devkit from http://www.image-net.org/challenges/LSVRC/. Although, it seems like image-net.org has been down for several days.\r\n\r\nIs there any alternative repository for the same data which I can use to run the tflite accuracy tool?\r\n", "comments": ["@pritamdamania87 You can use the site [academic torrents](http://academictorrents.com/browse.php?search=imagenet) to download the data."]}, {"number": 22331, "title": "broken link in documentation for autograph", "body": "This page from docs links to the limitations, but in the wrong subfolder:\r\n\r\nhttps://www.tensorflow.org/guide/autograph\r\n\r\nWRONG PATH:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/autograph/LIMITATIONS.md\r\n\r\n> AutoGraph helps you write complicated graph code using normal Python. Behind the scenes, AutoGraph automatically transforms your code into the equivalent TensorFlow graph code. AutoGraph already supports much of the Python language, and that coverage continues to grow. For a list of supported Python language features, see the Autograph capabilities and limitations.\r\n\r\nBut the original Jupyter Notebook refers to the markdown file in the correct subfolder:\r\n\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/guide/autograph.ipynb\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/LIMITATIONS.md\r\n\r\n", "comments": ["@denfromufa Thank you for your post. Yes you are right about the pointing of autograph.ipynb link, we have moved the notebooks to the tensorflow/docs repository. This update in the Readme file as well. I am closing this issue as you can access the notebooks from the tensorflow/docs repository.\r\n@MarkDaoust Can you please update the documentation link with correct address."]}, {"number": 22330, "title": "Eigenvalue decomposition of asymmetric matrices", "body": "There is no general eigenvalue decomposition available in TensorFlow.  ```tf.self_adjoint_eig``` currently only supports self-adjoint matrices.  A full implementation of the eigenvalue decomposition for more general (diagonalizable) matrices could be very useful.  Even a version without gradients would be helpful.\r\n", "comments": ["@rmlarsen any update?", "@mbrubake,\r\nSorry for the delayed response. A full implementation of the **`eigenvalue decomposition`** is done via [tf.linalg.eigh](https://www.tensorflow.org/api_docs/python/tf/linalg/eigh). Can you please confirm if this is what you are looking for, so that we can close this issue? Thanks!\r\n", "`tf.linalg.eig` now provides this functionality "]}, {"number": 22329, "title": "Error when loading model from InputStream in Tensorflow Lite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Samsung Galaxy, android 6.0 API 23 \r\n- **TensorFlow installed from (source or binary)**: gradle\r\n- **TensorFlow version (use command below)**: tensorflow-android:1.9.0\r\n- **Python version**: no python\r\n- **Bazel version (if compiling from source)**: no bazel\r\n- **GCC/Compiler version (if compiling from source)**: no compiler\r\n- **CUDA/cuDNN version**: no version\r\n- **GPU model and memory**: Mobile\r\n- **Exact command to reproduce**: initialize tensorflowInferenceInterface with a InputBuffer > 17 MO\r\n\r\n### Describe the problem\r\nI use Tensorflow Lite since Tensorflow mobile is not ready for production yet.\r\n\r\nWhen I tried to directly load Inception V3 from a file from External storage, it returns a OOM error\r\n\r\nBy looking at the code, I saw : \r\n      `byte[] buf = new byte[16384];` \r\nat the line 143 of TensorflowInferenceInterface\r\n\r\nbut my network uses more than 16384\r\nI think it should use baosInitSize\r\n      `int baosInitSize = is.available() > 16384 ? is.available() : 16384;` at line 140\r\n\r\n### Source code / logs\r\njava.lang.OutOfMemoryError: Failed to allocate a 87514870 byte allocation with 16773184 free bytes and 29MB until OOM\r\n", "comments": ["`TensorflowInferenceInterface` is actually a component of TensorFlow Mobile. If you want to use TensorFlow Lite (which *is* ready for production, and is the officially recommended path for mobile deployment), you can follow this [API guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/apis.md#java) and this [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/demo_android.md).\r\n\r\nAs for the issue you're facing, the code in `TensorFlowInterface` looks fine; `buf` is intentionally restricted in size to avoid extremely large temporary buffer allocations. Note the way it's used as a streaming buffer. Have you tried setting the `largeHeap` [manifest attribute](https://developer.android.com/guide/topics/manifest/application-element#largeHeap)?\r\n\r\nIn any case, I would strongly encourage taking a look at this [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/demo_android.md) for TensorFlow Lite; it uses MobileNet for classification, a model specifically optimized for resource-constrained devices, and should have a much smaller memory (and latency) footprint than Inception. TensorFlow Lite also allows you to use a MappedByteBuffer when providing the model, which avoids using the JVM heap entirely (see this [snippet of code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L183))."]}, {"number": 22328, "title": "Fixed bug in estimator.py where [estimator]._distribution was called \u2026", "body": "\u2026instead of [estimator]._train_distribution causing an AttributeError.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Could u please also update the unit test?", "Could you point me to where the previous unit test for this is, if it exists? Looked like there was a TODO for @yuefengz  to write the initial one.", "Ping @yuefengz for review.", "Nagging Reviewer @yuefengz: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "I think we've merged two similar PRs a while ago. Could you update your repository please? ", "Looks like on master the estimator code no longer exists here, however the bug is still present in the new location: https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/estimator.py#L1336", "The new estimator has code `self._train_distribution.unwrap(per_device_hook)[0]`. Am I looking at the right file?", "No, that looks correct - I must have been looking at a different version and thinking it was the current master. Which Tensorflow version is/will be the first to use this estimator class?", "@karmel , could you comment on TensorFlow version to use the Estimator repo?", "@asaquib , can u resolve the merge conflict? Thanks.", "@case540 -- should PRs go through tensorflow/estimator? Or here?", "tensorflow/estimator. If this change is rebased, you will see obvious errors/issues. These files now live in https://github.com/tensorflow/estimator so please create a new PR there"]}, {"number": 22327, "title": "Fixed broken links", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n\r\nI signed it!", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "Hi @Shujian2015, \r\n\r\nThanks for the fix. \r\n\r\nBut we usually don't go back and fix docs old release snap-shots.\r\n\r\nCan you re-make this PR against master (if the issue still exists)?", "Hi @MarkDaoust, the links in master are still broken. I made a PR here: https://github.com/tensorflow/tensorflow/pull/22391\r\nThanks."]}, {"number": 22326, "title": "CollectiveAllReduceStrategy errors with official models", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI modified the tensorflow/models repo utility function to use CollectiveAllReduceStrategy instead of MirroredStrategy. See the one-line change at: https://gist.github.com/nvcastet/60b8c0c66da4cf2949e38fc790208a1c#file-distribution_utils-py-L47\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04.4 ppc64le\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.11\r\n- **Python version**: Python 2.7.15\r\n- **CUDA/cuDNN version**: Cuda 9.2 CuDNN 7.2.1\r\n- **GPU model and memory**: V100 16GB \r\n- **Exact command to reproduce**: ~/models/official/mnist$ python mnist.py --num_gpus 2\r\n### Describe the problem\r\nUsing the MNIST model script from the tensorflow/models repo after modifying the distribution_utils.py utility file to use CollectiveAllReduceStrategy (see modification above).\r\n`python mnist.py --num_gpus 2` crashes with\r\n```\r\n2018-09-17 19:28:36.648966: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at collective_ops.cc:210 : Internal: Second consumer arrived for key 10003:0:0:0:0:1\r\n         [[{{node conv2d_1/bias/replica_1/Initializer/CollectiveBcastRecv}} = CollectiveBcastRecv[T=DT_FLOAT, _class=[\"loc:@conv2d_1/bias/replica_1/Assign\"], group_key=1, group_size=2, instance_key=10003, shape=[64], _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n         [[{{node GroupCrossDeviceControlEdges_0/group_deps_27/_2}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_238_GroupCrossDeviceControlEdges_0/group_deps_27\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n```\r\nFull log of the run with stack trace: [run.log](https://github.com/tensorflow/tensorflow/files/2390053/run.log)", "comments": ["FYI @yuefengz ", "I don't think this is a duplicate of #22321.\r\n\r\nThe specific error:\r\nInternal: Second consumer arrived for key 10003:0:0:0:0:1\r\n\r\nIs most typically caused by an RPC response failure that results in a second RecvBuf request being sent by the client for the same key.  This kind of error cannot be tolerated, only detected with this failure.  It should be rare.  If you're experiencing it frequently, there's likely something wrong with the configuration.\r\n", "@poxvoculi I experience it constantly. This is a single worker run using 2 GPUs. I also sometimes got an assertion failure at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/collective_rma_local.cc#L49\r\n`CHECK_EQ(recv_bytes, hook->prod_value->TotalBytes());`.\r\nLet me know if i can provide more debugging info.", "I believe the problem is in the variable creator which only sees a partial name: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/collective_all_reduce_strategy.py#L147 and therefore it assigns same key for different variables. \r\n\r\nLet me see whether there is a workaround. I am not sure why pure Estimator model doesn't have this problem.", "@yufengg Correct, I see several variables with names `kernel` and `bias`:\r\n```\r\nvar name=beta1_power key=10006\r\nvar name=beta2_power key=10007\r\nvar name=bias key=10003\r\nvar name=bias key=10003\r\nvar name=bias key=10003\r\nvar name=bias key=10003\r\nvar name=conv2d/bias/Adam/ key=10010\r\nvar name=conv2d/bias/Adam_1/ key=10011\r\nvar name=conv2d/kernel/Adam/ key=10008\r\nvar name=conv2d/kernel/Adam_1/ key=10009\r\nvar name=conv2d_1/bias/Adam/ key=10014\r\nvar name=conv2d_1/bias/Adam_1/ key=10015\r\nvar name=conv2d_1/kernel/Adam/ key=10012\r\nvar name=conv2d_1/kernel/Adam_1/ key=10013\r\nvar name=count key=10005\r\nvar name=dense/bias/Adam/ key=10018\r\nvar name=dense/bias/Adam_1/ key=10019\r\nvar name=dense/kernel/Adam/ key=10016\r\nvar name=dense/kernel/Adam_1/ key=10017\r\nvar name=dense_1/bias/Adam/ key=10022\r\nvar name=dense_1/bias/Adam_1/ key=10023\r\nvar name=dense_1/kernel/Adam/ key=10020\r\nvar name=dense_1/kernel/Adam_1/ key=10021\r\nvar name=global_step key=10001\r\nvar name=kernel key=10002\r\nvar name=kernel key=10002\r\nvar name=kernel key=10002\r\nvar name=kernel key=10002\r\nvar name=total key=10004\r\n```\r\nHere error is:\r\n`InternalError (see above for traceback): BufRendezvous::ProvideBuf already called for key 10003:0:0:0:0:1` which corresponds to the `bias` variables.", "@yuefengz : In the code line you referred in your last comment, what about replacing\r\n`collective_instance_key = self._collective_keys.get_instance_key(key_id=kwargs[\"name\"])` with `collective_instance_key = self._collective_keys.get_instance_key()`? In this case, we get a different id for each new variable. Also, each replica of the same variable inside a worker still gets the same key because in the following code we loop over the devices using same key.", "@nvcastet that should work. My PR should be merged soon.", "@yuefengz Thanks!\r\nIn my run, I have seen those warnings:\r\n```\r\n2018-09-19 20:50:08.294697: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,784], [?]], output_types=[DT_FLOAT, DT_INT32]](IteratorFromStringHandleV2)]]\r\n\t [[{{node FunctionBufferingResourceGetNext}} = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FunctionBufferingResource)]]\r\n\t [[{{node Identity_1/_469}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_780_Identity_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n2018-09-19 20:50:08.294769: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,784], [?]], output_types=[DT_FLOAT, DT_INT32]](IteratorFromStringHandleV2)]]\r\n\t [[{{node FunctionBufferingResourceGetNext}} = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FunctionBufferingResource)]]\r\n\t [[{{node Adam/epsilon/_447}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_337_Adam/epsilon\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\n2018-09-19 20:50:08.295007: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?,784], [?]], output_types=[DT_FLOAT, DT_INT32]](IteratorFromStringHandleV2)]]\r\n\t [[{{node FunctionBufferingResourceGetNext}} = FunctionBufferingResourceGetNext[output_types=[DT_FLOAT, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](FunctionBufferingResource)]]\r\n```\r\nDespite those, the run finished correctly.\r\nDo you know if they are expected?", "I guess they are fine. Ccing @poxvoculi to confirm.", "A warning like this is not expected.  It says that a pending (not yet\ncomplete) Collective operation was aborted because somewhere else during\nthe step an Out-of-range error status occurred.   The exact origin of this\nkind of error status can be hard to track down.  You should look at the\nlogs of all of the worker processes and try to find the first time at which\nan error status arises.\n\n\nOn Thu, Sep 20, 2018 at 7:17 AM, Yuefeng Zhou <notifications@github.com>\nwrote:\n\n> I guess they are fine. Ccing @poxvoculi <https://github.com/poxvoculi> to\n> confirm.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22326#issuecomment-423198109>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AO818XkpFU0BrY5wIHYwhWA5wt5tMpzZks5uc6N2gaJpZM4WsmpG>\n> .\n>\n", "@yuefengz FYI The model you put at https://github.com/tensorflow/ecosystem/blob/master/distribution_strategy/keras_model_to_estimator.py has the same out-of-range issue.", "https://github.com/tensorflow/tensorflow/commit/e692dda4c8b199555e2fa32132a7784e0893c870 should fix this error. Closing this issue.\r\n\r\nI'll take a look at the out-of-range error. Right now it is not a fatal error.", "@yuefengz Let me open another issue for the out-of-range error. I usually get the warnings but sometimes get the assertion failure at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/collective_rma_local.cc#L49\r\n`CHECK_EQ(recv_bytes, hook->prod_value->TotalBytes());`", "@nvcastet Does it happen when you first use your cluster or after the first run?\r\nSometimes, even if you terminate your process, the cluster still has the session open. You have to wait for a while before your next run. But we can think of some way to garbage collect the session immediately.", "@yuefengz Happens in one box on 2 GPUs. It always only happens at the last step of the epoch.", "@yuefengz I don't use a client script. I just run the script directly on the cluster node.", "@nvcastet I see, feel free to file a new issue. I will take a look shortly."]}, {"number": 22325, "title": "bazel build tensorflow on windows 10 getting cudnn.h- system cannot find the file specified", "body": "From stackoverflow:\r\n\r\nhttps://stackoverflow.com/questions/52335703/bazel-build-tensorflow-on-windows-10-getting-cudnn-h-system-cannot-find-the-fil\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: 1.11.0\r\n- **TensorFlow version (use command below)**:  \r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.16.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  9.2/7.2.1\r\n- **GPU model and memory**: Nvidia M1000M\r\n- **Exact command to reproduce**:\r\n\r\nAdditional information:\r\nI've added these to my path (and rebooted):\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\libnvvp\r\nC:\\tools\\msys64\r\nC:\\tools\\bazel\r\nC:\\tools\\bazel\\bazel.exe\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\extras\\CUPTI\\libx64\r\n\r\nThese are the system variables I've set:\r\nBAZEL_SH  C:\\tools\\msys64\\usr\\bin\\bash.exe\r\nBAZEL_VC  C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\r\nBAZEL_VS  C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\r\n\r\n### Describe the problem\r\nI keep getting this error when trying to build the tensorflow-gpu using bazel and python in Windows 10:\r\n\r\nCuda Configuration Error: Error reading C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h: ja\r\nva.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(\"grep\" --color=never -A1 -E \"#de\r\nfine CUDNN_MAJOR\" \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h\"): The system cannot fin\r\nd the file specified.\r\nThis is the command I'm trying to run:\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nI've confirmed the C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h is there.\r\n\r\nI've tried running it from VS2015 x64 Native Tools Command Prompt, cmd, and powershell and get the same error.\r\n\r\nI'm using bazel 0.16.1, CUDA 9.2, Anaconda3 (Python 3.6.5), and CUDNN 7.2.1. I \"installed\" the CUDDNN files by unzipping its cuda folder into my C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2 folder (i.e. the whole \"cuda\" folder). I specified the C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2\\cuda path in the cudnn path question when I ran the configure.py. The configure.py completes without error.\r\n\r\nI also tried putting the CUDNN files directly in the C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2 folder (instead of a cuda folder in there) and specified the default location and still get basically the same error: \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/include/cudnn.h\": The system cannot find the file specified.\r\n\r\nThis is the full error:\r\n\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package'\r\n: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Trac\r\neback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1458\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1185, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 909, in _get_cuda_config\r\n                _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 584, in _cudnn_version\r\n                find_cuda_define(repository_ctx, cudnn_header_dir, \"c...\", ...)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 534, in find_cuda_define\r\n                auto_configure_fail((\"Error reading %s: %s\" % (str(h...)))\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 315, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Error reading C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h: ja\r\nva.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(\"grep\" --color=never -A1 -E \"#de\r\nfine CUDNN_MAJOR\" \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h\"): The system cannot fin\r\nd the file specified.\r\n\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_\r\ndefs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1458\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1185, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 909, in _get_cuda_config\r\n                _cudnn_version(repository_ctx, cudnn_install_base..., ...)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 584, in _cudnn_version\r\n                find_cuda_define(repository_ctx, cudnn_header_dir, \"c...\", ...)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 534, in find_cuda_define\r\n                auto_configure_fail((\"Error reading %s: %s\" % (str(h...)))\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 315, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Error reading C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h: ja\r\nva.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(\"grep\" --color=never -A1 -E \"#de\r\nfine CUDNN_MAJOR\" \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.2/cuda/include/cudnn.h\"): The system cannot fin\r\nd the file specified.\r\n\r\n\r\n### Source code / logs\r\nSolution/workaround is:\r\n\r\nThe problem is a bug in cuda_configure.bzl: it uses ctx.execute instead of ctx.action.run_shell.\r\nTo work around it: add c:\\tools\\msys64\\usr\\bin to your PATH. That's where grep.exe is so ctx.execute can find grep on the PATH.\r\n", "comments": ["Thanks for debugging the problem.\r\nWould you like to contribute a fix to cuda_configure.bzl ?", "Definitely, I'll see what I can do.", "Hi @rcorvus I am in the exact same position as you. I did as you said and added msys64\\usr\\bin to my path but am still not seeing success. Any update on this issue?\r\n\r\nEDIT: I had bazel version 0.17. It is important to have 0.16.1 for the workaround to be successful.\r\n\r\nThanks,\r\nZach", "I don't know why @rcorvus's fix only works with Bazel 0.16.1. \r\nWith Bazel 17.x, manually changing this line fixed my [`repository_ctx.execute`](https://docs.bazel.build/versions/master/skylark/lib/repository_ctx.html#execute) not finding grep:\r\n\r\nIn line 531 (or 562, or whatever the line now is) of cuda_configure.bzl:\r\n\r\n```python\r\n    result = repository_ctx.execute(\r\n        # Grep one more lines as some #defines are splitted into two lines.\r\n        [\"grep\", \"--color=never\", \"-A1\", \"-E\", define, str(h_path)],\r\n    )\r\n```\r\n\r\nChange it to manually specify the exact path:\r\n\r\n```python\r\n    result = repository_ctx.execute(\r\n        # Grep one more lines as some #defines are splitted into two lines.\r\n        [\"C:/msys64/usr/bin/grep\", \"--color=never\", \"-A1\", \"-E\", define, str(h_path)],\r\n    )\r\n```\r\n\r\nCuriously, adding `\"C:/msys64/usr/bin/grep\"` to my `path` instead did not work, as @rcorvus suggested.\r\n\r\nPlease fix this; I spent two hours on this, and probably a couple dozen more people will too. Obviously my \"fix\" is not universal, and I don't know of one that would be.\r\n\r\nDoesn't help that Bazel's error message seems to imply that `cuddn.h` couldn't be found, when in reality `grep` couldn't be found...", "same problem, tensorflow bazel build is not ready for asian locale windows\r\n\r\nthis might be path-related", "Also had this issue building TF master from source with Bazel 0.19.2, and joseortiz3's suggested fix of manually specifying the path to the grep binary in cuda_configure.bzl (though now on line 562) unblocked my build.", "The problem is still there in bazel 0.21 (the fix is also the same).", "The problem still exist . I am trying to solve but don't know how", "Hi @rcorvus!\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22325\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22325\">No</a>\n"]}, {"number": 22324, "title": "[INTEL MKL] Fix typo error of an environment variable ", "body": "Change the name of an environment variable\r\nfrom\r\n     TF_MKL_OPTIMIZE_PRIMITVE_MEMUSE\r\nto\r\n    TF_MKL_OPTIMIZE_PRIMITIVE_MEMUSE", "comments": ["Can u fix the cl format lint error?\r\n\r\nhttps://source.cloud.google.com/results/invocations/c3df27fd-ff97-4e5f-b9a6-8f309dcf940b/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_clang_format_out/log\r\n\r\ndiff --git a/tensorflow/core/util/mkl_util.h b/tensorflow/core/util/mkl_util.h\r\nindex 5ea8f2ee47..06c3598f85 100644\r\n--- a/tensorflow/core/util/mkl_util.h\r\n+++ b/tensorflow/core/util/mkl_util.h\r\n@@ -2041,7 +2041,7 @@ class MklPrimitiveFactory {\r\n   static inline bool IsPrimitiveMemOptEnabled() {\r\n     bool is_primitive_mem_opt_enabled = true;\r\n     TF_CHECK_OK(ReadBoolFromEnvVar(\"TF_MKL_OPTIMIZE_PRIMITIVE_MEMUSE\", true,\r\n-          &is_primitive_mem_opt_enabled));\r\n+                                   &is_primitive_mem_opt_enabled));\r\n     return is_primitive_mem_opt_enabled;\r\n   }\r\n", "I fixed a couple of coding style issues by running cpplint.py utility. \r\nBut I am not sure if it addressed the issue that Qianli pointed out (not so obvious to me :)).\r\nThanks!", "Yes, please change it back to be consistent with the coding style in other\nfiles in this directory.\n\nOn Fri, Sep 21, 2018 at 9:16 AM, Guozhong Zhuang <notifications@github.com>\nwrote:\n\n> *@gzmkl* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/core/util/mkl_util.h\n> <https://github.com/tensorflow/tensorflow/pull/22324#discussion_r219550058>\n> :\n>\n> > @@ -13,8 +13,8 @@ See the License for the specific language governing permissions and\n>  limitations under the License.\n>  ==============================================================================*/\n>\n> -#ifndef TENSORFLOW_CORE_UTIL_MKL_UTIL_H_\n> -#define TENSORFLOW_CORE_UTIL_MKL_UTIL_H_\n> +#ifndef TENSORFLOW_TENSORFLOW_CORE_UTIL_MKL_UTIL_H_\n>\n> Well, I ran cpplint.py and somehow the utility complains about it.\n>\n> If it is fine, let me change it back, to make it consistent across all\n> header files.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/22324#discussion_r219550058>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA8ObAIMIpSb0aaey7HehSpbKJjVCJECks5udRDZgaJpZM4WsjrE>\n> .\n>\n", "Hi Tatiana, \r\nI noticed that this PR is \"merge\" blocked due to some CI build issue. \r\nPlease let me know what I need follow up.\r\nRegards,\r\nGuozhong"]}, {"number": 22323, "title": "R1.10", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "R1.10 has already been merged back to master."]}, {"number": 22322, "title": "Error compiling/importing a custom (GPU) op on Windows w/ Bazel and TF r1.11", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro x64 17134.x\r\n- **TensorFlow installed from (source or binary)**: binary, w/ Bazel\r\n- **TensorFlow version (use command below)**: r1.11\r\n- **Python version**: 3.6.5 x64\r\n- **Bazel version (if compiling from source)**: 17.1\r\n- **GCC/Compiler version (if compiling from source)**: VS 2015 C++ tools\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7\r\n- **GPU model and memory**: GTX1080\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nI saw that the support for building TF on Windows with Bazel has been added recently, so I followed the [official docs](https://www.tensorflow.org/install/install_sources_windows) and proceeded to setup my environment, clone TF r1.11 and build the library (with GPU support) to make sure everything was compiling fine. This worked without issues, so that's something.\r\n\r\nThen, I tried to compile a custom GPU op following the [docs here](https://www.tensorflow.org/extend/adding_an_op#compile_the_op_using_bazel_tensorflow_source_installation) and hit a roadblock. I'm not sure if this is officially supported yet, but since I saw you guys added the ability to load user ops on Windows too a while back (plus the fact that building TF on Windows now works fine), it seemed plausible at this point.\r\n\r\nI wrote a dummy GPU op (code [here](https://gist.github.com/Sergio0694/2b463571e6f82519137e9785caf90b08) and kernel [here](https://gist.github.com/Sergio0694/48a5bb39ea92494371b3c847d23d09f6)) and used the `BUILD` script as explained in the docs (replacing `.so` with `.dll`), but the build failed with the [this output log](https://gist.github.com/Sergio0694/3f379c9007fa2abd588dac5be0fe996e).\r\n\r\nI also tried to build the included `fact.cc` op (replacing `.so` again with `.dll`), which compiled fine (as that op is CPU only), but when I tried to import the module into TF it failed with the usual \"can't find entry point in dynamic library...\" error.\r\n\r\nSo here are my two questions:\r\n1) Is there an official statement about the ability to build GPU ops on Windows, now that TF supports building the rest of the library on Windows?\r\n2) If so (I mean, the whole TF library builds just fine, so I guess I'm just doing something wrong here), what's the right procedure to build a custom GPU op on Windows with Bazel? Would it be possible to update the docs so that every TF user on Windows would be able to do the same as well?\r\n\r\nThanks! \ud83d\ude04\r\n", "comments": ["Pinging @guschmue for his previous work on this topic on [this issue in tensorflow/models](https://github.com/tensorflow/models/issues/1103) and @mrry just in case, as he was following that issue as well \ud83d\ude0a", "FYI, @mrry.", "I'll add more info on the issue here, following a conversation with @arnaldog12 about this.\r\n\r\n- I'm using TF from Python, but I have a few custom GPU ops that I need to compile and use in my graph.\r\n- Right now I'm working on Ubuntu, and I managed to compile the various ops without issues, following the docs. The thing is that I'd like to distribute my trained network to users on possibly other platforms (eg. Windows), so they'd need to be able to compile those GPU ops there as well. I'd love to see feature parity with Linux/MacOS on this, ie. the ability to compile custom GPU ops with TF _installed from binary_. I wouldn't want every user to have to build the whole TF package from scratch.\r\n- Being able to build the custom GPU op along with TF (so when installed from source) would still be a valid alternative at least for now (better than nothing). Unfortunately, none of the two approaches seem to work at the moment.\r\n\r\nAs for the error logs, [this](https://gist.github.com/Sergio0694/3f379c9007fa2abd588dac5be0fe996e) is the error log when trying to build the GPU op with Bazel on Windows (with TF r1.11) following [these docs](https://www.tensorflow.org/extend/adding_an_op#compile_the_op_using_bazel_tensorflow_source_installation) and replacing `.so` with `.dll`. As mentioned in the first post, building a GPU op fails, and building a CPU op works, but fails to load when calling `tf.load_op_library`. Made these attempts just in case, I realize this is probably just not supported on Windows yet.\r\n\r\n**My actual question**: would it be possible to get a variant of [this bash script](https://www.tensorflow.org/extend/adding_an_op#compile_the_op_using_your_system_compiler_tensorflow_binary_installation) (with the [two additional lines](https://www.tensorflow.org/extend/adding_an_op#gpu_support) to enable GPU support) to build a custom GPU op for TF on Windows, with TF installed from binary? Is it just a matter of coming up with the right build commands and setting up the environment/compiler properly, or is this just not possible on Windows as of now for some reason?\r\n\r\nThanks! \ud83d\ude04\r\n", "@Sergio0694 \r\n\r\nLooking to your logs, we can see that you have a some syntax errors in your test.cu.cc file, like described by this line:\r\n```\r\ntensorflow/core/user_ops/test.cu.cc(5): error C2144: errore di sintassi: 'void' deve essere preceduto da ';'\r\n```\r\nand semantic errors:\r\n```\r\ntensorflow/core/user_ops/test.cu.cc(14): error C2065: 'blockDim': identificatore non dichiarato\r\n```\r\nThat sounds weird since you're having no errors when building with Linux, as you said. \r\nDo you have some idea why these errors are Windows-specific? ", "@arnaldog12 Yeah, judging from those errors my guess is that either the `cuda.h` header is not loaded correctly, or that the CUDA compiler is not configured/setup properly on Windows.\r\n\r\nI mean, the errors are:\r\n\r\n- \"missing a `;` before `void`\", on the GPU kernel definition, which makes no sense. That line in particular starts with `__global__ void`, so my guess is the `__global__` macro from `cuda.h` is not imported right.\r\n- \"`blockDim` is not a declared identifier\", where `blockDim` is a built-in CUDA variable (see [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-variables)).\r\n\r\nSo, these seem to be related to the CUDA compiler/environment more than Windows itself (though I'm not an expert on this, so it's just a guess based on my observations above).\r\n\r\nThis is why I was wondering if the whole issue was just to come up with a proper shell/cmd script on Windows to configure the environment properly, as maybe the problem here is that the way the various header paths are loaded on Linux/MacOS simply isn't right on Windows (see the TF docs [here](https://www.tensorflow.org/extend/adding_an_op#gpu_support) about this).\r\n", "According to the documentation:\r\n\r\n> Note that if your CUDA libraries are not installed in /usr/local/lib64, you'll need to specify the path explicitly in the second (g++) command above. For example, add -L /usr/local/cuda-8.0/lib64/ if your CUDA is installed in /usr/local/cuda-8.0.\r\n\r\nSo, as ```/usr/local/lib64``` is not a valid path on Windows, you may use the -L option in your build command. Have you tried it?\r\nIn addition, I believe you should add CUDA and cudnn in your environment variables, like this:\r\n\r\n![variables_1](https://user-images.githubusercontent.com/4855538/47792865-b4061680-dcfb-11e8-9f58-476b6ff69609.PNG)\r\n\r\n![variables_2](https://user-images.githubusercontent.com/4855538/47792867-b7010700-dcfb-11e8-8f87-9770b1a0a4dc.PNG)\r\n\r\nOf course, you should use only the CUDA and cudnn versions compatible with the Tensorflow version you're using.\r\n\r\n", "Yeah, I did have the environment variables setup as you showed in the screens, but I hadn't tried to use the `-L` option, thanks for pointing it out. I wonder if the incorrect path on Windows is the same reason why the call to generate the `TF_LFLAGS` variable, which is `\" \".join(tf.sysconfig.get_link_flags())` returns an empty string.\r\n\r\nI tried to use the `-L` option to compile the first .cu.cc file, calling nvcc from the VS native tools window (otherwise nvcc doesn't fine the cl.exe compiler), but it failed saying that VS2017 is not supported with that version of nvcc (I'm using CUDA 9.0 as suggested by the TF install guide for r1.11).\r\n\r\nI have to point out though: I feel like every time I manage to make a tiny step, something else causes an error here, and I'm not even sure that even if I managed to build a .dll file with the op, TF would manage to load it correctly on Windows.\r\n\r\nWould it be possible to get a reply from anyone from the TF team to confirm whether or not this is actually supported/possible at all and if not, if there are plans to enable this feature natively (as in, with updated docs and with tf automatically loading the right import/link paths on Windows, just like on Linux/MacOS) in a future release?\r\n\r\nThanks again! \ud83d\ude04", "Hi Sergio,\r\n\r\nI'm closing the issue because it is 1+ years old, but please reopen if this is still relevant.\r\n\r\nAnd CC @gunan for build system related issues.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22322\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22322\">No</a>\n"]}, {"number": 22321, "title": "Distributed training fails when I use CollectiveAllReduceStrategy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI slightly updated mnist.py example so that it uses CollectiveAllReduceStrategy. Updated version is [here](https://github.com/dmitrievanthony/models/blob/ignite/official/mnist/mnist.py#L208).\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS 10.13.3\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.11.0-dev20180913\r\n\r\n- **Python version**:\r\n3.6.3\r\n\r\n- **Exact command to reproduce**:\r\nSee [updated example](https://github.com/dmitrievanthony/models/blob/ignite/official/mnist/mnist.py#L208).\r\n\r\n### Describe the problem\r\n\r\nHi, \r\n\r\nI'm trying to update `mnist` model from [official repository](https://github.com/tensorflow/models/tree/master/official/) so that it uses CollectiveAllReduceStrategy as it's shown in [keras_model_to_estimator_client.py](https://github.com/tensorflow/ecosystem/blob/master/distribution_strategy/keras_model_to_estimator_client.py). Updated example you can find [here](https://github.com/dmitrievanthony/models/blob/ignite/official/mnist/mnist.py#L208). Unfortunately, it fails on `deepcopy` of run config.\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"mnist.py\", line 286, in <module>\r\n    absl_app.run(main)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 274, in run\r\n    _run_main(main, args)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 238, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"mnist.py\", line 280, in main\r\n    run_mnist(flags.FLAGS)\r\n  File \"mnist.py\", line 226, in run_mnist\r\n    'data_format': data_format,\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 190, in __init__\r\n    model_dir)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1591, in maybe_overwrite_model_dir_and_session_config\r\n    config = run_config.RunConfig.replace(config, session_config=session_config)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/run_config.py\", line 849, in replace\r\n    copy.deepcopy(self),\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 280, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 280, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 280, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 169, in deepcopy\r\n    rv = reductor(4)\r\nTypeError: can't pickle _thread._local objects\r\n```\r\n", "comments": ["I met the same problem when using CollectiveAllReduceStrategy", "I couldn't reproduce this problem. I am wondering whether you are still running with standalone client mode?", "@dmitrievanthony Hi, Could you please share a small code snippet to reproduce this issue. Also you may clarify if you are running this with standalone client mode or not.", "@yuefengz, @harshini-gadige, yes I use standalone client mode. \r\n\r\nI've prepared an example based on keras_model_to_estimator_client.py. Please take a look [here](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/distribution_strategy/keras_model_to_estimator_client.py). It uses `FixedLengthRecordDataset` and `CollectiveAllReduceStrategy` strategy.\r\n\r\nI don't use Kubernetes, so to reproduce this problem I do the following:\r\n1. Start [worker1.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/ignite/worker1.py).\r\n2. Start [worker2.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/ignite/worker2.py).\r\n3. Set `TF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:1111\", \"localhost:1112\"],\"chief\":[\"localhost:1113\"]}, \"task\":{\"type\":\"chief\",\"index\":0}}'`\r\n4. Start [keras_model_to_estimator_client.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/distribution_strategy/keras_model_to_estimator_client.py).\r\nGet the exception.\r\n\r\n![screen](https://s3.amazonaws.com/helloworld23423423ew23/%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA+%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0+2018-09-18+%D0%B2+21.39.19.png)", "@dmitrievanthony But you haven't specified `remote_cluster` in your code?", "I specified `TF_CONFIG`. As far as I understand documentation it's another source of information about cluster if `remote_cluster` is not specified. Am I wrong?\r\n\r\nAnyway, I can say that in case of `tf.data.Dataset.from_tensor_slices` and `MirroredStrategy` `TF_CONFIG` is actually used and everything works fine.", "If I remember correctly, I suppose if you don\u2019t specify the remote_cluster, the client runs in independent worker mode, which means all your nodes should run the same Python program but not a standard TF server. I would recommend you to try standalone client mode, which is in effect when you do specify remote_cluster.\r\n\r\nPS: I would suggest you to use run_std_tensorflow_server in contrib.distribute package for your workers in standalone client mode.", "Looks like it would be better to make documentation or API more clear, but anyway... :)\r\n\r\nI tried the following approach:\r\n1. Set `export TF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:1111\", \"localhost:1112\"],\"chief\":[\"localhost:1113\"]}, \"task\":{\"type\":\"worker\",\"index\":0}}'`\r\n2. Start [worker1](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/ignite/worker1.py) (it's modified and now calls `run_standard_tensorflow_server`).\r\n3. Set `export TF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:1111\", \"localhost:1112\"],\"chief\":[\"localhost:1113\"]}, \"task\":{\"type\":\"worker\",\"index\":1}}'`\r\n4. Start [worker2](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/ignite/worker2.py) (it's modified and now calls `run_standard_tensorflow_server` and equal to `worker1.py` to be honest).\r\n5. Start [keras_model_to_estimator_client.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/distribution_strategy/keras_model_to_estimator_client.py#L106) with cluster specification passed via `remote_cluster`.\r\n\r\nUnfortunately, after all these actions I get the same exception:\r\n\r\n```\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode\r\nINFO:tensorflow:Using the Keras model provided.\r\nTraceback (most recent call last):\r\n  File \"keras_model_to_estimator_client.py\", line 126, in <module>\r\n    tf.app.run(argv=sys.argv)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"keras_model_to_estimator_client.py\", line 114, in main\r\n    keras_model=model, config=run_config, model_dir=model_dir)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py\", line 412, in model_to_estimator\r\n    model_dir)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1591, in maybe_overwrite_model_dir_and_session_config\r\n    config = run_config.RunConfig.replace(config, session_config=session_config)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/run_config.py\", line 849, in replace\r\n    copy.deepcopy(self),\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 280, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 280, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 280, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/Users/antondmitriev/anaconda3/lib/python3.6/copy.py\", line 169, in deepcopy\r\n    rv = reductor(4)\r\nTypeError: can't pickle _thread._local objects\r\n````", "Looks like this is a problem with python3. I have reproduced it with python3 without needing to change the example. Let me see how to fix it.", "Hi @yuefengz, yes, looks like you are right. I changed example so that it doesn't require kubernetes. From my perspective it's easier to start with such example without external dependencies.", "Hi @yuefengz, I tried to do the same things with `python2`. It workers better because it doesn't throw an exception, but I still get in a trouble because it hangs up. I posted details in #22407, please have a look (maybe I juts configured something wrongly).", "@dmitrievanthony Some bug has been introduced after 1.11 release. I will fix it soon. If you are using nightly build, you can comment out this line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/collective_all_reduce_strategy.py#L232. Anyway, my PR should be merged soon.", "@yuefengz, comment of this line unfortunately didn't help me.", "This should fix the problem: https://github.com/tensorflow/tensorflow/commit/f10b00558de87020554c9c0512537dab96dba918", "Closing this issue.", "\r\nYour code links are not accessible, you can update it again, I am looking for an example, I would like to thank\r\n\r\n> @yuefengz, @harshini-gadige, yes I use standalone client mode.\r\n> \r\n> I've prepared an example based on keras_model_to_estimator_client.py. Please take a look [here](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/distribution_strategy/keras_model_to_estimator_client.py). It uses `FixedLengthRecordDataset` and `CollectiveAllReduceStrategy` strategy.\r\n> \r\n> I don't use Kubernetes, so to reproduce this problem I do the following:\r\n> \r\n> 1. Start [worker1.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/ignite/worker1.py).\r\n> 2. Start [worker2.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/ignite/worker2.py).\r\n> 3. Set `TF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:1111\", \"localhost:1112\"],\"chief\":[\"localhost:1113\"]}, \"task\":{\"type\":\"chief\",\"index\":0}}'`\r\n> 4. Start [keras_model_to_estimator_client.py](https://github.com/dmitrievanthony/ecosystem/blob/ignite-2/distribution_strategy/keras_model_to_estimator_client.py).\r\n>    Get the exception.\r\n> \r\n> ![screen](https://camo.githubusercontent.com/bc7602eb7d47a70274fe41f3c04475ccc59771e0/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f68656c6c6f776f726c643233343233343233657732332f2544302541312544302542442544302542382544302542432544302542452544302542412b2544312538442544302542412544312538302544302542302544302542442544302542302b323031382d30392d31382b2544302542322b32312e33392e31392e706e67)\r\n\r\n", "Hi, @khaitranvan96kt. The issue was closed, so I removed examples. If you are interested in similar use cases you can have a look at the example I've prepared and tried to merge into \"ecosystem\" module: https://github.com/tensorflow/ecosystem/pull/101."]}, {"number": 22320, "title": "fatal error: google/protobuf/inlined_string_field.h: No such file or directory  #include <google/protobuf/inlined_string_field.h>", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: Commit cac963862be3faa421c559f39033c9bfb3b27a51\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nFollow up to issue #22240. I was able to fix the absl issue by pulling and copying over the absl folder to the external folder but I think there is an issue with the protobufs.\r\n\r\nAfter following this tutorial: https://tuatini.me/building-tensorflow-as-a-standalone-project/ and running the following command:\r\ng++ -std=c++11 -Wl,-rpath='$ORIGIN/lib' -Iinclude -Llib main.cpp -ltensorflow_cc -o exec\r\n\r\nI'm getting the error \"This file was generated by a newer version of protoc which is incompatible with your Protocol Buffer headers. Please update\" and \"google/protobuf/inlined_string_field.h: No such file or directory  #include <google/protobuf/inlined_string_field.h>\" \r\n\r\nAfter looking through the commits I think the protobuf url is outdated.\r\n\r\n![screenshot from 2018-09-17 12 39 06](https://user-images.githubusercontent.com/14967965/45640040-2cc85080-ba77-11e8-90e6-95fa13d909cd.png)\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": [" ~/Desktop/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/src protoc --version gives libprotoc 3.5.0 but inlined_string_field.h is not in 3.5.0 (generated after running build_all_linux.sh)", "Figured out a way around this and was able to run a custom c++ program with g++ outside the tensorflow repo without using bazel other than for building the .so files. I'll be posting a tutorial soon for anyone who needs it.", "+1", "> /Desktop/tensorflow/tensorflow/contrib/makefile/downloads/protobuf/src protoc --version gives libprotoc 3.5.0 but inlined_string_field.h is not in 3.5.0 (generated after running build_all_linux.sh)\r\n\r\nworkspace.bzl downloads 3.6.0, i changed it to 3.5.0, that fixes this for me.", "Yeah you can do that or you can get the 3.6.0 protobufs from the protobuf github repo which is what I ended up doing.", "@roshan-dongre Hi, can this issue be closed as you figured out a way to resolve this ?", "Hi @roshan-dongre , Can you explain where exactly did you copy absl folder for fixing #22240 (Path for external folder) ? \r\n\r\n\" I was able to fix the absl issue by pulling and copying over the absl folder to the external folder \" ", "Hi,\r\n  I'm facing a version issue trying to compile tensorflow-1.11.0 in order to use it in a C++ project.  I think the issue is that bazel use protobuf-3.6.0 in `tensorflow/workspace.bzl` but the `tensorflow/contrib/makefile/download_dependencies.sh` fetch version 3.5.0 thus when you follow the [instructions](https://tuatini.me/building-tensorflow-as-a-standalone-project/) where you copy the proto stuff in Tensorflow's install_dir, we get a version mismatch.\r\n\r\n`tensorflow/contrib/makefile/download_dependencies.sh` needs to be updated to match `tensorflow/workspace.bzl`."]}, {"number": 22319, "title": "Distributed training fails when I use FixedLengthRecordDataset", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI slightly updated `keras_model_to_estimator_client.py` example so that it uses FixedLengthRecordDataset. Updated version is [here](https://github.com/dmitrievanthony/ecosystem/blob/ignite/distribution_strategy/keras_model_to_estimator_client.py).\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 17.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.11.0-dev20180917\r\n\r\n- **Python version**:\r\n3.6.3\r\n\r\n- **Exact command to reproduce**:\r\nSee [updated example](https://github.com/dmitrievanthony/ecosystem/blob/ignite/distribution_strategy/keras_model_to_estimator_client.py).\r\n\r\n### Describe the problem\r\nHi,\r\n\r\nI'm investigating TensorFlow distributed training approaches with help of @yuefengz who kindly helps me. So far I updated [keras_model_to_estimator_client.py](https://github.com/tensorflow/ecosystem/blob/master/distribution_strategy/keras_model_to_estimator_client.py) example so that it uses FixedLengthRecordDataset, but as result the whole code fails.\r\n\r\n### Source code / logs\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2667, in gather\r\n    return params.sparse_read(indices, name=name)\r\nAttributeError: 'Tensor' object has no attribute 'sparse_read'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1628, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be at least rank 1 but is rank 0 for 'GatherV2' (op: 'GatherV2') with input shapes: [], [1], [].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"keras_model_to_estimator_client.py\", line 134, in <module>\r\n    tf.app.run(argv=sys.argv)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"keras_model_to_estimator_client.py\", line 129, in main\r\n    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn))\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 462, in train_and_evaluate\r\n    estimator, train_spec, eval_spec, _TrainingExecutor)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py\", line 264, in train_and_evaluate\r\n    session_config=run_config.session_config)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 798, in run_distribute_coordinator\r\n    session_config, rpc_layer)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 344, in _run_single_worker\r\n    worker_fn(strategy)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/distribute/estimator_training.py\", line 232, in _worker_fn\r\n    hooks=list(train_spec.hooks))\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1178, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1246, in _train_model_distributed\r\n    input_fn, model_fn_lib.ModeKeys.TRAIN, self._train_distribution)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1037, in _get_iterator_from_input_fn\r\n    lambda: self._call_input_fn(input_fn, mode))\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 480, in distribute_dataset\r\n    self._prefetch_on_device)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 814, in __init__\r\n    worker_input, len(worker_device_map), i)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/input_ops.py\", line 140, in auto_shard_dataset\r\n    return _auto_shard_impl(dataset=dataset, found_reader_op=False)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/input_ops.py\", line 107, in _auto_shard_impl\r\n    dataset._input_dataset, found_reader_op)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/input_ops.py\", line 107, in _auto_shard_impl\r\n    dataset._input_dataset, found_reader_op)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/input_ops.py\", line 107, in _auto_shard_impl\r\n    dataset._input_dataset, found_reader_op)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/input_ops.py\", line 67, in _auto_shard_impl\r\n    filenames_tensor, math_ops.range(index, num_files, num_shards))\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2669, in gather\r\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3232, in gather_v2\r\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1792, in __init__\r\n    control_input_ops)\r\n  File \"/home/gridgain/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1631, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Shape must be at least rank 1 but is rank 0 for 'GatherV2' (op: 'GatherV2') with input shapes: [], [1], [].\r\n```\r\nIt looks like the exception happens in `input_ops.py` in `_auto_shard_impl` method when TensorFlow tries to shard file names. Unfortunately, I don't clearly understand this command to be honest: \r\n\r\n```\r\n sharded_filenames_tensor = array_ops.gather(\r\n            filenames_tensor, math_ops.range(index, num_files, num_shards))\r\n```\r\n", "comments": ["It is not recommended to use MirroredStrategy for multi-node, could you try CollectiveAllReduceStrategy while we are fixing this issue?", "@yuefengz, I tried, please have a look at #22321 :)", "Seems auto-sharding is not perfect for all use cases. I'll make it disabled by default.", "Will there be a way to turn it on by users?\n\nOn Sat, Sep 22, 2018, 21:57 Yuefeng Zhou <notifications@github.com> wrote:\n\n> Seems auto-sharding is not perfect for all use cases. I'll make it\n> disabled by default.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22319#issuecomment-423755724>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ANc5F2a0weCwDHzFXA_Wqtt2SAmPSyxNks5udmTWgaJpZM4WsRgH>\n> .\n>\n", "Hi, @yuefengz. Looks like it's been fixed, right?", "Yes, commit https://github.com/tensorflow/tensorflow/commit/301e3043e67493ce3777d2b36b43d0210f7b920c#diff-6d6d850ad586ab99cc0dbaabba8a229c disables auto sharding by default, and instead adds an argument to `MirroredStrategy` to explicitly turn it on. \r\n\r\nIf this works for you now, feel free to close the issue. Thanks! ", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 22318, "title": "Hi when i run the this code[ input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])] then it give this error     [   1 input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])  NameError: name 'features' is not defined ]] can u tell me that how i fix it thanks", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@muneebullahkhan you would need to define features as tensor i.e. 28 by 28 in your case before using it.", "thanks wt-huang"]}, {"number": 22317, "title": "Hi when i run the this code[ input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])]", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing this as this is a duplicate", "Thanks\n\nOn Mon, Sep 17, 2018 at 9:39 PM wt-huang <notifications@github.com> wrote:\n\n> Closing this as this is a duplicate\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22317#issuecomment-422083218>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AlmlN76zL6T9FHc1__BnXnyW4ACzItYCks5ub9BFgaJpZM4WrwPN>\n> .\n>\n"]}, {"number": 22316, "title": "Hi ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing as this is a duplicate"]}, {"number": 22315, "title": "Ji", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing as this is a duplicate", "yes this is duplicate\n\n\nOn Mon, Sep 17, 2018 at 9:40 PM wt-huang <notifications@github.com> wrote:\n\n> Closed #22315 <https://github.com/tensorflow/tensorflow/issues/22315>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22315#event-1850246477>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AlmlN_P855fxDhJdxtQkNFQy_f8eMwDBks5ub9CJgaJpZM4WrwIS>\n> .\n>\n"]}, {"number": 22314, "title": "Problems after installing tensorflow with pip", "body": "Hi I encounter following issue, when I run \"import tensorflow as tf\" with python\r\n\r\ninstall command I used:\r\n`pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl`\r\n\r\npython version\r\n`Python 3.7.0`\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\victor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n- \r\nWhat's the problem here? Thank you for your help!", "comments": ["@high242 This error indicates that your system path or environment settings of CUDA  have not been configured correctly. Please refer a duplicate issue https://github.com/tensorflow/tensorflow/issues/11571 .  I am closing this issue since its a duplicate. Feel free to reopen if your issue still persists. ", "@high242 \r\ndid you solve the problem?", "> @high242\r\n> did you solve the problem?\r\n\r\ndid you solve the problem??\r\n", "Bonjour j'ai le m\u00eame probl\u00e8me : j'ai installe\u00e9 la version python 3.6 mais \u00e0 chaque j'ai cette erreur. Est ce que quelqu'un pourrait m'aider \u00e0 r\u00e9soudre mon probl\u00e8me\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"c:\\python36\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n"]}, {"number": 22313, "title": "[keras] fix: ReLU had dict argument when load model from ckeck point file", "body": "ReLU had dict argument when load model from ckeck point file\r\n\r\nWhen keras model had tf.keras.layers.ReLU layer and load the model from file, the arguments of ReLU layer had type dict. The dict argument make trouble when they are cast to float.\r\n\r\nWhat i added is the code to change the dict to number.\r\nI thinks this is a kind of hot fix.. maybe someone can fix the module for save or load keras model.\r\nso. there is no fundamental different between the type of inputs.\r\n\r\nI refered to this issue:\r\nkeras-team/keras#7107\r\n\r\n\r\n", "comments": []}, {"number": 22312, "title": "[lite]revised a parameter error", "body": "Hi @MarkDaoust , i found that when firstly use `interpreter `as a parameter pass into `eval_model` function, pass `interpreter_quant` two times to `eval_model` may be make some people confused.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 22311, "title": "ERROR when reload  shared_embedding_columns with partitioner ", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:1.10.1\r\n- **Python version**:3.5.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:NO\r\n- **GPU model and memory**:NO\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen use shared_embedding_columns without partitioner  to save model ,then import_meta_graph from check_point is success. But use shared_embedding_columns with partitioner, then import_meta_graph from check_point is failed.   \r\nReport error: KeyError: \r\nThe name 'run_2/input_from_feature_columns/a1_shared_embedding/a1_a1_shared_embedding' refers to an Operation not in the graph\r\n\r\n### Source code / logs\r\n\r\ncode for model\r\nfirst, mkdir mypath/model\r\n````\r\nfrom tensorflow.contrib.layers.python.layers import feature_column as fc\r\nfrom tensorflow.python.ops import variable_scope\r\nfrom tensorflow.python.framework import sparse_tensor as sparse_tensor_lib\r\nfrom tensorflow.contrib.layers.python.layers import feature_column_ops\r\nfrom tensorflow.python.training import saver\r\nimport tensorflow as tf\r\n\r\ndef mySharedEmbeddingColumn():\r\n    a1 = fc.sparse_column_with_keys(\"a1\", [\"marlo\", \"omar\", \"stringer\"])\r\n    b = fc.shared_embedding_columns([a1, a1], dimension=4, combiner=\"mean\")\r\n    input_tensor_c1 = sparse_tensor_lib.SparseTensor(\r\n        indices=[[0, 0], [1, 1], [2, 2]], values=[0, 1, 2], dense_shape=[3, 3])\r\n    dnn_partitioner = (\r\n        tf.min_max_variable_partitioner(\r\n        max_partitions=1))\r\n    with variable_scope.variable_scope(\"run_2\",partitioner=dnn_partitioner):\r\n      b2 = feature_column_ops.input_from_feature_columns({\r\n          b[1]: input_tensor_c1\r\n      }, [b[1]])\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        variable_names = [v.name for v in tf.trainable_variables()]\r\n        values = sess.run(variable_names)\r\n        for k,v in zip(variable_names, values):\r\n            print(\"Variable: \", k)\r\n            print(\"Shape: \", v.shape)\r\n            print(v)\r\n        saver.save(sess,'model/model-ckt')\r\n\r\nmySharedEmbeddingColumn()\r\n````\r\ncode for restore\r\n````\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\nsess = tf.Session(graph=graph)\r\nwith graph.as_default():\r\n    del tf.get_collection_ref(tf.GraphKeys.TRAIN_OP)[:]\r\n    check_point_path = 'model' \r\n    saver = tf.train.import_meta_graph('model/model-ckt.meta')\r\n    print(graph.get_operations())\r\n\r\n````\r\n\r\n", "comments": ["Mustafa, is this a known issue (who, in addition to you, should know about this?)", "I'm not aware of this failure. @rohan100jain for FeatureColumn and @karmel for saved model.", "The sample code up above has a lot going on. Can you simplify the example to replicate with just the shared embedding column and partitioner, without the contrib feature_column_ops? Does the problem still occur?", "@qiang2008 Hi, can you please share a simplified example as asked above ?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 22310, "title": "Tflite label_image resize bug", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNA\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.10\r\n- **Python version**:\r\n3.5\r\n- **Bazel version (if compiling from source)**:\r\n0.15\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNA\r\n- **CUDA/cuDNN version**:\r\nNA\r\n- **GPU model and memory**:\r\nNA\r\n- **Exact command to reproduce**:\r\nNA\r\n\r\n### Describe the problem\r\nIn the `label_image.cc` example of TFlite, a resize function present in `bitmap_helpers_impl.h` is being called in line [171](https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/contrib/lite/examples/label_image/label_image.cc#L171) - \r\n`   resize<float>(interpreter->typed_tensor<float>(input), in.data(),`\r\n\r\nIn the implementation of the function in `bitmap_helpers_impl.h`, the following [line](https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/contrib/lite/examples/label_image/bitmap_helpers_impl.h#L90) \r\n```\r\n  auto output_number_of_pixels =\r\n      wanted_height * wanted_height * wanted_channels;\r\n```\r\nassumes that for the input image, the height and width must be the same. And this works for the mobilenet example since the input size is 224 x 224. But, this would cause issues for any network where the expected input size does not have the same width and height. It should instead be - \r\n```\r\n   auto output_number_of_pixels =\r\n      wanted_width * wanted_height * wanted_channels;\r\n```\r\n\r\n### Source code / logs\r\nNA\r\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/22377#issuecomment-444550814"]}, {"number": 22309, "title": "About partitioned variable is Disabled in DistributionStrategy", "body": "### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): master\r\nPython version: 2.7\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\n\r\n### Describe the problem\r\n\r\nI noticed that the partitioned variable is disabled in base class `DistributionStrategy` in current master branch. However, some subclasses like ParameterServerStrategy is okay to use partitioner in current architecture.  So I want to know the reason for  disabling the partitioner in base class? Does MirroredValue can use that partitioner?\r\n\r\nAnd I want to do some code change in base class `DistributionStrategy` to make a pull request: To rename the  function `disable_partitioned_variables` to `distributed_custom_getter` in base class and move them out of  function `scope` to be a member function of `DistributionStrategy`. The subclasses should override this function to control the custom getter. Do you think this is a reasonable design ? If it is Okay, I will make a code review.", "comments": ["@yuefengz ", "@yuefengz \r\nYuefeng, \r\n\r\nWe are working for TensorFlow enhancement and optimization for Alibaba's production clusters. And \r\nTF's DistributionStrategy looks a promising feature for us since it could reduce the overhead for user writing distributed execution plan. However, there are still some limitations regarding to DistributionStrategy, as the one mentioned by Siyu. \r\n\r\nAny comments from you side is hight welcome and much appreciated.\r\n", "Thanks for bringing this up. I'll think about it and get back to you soon.", "@yuefengz \r\nHi yuefeng,\r\n\r\nAny update?\r\n\r\nThanks", "Actually, based on the architecture of Distributed Strategy, we are pushing its application inside Alibaba and it is the substrate of our so-called \"auto-parallel\" functionality, any discussion or interaction with people from Google working Distributed Strategy will be highly appreciated. \r\n\r\nThanks", "Why don't you submit an RFC or a PR as reference implementation?\r\n\r\nIt should be much more appreciated.", "https://github.com/tensorflow/tensorflow/pull/22473 This PR have enabled the `assign`, `assign_add` and `assign_sub` in `PartitionedVariable`. And I will support variable partitioned usage in `DistributionStrategy` in graph-mode after this PR.", "Nagging Assignee @yuefengz: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the delayed response.  I have made this PR to support variables partitioner in `ParameterServerStrategy`. Please have a look. https://github.com/tensorflow/tensorflow/pull/23254\r\n\r\nThanks all. ", "The PR #23254 has been merged. Closing this issue."]}, {"number": 22308, "title": "[WIP] Variable Length CuDNN LSTMs/RNNs", "body": "Hi! Started working on implementing variable sequence lengths in cudnn. Getting close, but also need to implement Pack/Unpack operations to make things useful. Hopefully will have this working within a week or so. Should be a huge performance difference.\r\n\r\nIf anyone else is interested in the subject and wants to help, please let me know!\r\n\r\nCheers", "comments": ["Hi @bstriner, from the change description, it seems that the change is still on-going (unfinished). Can you please confirm whether it is the case?", "@qlzh727 Correct. Hoping to finish this soon but just started over the weekend. Getting faster as I'm getting used to the TF APIs. Figured I would put the PR out there in case anyone wanted to help or start commenting on the code. Just finished getting the kernel written for alignment. Basically, 4 kernels:\r\n\r\n* Calculate alignment of packed sequence (done and tested)\r\n* Pack a sequence (unwritten)\r\n* Unpack a sequence (unwritten)\r\n* Variable length LSTM (mostly done)", "Packing operations seem to be calculating everything correctly. Would appreciate any tips on making the kernels faster, maybe adding in some shared memory. Here is how they work currently.\r\n* Pack extracts just the valid data from a sequence\r\n* Unpack pads that data back into the original sequence shape. Padding regions are zero.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops import packing_ops\r\n\r\nsequence = tf.constant(np.arange(6 * 3).reshape((6, 3, 1)), dtype=tf.float32) + 1\r\nsequence_lengths = tf.constant(np.array([6, 3, 2]), dtype=tf.int32)\r\nalignments, batch_sizes = packing_ops.packed_sequence_alignment(sequence_lengths, name='align')\r\npacked = packing_ops.pack_sequence(sequence, alignments, batch_sizes)\r\nunpacked = packing_ops.unpack_sequence(packed, alignments, batch_sizes)\r\n\r\nwith tf.Session() as sess:\r\n    _sequence_lengths, _sequence, _packed, _unpacked = sess.run([\r\n        sequence_lengths, sequence, packed, unpacked\r\n    ])\r\n    print(\"Sequence lengths: {}\".format(_sequence_lengths))\r\n    print(\"Sequence: \\n{}\".format(_sequence[:, :, 0]))\r\n    print(\"Packed: {}\".format(_packed[:, 0]))\r\n    print(\"Unpacked: \\n{}\".format(_unpacked[:, :, 0]))\r\n```\r\n\r\n> Sequence lengths: [6 3 2]\r\n> Sequence: \r\n> [[ 1.  2.  3.]\r\n>  [ 4.  5.  6.]\r\n>  [ 7.  8.  9.]\r\n>  [10. 11. 12.]\r\n>  [13. 14. 15.]\r\n>  [16. 17. 18.]]\r\n> Packed: [ 1.  2.  3.  4.  5.  6.  7.  8. 10. 13. 16.]\r\n> Unpacked: \r\n> [[ 1.  2.  3.]\r\n>  [ 4.  5.  6.]\r\n>  [ 7.  8.  0.]\r\n>  [10.  0.  0.]\r\n>  [13.  0.  0.]\r\n>  [16.  0.  0.]]", "@qlzh727 Just got everything working! Could use some polish if anyone has time to give it a review, but the code seems to be working correctly and is enough to get started training. Remaining tasks (can be follow-up issues):\r\n* Maybe refactor the packing kernels. Could probably make them faster. Lots of ways to write that kernel.\r\n* Only made the variable length rnn for int32 indices. Should probably template that out.\r\n* I added tests but can always add more. Only wrote tests for forward, not for backward yet.\r\n* Should setup some benchmarking\r\n* I'm pretty sure the descriptors are being deleted correctly but I would appreciate if anyone can take a look\r\n\r\nAnyways, here is a worked example of a bidirectional CuDNN LSTM with variable length sequences in tensorflow. You can see the variable lengths are working correctly in the output below.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import CudnnLSTM, CUDNN_RNN_BIDIRECTION\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops import packing_ops\r\n\r\nlengths = [6,3,2]\r\nl = lengths[0]\r\nn = len(lengths)\r\nm = 1\r\ndata_dtype = tf.float32\r\nindex_dtype = tf.int32\r\n\r\n# Create standard CudnnLSTM\r\nlstm = CudnnLSTM(\r\n    num_layers=1, num_units=m, dtype=data_dtype, direction=CUDNN_RNN_BIDIRECTION,\r\n    kernel_initializer=tf.initializers.constant(0.2),\r\n    bias_initializer=tf.initializers.constant(0.2))\r\n\r\n# Standard 3d sequence inputs\r\ninputs = tf.ones(shape=(l, n, m), dtype=data_dtype)\r\n# Length of each sequence in decreasing order\r\nsequence_lengths = tf.constant(lengths, dtype=index_dtype)\r\n# Calculate alignments\r\nalign = packing_ops.packed_sequence_alignment(sequence_lengths)\r\n# Pack the sequence\r\npacked_inputs = packing_ops.pack_sequence(inputs, *align)\r\n# Run the LSTM\r\npacked_outputs, _ = lstm(packed_inputs, sequence_lengths=sequence_lengths)\r\n# Unpack the outputs\r\nunpacked_outputs = packing_ops.unpack_sequence(packed_outputs, *align)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    _sequence_lengths, _packed_outputs, _unpacked_outputs = sess.run([\r\n        sequence_lengths, packed_outputs, unpacked_outputs])\r\n    print(\"Sequence lengths: {}\".format(_sequence_lengths))\r\n    print(\"Unpacked outputs forward: \\n{}\".format(_unpacked_outputs[:, :, 0]))\r\n    print(\"Unpacked outputs backward: \\n{}\".format(_unpacked_outputs[:, :, 1]))\r\n```\r\n\r\n> Sequence lengths: [6 3 2]\r\n> Unpacked outputs forward: \r\n> [[0.21531965 0.21531965 0.21531965]\r\n>  [0.35149407 0.35149407 0.35149407]\r\n>  [0.43308067 0.43308067 0.        ]\r\n>  [0.4820674  0.         0.        ]\r\n>  [0.5121958  0.         0.        ]\r\n>  [0.53126556 0.         0.        ]]\r\n> Unpacked outputs backward: \r\n> [[0.53126556 0.43308067 0.35149407]\r\n>  [0.5121958  0.35149407 0.21531965]\r\n>  [0.4820674  0.21531965 0.        ]\r\n>  [0.43308067 0.         0.        ]\r\n>  [0.35149407 0.         0.        ]\r\n>  [0.21531965 0.         0.        ]]", "Thanks for the contribution. Seems to be a very large PR, which will take some time on our side to digest. \r\n\r\nJames and Guangda, can both of you take a look first? Feel free to add more reviewer for this PR.", "As far as I remember, you also need to sort the seqs by seq length, in descending order. Are you doing this?\r\n\r\nBtw, check out Microsoft CNTK code which wraps cuDNN which seems to do all of that correctly.\r\n", "@albertz yes, but that uses ops in tensorflow already. An actual example goes like this. Will definitely need some examples for people to get it right.\r\n\r\n```python\r\n        u_order = argsort(utterance_lengths, direction='DESCENDING')\r\n        u_backorder = argsort(u_order)\r\n        sorted_lengths = tf.gather(utterance_lengths, indices=u_order, axis=0)\r\n        sorted_utterances = tf.gather(utterances, indices=u_order, axis=1)\r\n        u_align = packed_sequence_alignment(sorted_lengths)\r\n        u_packed = pack_sequence(sorted_utterances, *u_align)\r\n\r\n        with tf.variable_scope(\"Listener\") as listener_scope:\r\n            h, _ = CudnnLSTM(num_layers=3, num_units=params.listener_dim, direction=CUDNN_RNN_BIDIRECTION)(\r\n                u_packed, sequence_lengths=sorted_lengths\r\n            )\r\n            packed_logits = slim.fully_connected(h, num_outputs=vocab_size+1, activation_fn=None, scope='logits')\r\n            logits = unpack_sequence(packed_logits, *u_align)\r\n            logits = tf.gather(logits, indices=u_backorder, axis=1)\r\n```", "Actually, it might be worth making a combined operation that does the sorting and packing together. The LSTM itself is pretty straightforward. The packing operations are where we might want to make some changes.", "I've just started training on a BiLSTM with CTC loss. Iterations per second goes from 0.29 to 3.15102 by switching from `bidirectional_dynamic_rnn` to `CudnnLSTM`. So maybe the packing and sorting could be faster, but I'm getting about 10x speedup already.\r\n\r\nThis is on sequence lengths about 2500, 64 dims, batch size 8, 3 layer bilstm with 256 units.", "I'm thinking it might actually make sense to rewrite pack and unpack as reshapes and scatters.", "So it would be possible to do the sorting and packing together.\r\n* An operation with inputs (sequence lengths, batch_order) that outputs 2 alignments(fwd_align, back_align)\r\n* packing is gather(reshape(x, (s*b, d)), fwd_align, axis=0)\r\n* unpacking is reshape(scatter(x, back_align, axis=0, output_Shape=(s*b,d)), (s,b,d))\r\n", "Hi guys! Just redid the packing. Now there is just one custom operation `SequenceGatherScatterIndices`. This operation takes the total sequence length, the sorted sequence lengths, and the sort order. It produces a set of indices that can be used to pack and unpack using gather_nd and scatter_nd, so no longer need those custom ops for packing and unpacking. The single gather_nd or scatter_nd will both remove the padding and sort the batches, so you can do both at the same time.", "I've been training a lot over the weekend and everything seems to be working right. I'm thinking about dropping the packing kernels entirely. It seems like I can get the correct indices with existing operations by doing the below. Might be faster or slower than the custom kernel but definitely easier to write and maintain. Using the fact that boolean_mask seems to take things in deterministic row-major order.\r\n\r\n```python\r\ndef sequence_indices(sorted_sequence_lengths, batch_order):\r\n    mg = tf.meshgrid(tf.range(sorted_sequence_lengths[0]), batch_order, indexing='ij')\r\n    mg = tf.stack(mg, axis=-1)\r\n    mask = tf.transpose(tf.sequence_mask(lengths=sorted_sequence_lengths, maxlen=l), (1, 0))\r\n    idx = tf.boolean_mask(mg, mask)\r\n    return idx\r\n```\r\n\r\nThe LSTM itself seems to be about right. The only improvement might be caching the descriptors somehow. Should be able to reuse the forward descriptors for the backward pass instead of regenerating but I'm not sure exactly where that would go and how much of an improvement that would actually make.", "@albertz @qlzh727 Just did some cleanup and reverted a fair number of no-longer-necessary changes. Only major remaining tasks I think are documentation, formatting and maybe some extra testing. You can use `sequence_gather_scatter_indices` to go from unsorted sequence lengths to indices. You can then `gather_nd` and `scatter_nd` to use that sequence with variable length `CudnnLSTM`.\r\n\r\nDoes anyone know more about `V2`? I'm not sure what tests should be added for that specifically. There is also this comment in the code:\r\n```\r\n# TODO(jamesqin): switch default value to \"1\" on May 25th 2018, and get rid\r\n  # of V1 ops. \r\n```", "Looks like my editor trashed permissions so I'll reset those when I get a chance.", "Thank you Ben. I think it's likely this feature would be support natively in later Cudnn versions. I'd suggest not making more changes to it for now.", "@protoget Why would that be the case? I wouldn't expect nvidia to drop support. You can't emulate bilstm with just masking and it is >10x faster than a `bidirectional_dynamic_rnn`.", "Actually, maybe I'm not clear on what you mean. You think the creating of the descriptors is going to be rolled into CuDNN? That sounds cool.", "I mean later CudnnRNNForwardTrain() API in Cudnn might support sequence_length as a new param, so TF side we don't need to add additional ops to handle padding.", "Do you have a link or something? `cudnnRNNForwardTraining` has `seqLength` but that is just an int.", "We have been working closely with Nvidia in discussion of new features in various areas. Sorry I couldn't provide doc/link at this point.", "Aha! Some insider stuff I'll let it be. The PR as is works and gets my model running in reasonable time. I'll leave it up just-in-case but sounds like there will be some other changes coming.\r\n\r\nWould probably still need a function like `sequence_indices` to do the packing and unpacking but that would be a small change.", "> CudnnRNNForwardTrain\r\n\r\n@protoget  When will `tf.contrib.cudnn_rnn._CudnnRNN` support seqLength?\r\nSomeone give me a new PR ?\r\n\r\n@bstriner Here, document of `CudnnRNNForwardTrain`:\r\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNForwardTraining\r\n\r\n```\r\nseqLength\r\nInput. Number of iterations to unroll over. The value of this seqLength must not exceed the value that was used in cudnnGetRNNWorkspaceSize() function for querying the workspace size required to execute the RNN.\r\n```\r\n", "@shahzadlone Any update please ?", "@shahzadlone gentle ping", "@bstriner could you please resolve the conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 22307, "title": "Tensorflow library loading order causes segmentation fault ", "body": "My application depends on both RocksDB and tensorflow. If I load rocksDB shared library before loading the tensorflow model in my application then it results in SIGSEGV. However, if the loading order is reversed then there're no exceptions(and it works fine as expected). \r\n\r\nHere're the external dependencies of my application:\r\nTensorflow  version: 1.6\r\nRocksDB version: 5.7.3\r\nJava version: JDK-8\r\nOS platform and distribution: RedHat-7(RHEL-7.2)\r\n\r\nThe following code uses both RocksDB and tensorflow results in segmentation fault:\r\n\r\n```java\r\npublic class LibLoad{\r\n    public static String absPath(String relativePath) {\r\n\treturn System.getProperty(\"user.dir\") + \"/\" + relativePath;\r\n    }\r\n    public static void main(String[] args) throws InterruptedException {\r\n      System.load(LibLoad.absPath(\"libs/librocksdbjni6775043347215777920.so\"));\r\n      System.load(LibLoad.absPath(\"libs/tensorflow_native_libraries-1534022257987-0/libtensorflow_framework.so\"));\r\n    }\r\n}\r\n```\r\n\r\nActual exception:\r\n```\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGSEGV (0xb) at pc=0x0000000000000000, pid=3773, tid=140566680151808\r\n#\r\n# JRE version: Java(TM) SE Runtime Environment (8.0_05-b13) (build 1.8.0_05-b13)\r\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.5-b02 mixed mode linux-amd64 compressed oops)\r\n# Problematic frame:\r\n# C  0x0000000000000000\r\n#\r\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \u201culimit -c unlimited\u201d before starting Java again\r\n#\r\n# An error report file with more information is saved as:\r\n# /export/content/lid/apps/careers-banzai-jobs-embedding-samza/dev-i001/hs_err_pid3773.log\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   http://bugreport.sun.com/bugreport/crash.jsp\r\n# The crash happened outside the Java Virtual Machine in native code.\r\n# See problematic frame for where to report the bug.\r\n#\r\n  \r\n6  <signal handler called>\r\n#7  0x0000000000000000 in ?? ()\r\n#8  0x00007f17b8dbfbb0 in pthread_once () from /lib64/libpthread.so.0\r\n#9  0x00007f16dd60545a in void std::call_once<void (&)()>(std::once_flag&, void (&)()) ()\r\n   from libtensorflow_framework.so\r\n#10 0x00007f16dd60549e in tensorflow::port::TestCPUFeature(tensorflow::port::CPUFeature) ()\r\n \r\n```\r\n\r\nSimilar issues had been reported previously in the community and the suggestion solution was to load tensorflow shared library before the RocksDB shared library. \r\n\r\n1. Just curious about the actual root cause of this segmentation fault. Can someone help me on how to get to the bottom of it.\r\n2. What is the recommended long-term solution for this problem? Does the same codeflow that caused segmentation fault when loading the tensorflow model get triggered in other scenarios as well?\r\n\r\nAny help for solving this problem is greatly appreciated.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@jhseu @asimshankar in case they have any ideas.\r\nI would recommend building TF with debug symbols and retrying this to start.", "CC @m3bm3b in case he has ideas. From a cursory look at the stacktrace it seems `pthread_once` is somehow leading to an invalid program address?", "I agree that debug symbols could help, if you can get a symbolic stack trace from the debugger.\r\n\r\npthread_once() is fairly straightforward.\r\nI'm slightly suspicious that the libstdc++ implementation of call_once() \r\nis rather complicated (e.g., it uses TLS).   \r\nI'd be tempted to try replacing the use of call_once() \r\nwith a direct call to pthread_once() to see whether the problem is still present.\r\n\r\n", "@shanthoosh Using symbolic stack trace would work, closing this and feel free to open a new issue if any other errors.", "you can try to upgrade rocksDB to solve this issue. it works in my situation."]}, {"number": 22306, "title": "why options in session_config like intra_op_parallelism_threads/inter_op_parallelism_threads doesn't work in tensorflow estimator distribution mode\uff1f", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:just tried to import it.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:CentOS Linux release 7.2\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.7 to 1.10\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **CPU model and memory**:Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz,128G\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\nwhen i set intra_op_parallelism_threads/inter_op_parallelism_threads for estimator dist mode like this:\r\n`config = tf.ConfigProto()\r\nconfig.intra_op_parallelism_threads = 4\r\nconfig.inter_op_parallelism_threads = 4\r\nrun_config = tf.estimator.RunConfig().replace(session_config=config)\r\nmodel = tf.estimator.DNNLinearCombinedClassifier(\r\n            model_dir=model_dir,\r\n            linear_feature_columns=wide_columns,\r\n            dnn_feature_columns=deep_columns,\r\n            dnn_hidden_units=hidden_units,\r\n            config=run_config)`\r\nit doesn't work, i find the options isn't passed to kernals:\r\n\r\ntensorflow/python/estimator/training.py\r\n`if config.session_config is None:\r\n  session_config = config_pb2.ConfigProto(log_device_placement=False)\r\nelse:\r\n  session_config = config_pb2.ConfigProto(\r\n\t  log_device_placement=False,\r\n\t  gpu_options=config.session_config.gpu_options)`\r\nonly a few options are set, the code above indicate intra_op_parallelism_threads/inter_op_parallelism_threads have no effect\uff0cis a bug\uff1fwhy\uff1f\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@jart ", "@skye I suppose the user needs to switch to GraphRunner in this case?", "Nagging Assignee @jart: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you please take a look at this PR and see if it helps? \r\nhttps://github.com/tensorflow/models/pull/5046/files#diff-a418c41de193111b623e6c168f552206R51 \r\nThe above PR was effective for wide and deep in github.com/tensorflow/models \r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22305, "title": "Use own model on android demo(TF_OD) make app crash", "body": "My stack overflow question: https://stackoverflow.com/questions/52309707/use-own-model-on-android-demo-tf-od-but-app-was-crash-after-open-camera\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n    I had change camera from back to front.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    Windows10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n    Zenfone 5z\r\n- **TensorFlow version (use command below)**:\r\n    v1.10.0\r\n- **Python version**:\r\n    Python 3.5.4    \r\n- **GPU model and memory**:\r\n    GTX 950M,  Memory 8GB\r\n- **TensorFlow installed from**:\r\n    CMD\r\n- **CUDA/cuDNN version**:\r\n    CUDA v9.0  /  cuDNN v7.0\r\n- **Bazel version**:\r\n    0.11.1\r\n- **Exact command to reproduce**:\r\n    N/A\r\n\r\n\r\n-----------------------------------------------------------------------------------------------------------------\r\nI trained my model with this tutorial: https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10\r\n\r\nWhen I use the default model(ssd_mobilenet_v1_android_export.pb), the app can run correctly, but when I use my own model, the app was crash without detecting anything.\r\n\r\nI had change TF_OD_API_MODEL_FILE and TF_OD_API_LABELS_FILE to my file,  also I I created a labelmap.txt , that contains all the label what i trained, and I reserve the first line with \"???\"\r\n\r\nWhat I confused is the app can open the camera and display a little time, but before detecting anything, the app was crash. So I think the problem maybe is my model wrong, but when I used my model with python code, it can run well and detect correctly.\r\n\r\nSo why my own model use on android demo(TF_OD) make app crash?\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nExact command to reproduce", "What's the error when it crashes?", "I had read this article: https://towardsdatascience.com/detecting-pikachu-on-android-using-tensorflow-object-detection-15464c7a60cd\r\n\r\nIn the end of article, the writer said that he has encounter problem that let his model run on android.\r\n> in the export.py file (called by export_inference_graph.py), the parameter optimize_graph to False. Apparently, and as far as I understood, by optimizing the model, my model was creating some nodes not recognizable by the Android library, and thus, it was crashing. \r\n\r\nSo I think maybe I have the same problem and then go to check my \"export_inference_graph.py\", but I not found any parameter is \"optimize_graph\". \r\n Is any one has some idea what's wrong with my model?", "I tried to train again with faster_rcnn_inception_v2, and it can work on my cell phone, but run too slow.\r\nSo I'll try to train with ssd_mobilenet_v1 to see if it has any problem."]}]