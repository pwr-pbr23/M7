[{"number": 48526, "title": "Shard data across TFRecords", "body": "**System information**\r\n- Code straight from the documentation at https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter \r\n- Google Colab with TPU\r\n- TensorFlow version 2.4\r\n\r\n**Describe the current behavior**\r\n\r\nThis code works and writes a file to Google Cloud Storage\r\n\r\n```\r\ndataset = tf.data.Dataset.range(3)\r\ndataset = dataset.map(tf.io.serialize_tensor)\r\nwriter = tf.data.experimental.TFRecordWriter(\"/path/to/file.tfrecord\")\r\nwriter.write(dataset)\r\n```\r\n\r\nBut this code does not write any file. It also does not throw any error. Also, I can't for the life of me figure out how to print filename to debug that part.\r\n\r\n```\r\ndataset = tf.data.Dataset.range(3)\r\ndataset = dataset.map(tf.io.serialize_tensor)\r\n\r\ndef reduce_func(key, dataset):\r\n  filename = tf.strings.join(['/path/to/', tf.strings.as_string(key)])\r\n  writer = tf.data.experimental.TFRecordWriter(filename)\r\n  writer.write(dataset.map(lambda _, x: x))\r\n  return tf.data.Dataset.from_tensors(filename)\r\n\r\ndataset = dataset.enumerate()\r\ndataset = dataset.apply(tf.data.experimental.group_by_window(\r\n  lambda i, _: i % 2, reduce_func, tf.int64.max\r\n))\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nI would expect the second bit of code to write a TFRecord file.", "comments": ["@ymodak,\r\nI was able to reproduce the issue with TF v2.4.1, TF v2.5.0rc1 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5f0aadb91702f31f21a8d412cd3f85da/48526.ipynb). Thanks!", "@NA-Dev Datasets produce data on-demand, so to trigger the writing you need to iterate through the dataset, e.g. with\r\n```\r\nfor _ in dataset: pass\r\n```\r\nor\r\n```\r\n_ = list(dataset)\r\n```\r\n\r\nSorry for the confusion, I'm sending a PR to add the execution step to the doc.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48526\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48526\">No</a>\n"]}, {"number": 48525, "title": "Fix 'Illegal ambiguous match' on Windows with `--config=nogcp/noaws`.", "body": "At the moment, compiling the target `//tensorflow/core:core_cpu_internal` on Windows in combination with either `--config=nogcp` or `--config=noaws` (or of course both) gives the following Bazel error:\r\n\r\n```\r\nIllegal ambiguous match on configurable attribute \"deps\" in \r\n//tensorflow/core/common_runtime:core_cpu_internal:\r\n//tensorflow:windows\r\n//tensorflow:no_gcp_support\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\nERROR: Analysis of target '//tensorflow/core:core_cpu_internal' failed; build aborted\r\n```\r\n\r\nThis is because the select statement here matches both `//tensorflow:windows` and `//tensorflow:no_gcp_support`: https://github.com/tensorflow/tensorflow/blob/a07640071d446365e0ee6584cc8e0e953cb7afaf/tensorflow/core/platform/default/build_config.bzl#L661-L672\r\n(or similarly [here](https://github.com/tensorflow/tensorflow/blob/a07640071d446365e0ee6584cc8e0e953cb7afaf/tensorflow/core/platform/default/build_config.bzl#L682-L693) in the `noaws` case)\r\n\r\nSince the `\"//conditions:default\"` arm is to include the GCS dependency, there's no need to explicitly match against `//tensorflow:windows` in order to include it, so this PR fixes the problem by removing the Windows-specific arm. This doesn't affect the default behaviour (*not* having `--config=nogcp`) but resolves the ambiguous match otherwise.", "comments": ["Hi @rohan100jain, I don't suppose you've had a chance to take a look at this yet?", "Hi @allenlavoie, thanks for the review :)\r\n\r\nDid internal CI fail for some reason? I didn't think the `Linux GPU` failures were related to this PR but maybe I'm wrong."]}, {"number": 48524, "title": "ImportError: cannot import name 'RMSprop' from 'tensorflow.python.keras.optimizers'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- N/A\r\n- TensorFlow installed from (source or binary):\r\nby pip install tensorflow-cpu\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n-3.9.2\r\n- Bazel version (if compiling from source):\r\n-N/A\r\n- GCC/Compiler version (if compiling from source):\r\nnot compile\r\n- CUDA/cuDNN version:\r\nnot use GPU\r\n- GPU model and memory:\r\nnot use GPU\r\n\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nv2.5.0-rc0-36-g0d1805aede0 2.5.0-rc1\r\n\r\n**Describe the current behavior**\r\nWARNING:tensorflow:From /home/azuryl/anaconda3/envs/nmtpy=3.8/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\nTraceback (most recent call last):\r\n  File \"t/cnn/Neural-Machine-Translation-NMT-/NMT.py\", line 24, in <module>\r\n    from tensorflow.python.keras.optimizers import RMSprop\r\nImportError: cannot import name 'RMSprop' from 'tensorflow.python.keras.optimizers' (/home/azuryl/anaconda3/envs/nmtpy=3.8/lib/python3.9/site-packages/tensorflow/python/keras/optimizers.py)\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["The folder is https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras/optimizer_v2", "Also I suggest to use the public API namespace `tf.keras.optimizers.RMSprop`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@azuryl ,\r\n\r\nCould you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status.\r\n\r\nThanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48524\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48524\">No</a>\n"]}, {"number": 48522, "title": "Disable AWS and HDFS support by default", "body": "This PR disables AWS and HDFS support by default, as was discussed in:\r\nhttps://docs.google.com/document/d/1CB51yJxns5WA4Ylv89D-a5qReiGTC0GYum6DU-9nKGo/edit#heading=h.8ifswzr0wtbp\r\n\r\n/cc @mihaimaruseac @vnvo2409 @kvignesh1420 \r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@mihaimaruseac wondering if there is any update on this PR?", "@yongtang , there is a PR that is related to the CI failed on Windows. I think you need to rebase this PR. https://github.com/tensorflow/tensorflow/pull/48525", "@vnvo2409 Thanks. I have updated the PR with rebase.", "@mihaimaruseac The PR has been updated and I make an additional line change to fix the CI issue (similar to  #48525 but slightly different). It looks like all CI tests pass now. Please take a look and see if any additional change is needed.", "@gbaned @mihaimaruseac Any update on this PR?"]}, {"number": 48521, "title": "[TFLite] Potential out-of-bound array access in LOG_SOFTMAX optimized kernel", "body": "Hello,\r\n\r\nThe optimized LOG_SOFTMAX operator seems to have a bug that may lead to out-of-bound memory accesses.\r\n\r\nThe following [access](https://github.com/tensorflow/tensorflow/blob/89133a6a74da4b976a585b66a09a6601fce3b217/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L4143) is equivalent to `params.table[input_data[j] + max_q8 - max_val]` which can be be negative with int8 input_data.\r\n\r\nExample:\r\n```\r\ninput_data = [-109, -9, 0, 12, 78]\r\nmax_q8 = 127\r\nmax_val = 78\r\n```\r\n\r\nAccess for `j = 0` is `params.table[-109 + 127 - 78]` which result in a `params.table[-60]` access.\r\n\r\nThe tests currently don't highlight the bug by luck as we end-up accessing other fields in `LogSoftmaxOpData` but replacing [`LogSoftmaxOpData`](https://github.com/tensorflow/tensorflow/blob/89133a6a74da4b976a585b66a09a6601fce3b217/tensorflow/lite/kernels/activations.cc#L87) to remove all the unused parameters (or even just putting SoftmaxParams as fist member):\r\n```c++\r\nstruct LogSoftmaxOpData {\r\n    struct SoftmaxParams params = {};\r\n    float f_table[256];\r\n};\r\n```\r\n\r\nmakes the tests fail when running the tests in `bazel test -c opt` mode (though it may vary as it's undefined). \r\n\r\nValgrind output on the test binary:\r\n```\r\n[ RUN      ] LogSoftmaxOpTest/LogSoftmaxOpTest.LogSoftmaxInt8/0\r\n==8559== Invalid read of size 4\r\n==8559==    at 0x697D124: void tflite::optimized_ops::LogSoftmax<signed char>(tflite::SoftmaxParams const&, float, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char*) (optimized_ops.h:4143)\r\n==8559==    by 0x6976EF9: TfLiteStatus tflite::ops::builtin::activations::LogSoftmaxEval<(tflite::ops::builtin::activations::KernelType)1>(TfLiteContext*, TfLiteNode*) (activations.cc:1250)\r\n==8559==    by 0x13626C77: tflite::Subgraph::OpInvoke(TfLiteRegistration const&, TfLiteNode*) (subgraph.h:430)\r\n==8559==    by 0x13623ED3: tflite::Subgraph::Invoke() (subgraph.cc:1062)\r\n...\r\n```\r\n\r\nThibaut\r\n", "comments": ["@Tessil \r\nCould you please fill the issue template\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following details\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory\r\nand the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "@jianlijianli @renjie-liu @thaink could you take a look?", "Hi Tessil, see here https://github.com/tensorflow/tensorflow/blob/89133a6a74da4b976a585b66a09a6601fce3b217/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L3780\r\n\r\nit's always `const int32_t max_uint8 = std::numeric_limits<uint8_t>::max();` right?", "Hi,\r\n\r\nIt seems you're looking at the wrong kernel (Softmax instead of LogSoftmax).\r\n\r\nThe line is \r\nhttps://github.com/tensorflow/tensorflow/blob/de6d871ce4214e2856d7129deef579f2d64c8cc3/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L4137\r\n\r\nwhere `T` will be `int8_t` for the int8 version of the kernel.", "Oh Thanks, will send out a fix later", "Fixed at https://github.com/tensorflow/tensorflow/commit/40fa5028dcbd9f3a70afdfc7ed98732f69235040", "Perfect, thank you very much. I will close the ticket then.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48521\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48521\">No</a>\n"]}, {"number": 48519, "title": "[TFLite] Fix the (u)int8 reference LOG_SOFTMAX operator by properly precomputing the kernel parameters", "body": "Hello,\r\n\r\nThe https://github.com/tensorflow/tensorflow/commit/6e8b672422ab9522dcc29dd8d6bfc164e7c0aade commit introduced an optimized kernel for the LOG_SOFTMAX operator using LUT but by inadvertence removed the pre-processing needed for the reference kernel while doing so. This PR restores the pre-processing and adds some tests for the reference kernel.\r\n\r\nThibaut", "comments": ["@renjie-liu @jianlijianli could you review this PR?"]}, {"number": 48518, "title": "The repository 'file:/var/nccl-repo-2.2.13-ga-cuda9.2  Release' no longer has a Release file. Ubuntu-18.04", "body": "I am getting this error while enabling the GPU support. \r\nI am following the steps as mentioned here.\r\nhttps://www.tensorflow.org/install/gpu\r\n![err](https://user-images.githubusercontent.com/79437844/114683080-3e416a00-9cff-11eb-9994-035dd9f21d57.png)\r\n", "comments": ["@Rahul2203  Could you please provide the following information.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nAlso, Please provide the exact sequence of commands that you have executed before running into the error. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48518\">No</a>\n"]}, {"number": 48517, "title": "Enable broadcast send from non-collective group leader", "body": "This PR enables broadcast send(collective) from the non-collective group leader. The current master code ignores the `is_source` value in `CompleteGroupRequest` to raise the internal error if a non-collective group leader tries to send a tensor using BcastSend. The error message is as follows:`Instance 7 found no source for broadcast.  This could mean that there were group_size=2 BcastRecvs but no BcastSend.`. ", "comments": ["@rohan100jain Is the broken CI failure a problem of the code? Can you let me know if I can fix it?", "@allenlavoie I added unit tests. The master branch passes `DeviceResDistTest.BroadcastSourceRank0` while it fails for `DeviceResDistTest.BroadcastSourceRank3`."]}, {"number": 48516, "title": "PR to port resize_bilinear to TFLM broke the Xtensa build", "body": "https://github.com/tensorflow/tensorflow/pull/43426 broke the Xtensa build:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=vision_p6 XTENSA_CORE=P6_200528 micro_allocator_test -j8\r\n```\r\n\r\npasses if we revert the PR and fails on tip of tree with:\r\n```\r\nxt-clang++ -std=c++11 -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DXTENSA -DXTENSA -DTF_LITE_MCU_DEBUG_LOG -DTF_LITE_USE_CTIME --xtensa-core=P6_200528 -mcoproc -DMAX_RFFT_PWR=9 -DMIN_RFFT_PWR=MAX_RFFT_PWR -DVISION_P6 -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -o tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/bin/micro_allocator_test tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/obj/tensorflow/lite/micro/micro_allocator_test.o tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/obj/tensorflow/lite/micro/testing/test_conv_model.o tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm\r\ntensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/lib/libtensorflow-microlite.a(memory_helpers.o): In function `tflite::BytesRequiredForTensor(tflite::Tensor const&, unsigned int*, unsigned int*, tflite::ErrorReporter*)':\r\nmemory_helpers.cc:(.text._ZN6tflite22BytesRequiredForTensorERKNS_6TensorEPjS3_PNS_13ErrorReporterE+0x13f): dangerous relocation: call8: call target out of range: _Assert\r\ntensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/lib/libtensorflow-microlite.a(micro_allocator.o): In function `tflite::internal::InitializeTfLiteTensorFromFlatbuffer(tflite::SimpleMemoryAllocator*, bool, tflite::Tensor const&, flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> > const*, tflite::ErrorReporter*, TfLiteTensor*)':\r\nmicro_allocator.cc:(.text._ZN6tflite8internal36InitializeTfLiteTensorFromFlatbufferEPNS_21SimpleMemoryAllocatorEbRKNS_6TensorEPKN11flatbuffers6VectorINS6_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP12TfLiteTensor+0x47d): dangerous relocation: call8: call target out of range: _Assert\r\ntensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/lib/libtensorflow-microlite.a(micro_allocator.o): In function `tflite::internal::InitializeTfLiteEvalTensorFromFlatbuffer(tflite::SimpleMemoryAllocator*, tflite::Tensor const&, flatbuffers::Vector<flatbuffers::Offset<tflite::Buffer> > const*, tflite::ErrorReporter*, TfLiteEvalTensor*)':\r\nmicro_allocator.cc:(.text._ZN6tflite8internal40InitializeTfLiteEvalTensorFromFlatbufferEPNS_21SimpleMemoryAllocatorERKNS_6TensorEPKN11flatbuffers6VectorINS6_6OffsetINS_6BufferEEEEEPNS_13ErrorReporterEP16TfLiteEvalTensor+0x140): dangerous relocation: call8: call target out of range: _Assert\r\n/home/advaitjain/xtensa/XtDevTools/install/tools/RI-2020.4-linux/XtensaTools/bin/xt-ld: error: 3 warnings, treating warnings as errors\r\nclang-6.0: error: Xtensa-ld command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:788: tensorflow/lite/micro/tools/make/gen/xtensa_vision_p6_default/bin/micro_allocator_test] Error 1\r\n```\r\n\r\nCouple of things to note:\r\n * The PR author (@patriklaurell) does not have any way to reproduce the errors, and we do not currently have any CI for the Xtensa toolchain.\r\n * Only one of 3 xtensa builds fails so there is a distinct possibility that the issue here is some subtlety with the Xtensa toolchain rather than the PR itself. However, the code that the Xtensa linker is complaining about it untouched by the PR that causes the error, so something odd is happening.\r\n * The micro_allocator_test is currently disabled for bluepill and stm32f4 (the two platforms that test for dynamic memory allocation etc.)\r\n * PR #43426 was created right around the time where TFLM had new guidelines for sending smaller PRs as part of OP porting and it was treated as an exception to the rule (which meant the PR is quite large and touches a lot of files).\r\n\r\nNext steps:\r\n * https://github.com/tensorflow/tensorflow/pull/48515 reverts #43426 so that the Xtensa build is green again.\r\n * We should break up #43426 into smaller PRs following the newer guidelines for OP porting.\r\n * We should enable the micro_allocator_test on bluepill sooner rather than later\r\n\r\nTagging some of the folks that have a stake in this: @nyadla-sys @patriklaurell @freddan80 @petewarden ", "comments": ["The resize_bilinear porting PR was causing the bluepill tests to fail as well. https://github.com/tensorflow/tensorflow/pull/48529 adds those to the presubmit checks and this should give @patriklaurell a way to reproduce at least one error without needing the Xtensa toolchain.\r\n\r\nNote that the error itself is quite unexpected (for bluepill, the network_tester_test fails) and I still don't really have much insight on the root cause.", "> The resize_bilinear porting PR was causing the bluepill tests to fail as well. #48529 adds those to the presubmit checks and this should give @patriklaurell a way to reproduce at least one error without needing the Xtensa toolchain.\r\n> \r\n> Note that the error itself is quite unexpected (for bluepill, the network_tester_test fails) and I still don't really have much insight on the root cause.\r\n\r\nI'm unsure exactly what state I was in when I saw the errors from the quoted comment.\r\n\r\nContrary to what I said above, restoring the resize_bilinear changes on top of master does not break the bluepill build. This has been confirmed by both me and @patriklaurell.\r\n\r\nAdditionally, we have #48619 that should prevent the Xtensa build from being broken.\r\n\r\nWe will still port over resize_bilinear is smaller chunks but I don't expect it to cause any regressions.", "Note, we have a somewhat better documented case of a test passing for bluepill with `-Os` but failing without it:\r\nhttps://source.cloud.google.com/results/invocations/b1bccea3-e476-4a7c-b5d7-b51913a0378d/targets\r\n\r\nwhich is a log from the first commit of https://github.com/tensorflow/tensorflow/pull/49485\r\n\r\nAnd also mentioned in https://github.com/tensorflow/tensorflow/pull/48758#issuecomment-846302451\r\n"]}, {"number": 48515, "title": "Revert \"Support RESIZE_BILINEAR in TFLu\"", "body": "Reverts tensorflow/tensorflow#43426 and fixes the Xtensa build.\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/48516 for more context and next steps.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@patriklaurell @freddan80 "]}, {"number": 48514, "title": "Mapping an OpDef into a TF op (Python API)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF2\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI'm currently for a way to map an OpDef into a TF op. Currently, there is no way to retrieve the Python bindings for a certain TF op given the OpDef (there isn't a way to access the op registry from the Python API). It's possible to export the OpList proto and load that into Python easily, but looking a way to access the actual python bindings for any registered TF op.\r\n\r\n**Will this change the current api? How?**\r\n\r\nUnclear, also made this post asking if such a feature was already available.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAny end users who wish to work at the TF op level and want to access more detailed information about each op (number of arguments attributes, etc.)\r\n\r\n**Any Other info.**\r\n", "comments": ["You can get some information about ops (their names and argument names) via `tf.raw_ops.*` (e.g. `tf.raw_ops.Add`). I don't know if we want to put protos in that API directly, but potentially those objects could include more information about the op.\r\n\r\nIf something were attached to the `tf.raw_ops` endpoints, what specifically would you want? Type constraints? Attributes vs. inputs?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48513, "title": "TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'", "body": "I am trying to have a model that can get different input_shapes. I use a **(128,128,1)** input_shape for for training data as all my training images have the same size.\r\n\r\nHowever, after training the image size will be variable (e.g.; **312x256, 550x2300**, etc). How to create a model that can get variable input shapes. The model below raises error in **`get_crop_shape`**, if I use input_shape as **[None, None,1]**. I have to use **`get_crop_shape`** to prevent problems during concatenations as tensor shape will be different if the shape is not multiple of 128.\r\n\r\nif I use any shape excepts None (e.g.; **[128, 128, 1]** or **[129, 239, 1]**), the model works!\r\n\r\nI need the model accepts different input_shapes, so I can use it in tensorflow/java.\r\n\r\nIn Python, I create model, perform training and then save the model as **`model.save(\"model\")`**. Then, I create a new model based on the shape of new images and only load weights and it works! For instance, if I save my model after training as **`model.save(\"model\")`**. I can use it for different image shapes as below.\r\n\r\n```\r\nimport tensorflow as tf\r\nmodel1 = Unet(input_shape = (129,239,1))\r\nmodel1.load_weights(\"model/variables/variables\")\r\n\r\nmodel2= Unet(input_shape = (2300,3450,1))\r\nmodel2.load_weights(\"model/variables/variables\")\r\n\r\nprediction1 = model1(input_tensor1) # input_tensor1 shape is (None, 129,239,1), None is the batch size.\r\nprediction1 = model2(input_tensor2) # input_tensor2 shape is (None, 2300,3450,1), None is the batch size.\r\n\r\n```\r\n\r\nI ask the same question on tensorflow/java as well. [#288](https://github.com/tensorflow/java/issues/288) \r\n\r\n\r\n\r\n\r\n```\r\nimport os\r\nimport skimage.io as io\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\nimport imageio\r\nimport matplotlib.pyplot as plt\r\nimport copy as copy\r\nimport matplotlib\r\n\r\ndef get_crop_shape(target, query):\r\n\t# the height\r\n\tchannelHeight = target.get_shape()[1] - query.get_shape()[1]\r\n\tassert (channelHeight >= 0)\r\n\tchannelHeight1 = int(channelHeight/2)\r\n\tif channelHeight % 2 != 0:\r\n\t\tchannelHeight2 = channelHeight1 + 1\r\n\telse:\r\n\t\tchannelHeight2 = channelHeight1\r\n\t# the width\r\n\tchannelWidth = target.get_shape()[2] - query.get_shape()[2]\r\n\tassert (channelWidth >= 0)\r\n\tchannelWidth1 = int(channelWidth/2)\r\n\tif channelWidth % 2 != 0:\r\n\t\tchannelWidth2 = channelWidth1 + 1\r\n\telse:\r\n\t\tchannelWidth2 = channelWidth1\r\n\treturn (channelHeight1, channelHeight2), (channelWidth1, channelWidth2)\r\n\r\n\r\ndef getAct(x):\r\n\treturn tf.keras.layers.ReLU()(x)\r\n\r\n\r\ndef Unet(input_shape = (None,None,1), kernelSize = 3, drop_level = 0.5, nChannels = 1):\r\n\tinputs = tf.keras.layers.Input(shape = [input_shape[0], input_shape[1], input_shape[2]])\r\n\tconv1 = tf.keras.layers.Conv2D(64, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(inputs)\r\n\tconv1 =  getAct(conv1)\r\n\tconv1 = tf.keras.layers.Conv2D(64, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv1)\r\n\tconv1 =  getAct(conv1)\r\n\t#drop1 = tf.keras.layers.Dropout(drop_level)(conv1)\r\n\tpool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1) # drop1 --> conv1\r\n\t#\r\n\tconv2 = tf.keras.layers.Conv2D(128, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(pool1)\r\n\tconv2 =  getAct(conv2)\r\n\tconv2 = tf.keras.layers.Conv2D(128, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv2)\r\n\tconv2 =  getAct(conv2)\r\n\t#drop2 = tf.keras.layers.Dropout(drop_level)(conv2)\r\n\tpool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2) #drop2 --> conv2\r\n\t#\r\n\tconv3 = tf.keras.layers.Conv2D(256, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(pool2)\r\n\tconv3 =  getAct(conv3)\r\n\tconv3 = tf.keras.layers.Conv2D(256, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv3)\r\n\tconv3 =  getAct(conv3)\r\n\t#drop3 = tf.keras.layers.Dropout(drop_level)(conv3)\r\n\tpool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)  #drop3 --> conv3\r\n\t#\r\n\tconv4 = tf.keras.layers.Conv2D(512, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(pool3)\r\n\tconv4 =  getAct(conv4)\r\n\tconv4 = tf.keras.layers.Conv2D(512, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv4)\r\n\tconv4 =  getAct(conv4)\r\n\tdrop4 = tf.keras.layers.Dropout(drop_level)(conv4)\r\n\tpool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(drop4)\r\n\t#\r\n\tconv5 = tf.keras.layers.Conv2D(1024, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(pool4)\r\n\tconv5 =  getAct(conv5)\r\n\tconv5 = tf.keras.layers.Conv2D(1024, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv5)\r\n\tconv5 =  getAct(conv5)\r\n\tdrop5 = tf.keras.layers.Dropout(drop_level)(conv5)\r\n\tup6 = tf.keras.layers.Conv2DTranspose(512, kernelSize, strides = (2,2), padding = 'same', kernel_initializer = 'he_normal')(drop5)\r\n\tch, cw = get_crop_shape(drop4, up6)\r\n\tup6 = tf.keras.layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(up6) # add zeropadding.\r\n\tmerge6 = tf.keras.layers.concatenate([drop4,up6], axis = 3)\r\n\t#\r\n\tconv6 = tf.keras.layers.Conv2D(512, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(merge6)\r\n\tconv6 =  getAct(conv6)\r\n\tconv6 = tf.keras.layers.Conv2D(512, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv6)\r\n\tconv6 =  getAct(conv6)\r\n\tup7 = tf.keras.layers.Conv2DTranspose(256, kernelSize, strides = (2,2), padding = 'same', kernel_initializer = 'he_normal')(conv6)\r\n\tch, cw = get_crop_shape(conv3, up7)   #drop3 --> conv3\r\n\tup7 = tf.keras.layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(up7) # add zeropadding.\r\n\tmerge7 = tf.keras.layers.concatenate([conv3,up7], axis = 3)   #drop3 --> conv3\r\n\t#\r\n\tconv7 = tf.keras.layers.Conv2D(256, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(merge7)\r\n\tconv7 =  getAct(conv7)\r\n\tconv7 = tf.keras.layers.Conv2D(256, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv7)\r\n\tconv7 =  getAct(conv7)\r\n\tup8 = tf.keras.layers.Conv2DTranspose(128, kernelSize, strides = (2,2), padding = 'same', kernel_initializer = 'he_normal')(conv7)\r\n\tch, cw = get_crop_shape(conv2, up8) #drop2 --> conv2\r\n\tup8 = tf.keras.layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(up8) # add zeropadding.\r\n\tmerge8 = tf.keras.layers.concatenate([conv2,up8], axis = 3) #drop2 --> conv2\r\n\t#\r\n\tconv8 = tf.keras.layers.Conv2D(128, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(merge8)\r\n\tconv8 =  getAct(conv8)\r\n\tconv8 = tf.keras.layers.Conv2D(128, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv8)\r\n\tconv8 =  getAct(conv8)\r\n\tup9 = tf.keras.layers.Conv2DTranspose(64, kernelSize, strides = (2,2), padding = 'same', kernel_initializer = 'he_normal')(conv8)\r\n\tch, cw = get_crop_shape(conv1, up9) #drop1 --> conv1\r\n\tup9 = tf.keras.layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(up9) # add zeropadding.\r\n\tmerge9 = tf.keras.layers.concatenate([conv1,up9], axis = 3) #drop1 --> conv1\r\n\t#\r\n\tconv9 = tf.keras.layers.Conv2D(64, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(merge9)\r\n\tconv9 =  getAct(conv9)\r\n\tconv9 = tf.keras.layers.Conv2D(64, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n\tconv9 =  getAct(conv9)\r\n\tconv9 = tf.keras.layers.Conv2D(2, kernelSize, padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n\tconv9 =  getAct(conv9)\r\n\tconv10 = tf.keras.layers.Conv2D(nChannels, 1, activation = 'sigmoid')(conv9)\r\n\tmodel = tf.keras.Model(inputs = inputs, outputs = conv10)\r\n\tmodel.compile(optimizer = tf.keras.optimizers.Adam(1e-4, beta_1 = 0.9, beta_2 = 0.999), loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n\tprint(\"Using UnetLeakyPReLU ...\")\r\n\tprint(model.summary())\r\n\treturn model\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@micosacak \r\nCan you please share the error trace? Thanks", "@AdityaKane2001 \r\nHere is the output.\r\n\r\n```\r\n>>>my_model = Unet()\r\n2021-04-14 17:32:12.327108: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-04-14 17:32:12.327370: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<stdin>\", line 37, in Unet\r\n  File \"<stdin>\", line 3, in get_crop_shape\r\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'\r\n```\r\n", "@micosacak \r\nThis may be stupid on my part, but can you resize the inputs and use `input_shape=  (128,128,1)`? Resize layer is available in keras.", "@micosacak,\r\nOn running the code, I am facing an error stating `NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for model/variables/variables`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/ee834abcc2adb4dc8406da9460f8654d/48513.ipynb#scrollTo=XRqGykekNFk6). \r\n\r\nIn order to reproduce the issue reported, could you please provide the TensorFlow version, a minimal code snippet and the supporting files necessary to run the code.\r\n\r\nAlso, take a look at @AdityaKane2001's comment and check if it helps.\r\n\r\nThanks!", "@amahendrakar \r\n```\r\nmodel1 = Unet(input_shape = (129,239,1))\r\nmodel1.load_weights(\"model/variables/variables\")\r\n\r\n```\r\n\r\n`model1.load_weights(\"model/variables/variables\")` will raise error of course. Because you do not have a trained model saved as \"model\" in the path. If you want any dataset you can download an example from [here](http://medicalsegmentation.com/covid19/). But my question does not require an input data, I think. Because it is a more general  question. \r\n\r\nI think you did not get my question. \r\n\r\nMy question is: \r\n\r\n**How to generate a model that can get different input shapes, even after training?** For instance, if I use (128,128,1) as input shape and save the model, the model requires to have always (128,128,1) input shape. If the image shape is different from (128,128,1), then it raises error. If I use (None, None,1) as input shape, then I can not initiate a model, as it raises this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<stdin>\", line 37, in Unet\r\n  File \"<stdin>\", line 3, in get_crop_shape\r\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'\r\n\r\n```\r\n", "@AdityaKane2001 what do you mean by resizing? Is [this](https://keras.io/api/layers/preprocessing_layers/image_preprocessing/resizing/) what you are suggesting? ", "BTW; the tensorflow 2.4.1 raises the error, if the input shape is (256,256,1) as below:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ilias/py37CPUtf241/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 998, in __call__\r\n    input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\r\n  File \"/home/ilias/py37CPUtf241/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\", line 274, in assert_input_compatibility\r\n    ', found shape=' + display_shape(x.shape))\r\nValueError: Input 0 is incompatible with layer model_1: expected shape=(None, 128, 128, 1), found shape=(1, 256, 256, 1)\r\n\r\n\r\n\r\n```\r\n\r\nNow, if I try the same with tensorflow 2.2\r\n\r\n`WARNING:tensorflow:Model was constructed with shape (None, 128, 128, 1) for input Tensor(\"input_2:0\", shape=(None, 128, 128, 1), dtype=float32), but it was called on an input with incompatible shape (1, 256, 256, 1).\r\n`\r\n\r\nHowever, tensorflow 2.2 still perform image segmentation correctly!", "In order to prevent above errors; I do the following (as I have shown above), if the image shape is (256,256,1):\r\n\r\n```\r\nimport tensorflow as tf\r\nmodel1 = Unet(input_shape = (256,256,1))\r\nmodel1.load_weights(\"model/variables/variables\")\r\n```\r\n\r\nthen it does not show any error or warning message!\r\n\r\n", "@micosacak \r\nYes, I was suggesting `Resizing`  layer. The thing is, when you pass some integer values in shape, you essentially create a graph of  the model. All the functions used in the model are now traced by AutoGraph. This means that your input size is now fixed. Thus, using `None` in your input shape means that you are not allowing the model to be traced. Also, when you compile the model, the model now fixes ambiguous shapes of the tensors that are inside the graph. Obviously, this cannot be done when you pass `None` . Hence the function `get_crop_shape` fails because while tracing a dummy tensor is passed in that function. \r\n@amahendrakar please correct me if I am wrong.\r\nHope this helps.", "@micosacak I think there are two options.\r\n1. As @AdityaKane2001 mentioned, you can write your own preprocessing code that include reshaping the input image size to the size you have used in your training (say it as `target_size`), or\r\n2.  simply use `ImageDataGenerator` class that reshapes your input images to `target_size` as shown in this [tutorial](https://www.tensorflow.org/hub/tutorials/tf2_image_retraining). \r\n\r\nIn the above tutorial there are several models that were trained with different size. Later those models were used with `flower_dataset` that is of different size when compared to the original images that were used for training those models. Hope it helps. Thanks! ", "Thanks for your help, I will check the tutorial as well.\r\n\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48513\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48513\">No</a>\n"]}, {"number": 48512, "title": "CMAKE_DL_LIBS refers to which libraries\uff1fI can't find the libs in CMakelists.txt", "body": "", "comments": ["@ToBigboss \r\nCould you please fill the issue template\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following details\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory\r\nand the exact sequence of commands / steps that you executed before running into the problem. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48511, "title": "Output names for models defined via subclassing tf.kerasModel are ignored, fit with multiple losses raises error", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: cuda 11.1\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using a custom Model defined via inheritance from tf.keras.Model, if the model has multiple output I could not find a way to specify output names, but keras seems to assign a name automatically. Could these names be specified? (I will attach code down below to better illustrate the situation). The output names specified in a custom keras model defined by subclassing seems to be ignored\r\n\r\nI could not find how to specify output names in this setting. If specifying output names is not implemented for keras Model subclassing, then this would become a feature request.\r\nMaybe a custom model could specify the output names. I saw there is the attribute output_names in Model, but I am not sure who is setting it\r\n\r\n**Describe the expected behavior**\r\nI would expect the output names to be the one which are specified in the custom class, as it happens when using the functional API\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nX = np.random.rand(100, 30)\r\nY_1 = np.random.rand(100, 1)\r\nY_2 = np.random.rand(100, 5)\r\n\r\n\r\n# FUNCTIONAL API is ok\r\n\r\ninputs = tf.keras.layers.Input((30,))\r\n\r\noutput_1 = tf.keras.layers.Dense(1, name=\"myname1\")(inputs)\r\noutput_2 = tf.keras.layers.Dense(5, name=\"myname2\")(inputs)\r\n\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=[output_1, output_2])\r\n\r\nlosses = {\r\n    \"myname1\": 'mse',\r\n    \"myname2\": 'mse',\r\n}\r\n\r\nmodel.compile(optimizer='adam', loss=losses)\r\n\r\nmodel.fit(x=X, y=(Y_1, Y_2), epochs=2)\r\n\r\n# SUBCLASSING raises error\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.dense1 = tf.keras.layers.Dense(1, name=\"myname1\")\r\n        self.dense2 = tf.keras.layers.Dense(5, name=\"myname2\")\r\n        \r\n    def call(self, x):\r\n        return self.dense1(x), self.dense2(x) \r\n\r\n\r\nmodel = MyModel()\r\n\r\nlosses = {\r\n    \"myname1\": 'mse',\r\n    \"myname2\": 'mse',\r\n}\r\n\r\nmodel.compile(optimizer='adam', loss=losses)\r\n\r\nmodel.fit(x=X, y=(Y_1, Y_2), epochs=2)\r\n\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nEpoch 1/2\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-b791eb51ceb6> in <module>\r\n     18 model.compile(optimizer='adam', loss=losses)\r\n     19 \r\n---> 20 model.fit(x=X, y=(Y_1, Y_2), epochs=2)\r\n\r\n~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1098                 _r=1):\r\n   1099               callbacks.on_train_batch_begin(step)\r\n-> 1100               tmp_logs = self.train_function(iterator)\r\n   1101               if data_handler.should_sync:\r\n   1102                 context.async_wait()\r\n\r\n~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    826     tracing_count = self.experimental_get_tracing_count()\r\n    827     with trace.Trace(self._name) as tm:\r\n--> 828       result = self._call(*args, **kwds)\r\n    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n    830       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    869       # This is the first call of __call__, so we have to initialize.\r\n    870       initializers = []\r\n--> 871       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    872     finally:\r\n    873       # At this point we know that the initialization is complete (or less\r\n\r\n~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    723     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\r\n    724     self._concrete_stateful_fn = (\r\n--> 725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    726             *args, **kwds))\r\n    727 \r\n\r\n~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2967       args, kwargs = None, None\r\n   2968     with self._lock:\r\n-> 2969       graph_function, _ = self._maybe_define_function(args, kwargs)\r\n   2970     return graph_function\r\n   2971 \r\n\r\n~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3359 \r\n   3360           self._function_cache.missed.add(call_context_key)\r\n-> 3361           graph_function = self._create_graph_function(args, kwargs)\r\n   3362           self._function_cache.primary[cache_key] = graph_function\r\n   3363 \r\n\r\n~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3194     arg_names = base_arg_names + missing_arg_names\r\n   3195     graph_function = ConcreteFunction(\r\n-> 3196         func_graph_module.func_graph_from_py_func(\r\n   3197             self._name,\r\n   3198             self._python_function,\r\n\r\n~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    988         _, original_func = tf_decorator.unwrap(python_func)\r\n    989 \r\n--> 990       func_outputs = python_func(*func_args, **func_kwargs)\r\n    991 \r\n    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    632             xla_context.Exit()\r\n    633         else:\r\n--> 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    635         return out\r\n    636 \r\n\r\n~/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    975           except Exception as e:  # pylint:disable=broad-except\r\n    976             if hasattr(e, \"ag_error_metadata\"):\r\n--> 977               raise e.ag_error_metadata.to_exception(e)\r\n    978             else:\r\n    979               raise\r\n\r\nValueError: in user code:\r\n\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n        outputs = model.train_step(data)\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:755 train_step\r\n        loss = self.compiled_loss(\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:186 __call__\r\n        self.build(y_pred)\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:138 build\r\n        self._losses = self._conform_to_outputs(y_pred, self._losses)\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:62 _conform_to_outputs\r\n        struct = map_to_output_names(outputs, self._output_names, struct)\r\n    /home/ricvo/env/tf2.4/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:585 map_to_output_names\r\n        raise ValueError('Found unexpected keys that do not correspond '\r\n\r\n    ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['myname1', 'myname2']). Expected: ['output_1', 'output_2']\r\n\u200b```", "comments": ["I was able to reproduce the issue in TF v2.4,v2.5.0 rc0 and nightly.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/41829089cc923673991d69ae7b4dd95c/48511-2-4.ipynb) here", "@ricvo Everything works as expected when you change `call` return from \r\n\r\n`return self.dense1(x), self.dense2(x) `\r\nto \r\n`return {\"myname1\": self.dense1(x), \"myname2\": self.dense2(x)}`\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/1702d9e1dc72766af9c95f73384abfed/48511-2-4.ipynb). Thanks!\r\n\r\nPlease verify and close the issue if this was resolved for you. Thanks!\r\n", "@jvishnuvardhan thank you for the quick answer, I checked and it works well. Also with tf.data and adding metrics in compile, everything seems good specifying a dict. Sorry if I missed this, I hadn't seen this behaviour in the doc or in old issues. I incidentally just tested for nested structures and it works for that too, that's great\r\n```\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.dense1 = tf.keras.layers.Dense(1, name=\"myname1\")\r\n        self.dense2 = tf.keras.layers.Dense(5, name=\"myname2\")\r\n        \r\n    def call(self, x):\r\n        return ({\"myname1\": self.dense1(x), \"myname2\": self.dense2(x)}, {\"myname3\": self.dense1(x), \"myname4\": self.dense2(x)})\r\n\r\n\r\nmodel = MyModel()\r\n\r\nlosses = {\r\n    \"myname1\": 'mse',\r\n    \"myname2\": 'mse',\r\n}\r\n\r\nmetrics = (\r\n    {\r\n    \"myname1\": ['mse'],\r\n    \"myname2\": ['mse'],\r\n    },\r\n    {\r\n    \"myname3\": ['mse'],\r\n    \"myname4\": ['mse'],\r\n    },\r\n)\r\nmodel.compile(optimizer='adam', loss=(losses, None) , metrics=metrics)\r\n\r\n\r\nX = np.random.rand(100, 30)\r\nY_1 = np.random.rand(100, 1)\r\nY_2 = np.random.rand(100, 5)\r\n\r\nmodel.fit(x=X, y=({\"myname1\": Y_1, \"myname2\": Y_2}, {\"myname3\": Y_1, \"myname4\": Y_2}), epochs=2)\r\n\r\n```\r\n\r\n\r\n\r\nI still find a little odd that the data has to be passed differently in the subclassing case, i.e.\r\n```\r\nmodel.fit(x=X, y={\"myname1\": Y_1, \"myname2\": Y_2}, epochs=2)\r\n```\r\nand it doesn't work with a tuple in y data as in than in the functional API. But hey it works, and I actually prefer this way, the dictionaries are more explicit. Thank you\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48511\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48511\">No</a>\n"]}, {"number": 48510, "title": "Small updates to the docs", "body": " * fixed path to renode download script\r\n * updated file names and numbers for the RFCs\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48509, "title": "Documentation on hosted models", "body": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/guide/hosted_models#image_classification\r\n\r\n## Description of issue (what needs changing):\r\nAdditional information is required for the available models\r\n\r\n### Clear description\r\n\r\nModels available for image classification: only models are provided, there is no information with respect to preprocessing required, dataset where it was trained or labels file. This should be given, or at least, said somewhere in the docs.\r\n\r\n", "comments": ["@rola93  Please go through the TF HUB [link](https://tfhub.dev/s?deployment-format=lite&module-type=image-classification&q=quantized) and you will find more information related to the models provided in the documentation. Thanks!", "Thanks for your answer! Now need to learn [hot to extract label's file from metadata](https://stackoverflow.com/questions/67082697/how-to-extract-metadata-from-tflite-model) for some of them, but this is another problem.\r\n\r\nThank you!"]}, {"number": 48508, "title": "micro: DEPTH_TO_SPACE PR3-5", "body": "PR steps 3 through 5 for the DEPTH_TO_SPACE operator as per Issue #46025 ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48507, "title": "[r2.5 port][ROCm] Port PR 48187 to r2.5", "body": "/cc @mihaimaruseac @angerson\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/48187\r\n\r\n", "comments": []}, {"number": 48506, "title": "[TFLite] Add support for the usage of the Softmax reference kernel and register it in BuiltinRefOpResolver", "body": "Hello,\r\n\r\nThe PR adds a support for the usage of the Softmax reference kernel and register it in the `BuiltinRefOpResolver` to allow user to run the reference Softmax kernel. \r\n\r\nThe preparations and checks are similar to the ones in [TFLite Micro](https://github.com/tensorflow/tensorflow/blob/b3a0ede93c5252ef8728d283ba35c0d5ce2fb150/tensorflow/lite/micro/kernels/softmax_common.cc#L30).\r\n\r\nA second commit fix some memory leaks that where present in the activations tests.\r\n\r\nThibaut", "comments": ["@renjie-liu @jianlijianli @thaink could you review this PR?", "@Tessil Can you please resolve conflicts? Thanks!", "@gbaned Fixed it, thanks.", "@Tessil  Can you please resolve conflicts? Thanks!", "@gbaned Done, thanks.", "The change is landed at https://github.com/tensorflow/tensorflow/commit/688eba201334706e9bb3e4eed9211dd29d3f64d1.", "@Tessil if you have any leftover change after the above change, how about uploading a separate PR and closing this PR?", "@abattery the merged changes look good. I will close this PR then, thanks!"]}, {"number": 48505, "title": "How to append -lineinfo to nvcc flags?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.9.2\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 4.0.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: RTX2080Ti 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'd like to add lineinfo to nvcc flags without set the whole GPU kernels to debug mode. \r\nCould anybody tell me where should I modify? Or what argument I can pass to bazel?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@FindHao \r\n\r\nIn order to reproduce the issue reported here, Could you please provide the standalone code/colab gist. Thanks!\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48505\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48505\">No</a>\n"]}, {"number": 48504, "title": "MobileNetV3 keras models have bug with GlobalAveragePooling2D", "body": "**System information**\r\nTensorFlow version:\r\n```python\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\nv2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n```\r\n\r\n**Describe the current behavior**\r\nAccording to [MobileNetV3Small documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV3Small)\r\n\r\n> The weights for all 6 models are obtained and translated from the Tensorflow checkpoints from TensorFlow checkpoints found here.\r\n\r\nBut keras models cannot reproduce performance on ImageNet #48066.\r\n\r\nTensorflow pb model contains GlobalAvgPool **before** Conv2D with filter <1x1x576x1024>:\r\n![image](https://user-images.githubusercontent.com/22346860/114558652-93379e80-9c73-11eb-8165-5d5230c0494c.png)\r\n\r\nBut MobileNetV3Small from keras.application contains GlobalAvgPool **after** Conv2D with filter <1x1x576x1024>:\r\n![image](https://user-images.githubusercontent.com/22346860/114558795-b82c1180-9c73-11eb-95e4-3f1eb05dc374.png)\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6a278b9cf2fbed781661079b13d88b4106622a09/tensorflow/python/keras/applications/mobilenet_v3.py#L295-L308\r\n\r\nSuch version of MobileNetV3Small achieves top1: 56.79% instead of 68.1%.\r\n\r\n**Describe the expected behavior**\r\nTo achieve reported accuracy, change [model code](https://github.com/tensorflow/tensorflow/blob/6a278b9cf2fbed781661079b13d88b4106622a09/tensorflow/python/keras/applications/mobilenet_v3.py#L295-L315) to:\r\n\r\n```python\r\n  x = activation(x)\r\n\r\n  x = layers.GlobalAveragePooling2D()(x)\r\n  if channel_axis == 1:\r\n    x = layers.Reshape((last_conv_ch, 1, 1))(x)\r\n  else:\r\n    x = layers.Reshape((1, 1, last_conv_ch))(x)\r\n\r\n  x = layers.Conv2D(\r\n      last_point_ch,\r\n      kernel_size=1,\r\n      padding='same',\r\n      use_bias=True,\r\n      name='Conv_2')(x)\r\n  x = activation(x)\r\n\r\n  if include_top:\r\n    if dropout_rate > 0:\r\n      x = layers.Dropout(dropout_rate)(x)\r\n    x = layers.Conv2D(classes, kernel_size=1, padding='same', name='Logits')(x)\r\n    x = layers.Flatten()(x)\r\n    imagenet_utils.validate_activation(classifier_activation, weights)\r\n    x = layers.Activation(activation=classifier_activation,\r\n                          name='Predictions')(x)\r\n```\r\n", "comments": ["Thanks for the report. Please open a PR with the fix.", "Thanks for reporting the issue. Please https://github.com/tensorflow/tensorflow/pull/48542#issuecomment-824258134 for more updates. #48542 should fix the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48504\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48504\">No</a>\n"]}, {"number": 48503, "title": "LearningRateScheduler does not necessarily reduce learning rate", "body": "The underlying function can set the learning rate arbitrarily", "comments": []}, {"number": 48502, "title": "error when using XLA with multiple GPUs", "body": "The TF version is 2.4.1. When I enable XLA with mulit-gpu training by using MirroredStrategy, I got the following errors. \r\n\r\n```\r\n...\r\nwhile/NcclAllReduce_271: unsupported op: No registered 'NcclAllReduce' OpKernel for XLA_GPU_JIT devices compatible with node {{node while/NcclAllReduce_271}}\r\n        Stacktrace:\r\n                Node: __inference_train_multiple_steps_260276, function: \r\n                Node: while, function: __inference_train_multiple_steps_260276\r\n                Node: while/NcclAllReduce_271, function: while_body_135159\r\n\r\nwhile/NcclAllReduce_272: unsupported op: No registered 'NcclAllReduce' OpKernel for XLA_GPU_JIT devices compatible with node {{node while/NcclAllReduce_272}}\r\n        Stacktrace:\r\n                Node: __inference_train_multiple_steps_260276, function: \r\n                Node: while, function: __inference_train_multiple_steps_260276\r\n                Node: while/NcclAllReduce_272, function: while_body_135159\r\n\r\nwhile/NcclAllReduce_273: unsupported op: No registered 'NcclAllReduce' OpKernel for XLA_GPU_JIT devices compatible with node {{node while/NcclAllReduce_273}}\r\n        Stacktrace:\r\n                Node: __inference_train_multiple_steps_260276, function: \r\n                Node: while, function: __inference_train_multiple_steps_260276\r\n                Node: while/NcclAllReduce_273, function: while_body_135159\r\n\r\nwhile/NcclAllReduce_274: unsupported op: No registered 'NcclAllReduce' OpKernel for XLA_GPU_JIT devices compatible with node {{node while/NcclAllReduce_274}}\r\n        Stacktrace:\r\n                Node: __inference_train_multiple_steps_260276, function: \r\n                Node: while, function: __inference_train_multiple_steps_260276\r\n                Node: while/NcclAllReduce_274, function: while_body_135159\r\n....\r\n\r\n```\r\n\r\nHow to fix it?", "comments": ["Looks like `nccl` drivers are missing from your system config. By default TF uses `NcclAllReduce()` you may switch to Hierarchical Copy and try.\r\n```python\r\ntf.distribute.mirrorstrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n```\r\nhttps://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy?version=nightly#args", "> Looks like `nccl` drivers are missing from your system config. By default TF uses `NcclAllReduce()` you may switch to Hierarchical Copy and try.\r\n> \r\n> ```python\r\n> tf.distribute.mirrorstrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n> ```\r\n> \r\n> https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy?version=nightly#args\r\n\r\nThe task can run without XLA enabled. It seems that nccl drivers are installed correctly.", "Hi @cswhjiang, #45940 has some more info on using XLA with a distribution strategy. Please take a look at that thread. If that does not answer your question, please provide more information (are you using a custom training loop?) or reproducible code so we can help you debug. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48502\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48502\">No</a>\n"]}, {"number": 48501, "title": "LookupError: No gradient defined for operation 'CudnnRNN' (op type: CudnnRNN)", "body": "model = Sequential()\r\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\r\nmodel.add(LSTM(units=50))\r\nmodel.add(Dense(1))\r\n\r\nmodel.compile(loss='mean_squared_error', optimizer='adam')\r\nmodel.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)\r\n\r\nThis is the code i am using", "comments": ["@rajsingh02 \r\n\r\nCould you please fill the issue template\r\nIn order to expedite the trouble-shooting process, could you please provide the following details\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory\r\nand the exact sequence of commands / steps that you executed before running  and also the error you are facing into the problem. Thanks!\r\n", "@rajsingh02 \r\nHi  Can you please provide the above requested details. Thanks\r\n\r\n\r\n\r\n", "@rajsingh02 Can you please share a simple standalone code to reproduce the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48501\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48501\">No</a>\n"]}, {"number": 48498, "title": "Why tensorflow 2.4.1 requires libcusolver.so.10 other than libcusolver.so.11?????? It is so weird since other dynamic files are 11 ended", "body": "As the question said.", "comments": ["It has wasted me a lot of time, trying to fix problems of installation even following official instructions.... Anyone could help?", "You can either install CUDA 10 or build tensorflow from source with CUDA 11 .\nOr ... another choice is to google for tensorflow wheel package built with CUDA 11 :) (USE AT YOUR OWN RISK)", "@phone-burner Thanks", "@DabiaoMa \r\nCould you please fill the issue template\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following details\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nand the exact sequence of commands / steps that you executed before running into the problem. Thanks!\r\n\r\n", "@DabiaoMa \r\n\r\nCould you please confirm if the issue is resolved? if yes, please feel free to move this issue to closed status.\r\n\r\n\r\n", "> @DabiaoMa\r\n> \r\n> Could you please confirm if the issue is resolved? if yes, please feel free to move this issue to closed status.\r\n\r\nAt least I'm sure this issue is ready to be closed as my suggestion has helped him/her successfully solve the problem .", "@DabiaoMa , the comment [here](https://github.com/tensorflow/tensorflow/issues/45263#issuecomment-748865985) will clear your doubts on `libcusolver.so.10` in `cuDNN 11  `.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48498\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48498\">No</a>\n"]}, {"number": 48496, "title": "Build Failed with  Missing Dependencies Error  with Clang-11 and get 404 Not Found Warning ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.8.8\r\n- Installed using virtualenv? pip? conda?: cinda\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): clang-11\r\n- CUDA/cuDNN version:  CUDA 11.2, CuDNN 8\r\n- GPU model and memory: GTX 1080 Ti\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuilding latest master with Clang-11 failed with 404 Not Found Warning and  missing dependency declarations. I tried to access the file through chrome to download it, but the file does not exist.\r\n\r\nTried to use ` bazel clean --expunge` and ` rm -rf ~/.cache/bazel` for several times, still not working.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n``` Bash\r\n\u279c git pull\r\n\u279c tensorflow git:(master) ./configure\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /home/user/anaconda3/envs/tensorflow/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /home/user/anaconda3/envs/tensorflow/lib/python3.8/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/user/anaconda3/envs/tensorflow/lib/python3.8/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]:\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.2 in:\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1,6.1,6.1,6.1]:\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: y\r\nClang will be used as CUDA compiler.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]:\r\nClang will not be downloaded.\r\n\r\nPlease specify which clang should be used as device and host compiler. [Default is ]: /usr/bin/clang-11\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\n...\r\nConfiguration finished\r\n\r\n\u279c tensorflow git:(master) bazel build //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/user/anaconda3/envs/tensorflow/bin/python3 --action_env PYTHON_LIB_PATH=/home/user/anaconda3/envs/tensorflow/lib/python3.8/site-packages --python_path=/home/user/anaconda3/envs/tensorflow/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,6.1,6.1,6.1 --action_env LD_LIBRARY_PATH=/usr/lib:/usr/lib/clang/11.0.0/lib:/usr/local/cuda-11.2/lib64:/usr/local/cuda/targets/x86_64-linux/lib: --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/bin/clang-11 --config=cuda_clang\r\nINFO: Found applicable config definition build:short_logs in file /home/user/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/user/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda_clang in file /home/user/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/user/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda_clang in file /home/user/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/user/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:linux in file /home/user/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/user/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/5a5a94ed34b07079046ac81e7e97d980ce2c834f.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/sqlite.org/2021/sqlite-amalgamation-3350300.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/zlib/BUILD.bazel:5:11: undeclared inclusion(s) in rule '@zlib//:zlib':\r\nthis rule is missing dependency declarations for the following files included by 'zlib/adler32.c':\r\n  '/usr/lib/clang/11.0.0/include/stddef.h'\r\n  '/usr/lib/clang/11.0.0/include/__stddef_max_align_t.h'\r\n  '/usr/lib/clang/11.0.0/include/limits.h'\r\n  '/usr/lib/clang/11.0.0/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 0.532s, Critical Path: 0.22s\r\nINFO: 18 processes: 18 internal.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```Bash\r\n\u279c  tensorflow git:(master) env\r\nUSER=user\r\nLOGNAME=user\r\nHOME=/home/user\r\nPATH=/usr/local/cuda-11.2/bin:/home/user/anaconda3/envs/tensorflow/bin:/home/user/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\nSHELL=/usr/bin/zsh\r\nTERM=xterm-256color\r\nXDG_SESSION_ID=373\r\nXDG_RUNTIME_DIR=/run/user/1000\r\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus\r\nXDG_SESSION_TYPE=tty\r\nXDG_SESSION_CLASS=user\r\nMOTD_SHOWN=pam\r\nLANG=en_US.UTF-8\r\nLC_NUMERIC=zh_CN.UTF-8\r\nLC_TIME=zh_CN.UTF-8\r\nLC_MONETARY=zh_CN.UTF-8\r\nLC_PAPER=zh_CN.UTF-8\r\nLC_NAME=zh_CN.UTF-8\r\nLC_ADDRESS=zh_CN.UTF-8\r\nLC_TELEPHONE=zh_CN.UTF-8\r\nLC_MEASUREMENT=zh_CN.UTF-8\r\nLC_IDENTIFICATION=zh_CN.UTF-8\r\nSSH_TTY=/dev/pts/0\r\nSHLVL=1\r\nPWD=/home/user/tensorflow\r\nOLDPWD=/home/user\r\nZSH=/home/user/.oh-my-zsh\r\nPAGER=less\r\nLESS=-R\r\nLSCOLORS=Gxfxcxdxbxegedabagacad\r\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\r\nLD_PRELOAD=/home/user/anaconda3/envs/dlrm/lib/libiomp5.so\r\nLD_LIBRARY_PATH=/usr/lib:/usr/lib/clang/11.0.0/lib:/usr/local/cuda-11.2/lib64:/usr/local/cuda/targets/x86_64-linux/lib:\r\nCONDA_EXE=/home/user/anaconda3/bin/conda\r\n_CE_M=\r\n_CE_CONDA=\r\nCONDA_PYTHON_EXE=/home/user/anaconda3/bin/python\r\nCONDA_SHLVL=3\r\nCONDA_PREFIX=/home/user/anaconda3/envs/tensorflow\r\nCONDA_DEFAULT_ENV=tensorflow\r\nCONDA_PROMPT_MODIFIER=(tensorflow)\r\nCONDA_PREFIX_1=/home/user/anaconda3\r\nCXX=/usr/bin/clang++-11\r\nCC=/usr/bin/clang-11\r\nLDFLAGS=-L/usr/lib/clang/11.0.0/lib -lm -lrt\r\nCXXFLAGS=-stdlib=libc++ -L/usr/lib/clang/11.0.0/lib\r\n_=/usr/bin/env\r\n```\r\nAccessing the address :\r\nhttps://storage.googleapis.com/mirror.tensorflow.org/sqlite.org/2021/sqlite-amalgamation-3350300.zip\r\n\r\n``` XML\r\n<Error>\r\n<Code>NoSuchKey</Code>\r\n<Message>The specified key does not exist.</Message>\r\n<Details>No such object: mirror.tensorflow.org/sqlite.org/2021/sqlite-amalgamation-3350300.zip</Details>\r\n</Error>\r\n```\r\n", "comments": ["Can you sync back to master HEAD and try again please?", "Thanks for your reply, I tried again, but it still doesn't work\r\n\r\n```Batch\r\ngit pull\r\nbazel clean --expunge\r\nsudo rm -rf ~/.cache/bazel\r\nexport HTTPS_PROXY=http://proxy.xxx:1234\r\nexport HTTP_PROXY=http://proxy.xxx:1234\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nIt is strange that it could successfully build with gcc, but with clang, it keeps having the following error.\r\n\r\nAlso , I could find `/usr/lib/clang/11.0.0/include/stddef.h` and other files in that place. \r\n\r\nIs there anything because of the bazel use other prxoy settings?\r\nOr is there any addition information I have to pass when building with clang-11 ? \r\n\r\n```Batch\r\n \u279c  tensorflow git:(master) bazel build //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=133\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/user/anaconda3/envs/tfenv/bin/python3 --action_env PYTHON_LIB_PATH=/home/user/anaconda3/envs/tfenv/lib/python3.8/site-packages --python_path=/home/user/anaconda3/envs/tfenv/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,6.1,6.1,6.1 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.2/lib64:/usr/local/cuda/targets/x86_64-linux/lib: --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/bin/clang-11 --config=cuda_clang\r\nINFO: Found applicable config definition build:short_logs in file /home/user/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/user/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda_clang in file /home/user/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/user/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda_clang in file /home/user/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/user/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:linux in file /home/user/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/user/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/cf7d56ef364a855bf1019bf5291b54f795b55486.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nWARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/40548a2974f1aea06215272d9c2b47a14a24e556.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/files.pythonhosted.org/packages/12/59/eaa15ab9710a20e22225efd042cd2d6a0b559a0656d5baba9641a2a4a921/gast-0.4.0.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/b7a11274f90f07537e2151fa4424db257ff9a950.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /home/user/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /home/user/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/NVIDIA/cudnn-frontend/archive/360d6e7164dfb7c802493fd1c0464f0d815b852a.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/nasm/BUILD.bazel:8:10: undeclared inclusion(s) in rule '@nasm//:nasm':\r\nthis rule is missing dependency declarations for the following files included by 'nasm/asm/assemble.c':\r\n  '/usr/lib/clang/11.0.0/include/limits.h'\r\n  '/usr/lib/clang/11.0.0/include/stddef.h'\r\n  '/usr/lib/clang/11.0.0/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/libjpeg_turbo/BUILD.bazel:170:11 undeclared inclusion(s) in rule '@nasm//:nasm':\r\nthis rule is missing dependency declarations for the following files included by 'nasm/asm/assemble.c':\r\n  '/usr/lib/clang/11.0.0/include/limits.h'\r\n  '/usr/lib/clang/11.0.0/include/stddef.h'\r\n  '/usr/lib/clang/11.0.0/include/stdarg.h'\r\nINFO: Elapsed time: 1.238s, Critical Path: 0.86s\r\nINFO: 42 processes: 41 internal, 1 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "The 404 warnings when fetching dependencies from the mirror can safely be ignored.\r\n\r\nI was able to build with clang in the past using `CC=clang bazel build ...`", "@mihaimaruseac  well, I also tried using the `CC=clang-11 CXX=clang++-11 bazel build ...` , but it still kept reporting about this.  I installed clang-11 by the Instruction from the [apt.llvm.org](https://apt.llvm.org/). \r\n\r\nIs there any tricks about the clang's installation? In fact, by using this installation, clang should be called as `clang-11` , not `clang`. I wonder if bazel build would not find clang-11 correctly.", "Tried today with latest commit. Still not help. Have something like below:\r\n\r\n```\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (441 packages loaded, 29766 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/com_google_absl/absl/synchronization/BUILD.bazel:68:11: undeclared inclusion(s) in rule '@com_google_absl//absl/synchronization:synchronization':\r\nthis rule is missing dependency declarations for the following files included by 'com_google_absl/absl/synchronization/barrier.cc':\r\n  '/usr/lib/clang/11.1.0/include/limits.h'\r\n  '/usr/lib/clang/11.1.0/include/stddef.h'\r\n  '/usr/lib/clang/11.1.0/include/__stddef_max_align_t.h'\r\n  '/usr/lib/clang/11.1.0/include/stdint.h'\r\n  '/usr/lib/clang/11.1.0/include/stdarg.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 26.927s, Critical Path: 2.81s\r\nINFO: 525 processes: 481 internal, 44 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "The Issue is addressed.\r\n\r\nThe reason is because the default `clang-11` is a symbol link pointed to `/usr/lib/llvm-11/bin/clang`.  Similar of the default `clang` command. This would make the build fail.\r\n\r\nShould change to \r\n\r\n```Batch\r\n CC=/usr/lib/llvm-11/bin/clang bazel build ...\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48496\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48496\">No</a>\n"]}, {"number": 48495, "title": "[r2.5 port][ROCm] Port PR 48336 to r2.5", "body": "/cc @mihaimaruseac @angerson\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/48336", "comments": []}, {"number": 48494, "title": "Invalid (404) link in the the tf.data guide", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/data\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIn the sentence \"See Loading TFRecords for an end-to-end example.\" the link points to \r\nhttps://www.tensorflow.org/tutorials/load_data/tf_records which returns a 404\r\n\r\nIt needs to point to the correct url https://www.tensorflow.org/tutorials/load_data/tfrecord \r\n\r\n", "comments": ["This is fixed now with the tagged PR. Thanks!"]}, {"number": 48493, "title": "  [r2.5 port][ROCm] Port PR 48419 to r2.5 ", "body": "/cc @mihaimaruseac @angerson\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/48419", "comments": []}, {"number": 48492, "title": "tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for C:/tensorflow1/models/research/object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint.ckpt-0", "body": "When i try to run training using this command:\r\n\r\npython model_main_tf2.py --pipeline_config_path=training/ssd_mobilenet_v2_320x320_coco17_tpu-8.config --model_dir=training --alsologtostderr\r\n\r\nI get this error: \r\n\r\n2021-04-12 19:15:52.041217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-04-12 19:15:54.632151: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-04-12 19:15:54.632623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-04-12 19:15:54.652563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-04-12 19:15:54.652663: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-04-12 19:15:54.657606: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-04-12 19:15:54.657684: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-04-12 19:15:54.660457: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-04-12 19:15:54.661423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-04-12 19:15:54.667705: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-04-12 19:15:54.669994: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-04-12 19:15:54.670519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-04-12 19:15:54.670663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-04-12 19:15:54.670947: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-04-12 19:15:54.671897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.62GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-04-12 19:15:54.671961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-04-12 19:15:54.672085: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-04-12 19:15:54.672161: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-04-12 19:15:54.672229: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-04-12 19:15:54.672256: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-04-12 19:15:54.672371: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-04-12 19:15:54.672436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-04-12 19:15:54.672501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-04-12 19:15:54.672644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-04-12 19:15:55.109057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-04-12 19:15:55.109155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-04-12 19:15:55.109181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-04-12 19:15:55.109375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6637 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2021-04-12 19:15:55.110339: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\nI0412 19:15:55.111349 17276 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\nINFO:tensorflow:Maybe overwriting train_steps: None\r\nI0412 19:15:55.119328 17276 config_util.py:552] Maybe overwriting train_steps: None\r\nINFO:tensorflow:Maybe overwriting use_bfloat16: False\r\nI0412 19:15:55.119328 17276 config_util.py:552] Maybe overwriting use_bfloat16: False\r\nWARNING:tensorflow:From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\model_lib_v2.py:546: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nW0412 19:15:55.387193 17276 deprecation.py:339] From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\model_lib_v2.py:546: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nINFO:tensorflow:Reading unweighted datasets: ['C:/tensorflow1/models/research/object_detection/train.record']\r\nI0412 19:15:55.439054 17276 dataset_builder.py:163] Reading unweighted datasets: ['C:/tensorflow1/models/research/object_detection/train.record']\r\nINFO:tensorflow:Reading record datasets for input file: ['C:/tensorflow1/models/research/object_detection/train.record']\r\nI0412 19:15:55.440239 17276 dataset_builder.py:80] Reading record datasets for input file: ['C:/tensorflow1/models/research/object_detection/train.record']\r\nINFO:tensorflow:Number of filenames to read: 1\r\nI0412 19:15:55.441213 17276 dataset_builder.py:81] Number of filenames to read: 1\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nW0412 19:15:55.442210 17276 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nW0412 19:15:55.443208 17276 deprecation.py:339] From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nWARNING:tensorflow:From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nW0412 19:15:55.458168 17276 deprecation.py:339] From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nWARNING:tensorflow:From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nW0412 19:16:00.092059 17276 deprecation.py:339] From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\r\nWARNING:tensorflow:From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nW0412 19:16:02.495897 17276 deprecation.py:339] From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\r\nWARNING:tensorflow:From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\inputs.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nW0412 19:16:04.121161 17276 deprecation.py:339] From C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\inputs.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\", line 95, in NewCheckpointReader\r\n    return CheckpointReader(compat.as_bytes(filepattern))\r\nRuntimeError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for C:/tensorflow1/models/research/object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint.ckpt-0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\capta\\AppData\\Roaming\\Python\\Python37\\site-packages\\absl\\app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\capta\\AppData\\Roaming\\Python\\Python37\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 597, in train_loop\r\n    train_input, unpad_groundtruth_tensors)\r\n  File \"C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 386, in load_fine_tune_checkpoint\r\n    if not is_object_based_checkpoint(checkpoint_path):\r\n  File \"C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 346, in is_object_based_checkpoint\r\n    var_names = [var[0] for var in tf.train.list_variables(checkpoint_path)]\r\n  File \"C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 112, in list_variables\r\n    reader = load_checkpoint(ckpt_dir_or_file)\r\n  File \"C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 67, in load_checkpoint\r\n    return py_checkpoint_reader.NewCheckpointReader(filename)\r\n  File \"C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\", line 99, in NewCheckpointReader\r\n    error_translator(e)\r\n  File \"C:\\Users\\capta\\.conda\\envs\\tf24\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\", line 35, in error_translator\r\n    raise errors_impl.NotFoundError(None, None, error_message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for C:/tensorflow1/models/research/object_detection/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint.ckpt-0", "comments": ["@CaptainOrigami ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nAlso please share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. \r\n\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48492\">No</a>\n"]}]