[{"number": 7936, "title": "mnist_softmax_xla.py script does not show the correct timeline with xla flag turned on", "body": "I ran mnist_softmax_xla.py with XLA turned on but did not see the \"_XLALaunch\" in the timeline. It still shows the same timeline (i.e same as without XLA turned on). It looks like that XLA is not getting launched. Any help would be greatly appreciated?\r\n\r\n### Environment info\r\nOperating System: SUSE Linux 12\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n-rw-r--r-- 1 root root   558720 Jan 10 14:52 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Jan 10 14:52 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Jan 10 14:52 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Jan 10 14:52 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Jan 10 14:52 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 Nov 22 05:34 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 Nov 22 05:34 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 Nov 22 05:34 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Nov 22 05:34 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n\r\n![timeline_xla](https://cloud.githubusercontent.com/assets/21690396/23388520/8535e576-fd17-11e6-9d1e-92ec009aeab1.png)\r\n\r\n1. A link to the pip package you installed: \r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\n\r\n\r\n", "comments": ["I got the same issue following the instruction here: [https://www.tensorflow.org/versions/master/experimental/xla/jit](https://www.tensorflow.org/versions/master/experimental/xla/jit)\r\n\r\nStep 3 basically produce the same result as Step 2. I updated TensorFlow to 1.0. I even tried the upgrading the Python testcase to the 1.0 version using:\r\n`python tf_upgrade.py --infile mnist_softmax_xla.py --outfile mnist_softmax_xla_1.0.py`\r\n\r\nI tried both Python2.7 and Python 3.5, same results:\r\n\r\n`TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python mnist_softmax_xla_1.0.py\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\ntensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\ntensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\ntensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\ntensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n0.9201\r\n`", "@agupta74 I think we may need to build TensorFlow from the source code and turn on \r\n\r\n`Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]`\r\n\r\nThe official version we installed probably does not have XLA enabled. I will try to build it from source, and see if it works. ", "Yes, that's right. To use XLA, you need to build Tensorflow from source and enable XLA at configure time. XLA is still experimental and it's not ready to be enabled by default just yet. But feel free to try it out!", "I'm closing this out, since \"build TensorFlow from source and enable XLA\" is the correct advice.\r\n\r\nFeel free to file new issues if you run into them, and thanks for trying out XLA!  :)", "@tatatodd I think it is better to update the XLA page at: [https://www.tensorflow.org/versions/master/experimental/xla/jit](https://www.tensorflow.org/versions/master/experimental/xla/jit) to indicate this requirement. ", "@yehenrytian I believe it already does.  https://www.tensorflow.org/versions/master/experimental/xla/jit starts with:\r\n\r\n```\r\nUsing JIT Compilation\r\n\r\n* Note: TensorFlow must be compiled from source to include XLA.\r\n\r\nWhy use just-in-time (JIT) compilation?\r\n...\r\n```", "I tried to build tensorFlow with XLA enabled on Linux (Ubuntu): . But it looks like the example is still not working properly:\r\n\r\nTF_XLA_FLAGS=--xla_generate_hlo_graph=.* python2.7 mnist_softmax_xla_1.0.py\r\nExtracting ./data/train-images-idx3-ubyte.gz\r\nExtracting ./data/train-labels-idx1-ubyte.gz\r\nExtracting ./data/t10k-images-idx3-ubyte.gz\r\nExtracting ./data/t10k-labels-idx1-ubyte.gz\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 24 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n0.9148\r\n\r\nI \"rm\" the official TF I installed before I can install my build \".whl\" file, however, it looks like there is still some more problems here. Anyone can help? Thanks!", "@yehenrytian Why do you say that the example is not working properly?", "@tatatodd Basically, I saw the same graph as from Step 2 with the .json file generated from Step 3 (when opened in chrome). And I don't see the console output of:\r\n\r\n`computation cluster_0[_XlaCompiledKernel=true,_XlaNumConstantArgs=1].v82 [CPU:\r\npipeline start, before inline]: /tmp/hlo_graph_0.dot`\r\n\r\nAnd I don't see any .dot file generated in the /tmp folder. \r\n\r\nI build TF r1.0 branch with XLA enabled. I am wondering if I should build the master branch? I am building TF on Linux, without any OpenCL or CUDA support.  Do I need to finish the Youtube video of your TF Dev Summit 2017 on XLA: TF, Compiled first? :)  Please help! Thanks!", "I am not sure if it is a testcase issue or not. But I can get another XLA example here: [https://gist.github.com/yaroslavvb/53052184e50cdfec35f0a127dd6df843](https://gist.github.com/yaroslavvb/53052184e50cdfec35f0a127dd6df843) to work with and without XLA and get the correct timing together with the .dot files. ", "@yaroslavvb 's example works because he turns on JIT scope for the specific ops, which is needed if you are not using a GPU and want XLA for CPU to work via JIT.  \r\n\r\n`with jit_scope(compile_ops=True):`\r\n\r\nI left a note in the document to warn people.  \r\n\r\n> Note: Turning on JIT at the session level will not result in operations being compiled for the CPU. JIT compilation for CPU operations must be done via the manual method documented below. This decision was made due to the CPU backend being single-threaded.\r\n\r\nAnd at the start of the tutorial.  \r\n\r\n> Currently JIT at the session level, which is what is used for the tutorial, only supports GPU.\r\n\r\nDo not interpret this as hostile.  Let me know if you think there is a way to make it more clear.  Thank you for trying the tutorial, I am excited people are giving it a try.    Closing issue, please reopen if necessary.", "I encountered the same problem for cpu.Thanks a lot"]}, {"number": 7935, "title": "RuntimeError: No C++ shape function registered for standard op: NearestNeighbors", "body": "For bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nGithub issue :- [7524](https://github.com/tensorflow/tensorflow/issues/7524)\r\nStackoverflow question:- [here](http://stackoverflow.com/questions/42250340/runtimeerror-no-c-shape-function-registered-for-standard-op-nearestneighbors)\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04 / Windows\r\n\r\nInstalled version of CUDA and cuDNN: NO\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 1.0\r\n\r\nIf installed from source, provide NO\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nfrom tensorflow.contrib.learn.python.learn.estimators import kmeans as kmeans_lib\r\nfrom tensorflow.contrib.factorization.python.ops import clustering_ops\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef input_fn(x):\r\n\"\"\"Returns an input_fn\"\"\"\r\ndef _fn():\r\n       return tf.constant(x), None\r\nreturn _fn\r\n\r\nx = np.array([[random.random() for i in range(198)] for j in range(2384)], dtype=np.float32)\r\nkm = kmeans_lib.KMeansClustering(num_clusters=200, initial_clusters=clustering_ops.KMEANS_PLUS_PLUS_INIT)\r\nkm.fit(input_fn=input_fn(x), max_steps=300)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nHi, I found following 2 lines missing in gen_clustering_ops.py. I believe they should there in the file. If not then please raise a bug.  I couldn't find the correct place where the change is to be made\r\n![image](https://cloud.githubusercontent.com/assets/25343161/23388256/21b477fe-fd87-11e6-856c-c07dc3de4fa8.png)\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\nFile \"C:\\Users#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1568, in call_with_requiring\r\nreturn getattr(x, f)\r\nFile \"C:\\Users#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 610, in call_cpp_shape_fn\r\ndebug_python_shape_fn, require_shape_fn)\r\nFile \"C:\\Users#####\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 680, in _call_cpp_shape_fn_impl\r\n\"No C++ shape function registered for standard op: %s\" % op.type)\r\nRuntimeError: No C++ shape function registered for standard op: NearestNeighbors\r\n```", "comments": ["@mahatosourav91 If I understand correctly, the problem occurs on Windows?\r\n\r\n@mrry might have some thoughts here; perhaps NearestNeighbors isn't registered correctly on Windows?", "Quite possibly! The support for unsupported libraries in `tf.contrib` on Windows is incomplete, and left up to the individual contributors. I'm not sure who owns `tf.contrib.factorization`, but it'd be great if they could add this. \r\n\r\nI'm not sure who owns `tf.contrib.factorization`, but it'd be great if they could add a shape function for their ops to avoid this error :).", "Perhaps @agarwal-ashish  is the right contact to get a NearestNeighbors shape function registered on Windows.", "The upside of adding a shape function is that benefits Linux *and* Windows, and you don't have to mess around with CMake :).", "@tatatodd this error occur even in ubuntu 16.04", "@mahatosourav91 Ah yes my mistake, a NearestNeighbors shape function will indeed help all platforms.  Let's see what @agarwal-ashish has to say on this matter.", "I agree that adding a shape function to NearestNeighbors sounds like a good next step to fix this issue. @mahatosourav91 have you tried that / are planning on trying that ? ", "@agarwal-ashish Sorry I have a very little understanding of tensorflow framework right now, so I couldn't find the correct place where the change is to be made. Currently I am working by modifying the gen_clustering_ops.py which I know is a bad idea.", "@agarwal-ashish It would be very helpful if you can look into this issue [7748 ](https://github.com/tensorflow/tensorflow/issues/7784#issuecomment-283346222) also.", "I have a fix in the works that will enable `tf.contrib.factorization` on Windows.", "@mrry What about linux?", "The same change should fix Linux as well.", "@mrry, I am a newbie to the TensorFlow. Could you please elaborate on the below. I am looking for the resolution as I am facing the same issue. \r\n1) How would I get the fix related to tf.contrib.factorization? Is there a version number? or a release?\r\n2) If there is no release associated with fix, how can I reproduce the fix in my environment.", "@vinay-hebb The fix to this issue is included in the 1.1 release."]}, {"number": 7934, "title": "Feature Request: Tranponse of separable_conv2d?", "body": "I am trying to implement transposed version of separable_conv2d probably name it separable_conv2d_transpose\r\n\r\nThe existing conv operations in TF for convolution are:\r\n```\r\nconvolution\r\nconv2d\r\ndepthwise_conv2d\r\ndepthwise_conv2d_native\r\nseparable_conv2d\r\natrous_conv2d\r\natrous_conv2d_transpose\r\nconv2d_transpose\r\nconv1d\r\nconv3d\r\nconv3d_transpose\r\nconv2d_backprop_filter\r\nconv2d_backprop_input\r\nconv3d_backprop_filter_v2\r\ndepthwise_conv2d_native_backprop_filter\r\ndepthwise_conv2d_native_backprop_input\r\n```\r\n\r\nDoes anyone know how to approach an implementation of the op, or better combine existing backprop filters to achieve the result?\r\n", "comments": ["I'll recommend that you ask this question on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow), since there is a larger community that reads questions there.", "Closing due to lack of activity.  Please reopen if necessary."]}, {"number": 7933, "title": "pip (python3) does not install *most* example code", "body": "When installing on my Debian (stretch) using the \"native pip\" method via: \r\n\r\n# pip3 install -U tensorflow-gpu\r\n\r\nI get tensorflow to install, but the examples directory does not contain everything I believe it should based on the source tree in github: This is what is installed in the examples directory:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/examples$ tree\r\n.\r\n\u251c\u2500\u2500 __init__.py\r\n\u251c\u2500\u2500 __pycache__\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.cpython-35.pyc\r\n\u2514\u2500\u2500 tutorials\r\n    \u251c\u2500\u2500 __init__.py\r\n    \u251c\u2500\u2500 mnist\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 input_data.py\r\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 mnist.py\r\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 __pycache__\r\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.cpython-35.pyc\r\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 input_data.cpython-35.pyc\r\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 mnist.cpython-35.pyc\r\n    \u2514\u2500\u2500 __pycache__\r\n        \u2514\u2500\u2500 __init__.cpython-35.pyc\r\n\r\n5 directories, 10 files\r\n\r\nPS. I wonder if this installation bug is related to issue: https://github.com/tensorflow/tensorflow/issues/590\r\nsince other issues report having similar problems there even after issue 590 was closed. ", "comments": ["Ran into the same question with a Windows 10 installation into an Anaconda conda environment. TensorFlow 1.0.0 was installed with `pip` as well.", "@jmoraleda Perhaps you can try to uninstall first?\r\n\r\nNote the instructions here:\r\nhttps://www.tensorflow.org/install/install_linux#install_tensorflow\r\n\r\nThe sequence of commands should be:\r\n```\r\n$ sudo pip3 uninstall tensorflow # for Python 3.n\r\n$ pip3 install tensorflow-gpu # Python 3.n; GPU support \r\n```\r\n\r\nOnly if that fails, try this (for Python 3.5 with GPU support):\r\n```\r\n$ sudo pip3 uninstall tensorflow # for Python 3.n\r\n$ sudo pip3 install --upgrade \\\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl\r\n```", "@tatatodd Thank you for your reply. I just tried your suggestion, but it made no difference. Uninstalling (pip3 uninstall tensorflow-gpu) followed by install (pip3 install tensorflow-gpu) produces the same effect as my original problem (i.e. Tensor flow installs and works, but the content of the examples directory  is limeted to what I showed in my original message) .", "We are moving our example and model codes out of the pip package. Pip package will install the core TF library on your machine, and then you can download the example and tutorial codes from relevant webpages or our repository.", "@gunan. Thank you. That makes perfect sense. I imagine the download instructions and tutorials have not been updated yet to reflect this change. I will close this issue."]}, {"number": 7932, "title": "get_shape() does not work for output of tf.image.resize_nearest_neighbor()", "body": "During debugging, I was trying to get the shape of output tensor from tf.image.resize_nearest_neighbor(). It seems get_shape() does not work for it.\r\n\r\n\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks! Also, please provide a full code example of what you are trying to achieve.", "Closing due to lack of activity.  Please reopen if necessary."]}, {"number": 7931, "title": "Add bow_encoder for imports", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed the CLA", "CLAs look good, thanks!\n\n<!-- ok -->", "importing `tensorflow.contrib.layers.bow_encoder` fails with a \r\n\r\n> couldn't find attribute bow_encoder on module\r\n\r\n type error, and so I have been importing `tensorflow.contrib.layers.python.layers.encoders.bow_encoder`.\r\n\r\nIt is my understanding from [another PR/issue](https://github.com/tensorflow/tensorflow/issues/7569) that this commit will fix the imports.", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 7930, "title": "Error calc. gradient? ValueError: None values not supported.", "body": "```\r\nTraceback (most recent call last):\r\n  File \"cnn_mnist.py\", line 140, in <module>\r\n    adv_x = fgsm(batch_xs[0], y_conv, 0.3)\r\n  File \"cnn_mnist.py\", line 43, in fgsm\r\n    signed_grad = tf.sign(grad)\r\n  File \"C:\\Users\\soone\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 452, in sign\r\n    return gen_math_ops.sign(x, name=name)\r\n  File \"C:\\Users\\soone\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2447, in sign\r\n    result = _op_def_lib.apply_op(\"Sign\", x=x, name=name)\r\n  File \"C:\\Users\\soone\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 504, in apply_op\r\n    values, as_ref=input_arg.is_ref).dtype.name\r\n  File \"C:\\Users\\soone\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\soone\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\soone\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"C:\\Users\\soone\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 360, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n```\r\n\r\nHi guys. I'm new to tensorflow and is trying to work on a project for creating adversarial samples for fooling an MNIST classifier. \r\n\r\nI found there was a fast gradient sign method and I tried to implement this in my code. However whenever I try to calculate the gradient I would always get \"None values\" even though I'm sure all the weights are loaded properly. \r\n\r\nAny help is appreciated. Thank you!", "comments": ["Were you able to solve it? I am facing the same problem\r\n", "I believe I solved the issue by passing the placeholder X instead of batch_xs[0]"]}, {"number": 7929, "title": "anyone else receive the following warning ", "body": "Hey guys, I was wondering if there's anyone else receiving the following warnings. I've updated `tf` through pip and since then I get the following warnings:\r\n\r\n```\r\nUsing TensorFlow backend.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n```\r\nAny idea what's going on? Prior to the update to the latest version never had this warning before.", "comments": ["This is a performance warning. For user convenience, our binaries do not have all the processor extensions so they work on older computers too. However, it is warning you that you do have those extensions available, and you can achieve better performance by building your own binary.", "@aselle thanks for the clarification."]}, {"number": 7928, "title": "Branch 148665838", "body": "Pushing internal commits.", "comments": ["@tensorflow-jenkins Test this, please.", "@tensorflow-jenkins Test this, please."]}, {"number": 7927, "title": "tf.nn.rnn_cell.DropoutWrapper() drops out across time-steps", "body": " at `class DropoutWrapper(RNNCell):` it seems dropout is implemented across all inputs and outputs without any implementation options. \r\n\r\nWould like to have the option for rnn dropout where one 'dropout mask' is generated and is then applied to each time step. On the next batch, a new mask is generated. This method is described in: \r\n\r\n(https://arxiv.org/pdf/1512.05287.pdf)\r\n\r\n There a few other dropout methods that have been published recently including one from google.\r\n\r\n(https://www.aclweb.org/anthology/C/C16/C16-1165.pdf)\r\n\r\nAnd also 'zoneout' which is mentioned in Issue #2789.\r\n\r\nAre there any plans to incorporate these advances?\r\n\r\n-Brad\r\n\r\n", "comments": ["@ebrevdo might have thoughts on this.", "This would be a useful option.  We'll need to identify the best way to do it. What do you expect to happen if the dropout wrapper is used in two different calls to dynamic_rnn?  The same mask is used across both rnns?", "I think ideally each call to dynamic_rnn would generate a new random mask. Hence stacked rnn's using the dropoutwrapper would have recurrent dropout, but different masks for each layer. \r\n\r\nAlthough I guess you could add a `seed` option if someone wanted to make them the same?\r\n\r\n`DropoutWrapper( recurrent=True, seed=None)`\r\n\r\nArgs:\r\n\r\n**`recurrent`**: if True applies dropout mask to individual time slices on the input and output after each time-step (purposed*). If False, apply dropout randomly to input and output (current implementation*)\r\n", "Any thoughts on this moving forward? As a workaround right now I have to statically unroll my lstm so I can manually apply my own randomly generated dropout mask to successive time steps. ", "I have a partial solution.  Will try to get it in early next week.\n\nOn Mar 9, 2017 7:36 AM, \"Brad\" <notifications@github.com> wrote:\n\n> Any thoughts on this moving forward? As a workaround right now I have to\n> statically unroll my lstm so I can manually apply my own randomly generated\n> dropout mask to successive time steps.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-285385664>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim561TWhtBek8lkLNopv1Rzz1LZSMks5rkByTgaJpZM4MNheF>\n> .\n>\n", "Scratch that, it's now a full implemention. Going through review now. Hope to push it tomorrow.", "Howd the review go?", "Code is in. Use the argument variational_recurrent=True.\n\nOn Mar 27, 2017 6:30 AM, \"Brad\" <notifications@github.com> wrote:\n\n> Howd the review go?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-289453609>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_1mD-lH54oinjw_tDvUJVWqGprFks5rp7oJgaJpZM4MNheF>\n> .\n>\n", "Thanks a lot for this implementation.\r\n\r\nWe can't use it for variable length inputs though. What should be given as an `input_size` ? `input_size=tf.TensorShape([100, None, 200])` gives an error:\r\n\r\n`TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.`\r\n\r\nThe error is [on this line](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/rnn_cell_impl.py#L678).", "Have you tried tf.shape(inputs)[1:]?  I think that may work.\n\nOn Jun 30, 2017 4:59 AM, \"Hrant Khachatrian\" <notifications@github.com>\nwrote:\n\n> Thanks a lot for this implementation.\n>\n> We can't use it for variable length inputs though. What should be given as\n> an input_size ? input_size=tf.TensorShape([100, None, 200]) gives an\n> error:\n>\n> TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types\n> [int32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-312249028>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimw_hyoCcpdzszZGBlHkw74EliEtQks5sJOMsgaJpZM4MNheF>\n> .\n>\n", "Now it says `TypeError: int() argument must be a string or a number, not 'Tensor'`", "What version of tensorflow are you using?\n\nOn Jul 2, 2017 1:29 AM, \"Hrant Khachatrian\" <notifications@github.com>\nwrote:\n\n> Now it says TypeError: int() argument must be a string or a number, not\n> 'Tensor'\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-312477991>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6QGtA9_6hoxoZ0GhooX3VUV7gKTks5sJ1TTgaJpZM4MNheF>\n> .\n>\n", "0.12.head", "1.2.1 \r\nsame problem.. TypeError: int() argument must be a string or a number, not 'Tensor'", "Hi @Hrant-Khachatrian, when you say \"variable length inputs\", do you mean the sequence length is the variable? If so, I think the input_size for `tf.TensorShape([100, None, 200])` is just 200 (assuming batch_size=100, variable sequence length), since the input_size is the input's depth of the RNN cell.\r\n\r\nFor `TypeError: int() argument must be a string or a number, not 'Tensor'`, can you try inputs.get_shape()[1:]? - `tf.shape()` returns Tensor instead of TensorShape.\r\n\r\n", "For a variable length input (but padded batches) of shape `[batches, timesteps, 132]`, \r\n`inputs.get_shape()[1:]` or `[2:]` does not work in my case. Here is the error:\r\n\r\n`ValueError: Dimensions must be equal, but are 128 and 132 for 'Encoder/Encoder/while/Encoder/multi_rnn_cell/cell_1/mul' (op: 'Mul') with input shapes: [?,128], [1,132].`\r\n\r\nLSTMCell was instanced with 128 num_units\r\n\r\ntf-nightly-gpu (1.4.0.dev20171013)", "@georgesterpu \r\n\r\nFrom your error, it seems you have at least 2 layer of lstm? Is your dropout wrapper applied on the 2nd layer's cell? If so, the input size to your second layer should be your first layer's output, which is 128 instead of 132.", "Thanks, @oahziur, you are right.\r\nI have added an extra parameter, the layer number, to my build_cell function, and I instantiate the DropoutWrapper like this: \r\n\r\n```\r\ncells = DropoutWrapper(cells,\r\n                                   ..............\r\n                                   variational_recurrent=True,\r\n                                   dtype=tf.float32,\r\n                                   input_size=self._inputs.get_shape()[-1] if layer == 0 else tf.TensorShape(num_units),\r\n                                )\r\n```\r\nand it appears to work.\r\n\r\nHowever, I get a new error on the decoder side in a seq2seq model:\r\n`ValueError: Dimensions must be equal, but are 143 and 128 for 'Decoder/decoder/while/BasicDecoderStep/decoder/attention_wrapper/attention_wrapper/multi_rnn_cell/cell_0/mul' (op: 'Mul') with input shapes: [?,143], [1,128].`\r\n\r\nI did make sure that for the decoder cells, input size is still num_units (128), and not `self._inputs.get_shape()[-1]`, because it is initialised from the encoder's final state. Still, I get the same error. Do you have any suggestion here ?", "I have some questions regarding the implementation of `variational_recurrent` in the Dropout Wrapper. In the paper it says: 'Implementing our approximate inference is identical to implementing dropout in RNNs with the same network units dropped at each time step, randomly dropping inputs, outputs, and recurrent connections. This is in contrast to existing techniques, where different network units would be dropped at different time steps, and no dropout would be applied to the recurrent connections '\r\n\r\nWhen setting `variational_recurrent=True`, do I get the full functionality of variational dropout presented in the paper (Y. Gal, Z. Ghahramani) ? I have looked at the source code but I'm still not sure whether it includes all aspects of variational dropout presented in the paper. Recent papers like [this one](https://arxiv.org/pdf/1707.05589.pdf) use intra-layer dropout in addition to variational_dropout. But when `variational_recurrent` is already dropping output units at each time step I don't see the need for using dropout between RNN layers as well.", "After check out the implementation of `variational_recurrent` in the `DropoutWrapper`, I'm a little confused. When `variational_recurrent=True`, the mask is generated when the wrapper is initialized before the training process, and is applied across every batch. Would it be better if a new mask be generated every batch as @beeCwright first requested? \r\n\r\nSince the wrapper implementation is to build a rnncell object, how can it be aware of a new batch is coming. From my limited understanding, only `dynamic_rnn` knows when a batch is coming and `rnncell` only deals with single timestep input and state.  Correct me if I'm wrong :P", "You get a new batch every time you call session.run(), independent of the\ngraph, and a new dropout pattern is created with every call to\nsession.run().  Therefore a new dropout pattern is applied to every batch.\n\nOn Wed, Nov 22, 2017 at 1:07 AM, Yujia Liu <notifications@github.com> wrote:\n\n> After check out the implementation of variational_recurrent in the\n> DropoutWrapper, I'm a little confused. When variational_recurrent=True,\n> the mask is generated when the wrapper is initialized before the training\n> process, and is applied across every batch. Would it be better if a new\n> mask be generated every batch as @beeCwright\n> <https://github.com/beecwright> first requested?\n>\n> Since the wrapper implementation is to build a rnncell object, how can it\n> be aware of a new batch is coming. From my limited understanding, only\n> dynamic_rnn knows when a batch is coming and rnncell only deals with\n> single timestep input and state. Correct me if I'm wrong :P\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7927#issuecomment-346287303>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxd0_IPTH5UAYZ2RMZNTrJ-8OoZAks5s4-RTgaJpZM4MNheF>\n> .\n>\n"]}, {"number": 7926, "title": "Freeze graph erroring out claiming there are uninitialized values in embeddings", "body": "This is similar to #7172 but with my own model.\r\n\r\nI'm training a CNN with word embeddings and for some reason I'm getting `FailedPreconditionError` exception whenever I try to save a frozen version of the model for later use.\r\n\r\nThis is despite the fact that I call `sess.run(tf.global_variables_initializer())` just before training and I have no problem monitoring the training in tensorboard and checkpointing the model at regular intervals.\r\n\r\nThe problem occurs when I try to load a model from a checkpoint and save a frozen model. The function I'm using is as follows:\r\n\r\n```python\r\ndef freeze_model(checkpoint_path, model_save_path, output_node_names):\r\n    checkpoint = tf.train.get_checkpoint_state(checkpoint_path)\r\n    input_checkpoint = checkpoint.model_checkpoint_path\r\n\r\n    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=True)\r\n    graph = tf.get_default_graph()\r\n    input_graph_def = graph.as_graph_def()\r\n    with tf.Session() as sess:\r\n        saver.restore(sess, input_checkpoint)\r\n\r\n        output_graph_def = graph_util.convert_variables_to_constants(\r\n            sess,\r\n            input_graph_def,\r\n            output_node_names\r\n        )\r\n\r\n        with tf.gfile.GFile(model_save_path, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n```\r\n\r\nThe error that I'm getting is as follows. It looks like the problem is with two embeddings coefficients. If I set `clear_devices=False`, I get the same error but for only one embedding coefficient:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"myproject/train.py\", line 522, in <module>\r\n    tf.app.run()\r\n  File \"/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"myproject/train.py\", line 518, in main\r\n    trainer.save_model(preprocessor)\r\n  File \"myproject/train.py\", line 312, in save_model\r\n    ut.freeze_model(self.checkpoint_dir, model_save_path, C.OUTPUT_NODE_NAMES)\r\n  File \"/home/foo/anaconda2/lib/python2.7/site-packages/myproject/utils.py\", line 224, in freeze_model\r\n    output_node_names\r\n  File \"/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py\", line 218, in convert_variables_to_constants\r\n    returned_variables = sess.run(variable_names)\r\n  File \"/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/foo/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nFailedPreconditionError: Attempting to use uninitialized value embeddings/W\r\n\t [[Node: embeddings/W/_20 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_30_embeddings/W\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](embeddings/W)]]\r\n\t [[Node: embeddings/W/_21 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_30_embeddings/W\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\n### Environment info\r\ntensorflow-gpu: 1.0.0\r\nUbuntu 14.0.4.5\r\nCUDA Version 8.0.44\r\n\r\n\r\n### What other attempted solutions have you tried?\r\nI am assigning the embeddings pretrained word2vec values after initializing. I have tried running the model without assigning these values and I get the same error.\r\nI have tried running the training for 70K batches.\r\nI have tried running `tf.report_uninitialized_variables` after calling `tf.global_variables_initializer()` and it has come up empty.\r\n", "comments": ["Turns out that I was constructing the `Saver` before I made my session. Oops!", "@lminer  I have exactly same issue with you. How did you correct it?"]}, {"number": 7925, "title": "Hi, I think I have a similar problem. How can I run python from a directory other than tensorflow ? Thanks !", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 7924, "title": "tf.train.import_meta_graph('model.meta') cannot handle seq2seq models with attention?", "body": "Environment: \r\nUbuntu 16.04\r\nTensorFlow v1.0.0\r\n\r\nWhen attempting to import a saved graph using \"tf.train.import_meta_graph('model.meta'),\" I get the following error:\r\n\r\n    Traceback (most recent call last):\r\n      File \"test_load.py\", line 19, in <module>\r\n        new_saver = tf.train.import_meta_graph('model.meta')\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1577, in import_meta_graph **kwargs)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 498, in import_scoped_meta_graph producer_op_list=producer_op_list)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 259, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op)\r\n    ValueError: No op named attn_add_fun_f32f32f32 in defined operations.\r\n\r\nThis error isn't thrown when I retrain my model **without attention** and import the graph with the same line of code.\r\nIs loading a model trained with attention not currently supported? \r\n\r\nThanks!", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 7923, "title": "Added passing of kwargs to external optimizer minimize", "body": "I originally created this patch to track performance, but as it isn't an OP, you can't build timelines etc. Still it might be nice to have this interface.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "Can one of the admins verify this patch?", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "Oddly, the test don't trigger. Trying again. \r\n\r\nJenkins, test this please.", "better. Thanks Jenkins."]}, {"number": 7922, "title": "Unable to specify loss function in tf.contrib.learn.DNNLinearCombinedRegressor", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nHave been unable to find how to specify loss function for both linear and deep components of the DNNLinearCombinedRegressor initializer. Looked in many places including tf.train (initialize optimizer with specific loss function?) and in learn.estimators. Also could not find a solution on stackoverflow or anywhere else through google searches.\r\n\r\n### Environment info\r\nOperating System: mac OS 10.12.3\r\n\r\nInstalled version of CUDA and cuDNN: None.\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: \r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. Version 1.0.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nHave not been able to specify loss function. \r\n\r\n\r\n### What other attempted solutions have you tried?\r\nTried to create custom estimator using tf.contrib.losses to specify desired loss but unable to combine both deep and wide this way. Would prefer to use the pre-defined linearCombinedRegressor with a loss function specified for _each_ of the linear and deep optimizers. How?\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Hi @tatatodd - Ok. Then I have a feature request - would it be possible to include, as a parameter, the type of loss/objective function when initializing DNNLinearCombinedRegressor? This would greatly help make the loss function explicitly visible and ensure the model is converging on the expected function."]}, {"number": 7921, "title": "Error with bazel build when retraining inception final layer for new categories", "body": "I'm doing the TensorFlow tutorial on retraining the inception's final layer for new categories: https://www.tensorflow.org/tutorials/image_retraining.\r\n\r\nThen, when I did `bazel build tensorflow/examples/image_retraining:retrain`, it gave me the following errors:\r\n\r\n```\r\nERROR: /home/darth/tensorflow/WORKSPACE:3:1: //external:io_bazel_rules_closure: no such attribute 'urls' in 'http_archive' rule.\r\nERROR: /home/darth/tensorflow/WORKSPACE:3:1: //external:io_bazel_rules_closure: missing value for mandatory attribute 'url' in 'http_archive' rule.\r\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package.\r\nINFO: Elapsed time: 0.153s\r\n```\r\n\r\nThank you in advance for the help.", "comments": []}, {"number": 7920, "title": "Compilation failure on OS/X w/ XLA", "body": "OS/X compilation fails with XLA.\r\n\r\nError is:\r\n```\r\nRROR: /Users/davidn/github/tensorflow/tensorflow/compiler/xla/service/BUILD:108:1: C++ compilation of rule '//tensorflow/compiler/xla/service:versioned_computation_handle' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 93 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from tensorflow/compiler/xla/service/versioned_computation_handle.cc:16:\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:33:19: error: unknown type name 'int64'; did you mean 'tensorflow::int64'?\r\n  using Version = int64;\r\n                  ^~~~~\r\n                  tensorflow::int64\r\n./tensorflow/core/platform/default/integral_types.h:27:19: note: 'tensorflow::int64' declared here\r\ntypedef long long int64;\r\n                  ^\r\nIn file included from tensorflow/compiler/xla/service/versioned_computation_handle.cc:16:\r\n./tensorflow/compiler/xla/service/versioned_computation_handle.h:38:3: error: unknown type name 'string'; did you mean 'std::string'?\r\n  string ToString() const;\r\n  ^~~~~~\r\n  std::string\r\n/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/iosfwd:194:65: note: 'std::string' declared here\r\ntypedef basic_string<char, char_traits<char>, allocator<char> > string;\r\n                                                                ^\r\ntensorflow/compiler/xla/service/versioned_computation_handle.cc:22:1: error: unknown type name 'string'; did you mean 'std::string'?\r\nstring VersionedComputationHandle::ToString() const {\r\n^~~~~~\r\nstd::string\r\n/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/iosfwd:194:65: note: 'std::string' declared here\r\ntypedef basic_string<char, char_traits<char>, allocator<char> > string;\r\n                                                                ^\r\n```\r\n\r\nissue was probably introduced in 7817ac8055ca328e5bf902677f335502eb0da926.\r\n\r\nIt is trivial to fix.", "comments": ["A fix for this issue has been checked into our internal tree; the issue should be resolved shortly when those changes are pulled to Github.", "thanks. ", "Hi.  Do you guys think that the fix is in the public repo already?   I notice that the last commit for that file is 'fix opensource build breakage....'.  \r\n\r\nhowever, that fix wasn't the one that is breaking the OS/X build.  The files need proper namespaces on the std:: things, and a namespace on the int64\r\n\r\n```\r\ndiff --git a/tensorflow/compiler/xla/service/versioned_computation_handle.cc b/tensorflow/compiler/xla/service/versioned_computation_handle.cc\r\nindex a693c46..188c4fc 100644\r\n--- a/tensorflow/compiler/xla/service/versioned_computation_handle.cc\r\n+++ b/tensorflow/compiler/xla/service/versioned_computation_handle.cc\r\n@@ -19,7 +19,7 @@ limitations under the License.\r\n \r\n namespace xla {\r\n \r\n-string VersionedComputationHandle::ToString() const {\r\n+std::string VersionedComputationHandle::ToString() const {\r\n   return tensorflow::strings::StrCat(handle.handle(), \":v\", version);\r\n }\r\n \r\ndiff --git a/tensorflow/compiler/xla/service/versioned_computation_handle.h b/tensorflow/compiler/xla/service/versioned_computation_handle.h\r\nindex ecedc92..190168e 100644\r\n--- a/tensorflow/compiler/xla/service/versioned_computation_handle.h\r\n+++ b/tensorflow/compiler/xla/service/versioned_computation_handle.h\r\n@@ -30,12 +30,12 @@ struct VersionedComputationHandle {\r\n   // A version value unambiguously specifying the state of the computation at a\r\n   // particular point in time as it is being built. This value is the\r\n   // ComputationDataHandle of the current root instruction.\r\n-  using Version = int64;\r\n+  using Version = tensorflow::int64;\r\n \r\n   ComputationHandle handle;\r\n   Version version;\r\n \r\n-  string ToString() const;\r\n+  std::string ToString() const;\r\n   bool operator==(const VersionedComputationHandle& other) const {\r\n     return (handle.handle() == other.handle.handle()) &&\r\n```\r\n", "That change should have fixed it. Does it still actually fail to build?\r\n\r\nNote the fix was to `#include \"tensorflow/compiler/xla/types.h\"`, which defines `xla::int64` and `xla::string`. No namespace qualification should be needed now that header is included.", "i see.  that is strange.  i will update our repo again and see what happens....\r\n\r\ncheers\r\n", "its all ok now.  indeed - we did not have the fix in our central repo."]}, {"number": 7919, "title": "Warning during compilation on OS/X", "body": "... and maybe other systems too.\r\n\r\nthe warning is\r\n\r\n```\r\n./tensorflow/compiler/xla/service/backend.h:37:1: warning: class 'ThreadPoolDevice' was previously declared as a struct [-Wmismatched-tags]\r\n```\r\nIt appears on almost every file in the XLA compiler directory, making the spotting of real problems harder.\r\n\r\n", "comments": []}, {"number": 7918, "title": "Add support for loading specific frame from SequenceExample", "body": "Hi, \r\n\r\nI have converted my video dataset into TFRecords format by storing each video as a SequenceExample. The problem is I don't neccesarily need to load all frames of a video instead of 2 random frames. Is there any plan to add support for loading specific frame from a SequenceExample to save loading time?  Or is there any method I can use to speed up loading?\r\n", "comments": ["I think @jhseu and @skye might know about this.", "@ebrevdo is most familiar with SequenceExample plans.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7917, "title": "Unreachable statement", "body": "tensorflow\\compiler\\xla\\service\\algebraic_simplifier.cc line 675\r\n\r\nadditional: return Status::OK();", "comments": ["Could you fix this @tatatodd. Thanks!", "I'm on it...  thanks for filing the issue @maddin200 !", "OK this has been fixed - it should get synced over to the github repo in the next couple of days.  Closing this out."]}, {"number": 7916, "title": "tf.TFRecordReader returns multiples copies of the same data with only 1 epoch", "body": "I am using tf.TFRecordReader and tf.train.batch to evaluate one epoch of my dataset (it has only one file). In order to get a better performance I tried to use enqueue_many passing a list of reader calls, because supposedly it launches several reader threads. However its behaviour has not been as I expected. It returns each item duplicated, although I thought it would return the same list of items, but faster. I took the code from @Yaroslavvb github: https://github.com/yaroslavvb/stuff/blob/master/ericyue-slowreader/benchmark.py\r\n\r\nIs this behaviour right? It seems really strange to me and I don't know if it is a bug or a feature.\r\n\r\nHere you can see a snippet of the code:\r\n\r\n    reader = tf.TFRecordReader()\r\n    queue_batch = []\r\n    for i in range(enqueue_many_size):\r\n        _, serialized_example = reader.read(filename_queue)\r\n        queue_batch.append(serialized_example)\r\n    batch_serialized_example = tf.train.batch(\r\n        [queue_batch],\r\n        batch_size=batch_size,\r\n        num_threads=thread_number,\r\n        capacity=capacity,\r\n        enqueue_many=True)\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\nNvidia Geforce 1080\r\nPython 3.5\r\nTensorflow version: 1.0.0\r\n\r\nInstalled version of CUDA and cuDNN: \r\n/usr/local/cuda-8.0/lib64/libcudadevrt.a       /usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudart.so         /usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0     /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44  /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a   /usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\nThe output for batch size of 4 records, enqueue_many_size=2, and record size of 16 floats, is:\r\n\r\n    [array([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\r\n         11.,  12.,  13.,  14.,  15.],\r\n       [  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\r\n         11.,  12.,  13.,  14.,  15.],\r\n       [ 16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,\r\n         27.,  28.,  29.,  30.,  31.],\r\n       [ 16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,\r\n         27.,  28.,  29.,  30.,  31.]], dtype=float32)]\r\n", "comments": ["A full reproducible example is generally useful, I can't tell what code you used to generate the numbers.\r\n\r\nHowever, in process of making sanity check, I found a similar looking bug, reopened here with self-contained example https://github.com/tensorflow/tensorflow/issues/7038#issuecomment-282798791\r\n\r\nCurrent work-around is to create session with `tf.OptimizerOptions.L0`\r\n\r\n```\r\nconfig = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\r\nsess = tf.InteractiveSession(config=config)\r\n\r\n```", "It looks like this bug is fixed in master but not in TF 1.0. So I would recommend running with `config = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))\r\n` until TF 1.1 is released", "The problem was solved with that configuration. I'm sorry I forgot to include the whole code.  I will be more careful in the future. Fortunately, the problem was rather independent of the test set and you had already found it.\r\nThank you very much @yaroslavvb. ", "Thanks @yaroslavvb for the link to the bug and work-around!\r\n\r\nI'm closing this issue out, as it seems to be resolved."]}, {"number": 7915, "title": "url not present for python3.6", "body": "I noticed that the URL for python3.6 is not included on this page,\r\nhttps://www.tensorflow.org/install/install_linux#TF_PYTHON_URL.", "comments": ["@gunan, @wolffg, please comment.\r\n", "This seems to be a bug in our website. The whl files are there but we just dont have the links there.\r\n@wollf can we route this appropriately to update the link and the website?\r\n\r\nIn the meantime, you can replace all `35` in the python 3.5 links with `36` to download the pip package for python 3.6\r\nOr you can simply use `pip install tensorflow` to get the appropriate pip package from pypi.", "@jugglerix can we send out a fix and republish the website?", "Website updated. Closing issue."]}, {"number": 7914, "title": "Fails when atrous rate is higher than ~280.", "body": "When `tf.nn.atrous_conv2d` is called with a rate higher than ~280, tensorflow fails with the following message: `F tensorflow/stream_executor/cuda/cuda_dnn.cc:2742] failed to enqueue convolution on stream: CUDNN_STATUS_NOT_SUPPORTED`  \r\nFurther invistigation leads to when [tensorflow calls cudnn convolution backward filters operation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2681).\r\n\r\nTested with multiple machines with TITAN X's and all pip packages and CUDA/CUDNN upgraded to latest.", "comments": ["@israelg99 can you provide the full log before the failure?\r\n\r\nPerhaps the root problem is we're running out of memory, as in #5688 ?", "@tatatodd my logs are identical to #5688 and it shouldn't run out of memory(unless it's a bug), I tried running it on many configurations, it fails even on a small network with batch size 1, if it helps, theano is able to run the same configurations without a problem.\r\n\r\nThe same error occurs both when I run it on TF or Keras with TF backend.\r\n\r\n It should be easily reproducible by running a simple graph with `tf.nn.atrous.conv2d` with a rate higher than ~280.", "@israelg99 it might be useful to give a self-contained example someone could just copy-paste to reproduce your problem", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7913, "title": "can there any examples about multi GPU or multi machine training", "body": "the news said 64 GPU can acheive 56 speed,how can we build for training test.  \r\ncan there any examples?", "comments": ["cc @tfboyd ", "#7679 Tracking progress on another issue.  We will have both in one script that allows you to try different options, e.g. data formats, ps placement and variable update options.  I would love to release it now as I know many people are really interested.  Closing this issue to keep tracking on the other one.  "]}, {"number": 7912, "title": "Validating Tensorflow On OS X (Sierra 10.12.3)", "body": "Using python 2.7. Created virtualenv\r\nInstalled tensorflow on virtualenv as per instruction\r\nWent to validation steps and below is the output:\r\n\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('hello, tensorflow!')\r\n>>> sess = tf.Session()\r\n**_W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations._**\r\n>>> print (sess.run(hello))\r\nhello, tensorflow!\r\n\r\nAny idea what I have done wrong? The message does not appear to be a show stopper rather an unhelpful warning message. Will crack on with my R+D but would appreciate knowing what I can do to fix this!\r\n\r\nCheers,\r\n\r\nG", "comments": ["The messages simply say that TensorFlow can run faster on your machine, if you build from sources.\r\nThey are only warning messages, which will only print once in your programs.\r\nWhen using prebuilt pip packages, we aim maximum compatibility, so users with newer CPUs will see such messages."]}, {"number": 7911, "title": "TensorBoard Image download fails if I expand some node before download", "body": "TensorBoard seems unable to download flow diagram picture if before download picture I expand some node. In the save picture window the name of picture isn't generated, appears a generic \"download\" filename, and if I click \"save\", download starts but fails instantly. \r\n![sample_pic](https://cloud.githubusercontent.com/assets/8108287/23348702/ee3061fc-fcac-11e6-9c79-73df698cca0f.JPG)\r\n\r\n\r\nTensorflow v 1.0.0\r\nWindows 10 x64", "comments": ["@dandelionmane @jart might have some ideas here.", "I am going to tentatively close this issue because I cannot reproduce it. Feel free to re-open as an issue within the new repository for TensorBoard: https://github.com/tensorflow/tensorboard/issues\r\n\r\ntensorflow/tensorflow will no longer accept TensorBoard-related issues. I am sorry for not having responded.\r\n\r\nI did though file a different but related issue: tensorflow/tensorboard#88"]}, {"number": 7910, "title": "How tensorflow handles complex gradient ?", "body": "Let **z** is a complex variable, **C(z)** is its conjugation.\r\nIn complex analysis theory, the derivative of **C(z)** w.r.t **z** don't exist. But in tesnsorflow, we can calculate **dC(z)/dz** and the result is just **1**.\r\nHere is an example:\r\n>x = tf.placeholder('complex64',(2,2))\r\ny = tf.reduce_sum(tf.conj(x))\r\nz = tf.gradients(y,x)\r\nsess = tf.Session()\r\nX = np.random.rand(2,2)+1.j*np.random.rand(2,2)\r\nX = X.astype('complex64')\r\nZ = sess.run(z,{x:X})[0]\r\n\r\nThe input **X** is\r\n>[[ 0.17014372+0.71475762j  0.57455420+0.00144318j]\r\n    [0.57871044+0.61303568j  0.48074263+0.7623235j ]]\r\n\r\nand the result **Z** is\r\n>[[ 1.-0.j  1.-0.j]\r\n    [1.-0.j  1.-0.j]]\r\n       \r\nI don't understand why the gradient is set to be **1**?\r\nAnd I want to know **how tensorflow handles the complex gradients in general**.", "comments": ["This is a question better suited for StackOverflow. They monitor under TensorFlow flag."]}, {"number": 7909, "title": "is it tf.train.MomentumOptimizer is SGD+monment  Optimizer???", "body": "is it tf.train.MomentumOptimizer is SGD+monment  Optimizer???", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 7908, "title": "Changing computer make the pretrained model fail", "body": "I switched to another computer to keep training my model .But I found the loss is as high as random init after loading the pretrained model. I trained overagain on this new computer and the restore seems work.What's wrong? Hope for help.\r\n\r\nusing TF:1.0 ", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, include what model you are trying to train and how your invoking it, moving it to another machine, etc.. We ask for this in the issue submission template, because  it is really difficult to help without that information. Thanks!", "Thanks for response! I am using the latest version of TF(just installed  4 days ago). On Ubuntu 14.04. \r\nWith 4 GTX1080 .Using vgg16 and a InceptionV1 with just 4 conv layers. Both of them show this problem.\r\nThe part of Saving and reloading is shown bellow\r\n`    init_op = tf.initialize_all_variables()\r\n\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        #sess.run(init_op)\r\n        #new_saver = tf.train.import_meta_graph('/mnt/data2/sunanlan/sunanlan/DGD/check25_rota_guiyi/lr_0.031_step_39.ckpt.meta')\r\n\r\n        #print( tf.train.latest_checkpoint('./'))\r\n\r\n        #what=new_saver.restore(sess,'/mnt/data2/sunanlan/sunanlan/DGD/check25_rota_guiyi/lr_0.031_step_39.ckpt')\r\n        saver.restore(sess, \"/mnt/data2/sunanlan/sunanlan/DGD/from_2/step_66.ckpt\")\r\n        tf.train.start_queue_runners(sess=sess)\r\n        for epoch in range(100):\r\n            widgets = [\"train D epoch #%d|\" % epoch, Percentage(), Bar(), ETA()]\r\n            pbar = ProgressBar(maxval=1000, widgets=widgets)\r\n            pbar.start()\r\n            loss_avg = 0\r\n\r\n            for step in range(1001):\r\n                pbar.update(step)\r\n                imgs_n,labels_n = readdatas.read(lengbat,160,70,image_paths,labels_in,classes)\r\n                global is_train\r\n                is_train = True\r\n\r\n\r\n                _, loss_value,lrt,img_all = sess.run([train_tensor,total_loss,lr,img_new],{imgs:imgs_n,labels:labels_n})\r\n                loss_avg += loss_value\r\n                if step % 50 == 0:\r\n                    avg = loss_avg / 50\r\n                    print(\"loss: %f \" % (avg))\r\n                    print(\"lr: %f\" %(lrt))\r\n                    loss_avg = 0\r\n                if step == 1000:\r\n                     saver_path = saver.save(sess, \"./from_2/step_\"+str(epoch)+\".ckpt\")\r\n`\r\n\r\n", "I just use `scp -r ` to download it on another conputer. Is that wrong? And what shocked me today is I trained , reloaded InceptionV1 with 4 conv layers on the same computer and got the same phenomenon\r\n@aselle ", "@AnlanSun Your code seems to write checkpoints based on epoch number.  You load epoch 66, then start training from epoch=0 and overwrite checkpoints every 1000 steps.  Are you sure you didn't overwrite your own checkpoint file at some point?\r\n", "@AnlanSun Your code appears to generate checkpoint filenames numbered by `epoch`.  You load the checkpoint from epoch66 and then start counting from zero, saving a new checkpoint every 1000 training steps.  Are you sure you didn't accidentally overwrite your checkpoint file at some point?\r\n\r\nPS. Copying the file, e.g. using `scp` ought to be just fine.", "@prb12 Thanks for the response. I think I did not make that mistake. Because even it overwrite the existing file the loss should not start from that high. But is it a problem to overwrite a file?  Or while I am training a model loaded form like epoch66 , Is it fine to overwrite that expoch66 file?", "@prb12 I can send you all my code. Cause I really can not find the mistake about that. Is ti possible to get you email? Hope for help", "You can post your code, @AnlanSun, but you will have the largest chance of someone finding your problem if you reduce the size of your code to the smallest piece of code that can reproduce the problem. That exercise of reduction also stands a great chance of allowing you to re-check your code for errors. Please do that first and post your code.\r\n", "Also, This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7907, "title": "Importing tensorflow in newly created virtualenv fails due to setuptools ContextualZipFile", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/pypa/setuptools/issues/252\r\n### Environment info\r\nOperating System: MasOS Sierra 10.12.3\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): None\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: \r\n```\r\n$ pip --version\r\npip 9.0.1 from /Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages (python 2.7)\r\n```\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\n1.0.0\r\n```\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom six.moves import cPickle as pickle\r\nfrom six.moves import range\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nNone\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-4a2138a67abe> in <module>()\r\n      3 from __future__ import print_function\r\n      4 import numpy as np\r\n----> 5 import tensorflow as tf\r\n      6 from six.moves import cPickle as pickle\r\n      7 from six.moves import range\r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/tensorflow/__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>()\r\n    122 from tensorflow.python.platform import resource_loader\r\n    123 from tensorflow.python.platform import sysconfig\r\n--> 124 from tensorflow.python.platform import test\r\n    125 \r\n    126 from tensorflow.python.util.all_util import remove_undocumented\r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/test.py in <module>()\r\n     81 import sys\r\n     82 if sys.version_info.major == 2:\r\n---> 83   import mock                # pylint: disable=g-import-not-at-top,unused-import\r\n     84 else:\r\n     85   from unittest import mock  # pylint: disable=g-import-not-at-top\r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/mock/__init__.py in <module>()\r\n      1 from __future__ import absolute_import\r\n----> 2 import mock.mock as _mock\r\n      3 from mock.mock import *\r\n      4 __all__ = _mock.__all__\r\n      5 #import mock.mock as _mock\r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/mock/mock.py in <module>()\r\n     69 from pbr.version import VersionInfo\r\n     70 \r\n---> 71 _v = VersionInfo('mock').semantic_version()\r\n     72 __version__ = _v.release_string()\r\n     73 version_info = _v.version_tuple()\r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/pbr/version.pyc in semantic_version(self)\r\n    458         \"\"\"Return the SemanticVersion object for this version.\"\"\"\r\n    459         if self._semantic is None:\r\n--> 460             self._semantic = self._get_version_from_pkg_resources()\r\n    461         return self._semantic\r\n    462 \r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/pbr/version.pyc in _get_version_from_pkg_resources(self)\r\n    444             # produced from a tarball where the package itself has not been\r\n    445             # installed into anything. Revert to setup-time logic.\r\n--> 446             from pbr import packaging\r\n    447             result_string = packaging.get_version(self.package)\r\n    448         return SemanticVersion.from_pip_string(result_string)\r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/pbr/packaging.py in <module>()\r\n     31 import pkg_resources\r\n     32 import setuptools\r\n---> 33 from setuptools.command import develop\r\n     34 from setuptools.command import easy_install\r\n     35 from setuptools.command import egg_info\r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/setuptools/command/develop.py in <module>()\r\n      9 \r\n     10 from pkg_resources import Distribution, PathMetadata, normalize_path\r\n---> 11 from setuptools.command.easy_install import easy_install\r\n     12 from setuptools import namespaces\r\n     13 import setuptools\r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py in <module>()\r\n     49 from setuptools.py27compat import rmtree_safe\r\n     50 from setuptools.command import setopt\r\n---> 51 from setuptools.archive_util import unpack_archive\r\n     52 from setuptools.package_index import (\r\n     53     PackageIndex, parse_requirement_arg, URL_SCHEME,\r\n\r\n/Users/rituraj_tiwari/PythonVirtualEnvs/tensorflow/lib/python2.7/site-packages/setuptools/archive_util.py in <module>()\r\n      9 from distutils.errors import DistutilsError\r\n     10 \r\n---> 11 from pkg_resources import ensure_directory, ContextualZipFile\r\n     12 \r\n     13 __all__ = [\r\n\r\nImportError: cannot import name ContextualZipFile\r\n```", "comments": ["I think this is not really a Tensorflow issue. I need to resolve this with setuptools."]}]