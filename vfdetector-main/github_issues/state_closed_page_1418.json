[{"number": 10437, "title": "Enabling TF to build LLVM AMDGPU backend for XLA", "body": "Hi,\r\nWhen I enable XLA during configure stage and run `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`, it starts pulling [llvm](https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.BUILD) and build for ARM, PPC, X86 and NVPTX backends. Is there a way to enable AMDGPU backend? (Matter of fact, all backends supported by LLVM?)\r\n", "comments": ["Not sure what will be different for a GPU target, but for a CPU target, I modified the llvm.BUILD to add the target to the llvm_targets = [] list, added the target { \"name\":, ... \"tbl_outs\": [(...)] } entries to llvm_target_list, and \"cc_library\" entries like the other CPU targets (<target>_asm_printer, <target>_code_gen, <target>_desc, <target>_disassembler, <target>_info).  Once I had all of this, in compiler/xla/service/cpu/BUILD I added @llvm//:<target>_code_gen and @llvm//:<target>_disassembler to the deps list for the \"cpu_compiler\" cc_library.\r\nFrom what I see, you'd have to do something like this to enable any new target, and there isn't a way to enable \"all\" of the targets that are available in upstream llvm...and I'm not sure what would change for the AMDGPU GPU target.", "Hi,\r\nThank you for response. I tried to add the entries you mentioned and ran into build errors.\r\nI documented my observations here:\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/Diy-mVlcNPQ\r\n", "Hmm...\r\n...looking more closely at the AMDGPU backend.  Unlike any of the other current supported backends, it looks like the intrinsics you're interested in are in lib/Target/AMDGPU, rather than what is in include/llvm/IR/IntrinsicsAMDGPU.td...and you're the only target that uses \"-gen-intrinsic\" directly.  I'm guessing that this is problematic.  The regular CMakeLists.txt rule for this is:\r\ntablegen(LLVM AMDGPUGenIntrinsics.inc -gen-tgt-intrinsic)\r\n Can you try \"-gen-tgt-intrinsic\" instead?", "Ah! Thank you for the insight of CMake->Bazel mapping of -gen* flags. Using the idea, I cleaned up llvm.BUILD file and able to build AMDGPU backend. \r\nThank you very much!", "Hi,\r\nWhen I changed [llvm.BUILD](https://gist.github.com/adityaatluri/8b8ca1f326d6f1537c17c98db84c6ae3) build to work for AMDGPU target, I ran [tf2.py](https://github.com/gpu0/tf-code/blob/master/tf2.py), it is giving out following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf1.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN4llvm6AMDGPU9isComputeEj\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\nHere are the configure steps I tried:\r\n```\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n]\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N]\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N]\r\nNo CUDA support will be enabled for TensorFlow\r\n......\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\n```\r\n\r\nAnd, the following build commands:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nsudo pip install /tmp/tensorflow_pkg/tensorflow-1.2.0rc1-cp27-cp27mu-linux_x86_64.whl\r\n```", "We aren't currently supporting the AMDGPU backend unfortunately, so marking 'community support'.", "Hi @michaelisard ,\r\nI am try to get llvm.BUILD file get right. The code isn't the issue", "_ZN4llvm6AMDGPU9isComputeEj == llvm::AMDGPU::isCompute(unsigned int)\r\nThis looks like it is in lib/Target/AMDGPU/Utils.\r\nSo, looks like you need to build a amdgpu_utils library, like aarch64_utils.  Then, add it as a dependency on the required libraries.", "Hi,\r\nI am able to pass build stage. But the library `_pywrap_tensorflow_internal.so` do not contain `LLVMInitializeAMDGPU` variants. Here is `nm -S` dump for different targets\r\n\r\n```\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python$ nm -S _pywrap_tensorflow_internal.so | grep LLVMInitializeAArch64\r\n00000000035caed0 0000000000000032 t LLVMInitializeAArch64AsmPrinter\r\n00000000036915c0 0000000000000059 t LLVMInitializeAArch64Disassembler\r\n0000000003686580 00000000000000b8 t LLVMInitializeAArch64Target\r\n00000000036ecc70 000000000000007d t LLVMInitializeAArch64TargetInfo\r\n00000000036ab9b0 0000000000000113 t LLVMInitializeAArch64TargetMC\r\n00000000036eca30 0000000000000008 t _ZZ31LLVMInitializeAArch64TargetInfoENUlN4llvm6Triple8ArchTypeEE_4_FUNES1_\r\n```\r\nFor AMDGPU,\r\n```\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python$ nm -S _pywrap_tensorflow_internal.so | grep LLVMInitializeAMDGPU\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python$\r\n```\r\n\r\nThe files in `./external/llvm/lib/Target/AMDGPU` do contain the function declarations\r\n```\r\n~/tensorflow/bazel-tensorflow/external/llvm/lib/Target/AMDGPU$ grep -r LLVMInitialize  *\r\n\r\nAMDGPUAsmPrinter.cpp:extern \"C\" void LLVMInitializeAMDGPUAsmPrinter() {\r\nAMDGPUTargetMachine.cpp:extern \"C\" void LLVMInitializeAMDGPUTarget() {\r\nAsmParser/AMDGPUAsmParser.cpp:extern \"C\" void LLVMInitializeAMDGPUAsmParser() {\r\nDisassembler/AMDGPUDisassembler.cpp:extern \"C\" void LLVMInitializeAMDGPUDisassembler() {\r\nMCTargetDesc/AMDGPUMCTargetDesc.cpp:extern \"C\" void LLVMInitializeAMDGPUTargetMC() {\r\nTargetInfo/AMDGPUTargetInfo.cpp:extern \"C\" void LLVMInitializeAMDGPUTargetInfo() {\r\n```", "Hi @petecoup \r\nI forked tf and made my changes to llvm-amdgpu branch.\r\nhttps://github.com/adityaatluri/tensorflow/tree/llvm-amdgpu\r\n", "Yes.  For the CPU targets, in compiler/xla/service/cpu/cpu_compiler.cc, there is CpuCompiler::InitializeLLVMTarget, where CPU targets can add their own set of these initialization routines.  Looks like the NVPTX backend is initialized in compiler/xla/servce/gpu/llvm_gpu_backend/gpu_backend_lib.cc.  Don't know what that means for where to put AMDGPU...good luck!", "Hi @petecoup ,\r\nI am not concerned with code right now. If I could get TF to build AMDGPU target, that would make life lot easier. Is it possible for you to try out my forked version of TF? As long as the tf shared library has all the routines inside AMDGPU target, I can figure out a way to use them inside xla code.", "@adityaatluri, these targets are defined in the [BUILD](https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.BUILD#L25-L31) file. You could try adding AMDGPU to this list. You will also have to take care of any ```.def``` files that LLVM uses for this backend though.", "@annanay25 I already did https://github.com/adityaatluri/tensorflow/blob/llvm-amdgpu/third_party/llvm/llvm.BUILD#L826\r\n", "Okay, then I am in a very similar position as you are right now!\r\n\r\nRef: https://github.com/tensorflow/tensorflow/issues/10471\r\n\r\nTLDR;\r\nI build Polly with LLVM, but I am unable to get Polly's passes initialized in the XLA pipeline.\r\nA solution to your issue will hopefully give some insight to mine as well :)\r\n", "Not sure what you mean \"...not concerned with code right now\".  When compiling the offline compiler, tfcompile, directly calling these LLVMInitializeAMDGPU* routines is the only way you can get the target initialized so you can generate code for it (and I don't think it will even link in the guts of your target otherwise).  I imagine it is the same for the python wrapper.  For CPU targets, this is where I mentioned earlier, compiler/xla/service/cpu/cpu_compiler.cc:CpuCompiler::InitializeLLVMTarget.\r\n", "What I meant to say is, once I get AMDGPU target to build, I can reference it inside xla code. Having xla code supporting the target and not able to link/build is unproductive use of time. I have gone through xla code and looks like a straight forward implementation for AMDGPU backend. I need help with getting AMDGPU target built right. ", "@michaelisard \r\nCan you assign this to some one at google who can help with this issue. Currently this is a major hurdle in enabling AMDGPU backend for xla. Once I can see LLVMInitializeAMDGPU* symbols inside _pywrap_tensorflow_internal.so, I am good to go! \r\nThanks!", "@hawkinsp kindly volunteered to help out.", "@michaelisard @hawkinsp Thank you very much. Appreciate the effort! \ud83d\udc4d ", "@adityaatluri I have a pending change that adds bazel build rules for the AMDGPU LLVM backend that should land in the next few days. That takes care of building the backend.\r\n\r\nOnce that it is landed, you will need to do at least these three things to use the backend:\r\na) uncomment `\"AMDGPU\"` in the `llvm_targets` list at the top of the build file.\r\nIt will appear roughly here once the change lands:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.BUILD#L25\r\nI left it commented out for now to avoid making builds longer since there are no actual users yet.\r\n\r\nb) add a bazel build dependency on `\"@llvm//:amdgpu_target\"` from somewhere.\r\nc) add a call to `LLVMInitializeAMDGPUTarget()`.\r\n\r\nNote that (c) is essential --- if a symbol is not used then bazel will strip it from the binary. That's probably what's going wrong with your attempt.\r\n\r\nAs an experiment I added such a call here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc#L127\r\nand I verified that LLVMInitializeAMDGPUTarget was present in the pywrap_tensorflow.so symbols. (Obviously the XLA *CPU* plugin isn't the right place in reality --- you really want to add it to the GPU plugin or add a separate AMDGPU plugin, depending on how similar you expect the two to be.)", "@hawkinsp Awesome!\r\nThank you for the insight into the bazel build process. I was able to do first 2 steps. I'll check for step 3.\r\nDo you know a time frame where the patch will be upstreamed?\r\nDo you have any plans to extending it to Stream Executor?", "The patch will be upstreamed in the next few days, whenever we sync up our branch with the Github repository.\r\n\r\nI'm afraid I don't have time to work on a AMDGPU Tensorflow StreamExecutor plugin, nor access to the necessary hardware. But if you are interested in working on one, we can definitely give you pointers.\r\n\r\nOne thought: XLA actually only uses a relatively small subset of the StreamExecutor API, so it isn't quite as imposing as it might seem. I believe the only essential pieces are those for things like opening the device, allocating memory, and launching kernels.", "Thank you for offering help, do you expect to change XLA architecture to fit in AMDGPU code gen along with NVPTX? Because, the triple is hard coded to nvptx [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc#L81)", "I don't think we have any very specific thoughts on how this should best be done.\r\n\r\nThere is a somewhat under-publicized mailing list for folks working on XLA development which might make a better forum for this sort of wide-ranging discussion than a Github issue:\r\nhttps://groups.google.com/forum/#!forum/xla-dev\r\n\r\nTo answer your specific question: I suspect the right thing to do is to have a separate AMDGPU XLA plugin, separate from the NVidia GPU XLA plugin. The two can share much or even most of the code (as do the current GPU and CPU backends, which share most of their LLVM logic, for example), but details like the target triple, or exactly which compiler passes to use, can differ between plugins. \r\n\r\nAs I mentioned, we don't really have a lot of time ourselves to work on an additional backend, but we would be very happy if one did exist, so this is a \"contributions welcome\" situation.", "Hi @hawkinsp \r\nThank you! "]}, {"number": 10436, "title": "Failed to build on Mac OSX: \"Could not find python binary\"", "body": "I am trying to compile tensorflow from source but cannot build `master`. The build for the pip package fails with the following error message\r\n\r\n```\r\nERROR: /Users/till/git/tensorflow/tensorflow/tensorboard/components/vz_sorting/BUILD:8:1: Compiling 2 TypeScript files failed: execrooter failed: error executing command \r\n  (cd /private/var/tmp/_bazel_till/3e885964b28c274cf7e8652af4ed1911/execroot/tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter bazel-out/local-py3-opt/bin/tensorflow/tensorboard/components/vz_sorting/vz_sorting-tsc-execroot.json): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nTraceback (most recent call last):\r\n  File \"bazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter\", line 168, in <module>\r\n    Main()\r\n  File \"bazel-out/host/bin/tensorflow/tensorboard/scripts/execrooter\", line 148, in Main\r\n    raise AssertionError('Could not find python binary: ' + PYTHON_BINARY)\r\nAssertionError: Could not find python binary: python3\r\n```\r\n\r\nI am running Mac OSX 10.12.4, bazel 0.4.5, python 3.6.1 in a conda virtual environment, Apple LLVM version 8.1.0 (clang-802.0.42). This might be related to https://github.com/bazelbuild/bazel/issues/2752.\r\n\r\nAny suggestions would be greatly appreciated.", "comments": ["What is the output of `which python3` in the terminal you are trying to build TF?\r\nAlso, did you run configure before building?", "@gunan, thanks for looking into this. Yes, I have run configure. Here is the output.\r\n\r\n```\r\ntensorflow git:(master) ./configure\r\nPlease specify the location of python. [Default is /Users/till/miniconda3/envs/tensorflow/bin/python]: \r\nFound possible Python library paths:\r\n  /Users/till/miniconda3/envs/tensorflow/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Users/till/miniconda3/envs/tensorflow/lib/python3.6/site-packages]\r\n\r\nUsing python library path: /Users/till/miniconda3/envs/tensorflow/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] \r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] \r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] \r\nNo CUDA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with MPI support? [y/N] \r\nMPI support will not be enabled for TensorFlow\r\n.........\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n```\r\n\r\nAnd some more diagnostics:\r\n\r\n```\r\nwhich python  # /Users/till/miniconda3/envs/tensorflow/bin/python\r\nwhich python3  # /Users/till/miniconda3/envs/tensorflow/bin/python3\r\npython3 --version  # Python 3.6.1 :: Continuum Analytics, Inc.\r\n```", "All looks good, thanks.\r\nWhat is the exact bazel command you are running after configure?", "I have tried the following both with the same error:\r\n\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nAlso, here is the full environment.\r\n\r\n```\r\npip freeze\r\nappnope==0.1.0\r\nbleach==1.5.0\r\ncycler==0.10.0\r\ndecorator==4.0.11\r\nentrypoints==0.2.2\r\nhtml5lib==0.999\r\nipykernel==4.6.1\r\nipython==6.1.0\r\nipython-genutils==0.2.0\r\nipywidgets==6.0.0\r\njedi==0.10.2\r\nJinja2==2.9.6\r\njsonschema==2.6.0\r\njupyter==1.0.0\r\njupyter-client==5.0.1\r\njupyter-console==5.1.0\r\njupyter-core==4.3.0\r\nMarkupSafe==0.23\r\nmatplotlib==2.0.2\r\nmistune==0.7.4\r\nnbconvert==5.2.1\r\nnbformat==4.3.0\r\nnotebook==5.0.0\r\nnumpy==1.12.1\r\npandocfilters==1.4.1\r\npexpect==4.2.1\r\npickleshare==0.7.4\r\nprompt-toolkit==1.0.14\r\nptyprocess==0.5.1\r\nPygments==2.2.0\r\npyparsing==2.1.4\r\npython-dateutil==2.6.0\r\npytz==2017.2\r\npyzmq==16.0.2\r\nqtconsole==4.3.0\r\nscipy==0.19.0\r\nsimplegeneric==0.8.1\r\nsix==1.10.0\r\nterminado==0.6\r\ntestpath==0.3\r\ntornado==4.5.1\r\ntqdm==4.14.0\r\ntraitlets==4.3.2\r\nwcwidth==0.1.7\r\nwidgetsnbextension==2.0.0\r\n```", "I see, configuration and bazel command all looks good.\r\n@jart This looks related to https://github.com/tensorflow/tensorflow/commit/b659bc39f27e81b3249f73710671059589c5daa1#diff-f0892bf77329d113fc9bd987ae1d7ec3\r\nAny ideas?", "does it work if you disable sandbox?", "@jart, I have tried `bazel build --config=opt --genrule_strategy=standalone --spawn_strategy=standalone //tensorflow/tools/pip_package:build_pip_package\r\n` without success. Is there another way to turn of sandboxing?", "@nlopezgi wrote the Python auto configure script IIRC.", "I'm quite sure this is related to https://github.com/bazelbuild/bazel/issues/2752. Until that fix percolates through the releases: I got the build to move on without tensorboard by deleting line https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L162, i. e.\r\n\r\n```\"//tensorflow/tensorboard\",```\r\n\r\nI also tried changing the 239 occurrences of ```PYTHON_?(BIN)(ARY)?_?(PATH)? ?=``` in 132 files, plus the other 196 occurrences in 108 files in ```/tmp/_bazel/```, but bazel apparently checks hashes for the files it generates. ", "@MatthiasWinkelmann, thanks for the suggestion.\r\n\r\nI've just tried compiling with the new bazel 0.5.1 but get the same error.", "I can compile tensorflow using the current `master` of bazel so hopefully this issue will be fixed in bazel 0.5.2. Thanks for your help @gunan, @jart, @MatthiasWinkelmann.", "@tillahoffmann , could you tell me how to install the master version of bazel? so that I can solve the problem like yours? Thank you!", "@justprotato \r\n\r\nTo test out bazel, you need to compile it. To compile a development version of Bazel, you need the latest released version of bazel, which can be compiled from source.\r\n\r\nbazel build //src:bazel builds the Bazel binary using bazel from your PATH and the resulting binary can be found at bazel-bin/src/bazel. This is the recommended way of rebuilding Bazel once you have bootstrapped it.\r\n\r\n1. Install a release version of Bazel\r\n\r\n2. git clone the master branch of Bazel then checkout HEAD (https://github.com/bazelbuild/bazel/issues/2140)\r\n\r\n3. Run:  `bazel build //src:bazel` in the directory where you have the master branch files.\r\n\r\n4. The output files are in the src folder inside the bazel-bin folder.\r\n\r\n5. Replace your bazel file (example: /usr/local/bin/bazel) with the one in the bazel-bin/src folder. \r\n\r\nThen to install TensorFlow\r\n\r\n1. Build the pip package (https://www.tensorflow.org/install/install_sources)\r\n\r\n2. From the root of the TensorFlow directory run: './tensorflow/tools/pip_package/build_pip_package.sh /tmp/tensorflow_pkg'\r\n\r\n3. Run: 'pip install /tmp/tensorflow_pkg/tensorflow-1.2.0rc2-cp36-cp36m-linux_x86_64.whl'", "> ERROR:\r\n /private/var/tmp/_bazel_sohilshrestha/ae3fc0f2acf0ccc3faaa1a969a080e66/external/base/image/BUILD:6:1: Couldn't build file external/base/image/003.tar.gz.nogz.sha256: SHA256 external/base/image/003.tar.gz.nogz.sha256 failed (Exit 1): sha256 failed: error executing command \r\n  (cd /private/var/tmp/_bazel_sohilshrestha/ae3fc0f2acf0ccc3faaa1a969a080e66/execroot/phd && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/bazel_tools/tools/build_defs/hash/sha256 bazel-out/darwin-opt/bin/external/base/image/003.tar.gz.nogz bazel-out/darwin-opt/bin/external/base/image/003.tar.gz.nogz.sha256)\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nTraceback (most recent call last):\r\n  File \"bazel-out/host/bin/external/bazel_tools/tools/build_defs/hash/sha256\", line 203, in <module>\r\n    Main()\r\n  File \"bazel-out/host/bin/external/bazel_tools/tools/build_defs/hash/sha256\", line 176, in Main\r\n    raise AssertionError('Could not find python binary: ' + PYTHON_BINARY)\r\nAssertionError: Could not find python binary: python3.6\r\n\r\n`which python3\r\n/usr/local/bin/python3`\r\n\r\nSimilar ERROR encountered. Any help please?\r\n\r\n", "What version of Python are you running?\r\nIs it 3.6?\r\nDifferent topic but I just read that:\r\n'Note: As of version 1.2, TensorFlow no longer provides GPU support on macOS.'\r\n\r\nI'm not sure if you want to train without a GPU. ", "I am running python 3.6. \r\nGPU support was disabled while configuring.", "The error you are getting does not seem to me to be related at all with this thread (this thread was for issues with python related to TF configure, while your error is occurring when trying to run a python script located in bazel_tools/tools/build_defs/hash/sha256.\r\nMy first impression is that you need to have python in your path (not just python3) as this script under bazel_tools might just have it hardcoded somewhere (i.e. this error is not coming from TF code, its native bazel tools). ", "@nlopezgi Sorry for posting it in this thread. Will post in Bazel issue. However i tried keeping python3 path in PATH variable. But it will still generate the same error. ", "did you add a symlink from usr/local/bin/python -> python3\r\n(all that this script requires is likely for usr/local/bin/python to exist and point to some python)", "Not sure how to do that. Can you guide me through it with specific command?"]}, {"number": 10435, "title": "build optimize_for_inference fail", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n    No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    Windows 7 x64\r\n- **TensorFlow installed from (source or binary)**:\r\n    binary (pip3 install....)\r\n- **TensorFlow version (use command below)**:\r\n    tensorflow 1.1.0 CPU only\r\n- **Bazel version (if compiling from source)**:\r\n    Build label: 0.5.0\r\n    Build target: bazel-out/msys_x64-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n    Build time: Fri May 26 12:12:09 2017 (1495800729)\r\n    Build timestamp: 1495800729\r\n    Build timestamp as int: 1495800729\r\n  \r\n    $ protoc --version\r\n    libprotoc 3.2.0\r\n- **CUDA/cuDNN version**:\r\n    CPU Only\r\n\r\n### Describe the problem\r\nI want to build optimize_for_inference and optimize and would like to optimize my graph by the optimizer.But when I run ` bazel build tensorflow/python/tools:optimize_for_inference `.It will get me a problem--` Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//'`.The problem occurs when i try to build anything with tensorflow as dependancy,like:`convert_graphdef_memmapped_format`.  \r\nI've seen all the issues from github or stack overflow.And I tried all the fixes from issues or stackoverflow.But the problem still exists.\r\n### Source code / logs\r\nSource code:\r\n``` \r\nbazel build tensorflow/python/tools:optimize_for_inference && bazel-bin/tensorflow/python/tools/optimize_for_inference --input=frozen_inception_graph.pb --output=optimized_inception_graph.pb --frozen_graph=True --input_names=Mul --output_names=softmax\r\n```\r\nlogs:\r\n``` \r\nD:\\tensorflow-r1.2>bazel build tensorflow/python/tools:optimize_for_inference\r\n\u001b[32mINFO: \u001b[0mLoading complete.  Analyzing...\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 257,202 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 501,196 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 951,646 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,433,556 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,197,176 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,997,976 bytes\r\n\u001b[1A\u001b[K\u001b[31m\u001b[1mERROR: \u001b[0mD:/tensorflow-r1.2/tensorflow/python/tools/BUILD:133:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such packa\r\nge '@protobuf//': Traceback (most recent call last):\r\n        File \"D:/tensorflow-r1.2/tensorflow/workspace.bzl\", line 117\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"D:/tensorflow-r1.2/tensorflow/workspace.bzl\", line 108, in _apply_patch\r\n                _execute_and_check_ret_code(repo_ctx, cmd)\r\n        File \"D:/tensorflow-r1.2/tensorflow/workspace.bzl\", line 92, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(3) when executing 'c:/tools/msys64/usr/bin/bash.exe -c patch -p1 -d C:/users/liba/appdata/local/temp/_bazel_liba/mffpt2ks/external/protobuf -i D:/tensorflow-r1.2/third_party/proto\r\nbuf/add_noinlines.patch':\r\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\r\n\r\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\r\n\r\nThis application has requested the Runtime to terminate it in an unusual way.\r\nPlease contact the application's support team for more information.\r\n and referenced by '//tensorflow/python/tools:optimize_for_inference'.\r\n\u001b[31m\u001b[1mERROR: \u001b[0mAnalysis of target '//tensorflow/python/tools:optimize_for_inference' failed; build aborted.\r\n\u001b[32mINFO: \u001b[0mElapsed time: 12.754s\r\n```\r\n", "comments": ["To me it looks like downloading protobuf failed.\r\nProtobuf package is >4MB, and your messages seem to abruptly end at around 3MB.\r\nThis may be caused by a flaky internet connection.\r\nI would recommend retrying build.", "@gunan Thanks for your reply!But it does not seem to be this problem.\r\n I did the following:\r\n1. Download file to some safe folder from url:\"http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz\"\r\n2. cd to folder\r\n3. python -m SimpleHTTPServer 8000\r\n4. Change tensorflow/tensorflow/workspace.bzl from url = \"http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz\" to url = \"http://localhost:8000/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz\"\r\n\r\nDoing this can make sure that protobuf can be downloaded.But it also have this problem.\r\n\r\nLog:\r\n``` \r\nD:\\tensorflow-r1.2>bazel build tensorflow/python/tools:optimize_for_inference\r\n\u001b[32mINFO: \u001b[0mLoading complete.  Analyzing...\r\n\u001b[1A\u001b[K\u001b[31m\u001b[1mERROR: \u001b[0mD:/tensorflow-r1.2/tensorflow/python/tools/BUILD:133:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such packa\r\nge '@protobuf//': Traceback (most recent call last):\r\n        File \"D:/tensorflow-r1.2/tensorflow/workspace.bzl\", line 117\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"D:/tensorflow-r1.2/tensorflow/workspace.bzl\", line 108, in _apply_patch\r\n                _execute_and_check_ret_code(repo_ctx, cmd)\r\n        File \"D:/tensorflow-r1.2/tensorflow/workspace.bzl\", line 92, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(3) when executing 'c:/tools/msys64/usr/bin/bash.exe -c patch -p1 -d C:/users/liba/appdata/local/temp/_bazel_liba/mffpt2ks/external/protobuf -i D:/tensorflow-r1.2/third_party/proto\r\nbuf/add_noinlines.patch':\r\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\r\n\r\nStderr: Assertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\r\n\r\nThis application has requested the Runtime to terminate it in an unusual way.\r\nPlease contact the application's support team for more information.\r\n and referenced by '//tensorflow/python/tools:optimize_for_inference'.\r\n\u001b[31m\u001b[1mERROR: \u001b[0mAnalysis of target '//tensorflow/python/tools:optimize_for_inference' failed; build aborted.\r\n\u001b[32mINFO: \u001b[0mElapsed time: 9.866s\r\n```", "Could you try installing \"patch\" using \"pacman\" of \"msys\" and retry building?\r\n", "@gunan I try `pacman -Syuu --noconfirm patch` and `pacman -S patch`.But it still no work.", "@meteorcloudy Could you look into this?", "@JcmeLs Looks like it failed when applying a [patch](https://github.com/tensorflow/tensorflow/blob/master/third_party/protobuf/add_noinlines.patch) to `cpp_file.cc`.\r\nCan you manually try \r\n`patch -p1 -d C:/users/liba/appdata/local/temp/_bazel_liba/mffpt2ks/external/protobuf -i D:/tensorflow-r1.2/third_party/proto\r\nbuf/add_noinlines.patch` see if you can reproduce the error?\r\n\r\n", "@meteorcloudy I try and get this problem  \r\n``` \r\nD:\\tensorflow-r1.2>patch -p1 -d C:/users/liba/appdata/local/temp/_bazel_liba/mffpt2ks/external/protobuf -i D:/tensorflow-r1.2/third_party/protobuf/add_noinlines.patch\r\npatching file src/google/protobuf/compiler/cpp/cpp_file.cc\r\nAssertion failed: hunk, file ../patch-2.5.9-src/patch.c, line 354\r\n\r\nThis application has requested the Runtime to terminate it in an unusual way.\r\nPlease contact the application's support team for more information.\r\n```\r\nIt seem patch problem.", "Found a similar issue here: https://github.com/squizlabs/PHP_CodeSniffer/issues/637 \r\nCan you try adding `--binary` option?", "@meteorcloudy Thanks,`patch -p1 -d C:/users/liba/appdata/local/temp/_bazel_liba/mffpt2ks/external/protobuf -i D:/tensorflow-r1.2/third_party/protobuf/add_noinlines.patch --binary`can work.But how to add `--binary` in script\uff1f\r\n\r\nI add `--binary` to workspace.bzl _apply_patch function.It seem work!Thanks again.", "@JcmeLs Add it here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L101\r\n\r\nI'll also send a PR to fix this.", "@meteorcloudy I get it!Thanks a lot!", "@JcmeLs BTW, building TF on Windows requires `--cpu=x64_windows_msvc and --host_cpu=x64_windows_msvc` as build option, and also adding `--copt=-w --host_copt=-w` to suppress warning messages. :)", "@meteorcloudy Thanks for your reminder!", "@meteorcloudy emmm...Actually,there is still a problem ` no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git'`.I found issues and I got the way to fix it.But in windows it can't run `./configure`.And I try to upgrade TensorFlow 1.2.0 RC1.But it still.\r\nlog:\r\n``` \r\n\u001b[32mINFO: \u001b[0mLoading complete.  Analyzing...\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mLoading package: tensorflow/tools/tfprof/internal\r\n\u001b[1A\u001b[K\u001b[31m\u001b[1mERROR: \u001b[0mD:/tensorflow-r1.2/tensorflow/core/BUILD:1395:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git'\r\n defined by D:/tensorflow-r1.2/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\n\u001b[31m\u001b[1mERROR: \u001b[0mD:/tensorflow-r1.2/tensorflow/core/BUILD:1395:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by D:/te\r\nnsorflow-r1.2/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\n\u001b[31m\u001b[1mERROR: \u001b[0mD:/tensorflow-r1.2/tensorflow/core/BUILD:1395:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defi\r\nned by D:/tensorflow-r1.2/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\n\u001b[32mINFO: \u001b[0mLoading package: tensorflow/core/kernels\r\n\u001b[1A\u001b[K\u001b[31m\u001b[1mERROR: \u001b[0mAnalysis of target '//tensorflow/python/tools:optimize_for_inference' failed; build aborted.\r\n\u001b[32mINFO: \u001b[0mElapsed time: 0.959s\r\n\r\nD:\\tensorflow-r1.2>./configure\r\n'.' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n```\r\n", "I also build it in Docker.I the image from gcr.io/tensorflow/tensorflow:latest-devel.And I run `bazel build tensorflow/python/tools:optimize_for_inference`.It also had a error.\r\nlog:\r\n``` \r\nERROR: /tensorflow/tensorflow/core/kernels/BUILD:1399:1: C++ compilation of rule '//tensorflow/core/kernels:padding_fifo_queue' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 101 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\r\nTarget //tensorflow/python/tools:optimize_for_inference failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```", "@JcmeLs You can download MSYS and run ./configure in it.", "@meteorcloudy It also can't run it.", "@JcmeLs Can you show me the output?\r\nMine is like:\r\n```\r\npcloudy@pcloudy0-w MSYS ~/workspace/tensorflow\r\n$ ./configure\r\nPlease specify the location of python. [Default is /c/Program Files/Anaconda3/python]:\r\nFound possible Python library paths:\r\n  C:\\Program Files\\Anaconda3\r\n  C:\\Program Files\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Program Files\\Anaconda3]\r\n\r\nUsing python library path: C:\\Program Files\\Anaconda3\r\nDo you wish to build TensorFlow with MKL support? [y/N]\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N]\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N]\r\nNo CUDA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with MPI support? [y/N]\r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished\r\n```", "@meteorcloudy Oh...sorry,I forgot to give you the log.\r\nLog:\r\n``` \r\nD:\\tensorflow-r1.2>bazel build tensorflow/python/tools:optimize_for_inference\r\n\u001b[32mINFO: \u001b[0mLoading complete.  Analyzing...\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/glennrp/libpng/archive/v1.2.53.zip: 43,598 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 1,157,380 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz: 81,821 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/pypi.python.org/packages/b7/7f/44d3cfe5a12ba002b253f6985a4477edfa66da53787a2a838a40f6415263/Werkzeug-0.11.10.tar.gz: 119,386 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/google/gemmlowp/archive/a6f29d8ac48d63293f845f2253eccbf86bc28321.tar.gz: 140,429 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/google/snappy/archive/1.1.4.zip: 147,579 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz: 103,268 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/github.com/glennrp/libpng/archive/v1.2.53.zip: 322,060 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/pypi.python.org/packages/b7/7f/44d3cfe5a12ba002b253f6985a4477edfa66da53787a2a838a40f6415263/Werkzeug-0.11.10.tar.gz: 183,736 bytes\r\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mDownloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 303,836 bytes\r\n\u001b[1A\u001b[K\u001b[31m\u001b[1mERROR: \u001b[0mD:/tensorflow-r1.2/util/python/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n        File \"D:/tensorflow-r1.2/third_party/py/python_configure.bzl\", line 282\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"D:/tensorflow-r1.2/third_party/py/python_configure.bzl\", line 238, in _create_local_python_repository\r\n                _check_python_lib(repository_ctx, python_lib)\r\n        File \"D:/tensorflow-r1.2/third_party/py/python_configure.bzl\", line 190, in _check_python_lib\r\n                _python_configure_fail(\"Invalid python library path:  %...)\r\n        File \"D:/tensorflow-r1.2/third_party/py/python_configure.bzl\", line 38, in _python_configure_fail\r\n                fail(\"\\n%sPython Configuration Error:...))\r\n\r\n\u001b[0;31mPython Configuration Error:\u001b[0m Invalid python library path:  C:UsersLIBAAppDataLocalProgramsPythonPython35libsite-packages\r\n and referenced by '//util/python:python_headers'.\r\n\u001b[31m\u001b[1mERROR: \u001b[0mAnalysis of target '//tensorflow/python/tools:optimize_for_inference' failed; build aborted.\r\n\u001b[32mINFO: \u001b[0mElapsed time: 1.954s\r\n```\r\nI set the `PYTHON_BIN_PATH=C:\\Users\\LIBA\\AppData\\Local\\Programs\\Python\\Python35` \r\n", "@meteorcloudy configure log:\r\n``` \r\nsh-4.4$ ./configure\r\nPlease specify the location of python. [Default is /c/Users/LIBA/AppData/Local/Programs/Python/Python35/python]:\r\nFound possible Python library paths:\r\n  C:\\Users\\LIBA\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\r\n  C:\\Users\\LIBA\\AppData\\Local\\Programs\\Python\\Python35\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\LIBA\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages]\r\n\r\nUsing python library path: C:\\Users\\LIBA\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] n\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n\r\nNo XLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] n\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] n\r\nNo CUDA support will be enabled for TensorFlow\r\n\u001b[32mINFO: \u001b[0mStarting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n```", "Can you try `PYTHON_BIN_PATH=C:/Users/LIBA/AppData/Local/Programs/Python/Python35` ? back slash doesn't work well in MSYS.", "Did you get this error after running ./configure?\r\n```\r\n\ufffd[0;31mPython Configuration Error:\ufffd[0m Invalid python library path:  C:UsersLIBAAppDataLocalProgramsPythonPython35libsite-packages\r\n and referenced by '//util/python:python_headers'.\r\n```", "@meteorcloudy Thank you so much!!!It seems work.I will sort out a solution.Thanks again!!!", "@JcmeLs Nice!", "@meteorcloudy emmm...Happy too early. There is an error that leads to build failure...\r\nlog:\r\n``` \r\n\u001b[31m\u001b[1mERROR: \u001b[0mC:/users/liba/appdata/local/temp/_bazel_liba/mffpt2ks/external/gif_archive/BUILD.bazel:8:1: C++ compilation of rule '@gif_archive//:gif' failed: msvc_cl.bat failed: error executing\r\n command external/local_config_cc/wrapper/bin/msvc_cl.bat /DOS_WINDOWS=OS_WINDOWS /DCOMPILER_MSVC /DNOGDI /DNOMINMAX /DPRAGMA_SUPPORTED /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_N\r\nO_WARNINGS ... (remaining 29 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.\r\nC:\\users\\liba\\appdata\\local\\temp\\_bazel_liba\\mffpt2ks\\execroot\\tensorflow-r1.2\\external\\gif_archive\\lib\\gif_hash.c(14): fatal error C1083: \u65e0\u6cd5\u6253\u5f00\u5305\u62ec\u6587\u4ef6: \u201cunistd.h\u201d: No such file or directoryC:\\u\r\nsers\\liba\\appdata\\local\\temp\\_bazel_liba\\mffpt2ks\\execroot\\tensorflow-r1.2\\external\\gif_archive\\lib\\gif_hash.c(14): fatal error C1083: \u65e0\u6cd5\u6253\u5f00\u5305\u62ec\u6587\u4ef6: \u201cunistd.h\u201d: No such file or directory\r\nTarget //tensorflow/python/tools:optimize_for_inference failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\u001b[32mINFO: \u001b[0mElapsed time: 1617.987s, Critical Path: 68.44s\r\n```\r\nShould I install VS?", "Are you compiling with `--cpu=x64_windows_msvc --host_cpu=x64_windows_msvc --copt=-w --host_copt=-w` ?", "https://github.com/tensorflow/tensorflow/commit/08ed32dbb9e8f67eec9efce3807b5bdb3933eb2f This commit is just merged into HEAD, so these options are necessary unless you are building from TF HEAD.", "Sorry ,I forgot!!!I will try again.", "@JcmeLs No problem, good luck.", "Unfortunately , it `building complete` but...`Target //tensorflow/python/tools:optimize_for_inference failed to build`\r\n\r\nLog:\r\n``` \r\n.\\tensorflow/core/platform/default/logging.h(126): error C2593: \u201coperator <<\u201d\u4e0d\u660e\u786e.\\tensorflow/core/platform/default/logging.h(126): error C2593: \u201coperator <<\u201d\u4e0d\u660e\u786e\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(495): note: \u53ef\u80fd\u662f\u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<\r\n(std::basic_streambuf<char,std::char_traits<char>> *)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(495): note: \u53ef\u80fd\u662f\u201cstd::basic_ostream<char,std::char_traits<char>> &std::\r\nbasic_ostream<char,std::char_traits<char>>::operator <<(std::basic_streambuf<char,std::char_traits<char>> *)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(475): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(c\r\nonst void *)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(475): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>:\r\n:operator <<(const void *)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(455): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(l\r\nong double)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(455): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::\r\noperator <<(long double)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(435): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(d\r\nouble)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(435): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::opera\r\ntor <<(double)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(415): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(f\r\nloat)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(415): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operat\r\nor <<(float)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(395): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(u\r\nnsigned __int64)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(395): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<cha\r\nr>>::operator <<(unsigned __int64)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(375): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(_\r\n_int64)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(375): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::oper\r\nator <<(__int64)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(355): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(u\r\nnsigned long)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(355): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>\r\n::operator <<(unsigned long)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(335): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(l\r\nong)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(335): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operato\r\nr <<(long)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(315): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(u\r\nnsigned int)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(315): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>:\r\n:operator <<(unsigned int)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(290): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(i\r\nnt)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(290): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator\r\n <<(int)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(270): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(u\r\nnsigned short)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(270): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>\r\n>::operator <<(unsigned short)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(236): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(s\r\nhort)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(236): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operat\r\nor <<(short)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(216): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(b\r\nool)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(216): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operato\r\nr <<(bool)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(209): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(s\r\ntd::ios_base &(__cdecl *)(std::ios_base &))\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(209): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostrea\r\nm<char,std::char_traits<char>>::operator <<(std::ios_base &(__cdecl *)(std::ios_base &))\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(202): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(s\r\ntd::basic_ios<char,std::char_traits<char>> &(__cdecl *)(std::basic_ios<char,std::char_traits<char>> &))\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(202): note: \u6216  \u201cstd::b\r\nasic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(std::basic_ios<char,std::char_traits<char>> &(__cdecl *)(std::basic_ios<char,std::char_traits<ch\r\nar>> &))\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(196): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(s\r\ntd::basic_ostream<char,std::char_traits<char>> &(__cdecl *)(std::basic_ostream<char,std::char_traits<char>> &))\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(196): note: \u6216\r\n\u201cstd::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(std::basic_ostream<char,std::char_traits<char>> &(__cdecl *)(std::basic_ostream<char,std\r\n::char_traits<char>> &))\u201d\r\n.\\tensorflow/core/framework/types.h(68): note: \u6216  \u201cstd::ostream &tensorflow::operator <<(std::ostream &,const tensorflow::DeviceType &)\u201d.\\tensorflow/core/framework/types.h(68): note: \u6216  \u201cstd::ost\r\nream &tensorflow::operator <<(std::ostream &,const tensorflow::DeviceType &)\u201d\r\n.\\tensorflow/core/lib/core/status.h(117): note: \u6216  \u201cstd::ostream &tensorflow::operator <<(std::ostream &,const tensorflow::Status &)\u201d.\\tensorflow/core/lib/core/status.h(117): note: \u6216  \u201cstd::ostre\r\nam &tensorflow::operator <<(std::ostream &,const tensorflow::Status &)\u201d\r\n.\\tensorflow/core/lib/core/stringpiece.h(176): note: \u6216  \u201cstd::ostream &tensorflow::operator <<(std::ostream &,tensorflow::StringPiece)\u201d.\\tensorflow/core/lib/core/stringpiece.h(176): note: \u6216  \u201cstd\r\n::ostream &tensorflow::operator <<(std::ostream &,tensorflow::StringPiece)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(692): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<char,std::char_traits<char>>(std::basic_ostre\r\nam<char,std::char_traits<char>> &,const char *)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(692): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator\r\n <<<char,std::char_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,const char *)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(739): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<char,std::char_traits<char>>(std::basic_ostre\r\nam<char,std::char_traits<char>> &,char)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(739): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<char\r\n,std::char_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,char)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(777): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<std::char_traits<char>>(std::basic_ostream<ch\r\nar,std::char_traits<char>> &,const char *)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(777): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<s\r\ntd::char_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,const char *)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(824): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<std::char_traits<char>>(std::basic_ostream<ch\r\nar,std::char_traits<char>> &,char)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(824): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<std::char\r\n_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,char)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(950): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<std::char_traits<char>>(std::basic_ostream<ch\r\nar,std::char_traits<char>> &,const signed char *)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(950): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operat\r\nor <<<std::char_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,const signed char *)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(957): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<std::char_traits<char>>(std::basic_ostream<ch\r\nar,std::char_traits<char>> &,signed char)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(957): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<st\r\nd::char_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,signed char)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(964): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<std::char_traits<char>>(std::basic_ostream<ch\r\nar,std::char_traits<char>> &,const unsigned char *)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(964): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::oper\r\nator <<<std::char_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,const unsigned char *)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(971): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<std::char_traits<char>>(std::basic_ostream<ch\r\nar,std::char_traits<char>> &,unsigned char)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(971): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<\r\nstd::char_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,unsigned char)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(981): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<char,std::char_traits<char>,T>(std::basic_ost\r\nream<char,std::char_traits<char>> &&,const _Ty &)\u201d\r\n        with\r\n        [\r\n            T=nullptr,\r\n            _Ty=nullptr\r\n        ]        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(1019): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<char,std::char_traits<char>>(std::basic_ostr\r\neam<char,std::char_traits<char>> &,const std::error_code &)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\ostream(1019): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &\r\nstd::operator <<<char,std::char_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,const std::error_code &)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\random(2583): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<char,std::char_traits<char>>(std::basic_ostre\r\nam<char,std::char_traits<char>> &,const std::bernoulli_distribution &)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\random(2583): note: \u6216  \u201cstd::basic_ostream<char,std::char_trait\r\ns<char>> &std::operator <<<char,std::char_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,const std::bernoulli_distribution &)\u201d\r\nC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\thread(246): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operator <<<char,std::char_traits<char>>(std::basic_ostrea\r\nm<char,std::char_traits<char>> &,std::thread::id)\u201dC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\\thread(246): note: \u6216  \u201cstd::basic_ostream<char,std::char_traits<char>> &std::operato\r\nr <<<char,std::char_traits<char>>(std::basic_ostream<char,std::char_traits<char>> &,std::thread::id)\u201d\r\n.\\tensorflow/core/platform/default/logging.h(126): note: \u5c1d\u8bd5\u5339\u914d\u53c2\u6570\u5217\u8868\u201c(std::ostream, const nullptr)\u201d\u65f6.\\tensorflow/core/platform/default/logging.h(126): note: \u5c1d\u8bd5\u5339\u914d\u53c2\u6570\u5217\u8868\u201c(std::ostream, co\r\nnst nullptr)\u201d\u65f6\r\n.\\tensorflow/core/platform/default/logging.h(186): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684\u51fd\u6570 \u6a21\u677f \u5b9e\u4f8b\u5316\u201cvoid tensorflow::internal::MakeCheckOpValueString<T2>(std::ostream *,const T &)\u201d\u7684\u5f15\u7528\r\n        with\r\n        [\r\n            T2=nullptr,\r\n            T=nullptr\r\n        ]        ]\r\n.\\tensorflow/core/platform/default/logging.h(229): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684\u51fd\u6570 \u6a21\u677f \u5b9e\u4f8b\u5316\u201cstd::string *tensorflow::internal::MakeCheckOpString<T1,T2>(const T1 &,const T2 &,const char *)\u201d\u7684\u5f15\u7528\r\n        with\r\n        [\r\n            T1=tensorflow::WorkerSession *,\r\n            T2=nullptr\r\n        ]        ]\r\nC:\\users\\liba\\appdata\\local\\temp\\_bazel_liba\\mffpt2ks\\execroot\\tensorflow-r1.2\\tensorflow\\core\\distributed_runtime\\base_rendezvous_mgr.cc(151): note: \u53c2\u89c1\u5bf9\u6b63\u5728\u7f16\u8bd1\u7684\u51fd\u6570 \u6a21\u677f \u5b9e\u4f8b\u5316\u201cstd::string *ten\r\nsorflow::internal::Check_NEImpl<T,nullptr>(const T1 &,const T2 &,const char *)\u201d\u7684\u5f15\u7528\r\n        with\r\n        [\r\n            T=tensorflow::WorkerSession *,\r\n            T1=tensorflow::WorkerSession *,\r\n            T2=nullptr\r\n        ]        ]\r\n\u001b[32mINFO: \u001b[0mBuilding complete.\r\n\u001b[1A\u001b[KTarget //tensorflow/python/tools:optimize_for_inference failed to build\r\n\u001b[32mINFO: \u001b[0mElapsed time: 33.263s, Critical Path: 7.84s\r\n```", "Looks like the error is \r\n```\r\n.\\tensorflow/core/platform/default/logging.h(126): error C2593: \u201coperator <<\u201d\u4e0d\u660e\u786e.\\tensorflow/core/platform/default/logging.h(126): error C2593: \u201coperator <<\u201d\u4e0d\u660e\u786e\r\n```\r\n\r\nI remember this bug, it should already be fixed at HEAD, can you try to build from HEAD instead of `tensorflow-r1.2`?", "Get~I will try again!", "@meteorcloudy Thank you very much!!!It can work now!!!", "Closing resolved issue.", "hi , some problems come when i install tensorflow serving , i donnot kown what's wrong with it , someone can help me ,thanks.\r\n1. I have install bazel-0.5.1 with brew on my mac\r\n2.when i execute ./configure , the log as below\r\n\r\n![image](https://user-images.githubusercontent.com/20752672/27464160-6e3d7c98-57fe-11e7-92da-53a90d971794.png)\r\n\r\n  You have bazel 0.5.1-homebrew installed.\r\nPlease specify the location of python. [Default is /usr/local/bin/python]: /usr/local/bin/python2.7\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\r\n/usr/local/lib/python2.7/site-packages/\r\nDo you wish to build TensorFlow with MKL support? [y/N]\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N]\r\nNo XLA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N]\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N]\r\nNo CUDA support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with MPI support? [y/N]\r\nMPI support will not be enabled for TensorFlow\r\nConfiguration finished \r\n\r\n3 but when i execute bazel build tensorflow_serving/...  , i got some errors below\r\n![image](https://user-images.githubusercontent.com/20752672/27464231-c9c9628e-57fe-11e7-8524-6aa5241802b4.png)\r\n\r\n\r\n    WARNING: /private/var/tmp/_bazel_yipeng5/08541152f1cd02b1d25360991f64186f/external/org_tensorflow/third_party/py/python_configure.bzl:31:3: Python Configuration Warning: 'PYTHON_LIB_PATH' environment variable is not set, using '' as default.\r\nERROR: /private/var/tmp/_bazel_yipeng5/08541152f1cd02b1d25360991f64186f/external/org_tensorflow/util/python/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/private/var/tmp/_bazel_yipeng5/08541152f1cd02b1d25360991f64186f/external/org_tensorflow/third_party/py/python_configure.bzl\", line 294\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/private/var/tmp/_bazel_yipeng5/08541152f1cd02b1d25360991f64186f/external/org_tensorflow/third_party/py/python_configure.bzl\", line 250, in _create_local_python_repository\r\n\t\t_check_python_lib(repository_ctx, python_lib)\r\n\tFile \"/private/var/tmp/_bazel_yipeng5/08541152f1cd02b1d25360991f64186f/external/org_tensorflow/third_party/py/python_configure.bzl\", line 198, in _check_python_lib\r\n\t\t_python_configure_fail(\"Invalid python library path:  %...)\r\n\tFile \"/private/var/tmp/_bazel_yipeng5/08541152f1cd02b1d25360991f64186f/external/org_tensorflow/third_party/py/python_configure.bzl\", line 38, in _python_configure_fail\r\n\t\tfail(\"%sPython Configuration Error:%s...))\r\nPython Configuration Error: Invalid python library path:\r\n and referenced by '@org_tensorflow//util/python:python_headers'.\r\nERROR: Analysis of target '//tensorflow_serving/servables/tensorflow/testdata:export_half_plus_two' failed; build aborted.\r\nINFO: Elapsed time: 2.111s\r\n\r\n\r\n\r\n  \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "https://github.com/gunan I am using windows with anaconda3, not bazel and I got the same error as \"'optimize_for_inference' is not recognized as an internal or external command,\r\noperable program or batch file.\".... Need your help!! My internet seems to be good. Thanks in advance."]}, {"number": 10434, "title": "Building TensorFlow 1.2 from source results in catastrophic error?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.2\r\n- **Bazel version (if compiling from source)**: 4.5.1\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GTX 860M\r\n- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\n### Describe the problem\r\nI have successfully installed TensorFlow 1.2 by upgrading the 1.1 version via Pip, but I wanted to install 1.2 from source to get the full benefits. However, strangely I received this error when building the pip package from bazel.\r\n\r\nINFO: From Compiling tensorflow/core/kernels/batch_norm_op_gpu.cu.cc:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h(271): internal error: assertion failed at: \"/dvs/p4/build/sw/rel/gpu_drv/r361/r361_00/drivers/compiler/edg/EDG_4.10/src/folding.c\", line 9819\r\n\r\n\r\n```\r\n1 catastrophic error detected in the compilation of \"/tmp/tmpxft_00007cc2_00000000-7_batch_norm_op_gpu.cu.cpp1.ii\".\r\nCompilation aborted.\r\nAborted (core dumped)\r\nERROR: /home/kwotsin/tensorflow_official/tensorflow/tensorflow/core/kernels/BUILD:2730:1: output 'tensorflow/core/kernels/_objs/batch_norm_op_gpu/tensorflow/core/kernels/batch_norm_op_gpu.cu.pic.o' was not created.\r\nERROR: /home/kwotsin/tensorflow_official/tensorflow/tensorflow/core/kernels/BUILD:2730:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n```\r\n\r\nI've not seen this error before - is it because I did not git check out the 1.2 version but instead the master branch?\r\n", "comments": ["If you didn't check out 1.2 you will get master, not 1.2, which contains strictly more stuff.\r\n\r\nWhat compiler are you using to build this? Because this compilation error happens while compiling a CUDA file, maybe there's a problem with the nvcc version you're using (assuming you're using nvcc).", "I get the same error when building with CUDA 8.0 and cuDNN 5.1.5 on Nvidia Jetson TX1 (ARM) from the `r1.0` branch.", "XQ, can you look at this? It looks like an internal compiler error in nvcc, but we must have triggered it only recently.", "Adding @benbarsdell, does this look like a NVCC problem? ", "Yes, looking into this now.\r\n@kwotsin could you provide the TF commit hash that you're on as well as the output of \"nvcc --version\"?", "I'm also seeing the same issue when building with CUDA 8.0, cuDNN 5.1.5 on Nvidia Jetson TX1 (ARM)\r\n\r\ngit commit (v1.1.0)\r\n1ec6ed51182adf8f1b03a3188c16cd8a45ca6c85\r\n\r\nnvcc --version\r\n```\r\nBuilt on Fri_Jul_15_14:52:12_CDT_2016\r\nCuda compilation tools, release 8.0, V8.0.33\r\n```", "note for TX1 users (https://devtalk.nvidia.com/default/topic/987306/jetson-tx1/internal-error-in-nvidia-driver-code-while-building-r0-12-or-master-tensorflow-on-tegra-tx1/2)\r\n\r\nAt least for Tensorflow r0.12, NVIDIA released a CUDA 8.0.64 version with Jetpack 3.0 for the TX2. From their forum it looked like they had issues with publicly released versions of CUDA 8.0.34 and nvcc for the TX1 that were NOT yet resolved.\r\n\r\n@kwotsin not sure if this applies to you, but is the full version of CUDA that you are using?\r\n@sunsided you're probably in the same boat as me on the TX1. I would try to contact NVIDIA and see if they have a fix that you can use. I have access to a TX2 that I will test with next.\r\n", "@jeichel-miovision What is the full version of CUDA you're referring to? I installed my CUDA quite some time ago so I suspect it's an earlier version of CUDA 8.0. FYI my error is from building on a laptop, not the TX1. \r\n\r\n@martinwicke I have tried to checkout with r1.2, and used `bazel clean` before I configured and built with bazel. \r\n\r\nI now have an error that goes like this:\r\n\r\n```\r\nubuntu@host:~/tensorflow_official/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nWARNING: /home/kwotsin/tensorflow_official/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': Use SavedModel Builder instead.\r\nWARNING: /home/kwotsin/tensorflow_official/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': Use SavedModel instead.\r\nINFO: Found 1 target...\r\nINFO: From Compiling tensorflow/core/kernels/softmax_op_gpu.cu.cc:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h(275): internal error: assertion failed at: \"/dvs/p4/build/sw/rel/gpu_drv/r361/r361_00/drivers/compiler/edg/EDG_4.10/src/folding.c\", line 9819\r\n\r\n\r\n1 catastrophic error detected in the compilation of \"/tmp/tmpxft_00001707_00000000-7_softmax_op_gpu.cu.cpp1.ii\".\r\nCompilation aborted.\r\nAborted (core dumped)\r\nERROR: /home/kwotsin/tensorflow_official/tensorflow/tensorflow/core/kernels/BUILD:2749:1: output 'tensorflow/core/kernels/_objs/softmax_op_gpu/tensorflow/core/kernels/softmax_op_gpu.cu.pic.o' was not created.\r\nERROR: /home/kwotsin/tensorflow_official/tensorflow/tensorflow/core/kernels/BUILD:2749:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 115.511s, Critical Path: 114.86s\r\n`\r\n\r\n\r\n1 catastrophic error detected in the compilation of \"/tmp/tmpxft_00001223_00000000-7_softmax_op_gpu.cu.cpp1.ii\".\r\nCompilation aborted.\r\nAborted (core dumped)\r\n```\r\n@benbarsdell Here is the output of my `nvcc --version`\r\n\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Fri_Jul_15_14:46:45_CDT_2016\r\nCuda compilation tools, release 8.0, V8.0.33\r\n\r\n```\r\nAlso, how do you check the TF commit hash?\r\n", "@kwotsin your nvcc --version actually posted the full release code: V8.0.33 \r\nIf possible, I would strongly recommend upgrading to the latest cuda version, >= V8.0.64, since there are known issues with the older versions.\r\n\r\nfor those interested in the jetson progress:\r\nI've also successful built TF v1.1.0 (1ec6ed51182adf8f1b03a3188c16cd8a45ca6c85) on the TX2 using CUDA V8.0.64 with bazel 0.5.1. \r\n\r\nI followed the EXACT same process as I did on my TX1, and as far as I can tell, I suspect the only major difference points to the difference in CUDA version.\r\n\r\nSpecifically for the TX1 / TX2 platform, I still had to patch TF v1.1.0 by making the modifications in lines 65 to 75 here: https://github.com/jetsonhacks/installTensorFlowTX2/blob/master/patches/tensorflow.patch\r\n\r\nI was then able to import tensorflow into python and run some basic test code to validate the installation.", "@jeichel-miovision thanks for your advice. I upgraded my cuda version and now installing from source is fixed. I have tried installing a version of TF higher than 1.0 in TX1 before but didn't manage to install it successfully. Perhaps it was because I used the older cuda version that came with jetpack 2.3.1.", "@jeichel-miovision, @kwotsin  Hi, may I ask how you upgrade your cuda on TX1/TX2? I used JetPack to install CUDA v8.0.33, and has seen the same error message `external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h(275): internal error: assertion failed at: \"/dvs/p4/build/sw/rel/gpu_drv/r361/r361_00/drivers/compiler/edg/EDG_4.10/src/folding.c\", line 9819`\r\n\r\nThanks in advance! :-)", "@sugartom I was not able to get the TX1 beyond 8.0.33 (but I didn't look into this aspect very hard). \r\n\r\nInstead I downloaded  JetPack 3.0 from NVIDIA (https://developer.nvidia.com/embedded/jetpack). When I installed CUDA for my TX2, the installer downloaded \"cuda-repo-l4t-8-0-local_8.0.64-1_arm64.deb\" into my jetpack_download folder and proceeded to install version 8.0.64 onto my TX2.\r\n\r\nI'm not sure how, or if it is currently possible, to install 8.0.64 on the TX1. \r\nFrom NVIDIA (https://developer.nvidia.com/embedded/jetpack)\r\n> JetPack 3.0 adds support for the new Jetson TX2 Developer Kit with a preview release of L4T with kernel 4.4, and continues to support the Jetson TX1 Developer Kit and the Jetson TK1 Developer Kit. Components for Jetson TX1 and Jetson TK1 Developer Kits remain unchanged from the previous version of JetPack. (A future release of JetPack will support both Jetson TX2 and Jetson TX1 with aligned API and Linux kernel versions.)", "@sugartom I also running an experiment and manually copied the cuda-repo-l4t-8-0-local_8.0.64-1_arm64.deb package to my TX1. `sudo dpkg -i cuda-repo-l4t-8-0-local_8.0.64-1_arm64.deb && sudo apt-get update && sudo apt-get upgrade`\r\n\r\nI'll try rebuilding tensorflow v1.1.0 throughout the day and post the results.", "@jeichel-miovision @sugartom I wasn't able to upgrade the cuda in the past due to some architectural issues. Basically the TX1 says it's using aarch64 and so even with the arm64 deb I couldn't install it. Perhaps a successful upgrade of CUDA would make it possible to install tensorflow above v1.0, which would enable a lot of other operations like quantization. thanks to @jeichel-miovision in advance for trying out!", "Hi, someone has try to compile it with the new Jetson Pack 3.1 ? \r\n\r\nI have this error.\r\n\r\n```bash\r\n\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/llvm/lib/MC/MachObjectWriter.cpp [for host]:\r\nexternal/llvm/lib/MC/MachObjectWriter.cpp: In member function 'void llvm::MachObjectWriter::writeNlist(llvm::MachObjectWriter::MachSymbolData&, const llvm::MCAsmLayout&)':\r\nexternal/llvm/lib/MC/MachObjectWriter.cpp:376:39: warning: 'AliaseeInfo' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     Address = AliaseeInfo->StringIndex;\r\n                                       ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-invalid-partial-specialization'\r\nINFO: From Compiling external/llvm/lib/MC/WasmObjectWriter.cpp [for host]:\r\nexternal/llvm/lib/MC/WasmObjectWriter.cpp: In member function 'virtual void {anonymous}::WasmObjectWriter::writeObject(llvm::MCAssembler&, const llvm::MCAsmLayout&)':\r\nexternal/llvm/lib/MC/WasmObjectWriter.cpp:802:27: warning: 'Index' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n       Export.Index = Index;\r\n                           ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-invalid-partial-specialization'\r\n[2,311 / 6,048] Compiling external/llvm/utils/TableGen/CodeGenRegisters.cpp [for host]\r\n\r\nServer terminated abruptly (error code: 14, error message: '', log file: '/home/nvidia/.cache/bazel/_bazel_nvidia/d2751a49dacf4cb14a513ec663770624/server/jvm.out')\r\n\r\n```\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I believe this is fixed, but I cannot say for sure. Can you reopen if this still occurs with a more recent TensorFlow?", "I get the same error when building with CUDA 8.0 and cuDNN 7.0 on ubuntu16.04 from the r1.13 branch.\r\ndevice\uff1a1080ti bazel version \uff1a0.20"]}, {"number": 10433, "title": "BeamsearchDecoder w/ AttentionWrapper ", "body": "Ubuntu 16.04\r\nTF version; 1.2.0.rc1\r\n\r\nHi Team, I'm implementing seq2seq framework using BeamSearchDecoder and got an error when using with AttentionWrapper that has no predefined time-steps.\r\n\r\nHere's my rough code\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n....\r\nself.encoder_cell = build_encoder_cell()\r\nself.encoder_inputs = tf.placeholder(dtype=tf.int32, shape=(None, None), name='encoder_inputs')\r\nself.encoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,), name='encoder_inputs_length')\r\n....\r\nself.encoder_outputs, self.encoder_last_state = tf.nn.dynamic_rnn(\r\n                cell=self.encoder_cell, inputs=self.encoder_inputs_embedded,\r\n                sequence_length=self.encoder_inputs_length, dtype=self.dtype,\r\n                time_major=False)\r\n....\r\nself.decoder_cell = build_decoder_cell()\r\nself.decoder_cell = seq2seq.AttentionWrapper(\r\n            cell=self.decoder_cell,\r\n            attention_mechanism=self.attention_mechanism,\r\n            attention_layer_size=self.hidden_units,\r\n            cell_input_fn=attn_decoder_input_fn,\r\n            initial_cell_state=encoder_last_state,\r\n            alignment_history=False,\r\n            name='Attention_Wrapper')\r\n\r\ntiled_batch_size = self.batch_size * self.beam_width\r\nself.decoder_initial_state = self.decoder_cell.zero_state(batch_size=tiled_batch_size)\r\n....\r\ninference_decoder = beam_search_decoder.BeamSearchDecoder(cell=self.decoder_cell,\r\n                                                               embedding=embed_and_input_proj,\r\n                                                               start_tokens=start_tokens,\r\n                                                               end_token=end_token,\r\n                                                               initial_state=self.decoder_initial_state,\r\n                                                               beam_width=self.beam_width,\r\n                                                               output_layer=output_layer,)\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n\r\nrunning the code made an error as below,\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n\r\n/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in __init__(self, cell, embedding, start_tokens, end_token, initial_state, beam_width, output_layer, length_penalty_weight)\r\n    173     self._initial_cell_state = nest.map_structure(\r\n    174         self._maybe_split_batch_beams,\r\n--> 175         initial_state, self._cell.state_size)\r\n    176     self._start_tokens = array_ops.tile(\r\n    177         array_ops.expand_dims(self._start_tokens, 1), [1, self._beam_width])\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/nest.pyc in map_structure(func, *structure, **check_types_dict)\r\n    323 \r\n    324   return pack_sequence_as(\r\n--> 325       structure[0], [func(*x) for x in entries])\r\n    326 \r\n    327 \r\n\r\n/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in _maybe_split_batch_beams(self, t, s)\r\n    363       print t\r\n    364       print s\r\n--> 365       return self._split_batch_beams(t, s)\r\n    366     else:\r\n    367       return t\r\n\r\n/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in _split_batch_beams(self, t, s)\r\n    314     print s\r\n    315     if isinstance(s, ops.Tensor):\r\n--> 316       s = tensor_util.constant_value_as_shape(s)\r\n    317     else:\r\n    318       s = tensor_shape.TensorShape(s)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc in constant_value_as_shape(tensor)\r\n    732     A `TensorShape` based on the constant value of the given `tensor`.\r\n    733   \"\"\"\r\n--> 734   shape = tensor.get_shape().with_rank(1)\r\n    735   if tensor.get_shape() == [0]:\r\n    736     return tensor_shape.scalar()\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in with_rank(self, rank)\r\n    630       return self.merge_with(unknown_shape(ndims=rank))\r\n    631     except ValueError:\r\n--> 632       raise ValueError(\"Shape %s must have rank %d\" % (self, rank))\r\n    633 \r\n    634   def with_rank_at_least(self, rank):\r\n\r\nValueError: Shape () must have rank 1\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n\r\nI added a log to track the tensor argument (t) and the shape argument (s) of the _split_batch_beams function\r\nIn the log, 1024 is the size of hidden units. batch_size is 80 and beam_width is 12. For each cell, I used tf.contrib.rnn.LSTMCell()\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n\r\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state:0\", shape=(?, 1024), dtype=float32) (t)\r\n1024 (s) \r\n1024 (s)\r\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state_1:0\", shape=(?, 1024), dtype=float32)\r\n1024\r\n1024\r\nTensor(\"decoder/AttentionWrapperZeroState/zeros_1:0\", shape=(960, 1024), dtype=float32)\r\n1024\r\n1024\r\nTensor(\"decoder/AttentionWrapperZeroState/zeros_2:0\", shape=(960, ?), dtype=float32)\r\nTensor(\"decoder/LuongAttention/strided_slice_2:0\", shape=(), dtype=int32)\r\nTensor(\"decoder/LuongAttention/strided_slice_2:0\", shape=(), dtype=int32)\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n\r\nIt turned out that the reason of the error was 'None' argument in the second dimension of self.encoder_inputs placeholder (which denotes max-time-steps).\r\nI implemented self.encoder_inputs placeholder to have (None, None) shape to support dynamic batch_size and time-steps of input feeds.\r\n\r\nWhen i hardcoded max-time-steps to be 80, the code worked well.\r\n(self.encoder_inputs = tf.placeholder(tf.int32, shape=(None, \"\"80\"\")), \r\n\r\nThe input argument of _split_batch_beams was like this\r\n\r\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state:0\", shape=(?, 1024), dtype=float32)\r\n1024\r\n1024\r\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state_1:0\", shape=(?, 1024), dtype=float32)\r\n1024\r\n1024\r\nTensor(\"decoder/AttentionWrapperZeroState/zeros_1:0\", shape=(960, 1024), dtype=float32)\r\n1024\r\n1024\r\nTensor(\"decoder/AttentionWrapperZeroState/zeros_2:0\", shape=(960, \"\"80\"\"), dtype=float32)\r\n\"\"80\"\"\r\n\"\"80\"\"\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\nAlso the error did not occur if i used normal decoder cell (not with AttentionWrapper)\r\nI'm wondering if this is a bug or a strict design rule for BeamSearchDecoder to support only AttentionWrapper with static_time_steps ", "comments": ["The issue was resolved by following ebrevdo@96698f7fdc0e9fa82413691ca7064db2723d2d56"]}, {"number": 10432, "title": "No `data_format` option for slim.separable_convolution2d ?", "body": "When I use slim.separable_convolution2d, according to the [CODE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1868)\r\n`data_format` is not an argument.\r\nInside the separable_convolution2d function, if num_output is not None, `data_format` is always 'channel_last' when calling convolutional_layers.SeparableConvolution2D (Alias SeparableConvolution2D = SeparableConv2D)\r\nBut according to the SeparableConv2D [CODE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/convolutional.py#L765), I think `data_format` is supported.", "comments": ["@sguada @michaelisard \r\nHas anyone looked into this issue?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@sleepfin I think the issue has been addressed in #12273 and could be closed."]}, {"number": 10431, "title": "Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat ", "body": "bug fix for #1853", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "Mind handling the CLA before we take a look?", "@jhseu hi Jonathan, so there's a mismatch between the email I used to sign CLA and the one associated with the commit, now my git author email is configured properly, could someone manually verify it or do i need to submit a new PR? thanks", "It'd be simpler to submit a new PR. Closing this for now. Thanks!"]}, {"number": 10430, "title": "group generated functions for registered ops into separate files (configurable)", "body": "wrappers.go is currently ~15k LOC. The lack of organization - code generated from `TF_GetAllOpList` is stored in a single file - makes it difficult to grok which functions exist and can work together. Also, there doesn\u2019t appear to be any tests for the funcs in wrappers.go. \r\n\r\nThis PR is a proof-of-concept that maps groups of functions to smaller wrapper files. \u2028Funcs are mapped using the  `wrap_opslist.json` config file. Here's an example of the mapping:\r\n\r\n```\r\n{\r\n\t\"wrappings\": [{\r\n\t\t\"func-grouping\": \"conv3D\",\r\n\t\t\"file-name\": \"conv3dop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"conv2D\",\r\n\t\t\"file-name\": \"conv2dop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"fft\",\r\n\t\t\"file-name\": \"fftop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"file\",\r\n\t\t\"file-name\": \"fileop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"iter\",\r\n\t\t\"file-name\": \"iterop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"matrix\",\r\n\t\t\"file-name\": \"matrixop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"loss\",\r\n\t\t\"file-name\": \"lossop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"quant\",\r\n\t\t\"file-name\": \"quantop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"random\",\r\n\t\t\"file-name\": \"randomop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"relu\",\r\n\t\t\"file-name\": \"reluop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"save\",\r\n\t\t\"file-name\": \"saveop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"slice\",\r\n\t\t\"file-name\": \"sliceop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"shape\",\r\n\t\t\"file-name\": \"shapeop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"softmax\",\r\n\t\t\"file-name\": \"softmaxop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"space\",\r\n\t\t\"file-name\": \"spaceop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"sparse\",\r\n\t\t\"file-name\": \"sparseop.go\"\r\n\t}, {\r\n\t\t\"func-grouping\": \"tensor\",\r\n\t\t\"file-name\": \"tensorop.go\"\r\n\t}]\r\n} \r\n```\r\n\r\nThe result of the work in this PR - files are organized into individual files under the op pkg.\r\nThere is still a large amount of source in wrappers.go - but more mappings can be done.. \r\n\r\nI\u2019m certainly willing to change how functions are grouped and make this more configurable.\u2028\u2028 (e.g. add a func level mapping)\r\n\r\nI wanted to see what you thoughts are @asimshankar  \r\n\r\nDoes this PR moving things in the right direction?\u2028\r\nIf so, do you have any suggestions for how-to organize functions - or - know someone with whom we should talk to about organizing TF wrapper funcs?\r\n\r\n\u2028If not, could you kindly provide direction as-to how to make the go functions that wrap the tensorflow operations to be more approachable? ", "comments": ["Can one of the admins verify this patch?", "Thanks very much for thinking about this @ctava .\r\n\r\nHowever, for now, I'd suggest holding off on this. We've been toying with a common-specification of package organization of generated op wrappers so that generated code in all languages is similar. For example, `tf.image.decode_jpeg` in Python and `image.DecodeJpeg` in Go etc.\r\n\r\nWe're still discussing the details a bit but should hopefully have some notes/code to share within a few weeks. At which time, we'd like to change the Go code generator to use the same configuration file as the code generator for Python and C++ (and others).\r\n\r\n(FYI @aselle @josh11b )", "@asimshankar ok cool. glad to hear there is a plan. happy to help in any way. design, unit testing etc.  please let me know how i can help and timing on next steps. thanks!"]}, {"number": 10429, "title": "Separate tensorboard from tensorflow (core)", "body": "I suggest to separate tensorboard from tensorflow.\r\n\r\nThis could mean:\r\n* break tensorboard to its own repos (under the same organization)\r\n* publish a separate tensorboard wheel / package on pypi. \r\n\r\nThe advantages of this are:\r\n* simpler building of tensorboard which will make it easier for non core tensorflow members to contribute to the development of the project.\r\n* tensorboard to operate on its own release cycle\r\n* simpler installation of tensorboard (it can be installed on its own without needing full install of tensorflow)", "comments": ["I agree. We're actually currently working on exactly that. Assigning to @dandelionmane.\r\n\r\nHowever TensorBoard will continue to be installed when you install the TensorFlow pip package. So this change will mostly impact developers, and should have zero user facing impact.", "You could still create a second wheel / pypi package for tensorboard and have the tensorflow package install / depend on that.", "This is now complete. Please check out the new repo here: [https://github.com/tensorflow/tensorboard](https://github.com/tensorflow/tensorboard). You can also pip install latest tensorboard by running `pip install tensorflow-tensorboard`. (We're working on getting `pip install tensorboard` to work too.)\r\n\r\nAlso, @cancan101, I know we've been a bit unresponsive on your TensorBoard issues and pull requests. Now that we're in our own repo, we're committing to be more responsive to GitHub issues and PRS. :) ", "Great job!\r\n\r\nJust a question -- is the (legacy) [tensorboard module](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tensorboard) in the original tensorflow repository going to removed, or will it be kept?", "@dandelionmane \r\n\r\n> (We're working on getting pip install tensorboard to work too.)\r\n\r\nIt seems tensorboard is used by https://github.com/dmlc/tensorboard.\r\nIs there a plan to provide a tf independent package which can be used by other ML frameworks?", "After upgrading TF to 1.3rc2, I have this error when starting tensorboard\r\n```\r\nNo such file or directory: '/usr/local/lib/python3.4/dist-packages/tensorflow/tensorboard/TAG'\r\n```\r\nI reported the issue at [Tenorboard issues](https://github.com/tensorflow/tensorboard/issues/352)", "Is the plan to also move tf.summary to tensorboard? Currently tensorboard still requires tensorflow to be importable, and if you want to generate a file yourself I assume you also need to use tf.summary? Not a big deal but it seems like tf.summary  and tensorboard are coupled"]}, {"number": 10428, "title": "TensorBoard graph key does not match documentation", "body": "\r\nThe key in the TensorBoard UI indicates a \"Reference edge\" as a single-headed arrow:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nwhile the documentation shows these as double-headed arrows:\r\n\r\n[![enter image description here][2]][2]\r\n\r\nMoreover, it appears that the edges indicated as references edges in the UI (according to the key there) are not in fact such edges. For example neither \r\n\r\n    cs = tf.constant([1,2,3], name='const_share')\r\n    vs = tf.Variable([1,2,3], name='var_share')\r\n    tf.add(cs, vs, name='opVS1')\r\n    tf.add(vs, cs, name='opVS2')\r\n\r\n<img src=\"https://i.stack.imgur.com/8z1Bn.png\" height=\"350\">\r\n\r\nnote\r\n\r\n    tf.add([4],[3], name='opA')\r\n\r\n[![enter image description here][3]][3]\r\n\r\nshould include reference edges (should they?). But in both cases the key in the UI says that they do.\r\n\r\n  [1]: https://i.stack.imgur.com/5MhdF.png\r\n  [2]: https://i.stack.imgur.com/fWRZL.png\r\n  [3]: https://i.stack.imgur.com/eZHmm.png", "comments": ["[Related SO question](https://stackoverflow.com/q/44345863/656912).", "By convention, dataflow edges are directed upwards, so that is why the \"dataflow edges\" lack arrowheads - we assume the reader understands the convention.\r\n\r\nIf we must make a dataflow edge that points downwards (or close to downwards like in your second picture) - and sometimes that is necessary, we add an arrowhead to make that clear.\r\n\r\nSeveral people (including some folks on deep mind) have told me that this is unclear. For starters, perhaps we can update the documentation to clarify the convention as well as why only some dataflow edges have arrowheads (They just flow downwards).", "@chihuahua Indeed, that wasn't at all clear (I'm glad to have company at DeepMind). \r\n\r\nIMV it would be a lot clearer if direction of flow was always indicated, and there was something distinct for \"reference edges\" (whatever those are).\r\n\r\nAnd of course: the documentation should agree with the docs!", "Assigning to @chihuahua as a documentation issue.", "FYI, I am moving forward with an internal code change that \r\n\r\n1. Makes reference edges orange (and dataflow edges the same grey as before). The final colors might differ based on input from a designer, but the bottom line is that dataflow and reference edges will contrast in color.\r\n2. Adds arrows to all dataflow edges no matter where they point.\r\n\r\nI will subsequently update tensorflow.org docs to match.\r\n\r\nHere is a preview. The edges from the \"save\" node are reference edges because \"assign ops\" can modify other tensor values.\r\n\r\n![qhfhkq7cwb9](https://cloud.githubusercontent.com/assets/4221553/26816188/4324c082-4a45-11e7-9e6e-2b21b61bd5ca.png)\r\n", "I have migrated this issue to tensorflow/tensorboard#48.\r\n\r\nThe issue is almost fixed (It is already in the code.). The documentation code just has to be updated, so [this tensorflow.org page](https://www.tensorflow.org/get_started/graph_viz) changes."]}, {"number": 10427, "title": "Implemented sinh and cosh", "body": "These commits contains the implementation of the sinh and cosh functions for CPU and GPU with gradients. This solves #7531 partially. The supported datatypes are float, double, complex64 and complex128.\r\n\r\nCompilation went through successfully on my system (CPU only) and the following test script seems to be running fine on Python 2.7\r\n\r\n```python\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.placeholder(shape=[None, 5], dtype=tf.float32)\r\ny = tf.placeholder(shape=[None, 5], dtype=tf.float32)\r\n\r\nW = tf.Variable(tf.random_uniform(shape=[5,5], dtype=tf.float32))\r\nw = tf.cosh(W)\r\nerror = tf.reduce_mean((tf.matmul(x, w) - y)**2)\r\ntrain = tf.train.GradientDescentOptimizer(1e-2).minimize(error)\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(tf.global_variables_initializer())\r\nfor i in range(100):\r\n    tmp_x = np.random.rand(10, 5)\r\n    tmp_y = np.random.rand(10, 5)\r\n\r\n    sess.run(train, feed_dict={x:tmp_x, y:tmp_y})\r\nprint(sess.run(W))\r\n```", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@lakshayg It looks like there's a bug in the C++ complex gradient for sinh: [ RUN      ] CWiseUnaryGradTest.Sinh_Complex\r\n2017-06-06 20:16:34.833848: E tensorflow/core/framework/tensor_testutil.cc:34] x = Tensor<type: complex64 shape: [2,3,2] values: ? ? ?...>\r\n2017-06-06 20:16:34.833874: E tensorflow/core/framework/tensor_testutil.cc:35] y = Tensor<type: complex64 shape: [2,3,2] values: ? ? ?...>\r\n2017-06-06 20:16:34.833936: E tensorflow/core/framework/tensor_testutil.cc:36] atol = 1e-06 rtol = 1e-06 tol = 9.1994e-06\r\ntensorflow/core/framework/tensor_testutil.cc:38: Failure\r\nValue of: false\r\n  Actual: false\r\nExpected: true\r\n0-th element is not close (-8.1365184783935547,1.0135481357574463) vs. (2.779362678527832,-0.21568894386291504)\r\n2017-06-06 20:16:34.834010: E tensorflow/core/framework/tensor_testutil.cc:34] x = Tensor<type: complex64 shape: [2,3,2] values: ? ? ?...>\r\n2017-06-06 20:16:34.834015: E tensorflow/core/framework/tensor_testutil.cc:35] y = Tensor<type: complex64 shape: [2,3,2] values: ? ? ?...>\r\n2017-06-06 20:16:34.834021: E tensorflow/core/framework/tensor_testutil.cc:36] atol = 1e-06 rtol = 1e-06 tol = 4.45043e-06\r\ntensorflow/core/framework/tensor_testutil.cc:38: Failure\r\nValue of: false\r\n  Actual: false\r\nExpected: true\r\n1-th element is not close (-1.5430805683135986,3.0861611366271973) vs. (-0.54030227661132812,1.0806045532226562)\r\n", "@rmlarsen I am also working on implementing the inverse hyperbolic functions and having some issues in understanding the code. It would be great if you could help me by answering the question at https://stackoverflow.com/questions/44389994/making-sense-of-gradforunarycwise-in-tensorflow", "@tensorflow-jenkins test this please", "I thinks this is somewhat strange. Of the 4 tests which failed, 3 ran successfully (Windows Cmake Tests, Linux GPU, Linux CPU Tests) before the last revision. Since the last revision involved correcting one of the test functions (changing `std::cos(x)` to `std::cosh(x)`), it surprises me that tests in unrelated files are failing.\r\n\r\nI ran the following commands on my machine and the tests were successful (these failed on jenkins)\r\n\r\n1. bazel test //tensorflow/python/kernel_tests:check_ops_test \r\n2. bazel test //tensorflow/python/kernel_tests:embedding_ops_test\r\n3. bazel test //tensorflow/contrib/seq2seq:beam_search_decoder_test\r\n4. bazel test //tensorflow/python/kernel_tests:sparse_ops_test\r\n5. bazel test //tensorflow/contrib/seq2seq:basic_decoder_test\r\n6. bazel test //tensorflow/contrib/layers:embedding_ops_test\r\n\r\nAny hints on how to proceed?", "@tensorflow-jenkins test this please", "@lakshayg I think the test failures are unrelated to your change."]}, {"number": 10426, "title": "Fixed typo in code", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 10425, "title": "Feature request -forwardsolve and backsolve", "body": "I would like to use OLS to estimate parameters in some parts of my models. We're most of the way there today: QR decomposition is already implemented; we just need a backsolve command. And while we're at it, we might as well add a forwardsolve command. For example, one solution would just be calls to SGETRS\tCGETRS\tDGETRS\tZGETRS in LAPACK and their transpose equivalents.", "comments": ["https://www.tensorflow.org/api_docs/python/tf/matrix_triangular_solve implements this."]}, {"number": 10424, "title": "Unable to train restore or save when running lamp server", "body": "Running apache mysql python server on port 80, I am unable to save my train data or restore. ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10423, "title": "Fix unaligned args in api_docs/python/tf/contrib/learn/Evaluable", "body": "This commit fixes unaligned args in api_docs/python/tf/contrib/learn/Evaluable\r\nby removing the extra line in arg `metrics:`.\r\n\r\nThis fix fixes #9313.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 10422, "title": "Ability to add to an existing named scope", "body": "Currently it is impossible to use `tf.name_scope` to add additional elements to an existing scope. Using the same name a second time creates a new scope (with an appended index). This means that any elements that need to be in the same scope must be defined close together in source, which is not always consistent with the logic of the code.\r\n\r\nA desirable feature would be the ability to add to an existing named scope, either with a new named parameter for `tf.name_scope` (e.g. Boolean `add_to_existing`), or with a new related function (e.g., `tf.existing_name_scope`).", "comments": ["Vote for it. \r\nDiscussed (but closed) in #6007.", "Actually [this](https://github.com/tensorflow/tensorflow/issues/6007#issuecomment-264322793) is a satisfactory solution (I don't know why it didn't occur to me!).", "Closing for now since it seems there's a workable solution. Please reopen if there's still an issue."]}, {"number": 10421, "title": "Specify which scopes are removed from TensorBoard main graph by default", "body": "The heuristics used by TensorBoard to determine which scopes are added to the \"main graph\" and which are not by default often results in scopes being included that obscure the underlying network structure.\r\n\r\nIt would be nice to have an additional named parameter for `tf.name_scope` that indicated whether that scope should be removed (or included) in the main graph by default.", "comments": ["Migrating to tensorflow/tensorboard"]}, {"number": 10420, "title": "I think your linux python3.5 1.2rc whl is actually installing TF 1.1.0, the Python 3.6 one works", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl\r\n- **TensorFlow version (use command below)**:\r\nwas trying to install 1.2rc but got 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n8.0 \r\n- **GPU model and memory**:\r\nTitan X\r\n- **Exact command to reproduce**:\r\n\r\nTHIS CREATES THE PROBLEM:\r\n```conda create --name tfpy35 python=3.5\r\nsource activate tfpy35\r\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl\r\npython\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n1.1.0\r\nIT SHOULD BE 1.2.0-RC1\r\n```\r\n\r\nTHIS WORKS:\r\n```\r\nconda create --name tensorflow python=3\r\nsource activate tensorflow\r\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp36-cp36m-linux_x86_64.whl\r\npython\r\n>>> import tensorflow as tf; print(tf.__version__)\r\n1.2.0-rc1\r\n```\r\n\r\n\r\n### Describe the problem\r\nI am using anaconda and created a py3.5 env\r\nI installed TF GPU as per https://www.tensorflow.org/versions/r1.2/install/install_linux#the_url_of_the_tensorflow_python_package\r\nfor python 3.5\r\nBut when I run python then import tensorflow as tf; print(tf.__version__)\r\nI get 1.1.0 \r\n\r\nIf I do exactly the same thing but create a python 3 enviornment and then load the 3.6 gpu whl it works\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n```\r\npaul@SPEED:~$ conda create --name tfpy35 python=3.5\r\nFetching package metadata ...........\r\nSolving package specifications: .\r\n\r\nPackage plan for installation in environment /home/paul/anaconda3/envs/tfpy35:\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    openssl:    1.0.2l-0     \r\n    pip:        9.0.1-py35_1 \r\n    python:     3.5.3-1      \r\n    readline:   6.2-2        \r\n    setuptools: 27.2.0-py35_0\r\n    sqlite:     3.13.0-0     \r\n    tk:         8.5.18-0     \r\n    wheel:      0.29.0-py35_0\r\n    xz:         5.2.2-1      \r\n    zlib:       1.2.8-3      \r\n\r\nProceed ([y]/n)? y\r\n\r\npython-3.5.3-1 100% |################################| Time: 0:00:03   4.96 MB/s\r\nsetuptools-27. 100% |################################| Time: 0:00:00   5.59 MB/s\r\nwheel-0.29.0-p 100% |################################| Time: 0:00:00   7.59 MB/s\r\n#\r\n# To activate this environment, use:\r\n# > source activate tfpy35\r\n#\r\n# To deactivate this environment, use:\r\n# > source deactivate tfpy35\r\n\r\npaul@SPEED:~$ source activate tfpy35\r\n\r\n(tfpy35) paul@SPEED:~$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl\r\nCollecting tensorflow-gpu==1.2.0rc1 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl\r\n  Using cached https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl\r\nCollecting markdown==2.2.0 (from tensorflow-gpu==1.2.0rc1)\r\nCollecting wheel>=0.26 (from tensorflow-gpu==1.2.0rc1)\r\n  Using cached wheel-0.29.0-py2.py3-none-any.whl\r\nCollecting numpy>=1.11.0 (from tensorflow-gpu==1.2.0rc1)\r\n  Using cached numpy-1.12.1-cp35-cp35m-manylinux1_x86_64.whl\r\nCollecting protobuf>=3.2.0 (from tensorflow-gpu==1.2.0rc1)\r\n  Using cached protobuf-3.3.0-cp35-cp35m-manylinux1_x86_64.whl\r\nCollecting werkzeug>=0.11.10 (from tensorflow-gpu==1.2.0rc1)\r\n  Using cached Werkzeug-0.12.2-py2.py3-none-any.whl\r\nCollecting bleach==1.5.0 (from tensorflow-gpu==1.2.0rc1)\r\n  Using cached bleach-1.5.0-py2.py3-none-any.whl\r\nCollecting html5lib==0.9999999 (from tensorflow-gpu==1.2.0rc1)\r\nCollecting six>=1.10.0 (from tensorflow-gpu==1.2.0rc1)\r\n  Using cached six-1.10.0-py2.py3-none-any.whl\r\nCollecting setuptools (from protobuf>=3.2.0->tensorflow-gpu==1.2.0rc1)\r\n  Using cached setuptools-36.0.1-py2.py3-none-any.whl\r\nInstalling collected packages: markdown, wheel, numpy, six, setuptools, protobuf, werkzeug, html5lib, bleach, tensorflow-gpu\r\nSuccessfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.2.0 numpy-1.12.1 protobuf-3.3.0 setuptools-36.0.1 six-1.10.0 tensorflow-gpu-1.2.0rc1 werkzeug-0.12.2 wheel-0.29.0\r\n\r\n(tfpy35) paul@SPEED:~$ python\r\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n1.1.0\r\n```\r\n\r\n\r\n\r\n\r\n", "comments": ["@av8ramit can you take a look please?", "I checked this on a clean nvidia docker image and I cannot reproduce the problem. Here are the commands I ran:\r\n```\r\n$ nvidia-docker run -it nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 bash\r\n# apt-get update\r\n# apt-get install python3-pip\r\n# pip3  install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc1-cp35-cp35m-linux_x86_64.whl\r\n# python3\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.2.0-rc1'\r\n```\r\n\r\nThere are bugs around anaconda where sometimes it cannot install a new version even when `--upgrade` flag is provided. It is possible you are running into one of these.\r\nYou should be able to check again on a clean conda environment.\r\n\r\nAs I cannot reproduce this, I will close this bug as not reproducible."]}, {"number": 10419, "title": "[Feature Request] A guide for creating custom DeepDream models", "body": "The current DeepDream guide located [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/deepdream), uses the inception5h model. From what I can tell, it does not appear very straightforward in terms of how to fine tune the model in order to create different DeepDream hallucinations from a custom data set. It also does not appear to be relatively easy to change the model that the guide uses. \r\n\r\nI think that an additional guide which shows individuals how to fine tune a model for the purposes of DeepDream would be useful for those trying to explore the artistic and visual aspects of TensorFlow models. I haven't been able to find any guide for creating custom DeepDream models in Tensorflow, so I am not sure where to start.\r\n\r\n\r\n", "comments": ["Thank you for the feedback. Generally speaking, tutorials are meant to cover a breadth of topics. Our goal is to show users what's possible and help them get the ball rolling.\r\n\r\nSince the DeepDream notebook is very popular, it might be worth going more in depth. I'm not sure if it's something we have time to work on. But I think it's worth asking @znah what he thinks.", "@jart At the moment, I've found it extremely confusing as to how to use other pre-trained models with the tutorial's code. I've tried using the model from: http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz, but I couldn't figure out how to get it working with the code. The stripped Android model (inception5h) seems to be really different from other models, and I haven't yet figured out how to use a different model with the DeepDream tutorial code, let alone a model that I finetuned myself. \r\n\r\nEdit:\r\n\r\nThis might be somewhat related: https://github.com/tensorflow/tensorflow/issues/4907 ?\r\n\r\nTrying to use the inception-2015-12-05 model with the DeepDream tutorial code results in:\r\n\r\n`ValueError: Attempted to map inputs that were not found in graph_def: [input:0]`\r\n\r\n", "Same issue as @ProGamerGov , the input tensors are not recognised for either of inception v3 or any subsequent models (model zoo, tensorflow).", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 10418, "title": "Expose canned estimators in core", "body": "I know this is still WIP but it would be good to expose this for adventurous external users. ", "comments": ["We're not quite ready for that, and we do not want to muddle the API compatibility story. \r\n\r\nCan you import directly, like\r\n\r\nfrom tensorflow.python.estimators.canned import ...\r\n\r\n?", "Apologies. I tried that earlier but didn't work. Turned out that I was using a different python lib. I am closing this. Totally agree with the importance of compatibility history."]}, {"number": 10417, "title": "configure: Fix default path when enabling MPI.", "body": "A minor fixup. Correct showing what the default path is when mpi is installed.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Thanks @caisq "]}, {"number": 10416, "title": "Printing TensorFlow Library Logs in Android Studio", "body": "I have built Tensor Flow using Bazel  and am using the generated TensorFlow Libraries in Android Studio\r\nfor ndk.\r\n\r\nBut I do not find the logs of Tensor Flow being printed in Android Studio.\r\nCan I know how do I acheive this .\r\n\r\nI have tried using Android/log.h in TensorFlow code but of no use\r\n", "comments": ["There should be a \"logcat\" tab on the \"Android Monitor\" view that displays this. TensorFlow prints messages in the standard Android fashion so it's no different than any other app.\r\n\r\nAny followup logging questions are better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since this is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thanks, but the logs of tensorflow are not being printed in Logcat of\nAndroid ndk.\n\nOn Jun 5, 2017 00:30, \"Andrew Harp\" <notifications@github.com> wrote:\n\n> Closed #10416 <https://github.com/tensorflow/tensorflow/issues/10416>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10416#event-1109207473>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJjYUHlVqRMrNLqghNYUGB-M5CcW84o1ks5sAv7cgaJpZM4NvGBX>\n> .\n>\n"]}, {"number": 10415, "title": "ValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: ", "body": "'generator/rnn_cell/rnn/basic_rnn_cell'; and the cell was not constructed as BasicRNNCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 10414, "title": "Solution to non context words due to wrap around", "body": "Due to wrap around non-context words also appear in the window for skip gram which degrades the quality for word vectors.\r\nSimple patch to avoid appearance of non-context words ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I have signed it\n\nOn Saturday, June 3, 2017, googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If you signed the CLA as a corporation, please let us know the\n>    company's name.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10414#issuecomment-305964758>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ATsP8QulWVwGFxnzV1KKgcNSR338oPEyks5sASzCgaJpZM4NvCgq>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Geoffrey, mind taking a look?", "@Humanity123 Can you explain what this does?  I don't follow the code.", "Since the original code didn't pay attention to wrap around. Non Context Words used to creep in. For Example if there are 100 words from 0 - 99 and the span of the window is 3 then there will be a case when the buffer contains 99 , 0 , 1\r\nSo to prevent this there is a simple check, if the buffer is not empty that is this is not the first word to be filled in buffer and  the next word to be filled is zero (0th index word) which means that there is a wrap around. We flush the items in the buffer and start from the beginning of the document that is we refill the buffer with the words from the starting of the document. This thus prevents  non context words to ever occur together.", "Recommended Changes have been made.\r\n", "Jenkins, test this please.", "Windows failure looks unrelated.  @jhseu: Go for merging.", "@Humanity123 Thank you for the contribution!"]}, {"number": 10413, "title": "Random crop a patch from a various-sized image read in TFRecord", "body": "I have posted my question on stackoverflow but no ones answered me. [here](https://stackoverflow.com/questions/44152661/random-crop-a-patch-from-a-various-sized-image-read-in-tfrecord)\r\nI found another similar question posted more than one year ago on stackoverflow as well, [How to read images with different size in a TFRecord file.\r\n](https://stackoverflow.com/questions/35028173/how-to-read-images-with-different-size-in-a-tfrecord-file)\r\nI tried the method someone suggested in the page, image = tf.reshape(image_data, tf.pack([image_rows, image_cols, 3])), but it still fails.\r\n\r\nAnd I read the comments given by  Yaroslav Bulatov, performing randomly crop and resize them to a fixed size before converting them to TFRecord. It should be available, but it may increase the size of TFRecord file for saving a lot of cropped images. \r\nBut what I'm curious about is that if the image size can't be utilized during training, why the [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py) provided by the tutorial document of Tensorflow would write the width, height and depth of the image in TFRecord? That really confuses me. So is it possible to read images with different size in a TFRecord file? If it's possible, how to do it? Thanks", "comments": ["`tf.pack` existed in versions of TensorFlow prior to 1.0.\r\nThat has been replaced by [`tf.stack`](https://www.tensorflow.org/api_docs/python/tf/stack) since then.\r\n\r\nClosing this out as we aim to keep Github issues focused on bugs and feature requests. "]}, {"number": 10412, "title": "Added store intermediate graph feature", "body": "", "comments": ["Can one of the admins verify this patch?", "@petewarden , done", "Jenkins, test this please."]}, {"number": 10411, "title": "Branch 157903115", "body": "Pushing internal commits.", "comments": ["@tensorflow-jenkins Test this, please.", "@tensorflow-jenkins Test this, please.", "@gunan @yifeif Getting timeouts now, but other issues seem resolved.", "@tensorflow-jenkins test this please."]}, {"number": 10410, "title": "Could not create Tensorflow Graph: Invalid argument: No OpKernel was registered to support Op 'Const' with these attrs", "body": "I am try to create to Graph by running below command\r\ntensorflow::Status s = session->Create(tensorflow_graph);\r\nBut I get below error \r\n\r\nCould not create Tensorflow Graph: Invalid argument: No OpKernel was registered to support Op 'Const' with these attrs.  Registered devices: [CPU], Registered kernels: \\<no registered kernels\\>\r\n\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "Thanks\r\nBut I figured out the issue..\r\nI am trying to use TensorFlow Libraries in android ndk.\r\nIf I place libraries of both 'arm64-v8a' and 'armeabiv-7a' libraries in my project , it works fine. If I remove either of these the above error is thrown.", "I have built Tensoflow using bazel. \r\nI have taken the libraries from tensorflow and used in android ndk, by stetting abi as either of  'arm64-v8a' or 'armeabiv-7a' and am executing \r\ntensorflow::Status s = session->Create(tensorflow_graph); in android ndk from \r\nhttps://github.com/miyosuda/TensorFlowAndroidMNIST/blob/master/jni-build/jni/tensorflow_jni.cc\r\n\r\nMore specifically from below attached file\r\n\r\n[tensorflow_jni.txt](https://github.com/tensorflow/tensorflow/files/1049525/tensorflow_jni.txt)\r\n\r\nBelow are the debug messages\r\n\r\n06-03 15:51:03.191 25365-26625/ch.zhaw.facerecognition I/native: tensorflow_jni.cc:113 Loading Tensorflow.\r\n06-03 15:51:03.191 25365-26625/ch.zhaw.facerecognition I/native: tensorflow_jni.cc:115 Making new SessionOptions.\r\n06-03 15:51:03.191 25365-26625/ch.zhaw.facerecognition I/native: tensorflow_jni.cc:118 Got config, 0 devices\r\n06-03 15:51:03.193 25365-26625/ch.zhaw.facerecognition I/native: tensorflow_jni.cc:121 Session created.\r\n06-03 15:51:03.193 25365-26625/ch.zhaw.facerecognition I/native: tensorflow_jni.cc:124 Graph created.\r\n06-03 15:51:03.193 25365-26625/ch.zhaw.facerecognition I/native: tensorflow_jni.cc:128 Acquired AssetManager.\r\n06-03 15:51:03.193 25365-26625/ch.zhaw.facerecognition I/native: tensorflow_jni.cc:130 Reading file to proto: /storage/emulated/0/Pictures/facerecognition/data/TensorFlow/vgg_faces.pb\r\n06-03 15:51:05.045 25365-26625/ch.zhaw.facerecognition I/native: tensorflow_jni.cc:135 Creating session.\r\n06-03 15:51:05.068 25365-26625/ch.zhaw.facerecognition E/native: tensorflow_jni.cc:138 Could not create Tensorflow Graph: Invalid argument: No OpKernel was registered to support Op 'Const' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                   <no registered kernels>\r\n                                                                 \r\n                                                                 \t [[Node: fc8/biases = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [2622] values: 0 0 0...>]()]]\r\n\r\n\r\n\r\n\r\n", "OpKernels not loading when with Android is a common issue. It might have something to do with what's in this talk: https://youtu.be/0r9w3V923rk?t=19m39s. There's also a bunch of [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) issues. Let us know if you're certain there's a bug in TensorFlow."]}, {"number": 10409, "title": "Fix TensorBoard SHA256 in cmake", "body": "", "comments": []}, {"number": 10408, "title": "Memory leak ", "body": "I have a memory leak with TensorFlow. I refered to https://stackoverflow.com/questions/35695183/tensorflow-memory-leak-even-while-closing-session to address my issue, and I followed the advices of the answer, that seemed to have solved the problem. However it does not work here. \r\n\r\nIn order to recreate the memory leak, I have created a simple example. First, I use this function (that I got here : https://stackoverflow.com/questions/276052/how-to-get-current-cpu-and-ram-usage-in-python) to check the memory use of the python process : \r\n\r\n    def memory():\r\n        import os\r\n        import psutil\r\n        pid = os.getpid()\r\n        py = psutil.Process(pid)\r\n        memoryUse = py.memory_info()[0]/2.**30  # memory use in GB...I think\r\n        print('memory use:', memoryUse)\r\n\r\nThen, everytime I call the `build_model` function, the use of memory increases.\r\n\r\nHere is the `build_model` function that has a memory leak : \r\n \r\n    def build_model():\r\n\r\n        '''Model'''\r\n\r\n        tf.reset_default_graph()\r\n\r\n\r\n        with tf.Graph().as_default(), tf.Session() as sess:\r\n            tf.contrib.keras.backend.set_session(sess)\r\n\r\n            labels = tf.placeholder(tf.float32, shape=(None, 1))\r\n            input = tf.placeholder(tf.float32, shape=(None, 1))\r\n\r\n            x = tf.contrib.keras.layers.Dense(30, activation='relu', name='dense1')(input)\r\n            x1 = tf.contrib.keras.layers.Dropout(0.5)(x)\r\n            x2 = tf.contrib.keras.layers.Dense(30, activation='relu', name='dense2')(x1)\r\n            y = tf.contrib.keras.layers.Dense(1, activation='sigmoid', name='dense3')(x2)\r\n\r\n\r\n            loss = tf.reduce_mean(tf.contrib.keras.losses.binary_crossentropy(labels, y))\r\n\r\n            train_step = tf.train.AdamOptimizer(0.004).minimize(loss)\r\n\r\n            #Initialize all variables\r\n            init_op = tf.global_variables_initializer()\r\n            sess.run(init_op)\r\n\r\n            sess.close()\r\n\r\n        tf.reset_default_graph()\r\n\r\n        return \r\n\r\n I would have thought that using the block ` with tf.Graph().as_default(), tf.Session() as sess: ` and then **closing the session** and **calling `tf.reset_default_graph`** would clear all the memory used by TensorFlow. Apparently it does not.\r\n\r\nThe memory leak can be recreated as following : \r\n\r\n    memory()\r\n    build_model()\r\n    memory()\r\n    build_model()\r\n    memory()\r\n\r\nThe output of this is (for my computer) :\r\n\r\n    memory use: 0.1794891357421875\r\n    memory use: 0.184417724609375\r\n    memory use: 0.18923568725585938\r\n\r\nClearly we can see that all the memory used by TensorFlow is not freed afterwards. Why?\r\n\r\nI hope I made myself clear.", "comments": ["Would you be able to plot memory usage over a thousand iterations? That will help us rule out the possibility that the memory usage is caused by modules being loaded, or garbage collector fanciness. It would also be great if you could narrow down the number of APIs being called.", "![image](https://cloud.githubusercontent.com/assets/19774802/26753716/0e980254-486d-11e7-9ac8-4f57150b7815.png)\r\n@jart Here you go. As you can see, the memory usage goes up in a linear way, which is exactly the problem.\r\n\r\nAbout the number of APIs being called, do you refer to Keras by saying that ? I only use tf.contrib.keras, which is part of tensorflow. Hence I only use tensorflow here.", "@Caselles Excellent. Thank you.\r\n\r\n@fchollet There appears to be some type of memory leak in Keras.", "Early tests seems to show that \r\n_GRAPH_LEARNING_PHASES is not cleared so there is tons of tensors that get kept.\r\n\r\nChanging to ```def reset_uids():\r\n  global _GRAPH_UID_DICTS\r\n  global _GRAPH_LEARNING_PHASES\r\n  _GRAPH_UID_DICTS = {}\r\n  _GRAPH_LEARNING_PHASES = {}```\r\n\r\nSeems to resolve the problem. \r\n\r\n> memory use: 0.13166046142578125\r\nmemory use: 0.13190841674804688\r\nmemory use: 0.13220977783203125\r\nmemory use: 0.13220977783203125\r\nmemory use: 0.13220977783203125\r\nmemory use: 0.13220977783203125\r\nmemory use: 0.13220977783203125\r\nmemory use: 0.13220977783203125\r\n\r\nWill do a PR on keras, it will then get merged into TF I guess?\r\n", "This is actually fixed in keras master branch.\r\nhttps://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L82\r\nSo waiting for the merge will fix the problem. ", "@Dref360 this is fixed in TF master: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/keras/python/keras/backend.py#L288\r\n\r\nDoes this resolve the problem?", "K.clear_session is never called. maybe K.set_session should do a clear_session?", "Before posting this issue, I tried calling K.clear_session(), but my tensorflow version was 1.1... Never lucky. By upgrading to 1.2 and calling K.clear_session(), the problem was solved. Thank you very much \ud83d\udc4d ", "Independently of `clear_session`, I would expect the use of graph scopes to prevent this behavior. What's the status of that?", "The graph is not cleared. Maybe because we still have a reference to it so the GC doesn't collect?\r\nIf you print `list(_GRAPH_LEARNING_PHASES.values())[1].graph.get_operations()` you can see that the operations are still there. ", "So the use of graph scopes will not prevent this behavior. Is this planned to be solved ? \r\n\r\n@Dref360 : Would you have a workaround to clear the operations still in the graph ? I would like **not** to clear all the operations in the graph, but only those that are created within the `with tf.Graph().as_default(), tf.Session() as sess:` block. \r\n\r\n@fchollet @jart : Can you provide a solution for this problem ? ", "The solution is to make learning phases part of a graph collection, I believe. Potentially the same may also be true of layer name UIDs, which also keeps graph references.\r\n\r\nLast time we tried this it caused a useless warning upon graph serialization which annoyed/scared users. We will also have to figure out how to remove that warning.", "Ok thank you. Would you have a workaround for now ? In order to clear only the operations within the `with tf.Graph().as_default(), tf.Session() as sess:` block.", "Add `K.clear_session()` at the end of your block (inside the block).", "Problem is, I'm trying to run two model at the same time in a website. Hence, calling `K.clear_session` in the code clears operations that are used elsewhere. So this creates a bug. I really need to be able to specify the operations that I want to clear. Could you be a little more precise on how I could do that ?", "There are only two possibilities:\n1) you no longer use the graph. In that case `clear_session` destroys it,\nand that is what you want.\n2) you are still using the graph. In that case you cannot garbage-collect\nit. And you don't have a \"memory leak\".\n\nOn 13 June 2017 at 11:01, Syzygy <notifications@github.com> wrote:\n\n> Problem is, I'm trying to run two model at the same time in a website.\n> Hence, calling K.clear_session in the code clears operations that are\n> used elsewhere. So this creates a bug. I really need to be able to specify\n> the operations that I want to clear. Could you be a little more precise on\n> how I could do that ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10408#issuecomment-308197476>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AArWb8x5Gi6WMApi-ZjfF8000k-dLIhSks5sDs5mgaJpZM4Nu1dH>\n> .\n>\n", "I'm sorry for not saying that earlier but calling `K.clear_session()` within the `tf.Graph().as_default(), tf.Session() as sess:` yields an error : \r\n\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-7-7793873c5c17> in <module>()\r\n----> 1 build_model()\r\n\r\n<ipython-input-6-3626073d2002> in build_model()\r\n     31 \r\n     32         K.set_session(sess)\r\n---> 33         K.clear_session()\r\n     34 \r\n     35     tf.reset_default_graph()\r\n\r\n/Users/Syzygy/anaconda/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)\r\n     75                 value = type()\r\n     76             try:\r\n---> 77                 self.gen.throw(type, value, traceback)\r\n     78                 raise RuntimeError(\"generator didn't stop after throw()\")\r\n     79             except StopIteration as exc:\r\n\r\n/Users/Syzygy/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py in get_controller(self, default)\r\n   3626     finally:\r\n   3627       if self._enforce_nesting:\r\n-> 3628         if self.stack[-1] is not default:\r\n   3629           raise AssertionError(\r\n   3630               \"Nesting violated for default stack of %s objects\"\r\n\r\nIndexError: list index out of range\r\n", "has there been any progress on this issue? I have the exact same problem, and exact same error while calling clear_session. I'm using the latest keras and tensorflow version.", "I have a very similar issue causing memory leak, but I'm only using tensorflow without keras. Here's the minimal code:\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfor i in range(30):\r\n    tf.Session().__enter__()\r\n    tf.constant(np.random.random((800,500,500,1)))\r\n    tf.get_default_session().close()\r\n    tf.reset_default_graph()\r\n\r\n\r\nWhen executing the loop the memory used keeps going up. How can I actually delete the old large constants and free the memory?\r\nI'm using tensorflow 1.2 with python 3.4 on ubuntu 14.04", "@fchollet Perhaps, everyone is in trouble with this bug. Do not forget.", "I am getting the same issue with multiple models prediction, either in sequential or in parallel execution. The memory doesn't seem to free the memory after use.", "I recently resolved my model's \"memory leak\" issue, it turns out to be a counter-paradigm of how to construct and run the model(i.e various TensorOps)...\r\nI mistakenly added [cosine_decay_restarts](https://www.tensorflow.org/api_docs/python/tf/train/cosine_decay_restarts) ops in every iteration of the training, something like:\r\n```\r\nwhile ... :\r\n    ....\r\n    dlr = tf.train.cosine_decay_restarts(\r\n            learning_rate=LEARNING_RATE,\r\n            global_step=cur_step,\r\n            first_decay_steps=LR_DECAY_STEPS,\r\n            t_mul=1.0,\r\n            m_mul=1.0,\r\n            alpha=LEARNING_RATE_ALPHA\r\n        )\r\n    lr = sess.run([dlr], feed_dict={cur_step: bno-DECAYED_LR_START})[0]\r\n    ...\r\n    #train the model using the decayed learning rate\r\n```\r\nAnd the training python process would get killed by the OOM killer at some point.\r\nMy instincts told me I shouldn't re-construct the `train.cosine_decay_restarts` in every loop, so after this simple remedy, the \"memory leak\" issue was gone...\r\n```\r\ndlr = tf.train.cosine_decay_restarts(\r\n    learning_rate=LEARNING_RATE,\r\n    global_step=cur_step,\r\n    first_decay_steps=LR_DECAY_STEPS,\r\n    t_mul=1.0,\r\n    m_mul=1.0,\r\n    alpha=LEARNING_RATE_ALPHA\r\n)\r\n...\r\nwhile ... :\r\n    ...\r\n    lr = sess.run([dlr], feed_dict={cur_step: bno-DECAYED_LR_START})[0]\r\n    ...\r\n    #train the model using the decayed learning rate\r\n```\r\nSo maybe everybody could make a coarse check of whether you are re-constructing the model in every loop...", "Hello, \r\n\r\nI am also having a problem with memory leak on running label image logic in a loop\r\nI use Tensorflow r1.0.1 and opencv 2.4.9\r\nI am not sure yet if memory leak is opencv related or tensorflow related.\r\nIn some posts, seems that capture.read() can lead to memory leak(opencv related),  in other posts could be because of operators initialization inside loop(tensorflow related)\r\nBelow part of my code, I would appreciate if smo can check the code inside the loop in case there is smth obvious I should change to avoid memory leak, thank you in advance\r\n`with tf.Session('',graph=graph, config=tf.ConfigProto(inter_op_parallelism_threads=1,intra_op_parallelism_threads=1)) as sess:\r\n\r\nret, frame = cap.read()\r\n\r\nfile_name = '...png'\r\n\r\nif frame is not None: \r\n\r\n      cv2.imwrite(file_name,frame)\r\n\r\n     input_name = \"file_reader\"\r\n\r\n     output_name = \"normalized\"\r\n\r\n     file_reader = tf.read_file(file_name, input_name)\r\n\r\n     if file_name.endswith(\".png\"):\r\n\r\n      image_reader = tf.image.decode_png(file_reader, channels = 3, name='png_reader')\r\n\r\n     float_caster = tf.cast(image_reader, tf.float32)\r\n\r\n     dims_expander = tf.expand_dims(float_caster, 0)\r\n\r\n\r\n     resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\r\n\r\n     normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\r\n\r\n     t = sess.run(normalized)\r\n\r\n     results = sess.run(output_operation.outputs[0],\r\n                      {input_operation.outputs[0]: t})\r\n\r\n     results = np.squeeze(results)\r\n\r\n     top_k = results.argsort()[-1:][::-1]\r\n\r\n     labels = load_labels(label_file)\r\n\r\n...\r\n`\r\nI am going to upgrade also opencv now in my system. Do you think I should upgrade also Tensorflow version. To be honest, I would keep same version of tensorflow in order to use .pb graph already generated because I am not sure if my .pb will be compatible with newer versions. \r\n\r\nAny suggestions to overcome memory leak problem and find what really causes this will be very helpful.\r\n\r\nThank you, ", "I have the same issue. Just by running the same code again, the memory doesn't seem to get freed by clear_session. Below is my code. Please help!\r\n\r\n`import keras\r\nfrom keras.applications.vgg16 import VGG16\r\n\r\nvgg16_model = VGG16()\r\n\r\nmodel = Sequential()\r\nfor layer in vgg16_model.layers[:-1]:\r\n    model.add(layer)\r\nmodel.summary()\r\n\r\nfor layer in model.layers:\r\n    layer.trainable = False\r\n\r\nmodel.add(Dense(2,activation='softmax'))\r\nmodel.summary()\r\n\r\nK.clear_session()\r\nK.reset_uids()`", "> \r\n> \r\n> Problem is, I'm trying to run two model at the same time in a website. Hence, calling `K.clear_session` in the code clears operations that are used elsewhere. So this creates a bug. I really need to be able to specify the operations that I want to clear. Could you be a little more precise on how I could do that ?\r\n\r\nDear sir. How did you resovle your issue? I have the same issue.  Thx.  @Caselles ", "You might find some useful info here: https://stackoverflow.com/questions/44327803/memory-leak-with-tensorflow", "Dear sir. Thank you for your reply. As you can see, calling K.clear_session in the code clears all graphs, however, I just want to clear one of them. Thx. @Caselles ", "Leaving this here so I remember to come back Monday and write out a work-around for the memory leak inside a loop issue using Keras. ", "@Caselles Is this still an issue? I tried you code with `tf-nightly` and could not reproduce the issue. I have used `K.get_session()` and `K.clear_session()`. The memory usage is almost constant. \r\n\r\n```\r\nfor i in range(1000):\r\n  memory()\r\n  K.get_session()\r\n  build_model()\r\n  K.clear_session()\r\n  memory()\r\n  K.get_session()\r\n  build_model()\r\n  K.clear_session()\r\n```\r\nOutput is \r\n\r\n```\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7448959350585938\r\nmemory use: 0.7444076538085938\r\nmemory use: 0.7444076538085938\r\nmemory use: 0.7444076538085938\r\nmemory use: 0.7444076538085938\r\nmemory use: 0.7444076538085938\r\nmemory use: 0.7444076538085938\r\nmemory use: 0.7444076538085938\r\n```\r\nPlease let me know what you think? Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/1445ddf65ab642ffb7e502e6260f5482/tf_10408_keras_runtime.ipynb). Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=10408\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=10408\">No</a>\n"]}]