[{"number": 15067, "title": "Update AWS C++ SDK to 1.3.15", "body": "This fix tries to address the issue raised in #15066 where AWS C++ SDK version was not high enough to support ECS.\r\n\r\nThis fix updates AWS C++ SDK to 1.3.15.\r\n\r\nThis fix fixes #15066.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 15066, "title": "Failing to retrieve AWS credentials running in ECS with the s3 provider", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\n### Describe the problem\r\nWhen using the S3 file system provider within an ECS container I am finding that even though my task has been assigned a task role that provides access to my S3 bucket it is unable to access it successfully.\r\n\r\nEverything works fine if I supply the credentials manually using the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.\r\n\r\nAfter some extensive head-desking I have found that the version of aws-sdk-cpp that is used (1.0.90) appears to have been released before the ECS support was added to the sdk (https://github.com/aws/aws-sdk-cpp/commit/786666db02f5ac7918642c1ee056b822e37a6f30), at a minimum I think we would want 1.0.97 so that everything works as expected.\r\n\r\nI had originally tried to bump the version but it appears that the bazel mirror has nothing but 1.0.90 available.", "comments": ["Added a PR #15067 to update AWS C++ SDK to 1.3.15 for the fix.", "Great thanks @yongtang I had originally intended the same fix but noticed that https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.3.15.tar.gz was not returning anything so was not sure how to make that bit happen."]}, {"number": 15065, "title": "Update 4_convolutions.ipynb", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Hi @liuyanfeng007, this seems very similar to #15064 and #15063 can you close the two that are duplicates.", "No response, just closing for now. Feel free to open a new one if you would like to push this in."]}, {"number": 15064, "title": "Update 4_convolutions.ipynb", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 15063, "title": "Update 4_convolutions.ipynb", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 15062, "title": "Bus Error when running a session", "body": "### System information\r\n- Running on an ODROID-XU4\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Linux Ubuntu Mate 16.04\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.4.0\r\n- Python version: 3.5.2\r\n- Bazel version: 0.8.0\r\n- GCC Version: 5.4.0\r\n- Exact command to reproduce:\r\npython3.5\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow')\r\nsess - tf.Session()\r\nprint(sess.run(hello))\r\n\r\n### Describe the problem\r\nI have built and installed TensorFlow as a pip package onboard an ODROID-XU4 (32-bit ARM), following the steps in this guide: https://hackernoon.com/running-yolo-on-odroid-yolodroid-5a89481ec141\r\nEverything went smoothly, and I was able to install TensorFlow after a lengthy build time. However, when I try to run any session (such as in the basic example above), the program fails with a Bus Error. Running `pip3 list` shows that Tensorflow is indeed installed, and no errors are thrown when I merely import TensorFlow.\r\n\r\n### Source code / logs\r\nI traced the bus error by looking into the /var/log/syslog file and found the following lines associated with the error: `Dec  2 21:24:21 odroid kernel: [ 3658.433306] Alignment trap: python3.5 (10189) PC=0xac0bac42 Instr=0xf9068a1f Address=0xbe8a7da4 FSR 0x811\r\nDec  2 21:24:21 odroid kernel: [ 3658.433313] Alignment trap: not handling instruction f9068a1f at [<ac0bac42>]\r\nDec  2 21:24:21 odroid kernel: [ 3658.439021] Unhandled fault: alignment exception (0x811) at 0xbe8a7da4`\r\nSo, it appears to be an alignment issue. I tried to force the kernel to attempt to fix the error instead of simply failing by using the following command: `echo 3 > /proc/cpu/alignment`\r\nThe three is meant to tell the kernel to fix these alignment issues. However, this strategy has not changed anything about the Bus Error when attempting to run a session. \r\n\r\nPerhaps this is related to the 32-bit architecture I am attempting to run on?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have updated the post with those specifications", "Is it 32-bit only? I'm afraid 32-bit won't work.", "Yes, the processor onboard is a 32-bit ARM big.LITTLE architecture. It has a Cortex A7 and a Cortex A15 for a total of 8 cores. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "@petewarden feel free to remove the label or unassign yourself. thanks!", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "I come across the same problem, though I installed TensorFlow on BeagleBone Black.\r\npython2.7.13\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow')\r\nsess - tf.Session()\r\nprint(sess.run(hello))\r\nBus eeror\r\n\r\nIs there any suggestions of how to fix this?", "We don't officially support odroid or BeagleBones, but I am surprised to see this error. I have some details on cross-compiling for the similar Pi platforms here:\r\nhttps://petewarden.com/2017/08/20/cross-compiling-tensorflow-for-the-raspberry-pi/\r\n\r\nI did see similar symptoms when I was cross-compiling and wasn't using matching alignment options on the Pi 3, which was fixed by some compile flags:\r\nhttps://raspberrypi.stackexchange.com/questions/48225/whats-causing-these-crashes-after-cross-compiling\r\n\r\nI don't know how this relates to your problem though, and since these are unsupported platforms, closing this bug.", "@petewarden I have the same issue, I guess my platforms is not supported too.\r\n\r\nWhere can I find the supported platform list?", "having the same issue on Raspberry Pi 3.", "@cedricve \r\nI solved **`Bus Error`**.\r\nI do not know if I can contribute to you, but I preserved the binaries pre-build below.\r\n**https://github.com/PINTO0309/Tensorflow-bin.git**\r\n**Tensorflow v1.11.0**\r\nI just built a native build on RaspberryPi3.", "@PINTO0309 I am no longer working with the device I had this problem with, however I am happy to see some progress on this. I will test this when I get the chance.", " @PINTO0309 It seems binaries are for Pi and TX2. I am also facing same issue with Odroid. Could you please explain how you solved bus error?\r\n\r\nThanks.", "@Kishwar \r\n\r\nI just built a **`native build`** on RaspberryPi3 and TX2.\r\nThere seems to be a problem with cross compile scripts.\r\nThe point you need to be aware of is that it takes dozens of hours for native builds to use low-spec devices.\r\nUnfortunately, I do not own \"Odroid\", so I can not try it.", "@PINTO0309 \r\n\r\nThanks for the response. Your binary for Pi is working on Odroid XU4. \r\n\r\n```\r\n>>> import tensorflow as tf\r\n/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n>>> print(sess.run(hello))\r\nb'Hello, TensorFlow!'\r\n```\r\n\r\nWarning msg because binaries are compiled for Python3.5 but Odroid have 3.6.\r\n\r\nIf anyone installing binary compiled with 3.5 with pip3 of python will get following error msg:\r\n\r\n**tensorflow-1.11.0-cp35-cp35m-linux_armv7l.whl is not a supported wheel on this platform.**\r\n\r\nSolution: \r\n1. Rename file to tensorflow-1.11.0-cp36-cp36m-linux_armv7l.whl \r\n2. Install using python3 -m pip install tensorflow-1.11.0-cp36-cp36m-linux_armv7l.whl \r\n\r\nEnjoy :)\r\n", "Hi All,\r\nI have written the installation guide for Odroid. Please check **STABLE SECTION**. We don't need to build because official binary is available with Tensorflow.\r\n\r\nhttps://github.com/Kishwar/tensorflow/tree/master/odroid-xu4"]}, {"number": 15061, "title": "saved_model.pb needs a different file extension", "body": "The `saved_model.pb` file extension type clashes with every other `.pb` file created. Apps that register the `.pb` file extension won't be able to tell apart Saved Models from other protobuf files...", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.4\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "/CC @mrry can you comment? I'm not sure why the file extension is significant.", "I'm also not sure why the file extension is significant. AIUI, `saved_model.pb` *is* a protobuf file, so an application that registers the `.pb` suffix in order to consume protobuf files ought to be able to consume a SavedModel.\r\n\r\n@lutzroeder Can you describe what you're trying to do as a feature request, perhaps? Based on your post, I'm not convinced that the file format \"needs\" a different file extension, but if we understood more about what you're doing, that would help.\r\n\r\n/cc @sukritiramesh @nfiedel \r\n\r\n", "If an app registers `.pb` it will list and open any protobuf file that uses the generic protobuf format, `saved_model.pb` or not. Since protobuf has no way of detecting the specific protobuf format (except for trying to decode the file) this can lead to some nasty conflicts. For example, an app registering `.pb` to open ONNX files (which are protobuf as well) will now show `saved_model.pb` is a valid onnx file, except for it isn't. By analogy imagine all binary files using `.bin` instead of having `.doc`, `.xls` and `.ppt`. Word and Excel could just register `.bin` but wouldn't that be a recipe for disaster?", "In that case, it sounds like the bug would be in the app that registers `*.pb` but only handles ONNX files.\r\n\r\nAnyway, I have no skin in this game, so I'll let @nfiedel and/or @sukritiramesh comment on whether they'd consider a different extension for SavedModel files. I suspect backwards compatibility will be the main concern, since `saved_model.pb` is just one of the files in a standard directory layout that the tools use.", "Thanks all for sharing context and the question here. For several reasons, I don't think it makes much sense to change the file extension of Saved Model's protobuffs.\r\n\r\nFirst is forwards & backwards compatibility and migrations. There are 1000s of users of SavedModel. Changing the extension would require a long period (many months) of backwards compatibility along with forward-support, such that new code could read old models. We've done such migrations in the past and they involve a large amount of work and risk (i.e. risk of outages to customers using these models and code in production settings).\r\n\r\nSecond is a general question about file extensions and suitability of them for various purposes. There are a variety of generalized proto introspection tools out there, for which it could be perfectly reasonable to open any proto and view/inspect it by an extension \".pb\". At the same time, relying on the file extension \".pb\" of just one file in a ML model to specify the ML platform seems problematic.\r\n Remember that there are also weights and assets such as vocab or embedding files, that could be arbitrarily encoded as csv, proto, pbtxt, or otherwise.\r\n\r\nIn summary, I don't think it's particularly feasible to rename the saved_model.pb in the SavedModel specification, and even if we did, I don't think it will actually solve the problem at hand.", "I understand the the pain involved and why such changes aren't easy to make but also seems like the current approach of everything is a `.pb` and there is no header to detect what is in the `.pb` isn't going to scale. Not expecting this to get fixed soon or easily but it sounds like the underlying issue of using `.pb` as a persistence and file exchange format needs some thinking.\r\n\r\nI wrote a generic `.pb` browser and found it mostly useless as the files aren't very readable without the right `.proto` and root object being known. This leaves success of opening a `.pb` mostly to the user guessing correctly and/or having the right tribal knowledge being available on StackOverflow.\r\n\r\nCurrently tinkering with a [viewer for neural networks](https://www.github.com/lutzroeder/Netron) and seeing the reverse issue. It registers `.pb` and all sorts of files light up in Finder/Explorer classified as TensorFlow. Opening these files and having no way to tell if they contain `GraphDef` or `MetaGraphDef` or `SavedModel` or something else entirely leads to all kinds of asserts and crashes. For all the beauty of Protocol Buffers, this seems to be broken and does need fixing.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Closing this issue since @nfiedel indicated `.pb` would not be changed."]}, {"number": 15060, "title": "AttentionWrapperZeroState: Input to reshape is a tensor with 32768 values, but the requested shape has 65536", "body": "I am building an encoder-decoder model with attention and BeamSearchDecoder using tensor flow documentation. I am getting following error:\r\n\r\n    ---------------------------------------------------------------------------\r\n    InvalidArgumentError                      Traceback (most recent call last)\r\n    <ipython-input-160-ac947b28f4dd> in <module>()\r\n         29                                           summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)],\r\n         30                                           text_length: [len(text)]*batch_size,\r\n    ---> 31                                           keep_prob: 1.0})[0] \r\n         32         # Remove the padding from the summaries\r\n         33         pad = vocab_to_int[\"<PAD>\"]\r\n\r\n    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n        893     try:\r\n        894       result = self._run(None, fetches, feed_dict, options_ptr,\r\n    --> 895                          run_metadata_ptr)\r\n        896       if run_metadata:\r\n        897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n       1122     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n       1123       results = self._do_run(handle, final_targets, final_fetches,\r\n    -> 1124                              feed_dict_tensor, options, run_metadata)\r\n       1125     else:\r\n       1126       results = []\r\n\r\n    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n       1319     if handle is None:\r\n       1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n    -> 1321                            options, run_metadata)\r\n       1322     else:\r\n       1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n    /Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n       1338         except KeyError:\r\n       1339           pass\r\n    -> 1340       raise type(e)(node_def, op, message)\r\n       1341 \r\n       1342   def _extend_graph(self):\r\n\r\n    InvalidArgumentError: Input to reshape is a tensor with 32768 values, but the requested shape has 65536\r\n         [[Node: decode_1/Reshape_2 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tile_batch_2/Reshape_2, decode_1/concat_2)]]\r\n\r\n    Caused by op u'decode_1/Reshape_2', defined at:\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n        \"__main__\", fname, loader, pkg_name)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\r\n        exec code in run_globals\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\r\n        app.launch_new_instance()\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n        app.start()\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\r\n        ioloop.IOLoop.instance().start()\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n        super(ZMQIOLoop, self).start()\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\r\n        handler_func(fd_obj, events)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n        return fn(*args, **kwargs)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n        self._handle_recv()\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n        self._run_callback(callback, msg)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n        callback(*args, **kwargs)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\r\n        return fn(*args, **kwargs)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\r\n        return self.dispatch_shell(stream, msg)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\r\n        handler(stream, idents, msg)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\r\n        user_expressions, allow_stdin)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n        res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\r\n        return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\r\n        interactivity=interactivity, compiler=compiler, result=result)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\r\n        if self.run_code(code, result):\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\r\n        exec(code_obj, self.user_global_ns, self.user_ns)\r\n      File \"<ipython-input-160-ac947b28f4dd>\", line 18, in <module>\r\n        loader = tf.train.import_meta_graph(checkpoint + '.meta')\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1698, in import_meta_graph\r\n        **kwargs)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 656, in import_scoped_meta_graph\r\n        producer_op_list=producer_op_list)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\r\n        op_def=op_def)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n        original_op=self._default_original_op, op_def=op_def)\r\n      File \"/Users/tusharagarwal/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n    InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 32768 values, but the requested shape has 65536\r\n         [[Node: decode_1/Reshape_2 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](tile_batch_2/Reshape_2, decode_1/concat_2)]]\r\n\r\nThe error occurs when trying to Make predictions(Test the Model Section) .Training runs fine. The error occurs when I set beam_size >=2. (The error shown is for beam_size=2). However it runs fine for beam_size = 1. I am not able to figure out what is going wrong. I know the code is long. Any help will be highly appreciated as I am unable to debug it and stuck on it for days. My code is:\r\n\r\nMy code is here:\r\n# Model Inputs\r\n\r\n    def model_inputs():\r\n        input_data = tf.placeholder(tf.int32, [None, None], name='input')\r\n        targets = tf.placeholder(tf.int32, [None, None], name='targets')\r\n        lr = tf.placeholder(tf.float32, name='learning_rate')\r\n        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\r\n        summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\r\n        max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\r\n        text_length = tf.placeholder(tf.int32, (None,), name='text_length')\r\n\r\n        return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length\r\n\r\n    def process_encoding_input(target_data, vocab_to_int, batch_size):  \r\n        ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\r\n        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\r\n\r\n        return dec_input\r\n# Encoding layer\r\n\r\n    def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\r\n        for layer in range(num_layers):\r\n            with tf.variable_scope('encoder_{}'.format(layer)):\r\n                cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\r\n                                                  initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\r\n                cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \r\n                                                        input_keep_prob = keep_prob)\r\n\r\n                cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\r\n                                                  initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\r\n                cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \r\n                                                        input_keep_prob = keep_prob)\r\n\r\n                enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \r\n                                                                        cell_bw, \r\n                                                                        rnn_inputs,\r\n                                                                        sequence_length,\r\n                                                                        dtype=tf.float32)\r\n                enc_output = tf.concat(enc_output,2)\r\n                # original code is missing this line below, that is how we connect layers \r\n                # by feeding the current layer's output to next layer's input\r\n                #rnn_inputs = enc_output\r\n        return enc_output, enc_state\r\n\r\n# Training Decoding layer  \r\n\r\n    def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\r\n                                vocab_size, max_summary_length,batch_size,enc_state):\r\n        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\r\n                                                            sequence_length=summary_length,\r\n                                                            time_major=False)\r\n\r\n        training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\r\n                                                           helper=training_helper,\r\n                                            initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(cell_state=enc_state),\r\n                                                           output_layer = output_layer)\r\n\r\n        training_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\r\n                                                                impute_finished = True,\r\n                                                               maximum_iterations=max_summary_length)\r\n        return training_logits\r\n\r\n# Inference Decoding layer  \r\n\r\n    def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\r\n                                 max_summary_length, batch_size,lengths,enc_state,beam_width):\r\n        '''Create the inference logits'''\r\n\r\n        start_tokens = tf.ones_like(lengths) * start_token\r\n\r\n\r\n\r\n        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n                        cell          = dec_cell,\r\n                        embedding     = embeddings,\r\n                        start_tokens  = start_tokens,\r\n                        end_token     = end_token,\r\n                        initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size * beam_width*2).clone(cell_state=enc_state) ,\r\n                        beam_width    = beam_width,\r\n                        output_layer  = output_layer)\r\n\r\n\r\n        inference_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\r\n                                                                 impute_finished = False,\r\n                                                                maximum_iterations=max_summary_length)\r\n\r\n        return inference_logits\r\n\r\n    def lstm_cell(lstm_size, keep_prob):\r\n        cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\r\n        return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\r\n\r\n# Decoding layer \r\n\r\n    def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\r\n                       max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\r\n        '''Create the decoding cell and attention for the training and inference decoding layers'''\r\n        output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\r\n\r\n        with tf.variable_scope(\"decode\"):\r\n            dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\r\n            attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\r\n                                                         enc_output,\r\n                                                         text_length,\r\n                                                         normalize=False,\r\n                                                         )\r\n            dec_cell = tf.contrib.seq2seq.AttentionWrapper(cell =dec_cell,\r\n                                                           attention_mechanism = attn_mech, \r\n                                                           attention_layer_size=rnn_size)\r\n\r\n            training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\r\n                                                      output_layer,\r\n                                                      vocab_size,\r\n                                                      max_summary_length,\r\n                                                      batch_size,enc_state)\r\n\r\n\r\n        beam_width = 2\r\n        enc_output = tf.contrib.seq2seq.tile_batch(enc_output, multiplier=beam_width)\r\n        lengths = tf.contrib.seq2seq.tile_batch(text_length, multiplier=beam_width)\r\n        enc_state = tf.contrib.seq2seq.tile_batch(enc_state, multiplier=beam_width)\r\n\r\n        with tf.variable_scope(\"decode\", reuse=True):\r\n            dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\r\n            attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\r\n                                                         enc_output,\r\n                                                         lengths,\r\n                                                         normalize=False,\r\n                                                         )\r\n            dec_cell = tf.contrib.seq2seq.AttentionWrapper(cell =dec_cell,\r\n                                                           attention_mechanism = attn_mech, \r\n                                                           attention_layer_size=rnn_size)\r\n\r\n\r\n            inference_logits = inference_decoding_layer(embeddings,\r\n                                                        vocab_to_int['<GO>'],\r\n                                                        vocab_to_int['<EOS>'],\r\n                                                        dec_cell,\r\n                                                        output_layer,\r\n                                                        max_summary_length,\r\n                                                        batch_size,lengths,enc_state,beam_width)\r\n        return training_logits, inference_logits\r\n\r\n\r\n    def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \r\n                      vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\r\n        '''Use the previous functions to create the training and inference logits'''\r\n\r\n        # Use Numberbatch's embeddings and the newly created ones as our embeddings\r\n        #embeddings = word_embedding_matrix\r\n        embeddings_s = tf.get_variable(\"word_embeddings\",[vocab_size, 300])\r\n        embeddings_t = tf.get_variable(\"word_embeddings2\",[vocab_size, 300])\r\n        enc_embed_input = tf.nn.embedding_lookup(embeddings_s, input_data)\r\n        enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\r\n        dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\r\n        dec_embed_input = tf.nn.embedding_lookup(embeddings_t, dec_input)\r\n        training_logits, inference_logits  = decoding_layer(dec_embed_input, \r\n                                                            embeddings_t,\r\n                                                            enc_output,\r\n                                                            enc_state, \r\n                                                            vocab_size, \r\n                                                            text_length, \r\n                                                            summary_length, \r\n                                                            max_summary_length,\r\n                                                            rnn_size, \r\n                                                            vocab_to_int, \r\n                                                            keep_prob, \r\n                                                            batch_size,\r\n                                                            num_layers)\r\n        return training_logits, inference_logits\r\n\r\n# Set the Hyperparameters\r\n    epochs = 100\r\n    batch_size = 64\r\n    rnn_size = 256\r\n    num_layers = 2\r\n    learning_rate = 0.005\r\n    keep_probability = 0.95\r\n\r\n\r\n# Build the graph\r\n    train_graph = tf.Graph()\r\n    # Set the graph to default to ensure that it is ready for training\r\n    with train_graph.as_default():\r\n\r\n        # Load the model inputs    \r\n        input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\r\n\r\n        # Create the training and inference logits\r\n        training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\r\n                                                          targets, \r\n                                                          keep_prob,   \r\n                                                          text_length,\r\n                                                          summary_length,\r\n                                                          max_summary_length,\r\n                                                          len(vocab_to_int)+1,\r\n                                                          rnn_size, \r\n                                                          num_layers, \r\n                                                          vocab_to_int,\r\n                                                          batch_size)\r\n\r\n        # Create tensors for the training logits and inference logits\r\n        training_logits = tf.identity(training_logits.rnn_output, 'logits')\r\n        #inference_logits = inference_logits.predicted_ids[:,:,0]\r\n        inference_logits = tf.identity(inference_logits.predicted_ids, name='predictions')\r\n\r\n        # Create the weights for sequence_loss, the sould be all True across since each batch is padded\r\n        masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\r\n\r\n        with tf.name_scope(\"optimization\"):\r\n            # Loss function\r\n            cost = tf.contrib.seq2seq.sequence_loss(\r\n                training_logits,\r\n                targets,\r\n                masks)\r\n\r\n            # Optimizer\r\n            optimizer = tf.train.AdamOptimizer(learning_rate)\r\n\r\n            # Gradient Clipping\r\n            gradients = optimizer.compute_gradients(cost)\r\n            capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\r\n            train_op = optimizer.apply_gradients(capped_gradients)\r\n    print(\"Graph is built.\")\r\n    graph_location = \"./graph\"\r\n    print(graph_location)\r\n    train_writer = tf.summary.FileWriter(graph_location)\r\n    train_writer.add_graph(train_graph)\r\n\r\n\r\n# Train the Model\r\n    learning_rate_decay = 0.95\r\n    min_learning_rate = 0.0005\r\n    display_step = 20 # Check training loss after every 20 batches\r\n    stop_early = 0 \r\n    stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\r\n    per_epoch = 3 # Make 3 update checks per epoch\r\n    #update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\r\n    update_check = 2\r\n\r\n    update_loss = 0 \r\n    batch_loss = 0\r\n    summary_update_loss = [] # Record the update losses for saving improvements in the model\r\n\r\n    checkpoint = \"./best_model.ckpt\" \r\n    with tf.Session(graph=train_graph) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        # If we want to continue training a previous session\r\n        #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\r\n        #loader.restore(sess, checkpoint)\r\n\r\n        for epoch_i in range(1, epochs+1):\r\n            update_loss = 0\r\n            batch_loss = 0\r\n            for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\r\n                    get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\r\n                start_time = time.time()\r\n                _, loss = sess.run(\r\n                    [train_op, cost],\r\n                    {input_data: texts_batch,\r\n                     targets: summaries_batch,\r\n                     lr: learning_rate,\r\n                     summary_length: summaries_lengths,\r\n                     text_length: texts_lengths,\r\n                     keep_prob: keep_probability})\r\n\r\n                batch_loss += loss\r\n                update_loss += loss\r\n                end_time = time.time()\r\n                batch_time = end_time - start_time\r\n\r\n                if batch_i % display_step == 0 and batch_i > 0:\r\n                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\r\n                          .format(epoch_i,\r\n                                  epochs, \r\n                                  batch_i, \r\n                                  len(sorted_texts_short) // batch_size, \r\n                                  batch_loss / display_step, \r\n                                  batch_time*display_step))\r\n                    batch_loss = 0\r\n\r\n                if batch_i % update_check == 0 and batch_i > 0:\r\n                    print(\"Average loss for this update:\", round(update_loss/update_check,3))\r\n                    summary_update_loss.append(update_loss)\r\n\r\n                    # If the update loss is at a new minimum, save the model\r\n                    if update_loss <= min(summary_update_loss):\r\n                        print('New Record!') \r\n                        stop_early = 0\r\n                        saver = tf.train.Saver() \r\n                        saver.save(sess, checkpoint)\r\n\r\n                    else:\r\n                        print(\"No Improvement.\")\r\n                        stop_early += 1\r\n                        if stop_early == stop:\r\n                            break\r\n                    update_loss = 0\r\n\r\n\r\n            # Reduce learning rate, but not below its minimum value\r\n            learning_rate *= learning_rate_decay\r\n            if learning_rate < min_learning_rate:\r\n                learning_rate = min_learning_rate\r\n\r\n            if stop_early == stop:\r\n                print(\"Stopping Training.\")\r\n                break\r\n\r\n\r\n    def text_to_seq(text):\r\n        '''Prepare the text for the model'''\r\n\r\n        text = clean_text(text)\r\n        return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\r\n# Test The Model   \r\n\r\n    input_sentences=[\"The coffee tasted great and was at such a good price! I highly recommend this to everyone!\",\r\n                   \"love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\"]\r\n    generagte_summary_length =  [3,2]\r\n\r\n    # input_sentences = test_text[0:10]\r\n    # generagte_summary_length = summary_l[0:10]\r\n    texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\r\n    checkpoint = \"./best_model.ckpt\"\r\n    if type(generagte_summary_length) is list:\r\n        if len(input_sentences)!=len(generagte_summary_length):\r\n            raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\r\n        generagte_summary_length_list = generagte_summary_length\r\n    else:\r\n        generagte_summary_length_list = [generagte_summary_length] * len(texts)\r\n    loaded_graph = tf.Graph()\r\n    with tf.Session(graph=loaded_graph) as sess:\r\n        # Load saved model\r\n        loader = tf.train.import_meta_graph(checkpoint + '.meta')\r\n        loader.restore(sess, checkpoint)\r\n        input_data = loaded_graph.get_tensor_by_name('input:0')\r\n        logits = loaded_graph.get_tensor_by_name('predictions:0')\r\n        text_length = loaded_graph.get_tensor_by_name('text_length:0')\r\n        summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\r\n        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\r\n        #Multiply by batch_size to match the model's input parameters\r\n        for i, text in enumerate(texts):\r\n            generagte_summary_length = generagte_summary_length_list[i]\r\n            answer_logits = sess.run(logits, {input_data: [text]*batch_size, \r\n                                              summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \r\n                                              text_length: [len(text)]*batch_size,\r\n                                              keep_prob: 1.0})[0] \r\n            # Remove the padding from the summaries\r\n            pad = vocab_to_int[\"<PAD>\"] \r\n            print('- Review:\\n\\r {}'.format(input_sentences[i]))\r\n            print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i[0]] for i in answer_logits if i[0] != pad])))\r\ntensorflow\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Figured out the problem. Thanks", "@tusharag171 hi! I am running into the same problem. Would you mind sharing your solution? Thanks in advance!", "Can u please share the solution to the problem?? we would be glad to you @tusharag171 \r\nplease!! ", "I encoutered the same problem.How to save it ,thanks!"]}, {"number": 15059, "title": "Running tensorflow-gpu in virtual env in remote Ubuntu system", "body": "I was attempting to install tensorflow gpu version for my gpu system.\r\n - I don't have sudo access in the computer. \r\n - Cuda version 7.5 (V7.5.17) is installed\r\nAfter following all the steps and finding a suitable version of tensorflow suitable to cuda 7.5 I have installed everything. But after installing when I am trying to import tensorflow, these errors are coming. \r\n![image](https://user-images.githubusercontent.com/31855851/33518214-456db3ce-d75f-11e7-8d7e-a4f62f1c6dcc.png)\r\n\r\nI don't know how to resolve it. Also, does this error affect the running of my code in gpu or can I ignore it and start coding?\r\nThe solutions I found on forum is related to reinstalling cuda but I cannot do that as I don't have sudo access. Can anyone please provide me some other solution to resolve this issue?\r\nI really want to setup my environment soon for the project. \r\nThank you!\r\n\r\nAdditional info\r\n\r\nOS Platform and Distribution Ubuntu v 16.04\r\nTensorFlow installed via creating virtualenv\r\nTensorFlow version 0.12.0rc0\r\nBazel version - Not used\r\nCUDA/cuDNN version - 7.5 \r\nGPU model and memory - NA\r\nExact command to reproduce - NA\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hello everyone the error is resolved. \r\nI followed this https://gist.github.com/digvijayky/e2f91af603b83396d6b3f2d281bb97a7\r\nLinked the cuda directory to path using these and it works now.\r\n![image](https://user-images.githubusercontent.com/31855851/33529532-1b021a4e-d840-11e7-9d0c-d15de6a77b1a.png)\r\n![image](https://user-images.githubusercontent.com/31855851/33529535-28574e6c-d840-11e7-824b-481baf93200f.png)\r\n"]}, {"number": 15058, "title": "Go bindings: Two variables interfere unless op.VarHandleOpSharedName is used", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch linux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**:  NA\r\n- **GPU model and memory**:\r\n\r\n### Describe the problem\r\nWhen creating and assigning values to two variables in the Go bindings, they conflict, one value gets assigned to both variables. The variables assign OPs race, so it is somewhat non deterministic which gets assigned to both.\r\n\r\nReproduce:\r\nUse `op.VarHandleOp()` to create two variable handles, and then use `op.AssignVariableOp()` to create OPs to assign the constant 1 to the first variable, and 2 to the second. Pull on the two assign OPs.\r\nEvaluate the variable reader outputs for the two variables.\r\nExpected result: First variable has a value of 1, and second variable has a value of 2.\r\nObserved result: Both variables have the same value, usually 2, but occasionally 1.\r\n\r\nIs this behavior correct?\r\n\r\nIf I use the optional parameter `op.VarHandleOpSharedName` in op.VarHandleOp, giving the two variables different names, it works as expected, so this is easy to work around.\r\n\r\n\r\n### Source code / logs\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n)\r\n\r\nfunc makeVariable(s *op.Scope, i int32, name string) (init *tf.Operation, output tf.Output) {\r\n\tconstant := op.Const(s, i)\r\n\tvariable := op.VarHandleOp(s, tf.Int32, tf.ScalarShape())\r\n\t//variable := op.VarHandleOp(s, tf.Int32, tf.ScalarShape(), op.VarHandleOpSharedName(name))\r\n\tinit = op.AssignVariableOp(s, variable, constant)\r\n\toutput = op.ReadVariableOp(s, variable, tf.Int32)\r\n\treturn\r\n}\r\n\r\nfunc main() {\r\n\ts := op.NewScope()\r\n\tinit1, output1 := makeVariable(s.SubScope(\"var1\"), 1, \"variable_1\")\r\n\tinit2, output2 := makeVariable(s.SubScope(\"var2\"), 2, \"variable_2\")\r\n\r\n\tgraph, err := s.Finalize()\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tsess, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\t_, err = sess.Run(nil, nil, []*tf.Operation{init1, init2})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tresults, err := sess.Run(nil, []tf.Output{output1, output2}, nil)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tfmt.Println(results[0].Value(), results[1].Value())\r\n}\r\n```\r\nPrints:\r\n```\r\n[isaac@d6-arch tfes]$ go run shape_bug_demo.go \r\n2017-12-02 09:17:06.516455: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-12-02 09:17:06.594468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-12-02 09:17:06.594676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.15GiB\r\n2017-12-02 09:17:06.594688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2 2\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nExact command to reproduce", "This is by design. TensorFlow \"resources\" (`DT_RESOURCE` tensors) are identified by their shared name (and in fact, in distributed settings, the shared name is used to identify variables across different sessions as each worker is running a different session).\r\n\r\nIn Python, the `VarHandleOp` is typically not invoked directly, but instead variables are created via the [`ResourceVariable`](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/resource_variable_ops.py#L66) class, which ensures that names are unique (with the use of `name_scope`s).\r\n\r\nI understand that the fact that the `shared_name` attribute is optional can be confusing. Unfortunately, this is something we won't be changing because of our backward compatibility guarantees.\r\n\r\nLong story short, the Go API you provide to create variables (`makeVariable` in your snippet, or some type abstraction that you might be creating similar to the Python `ResourceVariable` class), should require a name argument and set the attribute.\r\n\r\nHope that helps.\r\n\r\nI'm going to close this bug as things are working as intended. Improvements to these low level resource details are being discussed but are beyond the scope of this bug.\r\n\r\nFeel free to re-open if I'm mistaken.\r\n\r\nThanks.\r\n\r\n(FYI @alextp )", "@asimshankar Thank you for clarifying. This is a good and useful behavior.\r\n\r\nI first encountered this behavior when it was leading to a non-deterministic bug. It would be nice to make the result of pulling on two assign operation for the same resource deterministic. Or else return an error when two assign operations for the same resource are being pulled on in the same call to `session.Run()`. This would help developers to avoid my mistake (hopefully) without breaking backwards compatibility. AFAIK, there are no circumstances in which a developer would want to assign two different values to the same resource in the same run. If they do, it's by accident, and the API should warn them."]}, {"number": 15057, "title": "gpu is slower than cpu", "body": "Have I written custom code\uff1aNo\r\nOS Platform and Distribution: win10\r\nTensorFlow installed from: pip install tensorflow-gpu or pip install tensorflow\r\nTensorFlow version: 1.4 and 1.3\r\nBazel version: None\r\nCUDA/cuDNN version: cuda8.0 and cudnn 6.0\r\nGPU model and memory: GTX1050 2GB\r\nExact command to reproduce: no\r\n\r\nI have trained a cnn+lstm mode, and use the model to predict one image, however, I find the GPU is slowly than CPU. I have test the tensorflow-gpu 1.4, tensorflow-gpu 1.3\uff0c tensorflow 1.4 and tensorflow 1.4\r\nthe code is \r\n```python\r\n# -*- coding: utf-8 -*-\r\n# @Time    : 2017/10/26 14:09\r\n# @Author  : zhoujun\r\n\r\nimport tensorflow as tf\r\nfrom scipy.misc import imread\r\nimport time\r\nimport os\r\n\r\nclass PredictionModel:\r\n    \r\n    def __init__(self, model_dir, session=None):\r\n        if session:\r\n            self.session = session\r\n        else:\r\n            self.session = tf.get_default_session()\r\n        start = time.time()\r\n        self.model = tf.saved_model.loader.load(self.session, ['serve'], model_dir)\r\n        print('load_model_time:', time.time() - start)\r\n\r\n        self._input_dict, self._output_dict = _signature_def_to_tensors(self.model.signature_def['predictions'])\r\n\r\n    def predict(self, image):\r\n        output = self._output_dict\r\n        # \u8fd0\u884cpredict  op\r\n        start = time.time()\r\n        result = self.session.run(output, feed_dict={self._input_dict['images']: image})\r\n        print('predict_time:',time.time()-start)\r\n        return result\r\n\r\n\r\ndef _signature_def_to_tensors(signature_def):  # from SeguinBe\r\n    g = tf.get_default_graph()\r\n    return {k: g.get_tensor_by_name(v.name) for k, v in signature_def.inputs.items()}, \\\r\n           {k: g.get_tensor_by_name(v.name) for k, v in signature_def.outputs.items()}\r\n\r\n\r\ndef predict(model_dir, image,gpu_id = 0):\r\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\r\n\r\n    with tf.Session() as sess:\r\n        start = time.time()\r\n        model = PredictionModel(model_dir,session=sess)\r\n        predictions = model.predict(image)\r\n        transcription = predictions['words']\r\n        score = predictions['score']\r\n        return [transcription[0].decode(), score, time.time() - start]\r\n\r\n\r\nif __name__ == '__main__':\r\n    model_dir = 'model/'\r\n    image = imread('3_song.jpg', mode='L')[:, :, None]\r\n    result = predict(model_dir, image,0)\r\n    print(tf.__version__)\r\n    print(result)\r\n```\r\ntensorflow-gpu 1.4 log\r\n```sh\r\n2017-12-02 22:30:30.147490: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2017-12-02 22:30:31.083045: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Found device 0 with properties:\r\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.62GiB\r\n2017-12-02 22:30:31.083203: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nload_model_time: 2.9134938716888428\r\npredict_time: 4.135322570800781\r\n1.4.0\r\n```\r\ntensorflow-gpu 1.3 log\r\n```sh\r\n2017-12-02 22:39:46.346092: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-02 22:39:46.346191: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-02 22:39:47.187559: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX 1050\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.493\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.62GiB\r\n2017-12-02 22:39:47.187717: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0\r\n2017-12-02 22:39:47.188099: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y\r\n2017-12-02 22:39:47.188224: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0)\r\nload_model_time: 2.595602512359619\r\npredict_time: 1.6665771007537842\r\n1.3.0\r\n```\r\ntensorflow 1.4 log\r\n```sh\r\n2017-12-02 22:47:21.368827: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nload_model_time: 2.340376853942871\r\npredict_time: 3.538094997406006\r\n1.4.0\r\n```\r\ntensorflow 1.3 log\r\n```sh\r\n2017-12-02 22:51:33.877932: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-02 22:51:33.878031: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nload_model_time: 2.2079925537109375\r\npredict_time: 0.5485155582427979\r\n1.3.0\r\n```\r\nAs can be seen from the log, tensorflow1.4 slower than 1.3 #14942, and gpu mode slower than cpu. If needed, I can provide models and test images", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have update the issue", "@WenmuZhou Providing models and test images would help, since I'm not sure what RNN ops you are using. What would be even more convenient is providing a single file that builds and benchmarks the model.\r\n\r\n@ebrevdo, any ideas what the issue could be?\r\n", "Timing a single call to session.run is not realistic; and you're spending a lot of time on overhead, especially in the GPU case.  If you really want to know how long it takes to perform inference per session.run() call, you need to warm up the system and then time just the session.run() call:\r\n\r\nWhat you should be doing is calling between 1-10 session.run() prediction tasks and throwing away their time (this is the \"warm-up\"), then running predict() ~ 10 to 100 or 1000 times, and averaging the resulting timings.  This will give you more consistent and realistic numbers.", "@reedwm  I use `tf.contrib.rnn.BasicLSTMCell`\r\n\r\nthe model and image is in this https://drive.google.com/open?id=1WbjbJxzJRyYtxvGoZhyeVe2FgwEoc5c1", "@ebrevdo I have predict 1,000 images and average the times\r\n\r\n[log.zip](https://github.com/tensorflow/tensorflow/files/1535427/log.zip)\r\n", "Can you try with tensorflow nighty build and tell me if the performance is\nimproved?  There were some performance regressions in tf 1.4 that we fixed,\nbut not sure if any of them would slow you down.\n\nOn Wed, Dec 6, 2017 at 6:09 AM, zhoujun <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> I have predict 1,000 images and\n> average the times\n>\n> log.zip <https://github.com/tensorflow/tensorflow/files/1535427/log.zip>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15057#issuecomment-349650082>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2oW1TYE99tvJbyBDgk5z2GjXZpUks5s9qAugaJpZM4QzVan>\n> .\n>\n", "@ebrevdo how  can I install the tensorflow nightly", "@WenmuZhou you can use the docker images with \"nightly-\" from here: https://hub.docker.com/r/tensorflow/tensorflow/tags//", "@WenmuZhou you can also use pip install for pypi binaries `tf-nightly` and `tf-nightly-gpu`", "tf-nightly-gpu 1.5.0-dev20171120\r\n```sh\r\n2017-12-09 22:45:21.321091: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2017-12-09 22:45:22.160305: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1064] Found device 0 with properties: \r\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 2.00GiB freeMemory: 1.62GiB\r\n2017-12-09 22:45:22.161441: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1154] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nload_model_time: 2.7245426177978516\r\n4.763048410415649\r\n3.5927882194519043\r\n3.6728804111480713\r\n3.7763960361480713\r\n3.788919687271118\r\n3.6771552562713623\r\n3.7940075397491455\r\n3.70556640625\r\n3.7351977825164795\r\n3.963012933731079\r\n4.3758275508880615\r\n4.445350170135498\r\n3.9991774559020996\r\n3.9501402378082275\r\n4.175319194793701\r\n3.9032864570617676\r\n3.8774490356445312\r\n4.184133291244507\r\n4.077642917633057\r\n4.023866653442383\r\n4.1669580936431885\r\n4.283828020095825\r\n4.18477988243103\r\n4.104889392852783\r\n4.158928394317627\r\n4.290842294692993\r\n4.392872095108032\r\n4.166977643966675\r\n4.170665264129639\r\n4.235572338104248\r\n4.269734621047974\r\n4.34883189201355\r\n4.520952224731445\r\n4.301889181137085\r\n4.056428909301758\r\n4.011194705963135\r\n4.090587854385376\r\n4.21883487701416\r\n3.9880824089050293\r\n4.0440967082977295\r\n3.994976758956909\r\n4.007293462753296\r\n4.254953145980835\r\n5.212424993515015\r\n4.9619362354278564\r\n4.595417499542236\r\n4.669175386428833\r\n4.696617603302002\r\n4.930814743041992\r\n4.703015089035034\r\n4.683274984359741\r\n4.6691296100616455\r\n4.3999340534210205\r\n4.405350208282471\r\n4.167468070983887\r\n4.070374250411987\r\n4.114803791046143\r\n4.179715871810913\r\n4.213245153427124\r\n4.385056257247925\r\n4.23463773727417\r\n4.659383296966553\r\n4.66578221321106\r\n4.852110862731934\r\n5.042337894439697\r\n4.795732498168945\r\n4.809434413909912\r\n4.756434440612793\r\n4.5798728466033936\r\n4.447065591812134\r\n4.5793137550354\r\n4.24381685256958\r\n4.325646638870239\r\n4.384204149246216\r\n4.364959955215454\r\n4.5481116771698\r\n4.412975311279297\r\n4.33056902885437\r\n4.3485658168792725\r\n4.4717981815338135\r\n4.52445387840271\r\n4.318544864654541\r\n4.321290016174316\r\n4.327164173126221\r\n4.34274697303772\r\n4.447530746459961\r\n4.646075963973999\r\n4.394567489624023\r\n4.4357380867004395\r\n4.825327157974243\r\n4.836164712905884\r\n4.7376227378845215\r\n4.435471296310425\r\n4.40126895904541\r\n4.362128496170044\r\n4.400597333908081\r\n4.511887550354004\r\n4.704751968383789\r\n4.463193416595459\r\n4.490573406219482\r\navg time: 4.335829420089722\r\n```\r\ntf-nightly 1.5.0-dev20171120\r\n```sh\r\n2017-12-09 23:05:10.774382: I C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nload_model_time: 2.72861909866333\r\n3.9689061641693115\r\n3.729848861694336\r\n3.7932608127593994\r\n3.8207285404205322\r\n3.763514995574951\r\n3.877187490463257\r\n4.4169745445251465\r\n3.9273617267608643\r\n4.127046346664429\r\n4.061264276504517\r\n3.8851358890533447\r\n3.76558518409729\r\n3.738703966140747\r\n3.996413469314575\r\n4.546724319458008\r\n4.887501955032349\r\n4.631069183349609\r\n4.650303363800049\r\n4.602174282073975\r\n4.5963311195373535\r\n4.412041187286377\r\n4.993051052093506\r\n4.62782883644104\r\n4.496688365936279\r\n5.15169620513916\r\n5.236356019973755\r\n5.24039101600647\r\n4.847805500030518\r\n4.775449275970459\r\n4.996822357177734\r\n4.612450122833252\r\n4.5592358112335205\r\n4.247550964355469\r\n4.103948354721069\r\n4.132838726043701\r\n4.293198347091675\r\n4.7483580112457275\r\n5.134133815765381\r\n5.15913462638855\r\n5.087244272232056\r\n5.163210153579712\r\n5.42501163482666\r\n5.094269037246704\r\n5.124969720840454\r\n5.191107749938965\r\n5.19844126701355\r\n5.449011325836182\r\n5.687740087509155\r\n5.123981237411499\r\n5.205975770950317\r\n5.159977674484253\r\n5.085730791091919\r\n5.414042234420776\r\n4.894624948501587\r\n4.936810255050659\r\n4.920342445373535\r\n4.976094961166382\r\n5.166353225708008\r\n4.945138454437256\r\n5.000190019607544\r\n4.68733286857605\r\n4.466451644897461\r\n4.775496244430542\r\n4.483717918395996\r\n4.404090881347656\r\n5.27473783493042\r\n5.069267749786377\r\n5.367236137390137\r\n5.0340306758880615\r\n5.162541627883911\r\n5.210822343826294\r\n5.213692665100098\r\n5.316195249557495\r\n5.565897703170776\r\n5.242975234985352\r\n5.1944825649261475\r\n5.242218255996704\r\n5.385622262954712\r\n5.82612681388855\r\n5.615445613861084\r\n5.259502649307251\r\n5.338505029678345\r\n5.577911615371704\r\n5.083059310913086\r\n4.7085652351379395\r\n4.625895738601685\r\n4.646713972091675\r\n4.708309888839722\r\n4.907604694366455\r\n4.568413496017456\r\n5.159384489059448\r\n5.245308876037598\r\n5.268030405044556\r\n5.592416763305664\r\n5.172940492630005\r\n4.931399583816528\r\n4.745778322219849\r\n4.7465596199035645\r\n4.80203104019165\r\n4.963375568389893\r\navg time: 4.833953714370727\r\n```", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "i am seeing a similar issue. tensorflow 1.4 and 1.5 is 2x slower than 1.3 on windows with 1080ti", "@ebrevdo @reedwm any further comments?", "@WenmuZhou is this still an issue in the most recent nightlies?", "My network is not very good recently, and I will conduct this test on February 25, then you can also use this [code](https://drive.google.com/open?id=1WbjbJxzJRyYtxvGoZhyeVe2FgwEoc5c1) to test  ", "@WenmuZhou : As an aside, you might want to consider alternatives to BasicLSTMCell. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/performance_guide.md#rnn-performance (the contents will come to www.tensorflow.org on the next release)", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", " @asimshankar  the RNN cell I used is BasicLSTMCell", "@WenmuZhou : The performance guide is now published at https://www.tensorflow.org/performance/performance_guide#rnn_performance - do take a look there. It depends on your model needs, but if you can use `CudnnRNN`, you should do so.\r\n\r\nWe are working to make this easier in the future, but till then do see the recommendations in the guide linked to above.\r\n(FYI @drpngx )", "I'm also seeing this problem. I was using Bi-LSTM for sequence labeling (using `LSTMCell`) and speed on an Nvidia K80  GPU was halved upgrading from TF 1.3 to TF 1.5. I understand it's recommended to move to more optimized cell implementations, but are there plans to get the performance of `LSTMCell`/`BasicLSTMCell` back to where it was in TF 1.3? ", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is the slowdown for a single direct call to cell, or in dynamic_rnn?\n\nOn Wed, Mar 28, 2018, 4:27 AM hillelt <notifications@github.com> wrote:\n\n> I'm also seeing this problem. I was using Bi-LSTM for sequence labeling\n> (using LSTMCell) and speed on an Nvidia K80 GPU was halved upgrading from\n> TF 1.3 to TF 1.5. I understand it's recommended to move to more optimized\n> cell implementations, but are there plans to get the performance of\n> LSTMCell/BasicLSTMCell back to where it was in TF 1.3?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15057#issuecomment-376853263>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzkuOmtpTopwfQ47Rf_1qNj09NAAks5ti3OdgaJpZM4QzVan>\n> .\n>\n", "I didn't time a single direct call, but `dynamic_rnn` is two times slower.", "What about static_rnn?\n\nOn Tue, Apr 17, 2018, 9:28 AM hillelt <notifications@github.com> wrote:\n\n> I didn't time a single direct call, but dynamic_rnn becomes twice as slow.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15057#issuecomment-382055639>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7_TMqaDOYZSSSuLYFbZ8PlvmbhVks5tphgmgaJpZM4QzVan>\n> .\n>\n", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 91 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 106 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I've seen the issue was closed, but what was the solution?\r\nDo I have to rollback to TF1.3 to get an good performance in this situations?\r\nThanks =)", "@arruda  I think it was closed due to no update from the original poster. If you're still seeing slow-downs, I'd say you open a new issue, giving test cases and testing especially with newest TF versions.\r\n\r\n"]}, {"number": 15056, "title": "tf.losses.mean_squared_error is actually sum of squares", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171124\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nTo truly get mean squared error one has to explictly use `reduction=Reduction.MEAN`. This indicates bad naming. Maybe `squared_error` is a better name, also similar to other names in `tf.losses`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "My bad. `tf.losses.mean_squared_error` is indeed __mean__ square error. The bad naming is actually the default `reduction=Reduction.SUM_BY_NONZERO_WEIGHTS`. I thought it is a sum but this reduction method is actually an average over nonzero weights."]}, {"number": 15055, "title": "update Android libs", "body": "For android sample:\r\n- improve .gitignore\r\n- upgrade build tools to current most recent stable 3.0.1\r\n- add corresponding gradle", "comments": ["Can one of the admins verify this patch?", "15055", "Jenkins, test this please.", "Some kind of infra failure.\r\n\r\nJenkins, test this please.", "Failed to git pull :(\r\n\r\nJenkins, test this please.", "Jenkins, test this please.", "Can I do here something ? (It has a kind of infra failure)", "No, sorry, we're working on another breakage, we'll get back to regular issues soon.\r\n\r\nJenkins, test this please.", "@tensorflow-jenkins test this please"]}, {"number": 15054, "title": "~", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 15053, "title": "Increase test size to prevent timeout", "body": "", "comments": []}, {"number": 15052, "title": "Branch 177236996", "body": "", "comments": []}, {"number": 15051, "title": "DataLossError ; Input/output error", "body": "```\r\nDataLossError (see above for traceback): Unable to open table file C:\\Users\\jk-pc\\Desktop\\little_fighter_project\\log\\little-figter-2-0.001-alexnetv2-3-epochs.model: Unknown: NewRandomAccessFile failed to Create/Open: C:\\Users\\jk-pc\\Desktop\\little_fighter_project\\log\\little-figter-2-0.001-alexnetv2-3-epochs.model : \ufffds\ufffd\ufffd\ufffdQ\ufffd\u06a1C\r\n\r\n; Input/output error\r\n\t [[Node: save_1/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_1/Const_0_0, save_1/RestoreV2_2/tensor_names, save_1/RestoreV2_2/shape_and_slices)]]\r\n```\r\nhow can this be fixed? the model can not be loaded", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 15050, "title": "upgraded tensorflow to use GPU - she's a no worky anymore", "body": "Script was running fine (if a bit slow).  Decided it was  time to upgrade to the gpu version of tensorflow.  From what I can tell, for some reason it is looking for libcublas.so.8.0.  Since I just upgraded to the latest cuda libs, I'm running libcublas.so.9.0.176  (found in /usr/local/cuda-9.0/lib64/) and, yes, that is in my path (first item, by the way).  \r\n\r\n\r\nUsing TensorFlow backend.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mitch/MitchEng/data_Analytics/conv_radar.py\", line 3, in <module>\r\n    from keras.models import Sequential\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/keras/utils/conv_utils.py\", line 3, in <module>\r\n    from .. import backend as K\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/keras/backend/__init__.py\", line 83, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/mitch/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sorry,  Didn't see a template.  I'll work from your list\r\n\r\nHave I written custom code ?  Of course, But the code from above (causing the issue) is copy/paste from Keras.  Just trying to get GPU version up and running.\r\nOS Platform and Distibution - Linux Ubuntu 1604\r\nTensorFlow installed from - several places, most recently from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp27-none-linux_x86_64.whl (that's the one above)\r\nTensorFlow version - 1.4\r\nCuda/cuDNN version - cuda v 9.0/cuDNN version 7.0.4\r\nGPU model and memory - nvidia GeForce GTX 960M\r\nExact command to reproduce - from keras.models import Sequential", "Hi,\r\nthe prebuilt binaries are using cuda 8.0 and cudnn 6.0 as far as I know. That means you either need to build tensorflow from source or you try to create a symlink pointing from  libcublas.so.9.0 to  libcublas.so.8.0. However, with the latter you might run into problems later on. Another possibility is to download cuda/cudnn into some local folder and set the LD_LIBRARY_PATH to first search in this folder.", "Phil,\n\nThanks, that's pretty much what I was thinking, except I was hoping to hear that you were re-compiling the prebuilt binaries so that they would include \"Anything less than ...\".\n\nI'm relatively new to Python, so I don't know whether that is even doable, unless they're actually gcc compiled libraries, in which case, it should be doable.  However, I was able to get it going by maneuvering my environment around (re-installing etc...).\n\nFYI, pointing cudnn 6 at cudnn 7 targets didn't work.  Started to run, but tailed off to the infamous \"la la land\".  I haven't tried to create the old update-alternatives solution as yet.\n\nThanks for writing back though.  It did fortify my theories.   I may just bite the bullet and drag down the build env so I can build from source and then it will work with my system as a given.\n\nhave a GREAT day,\n\nMitch\n________________________________\nFrom: Phil <notifications@github.com>\nSent: Tuesday, December 5, 2017 1:33 AM\nTo: tensorflow/tensorflow\nCc: Moldy01; Author\nSubject: Re: [tensorflow/tensorflow] upgraded tensorflow to use GPU - she's a no worky anymore (#15050)\n\n\nHi,\nthe prebuilt binaries are using cuda 8.0 and cudnn 6.0 as far as I know. That means you either need to build tensorflow from source or you try to create a symlink pointing from libcublas.so.9.0 to libcublas.so.8.0. However, with the latter you might run into problems later on. Another possibility is to download cuda/cudnn into some local folder and set the LD_LIBRARY_PATH to first search in this folder.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F15050%23issuecomment-349232010&data=02%7C01%7C%7Cbcbbf7c37dc4400f278308d53bbaed62%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636480596384299918&sdata=WnC7fNQjG4TtaHcXSCyCysGxPG%2FsMxkXmJq6FNtyKGE%3D&reserved=0>, or mute the thread<https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FADu0acCj77SXheQN0gwKChx0F1542mKVks5s9P_zgaJpZM4Qy9p_&data=02%7C01%7C%7Cbcbbf7c37dc4400f278308d53bbaed62%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636480596384299918&sdata=N6%2FgG0iOTMaJnh%2Fzke48PhibDnrVHfMwR1CxwIZhiuY%3D&reserved=0>.\n", "Hi Moldy,\r\nThe tensorflow core is written in cpp so tf is gcc compiled. However, I'm don't quite get what you mean with re-compiling, as this is not possible from the given binar and you'd need the source code anyway.\r\n\r\nHowever, you could still try the last option I mentioned, to download cuda and cudnn manually and store them locally.\r\nThat is:\r\n1. create a folder /home/USERNAME/lib/\r\n2. download the cuda 8 installer and install it to  /home/USERNAME/lib/cuda-8.0\r\n3. download cudnn6 and extract the compressed file. There you'll find a folder structure that resembles the one found in /home/USERNAME/lib/cuda-8.0. Copy the files to cuda-8.0, keeping the directory structure.\r\n4. Prepend the lib folder to your LD_LIBRARY_PATH, i.e.\r\n` export LD_LIBRARY_PATH=/home/USERNAME/lib/cuda-8.0/lib64:$LD_LIBRARY_PATH`. Or even better, put this line into your `.bashrc` so you don't have to repeat step 4 every time you want to use tensorflow.\r\n\r\nIf any of the methods work for you, please consider closing this issue.\r\n\r\nCheers,\r\nPhil", "Phil,\n\nThanks for the info.  Sometimes I wish I had the time to contribute.  As for the \"re-compiling\" to which you refer.  I've seen it done before, however, I'm afraid I don't remember the technique used.  It is possible for the end binary to accept \"either\" xxx.so.8, xxx.so.7, xxx.so.6, etc.   The reason for my preface, is that your next question would undoubtedly be to query me for the technique, which as I write this, I'm afraid is eluding my memory searches at the moment.\n\nThat being said, I have to admit that the vast majority of my software experience comes on the Windows platforms, but if you think about it, if it were NOT possible, then how do windows apps cross so many versions??\n\nAs for using the last method: I'm afraid that I work with several other \"software creators\" (engineers, algorithm designers, programmers, etc...).  So, in order for me to pass along a \"setup\" for each to install so that their contributions will work with what I check in, implies that it is somewhat more generic.  I must put together a \"process\" for including my efforts to their own environments, which will include the end product as well (it is an embedded application).\n\nI can (and have done that), I was merely passing along the desire to utilize the latest available tensorflow WITH the latest available Cuda, cuDNN, etc...  We, attempt to make our embedded environment as recent as possible at the time of the first production unit.  Upgrades become difficult as the number of purchased units increases.\n\nAnd, yes, I have already started the process to set up my environment to build tensorflow from scratch, but will also admit that this is doubtful to be the best answer, as it then requires us (my company - well me right now) to do my own rebuilds each time something else change.\n\nThanks again for you willingness to follow up and for your good suggestions,\n\nMitch\n________________________________\nFrom: Phil <notifications@github.com>\nSent: Wednesday, December 6, 2017 1:20 AM\nTo: tensorflow/tensorflow\nCc: Moldy01; Author\nSubject: Re: [tensorflow/tensorflow] upgraded tensorflow to use GPU - she's a no worky anymore (#15050)\n\n\nHi Moldy,\nThe tensorflow core is written in cpp so tf is gcc compiled. However, I'm don't quite get what you mean with re-compiling, as this is not possible from the given binar and you'd need the source code anyway.\n\nHowever, you could still try the last option I mentioned, to download cuda and cudnn manually and store them locally.\nThat is:\n\n  1.  create a folder /home/USERNAME/lib/\n  2.  download the cuda 8 installer and install it to /home/USERNAME/lib/cuda-8.0\n  3.  download cudnn6 and extract the compressed file. There you'll find a folder structure that resembles the one found in /home/USERNAME/lib/cuda-8.0. Copy the files to cuda-8.0, keeping the directory structure.\n  4.  Prepend the lib folder to your LD_LIBRARY_PATH, i.e.\nexport LD_LIBRARY_PATH=/home/USERNAME/lib/cuda-8.0/lib64:$LD_LIBRARY_PATH. Or even better, put this line into your .bashrc so you don't have to repeat step 4 every time you want to use tensorflow.\n\nCheers,\nPhil\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F15050%23issuecomment-349566966&data=02%7C01%7C%7C4c2ba30ab7114b15504408d53c823f2f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636481452453004762&sdata=dot%2FGn4NNHzvrdqr722m0DZxBDMLkDXezCzsyC%2F0EIY%3D&reserved=0>, or mute the thread<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FADu0aT6pcrfKKrrsNQaXcEpPuRUzIWPrks5s9k5bgaJpZM4Qy9p_&data=02%7C01%7C%7C4c2ba30ab7114b15504408d53c823f2f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636481452453004762&sdata=10wyhkgO1uWtJ4D71AmnMHWdheIe5q6hOdTDEdbG3pQ%3D&reserved=0>.\n", "So, reading the thread, it looks like the root cause is the wrong cublas version. I'm closing this, feel free to reopen if you think it's a bug in tensorflow.", "Phil,\n\nI'm sorry.  I guess I was not all that clear.  I was not reporting a bug, it should have been thought of as an enhancement request.  Close if you like, I've got my work around, and am even considering setting up to build my own.  It is just nicer to be able to download an executable, and not bother.\n\nthanks for your attentive responses and suggestions,\n\nMitch\n________________________________\nFrom: drpngx <notifications@github.com>\nSent: Friday, December 8, 2017 6:42 PM\nTo: tensorflow/tensorflow\nCc: Moldy01; Author\nSubject: Re: [tensorflow/tensorflow] upgraded tensorflow to use GPU - she's a no worky anymore (#15050)\n\n\nClosed #15050<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F15050&data=02%7C01%7C%7Cf61482d219fa4c2b8f3e08d53ea62dbb%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636483805800061512&sdata=1xmM2jU%2FrzJEcSfJi%2BE9B%2FtRmEPxMxOHMJjNvXPyPv4%3D&reserved=0>.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F15050%23event-1379827961&data=02%7C01%7C%7Cf61482d219fa4c2b8f3e08d53ea62dbb%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636483805800061512&sdata=LeLZif1urHXOmA%2F90ItaI%2BW15MI1kDIm6NBE0bUSkwE%3D&reserved=0>, or mute the thread<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FADu0aSWgay-Ds3YLnIqWFOvJSIFLd2cSks5s-eWfgaJpZM4Qy9p_&data=02%7C01%7C%7Cf61482d219fa4c2b8f3e08d53ea62dbb%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636483805800061512&sdata=Bo7GfjLX%2FNcNGJX7r8LomlZ3mAJ2yjq70QxOvS2Vlcg%3D&reserved=0>.\n"]}, {"number": 15049, "title": "No OpKernel was registered to support Op 'OneHot' with these attrs. Tensorflow1.4", "body": "I have trained and saved a CNN model in python Tensorflow 1.3. \r\nI can successfully load and run the graph previously saved from my python model in Tensorflow 1.4 using CPU and c++ with no problem; but when I tried to load the same graph using Tensorflow 1.4 using GPU c++ and I get the following error:\r\n\r\n`-\t\tstatus\t{state_=unique_ptr {code=INVALID_ARGUMENT (3) msg=\"No OpKernel was registered to support Op 'OneHot' with these attrs.  Registered devices: [CPU,GPU], Registered kernels:\\n  <no registered kernels>\\n\\n\\t [[Node: one_hot = OneHot[T=DT_FLOAT, TI=DT_INT32, _... } }\ttensorflow::Status\r\n`\r\n\r\nMy system:\r\nWindows 10\r\nCuda 8.0\r\nCudnn 6\r\ncmake cmake-3.9.4-win64-x64\r\nPython 3.5.2\r\nVS2015\r\n@cuevas1208", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler here it is:\r\n\r\nOS Platform and Distribution   = Windows 10 Pro 64-bit (build 15063) \r\nTensorFlow installed from        = https://github.com/tensorflow/tensorflow/archive/v1.4.0.zip\r\nTensorFlow version                  = 1.4.0\r\nBazel version                            = N/A I am using cmake cmake-3.9.4-win64-x64\r\nCUDA/cuDNN version              = Cuda 8.0/Cudnn 6\r\nGPU model and memory         = NVIDIA Quadro M4000, VRAM 8152MB\r\nExact command to reproduce  = N/A running it in visual studio 2015", "Could you give more details about your code and log? As far as I known, OneHot[T=DT_FLOAT, TI=DT_INT32] kernel is registered for GPU.", "As @facaiy stated, can you post a code sample to reproduce the problem? OneHot does have both CPU and GPU implementations, but the error messages implies it has no implementation.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "When creating a Project file using the CMake program, one_hot_op.cc and one_hot_op.h files are not included in the tf_core_kernels.vcxproj file.\r\n\r\nAfter the build is done manually, I can check the result properly by applying the simple C ++ code const-> one_hot-> clientsession class using the corresponding library and object file.\r\n\r\nHowever, a little more complex sample code\r\nUsing FifoQueue, QueueEnQueue\r\nSamples using FixedLengthRecordReader-> ReaderReadUpTo-> DecodeRaw-> OneHot will show abnormal termination in r1.5 version.\r\nThis sample performs normal operation in r1.3.\r\n\r\nThis sample works normally when the OneHot is disconnected, but it ends abnormally when connecting OneHot. (stack -> PollLoop, PoolEvent....)\r\n\r\nCMake generation will require one_hot_op.cc and one_hot_op.h files to be included.\r\nand OneHot ...", "@mrry can you resolved the CMake issue?", "You could try removing this line from `tf_core_kernels.cmake`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5386775e64aac0bb5020974122645da900bc312a/tensorflow/contrib/cmake/tf_core_kernels.cmake#L188\r\n\r\nI believe it was disabled on GPU due to an Eigen/MSVC compilation error, but perhaps the tools have advanced sufficiently to make it no longer a problem?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing for now due to a lack of a response to @mrry's comment. @BrightJoy @cuevas1208, feel free to reopen if @mrry's comment does not resolve the issue."]}, {"number": 15048, "title": "[DO NOT SUBMIT] TEST", "body": "", "comments": []}, {"number": 15047, "title": "no such package 'tensorflow/examples/image_retraining': BUILD file not found on package path", "body": "### System information\r\n- I have not written any of my own code. I downloaded Tensorflow through virtualenv as recommended by tensorflow\r\n- My OS is High Sierra on a macbook pro\r\n- : I am using python 2.7\r\n- **Bazel version Build label: 0.7.0-homebrew:\r\n\r\n### I'm following the tensorflow tutorial on how to retrain inception v3, which I have linked here; https://www.tensorflow.org/tutorials/image_retraining\r\n\r\nWhen I input the bazel build tensorflow/examples/image_retraining:retrain  it says there is no build file. How do i get the build file. Im new to tensorflow so if you could put the code in here that would be extremely helpful because I am new to coding and new to github itself\r\n\r\n### Source code / logs\r\n\r\nbazel build tensorflow/examples/image_retraining:retrain\r\n\r\n....................................................................................\r\nERROR: no such package 'tensorflow/examples/image_retraining': BUILD file not found on package path.\r\nINFO: Elapsed time: 2.443s\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Hi. I am in the exact same situation so an answer would be much appreciated. ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 131 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15046, "title": "stop gradients for weights in tf.losses", "body": "In the case that the weights given to tf.losses.* depend in some way on the model parameters, \r\nthe derivative of that loss also calculated with respect to the weights. \r\n(Stupid) minimal example:\r\n```python\r\nimport tensorflow as tf\r\nx = tf.constant(0)\r\nw = tf.get_variable(name=\"W\", shape=(), initializer=tf.zeros_initializer())\r\nL = tf.losses.mean_squared_error(x, x, weights=w)\r\ntf.train.AdamOptimizer().compute_gradients(L)\r\n```\r\nresults in \r\n```\r\n[(<tf.Tensor 'gradients/mean_squared_error/Mul_grad/tuple/control_dependency_1:0' shape=() dtype=float32>,\r\n  <tf.Variable 'W:0' shape=() dtype=float32_ref>)]\r\n```\r\n\r\nI would expect the weights to be considered constant for the calculation of a loss. In case you agree with me, I can make a PR that adds `stop_gradient` around the weights parameter.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n", "comments": ["Perhaps you'd better to use tf.constant for weight\u2026 instead of a variable. ", "In my case the weights are not constant, but calculated in the model. The above example is only supposed to be a minimal example to demonstrate the problem.", "You can convert a variable to tensor when used, for example, `weights_ = weights + 0` or `weights_ = weights * 1`, and then pass `weights_` to `mean_squared_error`.", "that wouldn't change anything. `weights + 0` is still differentiable w.r.t. `weights`. Also, just to clarify, my question is not how to stop `tf.losses.*` to derive at the weights, that would be a simple `tf.stop_gradient`, but whether it is conceptually correct that I would need to do so.\r\n\r\nHaving a weighted loss `L(x, w)` with values `x` and weights `w` both depending on a parameter vector `P`, which of the following would be considered correct\r\n```\r\ndL/dP = dL/dx * dx/dP\r\n```\r\nor\r\n```\r\ndL/dP = dL/dx * dx/dP + dL/dw * dw/dP\r\n```\r\n\r\nI would argue that the first one corresponds to the interpretation of  `w` as weights.", "Yup, the role of weight seems subtle when involved with variable. As said by you, we can pretend a variable as constant by tf.stop_gradients, so I'd prefer to the second one, which is more general. ", "Right, I also disagree that we should stop the gradients. I understand where you're coming from, but I think this would be confusing non-transparent behavior for people. Also, once you stop the gradient within the loss it's not possible to go back to previous behavior.", "Then it would make sense to include this in the documentation, making this behaviour deliberate instead of a design accident.", "Sounds good. Feel free to send a PR\n\nOn Wed, Dec 6, 2017, 9:26 AM ngc92 <notifications@github.com> wrote:\n\n> Then it would make sense to include this in the documentation, making this\n> behaviour deliberate instead of a design accident.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15046#issuecomment-349711600>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbRpxcgHiiTO_uYH9NGhQRj2ZpF06ks5s9s5EgaJpZM4QyzEP>\n> .\n>\n"]}, {"number": 15045, "title": "Updated Tensor flow, incompatible function parameters. (Float32>int)", "body": "Hi, \r\n\r\n### System information\r\nWindows 10\r\nTensorFlow installed using pip. \r\nI am using CDU \r\nPython 3.6.3 (using Spyder)\r\nTensorFlow version: 1.4.0\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution:\r\nTensorFlow installed from\r\nBazel version:NA\r\nCUDA/cuDNN version:NA\r\nGPU model and memory: using cpu.\r\nExact command to reproduce: \r\nactivate myenvrionment\r\nspyder \r\n\r\n\r\nThis code was written on 0.x tensorFlow, it could be an update issue, but I can't find the outdated function. \r\n\r\n`\r\n    \r\n    from keras import optimizers\r\n    from keras.models import Model\r\n    from keras.layers import Dropout, Lambda\r\n    from keras.layers import Input, average\r\n    from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose\r\n    from keras.layers import ZeroPadding2D, Cropping2D\r\n    from keras import backend as K\r\n    \r\n    \r\n    def mvn(tensor):\r\n        '''Performs per-channel spatial mean-variance normalization.'''\r\n        epsilon = 1e-6\r\n        mean = K.mean(tensor, axis=(1,2), keepdims=True)\r\n        std = K.std(tensor, axis=(1,2), keepdims=True)\r\n        mvn = (tensor - mean) / (std + epsilon)\r\n        \r\n        return mvn\r\n    \r\n    \r\n    def crop(tensors):\r\n        '''\r\n        List of 2 tensors, the second tensor having larger spatial dimensions.\r\n        '''\r\n        h_dims, w_dims = [], []\r\n        for t in tensors:\r\n            b, h, w, d = K.get_variable_shape(t)\r\n            h_dims.append(h)\r\n            w_dims.append(w)\r\n        crop_h, crop_w = (h_dims[1] - h_dims[0]), (w_dims[1] - w_dims[0])\r\n        rem_h = crop_h % 2\r\n        rem_w = crop_w % 2\r\n        crop_h_dims = (crop_h / 2, crop_h / 2 + rem_h)\r\n        crop_w_dims = (crop_w / 2, crop_w / 2 + rem_w)\r\n        cropped = Cropping2D(cropping=(crop_h_dims, crop_w_dims))(tensors[1])\r\n        \r\n        return cropped\r\n    \r\n    \r\n    def dice_coef(y_true, y_pred, smooth=0.0):\r\n        '''Average dice coefficient per batch.'''\r\n        axes = (1,2,3)\r\n        intersection = K.sum(y_true * y_pred, axis=axes)\r\n        summation = K.sum(y_true, axis=axes) + K.sum(y_pred, axis=axes)\r\n        \r\n        return K.mean((2.0 * intersection + smooth) / (summation + smooth), axis=0)\r\n    \r\n    \r\n    def dice_coef_loss(y_true, y_pred):\r\n        return 1.0 - dice_coef(y_true, y_pred, smooth=10.0)\r\n    \r\n    \r\n    def jaccard_coef(y_true, y_pred, smooth=0.0):\r\n        '''Average jaccard coefficient per batch.'''\r\n        axes = (1,2,3)\r\n        intersection = K.sum(y_true * y_pred, axis=axes)\r\n        union = K.sum(y_true, axis=axes) + K.sum(y_pred, axis=axes) - intersection\r\n        return K.mean( (intersection + smooth) / (union + smooth), axis=0)\r\n    \r\n    \r\n    def fcn_model(input_shape, num_classes, weights=None):\r\n        ''' \"Skip\" FCN architecture similar to Long et al., 2015\r\n        https://arxiv.org/abs/1411.4038\r\n        '''\r\n        if num_classes == 2:\r\n            num_classes = 1\r\n            loss = dice_coef_loss\r\n            activation = 'sigmoid'\r\n        else:\r\n            loss = 'categorical_crossentropy'\r\n            activation = 'softmax'\r\n    \r\n        kwargs = dict(\r\n            kernel_size=3,\r\n            strides=1,\r\n            activation='relu',\r\n            padding='same',\r\n            use_bias=True,\r\n            kernel_initializer='glorot_uniform',\r\n            bias_initializer='zeros',\r\n            bias_regularizer=None,\r\n            activity_regularizer=None,\r\n            kernel_constraint=None,\r\n            bias_constraint=None,\r\n            trainable=True,\r\n        )\r\n        \r\n        data = Input(shape=input_shape, dtype='float', name='data')\r\n        mvn0 = Lambda(mvn, name='mvn0')(data)\r\n        pad = ZeroPadding2D(padding=5, name='pad')(mvn0)\r\n    \r\n        conv1 = Conv2D(filters=64, name='conv1', **kwargs)(pad)\r\n        mvn1 = Lambda(mvn, name='mvn1')(conv1)\r\n        \r\n        conv2 = Conv2D(filters=64, name='conv2', **kwargs)(mvn1)\r\n        mvn2 = Lambda(mvn, name='mvn2')(conv2)\r\n    \r\n        conv3 = Conv2D(filters=64, name='conv3', **kwargs)(mvn2)\r\n        mvn3 = Lambda(mvn, name='mvn3')(conv3)\r\n        pool1 = MaxPooling2D(pool_size=3, strides=2,\r\n                        padding='valid', name='pool1')(mvn3)\r\n    \r\n        \r\n        conv4 = Conv2D(filters=128, name='conv4', **kwargs)(pool1)\r\n        mvn4 = Lambda(mvn, name='mvn4')(conv4)\r\n    \r\n        conv5 = Conv2D(filters=128, name='conv5', **kwargs)(mvn4)\r\n        mvn5 = Lambda(mvn, name='mvn5')(conv5)\r\n    \r\n        conv6 = Conv2D(filters=128, name='conv6', **kwargs)(mvn5)\r\n        mvn6 = Lambda(mvn, name='mvn6')(conv6)\r\n    \r\n        conv7 = Conv2D(filters=128, name='conv7', **kwargs)(mvn6)\r\n        mvn7 = Lambda(mvn, name='mvn7')(conv7)\r\n        pool2 = MaxPooling2D(pool_size=3, strides=2,\r\n                        padding='valid', name='pool2')(mvn7)\r\n    \r\n    \r\n        conv8 = Conv2D(filters=256, name='conv8', **kwargs)(pool2)\r\n        mvn8 = Lambda(mvn, name='mvn8')(conv8)\r\n    \r\n        conv9 = Conv2D(filters=256, name='conv9', **kwargs)(mvn8)\r\n        mvn9 = Lambda(mvn, name='mvn9')(conv9)\r\n    \r\n        conv10 = Conv2D(filters=256, name='conv10', **kwargs)(mvn9)\r\n        mvn10 = Lambda(mvn, name='mvn10')(conv10)\r\n    \r\n        conv11 = Conv2D(filters=256, name='conv11', **kwargs)(mvn10)\r\n        mvn11 = Lambda(mvn, name='mvn11')(conv11)\r\n        pool3 = MaxPooling2D(pool_size=3, strides=2,\r\n                        padding='valid', name='pool3')(mvn11)\r\n        drop1 = Dropout(rate=0.5, name='drop1')(pool3)\r\n    \r\n    \r\n        conv12 = Conv2D(filters=512, name='conv12', **kwargs)(drop1)\r\n        mvn12 = Lambda(mvn, name='mvn12')(conv12)\r\n    \r\n        conv13 = Conv2D(filters=512, name='conv13', **kwargs)(mvn12)\r\n        mvn13 = Lambda(mvn, name='mvn13')(conv13)\r\n    \r\n        conv14 = Conv2D(filters=512, name='conv14', **kwargs)(mvn13)\r\n        mvn14 = Lambda(mvn, name='mvn14')(conv14)\r\n    \r\n        conv15 = Conv2D(filters=512, name='conv15', **kwargs)(mvn14)\r\n        mvn15 = Lambda(mvn, name='mvn15')(conv15)\r\n        drop2 = Dropout(rate=0.5, name='drop2')(mvn15)\r\n    \r\n    \r\n        score_conv15 = Conv2D(filters=num_classes, kernel_size=1,\r\n                            strides=1, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=True,\r\n                            name='score_conv15')(drop2)\r\n        upsample1 = Conv2DTranspose(filters=num_classes, kernel_size=3,\r\n                            strides=2, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=False,\r\n                            name='upsample1')(score_conv15)\r\n        score_conv11 = Conv2D(filters=num_classes, kernel_size=1,\r\n                            strides=1, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=True,\r\n                            name='score_conv11')(mvn11)\r\n        crop1 = Lambda(crop, name='crop1')([upsample1, score_conv11])\r\n        fuse_scores1 = average([crop1, upsample1], name='fuse_scores1')\r\n        \r\n        upsample2 = Conv2DTranspose(filters=num_classes, kernel_size=3,\r\n                            strides=2, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=False,\r\n                            name='upsample2')(fuse_scores1)\r\n        score_conv7 = Conv2D(filters=num_classes, kernel_size=1,\r\n                            strides=1, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=True,\r\n                            name='score_conv7')(mvn7)\r\n        crop2 = Lambda(crop, name='crop2')([upsample2, score_conv7])\r\n        fuse_scores2 = average([crop2, upsample2], name='fuse_scores2')\r\n        \r\n        upsample3 = Conv2DTranspose(filters=num_classes, kernel_size=3,\r\n                            strides=2, activation=None, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=False,\r\n                            name='upsample3')(fuse_scores2)\r\n        crop3 = Lambda(crop, name='crop3')([data, upsample3])\r\n        predictions = Conv2D(filters=num_classes, kernel_size=1,\r\n                            strides=1, activation=activation, padding='valid',\r\n                            kernel_initializer='glorot_uniform', use_bias=True,\r\n                            name='predictions')(crop3)\r\n        \r\n        model = Model(inputs=data, outputs=predictions)\r\n        if weights is not None:\r\n            model.load_weights(weights)\r\n        sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\r\n        model.compile(optimizer=sgd, loss=loss,\r\n                      metrics=['accuracy', dice_coef, jaccard_coef])\r\n    \r\n        return model\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        model = fcn_model((100, 100, 1), 2, weights=None)\r\n    \r\n\r\n`\r\n\r\n# Spyder execution log:\r\nrunfile('E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py', wdir='E:/AB/cardiac-segmentation-master/cardiac-segmentation-master')\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-3-d1b60b53383b>\", line 1, in <module>\r\n    runfile('E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py', wdir='E:/AB/cardiac-segmentation-master/cardiac-segmentation-master')\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 710, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 101, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 197, in <module>\r\n    model = fcn_model((100, 100, 1), 2, weights=None)\r\n\r\n  File \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 162, in fcn_model\r\n    crop1 = Lambda(crop, name='crop1')([upsample1, score_conv11])\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\layers\\core.py\", line 651, in call\r\n    return self.function(inputs, **arguments)\r\n\r\n  File \"E:/AB/cardiac-segmentation-master/cardiac-segmentation-master/fcn_model.py\", line 36, in crop\r\n    cropped = Cropping2D(cropping=(crop_h_dims, crop_w_dims))(tensors[1])\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\", line 603, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 1874, in call\r\n    self.cropping[1][0]: -self.cropping[1][1],\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\r\n    name=name)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 706, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5429, in strided_slice\r\n    name=name)\r\n\r\n  File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n\r\n  **File \"C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\n\r\n**TypeError: Value passed to parameter 'begin' has DataType float32 not in list of allowed values: int32, int64****\r\n\r\n\r\nPlease help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 15044, "title": "rolling window batch operation for tf.data.Dataset", "body": "_This is a feature request_\r\n\r\nFor Datasets that represent a sequence or time series, it can be useful to have a Dataset op that creates a rolling window batch over the given Dataset. For example, if I have a tf.data.Dataset whose elements represent a time series (line breaks separate elements):\r\n```\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n```\r\nThe rolling window batch would create a dataset with the following elements (for window size 4 and stride 1):\r\n```\r\n1 2 3 4\r\n2 3 4 5\r\n3 4 5 6\r\n4 5 6 7\r\n5 6 7 8\r\n6 7 8 9\r\n```\r\nThis operation will be extremely useful for extracting sub sequences from a time series for training RNNs and Reinforcement Learning models.", "comments": ["@mrry, care to comment on this feature request?", "This sounds like a reasonable thing to add as a custom transformation in `tf.contrib.data`. If each of the time series are small and can fit in memory, you can generate the windows using `Dataset.flat_map()` as I suggested [here](https://github.com/tensorflow/tensorflow/issues/14906#issuecomment-347396872). However, it might make sense to support a similar transformation of unbounded-length streams.", "I've got a solution running on my machine. I'll need to do more testing before I submit a pull req.", "How would we like to handle the event that the window + stride does not divide evenly into the length of the dataset?\r\n\r\neg: [0,1,2,3,4] window=3 stride=1 --> [[0,1,2],[1,2,3],[2,3,4]] OK\r\neg: [0,1,2,3,4] window=3 stride=3 --> [[0,1,2][3,4]] (size of last window is smaller) \r\n                                                                or just [[0,1,2]] (end early)\r\nmy current solution just quits out early", "Ending early should definitely be an option. I think there can also be an option to pad shorter final sequence with a given value, or the last value. \r\n@mrry what do you think?", "just submitted a pull. This one just has the end early version. Looking into how to fill out the last bit.\r\n\r\nAlso don't try to use it with anything too big fit into RAM.", "Status?\r\n\r\nI did something like this via `tf.contrib.signal` for flat_map but it would be nice to have sliding windows builtin in `tf.data`:\r\n```python\r\ndef window(x, width=16, step=4, axis=-1):\r\n    x = tf.contrib.signal.frame(x, width, step, axis=axis)\r\n    n = tf.shape(x, out_type=tf.int64)[axis - 1]\r\n    return tf.data.Dataset.range(n).map(lambda i: tf.gather(x, i, axis=axis - 1))\r\n```\r\n  ", "@mrry  Hi, I opened #16123 to resolve the issue. Because I'm not familiar with bazel, I get into trouble when I try to move c++ implementation from `core` to `contrib`. Could you take a look? And any help will be appreciated.", "I don't know if this is performant but this solution using slices has been working for me (tests included in gist).\r\n\r\nhttps://gist.github.com/jperl/954631259eda0f81be750b67e25f9bc4\r\n\r\n**edit: Seems like I have just reproduced a subset of `tf.contrib.signal.frame` \ud83e\udd15   "]}, {"number": 15043, "title": "a", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 15042, "title": "Only install enum34 on Python <3.4 versions (Take 2)", "body": "Python 3.6 sometimes has issues with enum34 because the standard library\r\nrelies on enum features not in enum34 (see\r\nhttps://bitbucket.org/stoneleaf/enum34/issues/19/enum34-isnt-compatible-with-python-36\r\nfor more details).\r\n\r\nWe'll avoid the new versioning syntax in setuptools to allow old versions of setuptools to still work (see #14779)\r\n\r\nDo you mind taking a look @gunan @yifeif?", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15041, "title": "Gradient computation occupies too much memories in \"cnn (using while_loop) + lstm\" network", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GTX Titan X 12GB\r\n- **Bazel version**: No use \r\n- **Exact command to reproduce**: using python *.py\r\n\r\n### Describe the problem\r\nI'm using while_loop function to build a cnn because of the scale of input tensors. And put the cnn feature into a lstm structure. The problem is that if I only do the forward computatoin it is good, but if I add the backward computation `GradientDescentOptimizer().minimize(loss))` the memory is **significantly insufficient** . \r\nI tried to split the two part of the model -- cnn and lstm part, and both do well with the whole computation. I think this is the fact:\r\n* Single while_loop cnn net is treated as time distributed when computing gradient. Every temporary feature vector and gradients occupy the same memory location at each time step.\r\n* When coneected with a lstm, the cnn net will be treated as many subnet in computing gradients. At every time step in backward computation, it will occupy new memory for its temporary feature vector and gradients. The number of timesteps is very large so that the memory is significantly not enough.\r\n\r\nHere is a simplified code of my project. It will show my cnn+lstm structure:\r\n\r\n### Source code\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib as contrib\r\n\r\ndef vgg_m(input_layer, reuse=None):\r\n    # input shape [batch_size, 120, 120, 5]\r\n    with tf.variable_scope('vgg_m', reuse=reuse):\r\n        conv_1 = tf.layers.conv2d(input_layer, 96, [3, 3], padding='same', activation=tf.nn.relu, name='conv_1')\r\n        pool_1 = tf.layers.max_pooling2d(conv_1, 3, 2, padding='same', name='pool_1')\r\n        norm_1 = tf.layers.batch_normalization(pool_1, name='norm_1')\r\n\r\n        conv_2 = tf.layers.conv2d(norm_1, 256, [3, 3], padding='same', activation=tf.nn.relu, name='conv_2')\r\n        pool_2 = tf.layers.max_pooling2d(conv_2, 3, 2, padding='same', name='pool_2')\r\n        norm_2 = tf.layers.batch_normalization(pool_2, name='norm_2')\r\n\r\n        conv_3 = tf.layers.conv2d(norm_2, 512, [3, 3], padding='same', activation=tf.nn.relu, name='conv_3')\r\n\r\n        conv_4 = tf.layers.conv2d(conv_3, 512, [3, 3], padding='same', activation=tf.nn.relu, name='conv_4')\r\n\r\n        conv_5 = tf.layers.conv2d(conv_4, 512, [3, 3], padding='same', activation=tf.nn.relu, name='conv_5')\r\n        pool_5 = tf.layers.max_pooling2d(conv_5, 3, 2, padding='same', name='pool_5')\r\n\r\n        flatten_6 = contrib.layers.flatten(pool_5)\r\n        fc_6 = tf.layers.dense(flatten_6, 512, name='fc6')\r\n\r\n    return fc_6\r\n\r\n# [batch, time_step, image_h, image_w, image_channel\r\ninput_tensor = tf.random_normal([64, 150, 120, 120, 5], dtype=tf.float32)\r\n\r\n# vgg_m is a cnn net whose input and output size is [batch, 120, 120, 5], [batch, 512]\r\n# in order to create vgg_m variables for reuse later.\r\nvgg_m(input_tensor[:, 0, :, :, :])\r\n\r\ntime_steps = 150\r\ninitial_t = tf.constant(0, dtype=tf.int32)\r\ninitial_outputs = tf.TensorArray(dtype=tf.float32, size=time_steps)\r\n\r\ndef _should_continue(t, *args):\r\n    return t < time_steps\r\n\r\ndef _iteration(t, outputs_):\r\n    # compute cnn feature at time t\r\n    single_output = vgg_m(input_tensor[:, t, :, :, :], reuse=True)\r\n    outputs_ = outputs_.write(t, single_output)\r\n    return t+1, outputs_\r\n\r\n_, outputs = tf.while_loop(_should_continue, _iteration, [initial_t, initial_outputs])\r\n\r\n# transpose the batch dim and time dim to build a [batch, time_step, 512] feature and send to lstm\r\noutputs = tf.transpose(outputs.stack(), perm=[1, 0, 2])\r\noutputs = tf.reshape(outputs, [-1, 150, 512])\r\n\r\nlstm_cell = tf.nn.rnn_cell.BasicLSTMCell(512)\r\nlstm_outputs, lstm_state = tf.nn.dynamic_rnn(\r\n            lstm_cell,\r\n            outputs,\r\n            sequence_length=tf.constant(150, dtype=tf.int32, shape=[64]),\r\n            dtype=tf.float32,\r\n        )\r\n\r\n# not really a \"loss\", just perform an loss example\r\nloss = tf.reduce_max(tf.reduce_max(tf.reduce_max(lstm_outputs, -1), -1), -1)\r\ntrain_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(train_op)\r\n```\r\n\r\n### Log\r\n```\r\n2017-12-01 22:44:24.228695: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-01 22:44:24.228717: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-01 22:44:24.228725: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-01 22:44:24.228731: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-01 22:44:24.228736: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-12-01 22:44:26.620700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:\r\nname: GeForce GTX TITAN X\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:4c:00.0\r\nTotal memory: 11.92GiB\r\nFree memory: 11.81GiB\r\n2017-12-01 22:44:26.620728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0\r\n2017-12-01 22:44:26.620735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y\r\n2017-12-01 22:44:26.620742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:4c:00.0)\r\n2017-12-01 22:44:29.102674: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2017-12-01 22:44:29.102971: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2017-12-01 22:44:29.103146: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 748.02MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2017-12-01 22:44:39.103321: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 112.50MiB.  Current allocation summary follows.\r\n2017-12-01 22:44:39.103363: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):   Total Chunks: 3, Chunks in use: 0 768B allocated for chunks. 20B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2017-12-01 22:44:39.103372: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):   Total Chunks: 1, Chunks in use: 0 512B allocated for chunks. 4B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2017-12-01 22:44:39.103379: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):  Total Chunks: 1, Chunks in use: 0 1.0KiB allocated for chunks. 1.0KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2017-12-01 22:44:39.103384: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n...(many Chunks)\r\n2017-12-01 22:44:39.103507: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2017-12-01 22:44:39.103514: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2017-12-01 22:44:39.103521: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2017-12-01 22:44:39.103528: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 112.50MiB was 64.00MiB, Chunk State:\r\n2017-12-01 22:44:39.103536: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a3c0000 of size 1280\r\n2017-12-01 22:44:39.103542: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a3c0500 of size 256\r\n2017-12-01 22:44:39.103547: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x230a3c0600 of size 512\r\n...(many Chunks)\r\n2017-12-01 22:44:39.104420: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x25cc3c8000 of size 88473600\r\n2017-12-01 22:44:39.104425: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x25d1828000 of size 122539008\r\n2017-12-01 22:44:39.104430: I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size:\r\n2017-12-01 22:44:39.104438: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 19 Chunks of size 256 totalling 4.8KiB\r\n...(many Chunks)\r\n2017-12-01 22:44:39.104599: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 245891072 totalling 234.50MiB\r\n2017-12-01 22:44:39.104605: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 268435456 totalling 256.00MiB\r\n2017-12-01 22:44:39.104611: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 353894400 totalling 1.65GiB\r\n2017-12-01 22:44:39.104617: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 405815296 totalling 387.02MiB\r\n2017-12-01 22:44:39.104623: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 513720320 totalling 489.92MiB\r\n2017-12-01 22:44:39.104629: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 4294967296 totalling 4.00GiB\r\n2017-12-01 22:44:39.104635: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 11.22GiB\r\n2017-12-01 22:44:39.104644: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\r\nLimit:                 12050517197\r\nInUse:                 12049901824\r\nMaxInUse:              12049901824\r\nNumAllocs:                     297\r\nMaxAllocSize:           4294967296\r\n\r\n2017-12-01 22:44:39.104660: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ********x************************xxxxxxxxxxx********************************************************\r\n2017-12-01 22:44:39.104678: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[64,512,30,30]\r\n2017-12-01 22:44:49.104957: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 225.00MiB.  Current allocation summary follows.\r\n2017-12-01 22:44:49.105020: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):   Total Chunks: 3, Chunks in use: 0 768B allocated for chunks. 20B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2017-12-01 22:44:49.105043: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):   Total Chunks: 1, Chunks in use: 0 512B allocated for chunks. 4B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2017-12-01 22:44:49.105063: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):  Total Chunks: 1, Chunks in use: 0 1.0KiB allocated for chunks. 1.0KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n...(similar chunks)\r\n2017-12-01 22:44:49.107700: I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 4294967296 totalling 4.00GiB\r\n2017-12-01 22:44:49.107714: I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 11.11GiB\r\n2017-12-01 22:44:49.107731: I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\r\nLimit:                 12050517197\r\nInUse:                 11927362816\r\nMaxInUse:              12049901824\r\nNumAllocs:                     297\r\nMaxAllocSize:           4294967296\r\n\r\n2017-12-01 22:44:49.107769: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ********x************************xxxxxxxxxxx*******************************************************_\r\n2017-12-01 22:44:49.107799: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[64,60,60,256]\r\n2017-12-01 22:44:49.107877: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[64,512,30,30]\r\n         [[Node: while/vgg_m/conv_4/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](while/vgg_m/conv_3/Relu, while/vgg_m/conv_4/convolution/Enter)]]\r\n\r\n....(same thing occuring again and again util it is closed.\r\n```\r\n### Is this an unknown bug\r\nI saw a different processing with `loop_state` in gradients function, so is this an unknown bug with multiple loop operation(lstm contains loop as well)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "Description updated, and anyone could help?", "/CC @zheng-xq @ebrevdo, can you comment?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This seems not to be a bug so much as a question of how to structure architectures to reduce gradient memory usage.  Ask on StackOverflow and look at [rev_block](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/rev_block).", "btw; one of the main reasons your memory is blowing up may be this line:\r\n\r\n```python\r\n    single_output = vgg_m(input_tensor[:, t, :, :, :], reuse=True)\r\n```\r\n\r\ntranspose input_tensor to time, major, unstack it into a TensorArray, and use .read(t) on the TensorArray.  this should significantly reduce memory usage if you backprop into input_tensor.", "> btw; one of the main reasons your memory is blowing up may be this line:\r\n> \r\n> ```python\r\n>     single_output = vgg_m(input_tensor[:, t, :, :, :], reuse=True)\r\n> ```\r\n> \r\n> transpose input_tensor to time, major, unstack it into a TensorArray, and use .read(t) on the TensorArray. this should significantly reduce memory usage if you backprop into input_tensor.\r\n\r\nI had the same problem but i dont know why it will make the memory explode at compute_gradients or minimize. Can you explain why it will happen?\r\nAlso, can i use tf.slice instead of the indexing operator?", "Both tf.slice and the indexing operator take a big matrix and take out a\nchunk which is smaller.  Since the output of the backprop of this operation\nmust have the same shape as the original big matrix, it means it takes the\ngradient of the slice and pads it with zeros so it's the same shape as the\ninput into the slice/indexing operator.  Do this once for each row of the\noriginal matrix and you've turned your problem into O(n^2) compute and\nmemory (sometimes O(n) memory if you compile with XLA).  TensorArray avoids\nthis by first slicing your big input tensor into rows, and forward and\nbackprop shapes are the shapes of the rows of your original tensor.  So you\nshould try to use TensorArray when you're iterating over slices of any\ntensor, especially if performing backprop into those rows.\n\nOn Fri, Jan 17, 2020 at 10:36 PM Max Shek-wai Chu <notifications@github.com>\nwrote:\n\n> btw; one of the main reasons your memory is blowing up may be this line:\n>\n>     single_output = vgg_m(input_tensor[:, t, :, :, :], reuse=True)\n>\n> transpose input_tensor to time, major, unstack it into a TensorArray, and\n> use .read(t) on the TensorArray. this should significantly reduce memory\n> usage if you backprop into input_tensor.\n>\n> I had the same problem but i dont know why it will make the memory explode\n> at compute_gradients or minimize. Can you explain why it will happen?\n> Also, can i use tf.slice instead of the indexing operator?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15041?email_source=notifications&email_token=AANWFG2U6TK4EJAI7XGG4EDQ6KPQNA5CNFSM4EGJ2JQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJJRYJA#issuecomment-575872036>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFGYSKB7VA3B3TUJK5CLQ6KPQNANCNFSM4EGJ2JQA>\n> .\n>\n", "> Both tf.slice and the indexing operator take a big matrix and take out a chunk which is smaller. Since the output of the backprop of this operation must have the same shape as the original big matrix, it means it takes the gradient of the slice and pads it with zeros so it's the same shape as the input into the slice/indexing operator. Do this once for each row of the original matrix and you've turned your problem into O(n^2) compute and memory (sometimes O(n) memory if you compile with XLA). TensorArray avoids this by first slicing your big input tensor into rows, and forward and backprop shapes are the shapes of the rows of your original tensor. So you should try to use TensorArray when you're iterating over slices of any tensor, especially if performing backprop into those rows.\r\n> [\u2026](#)\r\n> On Fri, Jan 17, 2020 at 10:36 PM Max Shek-wai Chu ***@***.***> wrote: btw; one of the main reasons your memory is blowing up may be this line: single_output = vgg_m(input_tensor[:, t, :, :, :], reuse=True) transpose input_tensor to time, major, unstack it into a TensorArray, and use .read(t) on the TensorArray. this should significantly reduce memory usage if you backprop into input_tensor. I had the same problem but i dont know why it will make the memory explode at compute_gradients or minimize. Can you explain why it will happen? Also, can i use tf.slice instead of the indexing operator? \u2014 You are receiving this because you modified the open/close state. Reply to this email directly, view it on GitHub <#15041?email_source=notifications&email_token=AANWFG2U6TK4EJAI7XGG4EDQ6KPQNA5CNFSM4EGJ2JQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJJRYJA#issuecomment-575872036>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AANWFGYSKB7VA3B3TUJK5CLQ6KPQNANCNFSM4EGJ2JQA> .\r\n\r\nThanks! But why Tensorflow did it that way? It seems more natural to me not to do it this way...", "> Both tf.slice and the indexing operator take a big matrix and take out a chunk which is smaller. Since the output of the backprop of this operation must have the same shape as the original big matrix, it means it takes the gradient of the slice and pads it with zeros so it's the same shape as the input into the slice/indexing operator. Do this once for each row of the original matrix and you've turned your problem into O(n^2) compute and memory (sometimes O(n) memory if you compile with XLA). TensorArray avoids this by first slicing your big input tensor into rows, and forward and backprop shapes are the shapes of the rows of your original tensor. So you should try to use TensorArray when you're iterating over slices of any tensor, especially if performing backprop into those rows.\r\n> [\u2026](#)\r\n> On Fri, Jan 17, 2020 at 10:36 PM Max Shek-wai Chu ***@***.***> wrote: btw; one of the main reasons your memory is blowing up may be this line: single_output = vgg_m(input_tensor[:, t, :, :, :], reuse=True) transpose input_tensor to time, major, unstack it into a TensorArray, and use .read(t) on the TensorArray. this should significantly reduce memory usage if you backprop into input_tensor. I had the same problem but i dont know why it will make the memory explode at compute_gradients or minimize. Can you explain why it will happen? Also, can i use tf.slice instead of the indexing operator? \u2014 You are receiving this because you modified the open/close state. Reply to this email directly, view it on GitHub <#15041?email_source=notifications&email_token=AANWFG2U6TK4EJAI7XGG4EDQ6KPQNA5CNFSM4EGJ2JQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJJRYJA#issuecomment-575872036>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AANWFGYSKB7VA3B3TUJK5CLQ6KPQNANCNFSM4EGJ2JQA> .\r\n\r\nAlso, how about tf.gather() instead of TensorArray?", "Gradient of tf.gather has the same exact issues *except* when you're\ngathering from a tf.Variable directly.  Which you're probably not.\n\nOn Tue, Jan 21, 2020 at 8:43 PM Max Shek-wai Chu <notifications@github.com>\nwrote:\n\n> Both tf.slice and the indexing operator take a big matrix and take out a\n> chunk which is smaller. Since the output of the backprop of this operation\n> must have the same shape as the original big matrix, it means it takes the\n> gradient of the slice and pads it with zeros so it's the same shape as the\n> input into the slice/indexing operator. Do this once for each row of the\n> original matrix and you've turned your problem into O(n^2) compute and\n> memory (sometimes O(n) memory if you compile with XLA). TensorArray avoids\n> this by first slicing your big input tensor into rows, and forward and\n> backprop shapes are the shapes of the rows of your original tensor. So you\n> should try to use TensorArray when you're iterating over slices of any\n> tensor, especially if performing backprop into those rows.\n> \u2026 <#m_-8721515996843760003_>\n> On Fri, Jan 17, 2020 at 10:36 PM Max Shek-wai Chu *@*.***> wrote: btw;\n> one of the main reasons your memory is blowing up may be this line:\n> single_output = vgg_m(input_tensor[:, t, :, :, :], reuse=True) transpose\n> input_tensor to time, major, unstack it into a TensorArray, and use\n> .read(t) on the TensorArray. this should significantly reduce memory usage\n> if you backprop into input_tensor. I had the same problem but i dont know\n> why it will make the memory explode at compute_gradients or minimize. Can\n> you explain why it will happen? Also, can i use tf.slice instead of the\n> indexing operator? \u2014 You are receiving this because you modified the\n> open/close state. Reply to this email directly, view it on GitHub <#15041\n> <https://github.com/tensorflow/tensorflow/issues/15041>?email_source=notifications&email_token=AANWFG2U6TK4EJAI7XGG4EDQ6KPQNA5CNFSM4EGJ2JQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJJRYJA#issuecomment-575872036>,\n> or unsubscribe\n> https://github.com/notifications/unsubscribe-auth/AANWFGYSKB7VA3B3TUJK5CLQ6KPQNANCNFSM4EGJ2JQA\n> .\n>\n> Also, how about tf.gather() instead of TensorArray?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15041?email_source=notifications&email_token=AANWFGZ6QS7XNTLQBQUKZ2LQ67FI5A5CNFSM4EGJ2JQKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJSGLVY#issuecomment-577005015>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG4Z36ZBSZW5E5MZIF3Q67FI5ANCNFSM4EGJ2JQA>\n> .\n>\n"]}, {"number": 15040, "title": "raw video files to tfrecords (code integration)", "body": "Since I had to address the issue of converting large (and many) raw RGB video files (e.g. .avi, .mp4 etc.) into tfrecords for threaded/QueueRunner training during a research project in the past, I was wondering if my resulting code [1] could be of any help to the TensorFlow community. If not, I would make a feature request and possibly participate or support the implementaton. \r\n\r\nWhen I had to address this at an early stage of my project, I couldn't find any useful implementations. This seems to have remained unchanged as of now.\r\n\r\n[1] https://github.com/ferreirafabio/video2tfrecords\r\n\r\nThanks in advance.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, macOS Sierra 10.13.1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: I use the CPU version\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: -\r\n", "comments": ["@mrry, could you comment on this feature request?", "hi, any news?", "I'm not sure what the feature request is. Can you please clarify?\r\n\r\nIt would be great to add better support for handling common video formats, and it's possible that integrating `tf.data` would make it easier (e.g. by providing a way to extract frames from a video directly, without transcoding to TFRecords).\r\n\r\n", "I was thinking about providing a pipeline that receives common video formats as input and produces tfrecords as output. Some parameters (e.g. frames per batch sample, color-depth etc.) could allow for adjusting the data usage/efficiency when calling a transcoding method. \r\n\r\nRelating to your statement, I believe there certainly should be a way of directly feeding frames from raw video formats into the computation graph. However, I think both solutions (with and without tfrecords) target different use-cases. For example, one wants to make use of queue runners / memory front-loading and multi-threading which, based on my (mediocre) TensorFlow knowledge, is better served with tfrecords. Or am I underestimating something?\r\n\r\nThat's also where my implementation I stated above could play a role if you/the team agrees. If there's no general interest in integrating this feature into the framework, I think some TensorFlow users would at least profit from a reference to the above implementation in the documentation.", "I'd appreciate your comment @mrry ", "Example code is useful, but in the interests of keeping the core repository small, it's something we don't typically merge into the core. My general rule for this is: \"If it can be built entirely using public APIs, and there is no performance downside to doing so, it doesn't need to be in the repository.\"\r\n\r\nAs I mentioned above, if there is some augmentation to `tf.data` that makes it easier to work with video, I'd be interested in that. I think you might be confused about the roles of TFRecords and `tf.data`: TFRecords is simply a data format, and `tf.data` is a library for reading data in many formats, including TFRecords. `tf.data` can perform multithreaded and pipelined reads of many different formats, including TFRecords, but potentially also including various video formats.", "@mrry \r\nI see. Although I was familiar with the difference between tf.data and TFRecords, I appreciate the clarification. A few people using the video2tfrecords repo contacted me and said they were grateful for having found it (although it wasn't to find it). \r\n\r\nBecause of this \"symptom\", I believe the TensorFlow documentation should contain a collection of useful repos that allow users to look up typical workflows that appear in e.g. production or science and finding appropriate repos there, could save lots of their time. Based on my quick research, I think there isn't such a collection. \r\n\r\nWouldn't it be useful to have such a starting point for users and add the above repo to it? (I'm not interested in fame. However, people reaching out to me and telling me it was hard to find the repo speaks words I believe.)", "That sounds like it could be useful! I'll pass you on to @wolffg, who oversees tensorflow.org, and might have an answer.", "I'd appreciate your comment @wolffg", "@ferreirafabio   I think your code is at early stage? a lot of hard coding. ", "@scotthuang1989 Absolutely! I quote the readme: \r\n\r\n> This implementation was created during a research project and grew historically. Therefore, we invite users encountering bugs to pull-request a correction.\r\n\r\nTo be honest, I don't know what you're implying. I don't think stating the condition of the repository is an argument for why it isn't helpful to other people.", "I don't imply anything.  I have some similar modules to generate tfrecords from dataset. I am in the process of improving them.  I just read your code and ask a question...", "We are going to be revising the community pages in 2018.  Until then, we don't really have a spot for repos like this outside of community/welcome.md, which will likely be changed as part of the revamp and is not altogether very visible right now.\r\n\r\nA way to boost visibility for this solution is to ask and answer a Stack Overflow question about it, which has good search ranking. \r\n\r\nI'm closing this, as I think @mrry answered the original question (we are unlikely to absorb this code into the main repo).  The other issue, about finding a visible home for links to community repos, is on our roadmap already.  Thanks!", "> We are going to be revising the community pages in 2018. Until then, we don't really have a spot for repos like this outside of community/welcome.md, which will likely be changed as part of the revamp and is not altogether very visible right now.\r\n\r\nI'm curious; have spots for such purposes been created? Thanks!"]}, {"number": 15039, "title": "Why tensorflow don't support tf.float64 on some ops?", "body": "Reproducing experiment of some paper using tensorflow may don't have good performance. I have tried my best to keep the structure and hyper-parameters unchanged. Does anyone encounter similar problem? Is it the code wrong?  I wonder that `caffe` can use `float64` but tensorflow only can use `float32`,  so there are more inaccuracy on `gradients`, why tf don't support `tf.float64`?", "comments": []}, {"number": 15038, "title": "Why tensorflow don't support tf.float64 on some ops?", "body": "Reproducing experiment of some paper using tensorflow may don't have good performance. I have tried my best to keep the structure and hyper-parameters unchanged. Does anyone encounter similar problem? Is it the code wrong?  I wonder that `caffe` can use `float64` but tensorflow only can use `float32`,  so there are more inaccuracy on `gradients`, why tf don't support `tf.float64`?", "comments": ["Most model developers try to use the minimum precision necessary. Many (most?) models can be done with float32, and it is generally much faster. Adding float64 support everywhere doubles the amount of code to maintain and to have in a binary. That being said, if there is a strong reason, we would be happy to support float64 in spaces as needed. Could you elaborate on what you are trying to do?\r\n", "I just want to reproducing experiment of some paper, but I can't get good performance. I think it maybe the error of gradient due to `tf.float32`, such as `convolution` ops.  I wonder that small numerical error will accumulate and get bigger error, like butterfly effect.\r\n\r\nLike [DeepTextures](https://github.com/leongatys/DeepTextures), the authors use `vgg19` of `caffe` to calculate gradient of loss and use [L-BFGS-B](https://github.com/leongatys/DeepTextures/blob/master/DeepImageSynthesis/ImageSyn.py#L54)(L-BFGS-B use float64 to optimize) of  `scipy.optimize` and get [good performance](https://github.com/leongatys/DeepTextures/blob/master/Example.ipynb). \r\n\r\nBut I try to use `vgg19` of `tensorflow.slim` pretrained on `imagenet` to calculate gardient and `L-BFGS-B`,  it converges so fast and the performance is so bad. Maybe I think the precision of gradient is not enough and it causes `L-BFGS-B` converge so fast.\r\n\r\nAnd I have tried to increase the [weight](https://github.com/leongatys/DeepTextures/blob/master/Example.ipynb)(tex_weights = [1e9,1e9,1e9,1e9,1e9]) of loss to `1e30` and use smaller learning rate, such as `1e-12`, so that gradient of loss becomes bigger and the effect of fractional part may become smaller.  And then I find that `L-BFGS-B` converges slower.  I'm not sure whether it is precision that causes the speed of convergence but I will try `tf.float64` if `tensorflow` support `tf.float64`.", "@aselle, it is `tf.control_dependencies` causes `L-BFGS-B` converge so fast. It is hard to find this problem, see my [new issue](https://github.com/tensorflow/tensorflow/issues/15077). I'm very puzzled, the behavior of `tf.control_dependencies`  is so weird.", "@gauss-clb I am going to close this. Your initial ask about float64 support has been heard. For questions about numerical issues with neural networks, I suggest asking on stackoverflow."]}]