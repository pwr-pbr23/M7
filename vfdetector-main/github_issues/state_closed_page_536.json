[{"number": 37651, "title": "r2.2-rc1 cherry-pick request: [Intel MKL] Fixing a bug in concat", "body": "The MKL concat op could give wrong results if not fixed. This only affects TF-MKL and not stock TensorFlow.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37651) for more info**.\n\n<!-- need_author_consent -->", "Manually overriding CLA to yes because the changes are from an already merged PR https://github.com/tensorflow/tensorflow/pull/37586 (in master).", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37651) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 37650, "title": "Profiling information is not captured in tensorflow 2.0 version.", "body": "I wanted to collect profiling information using current master tensorflow branch though the code for profiling is present inside Keras model. Still when I try to visualize the model in tensorboard its not collecting the profiling information.\r\n\r\nBefore the start of the test the message is thrown that the Profiler session started but when we load the logs to the tensorboard it shows empty profiling information.\r\nprofiler_session.cc:225] Profiler session started.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nRequest you to provide colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. \r\n\r\nPlease, fill [Github issue Template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!", "I am using on Ubuntu. My TF version is 2.0", "@Priyankajaiswalintel \r\n\r\nWill it be possible to provide colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "The standalone code example:\r\nimport tensorflow.keras as keras\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nimport matplotlib.pyplot as plt\r\n\r\nplt.imshow(x_train[0],cmap=plt.cm.binary)\r\nplt.show()\r\nx_train = tf.keras.utils.normalize(x_train, axis=1)\r\nx_test = tf.keras.utils.normalize(x_test, axis=1)\r\nprint(x_train[0])\r\nplt.imshow(x_train[0],cmap=plt.cm.binary)\r\nplt.show()\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nlog_dir=\"/root/priyanka/profile\"\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, profile_batch=3)\r\ncallbacks_list=[tensorboard_callback]\r\nprint(\"entering training\")\r\nmodel.fit(x_train, y_train, epochs=3,verbose=1,callbacks=callbacks_list)\r\n\r\nI am trying to run the above code it gives me event file but where I am trying to load the generated logs to tensoboard its not giving me any profiling information when I press the profiling tab in tensorboard.\r\n tensorboard --logdir=/root/priyanka/profile/train/ --port=6009", "Looks like you already have a profile generated! \r\nThe reason that you cannot see a trace maybe related to https://github.com/tensorflow/tensorboard/issues/3209. We have that fixed in TB 2.2 which should be released soon. To use TB 2.0, can you try relaunch your Chrome with --enable-blink-features=ShadowDOMV0,CustomElementsV0,HTMLImports ?\r\n\r\nAlternatively you can try install the latest tb-nightly and tensorboard_plugin_profile and see if that fix your issue.\r\n$ git clone https://github.com/tensorflow/profiler.git profiler\r\n$ mkdir profile_env\r\n$ python3 profiler/install_and_run.py --envdir=profile_env --logdir=/root/priyanka/profile/train/ --port=6009\r\n", "I am able to get the profiling data by using the above method thank you. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37650\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37650\">No</a>\n"]}, {"number": 37649, "title": "Can't disable TensorFlow logs", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Windows 10\r\n- TensorFlow installed from (source or\r\nbinary):  pip\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 2080 Super, 8gb\r\n\r\n**Describe the current behavior**\r\nCan't prevent logging, on either cpu or gpu.\r\n\r\n**Describe the expected behavior**\r\nShould be able to disable logs.\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport os, logging\r\nimport tensorflow as tf\r\n\r\n# One of these should prevent logs\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nlogging.disable(logging.WARNING)\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\nlogging.getLogger('tensorflow').setLevel(logging.FATAL)\r\ntf.get_logger().setLevel(logging.ERROR)\r\ntf.autograph.set_verbosity(1)\r\n\r\nx = tf.constant([[1.0]])\r\ntf.matmul(x, x)\r\n```\r\n\r\nOutput:\r\n```\r\n2020-03-16 14:23:41.863992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-03-16 14:23:43.322241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-03-16 14:23:43.368176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:91:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 48 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 462.00GiB/s\r\n2020-03-16 14:23:43.377139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-03-16 14:23:43.384321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-03-16 14:23:43.391462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll \r\n2020-03-16 14:23:43.396430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-03-16 14:23:43.403926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-03-16 14:23:43.410194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-03-16 14:23:43.422736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-03-16 14:23:43.426995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-03-16 14:23:43.430185: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-03-16 14:23:43.437794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:91:00.0 name: GeForce RTX 2080 SUPER computeCapability: 7.5\r\ncoreClock: 1.815GHz coreCount: 48 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 462.00GiB/s\r\n2020-03-16 14:23:43.445464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-03-16 14:23:43.449487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-03-16 14:23:43.453326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-03-16 14:23:43.457145: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-03-16 14:23:43.461076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-03-16 14:23:43.464880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-03-16 14:23:43.468782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-03-16 14:23:43.473079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-03-16 14:23:44.093908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-03-16 14:23:44.098926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-03-16 14:23:44.101402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-03-16 14:23:44.105271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6265 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 SUPER, pci \r\nbus id: 0000:91:00.0, compute capability: 7.5)\r\n2020-03-16 14:23:44.115422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n```", "comments": ["Can you try setting log level before importing tf?\r\n```python\r\nimport os\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\r\nimport tensorflow as tf\r\n```", "Yes that works. A TensorFlow update broke this then? As I recall in some 1.x release doing the os setting worked after the tf import.\r\n\r\nAlso, this means that none of these currently work, do they:\r\n```\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\ntf.get_logger().setLevel(logging.ERROR)\r\ntf.autograph.set_verbosity(1)\r\n```", "I think you have to disable v2 behavior before using those lines.\r\nAlso see https://www.tensorflow.org/guide/effective_tf2#api_cleanup", "> Can you try setting log level before importing tf?\r\n> \r\n> ```python\r\n> import os\r\n> os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\r\n> import tensorflow as tf\r\n> ```\r\n\r\nThat worked for me. Thanks.\r\nusing Tensorflow version 2.3.0"]}, {"number": 37648, "title": "[r2.2:Cherrypick] Disable new batchnorm codepath if momentum is not a Python float or int.", "body": null, "comments": []}, {"number": 37647, "title": "Refactor DirectedInterleaveDatasetOp", "body": "This PR refactors `DirectedInterleaveDatasetOp` and adds the kernel tests.", "comments": ["@aaudiber Thanks for your review! The comments are addressed below:\r\n\r\n- https://github.com/tensorflow/tensorflow/pull/37647/commits/a8f9799bdaf71474b0d0627df3e9c4019767277b\r\nMore test cases are added (e.g. `OneInputDatasetParams`, `ZeroInputDatasetParams`, `LargeNumInputDatasetsParams`, `SmallNumInputDatasetsParams`). I'm sorry that I messed up the commit history when addressing the code conflicts.\r\n\r\n- https://github.com/tensorflow/tensorflow/pull/37647/commits/4aad621976d7ea528d7e4f94bc18fc9c907796f5\r\nIn the test cases `LargeNumInputDatasetsParams` and `SmallNumInputDatasetsParams`, the kernel runs well. That means, the number of input tensors can be different from the number of input names. That's because the kernel sometimes does not use the input name to parse the input tensor. Instead, the kernel directly accesses the input tensor vector.  Hence, revise `DatasetOpsTestBase::CreateDatasetContext(...)` to throw a warning instead of an error when `CheckOpKernelInput()` fails."]}, {"number": 37646, "title": "[r2.2:Cherrypick]Disable failing tests in py38 version.", "body": "PiperOrigin-RevId: 301195707\nChange-Id: I43f5fad45b709b4fe0230bbd98fe0122dbc80ea1", "comments": []}, {"number": 37645, "title": "AttributeError: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sumannelli, Please provide all the information asked in the Template. Thanks!", "@sumannelli,\r\nFill the issue template to reproduce the issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I am having the same issue ```AttributeError: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list' ```. Attaching the file by running the above script ```tf_env_collect.sh```\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5022812/tf_env.txt). ``` Tensorflow version 1.14``` (as required by ```baselines``` [repository](https://github.com/openai/baselines) )\r\n**Steps to reproduce**\r\n1. Clone the repo ```https://github.com/openai/baselines.git```\r\n2. ``` cd baselines ```  ``` pip install -e . ```\r\n3. ``` python -m baselines.run --alg=her --env=FetchReach-v1 --num_timesteps=500 ```\r\n\r\nThanks for any help in advance\r\n\r\n\r\n\r\n"]}, {"number": 37644, "title": "NFC - minor spelling tweaks under lite/kernels directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite/kernels` directory.\r\nfollow-on of #35286", "comments": ["cc @mihaimaruseac", "I may accidentally close this PR. I will recreate the PR."]}, {"number": 37643, "title": "Could not use TPU provided by Google Collab with custom training", "body": "I try to use TPU provided by Google Collab with custom training.\r\nANN-architecture - Transformer, with batch_size=128. I tested with GPU and all works fine. I wanted to speed up process and use larger batch. I modified code for TPU  but with TPU i got \"Your session crashed for an unknown reason\". I cant get it how to catch errors and etc.\r\nCan you help me cause I cant train ANN with TPU.\r\n\r\nTensorflow version: 2.1.0 (also i used \"%tensorflow_version 2.x\" in collab notebook)\r\n\r\nWarnings from logs:\r\n> 2020-03-16 16:22:04.297929: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 780573800 exceeds 10% of system memory. \r\n\r\n> 2020-03-16 16:22:03.324150: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n\r\n\r\nPreprocessing code\r\n```\r\ndef _read_and_batch_from_files(input_arr, target_arr, shuffle=True, shuffle_val=20):\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices((input_arr, target_arr))\r\n  if shuffle:\r\n    dataset = dataset.shuffle(buffer_size=6000, seed=shuffle_val,\r\n                              reshuffle_each_iteration=True)\r\n  else:\r\n    dataset = dataset.batch(base_model_conf_obj.batch_size)\r\n  dataset = dataset.repeat()\r\n  return dataset\r\n \r\ninput_arr_test = np.load('/content/gdrive/My Drive/for_ds/x_test_tok.npy', allow_pickle=True)\r\ntarget_arr_test = np.load('/content/gdrive/My Drive/for_ds/y_test_tok.npy', allow_pickle=True)\r\n# i used this just for test before use init training/test files\r\ntrain_dataset =  _read_and_batch_from_files(input_arr_test, target_arr_test, \r\n                                            shuffle=True, shuffle_val=20)\r\n```\r\n\r\nInitialize TPU-strategy\r\n```\r\nclass SingleDeviceStrategy(object):\r\n  def __enter__(self, *args, **kwargs):\r\n    pass\r\n\r\n  def __exit__(self, *args, **kwargs):\r\n    pass\r\n\r\n  def scope(self):\r\n    return self\r\n\r\n  def experimental_distribute_dataset(self, dataset):\r\n    return dataset\r\n\r\n  def experimental_run_v2(self, func, args, kwargs):\r\n    return func(*args, **kwargs)\r\n\r\n  def reduce(self, reduction_type, distributed_data, axis):  # pylint: disable=unused-argument\r\n    return distributed_data\r\n\r\ndef connect_to_tpu(tpu=None):\r\n  if tpu:\r\n    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu)\r\n    tf.config.experimental_connect_to_host(cluster_resolver.get_master())\r\n    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\n    strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\r\n    return strategy, \"/task:1\" if os.environ.get(\"COLAB_TPU_ADDR\") else \"/job:worker\"\r\n  return SingleDeviceStrategy(), \"\"\r\n```\r\n\r\nTraining procedure\r\n```\r\nworker_tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\nd_strat, device = connect_to_tpu(worker_tpu)\r\nwith d_strat.scope():\r\n  dataset_iter = iter(d_strat.experimental_distribute_dataset(train_dataset))\r\n  for epoch in range(starting_epoch, num_epochs):\r\n    start = time.time()\r\n    inp, tar = next(dataset_iter)\r\n    def tpu_step(inp, tar):\r\n      tar_inp = tar[:, :-1]\r\n      tar_real = tar[:, 1:]\r\n      enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\r\n      with tf.GradientTape() as tape:\r\n        predictions, _ = transformer(inp, tar_inp,\r\n                                        True,\r\n                                        enc_padding_mask,\r\n                                        combined_mask,\r\n                                        dec_padding_mask)\r\n        loss = loss_function(tar_real, predictions)\r\n        loss_ce = loss * (1.0 / base_model_conf.batch_size)\r\n      variables = transformer.trainable_variables\r\n      gradients = tape.gradient(loss_ce, variables)\r\n      gradients = [(tf.clip_by_value(grad, -1.0, 1.0))\r\n                        for grad in gradients]\r\n      train_loss.update_state(loss_ce)\r\n      train_op = optimizer.apply_gradients(zip(gradients, variables))\r\n      with tf.control_dependencies([train_op]):\r\n        return tf.cast(optimizer.iterations, tf.float32)\r\n\r\n    @tf.function\r\n    def train_step(inp, tar):\r\n      distributed_metric = d_strat.experimental_run_v2(tpu_step, args=[inp, tar])\r\n      step = d_strat.reduce(tf.distribute.ReduceOp.MEAN, distributed_metric, axis=None)\r\n      return step\r\n\r\n    step = tf.cast(train_step(inp, tar), tf.int32)\r\n    train_loss.reset_states()\r\n    train_loss_res = train_loss.result().numpy()\r\n    print('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss_res,))\r\n    print('Time taken for train 1 epoch: {} secs\\n'.format(time.time() - start))\r\n    ckpt_save_path = ckpt_manager.save()\r\n    print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))\r\n    print('Time taken for test 1 epoch: {} secs\\n'.format(time.time() - start))\r\n```", "comments": ["@pikaliov \r\ni have tried to replicate the code shared by you,please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/5d9fc3bcf1e959340e8934997e063881/37643.ipynb).\r\n\r\nplease note we cannot replicate the issue unless all dependencies are shared, along with the cuda version.\r\n\r\nas per the error faced, could you please try CUDA_VISIBLE_DEVICES=0\r\nas per error there are many similar issues already there,can you check [if this helps](https://github.com/tensorflow/tensorflow/issues/7653) [link](https://github.com/tensorflow/tensorflow/issues/16860#issuecomment-539952685)", "@pikaliov\r\nplease update as per above comment", "@pikaliov\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 37642, "title": "Error importing tensorflow", "body": "Everything seemed to install OK,   MacOS Catalina 10.15.3, Python 3.7.6, but the code failed  on its first line :\r\n       import tensorflow as tf\r\n with error message:\r\nModuleNotFoundError: No module named 'tensorflow.python.tools' \r\n\r\ntensorflow version 2.1.0 installed today with pip3 version 20.0.2, details in attached file\r\n\r\n[junk.txt](https://github.com/tensorflow/tensorflow/files/4339257/junk.txt)\r\n\r\n", "comments": ["Hey, @John-Wheater please see  #34722 your problem is similar to this thread. Tell if the solution describe in this thread works for you or not. If it works then kindly close this issue.", "@John-Wheater \r\n\r\nJust to verify did you follow instructions from [Tensorflow website](https://www.tensorflow.org/install/source). \r\n\r\nTry uninstalling and reinstalling:\r\npip uninstall tensorflow\r\nthen reinstall:\r\npip intall tensorflow==2.1.0\r\nAfter you uninstall, in the python shell, run:\r\n\r\nhelp('modules')\r\nTensorflow should not be there in that list. Only then proceed to install it.\r\nAlso see #34722 and see if it helps you.Thanks!\r\n", "I did the things advised by ravikyram, but the error is still there.  See attached file.   I notice frequent pip messages saying that a 'cached' version has been used.  Is that OK?  Or should I try and find out in MacOS how to clear its caches?\r\n\r\nFinally, mine does seem to be the same problem as #34722, so should I close it, or add my experience to it?  Sorry, I'm quite new to all this.\r\n\r\n[github-mar-17.txt](https://github.com/tensorflow/tensorflow/files/4342336/github-mar-17.txt)\r\n", "OK, I tried editing my PYTHONPATH, and the problem went away.  So I'll close this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37642\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37642\">No</a>\n", "@John-Wheater no need to say sorry. All of us were beginner in the beginning of learning any new technology. Feel free to ask about any issues. And Tensorflow community is always there to help you. "]}, {"number": 37641, "title": "TFL: Port HARD_SWISH operator from TFLite to TFLu", "body": "This is needed for mobilenet v3.\r\n\r\nThere is some duplicated code because of references to Gemmlowp. If those could be replaced, the tensorflow lite hard_swish operator could be moved out to its own reference/hard_swish.h that both Lite and Micro could include.", "comments": ["@mansnils can you please resolve conflicts ?", "@mansnils can you please resolve conflicts ?", "Conflicts resolved.", "Gentle ping for review", "@njeffrie ping for review", "@njeffrie Can we review this please?", "@mansnils can you please check below errors \r\n\r\n` tensorflow/lite/micro/kernels/activations.cc:216:22: error: implicit conversion increases floating-point precision: 'const float' to 'double' \r\n\r\n  QuantizeMultiplier(output_multiplier, &output_multiplier_fixedpoint_int32,\r\n  ~~~~~~~~~~~~~~~~~~ ^~~~~~~~~~~~~~~~~\r\n/tensorflow/lite/micro/kernels/activations.cc:225:22: error: implicit conversion increases floating-point precision: 'const float' to 'double' \r\n\r\n  QuantizeMultiplier(reluish_multiplier, &reluish_multiplier_fixedpoint_int32,\r\n  ~~~~~~~~~~~~~~~~~~ ^~~~~~~~~~~~~~~~~~\r\n\r\n/tensorflow/lite/micro/kernels/activations.cc:216:22: error: implicit conversion increases floating-point precision: 'const float' to 'double' \r\n\r\n  QuantizeMultiplier(output_multiplier, &output_multiplier_fixedpoint_int32,\r\n  ~~~~~~~~~~~~~~~~~~ ^~~~~~~~~~~~~~~~~\r\n\r\n/tensorflow/lite/micro/kernels/activations.cc:330:14: note: in instantiation of function template specialization 'tflite::ops::micro::activations::HardSwishQuantized<unsigned char>' requested here\r\n      return HardSwishQuantized<uint8>(context, node);\r\n             ^\r\n\r\n/tensorflow/lite/micro/kernels/activations.cc:225:22: error: implicit conversion increases floating-point precision: 'const float' to 'double'\r\n\r\n  QuantizeMultiplier(reluish_multiplier, &reluish_multiplier_fixedpoint_int32,\r\n  ~~~~~~~~~~~~~~~~~~ ^~~~~~~~~~~~~~~~~~\r\n\r\n/tensorflow/lite/micro/kernels/activations.cc:216:22: error: implicit conversion increases floating-point precision: 'const float' to 'double' \r\n\r\n  QuantizeMultiplier(output_multiplier, &output_multiplier_fixedpoint_int32,\r\n  ~~~~~~~~~~~~~~~~~~ ^~~~~~~~~~~~~~~~~\r\n\r\n/tensorflow/lite/micro/kernels/activations.cc:333:14: note: in instantiation of function template specialization 'tflite::ops::micro::activations::HardSwishQuantized<signed char>' requested here\r\n      return HardSwishQuantized<int8>(context, node);\r\n             ^\r\n\r\n/tensorflow/lite/micro/kernels/activations.cc:225:22: error: implicit conversion increases floating-point precision: 'const float' to 'double'\r\n\r\n  QuantizeMultiplier(reluish_multiplier, &reluish_multiplier_fixedpoint_int32,\r\n  ~~~~~~~~~~~~~~~~~~ ^~~~~~~~~~~~~~~~~~\r\n6 errors generated.`", "@rthadur Errors are resolved now.", "@mansnils I see this internal errors , please check \r\n\r\n`/tensorflow/lite/micro/kernels/activations_test.cc:175:16: error: variable length array used \r\n T output_data[size];\r\n              ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:176:25: error: variable length array used \r\n T input_data_quantized[size];\r\n                       ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:197:27: error: variable length array used \r\n float dequantized_output[output_elements_count];\r\n                         ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:306:16: error: variable length array used \r\n T output_data[size];\r\n              ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:307:25: error: variable length array used \r\n T input_data_quantized[size];\r\n                       ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:324:27: error: variable length array used \r\n float dequantized_output[output_elements_count];\r\n                         ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:397:20: error: variable length array used \r\n float output_data[size];\r\n                  ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:175:16: error: variable length array used \r\n T output_data[size];\r\n              ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:771:26: note: in instantiation of function template specialization 'tflite::testing::(anonymous namespace)::TestHardSwishQuantized<signed char>' requested here\r\n       tflite::testing::TestHardSwishQuantized<int8_t>(\r\n                        ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:176:25: error: variable length array used \r\n T input_data_quantized[size];\r\n                       ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:197:27: error: variable length array used \r\n float dequantized_output[output_elements_count];\r\n                         ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:175:16: error: variable length array used \r\n T output_data[size];\r\n              ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:773:26: note: in instantiation of function template specialization 'tflite::testing::(anonymous namespace)::TestHardSwishQuantized<unsigned char>' requested here\r\n       tflite::testing::TestHardSwishQuantized<uint8_t>(\r\n                        ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:176:25: error: variable length array used \r\n T input_data_quantized[size];\r\n                       ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:197:27: error: variable length array used \r\n float dequantized_output[output_elements_count];\r\n                         ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:306:16: error: variable length array used \r\n T output_data[size];\r\n              ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:786:20: note: in instantiation of function template specialization 'tflite::testing::(anonymous namespace)::TestHardSwishQuantizedBias<unsigned char>' requested here\r\n tflite::testing::TestHardSwishQuantizedBias<uint8_t>(\r\n                  ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:307:25: error: variable length array used \r\n T input_data_quantized[size];\r\n                       ^\r\n/tensorflow/lite/micro/kernels/activations_test.cc:324:27: error: variable length array used \r\n float dequantized_output[output_elements_count];\r\n`", "@mansnils Can you please check @rthadur's comments and resolve conflicts?. Thanks!", "@rthadur errors and conflics are resolved.", "Sorry for reversing approval.  I think I approved too early before.  There are a bunch of changes we will need to make to land this in TF Micro.", "@njeffrie Thanks for the comments. Changes are now updated.", "@mansnils sorry for the delay , still seeing below errors , can you please check \r\n\r\n`tensorflow/lite/micro/kernels/activations_test.cc:407:51: error: implicit conversion turns string literal into bool: 'const char [14]' to 'bool' \r\n     CreateFloatTensor(output_data, output_dims, \"output_tensor\"),\r\n\r\n\r\ntensorflow/lite/micro/kernels/activations_test.cc:406:57: error: implicit conversion turns string literal into bool: 'const char [13]' to 'bool' \r\n     CreateFloatTensor(float_input_values, input_dims, \"input_tensor\"),\r\n\r\n\r\ntensorflow/lite/micro/kernels/activations_test.cc:212:48: error: implicit conversion turns string literal into bool: 'const char [14]' to 'bool' \r\n                           output_zero_point, \"output_tensor\"),\r\n\r\n\r\ntensorflow/lite/micro/kernels/activations_test.cc:778:24: note: in instantiation of function template specialization 'tflite::testing::(anonymous namespace)::TestHardSwishQuantized<signed char>' requested here\r\n     tflite::testing::TestHardSwishQuantized<int8_t>(\r\n                      ^\r\n\r\ntensorflow/lite/micro/kernels/activations_test.cc:210:29: error: implicit conversion turns string literal into bool: 'const char [13]' to 'bool' \r\n                           \"input_tensor\"),\r\n                           ^~~~~~~~~~~~~~\r\ntensorflow/lite/micro/kernels/activations_test.cc:212:48: error: implicit conversion turns string literal into bool: 'const char [14]' to 'bool' \r\n                           output_zero_point, \"output_tensor\"),\r\n                                              ^~~~~~~~~~~~~~~\r\ntensorflow/lite/micro/kernels/activations_test.cc:805:24: note: in instantiation of function template specialization 'tflite::testing::(anonymous namespace)::TestHardSwishQuantized<unsigned char>' requested here\r\n     tflite::testing::TestHardSwishQuantized<uint8_t>(\r\n                      ^\r\n\r\ntensorflow/lite/micro/kernels/activations_test.cc:210:29: error: implicit conversion turns string literal into bool: 'const char [13]' to 'bool' \r\n                           \"input_tensor\"),\r\n                           ^~~~~~~~~~~~~~\r\n\r\ntensorflow/lite/micro/kernels/activations_test.cc:329:48: error: implicit conversion turns string literal into bool: 'const char [14]' to 'bool' \r\n                           output_zero_point, \"output_tensor\"),\r\n                                              ^~~~~~~~~~~~~~~\r\n\r\ntensorflow/lite/micro/kernels/activations_test.cc:826:20: note: in instantiation of function template specialization 'tflite::testing::(anonymous namespace)::TestHardSwishQuantizedBias<unsigned char>' requested here\r\n tflite::testing::TestHardSwishQuantizedBias<uint8_t>(\r\n                  ^\r\n\r\ntensorflow/lite/micro/kernels/activations_test.cc:327:29: error: implicit conversion turns string literal into bool: 'const char [13]' to 'bool' \r\n                           \"input_tensor\"),\r\n`", "@rthadur Errors and conflicts resolved. Please re-approve."]}, {"number": 37640, "title": "Sparce Tensor wrong exeption Message when passing argument with wrong Type", "body": "System information\r\n\r\n    Have I written custom code (as opposed to using a stock\r\n    example script provided in TensorFlow): No\r\n    OS Platform and Distribution (e.g.,\r\n    Linux Ubuntu 16.04): Linux Mint 19.3 (ubuntu)\r\n    TensorFlow installed from (source or\r\n    binary): binatry\r\n    TensorFlow version (use command below): both tf-nightly and tf-2.1\r\n    Python version: 3.8 and 3.6\r\n\r\n**Describe the current behavior**\r\nwhen passing an int32 tensor to Sparce Tensor\r\nlast Error displayed is:\r\n```\r\nValueError: Unable to create eager SparseTensor. Check that your shape is correctly defined. Eager SparseTensors don't support unknown dimesions.\r\ngot shape:\r\n    [4 4 4 4]\r\n```\r\nwhen looking back in stack trace, the right error is raced:\r\n\r\n`ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 4, 4, 4], dtype=int32)>`\r\n\r\n\r\n**Describe the expected behavior**\r\none of the following:\r\n1) conversion should not fail\r\n2) last Error should be:\r\nexpected int64 tensor for shape argument got int32 tensor\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport tensorflow as tf\r\n\r\nindices = tf.cast([[1,1,1,1],[1,3,1,1]],dtype=tf.int64)\r\nshape = tf.cast([4,4,4,4],dtype=tf.int64)\r\n\r\nheat_map = tf.SparseTensor(indices = indices, values = tf.ones(tf.shape(indices)[0]), dense_shape = shape)\r\n\r\nindices = tf.cast([[1,1,1,1],[1,3,1,1]],dtype=tf.int64)\r\nshape = tf.cast([4,4,4,4],dtype=tf.int32)\r\n\r\nheat_map = tf.SparseTensor(indices = indices, values = tf.ones(tf.shape(indices)[0]), dense_shape = shape)\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py\", line 142, in __init__\r\n    dense_shape, name=\"dense_shape\", dtype=dtypes.int64)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1317, in convert_to_tensor\r\n    (dtype.name, value.dtype.name, value))\r\nValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 4, 4, 4], dtype=int32)>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/bhb/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/ptvsd_launcher.py\", line 48, in <module>\r\n    main(ptvsdArgs)\r\n  File \"/home/bhb/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/lib/python/old_ptvsd/ptvsd/__main__.py\", line 432, in main\r\n    run()\r\n  File \"/home/bhb/.vscode/extensions/ms-python.python-2020.2.64397/pythonFiles/lib/python/old_ptvsd/ptvsd/__main__.py\", line 316, in run_file\r\n    runpy.run_path(target, run_name='__main__')\r\n  File \"/usr/lib/python3.6/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/test_sparce_tensor.py\", line 11, in <module>\r\n    heat_map = tf.SparseTensor(indices = indices, values = tf.ones(tf.shape(indices)[0]), dense_shape = shape)\r\n  File \"/home/bhb/Cloud/Code/Git/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py\", line 148, in __init__\r\n    \"got shape:\\n    {}\".format(dense_shape))\r\nValueError: Unable to create eager SparseTensor. Check that your shape is correctly defined. Eager SparseTensors don't support unknown dimesions.\r\ngot shape:\r\n    [4 4 4 4]\r\nBeendet\r\n```\r\n", "comments": ["@bela127,\r\nOn running the above code with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/d1fd04827fc673713a0b728c20bc15a1/37640-tf-nightly.ipynb), I got an error stating\r\n`ValueError: Unable to create eager SparseTensor. Check that your shape is correctly defined. Eager SparseTensors don't support unknown dimesions.\r\ngot shape:\r\n    [4 4 4 4]`\r\n\r\nHowever, with [TF2.1](https://colab.research.google.com/gist/amahendrakar/6bde0978e3a5a9ace8b36a36c40d4432/37640.ipynb), the error states \r\n`ValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: <tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 4, 4, 4], dtype=int32)>`\r\n\r\nIs the behavior for 2.1 as expected? Please find the attached gist. Thanks!", "2.1 behavior seems to be as expected.\r\nSo it seems only nightly has the miss leading Message.", "Was able to reproduce the issue with TF-2.2-rc0. Please find the gist [here](https://colab.research.google.com/gist/amahendrakar/1161b25237908e9a7d95aec53b2a0eb6/37640-2-2.ipynb). Thanks!", "I would like to fix this.\r\nwill be opening a PR soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37640\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37640\">No</a>\n"]}, {"number": 37639, "title": "TFLite build - missing interpreter_wrapper.i", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): amazonlinux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:2.1.0\r\n- Python version:3.6.9\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):2.0.0\r\n- GCC/Compiler version (if compiling from source):7.2.1\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\n**Describe the problem**\r\nRunning sh ./tensorflow/lite/tools/pip_package/build_pip_package.sh results in:\r\n...\r\nswig -python -c++ -I/root/tensorflow/tensorflow/lite/tools/pip_package/../../../.. -module interpreter_wrapper -outdir tflite_runtime -o interpreter_wrapper/interpreter_wrapper_wrap.cpp interpreter_wrapper/interpreter_wrapper.i\r\nUnable to find file 'interpreter_wrapper/interpreter_wrapper.i'.\r\nerror: command 'swig' failed with exit status 1\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nTrying to build Tensorflow lite for AWS Lambda in amazonlinux.\r\nDockerfile:\r\nFROM amazonlinux\r\n\r\nWORKDIR /tflite\r\n\r\nRUN yum groupinstall -y development\r\nRUN yum install -y python3.7\r\nRUN yum install -y python3-devel\r\nRUN pip3 install numpy wheel\r\nRUN git clone https://github.com/tensorflow/tensorflow.git\r\nRUN sh ./tensorflow/tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n\r\n\r\n**Any other info / logs**\r\nI am a total newb at compiling, so I apologise in advance for missing obvious info/logs.\r\nThe issue happens when `setup.py bdist bdist_wheel` is executed. I am afraid I cant figure out why the .i file is not there, or is not getting generated.\r\nDo I need to generate them separately?\r\n\r\nThanks\r\n", "comments": ["in  e638577, `interpreter_wrapper/interpreter_wrapper.i` was removed because now interpreter_wrapper functions are exported from C++ to Python with pybind11 instead of swig. It seems corresponding changes were not made in the shell script and related files.", "reference to the interpreter_wrapper.i has removed, but other issue is still blocking normal execution of tflite_runtime pip package.\r\nhttps://github.com/tensorflow/tensorflow/issues/38033", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37639\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37639\">No</a>\n", "Thanks for the reply, how to fix his issue now? Is this BKM still valid https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package"]}, {"number": 37638, "title": "[2.x] SparseTensor shape becomes <unknown> after some operations if using tf.function", "body": "With tf.function, if an argument `x` of a function is a 2-d `tf.SparseTensor`, its shape is `(None, None)`. However, after some operations such as `tf.sparse.transpose` and `tf.sparse.reduce_sum`, the shapes of the resulting tensors become `<unknown>`.\r\n\r\nPlease refer to this [script](https://colab.research.google.com/drive/17DqrrFVZePJlJsfymrRCXNGBMNPNKeqd) for reproduction.\r\n", "comments": ["@llan-ml \r\ncould you please share the tensorflow version and simple stand alone code for us to replicate the issue faced.", "@Saduf2019 I tested on `2.1` and `nightly`. The code is as follows:\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef foo(t):\r\n  print(t.shape)\r\n  print(tf.shape(t))\r\n  print(\"=====\")\r\n  t1 = tf.sparse.transpose(t)\r\n  print(t1.shape)\r\n  print(tf.shape(t1))\r\n  print(\"=====\")\r\n  t2 = tf.sparse.reduce_sum(t, axis=1)\r\n  print(t2.shape)\r\n  print(tf.shape(t2))\r\n  print(\"=====\")\r\n\r\nt = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1., 2], dense_shape=[3, 4])\r\ntf.function(foo)(t)\r\n```\r\n\r\nBTW, can't you access the colab link in my original post? ", "i am able to replicate this issue, please find gist [here](https://colab.sandbox.google.com/gist/Saduf2019/06ff7c15f1bd9b04f65bbb140019224c/untitled96.ipynb)", "It looks like [sparse_transpose](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/sparse_ops.py#L2597) defaults to an unknown rank for all but the fully-defined shapes, which is too strict.\r\n\r\nSo a workaround would be this:\r\n```\r\ndef foo(t):\r\n  print(t.shape)\r\n  print(tf.shape(t))\r\n  t = tf.SparseTensor(values=t.values, indices=t.indices, dense_shape=[3, 4])\r\n  ...\r\n```\r\n\r\nIt should be straightforward to add an extra check so that a known rank is preserved.", "@mdanatg @llan-ml I added a PR #38142 to address the `sparse.transpose` issue. For the other issue `sparse.reduce_sum`, it is inference from the C++ `SparseReduceSumSparse` ops which\r\nwill output unknown shape anyway. So it may not be easily fixable. I leave the `sparse.reduce_sum`.\r\n\r\nPlease take a look at PR #38142 for shape of `sparse.transpose`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37638\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37638\">No</a>\n"]}, {"number": 37637, "title": "Segmentation fault when using Cloud TPU", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.12 (stretch)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): preinstalled by Google Cloud Platform\r\n- TensorFlow version (use command below): v1.15.0-rc3-22-g590d6ee\r\n- Python version: Python 2.7.13 (default, Sep 26 2018, 18:42:22) [GCC 6.3.0 20170516] on linux2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: ?\r\n- GPU model and memory: TPU (provided by Google Cloud), version v3-8\r\n\r\n**Describe the current behavior**\r\nI request a VM and corresponding TPU using `ctpu up --zone=europe-west4-a --tpu-size=v3-8 --name europe4 --tf-version=1.15 --machine-type=n1-standard-2 --project my-project`\r\nAfter connecting to it, and I start training a translation model using `t2t-trainer   --model=transformer   --hparams_set=transformer_tpu   --problem=translate_ende_wmt32k_packed   --train_steps=10   --eval_steps=3   --data_dir=gs://my-project/data   --output_dir=gs://my-project/training/tmp   --use_tpu=True   --cloud_tpu_name=europe4 --tpu_num_shards=8   --schedule=train` (this is the command specified in the [tutorial](https://cloud.google.com/tpu/docs/tutorials/transformer?hl=en#train_a_language_model_on_a_pod)).\r\nBut the training does not start, the command is aborted due to an segmentation fault:\r\n\r\n```\r\nmy-user@europe4:~$ t2t-trainer   --model=transformer   --hparams_set=transformer_tpu   --problem=translate_ende_wmt32k_packed   --train_steps=10   --eval_steps=3   --data_dir=gs://my-project/data   --output_dir=gs://my-project/training/tmp   --use_tpu=True   --cloud_tpu_name=europe4 --tpu_num_shards=8   --schedule=train\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/expert_utils.py:68: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/adafactor.py:27: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/multistep_optimizer.py:32: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/mesh_tensorflow/ops.py:4237: The name tf.train.CheckpointSaverListener is deprecated. Please use tf.estimator.CheckpointSaverListener instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/mesh_tensorflow/ops.py:4260: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/models/research/neural_stack.py:38: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/rl/gym_utils.py:235: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_lib.py:111: The name tf.OptimizerOptions is deprecated. Please use tf.compat.v1.OptimizerOptions instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_gan/python/estimator/tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\r\n\r\nWARNING:tensorflow:From /usr/local/bin/t2t-trainer:32: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n\r\nWARNING:tensorflow:From /usr/local/bin/t2t-trainer:32: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\r\n\r\nWARNING:tensorflow:From /usr/local/bin/t2t-trainer:33: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/hparams_lib.py:49: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\r\n\r\nW0316 12:49:12.574630 140506290238912 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/hparams_lib.py:49: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_lib.py:839: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\r\n\r\nW0316 12:49:12.673067 140506290238912 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_lib.py:839: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_lib.py:116: The name tf.GraphOptions is deprecated. Please use tf.compat.v1.GraphOptions instead.\r\n\r\nW0316 12:49:12.675126 140506290238912 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_lib.py:116: The name tf.GraphOptions is deprecated. Please use tf.compat.v1.GraphOptions instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_lib.py:129: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\r\n\r\nW0316 12:49:12.675529 140506290238912 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/trainer_lib.py:129: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\r\n\r\nI0316 12:49:12.683859 140506290238912 discovery.py:271] URL being requested: GET https://www.googleapis.com/discovery/v1/apis/tpu/v1alpha1/rest\r\nI0316 12:49:12.725476 140506290238912 discovery.py:867] URL being requested: GET https://tpu.googleapis.com/v1alpha1/projects/my-project/locations/europe-west4-a/nodes/europe4?alt=json\r\nI0316 12:49:12.725781 140506290238912 transport.py:157] Attempting refresh to obtain initial access_token\r\nI0316 12:49:12.812311 140506290238912 discovery.py:271] URL being requested: GET https://www.googleapis.com/discovery/v1/apis/tpu/v1alpha1/rest\r\nI0316 12:49:12.851186 140506290238912 discovery.py:867] URL being requested: GET https://tpu.googleapis.com/v1alpha1/projects/my-project/locations/europe-west4-a/nodes/europe4?alt=json\r\nI0316 12:49:12.851486 140506290238912 transport.py:157] Attempting refresh to obtain initial access_token\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/text_encoder.py:940: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nW0316 12:49:12.991070 140506290238912 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/text_encoder.py:940: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\r\n\r\nWARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7fc9fc26f398>) includes params argument, but params are not passed to Estimator.\r\nW0316 12:49:13.249932 140506290238912 estimator.py:1984] Estimator's model_fn (<function wrapping_model_fn at 0x7fc9fc26f398>) includes params argument, but params are not passed to Estimator.\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 20, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc9fc006510>, '_model_dir': 'gs://my-project/training/tmp', '_protocol': None, '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 0.95\r\n}\r\nallow_soft_placement: true\r\ngraph_options {\r\n}\r\ncluster_def {\r\n  job {\r\n    name: \"worker\"\r\n    tasks {\r\n      key: 0\r\n      value: \"10.240.1.18:8470\"\r\n    }\r\n  }\r\n}\r\nisolate_session_state: true\r\n, 'use_tpu': True, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc9fd129250>, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': None, '_experimental_max_worker_delay_secs': None, '_evaluation_master': u'grpc://10.240.1.18:8470', '_eval_distribute': None, '_train_distribute': None, '_master': u'grpc://10.240.1.18:8470'}\r\nI0316 12:49:13.251069 140506290238912 estimator.py:209] Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 20, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc9fc006510>, '_model_dir': 'gs://my-project/training/tmp', '_protocol': None, '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 0.95\r\n}\r\nallow_soft_placement: true\r\ngraph_options {\r\n}\r\ncluster_def {\r\n  job {\r\n    name: \"worker\"\r\n    tasks {\r\n      key: 0\r\n      value: \"10.240.1.18:8470\"\r\n    }\r\n  }\r\n}\r\nisolate_session_state: true\r\n, 'use_tpu': True, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc9fd129250>, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': None, '_experimental_max_worker_delay_secs': None, '_evaluation_master': u'grpc://10.240.1.18:8470', '_eval_distribute': None, '_train_distribute': None, '_master': u'grpc://10.240.1.18:8470'}\r\nINFO:tensorflow:_TPUContext: eval_on_tpu True\r\nI0316 12:49:13.252732 140506290238912 tpu_context.py:209] _TPUContext: eval_on_tpu True\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/bin/t2t_trainer.py:328: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nW0316 12:49:13.368489 140506290238912 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/bin/t2t_trainer.py:328: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nINFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.18:8470) for TPU system metadata.\r\nI0316 12:49:14.371850 140506290238912 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.240.1.18:8470) for TPU system metadata.\r\n2020-03-16 12:49:14.373167: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\r\nINFO:tensorflow:Found TPU system:\r\nI0316 12:49:14.384757 140506290238912 tpu_system_metadata.py:148] Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nI0316 12:49:14.385152 140506290238912 tpu_system_metadata.py:149] *** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nI0316 12:49:14.385714 140506290238912 tpu_system_metadata.py:150] *** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nI0316 12:49:14.385898 140506290238912 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 18153004822558697212)\r\nI0316 12:49:14.386076 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 18153004822558697212)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3131077452827063453)\r\nI0316 12:49:14.386504 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3131077452827063453)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 285863435827645433)\r\nI0316 12:49:14.386677 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 285863435827645433)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7574921377020815195)\r\nI0316 12:49:14.386845 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7574921377020815195)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6156291304405420504)\r\nI0316 12:49:14.387026 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6156291304405420504)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2147180529096251620)\r\nI0316 12:49:14.387211 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2147180529096251620)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 16404941304364531224)\r\nI0316 12:49:14.387377 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 16404941304364531224)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 4697263980141791991)\r\nI0316 12:49:14.387541 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 4697263980141791991)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15388571662384413779)\r\nI0316 12:49:14.387706 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15388571662384413779)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 15368005949461242606)\r\nI0316 12:49:14.387868 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 15368005949461242606)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2717797883427288239)\r\nI0316 12:49:14.388044 140506290238912 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2717797883427288239)\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nW0316 12:49:14.393487 140506290238912 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nINFO:tensorflow:Calling model_fn.\r\nI0316 12:49:14.414323 140506290238912 estimator.py:1145] Calling model_fn.\r\nINFO:tensorflow:num_partitions = 1 partition_id = 0\r\nI0316 12:49:14.414944 140506290238912 problem.py:826] num_partitions = 1 partition_id = 0\r\nINFO:tensorflow:Reading data files from gs://my-project/data/translate_ende_wmt32k_packed-train*\r\nI0316 12:49:14.415206 140506290238912 problem.py:644] Reading data files from gs://my-project/data/translate_ende_wmt32k_packed-train*\r\nINFO:tensorflow:partition: 0 num_data_files: 100\r\nI0316 12:49:14.476078 140506290238912 problem.py:670] partition: 0 num_data_files: 100\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/problem.py:680: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\r\nW0316 12:49:14.479042 140506290238912 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/data_generators/problem.py:680: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/data_reader.py:275: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse eager execution and: \r\n`tf.data.TFRecordDataset(path)`\r\nW0316 12:49:14.603265 140506290238912 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/data_reader.py:275: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse eager execution and: \r\n`tf.data.TFRecordDataset(path)`\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/data_reader.py:37: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nW0316 12:49:14.820894 140506290238912 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/data_reader.py:37: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/data_reader.py:417: output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_shapes(dataset)`.\r\nW0316 12:49:14.870455 140506290238912 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/utils/data_reader.py:417: output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.data.get_output_shapes(dataset)`.\r\nINFO:tensorflow:Setting T2TModel mode to 'train'\r\nI0316 12:49:14.991553 140506290238912 t2t_model.py:2248] Setting T2TModel mode to 'train'\r\nINFO:tensorflow:Using variable initializer: uniform_unit_scaling\r\nI0316 12:49:15.940623 140506290238912 api.py:255] Using variable initializer: uniform_unit_scaling\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nW0316 12:49:15.987256 140506290238912 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/autograph/impl/api.py:255: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nW0316 12:49:16.365370 140506290238912 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/autograph/impl/api.py:255: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nINFO:tensorflow:Transforming feature 'inputs' with symbol_modality_33288_512.bottom\r\nI0316 12:49:16.533937 140506290238912 t2t_model.py:2248] Transforming feature 'inputs' with symbol_modality_33288_512.bottom\r\nINFO:tensorflow:Transforming feature 'inputs_position' with identity_modality.bottom\r\nI0316 12:49:16.568705 140506290238912 t2t_model.py:2248] Transforming feature 'inputs_position' with identity_modality.bottom\r\nINFO:tensorflow:Transforming feature 'inputs_segmentation' with identity_modality.bottom\r\nI0316 12:49:16.570141 140506290238912 t2t_model.py:2248] Transforming feature 'inputs_segmentation' with identity_modality.bottom\r\nINFO:tensorflow:Transforming feature 'targets' with symbol_modality_33288_512.targets_bottom\r\nI0316 12:49:16.571368 140506290238912 t2t_model.py:2248] Transforming feature 'targets' with symbol_modality_33288_512.targets_bottom\r\nINFO:tensorflow:Transforming feature 'targets_position' with identity_modality.bottom\r\nI0316 12:49:16.598011 140506290238912 t2t_model.py:2248] Transforming feature 'targets_position' with identity_modality.bottom\r\nINFO:tensorflow:Transforming feature 'targets_segmentation' with identity_modality.bottom\r\nI0316 12:49:16.599239 140506290238912 t2t_model.py:2248] Transforming feature 'targets_segmentation' with identity_modality.bottom\r\nINFO:tensorflow:Building model body\r\nI0316 12:49:16.600420 140506290238912 t2t_model.py:2248] Building model body\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/models/transformer.py:96: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nW0316 12:49:16.674422 140506290238912 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/models/transformer.py:96: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/layers/common_layers.py:3077: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\r\n\r\nW0316 12:49:16.710041 140506290238912 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensor2tensor/layers/common_layers.py:3077: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\r\n\r\nSegmentation fault\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nThe training should start. This worked 2 weeks ago.\r\n\r\n**Standalone code to reproduce the issue** \r\nRequest a VM and TPU using `ctpu up --zone=europe-west4-a --tpu-size=v3-8 --name europe4 --tf-version=1.15 --machine-type=n1-standard-2 --project my-project`\r\nThen, generate the data using `t2t-datagen --problem=translate_ende_wmt32k_packed --data_dir=${DATA_DIR} --tmp_dir=${TMP_DIR}`\r\nFinally, start the training using `t2t-trainer --model=transformer --hparams_set=transformer_tpu --problem=translate_ende_wmt32k_packed --eval_steps=3 --data_dir=${DATA_DIR} --output_dir=${MODEL_DIR}/translate_ende --use_tpu=True --cloud_tpu_name=${TPU_NAME} --train_steps=10` \r\nBoth steps are copied from the tutorial: https://cloud.google.com/tpu/docs/tutorials/transformer?hl=en#train_a_language_model_on_a_single_or_a_pod\r\n\r\nThis did work 2 weeks ago, I'm not sure what changed in the meantime.\r\nTraining using only a CPU works fine.", "comments": ["I tried setting up a new project, with a new bucket, and generating fresh data using the `t2t-datagen` command. It still failed. So this doesn't seem to be related to corrupted data on my side.\r\n\r\nHowever, as a workaround, I've found this solution:\r\n - I request a TPU with TF version 1.15.0\r\n - I request a VM with TF version 1.14.0\r\n - I manually upgrade some of the installed pip packages\r\n\r\nTo do so, I run\r\n```\r\nsudo pip install tensorflow==1.15.0\r\nsudo pip install tensorflow-hub==0.7.0\r\nsudo pip install tensorflow-gan==2.0.0\r\nsudo pip install tensorflow-probability==0.8.0\r\nsudo pip install tensor2tensor==1.14.0\r\nsudo pip install requests==2.22.0\r\nsudo pip install tensorflow-metadata==0.21.1\r\n```\r\nI'm not sure I would need to update all of them, but this combination worked for me. Using this, I can execute my code that's based on TF 1.15.0 without the segmentation fault.", "It's the `gym` package which is causing the issue. Seems that it's latest version is not compatible with tensor2tensor. \r\nTry installing `pip install --user --upgrade gym==0.14.0` onto VM with TF1.15. And it should be working.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37637\">No</a>\n"]}, {"number": 37636, "title": "Expose ability to enable NNApi in C api", "body": "Small change that doesn't add any feature, just exposes the existing one.", "comments": ["I would rather put this in `c_api_experimental.h`. My preference could actually be to expose a pure C wrapper for the NnApiDelegate so that we can use the already-stable api for adding delegates. We will deprecate the `Interpreter::UseNnApi()` method in the future.", "@jdduke hope that putting in experimental api is enough for now"]}, {"number": 37635, "title": "Class type changes after saving and loading", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.8.1\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nCreating a model containing a certain layer class (in my case `tf.keras.layers.BatchNormalization`) changes after saving and loading the model.\r\n\r\nBefore loading it is of type \r\n```python\r\ntensorflow.python.keras.layers.normalization_v2.BatchNormalization\r\n```\r\n\r\nWhich is the same as `tf.keras.layers.BatchNormalization`, but after loading it is of type:\r\n\r\n```python\r\ntensorflow.python.keras.layers.normalization.BatchNormalization\r\n```\r\n\r\nWhich is not the same.\r\n\r\n**Describe the expected behavior**\r\nExpected behavior is that the type of the layer remains unchanged after saving and loading.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential([\r\n\ttf.keras.layers.BatchNormalization()\r\n])\r\nmodel.build(input_shape=(1,))\r\nmodel.save('/tmp/model.h5')\r\n\r\nloaded_model = tf.keras.models.load_model('/tmp/model.h5')\r\n\r\n# True\r\nprint(isinstance(model.layers[0], tf.keras.layers.BatchNormalization))\r\n\r\n# False\r\nprint(isinstance(loaded_model.layers[0], tf.keras.layers.BatchNormalization))\r\n\r\n# AttributeError: module 'tensorflow' has no attribute 'python'\r\nimport tensorflow.python.keras.layers.normalization\r\nprint(isinstance(loaded_model.layers[0], tensorflow.python.keras.layers.normalization.BatchNormalization))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nThe reason I want to do this is because I want to freeze updating of the batch normalization weights, which can be done by setting `layer.training = False`. My goal is to do something like this:\r\n```python\r\nfor l in model.layers:\r\n    if isinstance(l, tf.keras.layers.BatchNormalization):\r\n        l.training = False\r\n```", "comments": ["I've dug a bit deeper into this issue.\r\n\r\nIt seems to come from the issue that there are two versions of `BatchNormalization`. There is a `normalization_v2.BatchNormalization` and a `normalization.BatchNormalization`. If tf2 is enabled, v2 should be used; if tf1 is enabled, v1 should be used.\r\n\r\nThis check should be implemented here: https://github.com/tensorflow/tensorflow/blob/0d7620c3bc50b01054804d8c631b7eed45a7925b/tensorflow/python/keras/layers/serialization.py#L51-L54\r\n\r\nIf tf2 is enabled, it imports from `normalization_v2`, therefore overriding the global class `BatchNormalization` with the one from `normalization_v2.BatchNormalization`. The problem is that even though `tf2` is enabled, the code path here thinks `tf2` is disabled. The cause is the order in which code is executed. It currently happens like this:\r\n\r\n1. `tensorflow_core/__init__.py` is called at some point for the first time, which indirectly imports `tensorflow/python/keras/layers/serialization.py`.\r\n2. [`tensorflow/python/keras/layers/serialization.py`](https://github.com/tensorflow/tensorflow/blob/0d7620c3bc50b01054804d8c631b7eed45a7925b/tensorflow/python/keras/layers/serialization.py#L51-L54) checks to see if tf2 is enabled using [`tensorflow/python/tf2.py`](https://github.com/tensorflow/tensorflow/blob/v1.15.2/tensorflow/python/tf2.py#L43), which [checks `TF2_BEHAVIOR`](https://github.com/tensorflow/tensorflow/blob/v1.15.2/tensorflow/python/tf2.py#L46) environment flag, which defaults to `False`, so it returns False, so the modules are imported as if tf2 is disabled.\r\n3. Later on in [`tensorflow_core/__init__.py`](https://github.com/tensorflow/tensorflow/blob/5c00e793c61860bbf26778cd4704313e867645be/tensorflow/api_template.__init__.py#L98-L100), tf2 functionality is enabled.\r\n\r\nThe same problem presumably holds for the other layers in the same code block (`recurrent_v2` and `preprocessing.normalization`).\r\n\r\nI see two workarounds for now.\r\n\r\n1. By far the easiest is to prefix the command with `TF2_BEHAVIOUR=1` so that the environment variable already enables TF2 before anything is imported. This is the one I'll be using for now.\r\n2. Load models using `tf.keras.models.load_model(path, custom_objects={'BatchNormalization': tf.keras.layers.BatchNormalization}`, but this is a bit less convenient.", "@hgaiser,\r\nI was able to reproduce the issue with [TF2.1](https://colab.research.google.com/gist/amahendrakar/d51763911edd48c97e8c28050d8ebd18/37635.ipynb). However the issue seems to be fixed in [TF-nightly](https://colab.research.google.com/gist/amahendrakar/e5eb5193e41a38359d52fbe0d226240d/37635-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Hey @amahendrakar , thank you for your response. The `AssertionError` isn't the issue, the issue is that after saving and loading a model the type of the layer has changed. Expected output of the code you executed should be:\r\n```shell\r\nTrue\r\nTrue\r\nFalse\r\n```\r\n\r\nNote that this also goes wrong with creating models via `tf.keras.applications`:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.applications.resnet50.ResNet50()\r\nassert(isinstance(model.layers[3], tf.keras.layers.BatchNormalization)), f\"Layer is {model.layers[3]}, but expected {tf.keras.layers.BatchNormalization}.\"\r\n```\r\n\r\nThe default BatchNormalization should be the one from tf2. This is broken both in TF2.1 and TF-nightly. This outputs:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 4, in <module>\r\n    assert(isinstance(model.layers[3], tf.keras.layers.BatchNormalization)), f\"Layer is {model.layers[3]}, but expected {tf.keras.layers.BatchNormalization}.\"\r\nAssertionError: Layer is <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7f8ba5971a60>, but expected <class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'>.\r\n```\r\nNote the subtle difference of `normalization` vs `normalization_v2`.", "@hgaiser  this is similiar to #36700, a fix is underway and should be available in the next rc candidate of TF 2.2.0", "Yes seems identical, I will close this issue then. Thanks !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37635\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37635\">No</a>\n"]}, {"number": 37634, "title": "[Feature] use np.asarray_chkfinite in tf.keras.backend.cast_to_floatx", "body": "Use `numpy.asarray_chkfinite` instead of `numpy.asarray` in `tf.keras.backend.cast_to_floatx`.\r\n\r\nIt can help prevent user pass value which contains None, `inf`, `np.nan` or `np.inf` as input.\r\n\r\nrelated to issue #37196\r\nfixes #37627", "comments": ["Thanks for sending the PR, and sorry for the late reply.\r\n\r\nFrom the API perspective, I think we probably want to keep the existing behavior. Casting np array with NaN in it might be a legit use case. For the particular issue #37196, I think the proper fix is to add check in L2 regularizer.\r\n\r\nI am closing this PR, and feel free to send new PR for the regularizer fix.\r\n\r\nThanks.", "@qlzh727 Thank you for your comment! I will send a new PR for the regularizer fix soon. fix #37196 in other PR "]}, {"number": 37633, "title": "set_shape is not loaded from saved model", "body": "**System information** \r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.10\r\n\r\n**Describe the current behavior**\r\nWhen loading a saved keras model that contains .set_shape on a tensor, this is not loaded.\r\n\r\n``` python\r\nimport tensorflow as tf\r\n\r\ninp = tf.keras.Input((None, 3))\r\ninp.set_shape((None, 2, 3))\r\nx = tf.keras.layers.Dense(3)(inp)\r\n\r\nmodel = tf.keras.Model(inp, x)\r\nmodel.summary()\r\n\r\nmodel.save(\"test.h5\")\r\nloaded = tf.keras.models.load_model(\"test.h5\")\r\nloaded.summary()\r\n```\r\n\r\nmodel.summary():\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, None, 3)]         0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 2, 3)              12        \r\n=================================================================\r\n```\r\n\r\nloaded.summary():\r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, None, 3)]         0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, None, 3)           12        \r\n=================================================================\r\n```\r\n--> Shape is not identical!\r\n", "comments": ["Was able to reproduce the reported issue with Tf 2.1 and Tf-nightly==2.2.0.dev20200316..\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/9f42b6c399fc75f3d58a36ae8243d9b3/untitled460.ipynb). Thanks!", "Why not just set the correct shape in `tf.keras.Input`? I understand that this is a toy example, but is there a valid use case for this? ", "In my case, I'm using some operations which end up leaving the last dimension of the output tensor as a '?', even though I actually know the shape. So I set it manually in order to feed it into a dense layer (which requires the last dimension to be known).\r\n\r\nIn fact, I found out which operation left the dimension unknown: It's tf.shape, which is apparently also not properly loaded from a saved model:\r\n``` python\r\nimport tensorflow as tf\r\n\r\ninp = tf.keras.Input((2, 3))\r\nx = tf.zeros(tf.shape(inp)[:2])\r\n\r\nmodel = tf.keras.models.Model(inp, x)\r\nmodel.summary()\r\nmodel.save(\"test.h5\")\r\nloaded = tf.keras.models.load_model(\"test.h5\")\r\nloaded.summary()\r\n```\r\n\r\nmodel.summary():\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 2, 3)]            0\r\n_________________________________________________________________\r\ntf_op_layer_Shape (TensorFlo [(3,)]                    0\r\n_________________________________________________________________\r\ntf_op_layer_strided_slice (T [(2,)]                    0\r\n_________________________________________________________________\r\ntf_op_layer_zeros (TensorFlo [(None, 2)]               0\r\n=================================================================\r\n```\r\n\r\nloaded.summary():\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 2, 3)]            0\r\n_________________________________________________________________\r\ntf_op_layer_Shape (TensorFlo (3,)                      0\r\n_________________________________________________________________\r\ntf_op_layer_strided_slice (T (2,)                      0\r\n_________________________________________________________________\r\ntf_op_layer_zeros (TensorFlo (None, None)              0\r\n=================================================================\r\n```\r\nAs you can see, the output has an unknown shape after loading the model.", "> Why not just set the correct shape in `tf.keras.Input`? I understand that this is a toy example, but is there a valid use case for this?\r\n\r\nI strongly support this view. it's not advisable to freewheeling like such model building idea.", "> \r\n> \r\n> \r\n> I strongly support this view. it's not advisable to freewheeling like such model building idea.\r\n\r\nset_shape (and tf.shape apparently) are public, documented functions, and saving + loading is a very basic use case, so I don't see what you mean by 'freewheeling'.", "> > I strongly support this view. it's not advisable to freewheeling like such model building idea.\r\n> \r\n> set_shape (and tf.shape apparently) are public, documented functions, and saving + loading is a very basic use case, so I don't see what you mean by 'freewheeling'.\r\n\r\nSorry, I didn't mean to offend. My opinion is that \"just set the correct shape in tf.keras.Input\" already works well, while it's uncommon that you define _inp_ first and then _inp.set_shape()_.", "> \r\n> \r\n> > > I strongly support this view. it's not advisable to freewheeling like such model building idea.\r\n> > \r\n> > \r\n> > set_shape (and tf.shape apparently) are public, documented functions, and saving + loading is a very basic use case, so I don't see what you mean by 'freewheeling'.\r\n> \r\n> Sorry, I didn't mean to offend. My opinion is that \"just set the correct shape in tf.keras.Input\" already works well, while it's uncommon that you define _inp_ first and then _inp.set_shape()_.\r\n\r\nAh I see, sorry for the confusion. I just broke it down to a minium example for this issue. In my actual use case, I do define the input fully, but there are many layers and operations in between the input and set_shape which result in unknown dimensions, something like this:\r\n``` python\r\ninp = tf.keras.Input((3, 4))\r\nx = ...  # many layers, output shape is (None, None, None)\r\nx.set_shape((None, 2, 3))\r\nx = tf.keras.layers.Dense(3)(x)\r\n```", "set_shape doesn't actually produce any ops, so it can't be saved in the SavedModel. You could create a custom layer that runs set_shape, and pass the layer as a custom object when loading.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37633\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37633\">No</a>\n"]}, {"number": 37632, "title": "TFr1.15", "body": "Fetch the Tensorflow r1.15", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37632) for more info**.\n\n<!-- need_sender_cla -->", "Sorry, wrong action"]}, {"number": 37631, "title": "NFC - minor spelling tweaks under lite/delegates directory ", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite/delegates` directory.\r\nfollow-on of #35286", "comments": ["cc @mihaimaruseac", "@mihaimaruseac resolved conflict"]}, {"number": 37630, "title": "TFRecordDataset is loaded into memory entirely, yielding out-of-memory", "body": "\r\n\r\n**System information** \r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): complex_model_m_gpu tier on the nightly docker image of tf\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: /\r\nTensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): Nightly\r\nPython version: - 3.6.9\r\nversion (if compiling from source): /\r\nGCC/Compiler version (if compiling from\r\nsource): /\r\nCUDA/cuDNN version: - GPU model and memory: Using K80 GPU's\r\n\r\n**Describe the current behavior**\r\nI previously loaded my images as npz bytestrings into a TFRecord, together with the label of the image. I would then convert the image and label to the expected format using TFRecordDataset mappings onto `tf.py_function()`s for training . \r\n\r\nI now try to do the formatting beforehand. This yields TFRecords that are much larger. I therefore use the `GZIP` compression. At runtime though, it seems the entire TFRecord is loaded into memory, making the training crash after some time. I even split the TFRecords into smaller partitions, but this makes no difference. How should this be tackled and/or is this normal behavior?\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/39876179/76743276-ab887180-6772-11ea-9017-5de8f0683a36.png)\r\n", "comments": ["@jonyvp \r\nplease share simple stand alone code for us to replicate this issue in our local", "The issue doesn't seem related to any compression, but to the `shuffle()` being called on the TFRecordDataset. As the data in the TFRecord is now much larger than before (as all pre_processing operations are performed on it), the larger objects are loaded in the shuffle `buffer_size` and thus taking in much more space. Furthermore, during training, it seems more RAM is being used throughout the epochs. \r\n\r\nIs there any advice on the shuffle and buffer_size? A smaller buffer_size yields a non-perfect shuffle.", "@jonyvp Can you please share a standalone code to reproduce the issue? Please use any public dataset if your data is private. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37629, "title": "Build TF 2.1.0 with RHEL6/RHEL7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (Linux Redhat 6):\r\n- TensorFlow installed from (source ):\r\n- TensorFlow version:  2.1.0\r\n- Python version:   3.7.2\r\n- Bazel version (if compiling from source):   0.28.1\r\n- GCC/Compiler version (if compiling from source):    7.4.0\r\n\r\n**Describe the problem**\r\nNot able to build TF 2.1.0 on RHEL6 and RHEL7\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n./configure\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n\r\nERROR: /servers/scratch03/niraj/tf-2.1/tensorflow-2.1.0/tensorflow/compiler/tf2tensorrt/BUILD:524:1: SWIGing tensorflow/compiler/tf2tensorrt/utils/py_utils.i failed (Exit 1)\r\n/home/niraj/.cache/bazel/_bazel_niraj/install/efb644d4834d79efdfe4de9f85220414/_embedded_binaries/process-wrapper: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/niraj/.cache/bazel/_bazel_niraj/install/efb644d4834d79efdfe4de9f85220414/_embedded_binaries/process-wrapper)\r\n/home/niraj/.cache/bazel/_bazel_niraj/install/efb644d4834d79efdfe4de9f85220414/_embedded_binaries/process-wrapper: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /home/niraj/.cache/bazel/_bazel_niraj/install/efb644d4834d79efdfe4de9f85220414/_embedded_binaries/process-wrapper)\r\n", "comments": ["@npandey2385 \r\nCan you please check with [tested builded configurations](https://www.tensorflow.org/install/source#tested_build_configurations) and see if the issue still persists.Thanks!", "Yes I am using the bazel > 0.27.1 , gcc > 7.3.1.\r\nI read in following article that currently TF 2.1.0 supported only Ubuntu (18.04) for Linux .\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/35754\r\n\r\n\r\nI am not sure will it work for RHEL6/7 as well ", "@npandey2385 \r\n\r\nCan you please share complete error log. Thanks!", "These are the msg I am getting on terminal.\r\n\r\nINFO: From ProtoCompile tensorflow/compiler/xla/xla_data.pb.h:                                                                \r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.                                     \r\nINFO: From Compiling tensorflow/compiler/tf2xla/ops/xla_ops.cc:                                                               \r\ntensorflow/compiler/tf2xla/ops/xla_ops.cc: In static member function 'static tensorflow::Status tensorflow::{anonymous}::<lambda(tensorflow::shape_inference::InferenceContext*)>::_FUN(tensorflow::shape_inference::InferenceContext*)':                   \r\ntensorflow/compiler/tf2xla/ops/xla_ops.cc:245:74: warning: 'rhs_batch_dimension_or_constant.tensorflow::shape_inference::DimensionOrConstant::val' may be used uninitialized in this function [-Wmaybe-uninitialized]                                       \r\n               lhs_batch_dimension_size, \" and \", rhs_batch_dimension_size);                                                  \r\n                                                                          ^                                                   \r\ntensorflow/compiler/tf2xla/ops/xla_ops.cc:245:74: warning: 'lhs_batch_dimension_or_constant.tensorflow::shape_inference::DimensionOrConstant::val' may be used uninitialized in this function [-Wmaybe-uninitialized]                                       \r\ntensorflow/compiler/tf2xla/ops/xla_ops.cc:214:45: warning: 'rhs_contracting_dimension_or_constant.tensorflow::shape_inference::DimensionOrConstant::val' may be used uninitialized in this function [-Wmaybe-uninitialized]                                 \r\n               rhs_contracting_dimension_size);                                                                               \r\n                                             ^                                                                                \r\ntensorflow/compiler/tf2xla/ops/xla_ops.cc:214:45: warning: 'lhs_contracting_dimension_or_constant.tensorflow::shape_inference::DimensionOrConstant::val' may be used uninitialized in this function [-Wmaybe-uninitialized]                                 \r\nERROR: /servers/scratch03/niraj/tf-2.1/tensorflow-2.1.0/tensorflow/compiler/tf2tensorrt/BUILD:524:1: SWIGing tensorflow/compiler/tf2tensorrt/utils/py_utils.i failed (Exit 1)                                                                               \r\n/home/niraj/.cache/bazel/_bazel_niraj/install/efb644d4834d79efdfe4de9f85220414/_embedded_binaries/process-wrapper: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/niraj/.cache/bazel/_bazel_niraj/install/efb644d4834d79efdfe4de9f85220414/_embedded_binaries/process-wrapper)                                                                          \r\n/home/niraj/.cache/bazel/_bazel_niraj/install/efb644d4834d79efdfe4de9f85220414/_embedded_binaries/process-wrapper: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /home/niraj/.cache/bazel/_bazel_niraj/install/efb644d4834d79efdfe4de9f85220414/_embedded_binaries/process-wrapper)                                                                          \r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build                                                       \r\nUse --verbose_failures to see the command lines of failed build steps.                                                        \r\nINFO: Elapsed time: 11996.296s, Critical Path: 85.38s                                                                         \r\nINFO: 5136 processes: 5136 local.                                                                                             \r\nFAILED: Build did NOT complete successfully    ", "Hi, Any update on this ?", "Also, note TF 2.2 has now been released some time ago.", "@npandey2385,\r\nIs this still an issue?\r\n\r\nCould you please follow [this guide](https://www.tensorflow.org/install/source) and check if you are facing the same error with TF v2.4.1 as well? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37629\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37629\">No</a>\n"]}, {"number": 37628, "title": "ESP32 tensor allocation in PSRAM", "body": "I'm using the Arduino TensorFlowLite library with an ESP32, and after testing some simple models successfully, i'm trying to load '[Optimized models for common mobile and edge use cases](https://www.tensorflow.org/lite/models)'. \r\nModel size is limited by the ESP32's standard 4MB FLASH, and tensor allocation by its default 520KB RAM. However, the maximum tensor pool size that can be allocated seems to be around 95*1024B; hitting the limit of ESP.getMaxAllocHeap()\r\nNow, many dev boards (e.g. ESP32-CAM) have 4-8MB external PSRAM. As far as i can see, this is only available via dedicated allocation functions. Can the TensorFlowLite library be expanded to allow tensor allocation in PSRAM? Espressif has an example of how to handle this in the [esp32-camera](https://github.com/espressif/esp32-camera/blob/9f99b1b03ccfbd1937c3aee96f563af9df1e0ba6/driver/camera.c#L253) repo. Thanks for considering this.\r\n", "comments": ["Hi,\r\n\r\nAs you mentioned it, the Tensorflow Micro interpreter should be given a pointer to an allocated pool of memory (tensor arena) where the tensors will live. [Example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/main_functions.cc#L36).\r\n\r\nHow this pool is allocated is not the responsibility of Tensorflow and can be platform specific. \r\nIt must be available during the entire lifetime of the Tensorflow interpreter. But it does not matter whether it is statically or dynamically allocated, nor if resides on the same die or an external SPI RAM chip.\r\n\r\nTherefore, for ESP32, you can use the method of your choice mentioned in [Support for external RAM](https://docs.espressif.com/projects/esp-idf/en/latest/api-guides/external-ram.html) to allocate the pool on PSRAM.\r\n\r\nAs an example, with the correct ESP-IDF `skdconfig` options, you should be able to allocate a 1MiB tensor arena on PSRAM like so:\r\n```\r\nconstexpr int kTensorArenaSize = 1 * 1024 * 1024;\r\nEXT_RAM_ATTR uint8_t tensor_arena[kTensorArenaSize];\r\n```\r\n", "Thanks for pointing out it should and can be done outside TensorFlowLite. I'm using the Arduino IDE so i can't change `skdconfig` options, and using `EXT_RAM_ATTR` does not compile. However, this seems to work (inspired by the esp32-camera code), i can indeed allocate 1MB.\r\n```  uint8_t * tensor_pool;\r\n  tensor_pool = (uint8_t*) heap_caps_calloc(tensor_pool_size, 1, MALLOC_CAP_SPIRAM | MALLOC_CAP_8BIT);\r\n```"]}, {"number": 37627, "title": "[Proposal] use numpy.asarray_chkfinite in tf.keras.backend.cast_to_floatx", "body": "Maybe we can use `numpy.asarray_chkfinite` instead of `numpy.asarray` in `tf.keras.backend.cast_to_floatx`?\r\n\r\n```python\r\n@keras_export('keras.backend.cast_to_floatx')\r\ndef cast_to_floatx(x):\r\n  if isinstance(x, (ops.Tensor,\r\n                    variables_module.Variable,\r\n                    sparse_tensor.SparseTensor)):\r\n    return math_ops.cast(x, dtype=floatx())\r\n  return np.asarray(x, dtype=floatx())\r\n```\r\n\r\nIt can help prevent user pass value which contains None, `inf`, `np.nan` or `np.inf` as input.\r\n\r\nrelated to issue #37196\r\n\r\n", "comments": ["@howl-anderson, Can you raise the Pull request to fix this issue? ", "@gadagashwini Yes, I will submit a PR for this later!", "Replied on #37196. See https://github.com/tensorflow/tensorflow/pull/37634#issuecomment-635080461 for more details."]}, {"number": 37626, "title": "[tflite][java] has_rank_one_input_condition error when use runForMultipleInputsOutputs()", "body": "I obtain an error when use two inputs in runForMultipleInputsOutputs().\r\n\r\n**System information** \r\n- Custom code\r\n- Android API 29\r\n- Android simulator\r\n- Used libraries:\r\n-- org.tensorflow:tensorflow-lite:2.1.0\r\n-- org.tensorflow:tensorflow-lite-gpu:2.1.0\r\n-- org.tensorflow:tensorflow-lite-support:0.0.0-nightly\r\n\r\n- Used model information\r\n[The model tolite file](https://github.com/tensorflow/tensorflow/files/4336843/classifier_lstm2.tflite.zip)\r\n\r\n[{'name': 'input_1', 'index': 0, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'input_2', 'index': 1, 'shape': array([1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 10)]         0                                            \r\n__________________________________________________________________________________________________\r\nembedding_1 (Embedding)         (None, 10, 256)      15616       input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ninput_2 (InputLayer)            [(None, 2)]          0                                            \r\n__________________________________________________________________________________________________\r\nlstm_1 (LSTM)                   (None, 256)          525312      embedding_1[0][0]                \r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 258)          0           input_2[0][0]                    \r\n                                                                 lstm_1[0][0]                     \r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 61)           15799       concatenate[0][0]                \r\n==================================================================================================\r\n```\r\n\r\n**The error**\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/select.cc:80 data->has_rank_one_input_condition was not true.\r\nNode number 23 (SELECT) failed to prepare.\r\nNode number 19 (WHILE) failed to invoke.\r\n```\r\n\r\n**The code**\r\n\r\n```\r\nMappedByteBuffer tfliteModelCaption = FileUtil.loadMappedFile(activity, \"classifier_caption.tflite\");\r\nInterpreter.Options options = new Interpreter.Options();\r\nInterpreter tflite = new Interpreter(tfliteModelCaption, options);\r\nfloat[][] inputString = new float[1][10];\r\ninputString[0][9] = 4;\r\nfloat[][] inputData = new float[1][2];\r\ninputData[0][0] = 0;\r\ninputData[0][1] = 1;\r\nObject[] inputArray = {inputString, inputData};\r\nMap<Integer, Object> outputMap = new HashMap<>();\r\noutputMap.put(0, new float[1][61]);\r\ntflite.runForMultipleInputsOutputs(inputArray, outputMap);\r\ntflite.close();\r\nfloat[] result = ((float[][]) outputMap.get(0))[0];\r\ntextViewAge.setText(\"Test: \" + Arrays.toString(result));\r\n```\r\n\r\nThe error seems to means the shape of the inputs does not fit the model input, but I checked it many time and for me it looks the same. I create a similar model with only one input (no input_2)  and I can use it without error.\r\n\r\n\r\n\r\n\r\n", "comments": ["@aruno14 \r\ncould you please share a simple stand alone code for us to replicate the error faced in our local, if possible please share a colab gist of the same.", "@Saduf2019 Thank you for your response.\r\nI created a simple project and put it on GitHub.\r\nhttps://github.com/aruno14/TensorflowLiteTest", "@aruno14 few questions.\r\n1.Are you using two different models? \r\n2. Can you please describe the steps you followed before noticing this error? \r\n3. what commands you used to converter your model to tf_lite model?\r\n4. Did you build a model with single input, converted to `tf-lite` format and then tested the model. Then, the same procedure was followed in developing model with two inputs. Is this what you followed? \r\nThanks!", "@jvishnuvardhan Thank you for your reply.\r\n- 1.Are you using two different models?\r\nIn the code I shared I use only one model. In order to check the code, I created an other similar model with only one input, (the input_1). Then, I used the same code, I used runForMultipleInputsOutputs with one input and it executes without error.\r\n- 2.Can you please describe the steps you followed before noticing this error?\r\nI have an android application using tflite. It uses several models for sex and age recognition using one image as input. The code works well.\r\nThen I create a model to generate image description using LSTM. In this model there are two inputs, the image (image representation) and the previous words. I can use it in local (in python) but when I tried to used it in android I got an error. Then I tried the below models to find the error:\r\n- - a simple LSTM model -> no error\r\n- - a model with LSTM and two simple inputs: \r\n- - - vector representing the string\r\n- - - a vector [0, 1] or [1, 0]\r\n- - -> error\r\n- - a model with two inputs of same shape -> error\r\n- 3.what commands you used to converter your model to tf_lite model?\r\nI used the below code with the version 2.1.0 of tensorflow.\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"<model_folder>\")\r\nconverter.experimental_new_converter = True\r\ntfmodel = converter.convert()\r\nopen(args.output , \"wb\").write(tfmodel)\r\ninterpreter= tf.lite.Interpreter(\"<tflite_model_name>\")\r\nprint(interpreter.get_input_details())\r\n```\r\nI cannot convert the model using experimental_new_converter = False\r\n- 4.Did you build a model with single input, converted to tf-lite format and then tested the model.\r\nYes, I did.\r\n- Then, the same procedure was followed in developing model with two inputs. Is this what you followed?\r\nYes.", "> float[][] inputData = new float[1][2];\r\n\r\nThe model is expecting an input of shape [1, 10], but you've provided a shape of [1, 2].", "I am sorry, on the GitHub repository the code was not correct. I corrected it.\r\nBut I get the same error with [1, 10] or [1, 2].\r\nIn order to be sure of the model inputs I added the below output:\r\n```\r\n\"Test: \" + tflite.getInputTensorCount()\r\n   + \" - \" + tflite.getInputTensor(0).name() + \":\" + Arrays.toString(tflite.getInputTensor(0).shape()) + \":\" + tflite.getInputTensor(0).dataType()\r\n   + \" - \" + tflite.getInputTensor(1).name() + \":\" + Arrays.toString(tflite.getInputTensor(1).shape()) + \":\" + tflite.getInputTensor(1).dataType()\r\n```\r\nThe result is bellow:\r\n`Test: 2 - input_1:[1, 10]:FLOAT32 - input_2:[1, 2]:FLOAT32`\r\n\r\nIn addition, I tried the new model below with two inputs and I could execute it without error.\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 2)]          0                                            \r\n__________________________________________________________________________________________________\r\ninput_2 (InputLayer)            [(None, 2)]          0                                            \r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 4)            0           input_1[0][0]                    \r\n                                                                 input_2[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 4)            20          concatenate[0][0]                \r\n==================================================================================================\r\n```\r\n\r\nThere error seems to be related to LSTM model.\r\n\r\n\r\n", "@renjie-liu can you take a look?", "Hi,\r\n\r\nCan you share the tf snippet about how you build the lstm?\r\n\r\nthanks!", "I put the code I used to build the model below:\r\n```\r\nimport tensorflow\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import LSTM, Embedding\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.layers import concatenate\r\n\r\nvocab_size = 10\r\nmax_length = 10\r\ndef create_model():\r\n    input_str = Input(shape=(max_length,))\r\n    x = Embedding(vocab_size, 256, mask_zero=True)(input_str)\r\n    x = LSTM(256)(x)\r\n    input_int = Input(shape=(2))\r\n    y = input_int\r\n    z = concatenate([y, x])\r\n    outputs = Dense(vocab_size, activation='softmax')(z)\r\n    return Model([input_int, input_str], outputs, name='lstm')\r\n\r\nmodel = create_model()\r\nmodel.summary()\r\nmodel.save(\"model_lstm2\")\r\n```", "seems export is fine, to failed at control flow (select & while)\r\n\r\nHi Haoliang, any idea?\r\n\r\nthanks", "Thank you to all to look at my problem.\r\nI would like to know if it is a tensorflow bug, or if I have to change my code.\r\nIf it is a tensorflow bug, I can only wait for a new version, right ?\r\n\r\nThanks.", " Looping in @miaout17 for troubleshooting.", "I'm seeing this error too with an NMT model I converted from Keras. \r\n\r\n  ```\r\n  java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/select.cc:80 data->has_rank_one_input_condition was not true.\r\n    Node number 24 (SELECT) failed to prepare.\r\n    \r\n    Node number 19 (WHILE) failed to invoke.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:154)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:311)\r\n        ...\r\n```\r\n\r\nHere is the Keras model creation code; https://github.com/prateekjoshi565/machine_translation/blob/master/german_to_english.ipynb\r\n\r\nHere is the tflie conversion script;\r\n```\r\ntflite_convert \\\r\n     --keras_model_file model.h1.24_jan_19 \\\r\n     --output_file translate.tflite \\\r\n    --experimental_new_converter\r\n\r\n```\r\n\r\n**Host config**\r\nKeras 2.3.1       \r\nKeras-Applications 1.0.8       \r\nKeras-Preprocessing 1.1.0      \r\ntensorflow 2.1.0       \r\npython 3.7.4\r\n\r\n**Target config**\r\nimplementation 'org.tensorflow:tensorflow-lite:2.1.0'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:2.1.0'\r\n\r\nHere is a link to the model file binaries; https://gofile.io/?c=9TzxiC", "@vikramambrose Can you please open a new issue so that it will be easy for the users to follow. You can tag me in the new issue. Thanks!", "@vikramambrose One suggestion for you is to import keras from tensorflow. If we mix different components with different versions, then we might face compatibility issue. Thanks!", "> I put the code I used to build the model below:\r\n> \r\n> ```\r\n> import tensorflow\r\n> from tensorflow.keras.models import Model\r\n> from tensorflow.keras.layers import LSTM, Embedding\r\n> from tensorflow.keras.layers import Input, Dense\r\n> from tensorflow.keras.layers import concatenate\r\n> \r\n> vocab_size = 10\r\n> max_length = 10\r\n> def create_model():\r\n>     input_str = Input(shape=(max_length,))\r\n>     x = Embedding(vocab_size, 256, mask_zero=True)(input_str)\r\n>     x = LSTM(256)(x)\r\n>     input_int = Input(shape=(2))\r\n>     y = input_int\r\n>     z = concatenate([y, x])\r\n>     outputs = Dense(vocab_size, activation='softmax')(z)\r\n>     return Model([input_int, input_str], outputs, name='lstm')\r\n> \r\n> model = create_model()\r\n> model.summary()\r\n> model.save(\"model_lstm2\")\r\n> ```\r\n\r\nI tested this code but the output model is running OK. Could you try it again with the latest nightly?", "@thaink Thank you for your comment.\r\nI tested your code but I got the same error as before. Maybe, I do not use the latest nightly.\r\nI used the below code in the Gradle of my android project:\r\n\r\n`implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'`\r\n`implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'`\r\n\r\nHow can I get the latest nightly version ?\r\n", "@aruno14 could you create the model file again with the recent TF version and then try TFLite conversion?", "@abattery I updated my tensorflow (`pip install tensorflow`) and now it works! :)\r\nI will create the real model I wanted to use and check if it works too. Thank you.\r\n\r\nI also tried with the latest `tf-nightly` but I could not convert the model in TFLite.\r\nI got the below output:\r\n`  %cst_15 = \"std.constant\"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_16 = \"std.constant\"() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_17 = \"std.constant\"() {value = dense<[0, -1, 0]> : tensor<3xi32>} : () -> tensor<3xi32>\r\n  %cst_18 = \"std.constant\"() {value = dense<0> : tensor<3xi32>} : () -> tensor<3xi32>\r\n  %cst_19 = \"std.constant\"() {value = dense<1> : tensor<3xi32>} : () -> tensor<3xi32>\r\n  %0 = \"tfl.cast\"(%arg1) : (tensor<?x10xf32>) -> tensor<?x10xi32>\r\n  %1 = \"tfl.gather\"(%cst, %0) {axis = 0 : i32} : (tensor<10x256xf32>, tensor<?x10xi32>) -> tensor<?x10x256xf32>\r\n  %2 = \"tfl.shape\"(%1) : (tensor<?x10x256xf32>) -> tensor<3xi32>\r\n  %3 = \"tfl.strided_slice\"(%2, %cst_15, %cst_16, %cst_16) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %4 = \"tfl.pack\"(%3, %cst_2) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %5 = \"tfl.fill\"(%4, %cst_1) : (tensor<2xi32>, tensor<f32>) -> tensor<?x256xf32>\r\n  %6 = \"tfl.unidirectional_sequence_lstm\"(%1, %cst_10, %cst_11, %cst_12, %cst_13, %cst_4, %cst_5, %cst_6, %cst_7, %cst_3, %cst_3, %cst_3, %cst_8, %cst_9, %cst_8, %cst_8, %cst_3, %cst_3, %5, %5, %cst_3, %cst_3, %cst_3, %cst_3) {cell_clip = 1.000000e+01 : f32, fused_activation_function = \"TANH\", proj_clip = 0.000000e+00 : f32, time_major = false} : (tensor<?x10x256xf32>, tensor<256x256xf32>, tensor<256x256xf32>, tensor<256x256xf32>, tensor<256x256xf32>, tensor<256x256xf32>, tensor<256x256xf32>, tensor<256x256xf32>, tensor<256x256xf32>, none, none, none, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, none, none, tensor<?x256xf32>, tensor<?x256xf32>, none, none, none, none) -> tensor<?x10x256xf32>\r\n  %7 = \"tfl.strided_slice\"(%6, %cst_17, %cst_18, %cst_19) {begin_mask = 5 : i32, ellipsis_mask = 0 : i32, end_mask = 5 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x10x256xf32>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> tensor<?x256xf32>\r\n  %8 = \"tfl.concatenation\"(%arg0, %7) {axis = 1 : i32, fused_activation_function = \"NONE\"} : (tensor<?x2xf32>, tensor<?x256xf32>) -> tensor<?x258xf32>\r\n  %9 = \"tfl.fully_connected\"(%8, %cst_14, %cst_0) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<?x258xf32>, tensor<10x258xf32>, tensor<10xf32>) -> tensor<?x10xf32>\r\n  %10 = \"tfl.softmax\"(%9) {beta = 1.000000e+00 : f32} : (tensor<?x10xf32>) -> tensor<?x10xf32>\r\n  \"std.return\"(%10) : (tensor<?x10xf32>) -> ()\r\n}) {arg0 = {tf_saved_model.index_path = [\"input_2\"]}, arg1 = {tf_saved_model.index_path = [\"input_1\"]}, result0 = {tf_saved_model.index_path = [\"dense\"]}, sym_name = \"serving_default\", tf.entry_function = {control_outputs = \"\", inputs = \"serving_default_input_2:0,serving_default_input_1:0\", outputs = \"StatefulPartitionedCall:0\"}, tf_saved_model.exported_names = [\"serving_default\"], type = (tensor<?x2xf32>, tensor<?x10xf32>) -> tensor<?x10xf32>} : () -> ()`", " @aruno14 Could you share the full output of the error?", "@abattery I joined a file with the complete output because it is very long:\r\n[tflite-nightly.txt](https://github.com/tensorflow/tensorflow/files/4685875/tflite-nightly.txt)\r\n\r\nI also find the error message:\r\n`error: Failed to duplicate values for the stateful op`\r\n\r\n\r\n", "@aruno14 are you using any stateful RNN ops in the model?\r\n\r\nfyi @renjie-liu ", "@abattery I used the script given by @thaink to generate the model. So It only contains `LSTM ` and `Embedding`.", "@aruno14 the error is because the batch size is not set, can you fix the batch size when export to tflite?\r\n\r\nthanks!", "@renjie-liu Sorry, I don't know how to set the batch size...", "@aruno14 can you try the codelab here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb\r\n\r\nspecially see the using concre function part to set the shape\r\n\r\nthanks!", "@renjie-liu Thank you for your link. It was a little too complicated for me.\r\nFinally, I found how to setup batch size on input and it works fine. I used below code:\r\n```\r\nimport tensorflow\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import LSTM, Embedding\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.layers import concatenate\r\nvocab_size = 10\r\nmax_length = 10\r\ndef create_model():\r\n    input_str = Input(shape=(max_length), batch_size=1)\r\n    x = Embedding(vocab_size, 256, mask_zero=True)(input_str)\r\n    x = LSTM(256)(x)\r\n    input_int = Input(shape=(2), batch_size=1)\r\n    y = input_int\r\n    z = concatenate([y, x])\r\n    outputs = Dense(vocab_size, activation='softmax')(z)\r\n    return Model([input_int, input_str], outputs, name='lstm')\r\n\r\nmodel = create_model()\r\nmodel.summary()\r\nmodel.save(\"model_lstm2\")\r\n```\r\n\r\nI could convert it to tflite model and used it in an Android app.\r\n\r\nI also tried to use concre function, but I don't know how to set it correctly (shape and 2 inputs).\r\nI used below code inserted after the above code:\r\n\r\n```\r\nrun_model = tf.function(lambda x: model(x))\r\nBATCH_SIZE = 1\r\nSTEPS = 1\r\nINPUT_SIZE = 2\r\nconcrete_func = run_model.get_concrete_function(tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\r\nmodel.save(\"model_lstm2\", save_format=\"tf\", signatures=concrete_func)\r\n```\r\n\r\nAnd got below error:\r\n\r\n```\r\nWARNING:tensorflow:Model was constructed with shape (1, 2) for input Tensor(\"input_2:0\", shape=(1, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).\r\nTraceback (most recent call last):\r\n  File \"analyse-lstm-github.py\", line 42, in <module>\r\n    concrete_func = run_model.get_concrete_function(tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 1153, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 1059, in _get_concrete_function_garbage_collected\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 688, in _initialize\r\n    *args, **kwds))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2838, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3195, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 3057, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 595, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nAssertionError: in user code:\r\n\r\n    analyse-lstm-github.py:38 None  *\r\n        run_model = tf.function(lambda x: model(x))\r\n    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py:950 __call__  **\r\n        outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:383 call\r\n        inputs, training=training, mask=mask)\r\n    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:514 _run_internal_graph\r\n        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\r\n\r\n    AssertionError: Could not compute output Tensor(\"dense/Identity:0\", shape=(1, 10), dtype=float32)\r\n```\r\n", "sorry for your trouble and glad you found a way to make it work :)\r\n\r\nI think the error message says\r\n\r\n```\r\nModel was constructed with shape (1, 2) for input Tensor(\"input_2:0\", shape=(1, 2), dtype=float32), but it was called on an input with incompatible shape (1, 1, 2).\r\n```\r\nfor the first input, it should be 2-d, and you have two inputs, you would need to construct two separate `tf.TensorSpec`s.\r\n", "Finally, I found how to use `concrete_func`, I put the code below maybe it can help someone.\r\n\r\n```\r\nimport tensorflow\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import LSTM, Embedding\r\nfrom tensorflow.keras.layers import Input, Dense\r\nfrom tensorflow.keras.layers import concatenate\r\nimport tensorflow.compat.v2 as tf\r\ntf.enable_v2_behavior()\r\n\r\nvocab_size = 10\r\nmax_length = 10\r\ndef create_model():\r\n    input_str = Input(shape=(max_length,))\r\n    x = Embedding(vocab_size, 256, mask_zero=True)(input_str)\r\n    x = LSTM(256)(x)\r\n    input_int = Input(shape=(2))\r\n    y = input_int\r\n    z = concatenate([y, x])\r\n    outputs = Dense(vocab_size, activation='softmax')(z)\r\n    return Model([input_int, input_str], outputs, name='lstm')\r\n\r\nmodel = create_model()\r\nmodel.summary()\r\nmodel.save(\"model_lstm2\")\r\nrun_model = tf.function(lambda x, y: model([x, y]))\r\nBATCH_SIZE = 1\r\nSTEPS = 1\r\nINPUT_SIZE = 2\r\nconcrete_func = run_model.get_concrete_function(tf.TensorSpec((BATCH_SIZE, 2), model.inputs[0].dtype),tf.TensorSpec((BATCH_SIZE, 10), model.inputs[0].dtype))\r\nmodel.save(\"model_lstm2\", save_format=\"tf\", signatures=concrete_func)\r\n```", "great! thanks a lot! :D", "I check the error with my real model (to generate caption of image). I had no error in tflite conversation and `has_rank_one_input_condition` error disappear. So it seems to be fixed.\r\n\r\nHowever, now I have a `Cannot copy between a TensorFlowLite tensor with shape [0, 6441] and a Java object with shape [1, 6441]` error when I execute `runForMultipleInputsOutputs`.\r\nI checked the model I load with `tfliteModel.getOutputTensor(0).shape()` it outputs `[1, 6441]`.\r\nWhat is a shape with a `0` size ?\r\nDo I have to open a new issue for this problem?\r\n\r\n", "do you mind attaching your model and the code you invoke the model?\r\n\r\nthanks", "The model file (it is a zip archive containing a bz2 archive, I had to do that to be under 10MB):\r\n[classifier_caption.tflite.bz2.zip](https://github.com/tensorflow/tensorflow/files/4693741/classifier_caption.tflite.bz2.zip)\r\n\r\nThe code invoking the model:\r\n```\r\nInterpreter tflite = new Interpreter(FileUtil.loadMappedFile(activity, \"classifier_caption.tflite\"), new Interpreter.Options());\r\nfinal short lenght = 10;\r\nfinal short outputSize = 6441;\r\nfloat[][] inputString = new float[1][lenght];\r\nfloat[][] inputImage = new float[1][5120];\r\n\r\nint[] outputString = new int[lenght];\r\nObject[] inputArray = new Object[2];\r\ninputArray[1] = inputString;\r\ninputArray[0] = inputImage;\r\nMap<Integer, Object> outputMap = new HashMap<>();\r\noutputMap.put(0, new float[1][outputSize]);\r\ntflite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n```\r\n\r\n\r\n", "@xunkai55, Hi Xunkai, can you help with the java api?\r\n\r\nthanks a lot!", "Will take a look.", "@aruno14 Thanks a ton! You saved me so much time!", "I checked with version 2.3.0 of TensorFlow Lite, but I got a similar error:\r\n`Cannot copy from a TensorFlowLite tensor (StatefulPartitionedCall:0) with shape [0, 6441] to a Java object with shape [1, 6441]`\r\n", "@aruno14 Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "@sushreebarsa I did not work on this project for a long time, so I don't remember well. But, I rebuild my android app with the last  dependencies (org.tensorflow:tensorflow-lite:2.3.0) and the error seems to have disappeared. I did not recreate the model with tf 2.5 or 2.4.1.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37626\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37626\">No</a>\n", "I checked with my full model. All seems to work well now.\r\nThank you."]}, {"number": 37625, "title": "source code about the libhexagon_interface.so", "body": "Hello, \r\nTFlite heagon delegate is based on Hexagon nn.\r\nI have got the hexagon nn SDK from Qualcomm, but I can not build the \"libhexagon_interface.so\".\r\nGoogle add 4 fucntions in libhexagon_interface:\r\n```\r\nvoid hexagon_nn_global_teardown(void);\r\nvoid hexagon_nn_global_init(void);\r\nbool hexagon_nn_is_device_supported();\r\nint hexagon_nn_hexagon_interface_version(void);\r\n```\r\n\r\nCould you please share the source code of the foure functions  and the \"android.min\" for building libhexagon_interface.so ? ", "comments": ["See https://github.com/tensorflow/tensorflow/issues/35506"]}, {"number": 37624, "title": "[Intel MKL] Enable DepthwiseConv2D + BiasAdd (+ Relu/Relu6/Elu) fusion", "body": "", "comments": ["@penpornk Could you please review this PR?", "I'm sorry for the delay! I'll review this in a few days.", "@penpornk Thanks for the comments. I have updated my code, please check. Thank you!", "@penpornk Thank you for your comments! The two nits are fixed, please check."]}, {"number": 37623, "title": "Building tensorflow lite for Android (C++) undefined reference error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04, MacOS 10.15.3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: latest\r\n- Python version: 2.7.17\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 2.2.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI want to build tensorflow lite in android with C++, so I followed the [document](https://www.tensorflow.org/lite/guide/build_arm64)\r\nBuild works fine, but when I load libtensorflowlite.a into an Android and build, undefined reference of default libraries apperas.\r\nAt first, undefined std::__cxx11::string... error happened, so I added a compiler flag(-D_GLIBCXX_USE_CXX11_ABI=0).\r\nBut other undefined error still happens.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nCMakeLists.txt\r\n```\r\nCMAKE_MINIMUM_REQUIRED(VERSION 3.3)\r\nproject(tflite)\r\n\r\nadd_library(tflite INTERFACE)\r\n\r\nset (TFLITE_PATH ${CMAKE_CURRENT_SOURCE_DIR})\r\nset (TFLITE_LIB_PATH \"${TFLITE_PATH}/${CMAKE_ANDROID_ARCH_ABI}\")\r\nset (TFLITE_INCLUDE_PATH \"${TFLITE_PATH}/include\")\r\n\r\ntarget_link_libraries(\r\n        tflite\r\n        INTERFACE\r\n        \"${TFLITE_LIB_PATH}/libtensorflow-lite.a\"\r\n)\r\n\r\ntarget_include_directories(\r\n        tflite\r\n        INTERFACE\r\n        \"${TFLITE_INCLUDE_PATH}\"\r\n        \"${TFLITE_INCLUDE_PATH}/tensorflow\"\r\n        \"${TFLITE_INCLUDE_PATH}/tensorflow/thirdparty\"\r\n)\r\n```\r\n\r\nError log\r\n```\r\nBuild command failed.\r\nError while executing process /Users/yonggyulee/Library/Android/sdk/cmake/3.6.4111459/bin/cmake with arguments {--build /Users/yonggyulee/Downloads/cppinclude-a4125304137c01ff2f171981e6cdaab051d36d47/app/.cxx/cmake/debug/arm64-v8a --target cpp-test}\r\n[1/4] Building CXX object src/main/cpp/cpp_test/CMakeFiles/cpp-test.dir/src/TestClass.cpp.o\r\n[2/4] Building CXX object src/main/cpp/cpp_test/CMakeFiles/cpp-test.dir/src/LoadModel.cpp.o\r\n[3/4] Building CXX object src/main/cpp/cpp_test/CMakeFiles/cpp-test.dir/src/tftest.cpp.o\r\n[4/4] Linking CXX shared library ../../../../build/intermediates/cmake/debug/obj/arm64-v8a/libcpp-test.so\r\nFAILED: : && /Users/yonggyulee/Library/Android/sdk/ndk/20.0.5594570/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang++  --target=aarch64-none-linux-android24 --gcc-toolchain=/Users/yonggyulee/Library/Android/sdk/ndk/20.0.5594570/toolchains/llvm/prebuilt/darwin-x86_64 --sysroot=/Users/yonggyulee/Library/Android/sdk/ndk/20.0.5594570/toolchains/llvm/prebuilt/darwin-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -fno-addrsig -Wa,--noexecstack -Wformat -Werror=format-security   -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--warn-shared-textrel -Wl,--fatal-warnings -Wl,--no-undefined -Qunused-arguments -Wl,-z,noexecstack -shared -Wl,-soname,libcpp-test.so -o ../../../../build/intermediates/cmake/debug/obj/arm64-v8a/libcpp-test.so src/main/cpp/cpp_test/CMakeFiles/cpp-test.dir/src/TestClass.cpp.o src/main/cpp/cpp_test/CMakeFiles/cpp-test.dir/src/LoadModel.cpp.o src/main/cpp/cpp_test/CMakeFiles/cpp-test.dir/src/tftest.cpp.o  ../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a -latomic -lm && :\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(model.o): In function `tflite::FlatBufferModel::GetMinimumRuntime() const':\r\nmodel.cc:(.text+0x51c): undefined reference to `std::string::_Rep::_S_empty_rep_storage'\r\nmodel.cc:(.text+0x574): undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)'\r\nmodel.cc:(.text+0x580): undefined reference to `std::string::compare(char const*) const'\r\nmodel.cc:(.text+0x58c): undefined reference to `std::string::_Rep::_S_empty_rep_storage'\r\nmodel.cc:(.text+0x604): undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&)'\r\nmodel.cc:(.text+0x624): undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&)'\r\nmodel.cc:(.text+0x704): undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)'\r\nmodel.cc:(.text+0x748): undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(allocation.o): In function `tflite::FileCopyAllocation::FileCopyAllocation(char const*, tflite::ErrorReporter*)':\r\nallocation.cc:(.text+0x130): undefined reference to `__fxstat'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(subgraph.o): In function `tflite::Subgraph::ReserveNodes(int)':\r\nsubgraph.cc:(.text+0x51c): undefined reference to `std::__throw_length_error(char const*)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(subgraph.o): In function `tflite::Subgraph::PrepareOpsStartingAt(int, int*)':\r\nsubgraph.cc:(.text+0xee0): undefined reference to `std::__throw_length_error(char const*)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(subgraph.o): In function `tflite::Subgraph::Invoke()':\r\nsubgraph.cc:(.text+0x32b4): undefined reference to `std::__throw_length_error(char const*)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(subgraph.o): In function `std::vector<std::pair<TfLiteNode, TfLiteRegistration>, std::allocator<std::pair<TfLiteNode, TfLiteRegistration> > >::_M_default_append(unsigned long)':\r\nsubgraph.cc:(.text._ZNSt6vectorISt4pairI10TfLiteNode18TfLiteRegistrationESaIS3_EE17_M_default_appendEm[_ZNSt6vectorISt4pairI10TfLiteNode18TfLiteRegistrationESaIS3_EE17_M_default_appendEm]+0x214): undefined reference to `std::__throw_length_error(char const*)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(subgraph.o): In function `std::vector<TfLiteTensor, std::allocator<TfLiteTensor> >::_M_default_append(unsigned long)':\r\nsubgraph.cc:(.text._ZNSt6vectorI12TfLiteTensorSaIS0_EE17_M_default_appendEm[_ZNSt6vectorI12TfLiteTensorSaIS0_EE17_M_default_appendEm]+0x240): undefined reference to `std::__throw_length_error(char const*)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(graph_info.o):graph_info.cc:(.text._ZNSt6vectorIiSaIiEE14_M_fill_insertEN9__gnu_cxx17__normal_iteratorIPiS1_EEmRKi[_ZNSt6vectorIiSaIiEE14_M_fill_insertEN9__gnu_cxx17__normal_iteratorIPiS1_EEmRKi]+0x5c0): more undefined references to `std::__throw_length_error(char const*)' follow\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(minimal_logging_default.o): In function `tflite::logging_internal::MinimalLogger::LogFormatted(tflite::LogSeverity, char const*, std::__va_list)':\r\nminimal_logging_default.cc:(.text+0x3c): undefined reference to `__fprintf_chk'\r\nminimal_logging_default.cc:(.text+0x60): undefined reference to `__vfprintf_chk'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(nnapi_delegate_disabled.o): In function `tflite::StatefulNnApiDelegate::StatefulNnApiDelegate()':\r\nnnapi_delegate_disabled.cc:(.text+0x34): undefined reference to `std::string::_Rep::_S_empty_rep_storage'\r\nnnapi_delegate_disabled.cc:(.text+0x44): undefined reference to `std::string::_Rep::_S_empty_rep_storage'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(nnapi_delegate_disabled.o): In function `tflite::StatefulNnApiDelegate::Data::~Data()':\r\nnnapi_delegate_disabled.cc:(.text+0x1c4): undefined reference to `std::string::_Rep::_S_empty_rep_storage'\r\nnnapi_delegate_disabled.cc:(.text+0x1cc): undefined reference to `std::string::_Rep::_S_empty_rep_storage'\r\nnnapi_delegate_disabled.cc:(.text+0x1e0): undefined reference to `std::string::_Rep::_S_empty_rep_storage'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(nnapi_delegate_disabled.o):nnapi_delegate_disabled.cc:(.text+0x1f4): more undefined references to `std::string::_Rep::_S_empty_rep_storage' follow\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(nnapi_delegate_disabled.o): In function `tflite::StatefulNnApiDelegate::Data::~Data()':\r\nnnapi_delegate_disabled.cc:(.text+0x254): undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)'\r\nnnapi_delegate_disabled.cc:(.text+0x288): undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)'\r\nnnapi_delegate_disabled.cc:(.text+0x2bc): undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(arena_planner.o): In function `tflite::ArenaPlanner::CreateTensorAllocationVector(int, int)':\r\narena_planner.cc:(.text+0x19e8): undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)'\r\narena_planner.cc:(.text+0x1a3c): undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(arena_planner.o): In function `std::vector<tflite::ArenaAllocWithUsageInterval, std::allocator<tflite::ArenaAllocWithUsageInterval> >::_M_default_append(unsigned long)':\r\narena_planner.cc:(.text._ZNSt6vectorIN6tflite27ArenaAllocWithUsageIntervalESaIS1_EE17_M_default_appendEm[_ZNSt6vectorIN6tflite27ArenaAllocWithUsageIntervalESaIS1_EE17_M_default_appendEm]+0x1a8): undefined reference to `std::__throw_length_error(char const*)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(arena_planner.o): In function `std::pair<std::_Rb_tree_iterator<int>, bool> std::_Rb_tree<int, int, std::_Identity<int>, std::less<int>, std::allocator<int> >::_M_insert_unique<int const&>(int const&)':\r\narena_planner.cc:(.text._ZNSt8_Rb_treeIiiSt9_IdentityIiESt4lessIiESaIiEE16_M_insert_uniqueIRKiEESt4pairISt17_Rb_tree_iteratorIiEbEOT_[_ZNSt8_Rb_treeIiiSt9_IdentityIiESt4lessIiESaIiEE16_M_insert_uniqueIRKiEESt4pairISt17_Rb_tree_iteratorIiEbEOT_]+0xa8): undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)'\r\narena_planner.cc:(.text._ZNSt8_Rb_treeIiiSt9_IdentityIiESt4lessIiESaIiEE16_M_insert_uniqueIRKiEESt4pairISt17_Rb_tree_iteratorIiEbEOT_[_ZNSt8_Rb_treeIiiSt9_IdentityIiESt4lessIiESaIiEE16_M_insert_uniqueIRKiEESt4pairISt17_Rb_tree_iteratorIiEbEOT_]+0xf8): undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(simple_memory_arena.o): In function `tflite::SimpleMemoryArena::Allocate(TfLiteContext*, unsigned long, unsigned long, int, int, int, tflite::ArenaAllocWithUsageInterval*)':\r\nsimple_memory_arena.cc:(.text+0x15c): undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)'\r\n../../../../src/main/cpp/cpp_test/tflite/arm64-v8a/libtensorflow-lite.a(simple_memory_arena.o): In function `tflite::SimpleMemoryArena::Deallocate(TfLiteContext*, tflite::ArenaAllocWithUsageInterval const&)':\r\nsimple_memory_arena.cc:(.text+0x228): undefined reference to `std::__detail::_List_node_base::_M_unhook()'\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\nninja: build stopped: subcommand failed.\r\n```\r\n\r\n\r\n", "comments": ["How did you build the static library? If you're using bazel, it generally does not produce self-contained, static libraries. Is there a reason you cannot use the shared library target? `libtensorflowlite.so` or `libtensorflowlite_c.so`?", "@jdduke I couldn't find an official document about building _shared_ C++ library for android. I just followed the [official document](https://www.tensorflow.org/lite/guide/build_arm64). Tried with docker, and manually clone the repository both.\r\nStatic / Shared doesn't matter. I just need any C++ library for android(arm64v8a).\r\n", "For now, your best best is to give https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/c a try and use the C API shared library. We're working on a revamped C++ tutorial for Android clients that *aren't* using Bazel, which should make this a lot more clear.", "Thanks. I'll give it a try and come back with results.", "@jdduke \r\n\r\nI've cloned the latest version, download build dependencies, and build it with bazel. ```bazel build -c opt --cxxopt=--std=c++11 --config=android_arm64 //tensorflow/lite/c:tensorflowlite_c```, and copied the output to my Android Studio Project as ```libtensorflowlite_c2.so```\r\n\r\nAnd when I tried to test, this error occurs.\r\n#### test.cpp\r\n```\r\n#include <memory>\r\n#include \"test.h\"\r\n\r\n#include \"tensorflow/lite/c/common.h\"\r\n#include \"tensorflow/lite/c/builtin_op_data.h\"\r\n#include \"tensorflow/lite/c/c_api.h\"\r\n#include \"tensorflow/lite/c/c_api_experimental.h\"\r\n\r\ntest::test(){}\r\n\r\nvoid test::build_model() {\r\n    \r\n    TfLiteModel* model = TfLiteModelCreate(gaze_model_tflite, gaze_model_tflite_len);\r\n    TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n    TfLiteInterpreterOptionsSetNumThreads(options, 1);\r\n\r\n    TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n    \r\n}\r\n```\r\n\r\n#### CMakeLists.txt\r\n```\r\ncmake_minimum_required(VERSION 3.3)\r\nproject(tflite)\r\n\r\n#add_library(tflite INTERFACE)\r\n\r\nSET (TFLITE_PATH ${CMAKE_CURRENT_SOURCE_DIR})\r\nSET (TFLITE_LIB_PATH \"${TFLITE_PATH}/lib/android/${CMAKE_ANDROID_ARCH_ABI}\")\r\nSET (TFLITE_INCLUDE_PATH \"${TFLITE_PATH}/include\")\r\nSET (TFLITE_LITE_PATH \"${TFLITE_INCLUDE_PATH}/tensorflow/lite\")\r\nSET (TFLITE_THIRDPARTY_PATH \"${TFLITE_LITE_PATH}/thirdparty\")\r\n\r\nadd_library(\r\n        tflite\r\n        SHARED\r\n\r\n        test.cpp. # dummy\r\n)\r\n\r\nlink_directories(${CMAKE_CURRENT_SOURCE_DIR}/lib/android/arm64-v8a)\r\nlink_libraries(tflite tensorflowlite_c2)\r\n\r\ntarget_include_directories(tflite\r\n    PUBLIC\r\n    \"${TFLITE_INCLUDE_PATH}\"\r\n    \"${TFLITE_INCLUDE_PATH}/tensorflow\"\r\n    \"${TFLITE_INCLUDE_PATH}/tensorflow/lite\"\r\n    \"${TFLITE_INCLUDE_PATH}/tensorflow/lite/toolss\"\r\n    \"${TFLITE_INCLUDE_PATH}/tensorflow/lite/tools/make\"\r\n    \"${TFLITE_INCLUDE_PATH}/tensorflow/lite/tools/make/downloads\"\r\n    \"${TFLITE_INCLUDE_PATH}/tensorflow/lite/tools/make/downloads/flatbuffers\"\r\n    \"${TFLITE_INCLUDE_PATH}/tensorflow/lite/tools/make/downloads/flatbuffers/include\"\r\n    \"${TFLITE_INCLUDE_PATH}/tensorflow/lite/tools/make/downloads/flatbuffers/include/flatbuffers\"\r\n    \"${TFLITE_INCLUDE_PATH}/tensorflow/lite/tools/make/downloads/absl/absl\r\n)\r\n```\r\n\r\n#### Error Log\r\n```\r\nBuild command failed.\r\nError while executing process /Users/yonggyulee/Library/Android/sdk/cmake/3.6.4111459/bin/cmake with arguments {--build /Users/yonggyulee/codes/tflite_06/app/.cxx/cmake/debug/arm64-v8a --target native-lib}\r\n[1/1] Re-running CMake...\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /Users/yonggyulee/codes/tflite_06/app/.cxx/cmake/debug/arm64-v8a\r\n[1/5] Building CXX object src/main/cpp/tony/CMakeFiles/tony.dir/src/test.cpp.o\r\n[2/5] Linking CXX shared library ../../../../build/intermediates/cmake/debug/obj/arm64-v8a/libtony.so\r\nFAILED: : && /Users/yonggyulee/Library/Android/sdk/ndk/20.0.5594570/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang++  --target=aarch64-none-linux-android27 --gcc-toolchain=/Users/yonggyulee/Library/Android/sdk/ndk/20.0.5594570/toolchains/llvm/prebuilt/darwin-x86_64 --sysroot=/Users/yonggyulee/Library/Android/sdk/ndk/20.0.5594570/toolchains/llvm/prebuilt/darwin-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -fno-addrsig -Wa,--noexecstack -Wformat -Werror=format-security  -std=c++11 -frtti -fexceptions -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--warn-shared-textrel -Wl,--fatal-warnings -Wl,--no-undefined -Qunused-arguments -Wl,-z,noexecstack -shared -Wl,-soname,libtony.so -o ../../../../build/intermediates/cmake/debug/obj/arm64-v8a/libtony.so src/main/cpp/tony/CMakeFiles/tony.dir/src/test.cpp.o  ../../../../build/intermediates/cmake/debug/obj/arm64-v8a/libtflite.so -llog -latomic -lm && :\r\nsrc/main/cpp/tony/CMakeFiles/tony.dir/src/test.cpp.o: In function `test::build_model()':\r\n/Users/yonggyulee/codes/tflite_06/app/src/main/cpp/tony/src/test.cpp:26: undefined reference to `TfLiteModelCreate'\r\n/Users/yonggyulee/codes/tflite_06/app/src/main/cpp/tony/src/test.cpp:27: undefined reference to `TfLiteInterpreterOptionsCreate'\r\n/Users/yonggyulee/codes/tflite_06/app/src/main/cpp/tony/src/test.cpp:28: undefined reference to `TfLiteInterpreterOptionsSetNumThreads'\r\n/Users/yonggyulee/codes/tflite_06/app/src/main/cpp/tony/src/test.cpp:30: undefined reference to `TfLiteInterpreterCreate'\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\nninja: build stopped: subcommand failed.\r\n```", "There should be a `-ltensorflowlite_c2` entry in the linking command, but I don't see it.", "@jdduke Oh I shouldn't rename the .so file. Thanks for your help!\r\nBtw, I have another question.\r\nIt works for a normal CPU based inference. When I try to use GPU delegates, the C API doesn't seem to accept V2-series of gpu delegate.\r\nhttps://github.com/tensorflow/tensorflow/blob/99e754b3a189eefab15fdbf326115d312e44fc7b/tensorflow/lite/delegates/gpu/gl_delegate.h#L122\r\nShould I open a new issue?", "Which target are you building? It should be [this one](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/BUILD#L140), and you should be using [the new V2 C API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/delegate.h).", "@jdduke \r\nThanks for your kind help!\r\nI'm targetting for an Android.\r\nWhen using [the new V2 C API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/delegate.h), delegate makes an error. Since I couldn't find an official document about using C to use GPU or NNAPI delegates, I just inferenced the functions with their name and old examples.\r\n* [NNAPI build](https://github.com/tensorflow/tensorflow/blob/0298dd6900d4278e1a3f8c4f6a6956dd9e7ca7f0/tensorflow/lite/delegates/nnapi/BUILD#L11) linking fails during runtime. Built with ```bazel build -c opt --config=android_arm64 //tensorflow/lite/delegates/nnapi:nnapi_delegate```\r\n* [Web Document](https://www.tensorflow.org/lite/performance/gpu_advanced#android_cc) : Use deprecated function & C++ ```Interpreter```\r\n* [GitHub gpu README](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/gpu#basic-usage) : C++ ```Interpreter```\r\n* [GitHub lite/c/c_api_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/c_api_test.cc#L171) : GPU delegate example not included. ([```.Prepare```](https://github.com/tensorflow/tensorflow/blob/445ad96dba12bef2d5356f9f23d78c8609990025/tensorflow/lite/delegates/gpu/delegate.cc#L235) is different)\r\n\r\n## Try 1\r\n#### C Code\r\n```\r\nTfLiteModel* model = TfLiteModelCreate(gaze_model_tflite, gaze_model_tflite_len);\r\n    \r\nTfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\nTfLiteInterpreterOptionsSetNumThreads(options, 1);\r\n\r\nconst TfLiteGpuDelegateOptionsV2 gpuDelegate = {\r\n        .is_precision_loss_allowed = false,\r\n        .inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER,\r\n        .inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY,\r\n        .inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,\r\n        .inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO\r\n}; // TfLiteGpuDelegateOptionsV2Default() doesn't fix the error either\r\nTfLiteDelegate* delegate = TfLiteGpuDelegateV2Create(&gpuDelegate);\r\nTfLiteInterpreterOptionsAddDelegate(options, delegate);\r\n    \r\nTfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);  // error here; interpreter is NULL\r\n```\r\n\r\n#### Android Studio Debugger\r\n![\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2020-03-25 11 57 44](https://user-images.githubusercontent.com/35574936/77497108-df107d80-6e8f-11ea-9c4e-f5e38eba432d.png)\r\n\r\n#### Error log\r\n```\r\n... E/tflite: TfLiteGpuDelegate Init: SLICE: Slicing is supported for 3 or 4 dimensional tensors only.\r\n... E/tflite: TfLiteGpuDelegate Prepare: delegate is not initialized\r\n... E/tflite: Node number 139 (TfLiteGpuDelegateV2) failed to prepare.\r\n... E/tflite: Restored previous execution plan after delegate application failure.\r\n```\r\n\r\n## Try 2\r\nUsed [delegate example](https://github.com/tensorflow/tensorflow/blob/c3d655f51bdcf3b1fa135b8e06044bc95551911a/tensorflow/lite/c/c_api_test.cc#L171) (Not compatible)\r\n#### C Code\r\n```\r\nTfLiteModel* model = TfLiteModelCreate(gaze_model_tflite, gaze_model_tflite_len);\r\n\r\nTfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\nTfLiteInterpreterOptionsSetNumThreads(options, 1);\r\n\r\nconst TfLiteGpuDelegateOptionsV2 gpuDelegate = TfLiteGpuDelegateOptionsV2Default();\r\nTfLiteDelegate* delegate = TfLiteGpuDelegateV2Create(&gpuDelegate);\r\nbool delegate_prepared = false;\r\ndelegate->data_ = &delegate_prepared; // allocating also fails when TfLiteGpuDelegateV2Delete()\r\ndelegate->Prepare = [](TfLiteContext* context, TfLiteDelegate* delegate){\r\n    *static_cast<bool*>(delegate->data_) = true;\r\n    return kTfLiteOk;\r\n};\r\n\r\nTfLiteInterpreterOptionsAddDelegate(options, delegate);\r\nTfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n\r\n/*\r\n Everything including an inference works well\r\n*/\r\n\r\nTfLiteGpuDelegateV2Delete(delegate); // Error here of course; Is it ok to remove this line? Below works fine\r\nTfLiteInterpreterOptionsDelete(options);\r\nTfLiteModelDelete(model);\r\nTfLiteInterpreterDelete(interpreter);\r\n```\r\n", "Hi @lackhole, the issue you're hitting with the GPU delegate is unrelated to use of native APIs. If you can share your model we can diagnose what slice behavior is needed in the GPU delegate, but otherwise you'll have to use CPU inference for now. It's probably worth filing a separate bug for the GPU issue.", "@jdduke I really appreciate you. Another model using CNN works well with GPU delegate.\r\nSince the original issue(build for C/C++) is solved, I'm closing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37623\">No</a>\n", "@jdduke is this still in progress? Quoting your comment,\r\n\r\n> We're working on a revamped C++ tutorial for Android clients that aren't using Bazel, which should make this a lot more clear.", "Yes, still WIP, thanks for your patience.", "Thanks! Is there an issue I can subscribe to to know when it will be available?"]}, {"number": 37622, "title": "grpc+verbs fail with tensorflow/core/common_runtime/process_state.cc:128] Check failed: 0 == cpu_allocators_.size() (0 vs. 1)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  centos 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  r1.15 from source\r\n- Python version: - Bazel\r\nversion (if compiling from source): python 3.6.8, bazel 0.26.1\r\n- GCC/Compiler version (if compiling from\r\nsource):  gcc version 7.3.1\r\n- CUDA/cuDNN version: - GPU model and memory: CPU only\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nUsing grpc+verbs and getting the following error:\r\n2020-03-15 21:08:10.425600: F tensorflow/core/common_runtime/process_state.cc:128] Check failed: 0 == cpu_allocators_.size() (0 vs. 1)AddCPUAllocVisitor must be called prior to first call to ProcessState::GetCPUAllocator\r\nAborted\r\n\r\n**Describe the expected behavior**\r\nShould run without errors\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@moshevolo,\r\nProvide the exact sequence of commands / steps that you executed before running into the problem. Thanks", "This is the script I used:\r\n======================= Python Program ============================\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\ntfds.disable_progress_bar()\r\n\r\nBUFFER_SIZE = 60000\r\nBATCH_SIZE = 64\r\n\r\ndef make_datasets_unbatched():\r\n  # Scaling MNIST data from (0, 255] to (0., 1.]\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\n  datasets, info = tfds.load(name='mnist',\r\n                            with_info=True,\r\n                            as_supervised=True)\r\n\r\n  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n  model.compile(\r\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n      metrics=['accuracy'])\r\n  return model\r\n\r\nimport json\r\nimport os\r\n\r\ndef main (argv):\r\n    # Single worker\r\n    # ==============\r\n    if len(argv) < 2:\r\n        print(\"Must provide argument\\n\\ts for single worker, 0 or 1 for 2 workers\")\r\n        exit()\r\n    elif argv[1] == 's':\r\n        train_datasets = make_datasets_unbatched().batch(BATCH_SIZE)\r\n        single_worker_model = build_and_compile_cnn_model()\r\n        # steps_per_epoch specify how many batches are declared as epoch, if not specified\r\n        # then the whole data set\r\n        single_worker_model.fit(x=train_datasets, epochs=3) #, steps_per_epoch=5)\r\n        exit()\r\n\r\n\r\n    # Multi worker\r\n    # ============\r\n\r\n    os.environ['TF_CONFIG'] = json.dumps({\r\n        'cluster': {\r\n            'worker': [\"10.59.68.123:222\", \"10.59.68.124:222\"]\r\n        },\r\n        'task': {'type': 'worker', 'index': int(argv[1])},\r\n        'rpc_layer':'grpc+verbs'\r\n        #'rpc_layer':'grpc'\r\n    })\r\n\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n    NUM_WORKERS = 2\r\n    # Here the batch size scales up by number of workers since\r\n    # `tf.data.Dataset.batch` expects the global batch size. Previously we used 64,\r\n    # and now this becomes 128.\r\n    GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\r\n\r\n    # Creation of dataset needs to be after MultiWorkerMirroredStrategy object\r\n    # is instantiated.\r\n    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\n    with strategy.scope():\r\n        print(\"building model\")\r\n        # Model building/compiling need to be within `strategy.scope()`.\r\n        multi_worker_model = build_and_compile_cnn_model()\r\n\r\n        # Keras' `model.fit()` trains the model with specified number of epochs and\r\n        # number of steps per epoch. Note that the numbers here are for demonstration\r\n        # purposes only and may not sufficiently produce a model with good quality.\r\n        # steps_per_epoch specify how many batches are declared as epoch, if not specified\r\n        # then the whole data set\r\n        print(\"training\")\r\n        multi_worker_model.fit(x=train_datasets, epochs=3) #, steps_per_epoch=5)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import sys\r\n    main(sys.argv)\r\n\r\n================== End Python program ===============================\r\nCommand line:\r\nThe argument 0 is the task index\r\n> python mnist+keras.py 0", "Typo in last reply in file name, the command line is:\r\npython mnist_keras.py 0", "Is there any progress on this issue?\r\nOr can you refer me to a known tf version and script that is known to work with grpc+verbs\r\nI can work from there\r\n\r\nThanks", "Does it fail when executing on single worker or multiworker strategy?\r\n", "On multi worker strategy\r\n\r\nI also tried on tf 2.0 and it fails\r\nAlso tried with google tensorflow API as follow:\r\ncloned the models api\r\nUsed bazelisk to build tf2\r\nran \r\nbazel build --config=opt --config=verbs //tensorflow/tools/pip_package:build_pip_package\r\nThen:\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nThen ip installed it\r\nCreated a file named export multi_host_cfg0.txt\r\nThe file content is:\r\n{\r\n        \"task\": {\r\n            \"type\": \"worker\",\r\n            \"index\": 1\r\n        },\r\n        \"cluster\": {\r\n            \"worker\": [\"10.59.68.123:222\",\"10.59.68.124:222\"]\r\n        },\r\n        \"rpc_layer\":\"grpc+verbs\"\r\n}\r\nRan:\r\nexport TF_CONFIG=$(cat multi_host_cfg0.txt)\r\nThen ran the program which you can see in:\r\nhttps://github.com/tensorflow/models/blob/master/official/vision/image_classification/mnist_main.py\r\n\r\nLike this:\r\npython models/official/vision/image_classification/mnist_main.py --num_gpus=0 --train_epochs=5 \\\r\n            --distribution_strategy=multi_worker_mirrored --model_dir=./tf2src_model\r\n\r\n\r\n\r\nI got the following output:\r\n\r\n2020-03-18 14:59:05.767217: E tensorflow/c/c_api_experimental.cc:772] No server factory registered for the given ServerDef: cluster {\r\n  job {\r\n    name: \"worker\"\r\n    tasks {\r\n      key: 0\r\n      value: \"10.59.68.123:222\"\r\n    }\r\n    tasks {\r\n      key: 1\r\n      value: \"10.59.68.124:222\"\r\n    }\r\n  }\r\n}\r\njob_name: \"worker\"\r\ntask_index: 1\r\ndefault_session_config {\r\n  device_filters: \"/job:worker/task:1\"\r\n  graph_options {\r\n    rewrite_options {\r\n      scoped_allocator_optimization: ON\r\n      scoped_allocator_opts {\r\n        enable_op: \"CollectiveReduce\"\r\n      }\r\n    }\r\n  }\r\n  experimental {\r\n    collective_group_leader: \"/job:worker/replica:0/task:0\"\r\n  }\r\n}\r\nprotocol: \"grpc+verbs\"\r\n\r\nThe available server factories are: [ GRPC_SERVER ]\r\nTraceback (most recent call last):\r\n  File \"models/official/vision/image_classification/mnist_main.py\", line 171, in <module>\r\n    app.run(main)\r\n  File \"/root/.virtualenvs/tf2src/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/root/.virtualenvs/tf2src/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"models/official/vision/image_classification/mnist_main.py\", line 164, in main\r\n    stats = run(flags.FLAGS)\r\n  File \"models/official/vision/image_classification/mnist_main.py\", line 87, in run\r\n    tpu_address=flags_obj.tpu)\r\n  File \"/root/tf_models/models/official/utils/misc/distribution_utils.py\", line 132, in get_distribution_strategy\r\n    communication=_collective_communication(all_reduce_alg))\r\n  File \"/root/.virtualenvs/tf2src/lib64/python3.6/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 90, in __init__\r\n    communication=communication))\r\n  File \"/root/.virtualenvs/tf2src/lib64/python3.6/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 144, in __init__\r\n    self._initialize_strategy(cluster_resolver)\r\n  File \"/root/.virtualenvs/tf2src/lib64/python3.6/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 150, in _initialize_strategy\r\n    self._initialize_multi_worker(cluster_resolver)\r\n  File \"/root/.virtualenvs/tf2src/lib64/python3.6/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 266, in _initialize_multi_worker\r\n    context.context().ensure_initialized()\r\n  File \"/root/.virtualenvs/tf2src/lib64/python3.6/site-packages/tensorflow_core/python/eager/context.py\", line 505, in ensure_initialized\r\n    server_def_str)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No server factory registered for the given ServerDef: cluster {\r\n  job {\r\n    name: \"worker\"\r\n    tasks {\r\n      key: 0\r\n      value: \"10.59.68.123:222\"\r\n    }\r\n    tasks {\r\n      key: 1\r\n      value: \"10.59.68.124:222\"\r\n    }\r\n  }\r\n}\r\njob_name: \"worker\"\r\ntask_index: 1\r\ndefault_session_config {\r\n  device_filters: \"/job:worker/task:1\"\r\n  graph_options {\r\n    rewrite_options {\r\n      scoped_allocator_optimization: ON\r\n      scoped_allocator_opts {\r\n        enable_op: \"CollectiveReduce\"\r\n      }\r\n    }\r\n  }\r\n  experimental {\r\n    collective_group_leader: \"/job:worker/replica:0/task:0\"\r\n  }\r\n}\r\nprotocol: \"grpc+verbs\"\r\n\r\nThe available server factories are: [ GRPC_SERVER ]\r\n(tf2src) [root@NARCH-R2-13 tf_models]# more multi_host_cfg1.txt\r\n{\r\n        \"task\": {\r\n            \"type\": \"worker\",\r\n            \"index\": 1\r\n        },\r\n        \"cluster\": {\r\n            \"worker\": [\"10.59.68.123:222\",\"10.59.68.124:222\"]\r\n        },\r\n        \"rpc_layer\":\"grpc+verbs\"\r\n}", "Sorry,\r\n\r\nHad to pip uninstall the previous trial build and then reinstall the new build (it had the same name).\r\nOnce I did than, grpc_verbs works, but I get the same error as in the original bug:\r\n\r\n2020-03-18 15:21:53.628933: F tensorflow/core/common_runtime/process_state.cc:128] Check failed: 0 == cpu_allocators_.size() (0 vs. 1)AddCPUAllocVisitor must be called prior to first call to ProcessState::GetCPUAllocator\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f72b0644740 (most recent call first):\r\n  File \"/root/.virtualenvs/tf2src/lib64/python3.6/site-packages/tensorflow_core/python/eager/context.py\", line 505 in ensure_initialized\r\n  File \"/root/.virtualenvs/tf2src/lib64/python3.6/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 266 in _initialize_multi_worker\r\n  File \"/root/.virtualenvs/tf2src/lib64/python3.6/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 150 in _initialize_strategy\r\n  File \"/root/.virtualenvs/tf2src/lib64/python3.6/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 144 in __init__\r\n  File \"/root/.virtualenvs/tf2src/lib64/python3.6/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 90 in __init__\r\n  File \"/root/tf_models/models/official/utils/misc/distribution_utils.py\", line 132 in get_distribution_strategy\r\n  File \"models/official/vision/image_classification/mnist_main.py\", line 87 in run\r\n  File \"models/official/vision/image_classification/mnist_main.py\", line 164 in main\r\n  File \"/root/.virtualenvs/tf2src/lib/python3.6/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/root/.virtualenvs/tf2src/lib/python3.6/site-packages/absl/app.py\", line 299 in run\r\n  File \"models/official/vision/image_classification/mnist_main.py\", line 171 in <module>\r\nAborted", "Hello,\r\n\r\nAsking again...\r\nIf there is no solution yet for the above 2 versions please let me know if there ANY tf versions that is known to work with RDMA (grpc+verbs).\r\n\r\nThanks", "??????????", "Same problem in GDR", "Hey, I have a question about your tensorflow 2.0 with verbs. Did you download the source code and build it by yourself, or just simply 'pip install tensorflow'? Is it true that tensorflow 2.0 is built with '--config=verbs' by defalut? Thx.\r\n", "This reason is that GPU is initialized before initialize verbs.", "I am encountering the same error, have you fixed this problem? @Keepmoving-ZXY ", "@yanivbl6 @byronyi Could you take a look at this issue? A lot of people are still facing it, just checked with Bug [24250](https://github.com/tensorflow/tensorflow/pull/24250) , the issue mentioned by  CheukNgai[https://github.com/tensorflow/tensorflow/pull/24250#issuecomment-447768924](url) still exists with verbs. ", "I fix this problem  by letting verbs server start before all device initialize.", "@clustar-zxy Thanks for the hint, fix it.", "@moshevolo It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37622\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37622\">No</a>\n"]}]