[{"number": 12458, "title": "[android demo] Complete path to nsync library destination", "body": "......\r\nCMake Error at CMakeLists.txt:30 (set_target_properties):\r\n  set_target_properties called with incorrect number of arguments.\r\n.....", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 12457, "title": "Branch 165958212", "body": "", "comments": ["@andrewharp, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @rohan100jain and @dandelionmane to be potential reviewers.", "https://github.com/tensorflow/tensorflow/pull/12455 is almost done testing and ready for merging :)", "Ok, let's go with that then :)"]}, {"number": 12456, "title": "TF 1.3 Install error", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**:  3.6\r\n\r\n### Describe the problem\r\nError when installing with pip\r\n\r\npip3 install tensorflow-gpu\r\n\r\nThis error appears :\r\n\r\n  ```\r\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/packages/urllib3/response.py\", line 357, in stream\r\n    data = self.read(amt=amt, decode_content=decode_content)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/packages/urllib3/response.py\", line 314, in read\r\n    data = self._fp.read(amt)\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 63, in read\r\n    self._close()\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 50, in _close\r\n    self.__callback(self.__buf.getvalue())\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/controller.py\", line 275, in cache_response\r\n    self.serializer.dumps(request, response, body=body),\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/serialize.py\", line 55, in dumps\r\n    \"body\": _b64_encode_bytes(body),\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/serialize.py\", line 12, in _b64_encode_bytes\r\n    return base64.b64encode(b).decode(\"ascii\")\r\n  File \"/home/ubuntu/anaconda3/lib/python3.6/base64.py\", line 58, in b64encode\r\n    encoded = binascii.b2a_base64(s, newline=False)\r\n```", "comments": ["What's the output from `pip3 show pip`?", "TF 1.3 cannot be installed via pip3 install tensorflow-gpu.\r\nError message is above.\r\n\r\n\r\nFor info, here the output of pip3\r\n\r\n(tf_gpu_13) ubuntu@ip-172-31-9-240:/root$ pip3 show pip\r\n---\r\nMetadata-Version: 1.1\r\nName: pip\r\nVersion: 8.1.1\r\nSummary: The PyPA recommended tool for installing Python packages.\r\nHome-page: https://pip.pypa.io/\r\nAuthor: The pip developers\r\nAuthor-email: python-virtualenv@groups.google.com\r\nLicense: MIT\r\nLocation: /usr/lib/python3/dist-packages\r\nRequires:\r\nClassifiers:\r\n  Development Status :: 5 - Production/Stable\r\n  Intended Audience :: Developers\r\n  License :: OSI Approved :: MIT License\r\n  Topic :: Software Development :: Build Tools\r\n  Programming Language :: Python :: 2\r\n  Programming Language :: Python :: 2.6\r\n  Programming Language :: Python :: 2.7\r\n  Programming Language :: Python :: 3\r\n  Programming Language :: Python :: 3.3\r\n  Programming Language :: Python :: 3.4\r\n  Programming Language :: Python :: 3.5\r\n  Programming Language :: Python :: Implementation :: PyPy\r\nEntry-points:\r\n  [console_scripts]\r\n  pip = pip:main\r\n  pip3 = pip:main\r\n  pip3.5 = pip:main\r\nYou are using pip version 8.1.1, however version 9.0.1 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n\r\n\r\n(tf_gpu_13) ubuntu@ip-172-31-9-240:/root$ pip show pip\r\nName: pip\r\nVersion: 9.0.1\r\nSummary: The PyPA recommended tool for installing Python packages.\r\nHome-page: https://pip.pypa.io/\r\nAuthor: The pip developers\r\nAuthor-email: python-virtualenv@groups.google.com\r\nLicense: MIT\r\nLocation: /home/ubuntu/anaconda3/envs/tf_gpu_13/lib/python3.6/site-packages\r\n\r\npip install oftens break with binaries install.\r\n\r\nIt's much safer to use conda packages when there are binaries due to compil/dependencies management. Can't google upload conda package for TF release ?\r\n\r\n\r\n\r\n\r\n\r\n", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "This is a bug : since TF 1.3 cannot be installed by pip on linux Ubuntu.", "TensorFlow 1.3 can be pip installed on Ubuntu. Try `pip install --upgrade pip` and then `pip install tensorflow`."]}, {"number": 12455, "title": "Branch 165951046", "body": "", "comments": []}, {"number": 12454, "title": "Misleading error message on type mismatch", "body": "- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Archlinux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.6\r\n```python\r\nimport tensorflow as tf\r\nx = tf.get_variable('asdfds', shape=[10], dtype=tf.int32)\r\nx*0.5\r\n```\r\nPrints:\r\n```\r\nTraceback (most recent call last):               \r\n  File \"a.py\", line 9, in <module>               \r\n    x*0.5                                        \r\nTypeError: unsupported operand type(s) for *: 'Variable' and 'float' \r\n```\r\nVariable with matching dtype can multiply with float, the problem here is int32 cannot multiply with float.\r\nSince usually the dtype of some tensor is not directly written in code, this misleading message can cause confusions.", "comments": ["Hi @ppwwyyxx @ali01, I am new to open source, how can I start working on this?", "@ppwwyyxx I think the error message is threw by Python interpreter directly. @ali01 Can we catch it and print the exacter error message? I do know the `Mul` op is registered on the kernels dir, but I failed to locate it. Could you give me some idea for fixing the issue? ", "Relevant code here:\r\nhttps://github.com/tensorflow/tensorflow/blob/c9db2486ead95f000395af14919f0ae8f508e429/tensorflow/python/ops/math_ops.py#L894-L907\r\n\r\n`x.__mul__(0.5)` returns NotImplemented, then `0.5.__rmul__(x)` is called. Both failed therefore Python throws an error with its predefined message. ", "Fixed by 2da8e8c60343d9e833f1c591c54ee2a27c240842"]}, {"number": 12453, "title": "Adaptive optmizers do not work well with multi-head networks when some labels are missing", "body": "Use case: we have a multi-task network with many outputs. Each example in the dataset has only subset of the labels; non-existing loss components are masked out. A single optimizer is used, because the presence of particular labels in the example stream is statically unknown.\r\n\r\nProblem: masked loss components give zero gradient estimates for the corresponding variables. That breaks adaptive optimizers (Momentum, Adam, ...) because steps are taken and slots are corrupted with the incoming zero gradient estimates. The desired behaviour is: do nothing for variables (and corresponding slots) that were effectively unused in the forward pass (like it is done for the embeddings using the IndexedSlices trick).\r\nRelated problem: tf.global_norm, tf.clip_by_global_norm are also affected by these stray zero gradient estimates.\r\n\r\nMinimal example/demonstration:\r\n```python\r\nIn [14]: feature = tf.constant([1.0])\r\nIn [15]: output = feature * 42.0\r\nIn [16]: loss = tf.squared_difference(output, [0.0])\r\nIn [17]: mask = tf.constant([False])\r\nIn [18]: masked_loss = tf.boolean_mask(loss, mask)\r\nIn [19]: masked_loss_grad = tf.gradients(masked_loss, feature)\r\nIn [20]: masked_loss_grad\r\nOut[20]: [<tf.Tensor 'gradients/mul_3_grad/Reshape:0' shape=(1,) dtype=float32>]\r\nIn [21]: masked_loss_grad[0].eval(session=session)\r\nOut[21]: array([ 0.], dtype=float32)\r\n```\r\nThis is technically correct -- the gradient is indeed zero -- but if `feature` was a Variable, we don't really want to apply this gradient in an adaptive optimizer, because the training example didn't have useful information about the loss function.\r\n\r\nFeature request: better way to deal with that (probably a special value for the gradients that would signify \"nonexisting\" gradients; e.g. tensorflow uses `None`, when this information is available statically).\r\n\r\nFor now here is a simple and ugly workaround:\r\n```python\r\ndef _is_all_zeros(grad):\r\n    all_zeros = tf.equal(tf.count_nonzero(grad), 0)\r\n    return all_zeros\r\n\r\nclass HackedMomentumOptimizer(tf.train.MomentumOptimizer):\r\n    def __init__(self, *args, **kwargs):\r\n        super(HackedMomentumOptimizer, self).__init__(*args, **kwargs)\r\n\r\n    def _apply_dense(self, grad, var):\r\n        all_zeros = _is_all_zeros(grad)\r\n        return tf.cond(all_zeros,\r\n                       lambda: tf.no_op(),\r\n                       lambda: super(HackedMomentumOptimizer, self).\r\n\t\t       _apply_dense(grad, var))\r\n\r\n    def _resource_apply_dense(self, grad, var):\r\n        all_zeros = _is_all_zeros(grad)\r\n        return tf.cond(all_zeros,\r\n                       lambda: tf.no_op(),\r\n                       lambda: super(HackedMomentumOptimizer, self).\r\n                       _resource_apply_dense(grad, var))\r\n\r\n    def _apply_sparse(self, grad, var):\r\n        all_zeros = _is_all_zeros(grad)\r\n        return tf.cond(all_zeros,\r\n                       lambda: tf.no_op(),\r\n                       lambda: super(HackedMomentumOptimizer, self).\r\n                       _apply_sparse(grad, var))\r\n\r\n    def _resource_apply_sparse(self, grad, var, indices):\r\n        all_zeros = _is_all_zeros(grad)\r\n        return tf.cond(all_zeros,\r\n                       lambda: tf.no_op(),\r\n                       lambda: super(HackedMomentumOptimizer, self).\r\n                       _resource_apply_sparse(grad, var, indices))\r\n\r\n\r\ndef _clip_gradients(gradients_variables, clip_norm=20.):\r\n    gradients, variables = six.moves.zip(*gradients_variables)\r\n\r\n    def _replace_nonexisting_grad(grad):\r\n        all_zeros = _is_all_zeros(grad)\r\n        return tf.cond(all_zeros,\r\n                       lambda: tf.zeros([], dtype=tf.float32),\r\n                       lambda: grad)\r\n    gradients_filtered = [_replace_nonexisting_grad(g) for g in gradients]\r\n    fixed_global_norm = tf.global_norm(gradients_filtered)\r\n    gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm,\r\n\t                                            use_norm=fixed_global_norm)\r\n    tf.summary.scalar('global_norm', global_norm)\r\n    return six.moves.zip(gradients, variables)\r\n```", "comments": ["@alextp You may have context on gradients as well as optimizers -- I don't know how much trouble we'd be causing if we added a special value for gradients.", "I think special-casing the optimizers is the right answer here. This code you wrote can be moved into C++ kernels (added initially in contrib) if you see a performance regression.\r\n\r\nTensorFlow distinguishes between not differentiable and differentiable but 0 gradient. Trying to further refine this distinction feels like a losing battle to me with a static computation graph.", "Same situation here: a multi-task NN with 5-6 outputs (tasks), where each task has \u2248 80% subset of overall labels of the same dataset.\r\n\r\nWhat is the best current solution for this problem in TensorFlow?\r\nWill directly using upper-mentioned `HackedMomentumOptimizer` for a combined loss work well?\r\nDo we need to weigh/mask the losses? \r\nHave you resolved this problem @ilya-edrenkin?\r\n\r\nCurrently in production, I use alternate multi-task learning where I have separate Optimizers for each loss and filter tf.contrib.Datasets accordingly available labels.\r\nThis has it's downsides: \r\n- Optimizers are not being able to effectively communicate (e.g. share internal states - velocity, etc), so they do not know about each other.\r\n- This is wasting a lot of computation as single forwards/backward passes could use information from multiple losses/tasks instead of only one.", "> Will directly using upper-mentioned HackedMomentumOptimizer for a combined loss work well?\r\n\r\nIt works for me at least.\r\n\r\n>Do we need to weigh/mask the losses?\r\n\r\nIf you want to use this hack, the masked loss's gradient must be zero.\r\n", " >I think special-casing the optimizers is the right answer here. This code you wrote can be moved into C++ kernels (added initially in contrib) if you see a performance regression.\r\n>TensorFlow distinguishes between not differentiable and differentiable but 0 gradient. Trying to further refine this distinction feels like a losing battle to me with a static computation graph.\r\n\r\nDear @alextp: I agree that adding special values for nonexisting gradients would cause a mess. Is my understanding correct that you are fine with using all-zero-gradient-tensor as a special case signifying that this gradient tensor should be ignored?\r\nIf yes, I can add this behaviour to tensorflow. What would be the best way to do it?\r\n1) Add a separate optimizer to contrib?\r\n2) Add it as an optional behaviour to the existing optimizers?\r\n3) Add it as the default behaviour to the existing optimizers?\r\n\r\nThank you!", "Add a separate optimizer to contrib.\n\nOn Mon, Sep 11, 2017 at 2:22 AM, Ilya Edrenkin <notifications@github.com>\nwrote:\n\n> I think special-casing the optimizers is the right answer here. This code\n> you wrote can be moved into C++ kernels (added initially in contrib) if you\n> see a performance regression.\n> TensorFlow distinguishes between not differentiable and differentiable but\n> 0 gradient. Trying to further refine this distinction feels like a losing\n> battle to me with a static computation graph.\n>\n> Dear @alextp <https://github.com/alextp>: I agree that adding special\n> values for nonexisting gradients would cause a mess. Is my understanding\n> correct that you are fine with using all-zero-gradient-tensor as a special\n> case signifying that this gradient tensor should be ignored?\n> If yes, I can add this behaviour to tensorflow. What would be the best way\n> to do it?\n>\n>    1. Add a separate optimizer to contrib?\n>    2. Add it as an optional behaviour to the existing optimizers?\n>    3. Add it as the default behaviour to the existing optimizers?\n>\n> Thank you!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12453#issuecomment-328470835>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxctbl7V4Nv7rbhMWsypOKa9heREYks5shPvDgaJpZM4O9lnu>\n> .\n>\n\n\n\n-- \n - Alex\n", "@ilya-edrenkin I run into the same problem and I can't find a proper solution... Could you please show an example of how to use your HackedMomentumOptimizer please, I am not sure to understand how I can use it in my code..\r\n\r\nThanks a lot!", "Dear @Threynaud : simply copy the code I have provided and use it as a drop-in replacement for the MomentumOptimizer.\r\nIf you use gradient clipping with global norm, you can also use the provided function.\r\n\r\n@alextp : will submit a PR this weekend or most likely next weekend. Thanks!", "@ilya-edrenkin Thanks a lot for your quick response!! Does it work the same way with Adam? ", "@Threynaud I haven't tried it myself yet, but in my understanding it should work. To try you can simply replace the superclass (i.e. inherit from tf.train.AdamOptimizer instead of tf.train.MomentumOptimizer).", "@ilya-edrenkin  I am so sorry but I think I am still missing something... The code works like a charm but I still get the erratic behavior related to the missing labels for one of the tasks! I think the problem is due to how I combine my 3 losses!\r\n\r\nWhat I am doing now is simply summing the 3 losses of the 3 heads of my network and then applying the optimizer one the resulting loss but I realized that it might not be the good way to do it because the HackedOptimizer can't detect when an example has missing labels for one of the task..\r\nI guess this is where gradient clipping comes handy but I really have no idea how to use this. Also, does this work with bash training?\r\n\r\nThis is basically what I'm doing for now:\r\n```\r\n# Loss definition\r\ntarget_skills = tf.cast(labels['skills'], tf.float32)\r\nloss_skills = tf.reduce_mean(\r\n    tf.losses.sigmoid_cross_entropy(\r\n        target_skills, logits_skills))\r\n\r\ntarget_fos = tf.cast(labels['fos'], tf.float32)\r\nloss_fos = tf.reduce_mean(\r\n    tf.losses.sigmoid_cross_entropy(\r\n        target_fos, logits_fos))\r\n\r\ntarget_d1 = tf.cast(labels['domain_lvl1'], tf.float32)\r\nloss_d1 = tf.reduce_mean(\r\n    tf.losses.softmax_cross_entropy(\r\n        target_d1, logits_d1))\r\n\r\nloss = loss_d1 + loss_skills + loss_fos\r\n\r\ntrain_op = HackedMomentumOptimizer().minimize(\r\n                loss, global_step=global_step)\r\n```\r\n\r\nI'm sorry for all those questions, I'm quite new with pure tf... :/\r\n\r\nEDIT:\r\nThis is what I did:\r\n```\r\noptimizer = HackedMomentumOptimizer()\r\ngvs = optimizer.compute_gradients(loss)\r\ngvs = _clip_gradients(gvs)\r\n\r\ntrain_op = optimizer.apply_gradients(\r\n    gvs, global_step=global_step)\r\n```\r\n\r\nbut I get the following `AssertionError`:\r\n```\r\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 124, in _model_fn\r\n    gvs = _clip_gradients(gvs)\r\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 389, in _clip_gradients\r\n    gradients_filtered = [_replace_nonexisting_grad(g) for g in gradients]\r\n  File \"/root/.local/lib/python2.7/site-packages/trainer/model.py\", line 388, in _replace_nonexisting_grad\r\n    lambda: grad)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 289, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1855, in cond\r\n    (isinstance(x, ops.Tensor) and isinstance(y, ops.Tensor)))\r\nAssertionError\r\n```", "1) To make it work as intended, you should mask out the \"nonexisting\" components of the loss function. This will make the gradients specific to this loss component equal to zero, which will be detected by the optimizer.\r\nBtw, if you are taking tf.reduce_mean of an empty tensor you are likely to get an 'inf'.\r\n2) The assertion error you are getting is likely a different problem -- looks like a type mismatch in the input of the gradient clipping function. For now, you can try make it work without gradient clipping, and solve this problem later.", "@ilya-edrenkin Thanks a lot! So if I understand correctly, it doesn't work for batch training, does it!?Also I will try with tf.reduce_sum instead!", "Why? It will work for batch training seamlessly.", "You're right, I misunderstood what you meant by \"loss components\" my bad..\r\nDoes the following looks ok to you:\r\n```\r\ntarget_skills = tf.cast(labels['skills'], tf.float32)\r\n\r\n# Compute the mask\r\nmask_skills = tf.count_nonzero(target_skills, 1)\r\nmask_skills = tf.cast(mask_skills, tf.bool)\r\n\r\n#Compute all the losses components\r\nlosses_skills = tf.losses.sigmoid_cross_entropy(\r\n    target_skills, logits_skills)\r\n\r\n# Apply the mask\r\nmasked_losses = tf.boolean_mask(losses_skills, mask_skills)\r\n\r\n# Reduce\r\nloss_skills = tf.reduce_sum(masked_losses)\r\n``` \r\n and that for each loss corresponding to each head of the network. Then sum all the masked losses and apply the optimizer on the resulting loss.\r\n\r\nI have troubles understanding how the optimizer handles the mask but it is because of my limited knowledge about gradient computation in tf...\r\n\r\nEDIT: I found out that I might need to set the `reduction` param of the loss to 'none' to  apply the mask...", "@Threynaud This workaround has a bug when all of the loss values are masked in a batch.", "@lipixun Do you refer to the fact that the gradient clipping helper routine does not handle the case when the global norm is zero?", "@ilya-edrenkin Ooops, no. My fault, I didn't read the above codes carefully. Applying`reduce_sum` instead of `reduce_mean` to masked loss values can avoid `nan` error.\r\n", "Fixed in #13274."]}, {"number": 12452, "title": "Fix issue in `variable` docs", "body": "A couple of doc error in `programmers_guide/variables.md`\r\n\r\nThis fix fixes #12444.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?"]}, {"number": 12451, "title": "The tensorflow wasn't compiled to use AVX2 on Windows", "body": "I instaled the python 3 and the tensorflow from pip.\r\nBut when I run the program I receive the warnings:\r\n![image](https://user-images.githubusercontent.com/29586672/29518621-76586372-8650-11e7-8a86-36e4fce5923d.png)\r\n\r\n\r\nHow to hidden or fix the warnings on Windows? Thank you so much!", "comments": ["They're just warnings informing you that you could get improved performance by compiling from source for your machine. You can safely ignore them. If you desperately want to suppress them then you can but you risk missing other warnings as afaik the only way to suppress is to set the over TF logging limit so not ideal. If you can live with them then it's fine. Nothing is really wrong!", "I don't think so. AFAIK it's not possible now to compile tensorflow with AVX2 instructions on Windows because there are some intristics used that Visual Studio compiler doesn't support now.", "Good to know. Either way, OP shouldn't be overly concerned about this unless they were expecting the binary to have AVX2!", "Use below code in your .py file\r\n\r\n`import os`\r\n`os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'`\r\n\r\n"]}, {"number": 12450, "title": "Freezing Model drops Output Accuracy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv1.2 and v1.3 (Same Problem repeats on both)\r\n- **Python version**: \r\n3.5\r\n- **CUDA/cuDNN version**:\r\nCuda compilation tools, release 8.0, V8.0.44\r\ncuDNN Version 5.1.10\r\n- **GPU model and memory**:\r\nGeForce GTX 1060 - 6GB\r\n\r\nI have a image segmentation network designed to classify roads and obstacles.  I want to freeze the model and serve it as a API. So I used the default TensorFlow tool for freezing the model. After freezing, the output given by the network are completely off and inaccurate.\r\n\r\nHere is one sample. \r\n\r\n**The Input Image**\r\n\r\n![00016](https://user-images.githubusercontent.com/13032916/29518128-d016402e-8695-11e7-95e5-1fdac3e5b0f6.png)\r\n\r\n**Output when tested using ckpt files**\r\n![16_actual](https://user-images.githubusercontent.com/13032916/29518247-3623ea56-8696-11e7-94e0-6eabe13ab066.png)\r\n\r\n**Output after Freezing the Model**\r\n![16](https://user-images.githubusercontent.com/13032916/29518258-4087803e-8696-11e7-9a56-4c5d45d7fda1.png)\r\n\r\nI have tried to freeze using different versions of tensorflow, but that has not helped. Since the network is performing as excepted when tested against checkpoint, the issue, I think is in the Freeze Models Script. The network uses Batch_normalisation. Could this be the reason for this drop because I saw a couple of issues linked to that of similar nature? How can I avoid that?\r\n", "comments": ["Are you able to provide code, an example image, and steps for us to reproduce this problem?", "@ali01  I have added more details and code [here](https://stackoverflow.com/questions/45796759/freezing-model-drops-output-accuracy)..", "## System information\r\n\r\n**Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\nYes\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\nWindows 10\r\n**TensorFlow installed from (source or binary):**\r\nSource\r\n**TensorFlow version (use command below):**\r\nv1.3\r\n**Python version:**\r\n3.5\r\n**CUDA/cuDNN version:**\r\nNo GPU, we're trying to deploy on CPU\r\n\r\nI've encountered the same problem with freezing the model. I'm working for a company where we want to serve the model in C++, so I've frozen the network and also found differences in network output.\r\n\r\nI've double checked the output of the frozen network in C++ and Python and they are the same. However the output is significantly different when comparing it to the model loaded with a checkpoint. There is no workaround for us to continue other than trying older versions of TensorFlow to see if the issue is not there.\r\n\r\nWe also use Batch Normalization in the network.", "I've managed to figure out the issue on our side. Some post-processing steps were frozen, which wasn't clear from our codebase.", "@anandcu3  have you had any luck resolveing your issue?", "Nevermind, read the stackoverflow.", "@Jabba @anandcu3  how do you solve it\uff1f can you give some advises?"]}, {"number": 12449, "title": "std::system_error after restoring model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, used saving/restoring example [here](https://www.tensorflow.org/programmers_guide/saved_model)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Red Hat 4.4.7-1\r\n- **TensorFlow installed from (source or binary)**: Anaconda\r\n- **TensorFlow version (use command below)**: 1.2, CPU-only\r\n- **Python version**: 3.5\r\n- **Exact command to reproduce**: [See example here](https://www.tensorflow.org/programmers_guide/saved_model). Error occurs on line `saver.restore(sess, \"/tmp/model.ckpt\")`\r\n\r\n### Describe the problem\r\nWhen restoring the model using the simple example listed above, the program terminates with the error: \r\n```\r\nterminate called after throwing and instance of 'std::system_error'\r\nwhat(): Resource temporarily unavailable\r\n```\r\nGiven such a simple example it can't be a memory issue, and there seems to be no other information about this error message online (eg. stackoverflow). Could it be a bug?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12448, "title": "Implement BiasAddGradHelper.", "body": "Registers the BiasAddGrad op as the gradient for BiasAdd by implementing BiasAddGradHelper.\r\n\r\nUnlike the other tested ops in nn_grad_tests.cc, BiasAdd takes two arguments. The test I added does test both arguments, but not simultaneously. Not sure if that's acceptable. Guidance appreciated.\r\n\r\nThis is only my second PR to TensorFlow. When giving feedback, please assume that I barely know what I'm doing.\r\n", "comments": ["@bpiel, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @DeNeutoy and @lakshayg to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for the PR\r\n@suharshs : Mind taking a look?", "@suharshs back to you", "Jenkins, test this please"]}, {"number": 12447, "title": "reuse RefCountIsOne function member", "body": "reuse RefCountIsOne function member, and replace the same implement.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 12446, "title": "tensorflow cmake compile problem on vs2015", "body": "------------------------\r\n\r\n### System information\r\n- **windows10**:\r\n- **install tensorflow**: from source by vs2015 \r\n- **TensorFlow version**: r1.2\r\n- **Python**: Anaconda 4.2.0, python3.5\r\n-**SWIG**:  swigwin3.0.10\r\n-**CMake**:  3.8.2\r\n- **Command line to compile**:\r\n`D:\\Developer\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\vcvars64.bat`\r\n`cd tensorflow\\tensorflow\\contrib\\cmake`\r\n` mkdir build`\r\n`cd build`\r\n`cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^`\r\n`More? -DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.10/swig.exe ^`\r\n`More? -DPYTHON_EXECUTABLE=D:/Developer/Anaconda3/python.exe ^`\r\n`More? -DPYTHON_LIBRARIES=D:/Developer/Anaconda3/libs/python35.lib ^`\r\n`More? -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX`\r\n### Describe the problem\r\nafter generate the configure files, run the command as bellow:\r\n`MSBuild /M:10 /p:Configuration=Release tf_tutorials_example_trainer.vcxproj`   \r\nI have used this command to compile the tensorflow c++ example project and some c++ libs successfully about a month ago.But now, I tried many times never success.\r\n\r\n#### Error log\r\nBut this time it didn't work, and return the compile error :  \r\n`\"C:\\Users\\Administrator\\Desktop\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_tutorials_example_trainer.vcxproj\"\r\n       (default target) (1) ->\r\n       \"C:\\Users\\Administrator\\Desktop\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj\" (default ta\r\n       rget) (6) ->\r\n       (CustomBuild target) ->\r\n         C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd\r\n       .exe\" exited with code 1. [C:\\Users\\Administrator\\Desktop\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_fram\r\n       ework.vcxproj]\r\n    277 Warning(s)\r\n    1 Error(s)`\r\n", "comments": ["@mrry I hope you can look this problem and help me please.", "Can you upload the full log from `MSBuild`? From that error message, it's not clear what went wrong.", "Try compiling without:\r\n-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\n\r\nI only managed to compile with SIMD after doing some code changes in kernel classes, haven't tried 1.3 but would not surprised if the issue is still there.\r\n\r\nCheers,\r\nRalf \r\n", "@mrry  This is the log file, but it seems too big, and not too many obvious message, maybe I'm not familiar with compile technology and the tensorflow projects.\r\n  [Example_trainer_ProjectOutput.txt](https://github.com/tensorflow/tensorflow/files/1240679/Example_trainer_ProjectOutput.txt)\r\n", "The full logs give some more context for the error message:\r\n\r\n```\r\n                     Generating C:/Users/Administrator/Desktop/tensorflow/tensorflow/core/util/version_info.cc (TaskId:136)\r\n                     Traceback (most recent call last): (TaskId:136)\r\n                       File \"C:/Users/Administrator/Desktop/tensorflow/tensorflow/tools/git/gen_git_source.py\", line 266, in <module> (TaskId:136)\r\n                         raw_generate(args.raw_generate) (TaskId:136)\r\n                       File \"C:/Users/Administrator/Desktop/tensorflow/tensorflow/tools/git/gen_git_source.py\", line 230, in raw_generate (TaskId:136)\r\n                         git_version = get_git_version(\".\") (TaskId:136)\r\n                       File \"C:/Users/Administrator/Desktop/tensorflow/tensorflow/tools/git/gen_git_source.py\", line 156, in get_git_version (TaskId:136)\r\n                         str(\"--work-tree=\" + git_base_path), \"describe\", \"--long\", \"--tags\" (TaskId:136)\r\n                       File \"C:\\Users\\Administrator\\Anaconda3\\lib\\subprocess.py\", line 626, in check_output (TaskId:136)\r\n                         **kwargs).stdout (TaskId:136)\r\n                       File \"C:\\Users\\Administrator\\Anaconda3\\lib\\subprocess.py\", line 693, in run (TaskId:136)\r\n                         with Popen(*popenargs, **kwargs) as process: (TaskId:136)\r\n                       File \"C:\\Users\\Administrator\\Anaconda3\\lib\\subprocess.py\", line 947, in __init__ (TaskId:136)\r\n                         restore_signals, start_new_session) (TaskId:136)\r\n                       File \"C:\\Users\\Administrator\\Anaconda3\\lib\\subprocess.py\", line 1224, in _execute_child (TaskId:136)\r\n                         startupinfo) (TaskId:136)\r\n                     FileNotFoundError: [WinError 2] The system cannot find the file specified (TaskId:136)\r\n03:14:03.769     3>C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\Users\\Administrator\\Desktop\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj]\r\n```\r\n\r\nIn this case, it looks like `git` can't be found in your `%PATH%`. Can you check that it is possible to run the command-line `git` tool from the command prompt where you started the build?", "@mrry  yes, I forget to add the `git` into my `Path`. Right now, I add it into my `Path` and rebuild the project, hope it will work. \r\nThanks for your help, I build it by cmake. And I find it a little miraculous\u3002"]}, {"number": 12445, "title": "proto files compiled with go: inconsistent package names: tensorflow.grpc tensorflow", "body": "when I compile proto files with go, I got the error:\r\n`protoc-gen-go: error:inconsistent package names: tensorflow.grpc tensorflow\r\n--go_out: protoc-gen-go: Plugin failed with status code 1.`\r\n\r\nmy command:\r\n`protoc -I=./tensorflow_serving  --go_out=plugins=grpc:. ./tensorflow/core/protobuf/*.proto`\r\ntensorflow version: 1.3.0\r\n\r\nI found that in the tensorflow/core/protobuf/master_service.proto and tensorflow/core/protobuf/worker_service.proto go package  declarations is \r\n\r\n> package tensorflow.grpc;\r\n\r\nand other proto files in protobuf is\r\n\r\n> package tensorflow.grpc;\r\n\r\ngo do not support different package names in one package\u3002\r\n", "comments": ["Could you provide more detailed instructions on reproducing the problem? The exact sequence of commands to reproduce the environment and the problem would be helpful, as would some more context on what you're trying to do.\r\n\r\nHaving dots in the package names in the `.proto` file by itself is not a problem. According to [protocol buffer reference pages](https://developers.google.com/protocol-buffers/docs/reference/go-generated#package), the Go code generator handles dots in the package name by converting them to underscores.\r\n\r\nThe error you're seeing might be because you're trying to generate code for all `.proto` files in a single Go package. Depending on what you want to do, you might want to either generate code from only the `.proto` files that you need use multiple `protoc` invocations, each with a different subset of the `.proto` files.\r\n\r\n", "@shallam66 I noticed that you are using the tensorflow serving protos with tensorflow protos. I had a similar issue when compiling protobufs when making a minimalist repo.\r\n\r\nhttps://github.com/Ouwen/minflow_serve_proto\r\n\r\nIn this repo are the protos from tensorflow set in the original tree structure. I had to add\r\n`option go_package` to all of `.proto` files which did not already have it.\r\n\r\nLet me know if this fits what you are trying to do. ", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "It is still a problem. I have 'option go_package' in each of my proto files and I'm getting this error.", "This is due to the existence of multiple proto files in a single folder.There can only be one proto file in the folder where the proto command is executed."]}, {"number": 12444, "title": "The sample code on PROGRAMMER'S GUIDE has a problem", "body": "Hello,\r\n\r\nThere is a problem on the PROGRAMMER'S GUIDE.\r\nhttps://www.tensorflow.org/programmers_guide/variables#sharing_variables\r\n\r\nIn the second code snippet of the chapter \"Sharing variables\"\r\n```Python\r\ninput1 = tf.random_normal([1,10,10,32])\r\ninput2 = tf.random_normal([1,20,20,32])\r\nx = conv_relu(input1, kernel_shape=[5, 5, 1, 32], bias_shape=[32])\r\nx = conv_relu(x, kernel_shape=[5, 5, 32, 32], bias_shape = [32])  # This fails.\r\n```\r\n\r\n### Problem\r\nThe shape doesn't match.\r\n\r\n### Solution\r\nChange the third line to\r\n```Python\r\nx = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])\r\n```\r\n\r\nAnd the code after this also have the problem.", "comments": ["Added PR #12452 for doc fix."]}, {"number": 12443, "title": "R1.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Please describe the purpose of the pull request in the future. Thanks!", "I am so sorry about this.I am new to github and I clicked the Pull Request by accident and didn't know how to cancel it.", "I signed it"]}, {"number": 12442, "title": "Installation failure on Windows 10", "body": "Hi,\r\n\r\ni'm having trouble installing tensorflow. It already worked and evereything was fine when i changed cuDNN to 6.0 for something else.\r\nBut now, even after installing everything from the drivers of the gpu, to CUDA and cudnn5.1 new, tensorflow installation is failing. I am always getting this error, but every solution i found in other issues with similar errors is not working.\r\nAll the pathes are set correct. Anyone has an idea?\r\n\r\n`>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tenso\r\nrflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_\r\nmodule\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 985, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 957, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tenso\r\nrflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tenso\r\nrflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tenso\r\nrflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_\r\nmodule\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line\r\n24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\"\r\n, line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tenso\r\nrflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tenso\r\nrflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pac\r\nkages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pac\r\nkages\\tensorflow\\python\\__init__.py\", line 49, in <module>                ed\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pac\r\nkages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>       e\r\n    raise ImportError(msg)                                                oved\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pac\r\nkages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_i\r\nmport_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlikages\\tensorflow\\python\\pywrap_tenso\r\nb\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 985, in _gcd_import          kages\\tensorflow\\python\\pywrap_tenso\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 957, in _find_and_load_unlock\r\ned                                                                        kages\\tensorflow\\python\\pywrap_tenso\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\rkoch\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help`", "comments": ["What version of Tensorflow please?\r\n\r\nI see you are trying to install on Windows, @mrry has a great installation troubleshooter script that can help pinpoint issues available here: [mrry's Self-check Script](https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c)", "It seems the current tensorflow build referenced from https://www.tensorflow.org/install/install_windows actually requires CuDNN 6.0 instead of CuDNN 5.1 as the text specifies.\r\n\r\nApparently the installation instructions are out of sync with the install package.", "Yes thank you @jubjamie this helped me a lot.\r\nThe install instructions are outdated and thus i had installed the wrong cudnn.\r\n\r\nNow it works fine again. \r\n", "Feel free to register your concern on #11784 I've been trying to get this fixed as you're not the first to have this issue with the installation docs."]}, {"number": 12441, "title": "Installation problem while building tensorflow(GPU) ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["_Warning: As you've not filled in the required info above you may not get the support you're looking for from the devs. Perhaps this is an issue for Stack Overflow?_\r\n\r\nYou won't get any support without completing the issue template. Please edit it in or re-file and you'll get much better support!"]}, {"number": 12440, "title": "How to Retrain Inception v3 without a reshape layer?", "body": "Hi all,\r\n\r\nI was trying the pretrained Inception v3 model available in tensorflow.When i was looking into the tensorgraph of the retrained model i found that while retrianing it uses a reshape layer followed by a full connected layer.\r\n\r\nIs there any method to retrain the model without using the reshape layer?\r\nPlease suggest me any possible solution if possible.\r\n\r\nThank and Regards", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\n\r\nDo you mean the reshape layer to 299,299,3 near the start?", "Thank you for comment @jubjamie .i am referring to transfer learning in tensorflow for Inception v3 model.In the retrain.py file a bottlenck_tensor is added after the final layer (pool_3) as pool_3/_reshape:0 .My question will tenosrflow retraining support if i alter the retrain.py file .for e.g remove the reshape layer being added for retraining?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12439, "title": "Train Multibox object detector included in the TF Detect Android demo", "body": "Can anyone guide me how to train our own multibox_model.pb included in the tensorflow android example??", "comments": ["@VajiraPrabuddhaka multibox_model.pb is deprecated, we now suggest using ssd mobilenet which is the current default in the TF Detect demo. This model may be trained using the [TF Object Detection API](https://github.com/tensorflow/models/tree/master/object_detection).\r\n\r\nClosing as this is neither a bug nor feature request. For further assistance please see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) where there's a larger support community.", "TF Object Detection API is meanwhile located at\r\nhttps://github.com/tensorflow/models/tree/master/research/object_detection"]}, {"number": 12438, "title": "Update Readme.md", "body": "Fixed run-on sentence(s?)\r\n\r\n(I'm just trying to help please don't hurt me.)", "comments": ["@TheLastAlly, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @alanyee and @superyyrrzz to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the contribution! I personally prefer the existing wording, so I'll close this out."]}, {"number": 12437, "title": "Tensorflow GPU import causes exeptions [Resolved]", "body": "There is a problem occures when I import tensorflow.\r\n\r\n> \r\n> \"C:\\Program Files\\Python35\\python.exe\" D:/NetDisks/DropBox/Dropbox/Develop/Python/TensorFlow/GPUTest.py\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n>     return importlib.import_module(mname)\r\n>   File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n>   File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n>   File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n> ImportError: DLL load failed: \u041d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u0434\u0443\u043b\u044c.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n>     return importlib.import_module('_pywrap_tensorflow_internal')\r\n>   File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n> ImportError: No module named '_pywrap_tensorflow_internal'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"D:/NetDisks/DropBox/Dropbox/Develop/Python/TensorFlow/GPUTest.py\", line 1, in <module>\r\n>     import tensorflow as tf\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import *\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n>     return importlib.import_module(mname)\r\n>   File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n>   File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n>   File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n>   File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n>   File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n>   File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n>   File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n> ImportError: DLL load failed: \u041d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u0434\u0443\u043b\u044c.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n>     from tensorflow.python.pywrap_tensorflow_internal import *\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n>     _pywrap_tensorflow_internal = swig_import_helper()\r\n>   File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n>     return importlib.import_module('_pywrap_tensorflow_internal')\r\n>   File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n>     return _bootstrap._gcd_import(name[level:], package, level)\r\n> ImportError: No module named '_pywrap_tensorflow_internal'\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> Process finished with exit code 1\r\n> \r\n> \r\n\r\nWindows 7 x64 SP1\r\n\r\nCPU: Intel Core I7 990x\r\nMotherboard: Asus P6T SE\r\nGPU: GTX 1080 TI\r\n\r\ntensorflow have been installed with native PIP, command: \r\n\r\n> pip install tensorflow-gpu\r\n\r\nPython 3.5.2\r\n\r\nCUDA 8.0 Toolkit\r\n\r\nCUDNN 5.1\r\n\r\nThe code is:\r\n`import tensorflow as tf`\r\n\r\nPath:\r\n\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin;\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\libnvvp;\r\n> C:\\Program Files\\Python35\\Scripts\\;\r\n> C:\\Program Files\\Python35\\;\r\n> C:\\Program Files\\Microsoft MPI\\Bin\\;\r\n> C:\\Windows\\;\r\n> C:\\Windows\\System32;\r\n> C:\\Program Files\\dotnet\\;\r\n> C:\\Program Files\\Anaconda3;\r\n> C:\\Program Files\\Anaconda3\\Scripts;\r\n> C:\\Program Files\\Anaconda3\\Library\\bin;\r\n> C:\\Program Files\\Anaconda2;\r\n> C:\\Program Files\\Anaconda2\\Scripts;\r\n> C:\\Program Files\\Anaconda2\\Library\\bin;\r\n> C:\\Program Files (x86)\\Anaconda3;\r\n> C:\\Program Files (x86)\\Anaconda3\\Scripts;\r\n> C:\\Program Files (x86)\\Anaconda3\\Library\\bin;\r\n> C:\\Program Files (x86)\\Anaconda2;\r\n> C:\\Program Files (x86)\\Anaconda2\\Scripts;\r\n> C:\\Program Files (x86)\\Anaconda2\\Library\\bin;\r\n> C:\\Program Files\\NVIDIA Corporation\\cuda\\bin;\r\n> C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;\r\n> C:\\Program Files\\NVIDIA Corporation\\cuda\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin;\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0;\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64;\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64;\r\n\r\nCUDA_HOME\r\n\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\r\n\r\nCUDA_PATH\r\n\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\r\n\r\nCUDA_PATH_V8_0\r\n\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\r\n\r\n\r\nMSVC++ 2015 x64 is installed\r\n\r\ncudnn64_5.dll is located at:\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\r\n\r\n\r\nAlso if i install a regular tensorflow, it works OK.\r\n\r\nHow do I deal with/fix this?\r\n\r\n", "comments": ["Newermind, I was using TF 1.3, whinch is using CUDNN 6.0"]}, {"number": 12436, "title": "zeros_like doesn't fully respect the optimize argument", "body": "[The definition of `zeros_like`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1463) is:\r\n\r\n```python\r\ndef zeros_like(tensor, dtype=None, name=None, optimize=True):\r\n  with ops.name_scope(name, \"zeros_like\", [tensor]) as name:\r\n    tensor = ops.convert_to_tensor(tensor, name=\"tensor\")\r\n\r\n    if tensor.shape.is_fully_defined():\r\n      # We can produce a zeros tensor independent of the value of 'tensor',\r\n      # since the shape is known statically.\r\n      return zeros(tensor.shape, dtype=dtype or tensor.dtype, name=name)\r\n\r\n    if dtype is not None and dtype != tensor.dtype:\r\n      return zeros(\r\n          shape_internal(tensor, optimize=optimize), dtype=dtype, name=name)\r\n    else:\r\n      return gen_array_ops._zeros_like(tensor, name=name)\r\n```\r\n\r\nWe can see that if the shape of `tensor` is already known, the `optimize` parameter is ignored, which is inconsistent with the [documented behavior](https://www.tensorflow.org/api_docs/python/tf/zeros_like).", "comments": ["I think `optimize` flag should be respected. Added a PR #12459 for it.", "@benoitsteiner, could you please update this issue upon concluding review of #12459? Thanks"]}, {"number": 12435, "title": "typo in docs: one ==> once", "body": "in tensorflow/docs_src/programmers_guide/datasets.md", "comments": ["@youkaichao, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @jsimsa and @jhseu to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 12434, "title": "OpenMP/OpenACC support for tensorflow", "body": "Is there any effort in porting the Tensorflow using OpenMP or OpenACC?", "comments": ["@andydavis1 and @benoitsteiner looked at it a while back. There were two issues\r\n1. TensorFlow already has its own set of primitives for parallelizing work-loads\r\n2. It was hard to make it build: compiling with -fopenmp made some seemingly unrelated parts of TensorFlow work incorrectly", "Thank you, Yaroslav. Closing for now.", "In the exploration of this issue, was attention paid to using Eigen in multi-threaded applications?\r\n\r\nPlease see the note on calling Eigen::initParallel() before creating application threads, in the following:\r\n https://eigen.tuxfamily.org/dox/TopicMultiThreading.html\r\n", "@yaroslavvb Could OpenACC be used to optimize parallel tasks like matmuls and convolutions (IDK if you use a library) on a wider range of accelerators (like AMD GPUs), that the CUDA kernels and CPU-oriented code are not targeting? I think that this could complement the existing Tensorflow primitives for parallelization."]}, {"number": 12433, "title": "Use XLA_TEST_F to allow disabling of individual tests", "body": "converting test to allow individual test cases to be disabled using the CC test disable manifest.", "comments": ["@DavidNorman, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener and @hawkinsp to be potential reviewers.", "Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 12432, "title": "[android demo] Fix applicationId to 'org.tensorflow.demo'", "body": "For nativeBuildSystem == 'cmake' change `applicationId = 'com.tensorflow.demo'` to `applicationId = 'org.tensorflow.demo'`\r\nOtherwise Android system treat bult app (com.tensorflow.demo) as separate one and install it alonside to org.tensorflow.demo", "comments": ["@ArtsiomCh, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp, @tensorflower-gardener and @markmcd to be potential reviewers.", "Can one of the admins verify this patch?"]}, {"number": 12431, "title": "[android demo] UnsatisfiedLinkError exception on app start if build with nativeBuildSystem = 'none'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\napp build at Android Studio 2.3.3 on Linux Ubuntu 16.04\r\napp tested on Meizu M2 Note (Android 5.1, API 22) \r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nMaster branch from GitHub\r\n- **Python version**: \r\n2.7 (not used in that case)\r\n- **Bazel version (if compiling from source)**:\r\n0.5.3 (not used in that case) \r\n- **CUDA/cuDNN version**:\r\nnone\r\n- **GPU model and memory**:\r\nnone\r\n- **Exact command to reproduce**:\r\nAndroid Studio/Run (build.gradle)\r\n\r\n### Describe the problem\r\nWith `nativeBuildSystem = 'none'` app exit with UnsatisfiedLinkError exception (see log below).\r\nQuick investigation shows call of `env.ImageUtils.convertYUV420SPToARGB8888(Native Method)` native method from `CameraActivity.onPreviewFrame(CameraActivity.java:113)`. \r\n`convertYUV420SPToARGB8888` suppose to be implemented in `libtensorflow_demo.so` wich is not created in case of `nativeBuildSystem = 'none'`. (With `nativeBuildSystem = 'cmake'` `libtensorflow_demo.so` created and `convertYUV420SPToARGB8888` works fine)\r\nQuick fix may be obtained by including `libtensorflow_demo.so` into JCenter and download it like the TensorFlow Inference Interface package.\r\nAnother solution would be Java implementation of `convertYUV420SPToARGB8888`, the same as it been done for `convertYUV420ToARGB8888` in commit #003deb8 at PR #10771 \"Refactor and implementation of the camera API 1, it fixes #8736 #10771\".\r\nI start working with that Java implementation of `convertYUV420SPToARGB8888` but I don't think I have enough skills to make it. At the moment I create Java wrapper for native convertYUV420SPToARGB8888 and make all native methods `private` to isolate the rest of the Java code from occasional use them:\r\n[https://github.com/ArtsiomCh/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/CameraActivity.java](url)\r\n[https://github.com/ArtsiomCh/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/env/ImageUtils.java](url)\r\nWould be great if @osdamv (creator of Java implementation of `convertYUV420ToARGB8888`) can help in porting Java implementation of `convertYUV420ToARGB8888` for `convertYUV420SPToARGB8888`\r\n\r\n### Source code / logs\r\n08-20 13:28:38.212 19058-19058/? I/TensorFlowInferenceInterface: Model load took 1303ms, TensorFlow version: 1.2.0\r\n08-20 13:28:38.217 19058-19058/? I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/tensorflow_inception_graph.pb'\r\n08-20 13:28:38.219 19058-19058/? I/TensorFlowImageClassifier: Read 1001 labels, output layer size is 1008\r\n08-20 13:28:38.219 19058-19058/? I/tensorflow: ClassifierActivity: Sensor orientation: 90, Screen orientation: 0\r\n08-20 13:28:38.219 19058-19058/? I/tensorflow: ClassifierActivity: Initializing at size 640x480\r\n08-20 13:28:38.223 19058-19058/? W/tensorflow: ImageUtils: Native library not found, native RGB -> YUV conversion may be unavailable.\r\n08-20 13:28:38.225 19058-19058/? E/art: No implementation found for void org.tensorflow.demo.env.ImageUtils.convertYUV420SPToARGB8888(byte[], int[], int, int, boolean) (tried Java_org_tensorflow_demo_env_ImageUtils_convertYUV420SPToARGB8888 and Java_org_tensorflow_demo_env_ImageUtils_convertYUV420SPToARGB8888___3B_3IIIZ)\r\n08-20 13:28:38.226 19058-19058/? E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                   Process: org.tensorflow.demo, PID: 19058\r\n                                                   java.lang.UnsatisfiedLinkError: No implementation found for void org.tensorflow.demo.env.ImageUtils.convertYUV420SPToARGB8888(byte[], int[], int, int, boolean) (tried Java_org_tensorflow_demo_env_ImageUtils_convertYUV420SPToARGB8888 and Java_org_tensorflow_demo_env_ImageUtils_convertYUV420SPToARGB8888___3B_3IIIZ)\r\n                                                       at org.tensorflow.demo.env.ImageUtils.convertYUV420SPToARGB8888(Native Method)\r\n                                                       at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:113)\r\n                                                       at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1288)\r\n                                                       at android.os.Handler.dispatchMessage(Handler.java:111)\r\n                                                       at android.os.Looper.loop(Looper.java:194)\r\n                                                       at android.app.ActivityThread.main(ActivityThread.java:5877)\r\n                                                       at java.lang.reflect.Method.invoke(Native Method)\r\n                                                       at java.lang.reflect.Method.invoke(Method.java:372)\r\n                                                       at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1020)\r\n                                                       at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:815)\r\n08-20 13:28:38.265 19058-19058/? I/Process: Sending signal. PID: 19058 SIG: 9", "comments": ["@andrewharp @petewarden Can either of you take a look?", "@ArtsiomCh The missing conversion does need to be remedied, but in the meantime as a workaround you could try overriding CameraActivity.useCamera2API = true. This will force it to use the camera2 API, which we may be too aggressive in disabling in some cases.", "@andrewharp Overriding useCamera2API works for TF Classify, sadly it doesn't for TF Detect or TF Stylize, at least the errors are different, I'm assuming they are related to the 'nativeBuildSystem' as well as I get an error log about before the problem, maybe @ArtsiomCh can confirm that.\r\n\r\nTF Detect Log:\r\n\r\n```\r\n...\r\n08-23 17:28:24.072 1250-1293/org.tensorflow.demo D/tensorflow: CameraActivity: Initializing buffer 2 at size 153599\r\n08-23 17:28:24.074 1250-1293/org.tensorflow.demo E/tensorflow: ObjectTracker: libtensorflow_demo.so not found, tracking unavailable\r\n08-23 17:28:24.074 1250-1293/org.tensorflow.demo I/tensorflow: MultiBoxTracker: Initializing ObjectTracker: 640x480\r\n08-23 17:28:24.077 1250-1293/org.tensorflow.demo E/tensorflow: ObjectTracker: Native object tracking support not found. See tensorflow/examples/android/README.md for details.\r\n08-23 17:28:24.101 1250-1293/org.tensorflow.demo E/tensorflow: MultiBoxTracker: Object tracking support not found. See tensorflow/examples/android/README.md for details.\r\n08-23 17:28:24.102 1250-1293/org.tensorflow.demo E/art: No implementation found for void org.tensorflow.demo.env.ImageUtils.convertYUV420ToARGB8888(byte[], byte[], byte[], int[], int, int, int, int, int, boolean) (tried Java_org_tensorflow_demo_env_ImageUtils_convertYUV420ToARGB8888 and Java_org_tensorflow_demo_env_ImageUtils_convertYUV420ToARGB8888___3B_3B_3B_3IIIIIIZ)\r\n08-23 17:28:24.102 1250-1293/org.tensorflow.demo W/tensorflow: ImageUtils: Native YUV -> RGB implementation not found, falling back to Java implementation\r\n08-23 17:28:24.419 1250-1294/org.tensorflow.demo V/RenderScript: 0x55a129a360 Launching thread(s), CPUs 6\r\n08-23 17:28:24.684 1250-1292/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[image_tensor], outputs:[detection_boxes, detection_scores, detection_classes, num_detections]\r\n                                                                                 \r\n                                                                                 --------- beginning of crash\r\n08-23 17:28:24.685 1250-1292/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n                                                                   Process: org.tensorflow.demo, PID: 1250\r\n                                                                   java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                     device='CPU'; T in [DT_FLOAT]\r\n                                                                     device='CPU'; T in [DT_INT32]\r\n                                                                     device='GPU'; T in [DT_STRING]\r\n                                                                     device='GPU'; T in [DT_BOOL]\r\n                                                                     device='GPU'; T in [DT_INT32]\r\n                                                                     device='GPU'; T in [DT_FLOAT]\r\n                                                                   \r\n                                                                   \t [[Node: Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/cond/Switch = Switch[T=DT_BOOL](Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater, Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater)]]\r\n                                                                       at org.tensorflow.Session.run(Native Method)\r\n                                                                       at org.tensorflow.Session.access$100(Session.java:48)\r\n                                                                       at org.tensorflow.Session$Runner.runHelper(Session.java:295)\r\n                                                                       at org.tensorflow.Session$Runner.run(Session.java:245)\r\n                                                                       at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:142)\r\n                                                                       at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.recognizeImage(TensorFlowObjectDetectionAPIModel.java:158)\r\n                                                                       at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:340)\r\n                                                                       at android.os.Handler.handleCallback(Handler.java:742)\r\n                                                                       at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                       at android.os.Looper.loop(Looper.java:154)\r\n                                                                       at android.os.HandlerThread.run(HandlerThread.java:61)\r\n08-23 17:28:24.689 1250-1292/org.tensorflow.demo E/MQSEventManagerDelegate: failed to get MQSService.\r\n...\r\n```\r\n\r\nTF Stylize Log:\r\n```\r\n...\r\n08-23 17:31:24.363 3146-3187/org.tensorflow.demo D/tensorflow: CameraActivity: Initializing buffer 1 at size 460799\r\n08-23 17:31:24.365 3146-3187/org.tensorflow.demo D/tensorflow: CameraActivity: Initializing buffer 2 at size 460799\r\n08-23 17:31:24.369 3146-3187/org.tensorflow.demo W/tensorflow: ImageUtils: Native library not found, native RGB -> YUV conversion may be unavailable.\r\n08-23 17:31:24.369 3146-3187/org.tensorflow.demo E/art: No implementation found for void org.tensorflow.demo.env.ImageUtils.convertYUV420ToARGB8888(byte[], byte[], byte[], int[], int, int, int, int, int, boolean) (tried Java_org_tensorflow_demo_env_ImageUtils_convertYUV420ToARGB8888 and Java_org_tensorflow_demo_env_ImageUtils_convertYUV420ToARGB8888___3B_3B_3B_3IIIIIIZ)\r\n08-23 17:31:24.369 3146-3187/org.tensorflow.demo W/tensorflow: ImageUtils: Native YUV -> RGB implementation not found, falling back to Java implementation\r\n08-23 17:31:24.589 3146-3187/org.tensorflow.demo I/tensorflow: StylizeActivity: Initializing at size preview size 1280x720, stylize size 256\r\n08-23 17:31:24.620 3146-3186/org.tensorflow.demo I/tensorflow: StylizeActivity: Width: 256 , Height: 256\r\n08-23 17:31:25.127 3146-3186/org.tensorflow.demo E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[input, style_num], outputs:[transformer/expand/conv3/conv/Sigmoid]\r\n                                                                                 \r\n                                                                                 --------- beginning of crash\r\n08-23 17:31:25.128 3146-3186/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n                                                                   Process: org.tensorflow.demo, PID: 3146\r\n                                                                   java.lang.IllegalArgumentException: input_max_range must be larger than input_min_range.\r\n                                                                   \t [[Node: transformer/contract/conv2/Relu_eightbit_quantize_transformer/contract/conv2/InstanceNorm/batchnorm/add_1 = QuantizeV2[T=DT_QUINT8, mode=\"MIN_FIRST\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](transformer/contract/conv2/InstanceNorm/batchnorm/add_1, transformer/contract/conv2/Relu_eightbit_min_transformer/contract/conv2/InstanceNorm/batchnorm/add_1, transformer/contract/conv2/Relu_eightbit_max_transformer/contract/conv2/InstanceNorm/batchnorm/add_1)]]\r\n                                                                       at org.tensorflow.Session.run(Native Method)\r\n                                                                       at org.tensorflow.Session.access$100(Session.java:48)\r\n                                                                       at org.tensorflow.Session$Runner.runHelper(Session.java:295)\r\n                                                                       at org.tensorflow.Session$Runner.run(Session.java:245)\r\n                                                                       at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:142)\r\n                                                                       at org.tensorflow.demo.StylizeActivity.stylizeImage(StylizeActivity.java:548)\r\n                                                                       at org.tensorflow.demo.StylizeActivity.access$1200(StylizeActivity.java:70)\r\n                                                                       at org.tensorflow.demo.StylizeActivity$3.run(StylizeActivity.java:500)\r\n                                                                       at android.os.Handler.handleCallback(Handler.java:742)\r\n                                                                       at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                       at android.os.Looper.loop(Looper.java:154)\r\n                                                                       at android.os.HandlerThread.run(HandlerThread.java:61)\r\n08-23 17:31:25.149 3146-3186/org.tensorflow.demo E/MQSEventManagerDelegate: failed to get MQSService.\r\n08-23 17:31:25.183 3146-3146/org.tensorflow.demo I/RequestQueue: Repeating capture request cancelled.\r\n...\r\n```", "In my case, the workaround with useCamera2API works with both TF Classify and TF Stylize. And don't work with TF Detect (which it's not supposed to work as of #10771).\r\n\r\nAny case, building with `nativeBuildSystem = 'cmake'` give us `libtensorflow_demo.so` with native implementations of `convertYUV420SPToARGB8888` and `convertYUV420ToARGB8888`. That makes the demo to work way faster than with Java implementations. I would suggest making that way to build as the default way as it gives the best experience. \r\nThe only issue with TF Detect needs to be fixed.\r\n\r\nI also found that Java implementation of `convertYUV420SPToARGB8888` may be not too complicated as I thought before. Hopefully, I'll be able to implement it within few days.", "@ArtsiomCh do you still have problems in TF Detect after building with cmake?", "Yes, TF Detect still not working even after cmake build. By the log output\nit looks like something goes wrong inside Tensorflow. Will try to fix it\nafter finishing solution for the current issue, which I hope to make PR\ntomorrow.\nIn the meantime, it would be great if you can create issue for TF Detect\ncase and describe what you had found. Then I'll top up it with errors I've\ngot and hopefully we'll be able to find a solution.\n\nWith Best regards,\nArtsiom Chapialiou\n\nOn Aug 24, 2017 05:20, \"Ra\u00fal D\u00edaz\" <notifications@github.com> wrote:\n\n@ArtsiomCh <https://github.com/artsiomch> do you still have problems in TF\nDetect after building with cmake?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/issues/12431#issuecomment-324580781>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ANm3oAIwXbowhvJjk54xtUCxeUoCTl75ks5sbUBOgaJpZM4O8r8E>\n.\n", "Unfortunately the detector requires a newer version of the native libs than the current latest published ones in the TF AAR on bintray. This is due to a native code change in https://github.com/tensorflow/tensorflow/commit/53aabd5cb0ffcc1fd33cbd00eb468dd8d8353df2 which added the SSD detector model.\r\n\r\nYou can work around this most easily by downloading the nightly build of the tensorflow.aar [here](https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/tensorflow.aar) and placing it into your project. The other option currently is to build the libraries yourself via bazel or make.\r\n\r\nWhen 1.4 is released this issue should be remedied, but we can see about publishing an updated AAR as this is a relatively major issue with the demo.\r\n\r\n@jch1 @shawngit fyi", "@ArtsiomCh What is the cmake error you're experiencing?", "@andrewharp  It was not a cmake error. I got a similar error as @rsd-raul above:\r\n`..java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs...`\r\nWill check if your solution helps. Thank you for explanations.", "@andrewharp I've continued conversation about TF Detect at https://github.com/tensorflow/tensorflow/issues/12593#issuecomment-326760562", "@ArtsiomCh We've pushed a 1.3.1-alpha AAR (built from master between 1.3.0 and any upcoming 1.3.1 branch) which contains the native patch necessary to run the model (fixes the Switch error). We'll add some notes for future incompatibilities of this nature to suggest, when using the 'none' build type, to download at a pinned revision branch and fix the TF AAR version in build.gradle to ensure that a known compatible AAR is downloaded.\r\n\r\nClosing this issue and tracking the missing `convertYUV420SPToARGB8888` method in #12593 (I see there is also a PR pending to add this as well)\r\n", "I did some modification in TF detect android app to detect person from loaded picture rather than clicking it by camera(Not using cameraActivity).\r\n\r\nEverything is working fine except the function named ImageUtils.convertYUV420SPToARGB8888. This function returns distorted image (please find attachment)\r\n![actual_image](https://user-images.githubusercontent.com/2331214/33017067-f6c5dd42-ce16-11e7-9cb0-7827cf490c8b.png)\r\n![converted_image](https://user-images.githubusercontent.com/2331214/33017069-f6f60e2c-ce16-11e7-9772-89d0a7d22612.png)\r\n\r\nSharing part of code below:\r\n\r\n            newbitmap = Bitmap.createScaledBitmap(bitmap, 640, 480, false);\r\n            ByteArrayOutputStream output = new ByteArrayOutputStream();\r\n            newbitmap.compress(Bitmap.CompressFormat.JPEG, 100, output);\r\n            imgbytes = output.toByteArray();\r\n\r\n            imageConverter =\r\n                new Runnable() {\r\n                    @Override\r\n                    public void run() {\r\n                        ImageUtils.convertYUV420SPToARGB8888(imgbytes, previewWidth, previewHeight, rgbBytes);\r\n                    }\r\n                };", "@lavinachitara I'm not an expert, but if you use a loaded picture then isn't it already in RGB format and no needs to do YUV to RGB converting? Trying to apply YUV to RGB converting to RGB source will produce kind of trash we can see at your screenshot.", "@ArtsiomCh Thank you, yes I agree, I changed the way to use it :+1: "]}, {"number": 12430, "title": "external/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory", "body": "## System information\r\nAfter run the tf_env_collect.sh in my terminal, i get this infomation:\r\n\r\n== cat /etc/issue ===============================================\r\nLinux saners 4.10.0-32-generic #36~16.04.1-Ubuntu SMP Wed Aug 9 09:19:02 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n== uname -a =====================================================\r\nLinux saners 4.10.0-32-generic #36~16.04.1-Ubuntu SMP Wed Aug 9 09:19:02 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.4)\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nMon Aug 21 17:31:46 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro K620         Off  | 0000:01:00.0      On |                  N/A |\r\n| 34%   41C    P8     1W /  30W |    307MiB /  1999MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1014    G   /usr/lib/xorg/Xorg                             134MiB |\r\n|    0      1723    G   compiz                                          82MiB |\r\n|    0      2368    G   /proc/self/exe                                  88MiB |\r\n+-----------------------------------------------------------------------------+\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n\r\nTensorflow version:('v1.3.0-rc1-1204-g084d29e', '1.3.0')\r\n\r\n##  Describe the problem\r\n\r\n1. 1\r\nwhen i use  \"sudo bazel build //tensorflow/examples/android:tensorflow_demo\"   to get .apk \r\nI meet the error:\r\nERROR: /home/saners/tensorflow/tensorflow/core/kernels/BUILD:4581:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: arm-linux-androideabi-gcc failed: error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables ... (remaining 77 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:0,\r\n                 from ./tensorflow/core/kernels/bias_op_gpu.h:21,\r\n                 from tensorflow/core/kernels/bias_op.cc:30:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/Tensor:84:26: fatal error: cuda_runtime.h: No such file or directory\r\n #include <cuda_runtime.h>\r\n                          ^\r\ncompilation terminated.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\n\r\n1.2\r\n\r\n when i use \"bazel build //tensorflow/examples/android:tensorflow_demo\" in the terminal\r\nthe error is changed : \r\ntensorflow/core/kernels/lrn_op.cc:34:31: fatal error: cuda/include/cuda.h: No such file or directory\r\n\r\n1.3\r\n\"import tensorflow\"  and  \"import tensorflow as tf\"  in python  is ok but \r\nwhy  it has some problems after run the _tf_env_collect.sh_ for example ''ImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\r\n\"\r\nand how to fix the error   \"LD_LIBRARY_PATH is unset\" and \"DYLD_LIBRARY_PATH is unset\"\r\n\r\nmy tensorflow has installed,but i don't know how to solve this problem.\r\nAnyone can help me ? I'am very tired to deal with the error but no method works.\r\nTHANK YOU VERY MUCH!", "comments": ["Can you please fill out the default form for TF github issues so that we can attempt to diagnose the issue?", "@andrewharp I am sorry,this is my first question after registered\uff0cand i don't konw how to fill out the default form and where is it?", "It's in the form when you first open a TensorFlow issue. If you create a new issue you'll see it."]}, {"number": 12429, "title": "ADB is the tool, USB debugging is the connection", "body": "ADB is Android Debug Bridge and allows the USB debugging connection (or over WiFi, which is a more complex topic later on.\r\n\r\nBasically you need a debugging connection to the phone either as USB or WiFi", "comments": ["@LunarWatcher, thanks for your PR! By analyzing the history of the files in this pull request, we identified @andrewharp, @vrv and @tensorflower-gardener to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}]