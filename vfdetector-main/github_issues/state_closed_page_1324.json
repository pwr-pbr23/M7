[{"number": 13381, "title": "tf.train.Saver setting `max_to_keep` parameter to 0 or None unintended behavior", "body": "As the [tf.train.Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver) documentation says:\r\n\r\n\"`max_to_keep` indicates the maximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, all checkpoint files are kept. Defaults to 5 (that is, the 5 most recent checkpoint files are kept.)\"\r\n\r\nHowever, when I use `max_to_keep=0`, the `checkpoint` file attribute `all_model_checkpoint_paths` only records the most recent checkpoint file (and I'm unable to load older checkpoint files). I believe this is due to the block:\r\n\r\n```python\r\nif len(self._last_checkpoints) > self.saver_def.max_to_keep:\r\n    self._checkpoints_to_be_deleted.append(self._last_checkpoints.pop(0))\r\n```\r\n\r\nIt should probably instead read something like:\r\n\r\n```python\r\nif len(self._last_checkpoints) > self.saver_def.max_to_keep > 0:\r\n    self._checkpoints_to_be_deleted.append(self._last_checkpoints.pop(0))\r\n```", "comments": ["Hi, @nrhine1 . Could you give a reproduceble case? \r\n\r\n`max_to_keep=0` is checked at the beginning of the function:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f807b39667e84a28e83105fd29533262c257a53e/tensorflow/python/training/saver.py#L1321-L1322\r\n\r\nAnd there exists a corresponding test case as well:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f807b39667e84a28e83105fd29533262c257a53e/tensorflow/python/training/saver_test.py#L1199-L1225\r\n\r\nI mean the function seems tested throughly, and works fine, at least to me.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I meet the same issue that `checkpoint` file only record the most recent checkpoint file if I setting max_to_keep=None.", "@vikotse Could you open a new issue and provide all necessary information for reproducing the issue? "]}, {"number": 13380, "title": "'train_x, train_y = sess.run([train_x, train_y])'  leads machine run slowly", "body": "Hi Team,\r\n     My program run without error, but it seems not work, while my GPU shows running. The main code are as follows:\r\n`def main(file_name, batch_size, iter_times):\r\n    x = tf.placeholder('float', (batch_size, 32, 32, 3) )\r\n    y = tf.placeholder('float', shape = [batch_size, 10] )\r\n\r\n    predictions, _, _, _ = inference_op(x, keep_prob = 0.5)\r\n    predictions = tf.cast(predictions, tf.float32)\r\n\r\n    cost = -tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits (labels = y, logits = predictions))\r\n    correct_prediction = tf.equal(y, predictions)\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\r\n    train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\r\n    init = tf.global_variables_initializer()\r\n    sess = tf.Session()\r\n    sess.run(init)\r\n    for i in xrange(iter_times):\r\n        train_x, train_y = read_data.fetch_data(file_name, batch_size) \r\n        train_x, train_y = sess.run([train_x, train_y]) \r\n        if i % 10 == 0 :\r\n            train_accuracy = accuracy.eval(feed_dict = {x: train_x, y: train_y})\r\n            print \"%d step accuarcy is %f\" % (i, sess.run(train_accuracy))\r\n        sess.run(train)\r\n\r\nmain('train.tfrecords', 5, 2000)`\r\n\r\nThis net is based on  VGG (without the 3rd,4th,5th hidden layers) and the dataset is SVHN. I don't know how to deal with this phenomenon......\r\n    ", "comments": ["What is `read_data.fetch_data(file_name, batch_size)` ?\r\nDoes it return an operation?\r\nOverwriting the operation with the data in the next step looks bad, but shouldn't result in an error.\r\nYou should probably take a look at https://www.tensorflow.org/programmers_guide/datasets", "@LaiPiXiong this question is better for stackoverflow, this list is mainly for bugs/features (ie, filling out bug template)", "@georgh read_data.fetch_data(file_name, batch_size) returns a tensor including images and labels. Thank you for your advice! I will read it carefully.", "@yaroslavvb OK, thanks for your advice, I will try it.", "@yaroslavvb Thanks again! I found the solution on stackoverflow. \"tf.train.start_queue_runners(sess)\" is needed before sess.run() ."]}, {"number": 13379, "title": "parameterized_docker_build.sh fails", "body": "### Issue\r\n\r\nExecuting parameterized_docker_build.sh fails to generate new docker image, throws error message.\r\n\r\n### System information\r\n- I have used a stock example script provided in TensorFlow\r\n- Windows 10 professional\r\n- TensorFlow install as docker image tensorflow/tensorflow:1.3.0-devel-py3\r\n- TensorFlow version 1.3\r\n- Python version 3\r\n- Bazel version 0.5.0\r\n- CUDA/cuDNN version not relevant (CPU install)\r\n- GPU model and memory not relevant (CPU install)\r\n\r\n### Set-up\r\n\r\n```\r\n>>> docker pull tensorflow/tensorflow:1.3.0-devel-py3\r\n>>> docker run -it tensorflow/tensorflow:1.3.0-devel-py3\r\n$ cd /\r\n$ cd /tensorflow/tensorflow/tools/docker\r\n$ chmod +x  /tensorflow/tensorflow/tools/docker/parameterized_docker_build.sh\r\n$ export TF_DOCKER_BUILD_IS_DEVEL=NO\r\n$ export TF_DOCKER_BUILD_TYPE=CPU\r\n$ export TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3\r\n$ export NIGHTLY_VERSION=\"1.head\"\r\n$ export TF_DOCKER_BUILD_CENTRAL_PIP=$(echo ${TF_DOCKER_BUILD_PYTHON_VERSION} | sed s^PYTHON2^http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=${TF_DOCKER_BUILD_PYTHON_VERSION},label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp27-cp27mu-manylinux1_x86_64.whl^ | sed s^PYTHON3^http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp35-cp35m-manylinux1_x86_64.whl^)\r\n```\r\n\r\n### Command triggering issue\r\n\r\n`$ /tensorflow/tensorflow/tools/docker/parameterized_docker_build.sh`\r\n\r\n### Error messages\r\n\r\n> Required build parameters:\r\n>   TF_DOCKER_BUILD_TYPE=cpu\r\n>   TF_DOCKER_BUILD_IS_DEVEL=no\r\n>   TF_DOCKER_BUILD_DEVEL_BRANCH=NO\r\n> \r\n> Optional build parameters:\r\n>   TF_DOCKER_BUILD_CENTRAL_PIP=http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl\r\n>   TF_DOCKER_BUILD_IMAGE_NAME=\r\n>   TF_DOCKER_BUILD_VERSION=\r\n>   TF_DOCKER_BUILD_PORT=\r\n>   TF_DOCKER_BUILD_PUSH_CMD=\r\n> ERROR: docker is not available on path", "comments": ["As the error message indicates, the script can't find docker on your path. Is docker available on your system? What is your \"which docker\" output?", "Looks like this is a problem in user environment setup.\r\nClosing.", "Sorry for reopening this but I am having the same problem (host ubuntu 16.04, running on image latest-devel) and I don't understand how it is a user environment setup issue as the procedure he is following is exactly that defined in:\r\ntensorflow/tensorflow/tools/docker/README.md\r\n\r\nShouldn't the development container (latest-devel) allow usage of docker natively or is there a magic trick to run the container so that it connects to docker running on the host? Doesn't seem like docker is included in the latest-devel image and the instruction explicitly state that the parametrized_docker_build.sh script must be run with a development image container.\r\n\r\nAm I missing something?\r\n\r\n\r\n", "One needs to [install docker](https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/#set-up-the-repository) in the container. Therefore the environment was not set up properly so @gunan was right.\r\n\r\nFor example and depending on your machine:\r\n\r\n```\r\n$ apt-get update && \\\r\n    apt-get install \\\r\n    apt-transport-https \\\r\n    ca-certificates \\\r\n    curl \\\r\n    software-properties-common && \\\r\n    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - && \\\r\n    add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\r\n    $(lsb_release -cs) stable\" && \\\r\n    apt-get install docker-ce\r\n```\r\n\r\n`$which docker`\r\n> /usr/bin/docker\r\n\r\nI'm still stalled, suggest you check out #13784 if there is anything you can contribute, don't forget to edit the readme file everytime you validate a solution. This repo has way too many contributors and no proper documentation."]}, {"number": 13378, "title": "Add explanation to assist in parameterized_docker_build.sh use", "body": "Explains that using parameterized_docker_build.sh is dependent upon running from an appropriate developer image.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@SaintNazaire Please sign the Google CLA, and respond on this thread with the words \"I signed it!\" to unblock the CLA check.", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 13377, "title": "bazel version 0.6.0 unable to build latest stable release (v1.3.1)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.3.1\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.6.0\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: `bazel build //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\n\r\nWith bazel version 0.6.0, building on the latest stable tagged release (v1.3.1) is not possible. I get the following error:\r\n\r\n```\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/8ea56cff279f39ec6c0003641e649819/external/io_bazel_rules_closure/closure/private/defs.bzl:27:16: The `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false\r\nERROR: error loading package '': Extension file 'closure/private/defs.bzl' has errors\r\nERROR: error loading package '': Extension file 'closure/private/defs.bzl' has errors\r\nINFO: Elapsed time: 0.569s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\nWhen I pass the flag mentioned in the error message (`--incompatible_disallow_set_constructor=false`) to the pip package build command, I get a similar message:\r\n\r\n```\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/8ea56cff279f39ec6c0003641e649819/external/org_python_pypi_backports_weakref/BUILD.bazel:17:1: no such package '@org_python_license//': The `set` constructor for depsets is deprecated and will be removed. Please use the `depset` constructor instead. You can temporarily enable the deprecated `set` constructor by passing the flag --incompatible_disallow_set_constructor=false and referenced by '@org_python_pypi_backports_weakref//:license'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\nINFO: Elapsed time: 0.626s\r\nFAILED: Build did NOT complete successfully (28 packages loaded)\r\n    currently loading: tensorflow/core\r\n```\r\n\r\nThis problem does not exist on `master`. It would be very useful to have even a tiny stable release (v1.3.2 or something) to include this fix for those of us that like to build against a stable release.\r\n", "comments": ["I guess it's because stable release was cut before Bazel 0.6 was released, so it was pinned to bazel 0.5.4 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu\r\n\r\nI think 1.4 is coming shortly", "I uninstalled Bazel 0.6.0 and downloaded 0.5.4 from https://github.com/bazelbuild/bazel/releases.\r\n\r\nIt worked.", "Thanks!! I have this too and will downgrade bazel!!", "cc @martinwicke perhaps https://github.com/tensorflow/tensorflow/blob/e55574f28257bdacd744dcdba86c839e661b1b2a/tensorflow/docs_src/install/install_sources.md should specify a bazel version? I've had similar issues before -- the latest version of bazel couldn't build any stable release on Mac (also couldn't build master either for a couple of days)", "@damienmg Bazel making backwards incompatible changes faster than we can release fixes is not ideal. \r\n\r\n@av8ramit we could do 1.3.2 the same way we did 1.3.1. But 1.4 is close enough so maybe not. In any case, we should definitely test for exact Bazel version (down to patch) in configure. It seems necessary. ", "Humm we have been testing for that incompatible change and upstreaming several fixes to lot of projects including tensorflow. We have also got a green build from Tensorflow team with 0.6.0rc. I would be interested in knowing how those breakage have not been caught during release testing.\r\n\r\n\r\nThere is a flag to deactivate that behavior to ensure people can use Bazel 0.6 until the problem is fixed. /Cc @vladmos who can say more about that.", "(PS breaking change should only happen in minor version change, we forbid them in patch release now)", "I bet you are testing TF at head, which works. This is about the latest\nstable (released) version of TF. Apparently, the flag alone does not fix\nthe issue though (see above).\n\nIf you guarantee no breaking changes for patch, we can check for Bazel\nminor version in configure and issue a stern warning if it doesn't match\nexactly.\n\nIt's a little sad because I'd love to get rid of configure and the less we\nrely on it the better. The problem is that the compatibility check and\nerror message inside our build files is useless if Bazel dies before\nexecuting it. That happened before, so we went back to a configure based\nsolution.\n", "We should probably think of a better way to test for Bazel version. Also we\nshould definitely include your stable release in bazel release testing of\ntensorflow.\n\n/cc @gunan\n\nOn Sat, Sep 30, 2017 at 5:19 PM Martin Wicke <notifications@github.com>\nwrote:\n\n> I bet you are testing TF at head, which works. This is about the latest\n> stable (released) version of TF. Apparently, the flag alone does not fix\n> the issue though (see above).\n>\n> If you guarantee no breaking changes for patch, we can check for Bazel\n> minor version in configure and issue a stern warning if it doesn't match\n> exactly.\n>\n> It's a little sad because I'd love to get rid of configure and the less we\n> rely on it the better. The problem is that the compatibility check and\n> error message inside our build files is useless if Bazel dies before\n> executing it. That happened before, so we went back to a configure based\n> solution.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13377#issuecomment-333315187>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADjHf7CreuNwsBcz2bSB7nytgzOoutnrks5snlwJgaJpZM4PocIU>\n> .\n>\n", "In file `third_party/gpus/cuda_configure.bzl` at line 120 `includes_cpp_set = set(includes_cpp)`:\r\nMay it be patched to become `depset(includes_cpp)`?", "It definitely may be. Would you like to send a pull request making the\nchange you proposed?\n\nOn Sat, Sep 30, 2017 at 11:48 AM, jerry73204 <notifications@github.com>\nwrote:\n\n> In file third_party/gpus/cuda_configure.bzl at line 120 includes_cpp_set\n> = set(includes_cpp):\n> May it be patched to become depset(includes_cpp)?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13377#issuecomment-333328228>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCObAP4ZWJIes9hYirgAohPZZ9RMmiks5sno0ZgaJpZM4PocIU>\n> .\n>\n", "Oops. I just submit a duplicated pull request (#13447). Please close it soon. Thanks.", "I meet the same problem, now I want know how to uninstall/downgrade my bazel0.10.0 to 0.5.4?\r\n<img width=\"812\" alt=\"screen shot 2018-03-12 at 19 17 45\" src=\"https://user-images.githubusercontent.com/31690605/37281786-32f74f3a-262d-11e8-96a5-97974853055c.png\">\r\n"]}, {"number": 13376, "title": "MatMul in TensorFlow is slower than dot product in numpy", "body": "I am observing that on my machine tf.matmul in tensorflow is running significantly slower than dot product in numpy. I have GTX 1080 GPU, and expecting tf.matmul to be at least as fast as when running the code using CPU (numpy).\r\n\r\n**Environment Info\r\n\r\nOperating System**\r\n\r\n```\r\nlsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.10\r\nRelease:\t16.10\r\nCodename:\tyakkety\r\n```\r\n**Installed version of CUDA and cuDNN:**\r\n\r\n```\r\nls -l /usr/local/cuda-8.0/lib64/libcud*\r\n-rw-r--r-- 1 root      root    556000 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root      root        16 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root      root        19 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rwxr-xr-x 1 root      root    415432 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root      root    775162 Feb 22  2017 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 voldemaro users       13 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 voldemaro users       18 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10\r\n-rwxr-xr-x 1 voldemaro users 84163560 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n-rw-r--r-- 1 voldemaro users 70364814 Nov  6  2016 /usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n```\r\n**TensorFlow Setup**\r\n\r\n```\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\n```\r\n**Code:**\r\n\r\n```\r\n'''\r\nCreated on Sep 28, 2017\r\n\r\n@author: voldemaro\r\n\r\nRunning on I7/GTX 1080\r\n\r\nno MKL\r\n('TF version: ', 'v1.0.0-rc2-15-g47bba63-dirty')\r\n('TF url: ', 'https://github.com/tensorflow/tensorflow/commit/47bba63')\r\nTiming in ms for 2048 x 2048 SVD of type <type 'numpy.float32'> and matmul for 16920 x 2048 of type <type 'numpy.float32'>\r\nnumpy default SVD    min:  3956.20, median:  4127.75, mean:  4264.41\r\nTF CPU SVD           min:  5926.43, median:  5951.70, mean:  5961.43\r\nTF GPU SVD           min:  5917.10, median:  6015.87, mean:  6039.63\r\nnumpy default .dot product min:  5816.97, median:  5933.43, mean:  5965.22\r\nTF CPU matmul        min: 21939.19, median: 22485.99, mean: 22374.69\r\nTF GPU matmul        min: 22026.52, median: 22109.97, mean: 22199.43\r\n'''\r\n\r\nfrom scipy import linalg;  # for svd\r\nimport numpy as np;\r\nimport os;\r\nimport sys;\r\nimport time;\r\n\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"2\"  # nospam\r\n\r\nimport tensorflow as tf;\r\nimport gc; gc.disable();\r\n\r\nNUM_RUNS = 5;\r\ndtype = np.float32;\r\nN=2048;\r\nM =  16920;\r\n\r\n\r\ndef get_tensorflow_version_url():\r\n    import tensorflow as tf\r\n    version=tf.__version__\r\n    commit = tf.__git_version__\r\n    # commit looks like this\r\n    # 'v1.0.0-65-g4763edf-dirty'\r\n    commit = commit.replace(\"'\",\"\")\r\n    if commit.endswith('-dirty'):\r\n        dirty = True\r\n        commit = commit[:-len('-dirty')]\r\n    commit=commit.rsplit('-g', 1)[1]\r\n    url = 'https://github.com/tensorflow/tensorflow/commit/'+commit\r\n    return url\r\n\r\ndef get_mkl_version():\r\n    import ctypes\r\n    import numpy as np\r\n    ver = np.zeros(199, dtype=np.uint8)\r\n    mkl = ctypes.cdll.LoadLibrary(\"libmkl_rt.so\")\r\n    mkl.MKL_Get_Version_String(ver.ctypes.data_as(ctypes.c_char_p), 198)\r\n    return ver[ver != 0].tostring()\r\n\r\ntimeline_counter = 0\r\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE);\r\n\r\n\r\ndef benchmark(message, func):\r\n    time_list = []\r\n    for i in range(NUM_RUNS):\r\n        start_time = time.time();\r\n        func();\r\n        time_list.append(time.time()-start_time);\r\n\r\n    time_list = 1000*np.array(time_list);  # get seconds, convert to ms\r\n    if len(time_list)>0:\r\n        min = np.min(time_list);\r\n        median = np.median(time_list);\r\n        formatted = [\"%.2f\"%(d,) for d in time_list[:10]];\r\n        result = \"min: %8.2f, median: %8.2f, mean: %8.2f\"%(min, median, np.mean(time_list))\r\n    else:\r\n        result = \"empty\"\r\n    print(\"%-20s %s\"%(message, result))\r\n    \r\n\r\nif np.__config__.get_info(\"lapack_mkl_info\"):\r\n    print(\"MKL version\", get_mkl_version())\r\nelse:\r\n    print(\"no MKL\")\r\n\r\nprint(\"TF version: \", tf.__git_version__)\r\nprint(\"TF url: \", get_tensorflow_version_url())\r\n\r\n\r\nsvd_array = np.random.random_sample((N,N)).astype(dtype);\r\nanother_array = np.random.random_sample((M,N)).astype(dtype);\r\n\r\ninit_OP = tf.global_variables_initializer();\r\n\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    init_holder_gpu = tf.placeholder(dtype, shape=(M,M));\r\n    \r\n    specVarGPU = tf.random_uniform((N,N), dtype=dtype);\r\n    S_gpu = tf.random_uniform((M,N), dtype=dtype);\r\n    V_gpu = tf.matmul(tf.matmul(tf.transpose(tf.transpose(tf.conj(S_gpu))), specVarGPU, ), tf.transpose(S_gpu));\r\n    [D2_gpu, E1_gpu,  E2_gpu] = tf.svd(specVarGPU);\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    init_holder_cpu = tf.placeholder(dtype, shape=(M,M));\r\n    specVarCPU = tf.random_uniform((N,N), dtype=dtype);\r\n    S_cpu = tf.random_uniform((M,N), dtype=dtype);\r\n    V_cpu = tf.matmul(tf.matmul(tf.transpose(tf.transpose(tf.conj(S_cpu))), specVarCPU, ), tf.transpose(S_cpu));\r\n    \r\n    \r\n    [D2_cpu, E1_cpu,  E2_cpu] = tf.svd(specVarCPU);\r\n    V_cpu = tf.matmul(tf.matmul(tf.transpose(tf.transpose(tf.conj(S_cpu))), E1_cpu), tf.transpose(S_cpu));\r\n\r\nprint(\"Timing in ms for %d x %d SVD of type %s and matmul for %d x %d of type %s\"%(N, N, dtype, M, N, dtype));\r\n\r\ndef func(): linalg.svd(svd_array)\r\nbenchmark(\"numpy default SVD\", func)\r\n\r\nconfig = tf.ConfigProto(allow_soft_placement = True, graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)));\r\nsess = tf.Session(config = config);\r\nsess.run(init_OP);\r\n\r\ndef func2(): sess.run([D2_cpu.op, E1_cpu.op,  E2_cpu.op]);\r\nbenchmark(\"TF CPU SVD\", func2);\r\n\r\ndef func3(): sess.run([D2_gpu.op, E1_gpu.op,  E2_gpu.op]);\r\nbenchmark(\"TF GPU SVD\", func3);\r\n\r\ndef func1(): np.transpose(np.asmatrix(another_array)).getH().dot(svd_array).dot(np.transpose(another_array));\r\nbenchmark(\"numpy default .dot product\", func1)\r\n\r\ndef func4(): sess.run([V_cpu]);\r\nbenchmark(\"TF CPU matmul\", func4)\r\n\r\ndef func5(): sess.run([V_gpu])\r\nbenchmark(\"TF GPU matmul\", func4)\r\n\r\n```\r\n", "comments": ["A few problems in your code -- your GPU matmul and CPU matmul are measuring the same thing, your V_cpu computation also includes an SVD. You are saying the problem is with matmul but the benchmark computation includes a bunch of other things like tf.conj. It's easy to get bad performance due to other reasons like type conversions/unexpected device transfers, so if the claim is a bug in TensorFlow, it's best to make example as simple as possible.\r\n\r\nAlso, it's unlikely to find a significant problem with matmul in TF -- unlike SVD, matmul is a core part of any neural net, so it's been tested/tuned for a while at google.\r\n\r\nJust leaving N,N matmuls you'll get different numbers in your code. TF CPU version might be a little slower because the timing includes the time to generate new random matrix. \r\n\r\n```\r\nnumpy default .dot product min:    35.85, median:    45.71, mean:    54.80\r\nTF CPU matmul        min:    38.19, median:    51.00, mean:    49.19\r\nTF GPU matmul        min:     5.75, median:     5.78, mean:    65.49\r\n```\r\n\r\nHere's an isolated matmul benchmark. You get at least 11x performance speed-up on GPU using optimal settings: https://github.com/yaroslavvb/stuff/blob/master/matmul_benchmark.py", "thats correct - apparently tensorflow does not optimize \"nested\" operations, so \r\n`tf.matmul(tf.transpose(tf.conj(a)), x)` takes significantly longer time than `b = tf.conj(a)`, `c = tf.transpose(b)`,  and `d = tf.matmul(c, x)`."]}, {"number": 13375, "title": "Add python 3 iterator support for `tf.convert_to_tensor`", "body": "I am working on TensorFlow 1.3.0 and Python 3.6.3.\r\n\r\nI found that I have to write many unnecessary code to cast python iterator (such as return value of map, filter, zip and dict_keys) to `list` so that `tf.convert_to_tensor` could work. Supporting convert python iterator to tensor could not only help python 3 user write concise code without many `list(xxx)`, but also memory saving.", "comments": ["How exactly would you do it? We'd have to ensure that accidentally iterables (Tensors, numpy arrays, strings) are not caught up in this. If it's possible safely, it's a good idea.", "Inside of the implementation of `tf.conver_to_tensor`, there is a `_tensor_conversion_func_registry` which includes the supported type and its conversion function. I am thinking of adding some `(type, conversion function)` pair to this registry, where the `type` is the specific name of the iterators, such as `zip`, `map`, `dict_keys`, etc.\r\n\r\nAdditionally, to avoid incidental false invoke, we can set the priority to be low. They will be called after the previous `Tensor`, `ndarray` check.\r\n\r\nWe can first call `numpy.fromiter` on the iterators to generate a `ndarray`, then call the existed conversion function on the `ndarray` to convert to `Tensor`. I guess TF will reuse the memory of numpy, so this could save memory and time from avoiding copy memory from Python VM to TF runtime.", "That sounds like a fine plan to me. This would have to be tested quite rigorously, we've had problems with the errors generated by convert_tensors before and we'd want to test even the error behavior to make sure that it's informative.\r\n\r\nBut given those caveats, I'd welcome PRs for this.", "Hi everybody, \r\nsorry in advance for the question (I'm new to TF contribtion) but isn't this type - conversion function association already performed by `register_tensor_conversion_function()` ? \r\n\r\nIf so the `_tensor_conversion_func_registry` detail should not be needed: it should be possible to perform the function registration somewhere else in the framework \r\n\r\nThanks \r\n\r\n\r\n", "Yes, @NicolaBernini is correct. Perhaps this is where to add conversion functions as @tongda proposed.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/0cfb16e025b3d20e8c8aca431fc0887814817c44/tensorflow/python/framework/constant_op.py#L212-L219\r\n\r\nSince `tf.conver_to_tensor`, if broken , may affect lots of users. If I were you, perhaps it would be better to write down all iterator types we desired at the beginning.\r\n\r\nAnyway, sounds reasonable and be extra careful.\r\n", "I was thinking of contributing to this project and this is a really good beginner issue so I made a simple python function to create a numb array of the constant. Im not sure if I am understanding this right. Is there any way I can push this onto one of the experimental branches?", "Closing, as we should have iterator support in TensorFlow 2.0."]}, {"number": 13374, "title": "Cifar10", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Closing as stale"]}, {"number": 13373, "title": "How to use the output of the graph as a parameter of the loss function", "body": "Is there any example about using the output of the graph as a parameter of the loss function?\r\nFor example:\r\n\r\n_,_,summary_str_train, **acc_train** = sess.run([train_step, cross_entropy, merged_summary_op, accuracy], feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.7})\r\n\r\nacc_train is the accuracy of the current minibatch, output of the graph, I want to use the value of the acc_train in the loss function? the cross_entropy_loss is a customized function. \r\n\r\ncross_entropy = tf.reduce_mean(cross_entropy_loss(logits=prediction, one_hot_labels=y, accuracy=acc_train))\r\n\r\n\r\nIs there any example about this? Thanks ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13372, "title": "Revert \"Updating the gemmlowp hash and location for cmake build in master.\"", "body": "Reverts tensorflow/tensorflow#13369", "comments": ["Reverting this. This breaks something else down the line, and the original breakage was a Jenkins issue."]}, {"number": 13371, "title": "Updating the dockerfile with the new LD_LIBRARY_PATH.", "body": "", "comments": ["Could you test to see if this fixes our problem?", "http://ci.tensorflow.org/view/Experimental/job/experimental-docker-gpu/", "http://ci.tensorflow.org/view/Experimental/job/experimental-docker-gpu/TF_DOCKER_BUILD_IS_DEVEL=YES,TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-linux/1/consoleFull\r\n\r\nSeems to have an issue.", "Looks like there are still failures?", "Yes, --action_env didn't work; @allenlavoie is investigating.", "I'm (still) looking at it. I can comment out the Bazel command, build the image, then enter the Docker container manually (nvidia-docker -it) and build the pip package fine (same flags). I haven't gotten it to run with the Docker build script, mostly because I change something and spend a few hours compiling CUDA kernels.", "If I add \"RUN ls /usr/local/nvidia/lib64/libcuda.so.1\", that also fails (and does it much quicker than compiling TensorFlow). But the final image (nvidia-docker -it) seems to have it?", "Maybe somehow we are not using nvidia-docker when building the image?", "Ah, so adding \"RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1\", then adding /usr/local/cuda/lib64/stubs/ to LD_LIBRARY_PATH should work. Looks like there's some file copying that doesn't happen until after. Will try a compile that way.", "Woo, that worked for me locally. I did not need an action_env (I guess LD_LIBRARY_PATH does get picked up, it was just useless in this case).", "I've opened https://github.com/tensorflow/tensorflow/pull/13399 (but feel free to ignore it and modify this one if you'd prefer).", "Fixed via #13399"]}, {"number": 13370, "title": "Error with conditional labelling of summary", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow --upgrade\r\n- **TensorFlow version (use command below)**:1.3.0\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Cuda compilation tools, release 7.5, V7.5.17\r\n- **GPU model and memory**: NVIDIA 1070 16G\r\n- **Exact command to reproduce**: python model_wgraph2.py\r\n\r\n### Describe the problem\r\n\r\nI am attempting to vary my labels for the summary conditional on some tf.bool data type.  I get an error when I attempt to use the tf.cond.  \r\n\r\nPer: https://github.com/tensorflow/tensor2tensor/issues/159,\r\nthis problem should have been fixed in version 1.1.0\r\n\r\n\"lukaszkaiser:\r\n\r\nThis was due to summaries not working with tf.conds right. Should be corrected in 1.1.0, please take a look and reopen if you see this again!\"\r\n--\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\ttext2 = tf.cast('blah', tf.string)\t\r\n\ttext_summary = tf.summary.text(\"NULL\", text2) #works fine without conditional\t\r\n\trightness = tf.cast(True, tf.bool)\r\n\ttext_summary = tf.cond( rightness, lambda: tf.summary.text(\"TRUE\", text2), lambda: tf.summary.text(\"FALSE\", text2) ) #breaks with error below\r\n\r\n###Error\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n", "comments": ["Try with this may be it help you. change your code according to this change\r\n`rightness = tf.cast(1, tf.bool)`\r\n", "Same error", "\tSolved:\r\n        #For whatever reason don't try to instantiate the summary inside of the conditional\r\ntext_summary_1 = tf.summary.text(\"TRUE\", text)\r\n\ttext_summary_2 = tf.summary.text(\"FALSE\", text)\r\n\ttext_summary = tf.cond( rightness, lambda: text_summary_1, lambda: text_summary_2 )", "Nix that from above, code runs but it doesn't behave as expected.  Not seeing that third summary at all.", "Okay so it looks like the key to making this work is making sure that the two summaries you make to put into the conditional do not get merged and put in the Tensorboard too.  This means you don't get to use the nice merge_all() functionality.  With merge_all() there is no way to differentiate between what your conditional chose, since they are all merged together.   Name-scoping the copy doesn't help, since the name-scope is apparently copied from whichever summary is branched in the conditional, which implies it is more a reference than a copy.  Either way, don't know, don't care, but this workaround seems to solve the immediate problem.   ", "https://github.com/tensorflow/tensorflow/issues/10059\r\n\r\n", "Looks like this has been solved in 1.5.0rc1: you can now use `merge_all` with (e.g.) `tf.while_loop`s again.  Thanks, tensorflowers.", "Strike that, still broken (or anyway not naively usable) in 1.5.0:\r\n\r\n\"Cannot use 'while/...' as input to 'Merge/MergeSummary' because 'while/...' is in a while loop.\"", "Any updates on this issue? I just stumbled upon this problem in tensorflow 1.6.0. It feels logical to allow for conditional labelling.\r\n\r\nMy workaround now is, thanks to Lee-L-Boyd, to save the summaries in different collections in the conditional functions, i.e. `summary_lib.scalar(plot_name, val, collections=['train'])` and then use the `merged_summary = tf.summary.merge_all('train')` functionality. I suppose it is possible to create multiple merged summaries that collects from your conditional's collections. These could then be added to the same writer as different summaries. This is a workaround, however, and it would be a lot more logical to just provide functionality for conditional labelling.", "I'm using TF 1.8 and faced the same problem, but when my target function contains condition like\r\n`\r\ny = tf.cond(y > eps, lambda: y * N, lambda: y)\r\n`\r\nwhere eps and N another op", "I had a similar error message, and in my case I was using a `tf.cond` with different functions called for true and false. I had to call the functions outside cond and then return with a `lambda` . That is,  @Lee-L-Boyd suggestion worked."]}, {"number": 13369, "title": "Updating the gemmlowp hash and location for cmake build in master.", "body": "", "comments": []}, {"number": 13368, "title": "tensorflow/go: add in LDFLAGS to support android", "body": "This is required since the libtensorflow_inference.so generated by contrib/android links against these libraries. Go requires these to be specified when compiling against it.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n", "That build failure seems very unrelated, but I have no idea what's causing it.\r\n\r\n@sb2nov @aselle ", "Yes this is a known issue.\r\n\r\nJenkins, test this please.", "ping @aselle "]}, {"number": 13367, "title": "Remove the extra define from xsmm_backwards config rule.", "body": "", "comments": []}, {"number": 13366, "title": "Branch 170368495", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.\r\n\r\nTimeout in flaky test.", "Jenkins, test this please.", "@jhseu PTAL", "Jenkins, test this please.", "Jenkins, test this please.\r\n"]}, {"number": 13365, "title": "Installing Tensorflow 1.3.0 succeeds even when the cuDNN version is not correct", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.4\r\n- **TensorFlow installed from (source or binary)**: pip (binary?)\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 5.1\r\n- **GPU model and memory**: TITAN X (Pascal), 12GB\r\n- **Exact command to reproduce**:  \r\n\r\n * Install tensorflow: `pip install tensorflow-gpu`\r\n * Try to use tensorflow: `python -c \"import tensorflow;\"\r\n\r\n### Describe the problem\r\nWhen imported, Tensorflow looks for `libcudnn.so.6` and does not find it, since only 5.1 is installed, and fails with an error message\r\n\r\nWhat I would expect to happen is a) it either compiles against the version of libcudnn.so actually available, or b) it tries to check whether the correct version exists at install time and the install fails with a helpful message if it does not.\r\n\r\nThis is also not documented in https://www.tensorflow.org/install/install_linux or https://www.tensorflow.org/install/install_sources, in fact all it says for software requirements is \"cuDNN (>= v3). We recommend version 5.1\", which is what I had installed.\r\n\r\nThank you!\r\n\r\n### Source code / logs\r\n\r\nN/A", "comments": ["Instructions at [https://www.tensorflow.org/install/install_linux](https://www.tensorflow.org/install/install_linux) say:\r\n\r\n\"If you have an earlier version of the preceding packages...\" (such as cuDNN < 6.0) \"...you may still run TensorFlow with GPU support, but only if you do the following:\r\n\r\nInstall TensorFlow from sources as documented in Installing TensorFlow from Sources.\"\r\n\r\nSorry the instructions are so long, I know it's easy to miss this.\r\n\r\n"]}, {"number": 13364, "title": "Fixed typo in function documentation", "body": "sry.. recognized this tiny typo..", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n\r\nThanks will merge once the tests finish."]}, {"number": 13363, "title": "Remove frame_length restrictions in tf.contrib.signal.stft/inverse_stft.", "body": "So far tf.contrib.signal.stft() does not allow a larger fft_length than frame_length.\r\n```\r\n raise ValueError('frame_length (%d) may not be larger than '\r\n                       'fft_length (%d)' % (frame_length_static,\r\n                                            fft_length_static))\r\n```\r\n I think it is worth to allow this option by zero-padding the input frames, matching fft_length, since it is the usual proceeding that provides a smooth time-freq representation. ", "comments": ["The error you're pointing at prevents frame_lengths that are larger than fft_length -- I think fft_length larger than frame_length works already (there are unit tests for that case).\r\n\r\nIn either case, I think we can remove that check now -- ever since https://github.com/tensorflow/tensorflow/commit/7b4c01794fbc2e6dc46e93a42416dd80929ce1e5, the FFT ops have supported padding/slicing to fft_length if their input isn't of the right size.", "I have a patch out for review that removes this -- I should be able to get it in before the TF 1.4 cut on Tuesday. Thanks for the suggestion (keep em coming!).", "My fix is checked in internally and will go live as soon as the internal<->github sync is done."]}, {"number": 13362, "title": "Trying to run 2_fullyconnected.ipynb but neural net doesn't learn", "body": "I'm trying to run that notebook as it is but model doesn't learn at all. Initially I got right results like in sample but now I'm instantly getting such output when train a multinomial logistic regression using simple gradient descent:\r\n\r\nInitialized\r\nLoss at step 0: 24.628622\r\nTraining accuracy: 9.7%\r\nValidation accuracy: 10.0%\r\nLoss at step 100: 35.364712\r\nTraining accuracy: 9.9%\r\nValidation accuracy: 9.6%\r\nLoss at step 200: 38.703053\r\nTraining accuracy: 9.9%\r\nValidation accuracy: 10.0%\r\nLoss at step 300: 30.087294\r\nTraining accuracy: 10.1%\r\nValidation accuracy: 10.0%\r\nLoss at step 400: 35.924911\r\nTraining accuracy: 9.9%\r\nValidation accuracy: 10.0%\r\nLoss at step 500: 38.568333\r\nTraining accuracy: 10.5%\r\nValidation accuracy: 10.0%\r\nLoss at step 600: 31.410255\r\nTraining accuracy: 10.8%\r\nValidation accuracy: 10.0%\r\nLoss at step 700: 42.134827\r\nTraining accuracy: 10.1%\r\nValidation accuracy: 10.0%\r\nLoss at step 800: 26.819206\r\nTraining accuracy: 9.9%\r\nValidation accuracy: 10.0%\r\nTest accuracy: 10.0%\r\n\r\nWhen I switch to switch to stochastic gradient descent training I get simmilar output. Restarting kernel and even VM doesn't work. I also tried to delete preveously generated pickle and generate it from scratch but it also doesn't help.\r\n\r\nWhat might cause my neural net to stop learning at some point?", "comments": ["The network is guessing at random.  This is MNIST so 10 possible choices -> 10% of getting anything right completely at random.  \r\nSome basic troubleshooting:\r\nLook at output. is it predicting a constant value?  \r\nCome up with fake data, see what network does.  ", "I think you will get more help over at [Stack Overflow](\r\nhttps://stackoverflow.com/questions/tagged/tensorflow) since this is probably not a bug"]}, {"number": 13361, "title": "'tensorflow.python.ops.nn' has no attribute 'selu'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 8.0\r\n- **GPU model and memory**: GTX 1080\r\n- **Exact command to reproduce**: \r\n```\r\nimport tensorflow as tf\r\n...\r\ny = tf.nn.selu(x)\r\n```\r\n\r\n### Describe the problem\r\nTensorFlow 1.3 cannot find selu activation. Relu and elu work fine.\r\nRelevant documentation: [tf/nn/selu](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/selu)\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"my_file.py\", line 212, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/file/location\", line 35, in model\r\n   y = tf.nn.selu(x)\r\nAttributeError: module 'tensorflow.python.ops.nn' has no attribute 'selu'\r\n```", "comments": ["I think this is only available in the master branch of TF for now. Probably in TF 1.4?", "@AVCarreiro I agree. Not sure how I missed that.\r\n\r\nCase closed."]}, {"number": 13360, "title": "Feature request: tf.reduce_median()", "body": "Hi, is there any plan to support a `tf.reduce_median` operator? Now one have to resort to `tf.nn.top_k` for implementing this.", "comments": ["could be a good idea for an external contribution -- name makes sense, doesn't need to be tested against existing models", "I guess technically median is not a reduce(fold) operation.", "@ppwwyyxx Yep. It only makes sense as a name. That doesn't matter though, I just want to say that it deserves an operator.", "I am marking as contributions welcome, but I imagine this is actually a very difficult feature to implement efficiently as a streaming (accelerated) median calculation requires some thought.", "I have added a new op `nth_element` and a python wrapper for selecting the n-th smallest value, whose API locates in `python.contrib.nn`. Hoping this helps.\r\nThis op could be a effective foundation of building reduce_median or other quantile function. If it's necessary, I'd like to add a python API called `reduce_median` based on this op.", "Don't you think that [tf.contrib.distributions.percentile(x,50.)](https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/percentile) can be used to create a reduce_median function ? ", "Hi @thjashin ! You can use[ tfp.stats.percentile](https://www.tensorflow.org/probability/api_docs/python/tfp/stats/percentile) to calculate the median of a distribution. Attaching relevant [thread](https://stackoverflow.com/a/47657076/11530462) for reference. Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 13359, "title": "Sess.list_devices() get  swig/python detected a memory leak error.", "body": "Look at the code:\r\n\r\n```python\r\ndef different_graph_session_test():\r\n\t'''\r\n\t\ta_0 is in graph g_0, a_1 is in graph g_1,a_2 is in default graph,\r\n\t\tso a_0 can only run on session of g_0, a_1 can only run on session of g_1,\r\n\t\ta_2 can only run on session of tf.get_default_graph()\r\n\t'''\r\n\tg_0 = tf.Graph()\r\n\tg_1 = tf.Graph()\r\n\twith g_0.as_default():\r\n\t\ta_0 = tf.constant(1)\r\n\twith g_1.as_default():\r\n\t\ta_1 = tf.constant(1)\r\n\ta_2 = tf.constant(1)\r\n\r\n\tassert not a_0.graph is a_1.graph, 'different_graph_session_test wrong!'\r\n\tassert a_2.graph is tf.get_default_graph(), 'different_graph_session_test wrong!'\r\n\t\r\n\twith tf.Session(graph=g_0) as sess:\r\n\t\tsess.run(a_0)\r\n\t\tdevices = sess.list_devices()\r\n\t\tfor d in devices:\r\n\t\t\tprint(d.name)\r\n\twith tf.Session(graph=g_1) as sess:\r\n\t\tsess.run(a_1)\r\n\t\tdevices = sess.list_devices()\r\n\t\tfor d in devices:\r\n\t\t\tprint(d.name)\r\n\twith tf.Session() as sess:\r\n\t\tsess.run(a_2)\r\n\t\tdevices = sess.list_devices()\r\n\t\tfor d in devices:\r\n\t\t\tprint(d.name)\r\n```\r\n\r\nIt will print\r\n\r\n```\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\n/job:localhost/replica:0/task:0/device:CPU:0\r\n/job:localhost/replica:0/task:0/device:GPU:0\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\n```\r\nIs it a bug or something wrong?", "comments": ["It's a bug. Fixed in https://github.com/tensorflow/tensorflow/commit/4b9e50686928b858a0045e1ddcc0bab4acc7ff37 (will be in 1.4)."]}, {"number": 13358, "title": "parameterized_docker_build.sh fails", "body": "### Issue\r\n\r\nExecuting parameterized_docker_build.sh fails to generate new docker image, throws out multiple error messages.\r\n\r\n### System information\r\n- **I have used a stock example script provided in TensorFlow**\r\n- **Windows 10 professional**\r\n- **TensorFlow not installed (looking for generating a dockerfile)**\r\n- **TensorFlow version 1.3:latest (eventually looking for this version)**\r\n- **Python version 3 (eventually looking for this version)**\r\n- **Bazel version not yet installed**\r\n- **CUDA/cuDNN version not relevant (CPU install)**\r\n- **GPU model and memory not relevant (CPU install)**\r\n\r\n### Set-up\r\n\r\n```\r\n>>> Docker run -it Ubuntu:latest /bin/bash\r\n$ apt-get update\r\n$ apt-get install curl\r\n$ cd /opt\r\n$ curl -O https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/docker/parameterized_docker_build.sh\r\n$ curl -O https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/docker/parameterized_docker_build.sh\r\n$ chmod +x opt/parameterized_docker_build.sh\r\n$ export TF_DOCKER_BUILD_IS_DEVEL=NO\r\n$ export TF_DOCKER_BUILD_TYPE=CPU\r\n$ export TF_DOCKER_BUILD_PYTHON_VERSION=PYTHON3\r\n$ export NIGHTLY_VERSION=\"1.head\"\r\n$ export TF_DOCKER_BUILD_CENTRAL_PIP=$(echo ${TF_DOCKER_BUILD_PYTHON_VERSION} | sed s^PYTHON2^http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=${TF_DOCKER_BUILD_PYTHON_VERSION},label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp27-cp27mu-manylinux1_x86_64.whl^ | sed s^PYTHON3^http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-${NIGHTLY_VERSION}-cp35-cp35m-manylinux1_x86_64.whl^)\r\n```\r\n\r\n### Command triggering issue\r\n\r\n`$ sh opt/parameterized_docker_build.sh`\r\n\r\n### Error messages\r\n\r\n> opt/parameterized_docker_build.sh: 70: opt/parameterized_docker_build.sh: Bad substitution\r\n> opt/parameterized_docker_build.sh: 71: opt/parameterized_docker_build.sh: source: not found\r\n> opt/parameterized_docker_build.sh: 81: opt/parameterized_docker_build.sh: to_lower: not found\r\n> opt/parameterized_docker_build.sh: 82: opt/parameterized_docker_build.sh: to_lower: not found\r\n> opt/parameterized_docker_build.sh: 83: opt/parameterized_docker_build.sh: to_lower: not found\r\n> opt/parameterized_docker_build.sh: 84: opt/parameterized_docker_build.sh: to_lower: not found\r\n> Required build parameters:\r\n>   TF_DOCKER_BUILD_TYPE=\r\n>   TF_DOCKER_BUILD_IS_DEVEL=\r\n>   TF_DOCKER_BUILD_DEVEL_BRANCH=\r\n> \r\n> Optional build parameters:\r\n>   TF_DOCKER_BUILD_CENTRAL_PIP=http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp35-cp35m-manylinux1_x86_64.whl\r\n>   TF_DOCKER_BUILD_IMAGE_NAME=\r\n>   TF_DOCKER_BUILD_VERSION=\r\n>   TF_DOCKER_BUILD_PORT=\r\n>   TF_DOCKER_BUILD_PUSH_CMD=\r\n> opt/parameterized_docker_build.sh: 102: opt/parameterized_docker_build.sh: [[: not found\r\n> opt/parameterized_docker_build.sh: 114: opt/parameterized_docker_build.sh: [[: not found\r\n> opt/parameterized_docker_build.sh: 121: opt/parameterized_docker_build.sh: [[: not found\r\n> opt/parameterized_docker_build.sh: 124: opt/parameterized_docker_build.sh: die: not found\r\n> opt/parameterized_docker_build.sh: 128: opt/parameterized_docker_build.sh: [[: not found\r\n> opt/parameterized_docker_build.sh: 130: opt/parameterized_docker_build.sh: [[: not found\r\n> opt/parameterized_docker_build.sh: 141: opt/parameterized_docker_build.sh: die: not found\r\n> opt/parameterized_docker_build.sh: 145: opt/parameterized_docker_build.sh: [[: not found\r\n> opt/parameterized_docker_build.sh: 147: opt/parameterized_docker_build.sh: [[: not found\r\n> opt/parameterized_docker_build.sh: 150: opt/parameterized_docker_build.sh: die: not found\r\n> opt/parameterized_docker_build.sh: 156: opt/parameterized_docker_build.sh: [[: not found\r\n> \r\n> FINAL_IMAGE_NAME: tensorflow/tensorflow\r\n> FINAL_TAG: latest\r\n> Original Dockerfile: //Dockerfile\r\n> \r\n> Docker build will occur in temporary directory: /tmp/tmp.IItoQQJuhx\r\n> cp: error reading '//proc/1/task/1/personality': Operation not permitted\r\n> cp: error reading '//proc/1/task/1/syscall': Operation not permitted\r\n> cp: cannot open '//proc/1/task/1/mem' for reading: Permission denied\r\n> cp: error reading '//proc/1/task/1/clear_refs': Invalid argument", "comments": ["Use tensorflow's official [docker image ](https://hub.docker.com/r/tensorflow/tensorflow/)\r\nCPU and GPU Support also available with their docker image just choose the [right tag](https://hub.docker.com/r/tensorflow/tensorflow/tags/)\r\n`docker run -it -p 8888:8888 tensorflow/tensorflow`\r\n", "**THIS IS IMPORTANT BUT ONLY PART OF THE PROBLEM**\r\n**Read on and follow instructions below then refer to the [next issue](https://github.com/tensorflow/tensorflow/issues/13379)**\r\n\r\nYou are correct thank you very much.\r\n\r\nLet me explicit a bit your solution because I nearly failed to grasp it and this can be useful for future readership and eventual documentation.\r\n\r\nIf one docker pulls a developer image, it will contain the full tensorflow source code in addition to the binary. So for example let's docker pull tensorflow/tensorflow:1.3.0-devel-py3, then one should be able to run parameterized_docker_build.sh in the /tensorflow/tensorflow/tools/docker folder just  fine.\r\n\r\nThe smallest command line to generate a docker image will then be:\r\n`docker run -it tensorflow/tensorflow:`[right tag](https://hub.docker.com/r/tensorflow/tensorflow/tags/)\r\n\r\nPort mapping at 8888 `-p 8888:8888` in the strict context of this question is nice to have.\r\n\r\n"]}, {"number": 13357, "title": "[XLA] Failure in the OS/X xla/service tests: duplicate symbol", "body": "I am getting the following error when running the XLA service unit tests:\r\n\r\n```\r\nduplicate symbol __ZN3xla7PrintToEPKNS_14HloInstructionEPNSt3__113basic_ostreamIcNS3_11char_traitsIcEEEE in:\r\n    bazel-out/local-opt/bin/tensorflow/compiler/xla/service/_objs/user_computation_test/tensorflow/compiler/xla/service/user_computation_test.o\r\n    bazel-out/local-opt/bin/tensorflow/compiler/xla/service/libhlo_matchers.a(hlo_matchers.o)\r\n```\r\n\r\nCommand line for running the tests is:\r\n\r\n```\r\nbazel test --test_env TF_CPP_MIN_VLOG_LEVEL=2 --test_size_filters=small,medium,large --config opt --verbose_failures --test_output=all --nocache_test_results tensorflow/compiler/xla/service/...\r\n```\r\n\r\nThis is OS/X, head of the master branch, using bazel `0.5.4-homebrew`.\r\n\r\nIs this a known issue at the moment?\r\n", "comments": ["No, I think this a new bug. The fix is to move those two functions in hlo_matchers.h out of line --- currently they are defined in the header which causes a duplicate definition. I'll fix it but it will take a day or two to make it to Github.\r\n\r\n(Apologies for the breakage. Sadly our Mac continuous build doesn't build XLA yet because of limited resources, and this linking problem doesn't show up on Linux.)"]}, {"number": 13356, "title": "How to save the network graph into a file ?", "body": "I want to save the network graph, not the weights. Is there any tool to export the network to a file like json?", "comments": ["You can use tensor board for visualization.  check this [github repo ](https://www.tensorflow.org/versions/r0.12/how_tos/graph_viz/)", "this question is better for stackoverflow"]}, {"number": 13355, "title": "TensorArray grad bug", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: pip binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.6.1\r\n\r\n### Describe the problem\r\n\r\n`tf.TensorArray` in some cases does not correctly passes the gradient. See the test case.\r\n\r\n### Source code / logs\r\n\r\nThis fails:\r\n\r\n```\r\ndef test_tensorarray_grad_simple():\r\n  n_time = 1\r\n  n_dim = 1\r\n  x = [[1.42]]\r\n  dy = [[2.42]]\r\n\r\n  x = tf.convert_to_tensor(x)\r\n  x.set_shape(tf.TensorShape((n_time, n_dim)))\r\n  with tf.name_scope(\"gradients\"):\r\n    # Note that tensor_array_grad._GetGradSource() has this ugly hack\r\n    # which requires that we have the \"gradients\" prefix.\r\n    dy = tf.identity(tf.convert_to_tensor(dy), name=\"dy\")\r\n  dy.set_shape(tf.TensorShape((n_time, n_dim)))\r\n\r\n  ta = tf.TensorArray(tf.float32, size=n_time, element_shape=tf.TensorShape((n_dim,)))\r\n  for t in range(n_time):\r\n    ta = ta.write(index=t, value=x[t])\r\n  y = ta.stack()\r\n  y.set_shape(tf.TensorShape((n_time, n_dim)))\r\n  # y = y[::1]  -- if you add this, the test passes\r\n  dx, = tf.gradients(ys=[y], grad_ys=[dy], xs=[x])\r\n  vx, vdy, vy, vdx = session.run([x, dy, y, dx])\r\n  print(\"x:\", vx)\r\n  print(\"y:\", vy)\r\n  print(\"dy:\", vdy)\r\n  print(\"dx:\", vdx)\r\n  assert_allclose(vx, vy)\r\n  assert_allclose(vdy, vdx)\r\n```\r\n\r\nI get the output:\r\n```\r\nx: [[ 1.41999996]]\r\ny: [[ 1.41999996]]\r\ndy: [[ 2.42000008]]\r\ndx: [[ 0.]]\r\n```\r\n\r\nStrangely, if you add something like `y = y[::1]` before taking the gradient, it passes.\r\n", "comments": ["@alextp would you please take a look?", "@ebrevdo , can you take a look?", "This is definitely a bug, and it has to do with this line:\r\n```\r\ngrad_source = _GetGradSource(grad)\r\n```\r\nthe weird \"gradients\" prefix is there so that the TensorArray can differentiate between different calls to `tf.gradients`: each call to `tf.gradients` must create a separate gradient TensorArray.  The reason you're getting zeros is that your `gradients` name prefix is not the same name prefix created by `tf.gradients` (since `gradients` is already taken, it's probably creating a name prefix `gradients_1` - and this difference in prefix is confusing the TensorArray).\r\n\r\nNote that this will work:\r\n```python\r\ntf.gradients(y, x, [[[2.42]]])\r\n```\r\nbecause the grad_ys [[[2.42]]] is converted to a tensor *inside* the tf.gradients, and will have the appropriate name scope.\r\n\r\nOne solution might be to wrap all grad_ys going into `tf.gradients` in a `tf.identity` that brings them into the same gradient name scope.  Not sure it's the best solution.", "I have a solution; will test and push - if all is green, you will see it in 2-3 days.", "This is not fixed yet in 1.4.0. The test does not pass.", "Have you tried in the nightlies?\n\nOn Fri, Nov 3, 2017 at 3:30 AM, Albert Zeyer <notifications@github.com>\nwrote:\n\n> This is not fixed yet in 1.4.0. The test does not pass.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13355#issuecomment-341667268>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6SW6yBKxW8Y-V7cS0vy6yw7CeBtks5syutCgaJpZM4Pm22x>\n> .\n>\n", "No, the official 1.4.0 release, installed via pip install. I thought that the commit should be in that release but not sure. Specifically, my version is `v1.4.0-rc1-11-g130a514 1.4.0`.", "Commits to master don't always make it into 1.4 releases, they have to be\ncherrypicked during the RC and it's possible that this bugfix was not.\n\nOn Fri, Nov 3, 2017 at 9:03 AM, Albert Zeyer <notifications@github.com>\nwrote:\n\n> No, the official 1.4.0 release, installed via pip install. I thought that\n> the commit should be in that release but not sure. Specifically, my version\n> is v1.4.0-rc1-11-g130a514 1.4.0.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13355#issuecomment-341747699>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5_uz7AWhBuBuqT_8fD9y3r8uglRks5syzlVgaJpZM4Pm22x>\n> .\n>\n"]}, {"number": 13354, "title": "Dynamic loading / freeing GPU devices", "body": "I wonder whether there are any on-going works or plans on dynamic loading / freeing GPUs.\r\n\r\nWhat I mean by dynamic loading is a client-side feature <code>sess.load_device()</code> like below:\r\n\r\n```python\r\n# start with /gpu:0 only\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\nsess = tf.Session()\r\nwith tf.device('/gpu:0'):\r\n  a = tf.constant(0.5)\r\nsess.run(a)\r\n\r\n# add new device /gpu:1\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\r\nsess.load_device('/gpu:1')\r\nwith tf.device('/gpu:1'):\r\n  b = tf.constant(0.3)\r\nsess.run(a + b)\r\n\r\n# free device /gpu:0\r\nsess.free_device('/gpu:0')\r\n```\r\n\r\nI'm trying to test whether I can run <code>BaseGPUDeviceFactory::CreateDevices()</code> after the session is created, but if there are any better ways, would you please give me some hints?", "comments": ["Freeing GPUs inside process seems tricky since memory allocator is global. You could have several sessions storing things on various gpus, you'd need to check that you aren't destroying some precious memory state when releasing the GPU.\r\n\r\nYou could move GPU management a level up -- launch TensorFlow worker processes that reserve  GPUs and then destroy those processes to release.", "There are currently no plans for enabling dynamic unloading/loading of devices in a session."]}, {"number": 13353, "title": "Changing compiler to gcc not works", "body": "In my macOS, I have set `export CC=/usr/local/bin/gcc-6` and `export CXX=/usr/local/bin/g++-6`. Then I built tensorflow from source using `sh tensorflow/contrib/makefile/build_all_ios.sh`. But I saw it also compile using `clang`. So, how to change compiler? @martinwicke ", "comments": ["@petewarden knows more about the makefile. ", "Hi, @formath . I prefer to using `clang` by default, since we seems not the only one who gets stuck in `gcc` on Mac, see #336, #75,  #12266...", "@facaiy Yes. Running build_all_ios.sh using default clang works.\r\n@petewarden Please also have a look of how to set compiler to gcc. \r\nThanks all."]}, {"number": 13352, "title": "Adding links to the Windows GPU nightly binaries and build histories.", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}]