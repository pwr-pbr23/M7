[{"number": 34115, "title": "Run Time series forecasting Tutorials , the warning raised. W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nI try to run Time series forecasting Tutorials . the code was exactly same as the github shows.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWin10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNONE\r\n- TensorFlow installed from (source or binary):\r\nsource \r\n- TensorFlow version (use command below):\r\nTensorflow 2.0 stable version\r\n- Python version:\r\npy 3.7\r\n- Bazel version (if compiling from source):\r\nNONE\r\n- GCC/Compiler version (if compiling from source):\r\nNONE\r\n- CUDA/cuDNN version:\r\nNONE\r\n- GPU model and memory:\r\nNONE\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nwhen I try to run the Time series forecasting Tutorials , the warning raised .\r\nI don\r\n2019-11-09 11:05:44.540408: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_2511_2996' and '__inference___backward_standard_lstm_2511_2996_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_3085' both implement 'lstm_b5a954a4-22ab-43b7-aa70-42d55d0dde61' but their signatures do not match.\r\n**Describe the expected behavior**\r\nhow to solve this warning ?\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nthe same as Tutorial. https://tensorflow.google.cn/tutorials/structured_data/time_series\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@chris881, I tried the tutorial on colab. It executed as expected.\r\nPlease take a look at the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/441c50ce70cd7bc2afe5eb05b0e3bc1c/untitled250.ipynb).Thanks! ", "thanks a lot . i will double check the code."]}, {"number": 34114, "title": "Named dictionary inputs and outputs for tf.keras.Model", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**feature request:**\r\n\r\nIf the user passes in dictionaries for the `inputs` or `outputs` argument during `tf.keras.Model` initialization, preserve the dictionary datatype in `tf.keras.Model.inputs` and `tf.keras.Model.outputs` (i.e. avoid normalizing these arguments to lists). \r\n\r\nThis would allow us to refer to inputs and outputs by name rather than by indices in a list.\r\n\r\n**current behavior/background:**\r\n\r\n`tf.keras.Model` allows us to construct a `Model` with `inputs` and `outputs` arguments that are passed in as dictionaries of tensors. This is a very nice feature because it  allows you to evaluate a model by passing in dictionaries of named data, and to refer to outputs using their named outputs.\r\n\r\nHowever, if one wants to *extend* a model by attaching more layers to a given output, we lose all of the naming information given to us during the model initialization. This is because the `inputs` and `outputs` arguments are always \"flattened\" to lists and assigned to `Model.outputs` and `Model.inputs` regardless if the user passed in a dictionary or not for these arguments.\r\n\r\nAs a result of losing this naming information, we have to fall back to using integer based indexing to retrieve the output tensor of interest that we want to extend. While not the end of the world, this is more error prone since layer ordering may move around due to changes in the initial model.\r\n\r\nIn the following example, I illustrate how defining, and extending a model with new layers currently works:\r\n\r\n```python\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Input, Dense\r\n\r\ndef create_base_model():\r\n    # create initial model\r\n    a = Input([10], dtype=tf.float32)\r\n    b = Dense(4)(a)\r\n    c = Dense(8)(b)\r\n    model = Model(inputs={'a': a}, outputs={'b': b, 'c': c})\r\n    return model\r\n\r\ndef extend_base_model(model):\r\n    # create a new model to attach additional layers to tensor \"c\"\r\n    c = model.outputs[-2]\r\n    d = Dense(16)(c)\r\n    \r\n    # notice here, since model.inputs and model.outputs are flattened to a lists,\r\n    # we lose all naming information\r\n    new_model = Model(inputs=model.inputs, outputs=model.outputs+[d])\r\n    return new_model\r\n    \r\nbase = create_base_model()\r\nmodel = extend_base_model(base)\r\n\r\n# we lose all naming information in the inputs and outputs given by the base model\r\n# outputs: [<tensor>, <tensor> ,  <tensor>]\r\nmodel(tf.zeros([1, 10], dtype=tf.float32))\r\n```\r\n\r\nIf the feature request was adopted, here's how defining, extending, and executing models could work:\r\n\r\n```python\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Input, Dense\r\n\r\ndef create_base_model():\r\n    # create initial model\r\n    a = Input([10], dtype=tf.float32)\r\n    b = Dense(4)(a)\r\n    c = Dense(8)(b)\r\n    model = Model(inputs={'a': a}, outputs={'b': b, 'c': c})\r\n    return model\r\n\r\ndef extend_base_model(model):\r\n    # create a new model to attach additional layers to tensor \"c\"\r\n    c = model.outputs[\"c\"]\r\n    d = Dense(16)(c)\r\n    \r\n    new_model = Model(inputs=model.inputs, outputs={'d': d, **model.outputs})\r\n    return new_model\r\n    \r\nbase = create_base_model()\r\nmodel = extend_base_model(base)\r\n\r\n# we preserve all naming information in the inputs and outputs given by the base model\r\n# outputs {\"b\": <tensor>, \"c\": <tensor> , \"d\": <tensor>}\r\nmodel({\"a\": tf.zeros([1, 10], dtype=tf.float32))\r\n```\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes, it will change `Model.input` and `Model.output` data types *if* the user passes in a dictionary for these arguments during initialization. \r\n\r\ncurrent behavior:\r\n\r\n```python\r\na = Input([10], dtype=tf.float32)\r\nb = Dense(4)(a)\r\nmodel = Model(inputs={'a': a}, outputs={'b': b})\r\ntype(model.inputs)  # list\r\ntype(model.outputs) # list\r\n```\r\n\r\nnew behavior:\r\n\r\n```python\r\na = Input([10], dtype=tf.float32)\r\nb = Dense(4)(a)\r\nmodel = Model(inputs={'a': a}, outputs={'b': b})\r\ntype(model.inputs)  # dict\r\ntype(model.outputs) # dict\r\n```\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople designing and developing models will be able to more easily and clearly expose output tensors for downstream users to access. \r\n\r\nDownstream users who need to extend a pre-defined models will more easily be able to access tensors using their named outputs.\r\n\r\n**Any Other info.**\r\n\r\nNot sure if this is the right way to implement this, but it looks like the `_nested_outputs` and `_nested_inputs` variables listed below preserve the data types of the inputs and outputs arguments that were initially passed into the Model constructor.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/engine/network.py#L264-L265\r\n", "comments": ["You can access originally passed `inputs`, `outputs` argments in `Model.input` and `Model.output` (not `Model.inputs`, `Model.outputs`).", "This seems a good feature request to have, for now all we support is through DenseFeatures\r\n", "I just debugged the weirdest effects for more than a day until I found out that some of my outputs are swapped from the order I created them in (which makes sense given it's a dictionary, but duh...).\r\n\r\nDefinitely +1 for this request and the current behaviour should be better documented.\r\n\r\nE.g. what happens when I train my multi-output model with a tf.data.Dataset yielding structured tuples? Are they matched based on their name or also just flattened in some way? ", "And for other people having this problem:\r\n\r\nThe models seem to have a `output_names` property which can be used to map the list back to the named outputs.", "Thanks for the suggestion. Since this feature is affecting the existing API contract (type of the return value), I am not sure if we can easily change it in TF2 without breaking other users. Having said that, I think the proposed API is more intuitive, and we probably should consider it.\r\n\r\nAdding @fchollet and @omalleyt12 here. ", "There's actually two ways to pass dicts through the Functional API at the moment (which is a little ambiguous, so maybe we should clean this up). IMO here is the most supported way:\r\n\r\n```python\r\na = tf.keras.Input(10, name='a'). # 'a' becomes a key in the dict.\r\nb = tf.keras.Input(10, name='b')\r\nout = tf.keras.layers.Add()([a, b])\r\nmodel = tf.keras.Model([a, b], [out])\r\n\r\n# The keys of the dict are the names of the Inputs.\r\nx = {'a': ..., 'b': ...}. \r\ny_pred = model(x)\r\n```\r\n\r\nUsing this pattern, you can then extend the base model like this:\r\n\r\n```python\r\nout = model.outputs[0]\r\nout = tf.keras.layers.Dense(1)(out)\r\nextended_model = tf.keras.Model(model.inputs, [out])\r\n\r\n# This still works\r\nx = {'a': ..., 'b': ...}. \r\ny_pred = extended_model(x)\r\n```\r\n\r\nThis should work in the latest nightly\r\n\r\nSince we do allow the other way (directly passing nested structures to the Functional API `tf.keras.Model` constructor), maybe we should change `Model.inputs` and `Model.outputs` to match this structure in that case\r\n\r\nAlternatively, we could just drop support for nested structures in the `tf.keras.Model` constructor and require the names of the `tf.keras.Input`s to be passed when wanting to pass a dict through the Functional API, since having two ways to do this is confusing. This would be backwards incompatible though, so we'd have to make sure it's not impacting a large number of users\r\n\r\n", "@qlzh727 @omalleyt12  thanks for the detailed response. I was not aware that adding a  `name` parameter  to  `Input` would change the input type for `model()` to a dict. \r\n\r\nWhile this addresses `Model.inputs`, the naming information is still lost for `Model.outputs` if for example, we define a `Model` that outputs a dictionary:\r\n\r\n```python\r\nimport tensorflow as tf\r\nc = tf.keras.Input(2, name='c')                          \r\nd = tf.keras.Input(2, name='d')                          \r\nz = tf.keras.layers.Add()([c*10, d])                     \r\nj = tf.keras.layers.Add()([z, z])                        \r\nmodel = tf.keras.Model(inputs={'c':c, 'd': d}, outputs={'z': z, 'j': j})\r\n```\r\n\r\nTo extend the model above, we would have to resort back to indexing the `model.outputs` list:\r\n\r\n```python\r\nout = model.outputs[0]  # does `out` represent `z` or `j`?\r\nk = tf.keras.layers.add()([out, out, out])\r\nextended_model = tf.keras.Model(inputs=model.inputs, outputs=model.outputs+[k])\r\n```\r\nIt's not the end of the world that we're losing the naming information for `outputs` in `extended_models`, but it would make defining models so much less error prone if I could rely on the dictionary keys to refer to both `inputs` and `outputs` when extending models. \r\n\r\nPerhaps if backwards incompatible changes are not allowed at this point for tensorflow, would you consider adding a different  and lower-level model definition API that preserves `inputs` and `outputs` types *by default*? E.g. introduce a class called  `Module` where `Module.inputs` and `Module.outputs` is a dictionary if the user passes in a dictionary of tensors,  is a list if the user passes in a list of tensors, and is a tensor if the user passes in a tensor? \r\n\r\n`keras.Model` could even extend `keras.Module` to present to end users the current API behavior without causing any backward incompatable changes.\r\n\r\nThanks again for reviewing this issue,\r\n-Huy\r\n\r\n---\r\nAlso, unrelated to the feature request but I just tried adding the `name` parameter to `Input` and there seems to be some odd behavior with that method:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nc = tf.keras.Input(2, name='c')\r\nd = tf.keras.Input(2, name='d')\r\nout = tf.keras.layers.Add()([c*10, d])\r\nmodel = tf.keras.Model([c, d], [out])\r\n\r\nmodel({'c': tf.ones([1, 2]) * 2, 'd': tf.ones([1, 2]) * 3}).numpy() \r\n# As expected, this outputs: array([[23,  23]])\r\n\r\nmodel({'c': tf.ones([1, 2]) * 2, 'a': tf.ones([1, 2]) * 3}).numpy()\r\n# Unexpectedly, this outputs: array([[32, 32]])\r\n# I would expect that this call throws an exception\r\n```\r\n\r\n", "Whoops, also just saw @jjedele 's comments regarding `output_names`.  Although this would allow us to reconstruct model.output's naming information, in my opinion the API proposed above feels a little more intuitive and would be nicer to work with since we wouldn't have to go through the extra trouble of rebuilding the input and output dictionaries every time we want to extend a model.", "IIUC this breaks backward compatibility if users were passing a dictionary before? Seems niche case we can drop for the broader benefit here...", "@omalleyt12, is any of your recent updates to model training logic addressing this issue?", "@qlzh727 As for the output logic I think [34691](https://github.com/tensorflow/tensorflow/issues/34691) solves it.", "any update on this?", "@huyng @jjedele Regarding the output names: if one persists the Keras model as a SavedModel the output names are lost and are not retrievable. This happens if you use tf serving since then you do not have the option to load the model as a Keras model, only a SavedModel where this information is lost. It would be very nice the output names of the layers are persisted since this greatly facilitates multi output models in a live industry setting.", "I think this is supported since 2.2? Can anyone check how this works:\r\n```\r\nkeras_input = tf.keras.Input(shape=(5,))\r\noutput = tf.keras.layers.Dense(units=1)(keras_input)\r\nfunc_model = tf.keras.Model({'a': keras_input}, {'b': output})\r\nfunc_model({'a': np.random.random((32, 5))})\r\n```", "> I think this is supported since 2.2? Can anyone check how this works:\r\n> \r\n> ```\r\n> keras_input = tf.keras.Input(shape=(5,))\r\n> output = tf.keras.layers.Dense(units=1)(keras_input)\r\n> func_model = tf.keras.Model({'a': keras_input}, {'b': output})\r\n> func_model({'a': np.random.random((32, 5))})\r\n> ```\r\n\r\nThis piece of code works in v2.3.1, but I think it's just luck because `func_model({'zzzzzzz': np.random.random((32, 5))})` also works in the last line.\r\n\r\nThese [lines](https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/keras/engine/functional.py#L489-L492) show that the code matches the input tensors to the input nodes of a graph by orders, not by keys.\r\nhttps://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/keras/engine/functional.py#L489-L492", "@huyng It looks like you are using an older Version of Tensorflow `2.0`. Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version `2.6.0` and let us know if the issue still persists? Please refer to the similar[ issue ](https://github.com/tensorflow/tensorflow/issues/34691) & let us know if it helps ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34113, "title": "Decoding JPEGs much slower on Windows", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.15.0 GPU\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0/7.6.5\r\n- GPU model and memory: 2080 Ti\r\n\r\nMy input pipeline is much slower on Windows than Linux.  On Linux, I'm using 10% of my CPU (Threadripper 2950x), to parse 1000 640x480 images per second.  On Windows, I'm using 85% of my CPU to parse 300 640x480 images per second.  I found [this TODO](https://github.com/tensorflow/tensorflow/blob/4b628e8a154c4fbd74ed13d3284fb887a5103e41/tensorflow/core/lib/jpeg/jpeg_mem.cc#L438), which suggests that the issue is that the windows builds of Tensorflow don't use libjpeg-turbo.  I dug a little and found [this issue](https://github.com/tensorflow/tensorflow/issues/4807) and [this PR](https://github.com/tensorflow/tensorflow/pull/5547), which covers the discussion of using libjpeg-turbo, but neither mention any issues with Windows.\r\n\r\nIs it possible to use libjpeg-turbo on Windows?", "comments": ["@LucasSloan Can you please share a simple standalone code to reproduce the issue? Also, did you notice any similar  issue with GPU? Thanks!", "@LucasSloan Did you had any time to prepare simple standalone code to reproduce the issue? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 34112, "title": "DatasetVariantWrapper \"No unary variant device copy function found\"", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (but running on an official image)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04, running the Ubuntu 18.04 tensorflow-gpu Docker image provided by GCloud\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): tensorflow-gpu-2.0.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: P100 x1\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI'm running Tensorflow on GCloud using the gcr.io/deeplearning-platform-release/tf2-gpu.2-0 image. I'm attempting to train a subclassed tf.keras.Model. \r\n\r\nI can train my image both on my local machine and GCloud, provided the --runtime=nvidia arg is NOT provided. When I add that argument, the GCloud image fails with the following error:\r\n\r\ntensorflow.python.framework.errors_impl.InternalError:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper\r\n\t [[{{node MapDataset/_8}}]] [Op:__inference_get_input_516]\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect TensorFlow to continue to run successfully when using the --runtime=nvidia arg, i.e. enabling CUDA.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nBecause I have no understanding of the origin of this internal error, I am not sure how to create such a minimal case. Please advise.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFull failing log:\r\n\r\n  File \"/home/zoobot/zoobot/estimators/run_estimator.py\", line 33, in run_estimator\r\n    train_dataset = input_utils.get_input(config=config.train_config)\r\n  File \"/root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 526, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper\r\n\t [[{{node MapDataset/_8}}]] [Op:__inference_get_input_516]\r\n\r\nFull successful log, without using --runtime=nvidia:\r\n\r\n2019-11-08 23:34:49.753010: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-11-08 23:34:59.531666: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2019-11-08 23:34:59.531739: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-11-08 23:34:59.531778: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\r\n2019-11-08 23:34:59.532239: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-08 23:34:59.540322: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-11-08 23:34:59.540738: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cf105c9a70 executing computations on platform Host. Devices:\r\n2019-11-08 23:34:59.540776: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nWARNING:root:Loading multiple tfrecords with interleaving, shuffle=False\r\nWARNING:root:Loading multiple tfrecords with interleaving, shuffle=False\r\n2019-11-08 23:35:37.148322: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\r\n2019-11-08 23:35:37.148432: E tensorflow/core/platform/default/device_tracer.cc:70] CUDA error: <unknown>\r\n...\r\nEpoch 1/100\r\n...", "comments": ["I should perhaps add that I have successfully trained models on the same GCloud machine (with the same CUDA install) on TF 1.14. ", "Thanks @oanush. A brief update - I have changed my setup to run on the GCP AI Platform VM (https://cloud.google.com/ai-platform/deep-learning-vm/docs/) directly (i.e. without docker) and receive the same error.", "Seeing the same issue on TF2.0 , it only happens when using a GPU and a @tf.functions (graph mode), Everything seems to run ok when running on eager mode, or CPU only ", "Jiri, it looks like the graph is trying to copy an instance of `DatasetVariantWrapper` from CPU to GPU.   Should that ever happen?", "In general, this can happen -- for instance if the user program makes incorrect use of device scopes.\r\n\r\n@mwalmsley you can create a minimal reproducible example by starting with your user program and repeatedly removing complexity from it (e.g. replacing a complex model with a trivial one, replacing complex input pipeline with a trivial one) until there is nothing that can be removed without the issue going away. In the absence of knowing what the root cause is, this process is helpful in zeroing in on the issue. ", "> In general, this can happen -- for instance if the user program makes incorrect use of device scopes.\r\n\r\nCan this ever happen in a _correct_ program?", "Yes, for instance https://github.com/tensorflow/tensorflow/issues/34519.", "> Yes, for instance #34519.\r\n\r\nI did not phrase this unambiguously: is there a situation where TF should legitimately need to copy a `DatasetVariantWrapper` between CPU and GPU?  If yes, the right fix should be to teach TF to do this copy, right?", "In that case the answer is no. The tf.data implementation prevents such copies from happening silently (by not implementing / registering a method for copying the dataset variant) and instead relies on explicit APIs (such as the multi-device iterator [API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/multi_device_iterator_ops.py used by tf.distribute) to copy data between devices.", "@jsimsa @sanjoy: since tf.data intentionally prevented `DatasetVariantWrapper` from being copied (I assume it is for performance reason), there should be extra logic (specific to `DatasetVariantWrapper`) to ensure we never try to copy `DatasetVariantWrapper` between devices, right? Do you know how is this achieved?", "> Do you know how is this achieved?\r\n\r\nProbably placer needs to understand and respect this constraint.  Maybe @iganichev knows?", "As far as I know, the only dataset specific logic in placer is [1], which explicitly avoids copy of DatasetVariantWrapper between two DatasetOp nodes. I think we need logic in placer that prevents copy of DatasetVariantWrapper if we don't already have this. But that probably requires some expertise in tf.data to expose the necessary information.\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/colocation_graph.cc#L722", "> > Do you know how is this achieved?\r\n> \r\n> Probably placer needs to understand and respect this constraint. Maybe @iganichev knows?\r\n\r\nAs @lindong28 mentioned, placer does not know the type inside DT_VARIANT at placement time. So, it can't automatically colocate nodes based on it. If the relevant nodes, always take the same underlying types and they are not copiable, it should be easy to add logic similar to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/colocation_graph.cc#L722", "Hi,\r\nI'm running Tensorflow 2.2.0-rc2 on Google Colab with GPU enabled, and I have the same error when I run this little piece of code:\r\n\r\n``` python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef loop():\r\n    \r\n    H = tf.random.uniform(shape=[100, 1])\r\n    DS = tf.data.Dataset.from_tensor_slices(H).batch(10)  \r\n\r\n    for i in tf.range(100): \r\n        for h in DS:\r\n            test = h + h\r\n            \r\nloop()\r\n```\r\n\r\nThe exact error is the following:\r\n\r\n> InternalError                             Traceback (most recent call last)\r\n> <ipython-input-13-d7fba13d49c2> in <module>()\r\n>      11             test = h + h\r\n>      12 \r\n> ---> 13 loop()\r\n> \r\n> 5 frames\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n>      58     ctx.ensure_initialized()\r\n>      59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n> ---> 60                                         inputs, attrs, num_outputs)\r\n>      61   except core._NotOkStatusException as e:\r\n>      62     if name is not None:\r\n> \r\n> InternalError: 2 root error(s) found.\r\n>   (0) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper\r\n> \t [[{{node BatchDatasetV2/_18}}]]\r\n> \t [[Func/while/body/_1/input/_51/_46]]\r\n>   (1) Internal:  No unary variant device copy function found for direction: 1 and Variant type_index: tensorflow::data::(anonymous namespace)::DatasetVariantWrapper\r\n> \t [[{{node BatchDatasetV2/_18}}]]\r\n> 0 successful operations.\r\n> 0 derived errors ignored. [Op:__inference_loop_5186]\r\n> \r\n> Function call stack:\r\n> loop -> loop\r\n\r\nStrangely, if I unroll the for loop (as defined [there](https://www.tensorflow.org/tutorials/customization/performance#for_loops)), I don't have any error:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef loop():\r\n    \r\n    H = tf.random.uniform(shape=[100, 1])\r\n    DS = tf.data.Dataset.from_tensor_slices(H).batch(10)  \r\n\r\n    for i in range(100): \r\n        for h in DS:\r\n            test = h + h\r\n\r\nloop()\r\n```\r\n\r\nI have been able to reproduce this error on my own computer running Ubuntu 18.04.3 LTS, CUDA 10.2, and Tensorflow 2.2.0-rc2 installed with pip.\r\n\r\nCan you spot what is causing the error and if it's related to the problem described in this thread?\r\n", "@jsimsa, I remember you guys looked at this some time ago. What was the conclusion?", "This is related to https://github.com/tensorflow/tensorflow/issues/34519. Alex made a suggestion there that @saxenasaurabh agree with but I don't think anyone is actively working towards fixing this.", "With TF 2.2 I'm still running into the error in the title of the issue", "I have not been able to solve this all day; I'm getting the exact error in the title, verbatim, and all of the things discussed in the first several comments in this thread apply to my situation exactly, as well.\r\n\r\nMy code works fine on a standard CPU, for example, when I run it locally. But when I try to run it on a cloud TPU, this exact thing happens.", "Hey @Ryan-Rudes, @ppwwyyxx, @mgoutay, @mwalmsley, sorry and I know it is disappointing that TF team has not been able to provide a solution for such a long time.\r\n\r\nAs I am trying to finding a workaround to this issue, I came across [this](https://github.com/tensorflow/tensorflow/issues/34519#issuecomment-636138360) reply by @AdrienCorenflos and validated that the following code works. The workaround is to replace `dataset` with an `iterator` where the iterator is explicitly placed on the CPU device.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef loop():\r\n    H = tf.random.uniform(shape=[100, 1])\r\n    DS = tf.data.Dataset.from_tensor_slices(H).batch(10)  \r\n\r\n    with tf.device('/CPU:0'):\r\n        iterator = iter(DS)\r\n\r\n    for i in tf.range(10): \r\n        for h in iterator:\r\n            test = h + h\r\n            tf.print(test)            \r\nloop()\r\n```\r\n\r\nIt would be great if this workaround can unblock your ongoing work. On the other hand TF team should still try to fix the issue in TF so that users do not need to change code as described above.\r\n\r\nCould you try it out and see if this workaround works for you?", "This issue should be fixed as of TF 2.3. \r\n\r\n@Ryan-Rudes , @ppwwyyxx, @mwalmsley etc - can you see if you're still encountering this and reopen if so?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34112\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34112\">No</a>\n", "Hey Rachel and everyone, I think the issue still exists in TF 2.3. But it has been fixed in tf-nightly==2.4.0.dev20200902 and thus it should be fixed in the upcoming TF 2.4 release.\r\n\r\nIt can be validated using this colab example: https://colab.research.google.com/drive/1C4AXS8cyEsA_JmU7QVVk7UXM_uygVf05?authuser=1#scrollTo=jJBsJsRSR6AD", "Ah, I see that there are two subtly different issues here:\r\n\r\n(1) \r\n```\r\n    dataset = tf.data.Dataset.range(10)\r\n\r\n    @tf.function\r\n    def f():\r\n      for i in tf.range(1):\r\n        for x in dataset:\r\n          tf.print(x)\r\n\r\n    f()\r\n```\r\n\r\n(2) \r\n```\r\n    @tf.function\r\n    def f():\r\n      dataset = tf.data.Dataset.range(10)\r\n      for i in tf.range(1):\r\n        for x in dataset:\r\n          tf.print(x)\r\n    f()\r\n```\r\n\r\n(1) has been fixed as of TF 2.3. (see #34519)\r\n\r\n(2) is still an open issue. \r\n\r\nDepending on how your tf.function is defined, you might still be seeing this bug. I'm reopening this issue to track (2).", "Was able to replicate the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/6e008edb0b84e3b5527264d3de09e12f/untitled77.ipynb)..Thanks !", "@mwalmsley \r\nThis has been fixed in nightly please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e6b288651a3944a6b720b30e2ffd3b86/untitled635.ipynb).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34112\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34112\">No</a>\n"]}, {"number": 34111, "title": "How can I visualize the `learning_rate` metrics on Tensorboard?", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:3.6.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.0/7.4\r\n- GPU model and memory:Nvidia Geforce 840m\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI'm trying to visualize the `learning_rate` on Tensorboard using Estimator mode ( not Keras ).\r\nI'm testing many solutions proposed on StackOverflow and on Github. But I didn't get any results.\r\nI have tried also `tf.summary.scalar()` and `learning_rate_fn` functions, but any of those solutions doesn't work for me.\r\nI really search for a solution to my problem \r\n\r\nthat was some of my code \r\n```python\r\n # Calculate loss using mean squared error.\r\n    label_tensor = tf.convert_to_tensor(labels, dtype=tf.float32)\r\n    loss = tf.losses.mean_squared_error(\r\n        labels=label_tensor, predictions=logits)\r\n    tf.identity(loss, name='loss')\r\n    tf.summary.scalar('loss', loss)\r\n\r\n    # Create train hook list to visualize loss , accuracy and global_steps\r\n    # Configure the train OP for TRAIN mode.\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001) # Reducing the learning rate value from 0.001 to 0.0001\r\n        train_op = optimizer.minimize(\r\n            loss=loss,\r\n            global_step=tf.train.get_global_step()\r\n            )\r\n        \r\n    rmse_metrics = tf.metrics.root_mean_squared_error(labels=label_tensor,predictions=logits)\r\n    metrics = {'eval_mse': rmse_metrics}\t\r\n    tf.identity(rmse_metrics[1], name='root_mean_squared_error')\r\n    tf.summary.scalar('root_mean_squared_error', rmse_metrics[1])\r\n    learning_rate=\r\n    tf.identity(learning_rate, name='learning_rate')\r\n    tf.summary.scalar('learning_rate',learning_rate)\r\n\r\n    #return tf.estimator.EstimatorSpec(\r\n     #   mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        loss=loss,\r\n        train_op=train_op,\r\n        eval_metric_ops=metrics,\r\n        export_outputs={'marks': tf.estimator.export.RegressionOutput(logits)})\r\n    \r\n```\r\n\r\nAnyone can help me?\r\n\r\n\r\n\r\n\r\n", "comments": ["@abdou31 Please post this question on stack overflow as this is not related to build/install, bug/performance or docs related issues. Thanks!", "I post this question and I didn't got any answer!", "I post this question and I didn't got any answer!"]}, {"number": 34110, "title": "Ensured buffer encapsulation", "body": "I removed the buffer from global scope. Which I think makes more sense in case of future changes.", "comments": ["There's an updated commit to this. Located here: https://github.com/tensorflow/tensorflow/pull/34145/files#diff-26b3967588e26e01d50fea15bdf657cc\r\n\r\nSorry for the confusion :-)"]}, {"number": 34109, "title": "Don't compile CUDA kernels in debug mode.", "body": "-G causes kernels to use more registers and memory.\r\nThis results in lots of kernels using too many resources,\r\ncausing them to fail when launched\r\nThis makes it impossible to run a TensorFlow debug build.", "comments": []}, {"number": 34108, "title": "TfLite GL delegate is built with visibility hidden", "body": "Fixes #33676\r\n\r\nThere is no C++ API in OpenGL delegate code and it manually exports necessary symbols.\r\nSo `visibility=hidden` should suffice on gcc/clang targets\r\nOn msvc symbols are hidden by default.", "comments": ["@jdduke It seems I misunderstood bazel's default conditionals.\r\nAccording to docs default kicks in only when nothing else matches, so I updated commit by adding flag on android"]}, {"number": 34107, "title": "huge amount of memory usage in load_model (tf 2.0)", "body": "Hi, I retrained a Inception + DenseLayer(512) using keras in tf 2.0.\r\n\r\nWhen I load this model with tf 1.14, it uses something around 4G of GPU memory. But, when I'm doing the same with tf 2.0 it uses more than 25Gb of memory!!\r\nThis is correctly? That is some other config to load the model now?\r\n\r\nI'm using an ec2 with a tesla v100 and cuda 10.1 \r\n\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.14 and 2.0 (different environments)\r\n- Python version: 3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla V100 \r\n\r\n", "comments": ["@jessica-santos \r\n\r\nRequest you to share minimal standalone  code to reproduce the issue in our environment,it helps us localizing the issue faster. Thanks!", "Sure, this is the code I run to build and train my model:\r\n\r\n```\r\ndef build_model(classe=3):\r\n\r\n    base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\r\n    last = base_model.output\r\n\r\n    x = GlobalAveragePooling2D()(last)\r\n\r\n    x = Dense(512, activation='relu')(x)\r\n\r\n    x = Dropout(.3)(x)\r\n\r\n    preds = Dense(classe, activation='sigmoid')(x)\r\n    model = Model(base_model.input, preds)\r\n\r\n    return model\r\n\r\n\r\ndef compile_model(model, learning_rate=1e-4):\r\n\r\n    optimizer = Adam(lr=learning_rate)\r\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\r\n    return model \r\n\r\ncore_idg = ImageDataGenerator(brightness_range=[0.3, 0.5],\r\n                              rotation_range=35, \r\n                              fill_mode='nearest',\r\n                              zoom_range=0.125,\r\n                              rescale = 1./255)\r\n\r\ntrain_generator = core_idg.flow_from_dataframe(train_df, directory=None, x_col='Path', \r\n                                    y_col=classes_patologias, class_mode='other',\r\n                                    target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE)\r\n\r\nval_generator = core_idg.flow_from_dataframe(valid_df, directory=None, x_col='Path', \r\n                                    y_col=classes_patologias, class_mode='other',\r\n                                    target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE)`\r\n```\r\n\r\n```\r\nmodel = build_model(classe=len(classes_patologias))\r\nmodel = compile_model(model)\r\n```\r\n\r\n```\r\ncheckpoint = ModelCheckpoint(\r\n    filepath='model.h5',\r\n    monitor='val_loss',\r\n    verbose=1,\r\n    save_best_only=True,\r\n    mode='min',\r\n    period=1\r\n)\r\n```\r\n\r\n```\r\nmodel.fit_generator(\r\n    generator=train_generator,\r\n    steps_per_epoch=len(train_df) // BATCH_SIZE,\r\n    epochs=50,\r\n    validation_data=val_generator,\r\n    validation_steps=len(valid_df) // BATCH_SIZE,\r\n    class_weight=sample_weights,\r\n    verbose=1,\r\n    callbacks=[checkpoint, reduce_on_plateau, early_stop],\r\n    workers=4,\r\n    max_queue_size=16,\r\n    use_multiprocessing=False\r\n)\r\n```\r\n\r\nThis is the simple code to load the model:\r\n\r\n```\r\nfrom tensorflow.keras.models import load_model\r\nmodel = load_model('model.h5')\r\n```", "@jessica-santos \r\n\r\nLooks like code is incomplete. Request you to provide complete code snippet to reproduce the issue in our environment. Thanks!", "The only thing missing was those two callbacks:\r\n```\r\nearly_stop = EarlyStopping(monitor='val_loss',\r\n                               min_delta=0.0001,\r\n                               patience=5,\r\n                               mode='min',\r\n                               verbose=1)\r\n\r\nreduce_on_plateau = ReduceLROnPlateau(\r\n    monitor='val_loss',\r\n    factor=0.1,\r\n    patience=2,\r\n    verbose=1,\r\n    mode='min',\r\n    min_delta=0.01,\r\n    cooldown=0,\r\n    min_lr=0\r\n) \r\n```", "Looks like some undefined variables like `train_df, valid_df` are there in the code. Request you to provide the same . Thanks!", "As the name suggests, those are dataframes with train and valid data respectively. The columns are: `'Path', 'class_1','class_2','class_3'`.  `Path` contains the full path to the image and the classes variables contain 1 or 0 for each class.\r\n\r\nThe variable `classes_patologias` is the list `['class_1','class_2','class_3']`.\r\n\r\nAlso:\r\n```\r\nIMAGE_SIZE = 384\r\nBATCH_SIZE = 16\r\n```\r\n\r\nI cannot share the data, unfortunately. ", "@jessica-santos \r\n\r\nIs it possible to share dummy data so we can try to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "We use medical images, you can get a free dataset from Kaggle, like this one: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia", "Any updates?", "@rjafarau Do you have any simple standalone code to reproduce the issue? Standalone codes results in faster resolution. Thanks! ", "@jvishnuvardhan sure\r\n\r\n**System information**\r\nOfficial Tensorflow docker image tensorflow/tensorflow:2.0.0-gpu-py3-jupyter\r\nGPU model and memory: GeForce GTX 1070 Ti 8Gb\r\n\r\n```python\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\r\nfrom tensorflow.keras.applications import DenseNet121\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\r\nfrom tensorflow.keras.models import load_model\r\n\r\nN, H, W, C = 9000, 224, 224, 3\r\n\r\nexample_model = Sequential([\r\n    DenseNet121(input_shape=(H, W, C),\r\n                include_top=False,\r\n                weights='imagenet'),\r\n    GlobalAveragePooling2D(),\r\n    Dense(1, activation='sigmoid')\r\n])\r\n\r\nexample_model.compile(optimizer='adam',\r\n                      loss='binary_crossentropy',\r\n                      metrics=['accuracy'])\r\n\r\ncheckpoint_cb = ModelCheckpoint(filepath='example_model.h5',\r\n                                save_best_only=True)\r\nearly_stopping_cb = EarlyStopping(patience=5,\r\n                                  restore_best_weights=True)\r\n\r\nnp.random.seed(42)\r\nhistory = example_model.fit(\r\n    x=np.random.rand(N, H, W, C),\r\n    y=np.random.randint(2, size=N),\r\n    validation_split=0.2,\r\n    epochs=10,\r\n    batch_size=32,\r\n    callbacks=[checkpoint_cb, early_stopping_cb]\r\n)\r\n\r\n# here is a problem\r\nloaded_example_model = load_model('example_model.h5')\r\n```", "@rjafarau Your files size is very very big. You need to use data pipelines to optimize the memory usage.  I reduced your data size and saved and loaded model without any issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/edf35e772c9492914a6145e0434ffdd8/untitled706.ipynb). Thanks! ", "@jvishnuvardhan my original issue is not during the training, is when I load the trained model to do the predict. Do you know if there was any progress on that?", "@jessica-santos Can you please provide a simple standalone code to reproduce the issue? Without standalone code it is difficult to make any progress. You could take any public data and create a standalone code to reproduce the issue. Thanks! ", "@jvishnuvardhan Please make sure that you understand me and @jessica-santos correctly. The problem is not in the training process. I trained a model and saved it. After that, I have an 82Mb hdf5 file. I expect that loading a model from it won't take a lot of RAM. But it doesn't. In my case, it takes over 20Gb.\r\nIn my original code, I use ImageDataGenerator with default batch_size. So there is no problem in training.\r\nYou have all that you need (standalone code that reproduces the issue).", "@rjafarau I understand your problem. As I don't have your `82MB hdf5` file, I ran training to save the model with weights. However, data size was too big (I didn't use ImageDatagen) so the program was crashing. I reduced the data size and ran the model, saved (`81.42MB`) and loaded the model without any issue. Please let me know if I am missing anything here. Thanks!", "@jvishnuvardhan How much RAM do you have? ", "@rjafarau I had 12.72 GB ram available but it was not using much. Please check it yourself on the colab (top right corner). Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I have the same problem , anyone find the issue ?", "> @jvishnuvardhan Please make sure that you understand me and @jessica-santos correctly. The problem is not in the training process. I trained a model and saved it. After that, I have an 82Mb hdf5 file. I expect that loading a model from it won't take a lot of RAM. But it doesn't. In my case, it takes over 20Gb.\r\n> In my original code, I use ImageDataGenerator with default batch_size. So there is no problem in training.\r\n> You have all that you need (standalone code that reproduces the issue).\r\n\r\nHi , If you find the problem please let me know \r\n\r\nthx", "Please open a new issue, fill in issue template", "@grkashani In my case, a simple update to the newer version helped me (from 2.0 to 2.1 as I remember). Good luck!", "I have the same issue can anyone help me.", "here is the solution:\r\nhttps://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth "]}, {"number": 34106, "title": "add a 'dbg' build configuration as a shorthand for '--config=opt -c dbg'", "body": "Disable arm-neon by default for now to work around a gcc issue", "comments": []}, {"number": 34105, "title": "[Intel MKL] Compilation Fix in a test", "body": "This resolves a compilation error in a test which caused by a recent change in constructor signature.", "comments": []}, {"number": 34104, "title": "[ROCm] Fix for the broken ROCm CSB.", "body": "The following commit breaks the --config=rocm build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/f72695e1717a545bfc898b7230cc195bf28b43df\r\n\r\nThe above commit adds a couple of subtests that require support for the `StatefulUnirformFullInt` Op on the GPU. Currently ROCm does not support that Op on the GPU, which leads to those subtests failing.\r\n\r\nThe \"fix\" is to skip those subtests on the ROCm platform.\r\n\r\n------------------------\r\n\r\n/cc @chsigg @whchung ", "comments": ["@chsigg @tanzhenyu gentle ping", "The error message we get for the failure indicates that `StatefulUniformFullInt` is getting instantiated in the TF graph.\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'StatefulUniformFullInt' OpKernel for 'GPU' devices compatible with node {{node StatefulUniformFullInt}}                                                                                                                                                                                                                                                        \r\n        .  Registered:  device='XLA_GPU'; shape_dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; dtype in [DT_INT32, DT_INT64, DT_UINT32, DT_UINT64]                                                                                                                                                                                                      \r\n  device='XLA_CPU'; shape_dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; dtype in [DT_INT32, DT_INT64, DT_UINT32, DT_UINT64]                                                                                                                                                                                                                            \r\n  device='XLA_CPU_JIT'; shape_dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; dtype in [DT_INT32, DT_INT64, DT_UINT32, DT_UINT64]                                                                                                                                                                                                                        \r\n  device='CPU'; dtype in [DT_UINT64]                                                                                                                                                                                                                                                                                                                                                                                                 \r\n  device='CPU'; dtype in [DT_UINT32]                                                                                                                                                                                                                                                                                                                                                                                                 \r\n  device='CPU'; dtype in [DT_INT64]                                                                                                                                                                                                                                                                                                                                                                                                  \r\n  device='CPU'; dtype in [DT_INT32]                                                                                                                                                                                                                                                                                                                                                                                                  \r\n  device='XLA_GPU_JIT'; shape_dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; dtype in [DT_INT32, DT_INT64, DT_UINT32, DT_UINT64]                                                                                                                                                                                                                        \r\n [Op:StatefulUniformFullInt] name: model_1/random_crop/stateful_uniform_full_int/                                                                                                                                                                                                                                                                                                                                                    \r\n```", "@chsigg @tanzhenyu gentle ping", "@gbaned , anything we can do to help get this PR merged?"]}, {"number": 34103, "title": "Repeating numbers in uniform random Tensor created on GPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\nr1.12\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\n0.15.1\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n- CUDA/cuDNN version:\r\n9.0/7.4\r\n- GPU model and memory:\r\nQuadro M2200 4GiB\r\n\r\n**Describe the current behavior**\r\nI need to generate a tensor of uniform randoms inside a custom op. I basically copy the code for generating a tensor for zeros as is done [here](https://github.com/tensorflow/tensorflow/blob/5b900cfe4b3b848f577315a0dde09a729f770e95/tensorflow/contrib/rnn/kernels/lstm_ops.h#L31)\r\n\r\n```c++\r\ntemplate <typename Device, typename T>\r\nstruct TensorRandom {\r\n  void operator()(const Device& d, typename TTypes<T>::Flat t) {\r\n    t.device(d) = t.random(); // this is my only change to the TensorZeros function\r\n  }\r\n};\r\n```\r\n\r\nInside my kernel I have something like the following\r\n\r\n```c++\r\nTensorShape my_shape({N, H, W});\r\nTensor* random_mat;\r\nOP_REQUIRES_OK(ctx, ctx->allocate_output(\"random_mat\", my_shape, &random_mat));\r\nconst Device& device = ctx->eigen_device<Device>(); // this will be a gpu\r\nfunctor::TensorRandom<Device, T>()(device, random_mat.flat<T>());\r\n\r\nVLOG(1) << \"Random Mat \" << random_mat.shape().DebugString()\r\n              << random_mat.SummarizeValue(N * H * W);\r\n```\r\n\r\nWhen I compile this and run my op (which I've defined in the python api) I get something like this for (N,H,W)=(2,4,2)\r\n\r\n```\r\nRandom Mat: (2,4,2) [[[0.93335256, 0.53328224],\r\n        [0.18036943, 0.12565934],\r\n        [0.93335256, 0.53328224],\r\n        [0.042617  , 0.61869474]],\r\n\r\n       [[0.93335256, 0.53328224],\r\n        [0.70387461, 0.88239244],\r\n        [0.93335256, 0.53328224],\r\n        [0.76217792, 0.65087953]]]\r\n```\r\n\r\nNotice the first value is repeated every 4 elements. The second value is also repeated every 4 values. The rest are seemingly random.\r\n\r\n**Describe the expected behavior**\r\nI would expect the output to be random. Instead, the first and second value are always repeated every fourth element. \r\n\r\n**Code to reproduce the issue**\r\nSee above. Only way I was able to test was by also creating the python bindings for the Op\r\n\r\n**Other info / logs**\r\nThis issue occurs for all shapes of tensor I have tried. I've also checked the tensor is indeed aligned. I'm using single precision floating point numbers. I _highly_ suspect this is a GPU specific problem. If there is a better way to generate a random Tensor I'd be happy to use that as well", "comments": ["Hi, any updates on this?", "I've dug into this some more and a user on StackOverflow pointed me to the warning at the bottom of [this Eigen documentation](http://eigen.tuxfamily.org/dox/TopicMultiThreading.html) indicating the issue might be related to thread safety.\r\n\r\nIn light of the above, I'm trying to use [FillPhiloxRandom](https://github.com/tensorflow/tensorflow/blob/be5d5e3f7534269782a3bdb3d806988c172c9db0/tensorflow/core/kernels/random_op.h#L49) instead as suggested by a different user on SO. When I compile my op, which is inside of //tensorflow/contrib, I get an error \"undeclared inclusion(s) in rule <name of .so file being built>\" and \"this rule is missing dependency declarations for the following files included by <my_file.cc>: 'tensorflow/core/kernels/random_op.h'\".\r\n\r\nI tried adding \"//tensorflow/core/kernels:random_ops\" to srcs in BUILD but it doesn't like the dependence on //tensorflow/framework which I've found is a hardcoded rule in tf_custom_op_library. What is the supported way of filling a tensor with random numbers on the GPU?", "Closing since I ended up having to abandon this approach as it doesn't seem possible to use the FillPhiloxRandom function inside a contrib module without reimplementing the algorithm. Ended up using random uniform capabilities in array_ops to pass a random tensor directly into my op", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34103\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34103\">No</a>\n"]}, {"number": 34102, "title": "Download link for PoseNet tflite doesn't work", "body": "Pose Net download link from https://www.tensorflow.org/lite/models/pose_estimation/overview\r\ndoesn't work.\r\n\r\nhttps://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite", "comments": ["I was able to download it Firefox but same link doesn't work on chrome.", "Was able to download it on chrome. Try uninstalling and reinstalling your browser \r\n![image](https://user-images.githubusercontent.com/47574994/68514763-83708b00-0233-11ea-981f-5eaf521602a4.png). Thanks!\r\n"]}, {"number": 34101, "title": "batch_gather_nd op <del>Size Op</del> missing for Tensorflow Lite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary docker latest\r\n- TensorFlow version (or github SHA if from source):v2.0.0-rc2-26-g64c3d38 2.0.0\r\n\r\nAny model using `batch_gather_nd` (`gather_nd` when `batch!=0` and input tensor has indefinite batch dimension) will rely on `Size` op (`batch_gather_nd`->`meshgrid`->`size`->`Size`) when the batch dim is -1 (unspecified). Lack of `Size` op renders `gather_nd` op useless in many cases.\r\n\r\nAnother possible fix here: decouple `batch_gather_nd` from `Size`. (Is that possible?)\r\n\r\nHere I used a monkey patch here to solve the problem: when batch dimension is 1, do not call `meshgrid` and directly use the `range` above the line since `range` does not rely on `size`.\r\n```\r\n    mesh_list = meshgrid(*dim_ranges, indexing=\"ij\") if dim_ranges else []\r\n```\r\nchanged into\r\n```\r\n    if len(dim_ranges)==0:\r\n      mesh_list = [] \r\n    elif len(dim_ranges)==1:\r\n      mesh_list = dim_ranges\r\n    else:\r\n      mesh_list = meshgrid(*dim_ranges, indexing=\"ij\") if dim_ranges else []\r\n```\r\nThat might be helpful if anyone want to use `batch_gather_nd` on tensorflow lite.\r\n\r\nUpdate: I am wrong here. `batch_gather_nd` is not supported in tensorflow lite at all. The reason lies in that `batch_gather_nd` used `range` with indefinite length, which makes the dimension of the output empty; however `range` is used later and `tile`d and `concatenated` etc, however in the eyes of tensorflow optimizer, the shape is always [] so they are noop and any operation related are noop, effectively makes shape of every tensor related empty (`shape=[]`). So it is the `batch_gather_nd` op that is missing\r\n\r\nUpdate: I have just noticed that Tensorflow Lite will always generate code for batch=1, so a fast solution is: set a export_lite flag in keras model definition part and make the lambda layer for batch_gather_nd act specially when exporting for tensorflow lite - return the tensor as if batch=1 even when batch=None actually.\r\nSo the problem remained for tensorflow devs here is: support the harmonious implementation of batch_gather_nd between tensorflow and tensorflow lite; or natively provide a GatherNd layer in keras that works correctly in tensorflow lite.\r\n\r\nSummary:\r\n\r\nExpected behavior: using gather_nd when batch=1 should generate correct graph when a keras model is exported into tensorflow lite\r\n\r\nActual behavior: batch_gather_nd fails to generate correct dimension for its output in tensorflow lite.\r\n\r\nReproduce: To reproduce behavior, use gather_nd in one lambda layer (with batch=1) and export the model into tensorflow lite\r\n\r\nProblem: gather_nd should be specially dealt with in exporting process.\r\n\r\nMonkey patch in user code: in the gather_nd function in your Lambda layer, check out whether you are exporting into a tensorflow lite binary. If an exporting is in progress, take as if the dim is 1 and keep the `batch_dims=0`. When `arg=[params,indices]`, simply `gather_nd(arg[0][0], arg[1])` will do the job.\r\n\r\nSolution: make a dedicated GatherNd layer in keras who can correctly handle the different behavior between tensorflow and tensorflow lite.", "comments": ["@ThinerDAS,\r\nIs this still an issue? Could you please update TensorFlow to v2.3 and check if you are still facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34101\">No</a>\n"]}, {"number": 34100, "title": "many proto files lack go_package", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15.0\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: release .tar.gz\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nAs noted in https://github.com/golang/protobuf/issues/984, a number of `*.proto` files lack `go_version` statements. \r\n\r\nThe commands:\r\n\r\n```bash\r\nprotoc \\\r\n       -I./github.com/tensorflow/tensorflow \\\r\n       --go_out=plugins=grpc:./ \\\r\n       ./github.com/tensorflow/tensorflow/tensorflow/core/framework/*.proto\r\nprotoc \\\r\n       -I./github.com/tensorflow/tensorflow \\\r\n       --go_out=plugins=grpc:./ \\\r\n       ./github.com/tensorflow/tensorflow/tensorflow/core/protobuf/{saver,saved_model,struct,trackable_object_graph,saved_object_graph,meta_graph}.proto\r\n```\r\n\r\ngive the error:\r\n\r\n```bash\r\n2019/11/07 14:43:01 protoc-gen-go: error:inconsistent package import paths: \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf\", \"tensorflow/core/protobuf\"\r\n```\r\n\r\nI have worked around this locally by adding `go_package` statements to the following files so everything is emitted into `github.com/tensorflow/tensorflow/tensorflow/go/core`:\r\n\r\n* tensorflow/core/protobuf/saved_object_graph.proto\r\n* tensorflow/core/protobuf/struct.proto\r\n* tensorflow/core/protobuf/trackable_object_graph.proto\r\n\r\n```\r\noption go_package = \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf\";\r\n```\r\n\r\nA number of other `*.proto` files also appear to lack `go_package`; not sure if they all need update:\r\n\r\n```\r\n$ grep -L go_package $(find tensorflow/core -name \"*.proto\")\r\ntensorflow/core/kernels/boosted_trees/boosted_trees.proto\r\ntensorflow/core/util/test_log.proto\r\ntensorflow/core/util/memmapped_file_system.proto\r\ntensorflow/core/util/saved_tensor_slice.proto\r\ntensorflow/core/util/example_proto_fast_parsing_test.proto\r\ntensorflow/core/util/event.proto\r\ntensorflow/core/grappler/costs/op_performance_data.proto\r\ntensorflow/core/profiler/tfprof_options.proto\r\ntensorflow/core/profiler/tfprof_output.proto\r\ntensorflow/core/profiler/profiler_analysis.proto\r\ntensorflow/core/profiler/tfprof_log.proto\r\ntensorflow/core/profiler/op_profile.proto\r\ntensorflow/core/profiler/profiler_service.proto\r\ntensorflow/core/profiler/profile.proto\r\ntensorflow/core/protobuf/trace_events.proto\r\ntensorflow/core/protobuf/conv_autotuning.proto\r\ntensorflow/core/protobuf/autotuning.proto\r\ntensorflow/core/protobuf/graph_debug_info.proto\r\ntensorflow/core/protobuf/eager_service.proto\r\ntensorflow/core/protobuf/replay_log.proto\r\ntensorflow/core/protobuf/transport_options.proto\r\ntensorflow/core/protobuf/tpu/compilation_result.proto\r\ntensorflow/core/protobuf/tpu/optimization_parameters.proto\r\ntensorflow/core/protobuf/tpu/tpu_embedding_output_layout.proto\r\ntensorflow/core/protobuf/tpu/topology.proto\r\ntensorflow/core/protobuf/tpu/tpu_embedding_configuration.proto\r\ntensorflow/core/protobuf/tpu/dynamic_padding.proto\r\ntensorflow/core/protobuf/data/experimental/snapshot.proto\r\ntensorflow/core/debug/debug_service.proto\r\ntensorflow/core/debug/debugger_event_metadata.proto\r\n```\r\n\r\nClosely related to https://github.com/tensorflow/tensorflow/issues/16282, https://github.com/tensorflow/tensorflow/issues/30093 and a few other issues/PRs that attempt to incrementally add `go_package` statements.", "comments": ["@aronatkins \r\n\r\nCan you please elaborate about the issue & the context. It helps us in localizing the issue faster. Thanks!", "@ravikyram - What specific additional information do you need?\r\n\r\nWe were trying to compile the protocol buffer definitions into Go source/packages using `protoc`. Unfortunately, not every `*.proto` file contains a `go_package` declaration. As a result, `protoc` code-generates Go files in two different directory hierarchies.\r\n\r\nI've enumerated the three `*.proto` files that I needed to hand-patch and also found all the `*.proto` files without `go_package` in case that helps.\r\n\r\nWe use the protocol buffer definitions to load saved models from disk and perform prediction requests. \r\n\r\nSee golang/protobuf#984, where the protobuf folks indicate that the missing `go_package` declarations are probably an oversight of the TF project.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34100\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34100\">No</a>\n"]}, {"number": 34099, "title": "Warning when using fit_generator", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit_generator\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#predict_generator\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nI have had pain debugging here so it would be better if the documentation mentions the fallacy here.\r\n\r\nMy experience is: I used ctypes C++ to generate training data into a fixed numpy buffer and the generator merely invoked C++ method and return the same buffer in each iteration (with different content obviously). This is problematic as the internal implementation of `fit_generator` will iterate in advance. The correct usage is to make a copy of the buffer each time iterator is iterated\r\n\r\nThe document should probably mention explicitly that the method does not make deep copy of the return values of the generator on its generation, so e.g. **it is probably not a good idea to share a same numpy buffer across different batches - if one want to yield from same buffer due to certain reasons, one good practice is to make a `nparray.copy()` before `yield`**.\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 34098, "title": "Seemingly unavailable datatypes for Ops.", "body": "In [`training_ops.cc:640`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L640) I see many registrations for Ops, which have datatypes only supported for non-Windows.\r\n\r\nFor example:\r\n```cpp\r\n#define REGISTER_KERNELS(D, T)                                                \\\r\n  REGISTER_KERNEL_BUILDER(                                                    \\\r\n      Name(\"ApplyGradientDescent\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"), \\\r\n      ApplyGradientDescentOp<D##Device, T>);                                  \\\r\n  REGISTER_KERNEL_BUILDER(Name(\"ResourceApplyGradientDescent\")                \\\r\n                              .Device(DEVICE_##D)                             \\\r\n                              .HostMemory(\"var\")                              \\\r\n                              .TypeConstraint<T>(\"T\"),                        \\\r\n                          ApplyGradientDescentOp<D##Device, T>);\r\n#define REGISTER_CPU_KERNELS(T) REGISTER_KERNELS(CPU, T);\r\n\r\nTF_CALL_half(REGISTER_CPU_KERNELS);\r\nTF_CALL_bfloat16(REGISTER_CPU_KERNELS);\r\nTF_CALL_float(REGISTER_CPU_KERNELS);\r\nTF_CALL_double(REGISTER_CPU_KERNELS);\r\n#ifndef PLATFORM_WINDOWS\r\nTF_CALL_complex64(REGISTER_CPU_KERNELS);\r\nTF_CALL_complex128(REGISTER_CPU_KERNELS);\r\n#endif\r\n```\r\nSeeing this, we quickly switched and ran our code on a Linux build, hoping the Op would be available for complex128, but it turns out also on Linux, it's not, as indicated by following error:\r\n```\r\nNotFoundError: No registered 'ResourceApplyGradientDescent' OpKernel for 'GPU' devices compatible with node {{node ResourceApplyGradientDescent}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_COMPLEX64, use_locking=true\r\n\t.  Registered:  device='XLA_GPU'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]\r\n [Op:ResourceApplyGradientDescent]\r\n```\r\nThis TensorFlow 2.0 version was installed with `pip install tensorflow-gpu`, minutes ago. Let's say I'm currently interested in having access to the `ApplyGradientDescent` with complex128 for CPU.\r\n\r\nCurrently compiling TensorFlow myself, to test this. However, the actual questions are:\r\n - Why are these not supported on Windows?\r\n - Why are these seemingly not available on a pip-installed TensorFlow version on Linux? Is this an issue with the build process of the pip-release of TensorFlow for Linux?\r\n - Any recommendations on how to proceed? (I'm currently compiling TensorFlow from source on Linux, hoping for the Op to be available).", "comments": ["@Davidvdrm: Unfortunately compiler support for many complex operations was suboptimal. We need to provide the necessary complex overrides in order to provide wider support. Did you try compiling with nvcc of the clang CUDA compiler? Clang should give you access to those kernels.", "I have not yet, but i will try ASAP. The issue is clear now. Thank you for this. ", "@Davidvdrm: Were you able to verify if the problem was resolved with the clang CUDA compiler?", "Not yet, a different issue came up and i will get to the compiler tommorow.\r\nI will post an update as soon as i have results\r\n\r\nOp ma 11 nov. 2019 13:54 schreef Gaurav Jain <notifications@github.com>:\r\n\r\n> @Davidvdrm <https://github.com/Davidvdrm>: Were you able to verify if the\r\n> problem was resolved with the clang CUDA compiler?\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/34098?email_source=notifications&email_token=AKPJSZ2WT22X7CCAFJHNX5LQTFIW7A5CNFSM4JKW5G32YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDWXDYY#issuecomment-552432099>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AKPJSZ2K6UPCJIQT3ZYRMR3QTFIW7ANCNFSM4JKW5G3Q>\r\n> .\r\n>\r\n", "@jaingaurav I compiled from source using the experimental download clang option in the configuration and this resulted in the same error. In order to test even further i will also try compiling CUDA with clang and then compiling tensorflow with this. I am quite new to linux and compiler (thanks physics education) so it might take me a while to get through everything. If you have any pointers or a short guide this would be very much appreciated.", "@Davidvdrm: The main option needed to be set is the \"Do you want to use clang as CUDA compiler?\" to be set when running `./configure`. See https://www.tensorflow.org/install/source#sample_session for an example session. Note in the example it is using nvcc instead of clang for the CUDA compiler, which is NOT what you want.", "I do not reveive this option on windows. I'll try on a linux boot, but as mentioned before getting cudnn and cuda to work on linux is slightly beyond me. The following terminal entry is from the windows bazel build configuration.\r\n\r\n```\r\nC:\\Users\\David\\tmp\\tensorflow>configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.29.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\David\\Anaconda3\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\David\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\David\\Anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\nC:\\Users\\David\\tmp\\tensorflow>\r\n```", "@Davidvdrm: As you can see from the code snippet you posted, these complex ops are not supported on windows due to compiler incompatibilities. I think we'll have better luck when we upgrade to MSVC 2019. For now these ops are available only on linux with clang as the CUDA compiler.", "I was able to get the compilation started on linux with clang, but am currently running into a last issue which i don't understand at all. From some research it seems to be a problem with the compiler not having a back-end for the host. \r\n```\r\n1 warning generated.\r\nERROR: /home/david/tmp/tensorflow/tensorflow/core/kernels/BUILD:1976:1: C++ compilation of rule '//tensorflow/core/kernels:gather_functor_gpu' failed (Exit 1)\r\nerror: unable to create target: 'No available targets are compatible with triple \"nvptx64-nvidia-cuda\"'\r\n1 error generated when compiling for sm_35.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/david/tmp/tensorflow/tensorflow/python/tools/BUILD:98:1 C++ compilation of rule '//tensorflow/core/kernels:gather_functor_gpu' failed (Exit 1)\r\nINFO: Elapsed time: 455.025s, Critical Path: 68.46s\r\nINFO: 1675 processes: 1675 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "An extension on the previously posted error\r\n\r\nThe full error:\r\n```\r\nERROR: /home/david/.cache/bazel/_bazel_david/34d22352ac396fe0091b65c723a68a75/external/nccl_archive/BUILD.bazel:53:1: C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1)\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:431:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bcopy (const void *__src, void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:38:13: note: previous declaration is here\r\nextern void bcopy (const void *__src, void *__dest, size_t __n)\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:431:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:42:13: note: previous declaration is here\r\nextern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:42:14: note: previous declaration is here\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memmove (void *__dest, const void *__src, size_t __len))\r\n       ^\r\n/usr/include/string.h:46:14: note: previous declaration is here\r\nextern void *memmove (void *__dest, const void *__src, size_t __n)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:377:14: note: previous declaration is here\r\nextern void *mempcpy (void *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memset (void *__dest, int __ch, size_t __len))\r\n       ^\r\n/usr/include/string.h:60:14: note: previous declaration is here\r\nextern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (explicit_bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/string.h:435:13: note: previous declaration is here\r\nextern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:121:14: note: previous declaration is here\r\nextern char *strcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:451:14: note: previous declaration is here\r\nextern char *stpcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:124:14: note: previous declaration is here\r\nextern char *strncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:116:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpncpy (char *__dest, const char *__src, size_t __n))\r\n       ^\r\n/usr/include/string.h:459:14: note: previous declaration is here\r\nextern char *stpncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:126:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcat (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:129:14: note: previous declaration is here\r\nextern char *strcat (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from external/local_config_cuda/crosstool/extra_tools/lib/clang/10.0.0/include/__clang_cuda_runtime_wrapper.h:202:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:133:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncat (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:132:14: note: previous declaration is here\r\nextern char *strncat (char *__restrict __dest, const char *__restrict __src,\r\n             ^\r\nIn file included from bazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_reduce_scatter.cu.cc:10:\r\nIn file included from bazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/reduce_scatter.h:8:\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:243:7: warning: field 'comm' will be initialized after field 'tid' [-Wreorder-ctor]\r\n    : comm(comm), tid(tid), nthreads(nthreads), stepSize(stepSize), opCount(opCount) {\r\n      ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/reduce_scatter.h:30:5: note: in instantiation of member function 'ncclPrimitives<4, 2, 2, double, 1, 1, FuncMax<double> >::ncclPrimitives' requested here\r\n    prims(tid, nthreads, &ring->prev, &ring->next, NULL, stepSize, channel, comm, args->opCount);\r\n    ^\r\nbazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_reduce_scatter.cu.cc:14:1: note: in instantiation of function template specialization 'ncclReduceScatterRingKernel<4, FuncMax<double>, double>' requested here\r\nIMPL_COLL_R(ncclReduceScatter, ncclCollReduceScatter);\r\n^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:145:3: note: expanded from macro 'IMPL_COLL_R'\r\n  IMPL_COLL2(collf, max,  FuncMax,  colln, ncclMax);\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:130:3: note: expanded from macro 'IMPL_COLL2'\r\n  IMPL_COLL3(coll, op, ncclFunc, f64, double,   ncclColl, ncclOp, ncclFloat64)\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:101:3: note: expanded from macro 'IMPL_COLL3'\r\n  IMPL_COLL4(coll##Ring, op, ncclFunc, dtype, ctype, ncclColl, ncclOp, ncclType, 0) \\\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:96:3: note: expanded from macro 'IMPL_COLL4'\r\n  IMPL_COLL_FUNC(coll, op, ncclFunc, dtype, ctype) \\\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:48:3: note: expanded from macro 'IMPL_COLL_FUNC'\r\n  coll##Kernel<COLL_UNROLL, ncclFunc<ctype>, ctype>(args); \\\r\n  ^\r\n<scratch space>:121:1: note: expanded from here\r\nncclReduceScatterRingKernel\r\n^\r\nIn file included from bazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_reduce_scatter.cu.cc:10:\r\nIn file included from bazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/reduce_scatter.h:8:\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:143:16: warning: array index 1 is past the end of the array (which contains 1 element) [-Warray-bounds]\r\n      if (SRC) srcs[1] = recvPtr(0);\r\n               ^    ~\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:253:5: note: in instantiation of function template specialization 'ncclPrimitives<4, 2, 2, double, 1, 1, FuncMax<double> >::GenericOp<0, 0, 0, 1, 1, 0>' requested here\r\n    GenericOp<0, 0, 0, 1, 1, 0>(src, NULL, nelem, 0);\r\n    ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/reduce_scatter.h:46:11: note: in instantiation of member function 'ncclPrimitives<4, 2, 2, double, 1, 1, FuncMax<double> >::send' requested here\r\n    prims.send(thisInput+offset, nelem);\r\n          ^\r\nbazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_reduce_scatter.cu.cc:14:1: note: in instantiation of function template specialization 'ncclReduceScatterRingKernel<4, FuncMax<double>, double>' requested here\r\nIMPL_COLL_R(ncclReduceScatter, ncclCollReduceScatter);\r\n^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:145:3: note: expanded from macro 'IMPL_COLL_R'\r\n  IMPL_COLL2(collf, max,  FuncMax,  colln, ncclMax);\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:130:3: note: expanded from macro 'IMPL_COLL2'\r\n  IMPL_COLL3(coll, op, ncclFunc, f64, double,   ncclColl, ncclOp, ncclFloat64)\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:101:3: note: expanded from macro 'IMPL_COLL3'\r\n  IMPL_COLL4(coll##Ring, op, ncclFunc, dtype, ctype, ncclColl, ncclOp, ncclType, 0) \\\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:96:3: note: expanded from macro 'IMPL_COLL4'\r\n  IMPL_COLL_FUNC(coll, op, ncclFunc, dtype, ctype) \\\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:48:3: note: expanded from macro 'IMPL_COLL_FUNC'\r\n  coll##Kernel<COLL_UNROLL, ncclFunc<ctype>, ctype>(args); \\\r\n  ^\r\n<scratch space>:121:1: note: expanded from here\r\nncclReduceScatterRingKernel\r\n^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:140:5: note: array 'srcs' declared here\r\n    const T* srcs[RECV*NRECV+SRC];\r\n    ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:150:16: warning: array index 1 is past the end of the array (which contains 1 element) [-Warray-bounds]\r\n      if (DST) dsts[1] = directSendPtr<DIRECTSEND>(0, directOffset);\r\n               ^    ~\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:147:5: note: array 'dsts' declared here\r\n    T* dsts[SEND*NSEND+DST];\r\n    ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:150:16: warning: array index 1 is past the end of the array (which contains 1 element) [-Warray-bounds]\r\n      if (DST) dsts[1] = directSendPtr<DIRECTSEND>(0, directOffset);\r\n               ^    ~\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:294:5: note: in instantiation of function template specialization 'ncclPrimitives<4, 2, 2, double, 1, 1, FuncMax<double> >::GenericOp<0, 0, 1, 1, 1, 0>' requested here\r\n    GenericOp<0, 0, 1, 1, 1, 0>(src, NULL, nelem, 0);\r\n    ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/reduce_scatter.h:53:13: note: in instantiation of member function 'ncclPrimitives<4, 2, 2, double, 1, 1, FuncMax<double> >::recvReduceSend' requested here\r\n      prims.recvReduceSend(thisInput+offset, nelem);\r\n            ^\r\nbazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_reduce_scatter.cu.cc:14:1: note: in instantiation of function template specialization 'ncclReduceScatterRingKernel<4, FuncMax<double>, double>' requested here\r\nIMPL_COLL_R(ncclReduceScatter, ncclCollReduceScatter);\r\n^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:145:3: note: expanded from macro 'IMPL_COLL_R'\r\n  IMPL_COLL2(collf, max,  FuncMax,  colln, ncclMax);\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:130:3: note: expanded from macro 'IMPL_COLL2'\r\n  IMPL_COLL3(coll, op, ncclFunc, f64, double,   ncclColl, ncclOp, ncclFloat64)\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:101:3: note: expanded from macro 'IMPL_COLL3'\r\n  IMPL_COLL4(coll##Ring, op, ncclFunc, dtype, ctype, ncclColl, ncclOp, ncclType, 0) \\\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:96:3: note: expanded from macro 'IMPL_COLL4'\r\n  IMPL_COLL_FUNC(coll, op, ncclFunc, dtype, ctype) \\\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:48:3: note: expanded from macro 'IMPL_COLL_FUNC'\r\n  coll##Kernel<COLL_UNROLL, ncclFunc<ctype>, ctype>(args); \\\r\n  ^\r\n<scratch space>:121:1: note: expanded from here\r\nncclReduceScatterRingKernel\r\n^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:147:5: note: array 'dsts' declared here\r\n    T* dsts[SEND*NSEND+DST];\r\n    ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:150:16: warning: array index 1 is past the end of the array (which contains 1 element) [-Warray-bounds]\r\n      if (DST) dsts[1] = directSendPtr<DIRECTSEND>(0, directOffset);\r\n               ^    ~\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:289:5: note: in instantiation of function template specialization 'ncclPrimitives<4, 2, 2, double, 1, 1, FuncMax<double> >::GenericOp<0, 0, 1, 0, 1, 1>' requested here\r\n    GenericOp<0, 0, 1, 0, 1, 1>(src, dst, nelem, 0);\r\n    ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/reduce_scatter.h:60:11: note: in instantiation of member function 'ncclPrimitives<4, 2, 2, double, 1, 1, FuncMax<double> >::recvReduceCopy' requested here\r\n    prims.recvReduceCopy(thisInput+offset, thisOutput+chunkOffset, nelem);\r\n          ^\r\nbazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_reduce_scatter.cu.cc:14:1: note: in instantiation of function template specialization 'ncclReduceScatterRingKernel<4, FuncMax<double>, double>' requested here\r\nIMPL_COLL_R(ncclReduceScatter, ncclCollReduceScatter);\r\n^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:145:3: note: expanded from macro 'IMPL_COLL_R'\r\n  IMPL_COLL2(collf, max,  FuncMax,  colln, ncclMax);\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:130:3: note: expanded from macro 'IMPL_COLL2'\r\n  IMPL_COLL3(coll, op, ncclFunc, f64, double,   ncclColl, ncclOp, ncclFloat64)\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:101:3: note: expanded from macro 'IMPL_COLL3'\r\n  IMPL_COLL4(coll##Ring, op, ncclFunc, dtype, ctype, ncclColl, ncclOp, ncclType, 0) \\\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:96:3: note: expanded from macro 'IMPL_COLL4'\r\n  IMPL_COLL_FUNC(coll, op, ncclFunc, dtype, ctype) \\\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:48:3: note: expanded from macro 'IMPL_COLL_FUNC'\r\n  coll##Kernel<COLL_UNROLL, ncclFunc<ctype>, ctype>(args); \\\r\n  ^\r\n<scratch space>:121:1: note: expanded from here\r\nncclReduceScatterRingKernel\r\n^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:147:5: note: array 'dsts' declared here\r\n    T* dsts[SEND*NSEND+DST];\r\n    ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/primitives.h:529:7: warning: field 'comm' will be initialized after field 'tid' [-Wreorder-ctor]\r\n    : comm(comm), tid(tid), nthreads(nthreads), opCount(opCount) {\r\n      ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/reduce_scatter.h:76:35: note: in instantiation of member function 'ncclLLPrimitives<double, FuncMax<double>, 1, 1>::ncclLLPrimitives' requested here\r\n  ncclLLPrimitives<T, FUNC, 1, 1> LLprims(tid, nthreads, &ring->prev, &ring->next, channel, comm, args->opCount);\r\n                                  ^\r\nbazel-out/host/bin/external/nccl_archive/src/collectives/device/max_f64_reduce_scatter.cu.cc:14:1: note: in instantiation of function template specialization 'ncclReduceScatterRingLLKernel<4, FuncMax<double>, double>' requested here\r\nIMPL_COLL_R(ncclReduceScatter, ncclCollReduceScatter);\r\n^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:145:3: note: expanded from macro 'IMPL_COLL_R'\r\n  IMPL_COLL2(collf, max,  FuncMax,  colln, ncclMax);\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:130:3: note: expanded from macro 'IMPL_COLL2'\r\n  IMPL_COLL3(coll, op, ncclFunc, f64, double,   ncclColl, ncclOp, ncclFloat64)\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:101:3: note: expanded from macro 'IMPL_COLL3'\r\n  IMPL_COLL4(coll##Ring, op, ncclFunc, dtype, ctype, ncclColl, ncclOp, ncclType, 0) \\\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:97:3: note: expanded from macro 'IMPL_COLL4'\r\n  IMPL_COLL_FUNC(coll##LL, op, ncclFunc, dtype, ctype) \\\r\n  ^\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/device_hdrs/common.h:48:3: note: expanded from macro 'IMPL_COLL_FUNC'\r\n  coll##Kernel<COLL_UNROLL, ncclFunc<ctype>, ctype>(args); \\\r\n  ^\r\n<scratch space>:127:1: note: expanded from here\r\nncclReduceScatterRingLLKernel\r\n^\r\n6 warnings and 13 errors generated when compiling for sm_35.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/david/tmp/tensorflow/tensorflow/tools/pip_package/BUILD:223:1 C++ compilation of rule '@nccl_archive//:device_lib' failed (Exit 1)\r\nINFO: Elapsed time: 523.828s, Critical Path: 170.66s\r\nINFO: 2425 processes: 2425 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n"]}, {"number": 34097, "title": "python/keras/engine/network.py:layers took great deal of prediction time on deep networks when using methods like predict_on_batch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary, docker\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I am using keras and profiling with cProfile, I found that seemly innocent `@property`s of `network.py` become performance bottleneck.\r\n\r\n```\r\n  @property\r\n  def dynamic(self):\r\n    if self._is_graph_network:\r\n      return any(layer.dynamic for layer in self.layers)\r\n    return self._dynamic or any(layer.dynamic for layer in self.layers)\r\n  @property\r\n  def layers(self):\r\n    return trackable_layer_utils.filter_empty_layer_containers(\r\n        self._layers)\r\n```\r\n\r\nThe functions are called fairly frequently without caching the results, and as a result I see them along with related functions (`filter_empty_layer_containers` for example) took inproportional amount of cpu cycles when gpu is hungry. It does not seem right to call the functions again and again when their values are fixed, and they are using 2/3 of the training time. (When will anyone train a network whose dynamicness and number of layers are constantly fluctuating?)\r\n\r\nAnyone may try with standard deep resnet with predict_generator/fit_generator and find out whether they are most time consuming.\r\n", "comments": ["Please provide minimal reproducible code snippet to validate the reported issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34096, "title": "Cannot load saved model fitted via csv dataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI load data from a csv file to create the dataset for fitting my model. After saving the model, I can't load it again. I get the following error message:\r\n`ValueError: ('We expected a dictionary here. Instead we got: ', <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float32>)`\r\n\r\n**Describe the expected behavior**\r\nThe program should load the model.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ndataset: tf.data.Dataset = tf.data.experimental.make_csv_dataset('xor.csv', 4, label_name='result')\r\ncolumns = [tf.feature_column.numeric_column('a'), tf.feature_column.numeric_column('b')]\r\ninput_column = keras.layers.DenseFeatures(columns)\r\nlayers = [input_column,\r\n          keras.layers.Dense(4, activation='relu'),\r\n          keras.layers.Dense(2)]\r\nmodel = keras.Sequential(layers)\r\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\r\nmodel.fit(dataset, steps_per_epoch=4, epochs=2000)\r\nkeras.models.save_model(model, 'test_model.h5')\r\n# model.save('test_model.h5')\r\nmodel = keras.models.load_model('test_model.h5')\r\n```\r\nxor.csv:\r\n```\r\na,b,result\r\n0,0,0\r\n0,1,1\r\n1,0,1\r\n1,1,0\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n...\r\n  File \"main.py\", line 76, in main\r\n    model = keras.models.load_model('test_model.h5')\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 146, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 168, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/keras/layers/serialization.py\", line 106, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 303, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 380, in from_config\r\n    model.build(build_input_shape)\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 260, in build\r\n    super(Sequential, self).build(input_shape)\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\", line 682, in build\r\n    self.call(x, **kwargs)\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 281, in call\r\n    outputs = layer(inputs, **kwargs)\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 778, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/.../lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 237, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n    /.../lib/python3.7/site-packages/tensorflow_core/python/feature_column/dense_features.py:129 call\r\n        features)\r\n    ValueError: ('We expected a dictionary here. Instead we got: ', <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float32>)\r\n```\r\n", "comments": ["In TF 2.X save format is TensorFlow Model format (tf) by default.\r\nCan you please try using setting ```save_format='h5' ``` argument in [```tf.keras.models.save_model```](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model?hl=vi#arguments) and try again?", "> In TF 2.X save format is TensorFlow Model format (tf) by default.\r\n> Can you please try using setting `save_format='h5' ` argument in [`tf.keras.models.save_model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model?hl=vi#arguments) and try again?\r\n\r\nI tried it and the same error was thrown again.", "Is there a workaround for this bug?", "\r\n> Is there a workaround for this bug?\r\n\r\n\r\nHello **@EdwarDDay**,  if you are using **TF2.x**, it will work.\r\n\r\n**save with:** \r\ntf.keras.models.save_model(model, \"path/model\") # without extension .h5, only string of your model\r\n\r\n**and load with:**\r\nnew_model = tf.keras.models.load_model(\"path/model\") # as you have saved in first time, without extension again.\r\n\r\nlike this ...\r\n![model-keras](https://user-images.githubusercontent.com/29865600/70857116-49a53b00-1ee9-11ea-9021-bc5b264bf062.png)\r\n![directory](https://user-images.githubusercontent.com/29865600/70857131-9721a800-1ee9-11ea-8a15-b714d4ba1f66.png)\r\n", "> > Is there a workaround for this bug?\r\n> \r\n> Hello **@EdwarDDay**, if you are using **TF2.x**, it will work.\r\n> \r\n> **save with:**\r\n> tf.keras.models.save_model(model, \"path/model\") # without extension .h5, only string of your model\r\n> \r\n> **and load with:**\r\n> new_model = tf.keras.models.load_model(\"path/model\") # as you have saved in first time, without extension again.\r\n> \r\n> like this ...\r\n> ![model-keras](https://user-images.githubusercontent.com/29865600/70857116-49a53b00-1ee9-11ea-9021-bc5b264bf062.png)\r\n> ![directory](https://user-images.githubusercontent.com/29865600/70857131-9721a800-1ee9-11ea-8a15-b714d4ba1f66.png)\r\n\r\nThis solution does not work for my situation because for my project I need to save a single h5 file and not a whole directory, is there another way to fix this issue?", "Tried https://www.tensorflow.org/tutorials/load_data/pandas_dataframe example with saving/loading model in tf/h5 format and works correctly. Please test with latest version of TF and try again. Others who are experiencing similar issue please raise a new issue thread. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34096\">No</a>\n"]}, {"number": 34095, "title": "An op called \"int\" appears in Tensorflow", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nThe dependency \"@com_google_ortools//ortools/constraint_solver:cp\" was introduced in the tensorflow/compiler/xla/service/cpu:cpu_compiler deps\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: python 3.6.8\r\n- Bazel version (if compiling from source): 0.25.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version: non\r\n- GPU model and memory: non\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to re-develop xla. When I introduce \"@com_google_ortools//ortools/constraint_solver:cp\" dependency in the tensorflow/compiler/xla/service/cpu:cpu_compiler deps, the source code can be passed and passed. Pip is installed normally, but once the tensorflow interface is called, an op registration error named \"int\" is thrown.\r\nThis is the change of tensorflow/compiler/xla/service/cpu:cpu_compiler\r\n```\r\ncc_library(\r\n    name = \"cpu_compiler\",\r\n    srcs = [\"cpu_compiler.cc\"],\r\n    hdrs = [\"cpu_compiler.h\"],\r\n    deps = [\r\n        \":compiler_functor\",\r\n        \":buffer_info_util\",\r\n        \":conv_canonicalization\",\r\n        \":cpu_executable\",\r\n        \":cpu_hlo_support_checker\",\r\n        \":cpu_instruction_fusion\",\r\n        \":cpu_layout_assignment\",\r\n        \":cpu_options\",\r\n        \":disassembler\",\r\n        \":dot_op_emitter\",\r\n        \":ir_emission_utils\",\r\n        \":ir_emitter\",\r\n        \":parallel_task_assignment\",\r\n        \":simple_orc_jit\",\r\n        \"@com_google_absl//absl/memory\",\r\n        \"@com_google_absl//absl/strings\",\r\n        \":target_machine_features\",\r\n        \"@com_google_absl//absl/types:span\",\r\n        \"//tensorflow/compiler/xla/service:copy_insertion\",\r\n        \"//tensorflow/compiler/xla/service:hlo_casting_utils\",\r\n        \"//tensorflow/compiler/xla/service:dump\",\r\n        \"//tensorflow/compiler/xla/service:map_inliner\",\r\n        \"//tensorflow/compiler/xla/service:hlo_get_dimension_size_rewriter\",\r\n        \"//tensorflow/compiler/xla/service:conditional_to_select\",\r\n        \"//tensorflow/compiler/xla/service:scatter_expander\",\r\n        \"//tensorflow/compiler/xla/service:slice_sinker\",\r\n        \"//tensorflow/compiler/xla:cpu_function_runtime\",\r\n        \"//tensorflow/compiler/xla:literal\",\r\n        \"//tensorflow/compiler/xla:protobuf_util\",\r\n        \"//tensorflow/compiler/xla:status_macros\",\r\n        \"//tensorflow/compiler/xla:statusor\",\r\n        \"//tensorflow/compiler/xla:types\",\r\n        \"//tensorflow/compiler/xla:util\",\r\n        \"//tensorflow/compiler/xla:xla_data_proto\",\r\n        \"//tensorflow/compiler/xla/service:algebraic_simplifier\",\r\n        \"//tensorflow/compiler/xla/service:batch_dot_simplification\",\r\n        \"//tensorflow/compiler/xla/service:batchnorm_expander\",\r\n        \"//tensorflow/compiler/xla/service:buffer_assignment\",\r\n        \"//tensorflow/compiler/xla/service:buffer_liveness\",\r\n        \"//tensorflow/compiler/xla/service:call_inliner\",\r\n        \"//tensorflow/compiler/xla/service:cholesky_expander\",\r\n        \"//tensorflow/compiler/xla/service:conditional_simplifier\",\r\n        \"//tensorflow/compiler/xla/service:convolution_group_converter\",\r\n        \"//tensorflow/compiler/xla/service:dot_decomposer\",\r\n        \"//tensorflow/compiler/xla/service:dynamic_index_splitter\",\r\n        \"//tensorflow/compiler/xla/service:executable\",\r\n        \"//tensorflow/compiler/xla/service:flatten_call_graph\",\r\n        \"//tensorflow/compiler/xla/service:hlo\",\r\n        \"//tensorflow/compiler/xla/service:hlo_constant_folding\",\r\n        \"//tensorflow/compiler/xla/service:hlo_cse\",\r\n        \"//tensorflow/compiler/xla/service:hlo_dce\",\r\n        \"//tensorflow/compiler/xla/service:hlo_element_type_converter\",\r\n        \"//tensorflow/compiler/xla/service:hlo_ordering\",\r\n        \"//tensorflow/compiler/xla/service:hlo_pass\",\r\n        \"//tensorflow/compiler/xla/service:hlo_pass_pipeline\",\r\n        \"//tensorflow/compiler/xla/service:hlo_proto\",\r\n        \"//tensorflow/compiler/xla/service:hlo_proto_util\",\r\n        \"//tensorflow/compiler/xla/service:hlo_memory_scheduler\",\r\n        \"//tensorflow/compiler/xla/service:hlo_subcomputation_unification\",\r\n        \"//tensorflow/compiler/xla/service:hlo_verifier\",\r\n        \"//tensorflow/compiler/xla/service:indexed_array_analysis\",\r\n        \"//tensorflow/compiler/xla/service:llvm_compiler\",\r\n        \"//tensorflow/compiler/xla/service:reduce_precision_insertion\",\r\n        \"//tensorflow/compiler/xla/service:reshape_mover\",\r\n        \"//tensorflow/compiler/xla/service:rng_expander\",\r\n        \"//tensorflow/compiler/xla/service:sort_simplifier\",\r\n        \"//tensorflow/compiler/xla/service:transpose_folding\",\r\n        \"//tensorflow/compiler/xla/service:triangular_solve_expander\",\r\n        \"//tensorflow/compiler/xla/service:tuple_simplifier\",\r\n        \"//tensorflow/compiler/xla/service:while_loop_constant_sinking\",\r\n        \"//tensorflow/compiler/xla/service:while_loop_invariant_code_motion\",\r\n        \"//tensorflow/compiler/xla/service:while_loop_simplifier\",\r\n        \"//tensorflow/compiler/xla/service:zero_sized_hlo_elimination\",\r\n        \"//tensorflow/compiler/xla/service/llvm_ir:llvm_util\",  # fixdeps: keep\r\n        \"//tensorflow/core:lib\",  # fixdeps: keep\r\n        \"//tensorflow/core:stream_executor_no_cuda\",\r\n        \"@llvm//:aarch64_code_gen\",  # fixdeps: keep\r\n        \"@llvm//:aarch64_disassembler\",  # fixdeps: keep\r\n        \"@llvm//:arm_code_gen\",  # fixdeps: keep\r\n        \"@llvm//:arm_disassembler\",  # fixdeps: keep\r\n        \"@llvm//:core\",\r\n        \"@llvm//:mc\",  # fixdeps: keep\r\n        \"@llvm//:object\",\r\n        \"@llvm//:support\",\r\n        \"@llvm//:target\",  # fixdeps: keep\r\n        \"@llvm//:x86_code_gen\",  # fixdeps: keep\r\n        \"@llvm//:x86_disassembler\",  # fixdeps: keep\r\n        \"@com_google_ortools//ortools/constraint_solver:cp\",\r\n    ] + select({\r\n        \"//tensorflow:linux_ppc64le\": [\r\n            \"@llvm//:powerpc_disassembler\",\r\n            \"@llvm//:powerpc_code_gen\",\r\n        ],\r\n        \"//conditions:default\": [\r\n        ],\r\n    }),\r\n    alwayslink = True,  # Contains compiler registration\r\n)\r\n```\r\n\r\nThis is where the introduction of ortools depends on third_party/ortools/workspace.bzl.\r\n\r\n```\r\nload(\"//third_party:repo.bzl\", \"third_party_http_archive\")\r\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\r\nload(\"@bazel_tools//tools/build_defs/repo:git.bzl\", \"git_repository\", \"new_git_repository\")\r\n\r\ndef repo():\r\n    git_repository(\r\n        name = \"com_google_protobuf_cc\",\r\n        commit = \"0974557\",  # release v3.8.0\r\n        remote = \"https://github.com/protocolbuffers/protobuf.git\",\r\n    )\r\n\r\n    http_archive(\r\n        name = \"com_google_ortools\",\r\n        urls = [\r\n            \"http://mirror.tensorflow.org/github.com/google/or-tools/archive/v7.2.tar.gz\",\r\n            \"https://github.com/google/or-tools/archive/v7.2.tar.gz\",\r\n        ],\r\n        sha256 = \"13a4de5dba1f64e2e490394f8f63fe0a301ee55466ef65fe309ffd5100358ea8\",\r\n        strip_prefix = \"or-tools-7.2\",\r\n        build_file = \"//third_party/ortools:BUILD.bazel\",\r\n        patch_cmds = [\r\n            \"find  -name 'BUILD' -print0 | xargs -0 sed -i 's/com_google_protobuf_cc/com_google_protobuf/g'\",\r\n        ],\r\n    )\r\n\r\n    http_archive(\r\n        name = \"com_github_glog_glog\",\r\n        urls = [\r\n            \"http://mirror.tensorflow.org/github.com/google/glog/archive/v0.4.0.tar.gz\",\r\n            \"https://github.com/google/glog/archive/v0.4.0.tar.gz\",\r\n        ],\r\n        sha256 = \"f28359aeba12f30d73d9e4711ef356dc842886968112162bc73002645139c39c\",\r\n        strip_prefix = \"glog-0.4.0\",\r\n        #        build_file = \"//com_github_glog_glog:BUILD\",\r\n        patch_cmds = [\r\n            \"mkdir glog_internal && mkdir build && cd build && cmake .. && cp config.h ../glog_internal && cp config.h ../src\",\r\n            \"sed -i 's/:config_h/src\\/config.h/g' bazel/glog.bzl\",\r\n        ],\r\n    )\r\n\r\n```\r\n\r\n**This is the running environment and error information after installing the Tensorflow source code.\r\n```\r\nPython 3.6.8 (default, Oct  7 2019, 12:59:55) \r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/ubuntu/.virtualenvs/run/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n>>> tf.Graph\r\n<class 'tensorflow.python.framework.ops.Graph'>\r\n>>> tf.Graph()\r\n2019-11-08 09:13:06.526896: F tensorflow/core/framework/op.cc:200] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: Invalid name: int (Did you use CamelCase?); in OpDef: name: \"int\" input_arg { name: \"int\" description: \"int\" type: DT_FLOAT type_attr: \"int\" number_attr: \"int\" type_list_attr: \"int\" } input_arg { name: \"int\" description: \"int\" type: DT_FLOAT type_attr: \"int\" number_attr: \"int\" type_list_attr: \"int\" } attr { name: \"int\" type: \"int\" default_value { i: -1 } description: \"int\" has_minimum: true minimum: -1 } attr { name: \"int\" type: \"int\" default_value { s: \"\" } description: \"int\" } attr { name: \"int\" type: \"int\" description: \"int\" } attr { name: \"int\" type: \"int\" description: \"int\" } summary: \"int\" description: \"int\" is_stateful: true\r\nAborted (core dumped)\r\n```\r\n", "comments": ["This probably means something in the TF binary you built is creating an op names `\"int\"`.  The error is thrown from `ValidateOpDef` in op_def_util.cc, can you please get a stack trace from that method when it decides to return the error?  You can use `CurrentStackTrace` to get the current stack trace.", "> This probably means something in the TF binary you built is creating an op names `\"int\"`. The error is thrown from `ValidateOpDef` in op_def_util.cc, can you please get a stack trace from that method when it decides to return the error? You can use `CurrentStackTrace` to get the current stack trace.\r\n\r\nI printed the call stack. There is no op named \"int\" during compilation. This error occurs during tensorflow. Once the ortools dependency is removed, tensorflow works fine. I rely on ortools in my project. I don't know. How should this error be fixed?\r\n```\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  0x00007ffff7a24801 in __GI_abort () at abort.c:79\r\n#2  0x00007fffbc113d27 in tensorflow::internal::LogMessageFatal::~LogMessageFatal() ()\r\n   from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fffb4c1705c in tensorflow::OpRegistry::MustCallDeferred() const [clone .part.179] ()\r\n   from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#4  0x00007fffb4c175e1 in tensorflow::OpRegistry::LookUpSlow(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::OpRegistrationData const**) const () from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#5  0x00007fffb4c17be9 in tensorflow::OpRegistry::LookUp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::OpRegistrationData const**) const () from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#6  0x00007fffb4bcbf5d in tensorflow::FunctionLibraryDefinition::LookUp(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::OpRegistrationData const**) const () from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#7  0x00007fffb4c11c50 in tensorflow::OpRegistryInterface::LookUpOpDef(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::OpDef const**) const () from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#8  0x00007fffb4c91d7c in tensorflow::Graph::AddNode(tensorflow::NodeDef, tensorflow::Status*) ()\r\n   from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#9  0x00007fffb4c94477 in tensorflow::Graph::Graph(tensorflow::OpRegistryInterface const*) ()\r\n   from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#10 0x00007fffb7a0a233 in TF_Graph::TF_Graph() () from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007fffb7a0a35e in TF_NewGraph () from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007fffb77ae6e9 in _wrap_TF_NewGraph () from /home/chen/.virtualenvs/run-tf/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#13 0x000000000050a84f in ?? ()\r\n#14 0x000000000050c549 in _PyEval_EvalFrameDefault ()\r\n#15 0x00000000005093e5 in _PyFunction_FastCallDict ()\r\n#16 0x00000000005951c1 in ?? ()\r\n#17 0x000000000054a11f in ?? ()\r\n#18 0x0000000000551761 in ?? ()\r\n#19 0x00000000005aa69c in _PyObject_FastCallKeywords ()\r\n#20 0x000000000050ab53 in ?? ()\r\n#21 0x000000000050c549 in _PyEval_EvalFrameDefault ()\r\n#22 0x00000000005093e5 in _PyFunction_FastCallDict ()\r\n#23 0x00000000005951c1 in ?? ()\r\n#24 0x000000000054a11f in ?? ()\r\n#25 0x0000000000551761 in ?? ()\r\n#26 0x00000000005aa69c in _PyObject_FastCallKeywords ()\r\n#27 0x000000000050ab53 in ?? ()\r\n#28 0x000000000050c549 in _PyEval_EvalFrameDefault ()\r\n#29 0x0000000000509ce8 in ?? ()\r\n#30 0x000000000050aa1d in ?? ()\r\n#31 0x000000000050c549 in _PyEval_EvalFrameDefault ()\r\n#32 0x0000000000509ce8 in ?? ()\r\n#33 0x000000000050aa1d in ?? ()\r\n#34 0x000000000050c549 in _PyEval_EvalFrameDefault ()\r\n#35 0x0000000000509ce8 in ?? ()\r\n#36 0x000000000050aa1d in ?? ()\r\n#37 0x000000000050c549 in _PyEval_EvalFrameDefault ()\r\n#38 0x00000000005081d5 in ?? ()\r\n#39 0x000000000050a020 in ?? ()\r\n#40 0x000000000050aa1d in ?? ()\r\n#41 0x000000000050d320 in _PyEval_EvalFrameDefault ()\r\n#42 0x00000000005081d5 in ?? ()\r\n#43 0x000000000050a020 in ?? ()\r\n\r\n```", "I'm not too familiar with this area of the codebase, but it looks like we're crashing when we try to register the ops in `OpRegistry::deferred_`.  Maybe add logs to the places where we push back into `deferred_` and see if that provides a clue?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34095\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34095\">No</a>\n"]}, {"number": 34094, "title": "When i try run the  `tf.keras.layers.Bidirectional` on my windows system, it turns out CancelledError!  ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n    * no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n    * windows 10 1903\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n    * source \r\n- TensorFlow version (use command below):\r\n    * tensorflow-gpu 2.0\r\n- Python version:\r\n    * Python 3.6.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n    * cuda10\r\n- GPU model and memory:\r\n    * gtx1060 6G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nthis is my model, from the [tutorials](https://tensorflow.google.cn/tutorials/text/text_classification_rnn?hl=en):\r\n```python\r\nembedding_dim=16\r\n\r\nmodel = keras.Sequential([\r\n    layers.Embedding(encoder.vocab_size, embedding_dim),\r\n    layers.Bidirectional(tf.keras.layers.LSTM(32)),\r\n    layers.Dense(1, activation='sigmoid')\r\n])\r\n```\r\n\r\nbut it train error on my computer:\r\n```python\r\n---------------------------------------------------------------------------\r\nCancelledError                            Traceback (most recent call last)\r\n<ipython-input-11-35c641747dc0> in <module>\r\n      6     train_batches,\r\n      7     epochs=10,\r\n----> 8     validation_data=test_batches, validation_steps=20,verbose=2)\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    485       # In this case we have created variables on the first call, so we run the\r\n    486       # defunned version which is guaranteed to never create variables.\r\n--> 487       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    488     elif self._stateful_fn is not None:\r\n    489       # Release the lock early so that multiple threads can perform the call\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n   1825   @property\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nCancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node Adam/Adam/update/AssignSubVariableOp/_41}}]]\r\n\t [[Reshape_11/_38]] [Op:__inference_distributed_function_15971]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n\r\n```\r\n**Describe the expected behavior**\r\nIt should train fluently, and i try on Google cloab, it try fluently, but it train error on my computer. \r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nthe code\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nimport tensorflow_datasets as tfds\r\n# tfds.disable_progress_bar()\r\n(train_data, test_data), info = tfds.load(\r\n    'imdb_reviews/subwords8k', \r\n    split = (tfds.Split.TRAIN, tfds.Split.TEST), \r\n    with_info=True, as_supervised=True)\r\nencoder = info.features['text'].encoder\r\npadded_shapes = ([None],())\r\ntrain_batches = train_data.shuffle(1000).padded_batch(10, padded_shapes = padded_shapes)\r\ntest_batches = test_data.shuffle(1000).padded_batch(10, padded_shapes = padded_shapes)\r\n\r\nembedding_dim=16\r\n\r\nmodel = keras.Sequential([\r\n    layers.Embedding(encoder.vocab_size, embedding_dim,mask_zero=True),\r\n    layers.Bidirectional(tf.keras.layers.LSTM(32)),\r\n    layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(\r\n    train_batches,\r\n    epochs=10,\r\n    validation_data=test_batches, validation_steps=20,verbose=2)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nthe error information is showed above \r\n", "comments": ["The provided code is working as expected on Colab. \r\nPlease see gist [here](https://colab.sandbox.google.com/gist/gadagashwini/140cde9f8eab3a1f290e255260f33060/untitled249.ipynb). Thanks!", "It is so strange, when i change the neural network like this\uff1a\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nimport tensorflow_datasets as tfds\r\n# tfds.disable_progress_bar()\r\n(train_data, test_data), info = tfds.load(\r\n    'imdb_reviews/subwords8k', \r\n    split = (tfds.Split.TRAIN, tfds.Split.TEST), \r\n    with_info=True, as_supervised=True)\r\nencoder = info.features['text'].encoder\r\npadded_shapes = ([None],())\r\ntrain_batches = train_data.shuffle(10000).padded_batch(256, padded_shapes = padded_shapes)\r\ntest_batches = test_data.shuffle(10000).padded_batch(256, padded_shapes = padded_shapes)\r\n\r\nembedding_dim=16\r\n\r\nmodel = keras.Sequential([\r\n    layers.Embedding(encoder.vocab_size, embedding_dim,mask_zero=True),\r\n    layers.Bidirectional(tf.keras.layers.LSTM(32)),\r\n    layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(\r\n    train_batches,\r\n    epochs=10,\r\n    validation_data=test_batches, validation_steps=20)\r\n```\r\n\r\nIt will run error on the third epoch:\r\n```python\r\nEpoch 1/10\r\n98/98 [==============================].6931 - accuracy: 0.50 - 8s 4s/step - loss: 0.6932 - accuracy: 0.50 - 8s 3s/step - loss: 0.6932 - accuracy: 0.50 - 8s 2s/step - loss: 0.6929 - accuracy: 0.50 - 8s 2s/step - loss: 0.6931 - accuracy: 0.50 - 9s 1s/step - loss: 0.6930 - accuracy: 0.50 - 9s 1s/step - loss: 0.6929 - accuracy: 0.50 - 9s 1s/step - loss: 0.6932 - accuracy: 0.49 - 9s 1s/step - loss: 0.6930 - accuracy: 0.50 - 9s 933ms/step - loss: 0.6931 - accuracy: 0.500 - 9s 863ms/step - loss: 0.6932 - accuracy: 0.496 - 10s 806ms/step - loss: 0.6931 - accuracy: 0.49 - 10s 757ms/step - loss: 0.6931 - accuracy: 0.49 - 10s 714ms/step - loss: 0.6931 - accuracy: 0.49 - 10s 679ms/step - loss: 0.6929 - accuracy: 0.50 - 10s 646ms/step - loss: 0.6928 - accuracy: 0.50 - 11s 618ms/step - loss: 0.6928 - accuracy: 0.50 - 11s 594ms/step - loss: 0.6928 - accuracy: 0.50 - 11s 572ms/step - loss: 0.6928 - accuracy: 0.50 - 11s 551ms/step - loss: 0.6927 - accuracy: 0.50 - 11s 535ms/step - loss: 0.6927 - accuracy: 0.50 - 11s 518ms/step - loss: 0.6927 - accuracy: 0.50 - 12s 503ms/step - loss: 0.6926 - accuracy: 0.50 - 12s 492ms/step - loss: 0.6926 - accuracy: 0.50 - 12s 479ms/step - loss: 0.6925 - accuracy: 0.51 - 12s 467ms/step - loss: 0.6925 - accuracy: 0.51 - 12s 455ms/step - loss: 0.6924 - accuracy: 0.51 - 12s 445ms/step - loss: 0.6924 - accuracy: 0.51 - 13s 435ms/step - loss: 0.6923 - accuracy: 0.52 - 13s 426ms/step - loss: 0.6923 - accuracy: 0.52 - 13s 418ms/step - loss: 0.6923 - accuracy: 0.52 - 13s 409ms/step - loss: 0.6922 - accuracy: 0.52 - 13s 402ms/step - loss: 0.6921 - accuracy: 0.53 - 13s 397ms/step - loss: 0.6921 - accuracy: 0.53 - 14s 391ms/step - loss: 0.6920 - accuracy: 0.53 - 14s 385ms/step - loss: 0.6919 - accuracy: 0.53 - 14s 380ms/step - loss: 0.6919 - accuracy: 0.53 - 14s 375ms/step - loss: 0.6918 - accuracy: 0.53 - 14s 369ms/step - loss: 0.6917 - accuracy: 0.53 - 15s 365ms/step - loss: 0.6916 - accuracy: 0.53 - 15s 360ms/step - loss: 0.6914 - accuracy: 0.54 - 15s 357ms/step - loss: 0.6913 - accuracy: 0.54 - 15s 353ms/step - loss: 0.6911 - accuracy: 0.54 - 15s 349ms/step - loss: 0.6909 - accuracy: 0.54 - 16s 345ms/step - loss: 0.6905 - accuracy: 0.54 - 16s 342ms/step - loss: 0.6901 - accuracy: 0.55 - 16s 339ms/step - loss: 0.6899 - accuracy: 0.55 - 16s 335ms/step - loss: 0.6895 - accuracy: 0.55 - 16s 332ms/step - loss: 0.6889 - accuracy: 0.54 - 16s 328ms/step - loss: 0.6885 - accuracy: 0.54 - 17s 325ms/step - loss: 0.6880 - accuracy: 0.55 - 17s 321ms/step - loss: 0.6867 - accuracy: 0.55 - 17s 318ms/step - loss: 0.6853 - accuracy: 0.55 - 17s 316ms/step - loss: 0.6846 - accuracy: 0.55 - 17s 313ms/step - loss: 0.6831 - accuracy: 0.56 - 17s 310ms/step - loss: 0.6811 - accuracy: 0.56 - 18s 308ms/step - loss: 0.6792 - accuracy: 0.56 - 18s 305ms/step - loss: 0.6763 - accuracy: 0.56 - 18s 303ms/step - loss: 0.6754 - accuracy: 0.57 - 18s 301ms/step - loss: 0.6748 - accuracy: 0.57 - 18s 299ms/step - loss: 0.6744 - accuracy: 0.57 - 18s 297ms/step - loss: 0.6726 - accuracy: 0.57 - 19s 295ms/step - loss: 0.6712 - accuracy: 0.57 - 19s 293ms/step - loss: 0.6696 - accuracy: 0.58 - 19s 293ms/step - loss: 0.6684 - accuracy: 0.58 - 19s 291ms/step - loss: 0.6682 - accuracy: 0.58 - 19s 289ms/step - loss: 0.6682 - accuracy: 0.58 - 20s 287ms/step - loss: 0.6676 - accuracy: 0.58 - 20s 285ms/step - loss: 0.6668 - accuracy: 0.58 - 20s 285ms/step - loss: 0.6650 - accuracy: 0.58 - 20s 283ms/step - loss: 0.6631 - accuracy: 0.58 - 20s 281ms/step - loss: 0.6614 - accuracy: 0.59 - 20s 281ms/step - loss: 0.6592 - accuracy: 0.59 - 21s 279ms/step - loss: 0.6571 - accuracy: 0.59 - 21s 278ms/step - loss: 0.6553 - accuracy: 0.59 - 21s 277ms/step - loss: 0.6531 - accuracy: 0.60 - 21s 275ms/step - loss: 0.6514 - accuracy: 0.60 - 21s 274ms/step - loss: 0.6493 - accuracy: 0.60 - 22s 273ms/step - loss: 0.6477 - accuracy: 0.60 - 22s 271ms/step - loss: 0.6458 - accuracy: 0.60 - 22s 270ms/step - loss: 0.6440 - accuracy: 0.61 - 22s 268ms/step - loss: 0.6417 - accuracy: 0.61 - 22s 267ms/step - loss: 0.6396 - accuracy: 0.61 - 22s 266ms/step - loss: 0.6372 - accuracy: 0.61 - 23s 265ms/step - loss: 0.6350 - accuracy: 0.62 - 23s 264ms/step - loss: 0.6324 - accuracy: 0.62 - 23s 263ms/step - loss: 0.6303 - accuracy: 0.62 - 23s 261ms/step - loss: 0.6280 - accuracy: 0.62 - 23s 261ms/step - loss: 0.6258 - accuracy: 0.62 - 23s 260ms/step - loss: 0.6240 - accuracy: 0.63 - 24s 259ms/step - loss: 0.6220 - accuracy: 0.63 - 24s 258ms/step - loss: 0.6200 - accuracy: 0.63 - 24s 257ms/step - loss: 0.6180 - accuracy: 0.63 - 24s 256ms/step - loss: 0.6158 - accuracy: 0.63 - 24s 255ms/step - loss: 0.6139 - accuracy: 0.64 - 24s 254ms/step - loss: 0.6119 - accuracy: 0.64 - 25s 253ms/step - loss: 0.6097 - accuracy: 0.64 - 25s 252ms/step - loss: 0.6083 - accuracy: 0.64 - 29s 294ms/step - loss: 0.6083 - accuracy: 0.6457 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 2/10\r\n98/98 [==============================] - ETA: 1:11 - loss: 0.4023 - accuracy: 0.82 - ETA: 43s - loss: 0.4169 - accuracy: 0.8125 - ETA: 34s - loss: 0.4057 - accuracy: 0.824 - ETA: 30s - loss: 0.4111 - accuracy: 0.817 - ETA: 27s - loss: 0.4094 - accuracy: 0.818 - ETA: 25s - loss: 0.4069 - accuracy: 0.821 - ETA: 23s - loss: 0.4021 - accuracy: 0.822 - ETA: 22s - loss: 0.3942 - accuracy: 0.826 - ETA: 21s - loss: 0.3908 - accuracy: 0.830 - ETA: 20s - loss: 0.3920 - accuracy: 0.832 - ETA: 20s - loss: 0.4001 - accuracy: 0.829 - ETA: 19s - loss: 0.3995 - accuracy: 0.830 - ETA: 18s - loss: 0.3982 - accuracy: 0.831 - ETA: 18s - loss: 0.3966 - accuracy: 0.829 - ETA: 17s - loss: 0.3951 - accuracy: 0.831 - ETA: 17s - loss: 0.3948 - accuracy: 0.831 - ETA: 16s - loss: 0.3947 - accuracy: 0.831 - ETA: 16s - loss: 0.3959 - accuracy: 0.829 - ETA: 16s - loss: 0.3961 - accuracy: 0.829 - ETA: 15s - loss: 0.3958 - accuracy: 0.828 - ETA: 15s - loss: 0.3957 - accuracy: 0.828 - ETA: 15s - loss: 0.3958 - accuracy: 0.828 - ETA: 14s - loss: 0.3949 - accuracy: 0.828 - ETA: 14s - loss: 0.3925 - accuracy: 0.830 - ETA: 14s - loss: 0.3923 - accuracy: 0.831 - ETA: 14s - loss: 0.3898 - accuracy: 0.832 - ETA: 13s - loss: 0.3868 - accuracy: 0.835 - ETA: 13s - loss: 0.3854 - accuracy: 0.836 - ETA: 13s - loss: 0.3818 - accuracy: 0.838 - ETA: 13s - loss: 0.3816 - accuracy: 0.839 - ETA: 12s - loss: 0.3798 - accuracy: 0.841 - ETA: 12s - loss: 0.3788 - accuracy: 0.842 - ETA: 12s - loss: 0.3796 - accuracy: 0.841 - ETA: 12s - loss: 0.3801 - accuracy: 0.841 - ETA: 12s - loss: 0.3774 - accuracy: 0.843 - ETA: 12s - loss: 0.3795 - accuracy: 0.840 - ETA: 11s - loss: 0.3796 - accuracy: 0.839 - ETA: 11s - loss: 0.3810 - accuracy: 0.838 - ETA: 11s - loss: 0.3814 - accuracy: 0.837 - ETA: 11s - loss: 0.3821 - accuracy: 0.836 - ETA: 10s - loss: 0.3822 - accuracy: 0.836 - ETA: 10s - loss: 0.3820 - accuracy: 0.837 - ETA: 10s - loss: 0.3812 - accuracy: 0.838 - ETA: 10s - loss: 0.3828 - accuracy: 0.838 - ETA: 10s - loss: 0.3861 - accuracy: 0.838 - ETA: 9s - loss: 0.3870 - accuracy: 0.838 - ETA: 9s - loss: 0.3858 - accuracy: 0.83 - ETA: 9s - loss: 0.3855 - accuracy: 0.83 - ETA: 9s - loss: 0.3863 - accuracy: 0.83 - ETA: 9s - loss: 0.3870 - accuracy: 0.83 - ETA: 8s - loss: 0.3888 - accuracy: 0.83 - ETA: 8s - loss: 0.3902 - accuracy: 0.83 - ETA: 8s - loss: 0.3895 - accuracy: 0.83 - ETA: 8s - loss: 0.3894 - accuracy: 0.83 - ETA: 8s - loss: 0.3895 - accuracy: 0.83 - ETA: 8s - loss: 0.3896 - accuracy: 0.83 - ETA: 7s - loss: 0.3885 - accuracy: 0.83 - ETA: 7s - loss: 0.3875 - accuracy: 0.83 - ETA: 7s - loss: 0.3870 - accuracy: 0.83 - ETA: 7s - loss: 0.3874 - accuracy: 0.83 - ETA: 7s - loss: 0.3874 - accuracy: 0.83 - ETA: 6s - loss: 0.3867 - accuracy: 0.83 - ETA: 6s - loss: 0.3861 - accuracy: 0.83 - ETA: 6s - loss: 0.3872 - accuracy: 0.83 - ETA: 6s - loss: 0.3888 - accuracy: 0.83 - ETA: 6s - loss: 0.3885 - accuracy: 0.83 - ETA: 5s - loss: 0.3887 - accuracy: 0.83 - ETA: 5s - loss: 0.3881 - accuracy: 0.83 - ETA: 5s - loss: 0.3872 - accuracy: 0.83 - ETA: 5s - loss: 0.3877 - accuracy: 0.83 - ETA: 5s - loss: 0.3877 - accuracy: 0.83 - ETA: 4s - loss: 0.3873 - accuracy: 0.83 - ETA: 4s - loss: 0.3875 - accuracy: 0.83 - ETA: 4s - loss: 0.3885 - accuracy: 0.83 - ETA: 4s - loss: 0.3887 - accuracy: 0.83 - ETA: 4s - loss: 0.3887 - accuracy: 0.83 - ETA: 3s - loss: 0.3891 - accuracy: 0.83 - ETA: 3s - loss: 0.3891 - accuracy: 0.83 - ETA: 3s - loss: 0.3888 - accuracy: 0.83 - ETA: 3s - loss: 0.3875 - accuracy: 0.83 - ETA: 3s - loss: 0.3873 - accuracy: 0.83 - ETA: 3s - loss: 0.3871 - accuracy: 0.83 - ETA: 2s - loss: 0.3877 - accuracy: 0.83 - ETA: 2s - loss: 0.3870 - accuracy: 0.84 - ETA: 2s - loss: 0.3868 - accuracy: 0.84 - ETA: 2s - loss: 0.3860 - accuracy: 0.84 - ETA: 2s - loss: 0.3858 - accuracy: 0.84 - ETA: 1s - loss: 0.3858 - accuracy: 0.84 - ETA: 1s - loss: 0.3852 - accuracy: 0.84 - ETA: 1s - loss: 0.3844 - accuracy: 0.84 - ETA: 1s - loss: 0.3837 - accuracy: 0.84 - ETA: 1s - loss: 0.3837 - accuracy: 0.84 - ETA: 0s - loss: 0.3838 - accuracy: 0.84 - ETA: 0s - loss: 0.3842 - accuracy: 0.84 - ETA: 0s - loss: 0.3844 - accuracy: 0.84 - ETA: 0s - loss: 0.3844 - accuracy: 0.84 - ETA: 0s - loss: 0.3843 - accuracy: 0.84 - 20s 205ms/step - loss: 0.3843 - accuracy: 0.8410 - val_loss: 0.4529 - val_accuracy: 0.8188\r\nEpoch 3/10\r\n98/98 [==============================] - ETA: 1:11 - loss: 0.4761 - accuracy: 0.79 - ETA: 43s - loss: 0.4235 - accuracy: 0.8379 - ETA: 34s - loss: 0.4283 - accuracy: 0.842 - ETA: 29s - loss: 0.4134 - accuracy: 0.858 - ETA: 27s - loss: 0.4063 - accuracy: 0.867 - ETA: 28s - loss: 0.4038 - accuracy: 0.867 - ETA: 26s - loss: 0.3939 - accuracy: 0.867 - ETA: 24s - loss: 0.3958 - accuracy: 0.864 - ETA: 23s - loss: 0.3985 - accuracy: 0.858 - ETA: 22s - loss: 0.4041 - accuracy: 0.853 - ETA: 21s - loss: 0.4054 - accuracy: 0.851 - ETA: 20s - loss: 0.4099 - accuracy: 0.846 - ETA: 19s - loss: 0.4145 - accuracy: 0.842 - ETA: 19s - loss: 0.4123 - accuracy: 0.841 - ETA: 18s - loss: 0.4146 - accuracy: 0.838 - ETA: 18s - loss: 0.4176 - accuracy: 0.837 - ETA: 17s - loss: 0.4177 - accuracy: 0.838 - ETA: 17s - loss: 0.4201 - accuracy: 0.837 - ETA: 16s - loss: 0.4193 - accuracy: 0.839 - ETA: 16s - loss: 0.4174 - accuracy: 0.840 - ETA: 15s - loss: 0.4145 - accuracy: 0.843 - ETA: 15s - loss: 0.4145 - accuracy: 0.843 - ETA: 15s - loss: 0.4165 - accuracy: 0.842 - ETA: 15s - loss: 0.4200 - accuracy: 0.839 - ETA: 14s - loss: 0.4237 - accuracy: 0.837 - ETA: 14s - loss: 0.4267 - accuracy: 0.835 - ETA: 14s - loss: 0.4293 - accuracy: 0.833 - ETA: 14s - loss: 0.4321 - accuracy: 0.831 - ETA: 13s - loss: 0.4355 - accuracy: 0.828 - ETA: 13s - loss: 0.4375 - accuracy: 0.828 - ETA: 13s - loss: 0.4402 - accuracy: 0.826 - ETA: 13s - loss: 0.4415 - accuracy: 0.825 - ETA: 12s - loss: 0.4423 - accuracy: 0.826 - ETA: 12s - loss: 0.4420 - accuracy: 0.826 - ETA: 12s - loss: 0.4415 - accuracy: 0.826 - ETA: 12s - loss: 0.4400 - accuracy: 0.828 - ETA: 11s - loss: 0.4390 - accuracy: 0.829 - ETA: 11s - loss: 0.4400 - accuracy: 0.829 - ETA: 11s - loss: 0.4400 - accuracy: 0.829 - ETA: 11s - loss: 0.4394 - accuracy: 0.829 - ETA: 10s - loss: 0.4383 - accuracy: 0.829 - ETA: 10s - loss: 0.4391 - accuracy: 0.829 - ETA: 10s - loss: 0.4399 - accuracy: 0.827 - ETA: 10s - loss: 0.4410 - accuracy: 0.826 - ETA: 10s - loss: 0.4396 - accuracy: 0.826 - ETA: 10s - loss: 0.4376 - accuracy: 0.827 - ETA: 9s - loss: 0.4373 - accuracy: 0.826 - ETA: 9s - loss: 0.4363 - accuracy: 0.82 - ETA: 9s - loss: 0.4349 - accuracy: 0.82 - ETA: 9s - loss: 0.4333 - accuracy: 0.82 - ETA: 9s - loss: 0.4335 - accuracy: 0.82 - ETA: 8s - loss: 0.4320 - accuracy: 0.83 - ETA: 8s - loss: 0.4314 - accuracy: 0.83 - ETA: 8s - loss: 0.4308 - accuracy: 0.83 - ETA: 8s - loss: 0.4299 - accuracy: 0.83 - ETA: 8s - loss: 0.4287 - accuracy: 0.83 - ETA: 7s - loss: 0.4266 - accuracy: 0.83 - ETA: 7s - loss: 0.4244 - accuracy: 0.83 - ETA: 7s - loss: 0.4232 - accuracy: 0.83 - ETA: 7s - loss: 0.4213 - accuracy: 0.83 - ETA: 7s - loss: 0.4201 - accuracy: 0.83 - ETA: 6s - loss: 0.4175 - accuracy: 0.83 - ETA: 6s - loss: 0.4164 - accuracy: 0.83 - ETA: 6s - loss: 0.4146 - accuracy: 0.83 - ETA: 6s - loss: 0.4125 - accuracy: 0.84 - ETA: 6s - loss: 0.4105 - accuracy: 0.84 - ETA: 5s - loss: 0.4090 - accuracy: 0.84 - ETA: 5s - loss: 0.4079 - accuracy: 0.84 - ETA: 5s - loss: 0.4067 - accuracy: 0.84 - ETA: 5s - loss: 0.4053 - accuracy: 0.84 - ETA: 5s - loss: 0.4041 - accuracy: 0.84 - ETA: 4s - loss: 0.4026 - accuracy: 0.84 - ETA: 4s - loss: 0.4012 - accuracy: 0.84 - ETA: 4s - loss: 0.3994 - accuracy: 0.84 - ETA: 4s - loss: 0.3982 - accuracy: 0.84 - ETA: 4s - loss: 0.3961 - accuracy: 0.84 - ETA: 3s - loss: 0.3942 - accuracy: 0.84 - ETA: 3s - loss: 0.3919 - accuracy: 0.84 - ETA: 3s - loss: 0.3905 - accuracy: 0.84 - ETA: 3s - loss: 0.3894 - accuracy: 0.84 - ETA: 3s - loss: 0.3881 - accuracy: 0.85 - ETA: 2s - loss: 0.3867 - accuracy: 0.85 - ETA: 2s - loss: 0.3860 - accuracy: 0.85 - ETA: 2s - loss: 0.3849 - accuracy: 0.85 - ETA: 2s - loss: 0.3833 - accuracy: 0.85 - ETA: 2s - loss: 0.3816 - accuracy: 0.85 - ETA: 2s - loss: 0.3803 - accuracy: 0.85 - ETA: 1s - loss: 0.3787 - accuracy: 0.85 - ETA: 1s - loss: 0.3766 - accuracy: 0.85 - ETA: 1s - loss: 0.3758 - accuracy: 0.85 - ETA: 1s - loss: 0.3754 - accuracy: 0.85 - ETA: 1s - loss: 0.3743 - accuracy: 0.85 - ETA: 0s - loss: 0.3737 - accuracy: 0.85 - ETA: 0s - loss: 0.3728 - accuracy: 0.85 - ETA: 0s - loss: 0.3715 - accuracy: 0.85 - ETA: 0s - loss: 0.3701 - accuracy: 0.85 - ETA: 0s - loss: 0.3687 - accuracy: 0.85 - 18s 183ms/step - loss: 0.3687 - accuracy: 0.8575\r\n---------------------------------------------------------------------------\r\nCancelledError                            Traceback (most recent call last)\r\n<ipython-input-1-6e612f3b1eca> in <module>\r\n     31     train_batches,\r\n     32     epochs=10,\r\n---> 33     validation_data=test_batches, validation_steps=20)\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    485       # In this case we have created variables on the first call, so we run the\r\n    486       # defunned version which is guaranteed to never create variables.\r\n--> 487       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    488     elif self._stateful_fn is not None:\r\n    489       # Release the lock early so that multiple threads can perform the call\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n   1825   @property\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nCancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node Reshape_11/_38}}]] [Op:__inference_distributed_function_15947]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/15049049/68579628-8d062700-04af-11ea-91ff-de2ec2d0e98d.png)\r\n\r\n\r\nBut the[ Google colab can run  fluently](https://colab.research.google.com/drive/1d9rAJsacUYQRL51yh0osVjg1muxvwcdp?hl=en#scrollTo=5DJ-Qm7W5m6K):\r\n![image](https://user-images.githubusercontent.com/15049049/68579669-a27b5100-04af-11ea-91a6-1ef33208f9b0.png)\r\n\r\n\r\n\r\n", "I know that Google colab can run fluently, the strange things is that I can not run fluently on my jupyter and my Pycharm! I know that Google colab is very usefully, but I can't always run my deep learning code on Google colab. Pleas, it will run error on others environment!", "I have many pictures and error information can prove my view.", "What version of cudnn are you using?\r\nKill all other python processes that may be running.\r\nTry setting ```TF_FORCE_GPU_ALLOW_GROWTH=true``` environment variable and execute the script again. (to allow gpu growth)", "This is my cudnn `cudnn-10.0-windows10-x64-v7.6.0.64`,\r\nAnd how to set the variable? I try `os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]='true'` in my jupyter, but it still doen`t work. Can you tell me how to use it correctly?\r\n![image](https://user-images.githubusercontent.com/15049049/68636488-9ab2bf80-0536-11ea-96e2-ae57f006df14.png)\r\nThe log information:\r\n```python\r\nEpoch 1/10\r\n98/98 [==============================].6933 - accuracy: 0.48 - 8s 4s/step - loss: 0.6934 - accuracy: 0.47 - 8s 3s/step - loss: 0.6933 - accuracy: 0.48 - 8s 2s/step - loss: 0.6931 - accuracy: 0.49 - 8s 2s/step - loss: 0.6931 - accuracy: 0.49 - 9s 1s/step - loss: 0.6932 - accuracy: 0.49 - 9s 1s/step - loss: 0.6931 - accuracy: 0.50 - 9s 1s/step - loss: 0.6931 - accuracy: 0.50 - 9s 1s/step - loss: 0.6932 - accuracy: 0.49 - 9s 920ms/step - loss: 0.6932 - accuracy: 0.489 - 9s 851ms/step - loss: 0.6932 - accuracy: 0.487 - 10s 794ms/step - loss: 0.6932 - accuracy: 0.48 - 10s 746ms/step - loss: 0.6932 - accuracy: 0.49 - 10s 704ms/step - loss: 0.6931 - accuracy: 0.49 - 10s 669ms/step - loss: 0.6931 - accuracy: 0.50 - 10s 643ms/step - loss: 0.6930 - accuracy: 0.50 - 10s 615ms/step - loss: 0.6931 - accuracy: 0.49 - 11s 595ms/step - loss: 0.6930 - accuracy: 0.49 - 11s 571ms/step - loss: 0.6930 - accuracy: 0.49 - 11s 552ms/step - loss: 0.6929 - accuracy: 0.50 - 11s 533ms/step - loss: 0.6929 - accuracy: 0.50 - 11s 517ms/step - loss: 0.6929 - accuracy: 0.50 - 12s 502ms/step - loss: 0.6928 - accuracy: 0.50 - 12s 488ms/step - loss: 0.6928 - accuracy: 0.50 - 12s 475ms/step - loss: 0.6927 - accuracy: 0.50 - 12s 463ms/step - loss: 0.6927 - accuracy: 0.50 - 12s 452ms/step - loss: 0.6926 - accuracy: 0.50 - 12s 443ms/step - loss: 0.6925 - accuracy: 0.50 - 13s 433ms/step - loss: 0.6924 - accuracy: 0.50 - 13s 427ms/step - loss: 0.6923 - accuracy: 0.51 - 13s 419ms/step - loss: 0.6923 - accuracy: 0.51 - 13s 411ms/step - loss: 0.6922 - accuracy: 0.51 - 13s 403ms/step - loss: 0.6921 - accuracy: 0.51 - 13s 397ms/step - loss: 0.6920 - accuracy: 0.51 - 14s 391ms/step - loss: 0.6919 - accuracy: 0.51 - 14s 392ms/step - loss: 0.6918 - accuracy: 0.51 - 14s 387ms/step - loss: 0.6917 - accuracy: 0.52 - 14s 381ms/step - loss: 0.6915 - accuracy: 0.52 - 15s 376ms/step - loss: 0.6914 - accuracy: 0.52 - 15s 370ms/step - loss: 0.6912 - accuracy: 0.52 - 15s 366ms/step - loss: 0.6911 - accuracy: 0.52 - 15s 361ms/step - loss: 0.6909 - accuracy: 0.53 - 15s 357ms/step - loss: 0.6905 - accuracy: 0.53 - 16s 353ms/step - loss: 0.6904 - accuracy: 0.53 - 16s 348ms/step - loss: 0.6900 - accuracy: 0.54 - 16s 345ms/step - loss: 0.6896 - accuracy: 0.54 - 16s 342ms/step - loss: 0.6889 - accuracy: 0.54 - 16s 338ms/step - loss: 0.6883 - accuracy: 0.55 - 16s 335ms/step - loss: 0.6871 - accuracy: 0.55 - 17s 331ms/step - loss: 0.6864 - accuracy: 0.55 - 17s 329ms/step - loss: 0.6856 - accuracy: 0.55 - 17s 326ms/step - loss: 0.6853 - accuracy: 0.55 - 17s 323ms/step - loss: 0.6840 - accuracy: 0.56 - 17s 319ms/step - loss: 0.6820 - accuracy: 0.56 - 17s 316ms/step - loss: 0.6808 - accuracy: 0.56 - 18s 314ms/step - loss: 0.6797 - accuracy: 0.57 - 18s 312ms/step - loss: 0.6786 - accuracy: 0.57 - 18s 309ms/step - loss: 0.6771 - accuracy: 0.57 - 18s 307ms/step - loss: 0.6756 - accuracy: 0.57 - 18s 304ms/step - loss: 0.6740 - accuracy: 0.58 - 18s 302ms/step - loss: 0.6724 - accuracy: 0.58 - 19s 301ms/step - loss: 0.6705 - accuracy: 0.58 - 19s 299ms/step - loss: 0.6687 - accuracy: 0.58 - 19s 297ms/step - loss: 0.6673 - accuracy: 0.59 - 19s 295ms/step - loss: 0.6655 - accuracy: 0.59 - 19s 293ms/step - loss: 0.6639 - accuracy: 0.59 - 20s 291ms/step - loss: 0.6623 - accuracy: 0.59 - 20s 290ms/step - loss: 0.6603 - accuracy: 0.60 - 20s 288ms/step - loss: 0.6594 - accuracy: 0.60 - 20s 286ms/step - loss: 0.6594 - accuracy: 0.60 - 20s 285ms/step - loss: 0.6595 - accuracy: 0.60 - 20s 283ms/step - loss: 0.6598 - accuracy: 0.60 - 21s 282ms/step - loss: 0.6598 - accuracy: 0.60 - 21s 280ms/step - loss: 0.6594 - accuracy: 0.60 - 21s 278ms/step - loss: 0.6601 - accuracy: 0.60 - 21s 277ms/step - loss: 0.6600 - accuracy: 0.60 - 21s 275ms/step - loss: 0.6596 - accuracy: 0.60 - 21s 274ms/step - loss: 0.6599 - accuracy: 0.60 - 22s 273ms/step - loss: 0.6602 - accuracy: 0.60 - 22s 271ms/step - loss: 0.6601 - accuracy: 0.60 - 22s 270ms/step - loss: 0.6601 - accuracy: 0.60 - 22s 269ms/step - loss: 0.6597 - accuracy: 0.60 - 22s 269ms/step - loss: 0.6597 - accuracy: 0.60 - 22s 268ms/step - loss: 0.6594 - accuracy: 0.60 - 23s 267ms/step - loss: 0.6589 - accuracy: 0.60 - 23s 265ms/step - loss: 0.6585 - accuracy: 0.60 - 23s 264ms/step - loss: 0.6579 - accuracy: 0.60 - 23s 263ms/step - loss: 0.6571 - accuracy: 0.60 - 23s 262ms/step - loss: 0.6561 - accuracy: 0.60 - 24s 262ms/step - loss: 0.6551 - accuracy: 0.60 - 24s 261ms/step - loss: 0.6548 - accuracy: 0.60 - 24s 260ms/step - loss: 0.6542 - accuracy: 0.60 - 24s 259ms/step - loss: 0.6535 - accuracy: 0.60 - 24s 258ms/step - loss: 0.6539 - accuracy: 0.60 - 24s 257ms/step - loss: 0.6532 - accuracy: 0.61 - 25s 256ms/step - loss: 0.6525 - accuracy: 0.61 - 25s 255ms/step - loss: 0.6519 - accuracy: 0.61 - 25s 253ms/step - loss: 0.6508 - accuracy: 0.61 - 29s 297ms/step - loss: 0.6508 - accuracy: 0.6146 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 2/10\r\n98/98 [==============================] - ETA: 1:12 - loss: 0.5212 - accuracy: 0.84 - ETA: 45s - loss: 0.5332 - accuracy: 0.8203 - ETA: 36s - loss: 0.5352 - accuracy: 0.830 - ETA: 31s - loss: 0.5310 - accuracy: 0.826 - ETA: 27s - loss: 0.5303 - accuracy: 0.828 - ETA: 25s - loss: 0.5288 - accuracy: 0.830 - ETA: 24s - loss: 0.5295 - accuracy: 0.827 - ETA: 22s - loss: 0.5292 - accuracy: 0.825 - ETA: 21s - loss: 0.5275 - accuracy: 0.825 - ETA: 21s - loss: 0.5242 - accuracy: 0.824 - ETA: 20s - loss: 0.5237 - accuracy: 0.820 - ETA: 19s - loss: 0.5250 - accuracy: 0.813 - ETA: 19s - loss: 0.5227 - accuracy: 0.811 - ETA: 18s - loss: 0.5250 - accuracy: 0.803 - ETA: 18s - loss: 0.5283 - accuracy: 0.797 - ETA: 17s - loss: 0.5269 - accuracy: 0.794 - ETA: 17s - loss: 0.5282 - accuracy: 0.788 - ETA: 16s - loss: 0.5302 - accuracy: 0.783 - ETA: 16s - loss: 0.5313 - accuracy: 0.777 - ETA: 16s - loss: 0.5304 - accuracy: 0.775 - ETA: 15s - loss: 0.5317 - accuracy: 0.770 - ETA: 15s - loss: 0.5317 - accuracy: 0.770 - ETA: 15s - loss: 0.5306 - accuracy: 0.773 - ETA: 14s - loss: 0.5293 - accuracy: 0.775 - ETA: 14s - loss: 0.5271 - accuracy: 0.779 - ETA: 14s - loss: 0.5261 - accuracy: 0.780 - ETA: 14s - loss: 0.5262 - accuracy: 0.781 - ETA: 13s - loss: 0.5261 - accuracy: 0.781 - ETA: 13s - loss: 0.5241 - accuracy: 0.784 - ETA: 13s - loss: 0.5235 - accuracy: 0.784 - ETA: 13s - loss: 0.5231 - accuracy: 0.785 - ETA: 12s - loss: 0.5219 - accuracy: 0.787 - ETA: 12s - loss: 0.5218 - accuracy: 0.788 - ETA: 12s - loss: 0.5204 - accuracy: 0.789 - ETA: 12s - loss: 0.5183 - accuracy: 0.791 - ETA: 11s - loss: 0.5162 - accuracy: 0.793 - ETA: 11s - loss: 0.5142 - accuracy: 0.794 - ETA: 11s - loss: 0.5124 - accuracy: 0.795 - ETA: 11s - loss: 0.5112 - accuracy: 0.795 - ETA: 11s - loss: 0.5113 - accuracy: 0.795 - ETA: 10s - loss: 0.5097 - accuracy: 0.796 - ETA: 10s - loss: 0.5082 - accuracy: 0.796 - ETA: 10s - loss: 0.5072 - accuracy: 0.797 - ETA: 10s - loss: 0.5056 - accuracy: 0.798 - ETA: 10s - loss: 0.5037 - accuracy: 0.799 - ETA: 10s - loss: 0.5032 - accuracy: 0.799 - ETA: 9s - loss: 0.5018 - accuracy: 0.800 - ETA: 9s - loss: 0.4998 - accuracy: 0.80 - ETA: 9s - loss: 0.4993 - accuracy: 0.80 - ETA: 9s - loss: 0.4977 - accuracy: 0.80 - ETA: 9s - loss: 0.4961 - accuracy: 0.80 - ETA: 9s - loss: 0.4943 - accuracy: 0.80 - ETA: 8s - loss: 0.4935 - accuracy: 0.80 - ETA: 8s - loss: 0.4926 - accuracy: 0.80 - ETA: 8s - loss: 0.4924 - accuracy: 0.80 - ETA: 8s - loss: 0.4919 - accuracy: 0.80 - ETA: 7s - loss: 0.4916 - accuracy: 0.80 - ETA: 7s - loss: 0.4915 - accuracy: 0.80 - ETA: 7s - loss: 0.4906 - accuracy: 0.80 - ETA: 7s - loss: 0.4896 - accuracy: 0.80 - ETA: 7s - loss: 0.4891 - accuracy: 0.80 - ETA: 6s - loss: 0.4879 - accuracy: 0.80 - ETA: 6s - loss: 0.4854 - accuracy: 0.80 - ETA: 6s - loss: 0.4833 - accuracy: 0.80 - ETA: 6s - loss: 0.4823 - accuracy: 0.80 - ETA: 6s - loss: 0.4817 - accuracy: 0.81 - ETA: 5s - loss: 0.4819 - accuracy: 0.80 - ETA: 5s - loss: 0.4810 - accuracy: 0.81 - ETA: 5s - loss: 0.4805 - accuracy: 0.81 - ETA: 5s - loss: 0.4800 - accuracy: 0.81 - ETA: 5s - loss: 0.4797 - accuracy: 0.81 - ETA: 4s - loss: 0.4792 - accuracy: 0.81 - ETA: 4s - loss: 0.4787 - accuracy: 0.81 - ETA: 4s - loss: 0.4783 - accuracy: 0.81 - ETA: 4s - loss: 0.4772 - accuracy: 0.81 - ETA: 4s - loss: 0.4765 - accuracy: 0.81 - ETA: 3s - loss: 0.4752 - accuracy: 0.81 - ETA: 3s - loss: 0.4742 - accuracy: 0.81 - ETA: 3s - loss: 0.4735 - accuracy: 0.81 - ETA: 3s - loss: 0.4723 - accuracy: 0.81 - ETA: 3s - loss: 0.4717 - accuracy: 0.81 - ETA: 3s - loss: 0.4713 - accuracy: 0.81 - ETA: 2s - loss: 0.4702 - accuracy: 0.81 - ETA: 2s - loss: 0.4693 - accuracy: 0.81 - ETA: 2s - loss: 0.4685 - accuracy: 0.81 - ETA: 2s - loss: 0.4674 - accuracy: 0.81 - ETA: 2s - loss: 0.4664 - accuracy: 0.81 - ETA: 1s - loss: 0.4658 - accuracy: 0.81 - ETA: 1s - loss: 0.4649 - accuracy: 0.81 - ETA: 1s - loss: 0.4638 - accuracy: 0.81 - ETA: 1s - loss: 0.4626 - accuracy: 0.81 - ETA: 1s - loss: 0.4613 - accuracy: 0.82 - ETA: 0s - loss: 0.4608 - accuracy: 0.82 - ETA: 0s - loss: 0.4601 - accuracy: 0.82 - ETA: 0s - loss: 0.4590 - accuracy: 0.82 - ETA: 0s - loss: 0.4586 - accuracy: 0.82 - ETA: 0s - loss: 0.4580 - accuracy: 0.82 - 20s 205ms/step - loss: 0.4580 - accuracy: 0.8210 - val_loss: 0.3798 - val_accuracy: 0.8381\r\nEpoch 3/10\r\n25/98 [======>.......................] - ETA: 1:16 - loss: 0.3185 - accuracy: 0.88 - ETA: 45s - loss: 0.3397 - accuracy: 0.8633 - ETA: 35s - loss: 0.3497 - accuracy: 0.860 - ETA: 30s - loss: 0.3418 - accuracy: 0.864 - ETA: 28s - loss: 0.3387 - accuracy: 0.863 - ETA: 25s - loss: 0.3346 - accuracy: 0.867 - ETA: 24s - loss: 0.3322 - accuracy: 0.871 - ETA: 23s - loss: 0.3401 - accuracy: 0.866 - ETA: 21s - loss: 0.3372 - accuracy: 0.867 - ETA: 21s - loss: 0.3366 - accuracy: 0.866 - ETA: 20s - loss: 0.3373 - accuracy: 0.865 - ETA: 19s - loss: 0.3372 - accuracy: 0.866 - ETA: 18s - loss: 0.3353 - accuracy: 0.868 - ETA: 18s - loss: 0.3326 - accuracy: 0.870 - ETA: 18s - loss: 0.3307 - accuracy: 0.871 - ETA: 17s - loss: 0.3244 - accuracy: 0.874 - ETA: 17s - loss: 0.3203 - accuracy: 0.876 - ETA: 16s - loss: 0.3176 - accuracy: 0.877 - ETA: 16s - loss: 0.3161 - accuracy: 0.877 - ETA: 16s - loss: 0.3185 - accuracy: 0.876 - ETA: 15s - loss: 0.3160 - accuracy: 0.877 - ETA: 15s - loss: 0.3186 - accuracy: 0.875 - ETA: 15s - loss: 0.3164 - accuracy: 0.876 - ETA: 14s - loss: 0.3165 - accuracy: 0.876 - ETA: 14s - loss: 0.3165 - accuracy: 0.8760\r\n---------------------------------------------------------------------------\r\nCancelledError                            Traceback (most recent call last)\r\n<ipython-input-4-b49add8ca8c7> in <module>\r\n      2     train_batches,\r\n      3     epochs=10,\r\n----> 4     validation_data=test_batches, validation_steps=20)\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    485       # In this case we have created variables on the first call, so we run the\r\n    486       # defunned version which is guaranteed to never create variables.\r\n--> 487       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    488     elif self._stateful_fn is not None:\r\n    489       # Release the lock early so that multiple threads can perform the call\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n   1825   @property\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nCancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node Reshape_11/_38}}]] [Op:__inference_distributed_function_15947]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n```\r\n", "You need to update it from the terminal.\r\nFor jupyter notebook you may try this snippet at the top of your script;\r\n```python\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0],True)\r\n```\r\nMake sure you kill all other notebooks before executing the script.", "## First\r\nI only run one script, and try you method, but still turn out the same result:\r\nDosen`t my GPU memory is small? My GPU is GTX1060 6G, and my computer memory is 16G.\r\nThe error:\r\n```python\r\nEpoch 1/10\r\n250/250 [==============================] 0.6202 - accuracy: 0.65 - 26s 156ms/step - loss: 0.6193 - accuracy: 0.65 - 27s 155ms/step - loss: 0.6189 - accuracy: 0.65 - 27s 155ms/step - loss: 0.6186 - accuracy: 0.65 - 27s 155ms/step - loss: 0.6178 - accuracy: 0.65 - 27s 155ms/step - loss: 0.6171 - accuracy: 0.65 - 27s 154ms/step - loss: 0.6165 - accuracy: 0.65 - 27s 154ms/step - loss: 0.6161 - accuracy: 0.66 - 27s 154ms/step - loss: 0.6152 - accuracy: 0.66 - 27s 153ms/step - loss: 0.6143 - accuracy: 0.66 - 27s 153ms/step - loss: 0.6137 - accuracy: 0.66 - 28s 153ms/step - loss: 0.6126 - accuracy: 0.66 - 28s 153ms/step - loss: 0.6118 - accuracy: 0.66 - 28s 152ms/step - loss: 0.6109 - accuracy: 0.66 - 28s 152ms/step - loss: 0.6103 - accuracy: 0.66 - 28s 152ms/step - loss: 0.6096 - accuracy: 0.66 - 28s 152ms/step - loss: 0.6087 - accuracy: 0.66 - 28s 151ms/step - loss: 0.6079 - accuracy: 0.66 - 28s 151ms/step - loss: 0.6069 - accuracy: 0.66 - 28s 151ms/step - loss: 0.6061 - accuracy: 0.67 - 28s 151ms/step - loss: 0.6054 - accuracy: 0.67 - 29s 150ms/step - loss: 0.6047 - accuracy: 0.67 - 29s 150ms/step - loss: 0.6037 - accuracy: 0.67 - 29s 150ms/step - loss: 0.6028 - accuracy: 0.67 - 29s 150ms/step - loss: 0.6020 - accuracy: 0.67 - 29s 150ms/step - loss: 0.6016 - accuracy: 0.67 - 29s 149ms/step - loss: 0.6006 - accuracy: 0.67 - 29s 149ms/step - loss: 0.6000 - accuracy: 0.67 - 29s 149ms/step - loss: 0.5988 - accuracy: 0.67 - 29s 149ms/step - loss: 0.5992 - accuracy: 0.67 - 30s 148ms/step - loss: 0.5983 - accuracy: 0.67 - 30s 148ms/step - loss: 0.5977 - accuracy: 0.67 - 30s 148ms/step - loss: 0.5970 - accuracy: 0.67 - 30s 147ms/step - loss: 0.5962 - accuracy: 0.67 - 30s 147ms/step - loss: 0.5952 - accuracy: 0.68 - 30s 147ms/step - loss: 0.5942 - accuracy: 0.68 - 30s 147ms/step - loss: 0.5933 - accuracy: 0.68 - 30s 147ms/step - loss: 0.5925 - accuracy: 0.68 - 30s 147ms/step - loss: 0.5917 - accuracy: 0.68 - 30s 146ms/step - loss: 0.5910 - accuracy: 0.68 - 31s 146ms/step - loss: 0.5903 - accuracy: 0.68 - 31s 146ms/step - loss: 0.5895 - accuracy: 0.68 - 31s 146ms/step - loss: 0.5890 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5885 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5879 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5869 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5861 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5854 - accuracy: 0.68 - 31s 145ms/step - loss: 0.5852 - accuracy: 0.68 - 31s 144ms/step - loss: 0.5849 - accuracy: 0.68 - 32s 144ms/step - loss: 0.5843 - accuracy: 0.68 - 32s 144ms/step - loss: 0.5837 - accuracy: 0.68 - 32s 144ms/step - loss: 0.5839 - accuracy: 0.68 - 32s 144ms/step - loss: 0.5836 - accuracy: 0.68 - 32s 144ms/step - loss: 0.5830 - accuracy: 0.69 - 32s 143ms/step - loss: 0.5823 - accuracy: 0.69 - 32s 143ms/step - loss: 0.5815 - accuracy: 0.69 - 32s 143ms/step - loss: 0.5809 - accuracy: 0.69 - 32s 143ms/step - loss: 0.5803 - accuracy: 0.69 - 33s 143ms/step - loss: 0.5798 - accuracy: 0.69 - 33s 143ms/step - loss: 0.5792 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5788 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5777 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5779 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5771 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5761 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5756 - accuracy: 0.69 - 33s 142ms/step - loss: 0.5747 - accuracy: 0.69 - 33s 141ms/step - loss: 0.5740 - accuracy: 0.69 - 34s 141ms/step - loss: 0.5733 - accuracy: 0.69 - 34s 141ms/step - loss: 0.5725 - accuracy: 0.69 - 34s 141ms/step - loss: 0.5715 - accuracy: 0.70 - 34s 141ms/step - loss: 0.5711 - accuracy: 0.70 - 34s 141ms/step - loss: 0.5703 - accuracy: 0.70 - 34s 141ms/step - loss: 0.5696 - accuracy: 0.70 - 34s 141ms/step - loss: 0.5690 - accuracy: 0.70 - 34s 140ms/step - loss: 0.5683 - accuracy: 0.70 - 35s 140ms/step - loss: 0.5675 - accuracy: 0.70 - 35s 140ms/step - loss: 0.5667 - accuracy: 0.70 - 35s 140ms/step - loss: 0.5659 - accuracy: 0.70 - 35s 140ms/step - loss: 0.5652 - accuracy: 0.70 - 35s 140ms/step - loss: 0.5646 - accuracy: 0.70 - 39s 154ms/step - loss: 0.5646 - accuracy: 0.7052 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 2/10\r\n187/250 [=====================>........] - ETA: 2:45 - loss: 0.3774 - accuracy: 0.83 - ETA: 1:36 - loss: 0.3905 - accuracy: 0.82 - ETA: 1:15 - loss: 0.3880 - accuracy: 0.82 - ETA: 1:03 - loss: 0.3709 - accuracy: 0.84 - ETA: 55s - loss: 0.3636 - accuracy: 0.8500 - ETA: 49s - loss: 0.3592 - accuracy: 0.848 - ETA: 45s - loss: 0.3578 - accuracy: 0.848 - ETA: 42s - loss: 0.3551 - accuracy: 0.850 - ETA: 40s - loss: 0.3583 - accuracy: 0.844 - ETA: 39s - loss: 0.3525 - accuracy: 0.848 - ETA: 37s - loss: 0.3448 - accuracy: 0.855 - ETA: 36s - loss: 0.3424 - accuracy: 0.856 - ETA: 35s - loss: 0.3396 - accuracy: 0.859 - ETA: 34s - loss: 0.3392 - accuracy: 0.860 - ETA: 33s - loss: 0.3394 - accuracy: 0.862 - ETA: 33s - loss: 0.3419 - accuracy: 0.860 - ETA: 32s - loss: 0.3394 - accuracy: 0.862 - ETA: 32s - loss: 0.3401 - accuracy: 0.861 - ETA: 31s - loss: 0.3372 - accuracy: 0.863 - ETA: 31s - loss: 0.3382 - accuracy: 0.862 - ETA: 31s - loss: 0.3400 - accuracy: 0.861 - ETA: 30s - loss: 0.3366 - accuracy: 0.862 - ETA: 30s - loss: 0.3384 - accuracy: 0.863 - ETA: 29s - loss: 0.3384 - accuracy: 0.862 - ETA: 29s - loss: 0.3403 - accuracy: 0.860 - ETA: 28s - loss: 0.3406 - accuracy: 0.860 - ETA: 28s - loss: 0.3442 - accuracy: 0.860 - ETA: 28s - loss: 0.3500 - accuracy: 0.858 - ETA: 27s - loss: 0.3554 - accuracy: 0.854 - ETA: 27s - loss: 0.3558 - accuracy: 0.855 - ETA: 27s - loss: 0.3558 - accuracy: 0.853 - ETA: 27s - loss: 0.3603 - accuracy: 0.850 - ETA: 26s - loss: 0.3588 - accuracy: 0.850 - ETA: 26s - loss: 0.3581 - accuracy: 0.850 - ETA: 26s - loss: 0.3560 - accuracy: 0.852 - ETA: 26s - loss: 0.3572 - accuracy: 0.851 - ETA: 26s - loss: 0.3575 - accuracy: 0.852 - ETA: 25s - loss: 0.3595 - accuracy: 0.851 - ETA: 25s - loss: 0.3597 - accuracy: 0.851 - ETA: 25s - loss: 0.3606 - accuracy: 0.852 - ETA: 25s - loss: 0.3591 - accuracy: 0.853 - ETA: 25s - loss: 0.3608 - accuracy: 0.851 - ETA: 25s - loss: 0.3622 - accuracy: 0.850 - ETA: 25s - loss: 0.3621 - accuracy: 0.851 - ETA: 24s - loss: 0.3628 - accuracy: 0.850 - ETA: 24s - loss: 0.3631 - accuracy: 0.850 - ETA: 24s - loss: 0.3629 - accuracy: 0.849 - ETA: 24s - loss: 0.3611 - accuracy: 0.850 - ETA: 24s - loss: 0.3606 - accuracy: 0.850 - ETA: 24s - loss: 0.3598 - accuracy: 0.850 - ETA: 24s - loss: 0.3600 - accuracy: 0.850 - ETA: 23s - loss: 0.3598 - accuracy: 0.849 - ETA: 23s - loss: 0.3601 - accuracy: 0.849 - ETA: 23s - loss: 0.3591 - accuracy: 0.850 - ETA: 23s - loss: 0.3591 - accuracy: 0.850 - ETA: 23s - loss: 0.3584 - accuracy: 0.850 - ETA: 23s - loss: 0.3576 - accuracy: 0.850 - ETA: 22s - loss: 0.3562 - accuracy: 0.852 - ETA: 22s - loss: 0.3573 - accuracy: 0.852 - ETA: 22s - loss: 0.3573 - accuracy: 0.852 - ETA: 22s - loss: 0.3565 - accuracy: 0.853 - ETA: 22s - loss: 0.3553 - accuracy: 0.853 - ETA: 22s - loss: 0.3555 - accuracy: 0.852 - ETA: 22s - loss: 0.3563 - accuracy: 0.851 - ETA: 21s - loss: 0.3563 - accuracy: 0.851 - ETA: 21s - loss: 0.3568 - accuracy: 0.851 - ETA: 21s - loss: 0.3552 - accuracy: 0.852 - ETA: 21s - loss: 0.3548 - accuracy: 0.851 - ETA: 21s - loss: 0.3540 - accuracy: 0.852 - ETA: 21s - loss: 0.3534 - accuracy: 0.852 - ETA: 21s - loss: 0.3537 - accuracy: 0.851 - ETA: 20s - loss: 0.3536 - accuracy: 0.852 - ETA: 20s - loss: 0.3523 - accuracy: 0.852 - ETA: 20s - loss: 0.3522 - accuracy: 0.853 - ETA: 20s - loss: 0.3519 - accuracy: 0.853 - ETA: 20s - loss: 0.3520 - accuracy: 0.853 - ETA: 20s - loss: 0.3508 - accuracy: 0.853 - ETA: 20s - loss: 0.3506 - accuracy: 0.853 - ETA: 20s - loss: 0.3509 - accuracy: 0.852 - ETA: 19s - loss: 0.3509 - accuracy: 0.852 - ETA: 19s - loss: 0.3506 - accuracy: 0.853 - ETA: 19s - loss: 0.3513 - accuracy: 0.852 - ETA: 19s - loss: 0.3503 - accuracy: 0.852 - ETA: 19s - loss: 0.3494 - accuracy: 0.853 - ETA: 19s - loss: 0.3496 - accuracy: 0.853 - ETA: 19s - loss: 0.3499 - accuracy: 0.852 - ETA: 19s - loss: 0.3504 - accuracy: 0.852 - ETA: 18s - loss: 0.3506 - accuracy: 0.852 - ETA: 18s - loss: 0.3499 - accuracy: 0.852 - ETA: 18s - loss: 0.3496 - accuracy: 0.853 - ETA: 18s - loss: 0.3497 - accuracy: 0.853 - ETA: 18s - loss: 0.3501 - accuracy: 0.852 - ETA: 18s - loss: 0.3492 - accuracy: 0.853 - ETA: 18s - loss: 0.3499 - accuracy: 0.852 - ETA: 18s - loss: 0.3500 - accuracy: 0.852 - ETA: 17s - loss: 0.3498 - accuracy: 0.852 - ETA: 17s - loss: 0.3496 - accuracy: 0.852 - ETA: 17s - loss: 0.3494 - accuracy: 0.852 - ETA: 17s - loss: 0.3494 - accuracy: 0.852 - ETA: 17s - loss: 0.3496 - accuracy: 0.852 - ETA: 17s - loss: 0.3490 - accuracy: 0.852 - ETA: 17s - loss: 0.3483 - accuracy: 0.852 - ETA: 17s - loss: 0.3478 - accuracy: 0.853 - ETA: 16s - loss: 0.3477 - accuracy: 0.853 - ETA: 16s - loss: 0.3471 - accuracy: 0.853 - ETA: 16s - loss: 0.3466 - accuracy: 0.853 - ETA: 16s - loss: 0.3464 - accuracy: 0.854 - ETA: 16s - loss: 0.3453 - accuracy: 0.854 - ETA: 16s - loss: 0.3449 - accuracy: 0.855 - ETA: 16s - loss: 0.3450 - accuracy: 0.854 - ETA: 15s - loss: 0.3444 - accuracy: 0.855 - ETA: 15s - loss: 0.3432 - accuracy: 0.855 - ETA: 15s - loss: 0.3426 - accuracy: 0.856 - ETA: 15s - loss: 0.3418 - accuracy: 0.856 - ETA: 15s - loss: 0.3411 - accuracy: 0.857 - ETA: 15s - loss: 0.3404 - accuracy: 0.857 - ETA: 15s - loss: 0.3395 - accuracy: 0.858 - ETA: 15s - loss: 0.3388 - accuracy: 0.858 - ETA: 14s - loss: 0.3390 - accuracy: 0.858 - ETA: 14s - loss: 0.3387 - accuracy: 0.858 - ETA: 14s - loss: 0.3377 - accuracy: 0.858 - ETA: 14s - loss: 0.3374 - accuracy: 0.858 - ETA: 14s - loss: 0.3361 - accuracy: 0.859 - ETA: 14s - loss: 0.3361 - accuracy: 0.859 - ETA: 14s - loss: 0.3351 - accuracy: 0.860 - ETA: 14s - loss: 0.3349 - accuracy: 0.860 - ETA: 14s - loss: 0.3348 - accuracy: 0.860 - ETA: 14s - loss: 0.3351 - accuracy: 0.860 - ETA: 13s - loss: 0.3347 - accuracy: 0.860 - ETA: 13s - loss: 0.3339 - accuracy: 0.860 - ETA: 13s - loss: 0.3331 - accuracy: 0.861 - ETA: 13s - loss: 0.3323 - accuracy: 0.861 - ETA: 13s - loss: 0.3318 - accuracy: 0.862 - ETA: 13s - loss: 0.3307 - accuracy: 0.862 - ETA: 13s - loss: 0.3305 - accuracy: 0.862 - ETA: 13s - loss: 0.3299 - accuracy: 0.862 - ETA: 12s - loss: 0.3303 - accuracy: 0.862 - ETA: 12s - loss: 0.3301 - accuracy: 0.862 - ETA: 12s - loss: 0.3301 - accuracy: 0.862 - ETA: 12s - loss: 0.3292 - accuracy: 0.863 - ETA: 12s - loss: 0.3286 - accuracy: 0.863 - ETA: 12s - loss: 0.3279 - accuracy: 0.863 - ETA: 12s - loss: 0.3278 - accuracy: 0.863 - ETA: 12s - loss: 0.3280 - accuracy: 0.863 - ETA: 12s - loss: 0.3277 - accuracy: 0.864 - ETA: 11s - loss: 0.3273 - accuracy: 0.864 - ETA: 11s - loss: 0.3270 - accuracy: 0.864 - ETA: 11s - loss: 0.3266 - accuracy: 0.864 - ETA: 11s - loss: 0.3265 - accuracy: 0.865 - ETA: 11s - loss: 0.3267 - accuracy: 0.865 - ETA: 11s - loss: 0.3264 - accuracy: 0.865 - ETA: 11s - loss: 0.3269 - accuracy: 0.864 - ETA: 11s - loss: 0.3271 - accuracy: 0.864 - ETA: 10s - loss: 0.3280 - accuracy: 0.864 - ETA: 10s - loss: 0.3275 - accuracy: 0.864 - ETA: 10s - loss: 0.3278 - accuracy: 0.864 - ETA: 10s - loss: 0.3280 - accuracy: 0.864 - ETA: 10s - loss: 0.3280 - accuracy: 0.864 - ETA: 10s - loss: 0.3278 - accuracy: 0.864 - ETA: 10s - loss: 0.3274 - accuracy: 0.864 - ETA: 10s - loss: 0.3276 - accuracy: 0.864 - ETA: 10s - loss: 0.3278 - accuracy: 0.864 - ETA: 9s - loss: 0.3287 - accuracy: 0.864 - ETA: 9s - loss: 0.3279 - accuracy: 0.86 - ETA: 9s - loss: 0.3273 - accuracy: 0.86 - ETA: 9s - loss: 0.3266 - accuracy: 0.86 - ETA: 9s - loss: 0.3264 - accuracy: 0.86 - ETA: 9s - loss: 0.3258 - accuracy: 0.86 - ETA: 9s - loss: 0.3257 - accuracy: 0.86 - ETA: 9s - loss: 0.3252 - accuracy: 0.86 - ETA: 8s - loss: 0.3247 - accuracy: 0.86 - ETA: 8s - loss: 0.3246 - accuracy: 0.86 - ETA: 8s - loss: 0.3239 - accuracy: 0.86 - ETA: 8s - loss: 0.3234 - accuracy: 0.86 - ETA: 8s - loss: 0.3235 - accuracy: 0.86 - ETA: 8s - loss: 0.3238 - accuracy: 0.86 - ETA: 8s - loss: 0.3233 - accuracy: 0.86 - ETA: 8s - loss: 0.3230 - accuracy: 0.86 - ETA: 8s - loss: 0.3228 - accuracy: 0.86 - ETA: 7s - loss: 0.3228 - accuracy: 0.86 - ETA: 7s - loss: 0.3224 - accuracy: 0.86 - ETA: 7s - loss: 0.3226 - accuracy: 0.86 - ETA: 7s - loss: 0.3222 - accuracy: 0.86 - ETA: 7s - loss: 0.3218 - accuracy: 0.86 - ETA: 7s - loss: 0.3214 - accuracy: 0.86 - ETA: 7s - loss: 0.3213 - accuracy: 0.86 - ETA: 7s - loss: 0.3211 - accuracy: 0.8684\r\n207/250 [=======================>......] - ETA: 7s - loss: 0.3210 - accuracy: 0.86 - ETA: 6s - loss: 0.3208 - accuracy: 0.86 - ETA: 6s - loss: 0.3209 - accuracy: 0.86 - ETA: 6s - loss: 0.3207 - accuracy: 0.86 - ETA: 6s - loss: 0.3208 - accuracy: 0.86 - ETA: 6s - loss: 0.3204 - accuracy: 0.86 - ETA: 6s - loss: 0.3200 - accuracy: 0.86 - ETA: 6s - loss: 0.3198 - accuracy: 0.86 - ETA: 6s - loss: 0.3199 - accuracy: 0.86 - ETA: 6s - loss: 0.3197 - accuracy: 0.86 - ETA: 5s - loss: 0.3195 - accuracy: 0.86 - ETA: 5s - loss: 0.3191 - accuracy: 0.86 - ETA: 5s - loss: 0.3190 - accuracy: 0.86 - ETA: 5s - loss: 0.3190 - accuracy: 0.86 - ETA: 5s - loss: 0.3186 - accuracy: 0.86 - ETA: 5s - loss: 0.3180 - accuracy: 0.86 - ETA: 5s - loss: 0.3175 - accuracy: 0.86 - ETA: 5s - loss: 0.3169 - accuracy: 0.87 - ETA: 4s - loss: 0.3168 - accuracy: 0.87 - ETA: 4s - loss: 0.3168 - accuracy: 0.8701\r\n---------------------------------------------------------------------------\r\nCancelledError                            Traceback (most recent call last)\r\n<ipython-input-4-b49add8ca8c7> in <module>\r\n      2     train_batches,\r\n      3     epochs=10,\r\n----> 4     validation_data=test_batches, validation_steps=20)\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    322                 mode=ModeKeys.TRAIN,\r\n    323                 training_context=training_context,\r\n--> 324                 total_epochs=epochs)\r\n    325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    326 \r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    121         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    122       try:\r\n--> 123         batch_outs = execution_function(iterator)\r\n    124       except (StopIteration, errors.OutOfRangeError):\r\n    125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     84     # `numpy` translates Tensors to values in Eager mode.\r\n     85     return nest.map_structure(_non_none_constant_value,\r\n---> 86                               distributed_function(input_fn))\r\n     87 \r\n     88   return execution_function\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    455 \r\n    456     tracing_count = self._get_tracing_count()\r\n--> 457     result = self._call(*args, **kwds)\r\n    458     if tracing_count == self._get_tracing_count():\r\n    459       self._call_counter.called_without_tracing()\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    485       # In this case we have created variables on the first call, so we run the\r\n    486       # defunned version which is guaranteed to never create variables.\r\n--> 487       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    488     elif self._stateful_fn is not None:\r\n    489       # Release the lock early so that multiple threads can perform the call\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1824 \r\n   1825   @property\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1222     if executing_eagerly:\r\n   1223       flat_outputs = forward_function.call(\r\n-> 1224           ctx, args, cancellation_manager=cancellation_manager)\r\n   1225     else:\r\n   1226       gradient_name = self._delayed_rewrite_functions.register()\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    509               inputs=args,\r\n    510               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 511               ctx=ctx)\r\n    512         else:\r\n    513           outputs = execute.execute_with_cancellation(\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\nc:\\users\\sha\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nCancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node Adam/Adam/update/AssignSubVariableOp/_41}}]]\r\n\t [[Reshape_11/_38]] [Op:__inference_distributed_function_15947]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n```\r\nThe log:\r\n```python\r\n2019-11-13 10:46:11.769148: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-11-13 10:46:14.089390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-11-13 10:46:15.355578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\n2019-11-13 10:46:15.365151: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-13 10:46:15.371451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-13 10:46:15.433076: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-11-13 10:46:15.440982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\n2019-11-13 10:46:15.448352: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-13 10:46:15.453402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-13 10:46:15.984943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-13 10:46:15.989631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-11-13 10:46:15.992013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-11-13 10:46:15.995283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4708 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-11-13 10:46:23.005245: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_15158_15757' and '__inference___backward_cudnn_lstm_with_fallback_12868_14345_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_15947' both implement 'lstm_c5e66022-c906-49fc-bbca-7845a4964a4b' but their signatures do not match.\r\n2019-11-13 10:46:23.249969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2019-11-13 10:46:24.473378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-11-13 10:46:52.793050: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n         [[IteratorGetNext/_2]]\r\n2019-11-13 10:46:52.800603: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n         [[{{node IteratorGetNext}}]]\r\n2019-11-13 10:46:54.729218: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_19286_specialized_for_sequential_bidirectional_backward_lstm_StatefulPartitionedCall_at___inference_distributed_function_20831' and '__inference_standard_lstm_18937' both implement 'lstm_29225b91-0d24-4427-a6c9-1877aa788c13' but their signatures do not match.\r\n2019-11-13 10:47:19.792783: E tensorflow/stream_executor/dnn.cc:588] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1887): 'cudnnRNNBackwardDataEx( cudnn.handle(), rnn_desc.handle(), output_desc.data_handle(), output_data.opaque(), output_desc.data_handle(), output_backprop_data.opaque(), nullptr, nullptr, output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.data_handle(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), nullptr, nullptr, workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n2019-11-13 10:47:19.817604: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1899 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 32, 1, 1545, 100, 32]\r\n2019-11-13 10:47:19.828008: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 32, 1, 1545, 100, 32]\r\n         [[{{node gradients/cond_grad/If/then/_0/gradients/CudnnRNNV3_grad/CudnnRNNBackpropV3}}]]\r\n2019-11-13 10:47:19.842302: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.\r\n         [[{{node Adam/Adam/update/AssignSubVariableOp/_41}}]]\r\n         [[Reshape_11/_38]]\r\n2019-11-13 10:47:19.850236: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.\r\n         [[{{node Adam/Adam/update/AssignSubVariableOp/_41}}]]\r\n```\r\n\r\nThe code:\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nimport tensorflow_datasets as tfds\r\nimport os\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0],True)\r\n\r\n# tfds.disable_progress_bar()\r\n(train_data, test_data), info = tfds.load(\r\n    'imdb_reviews/subwords8k', \r\n    split = (tfds.Split.TRAIN, tfds.Split.TEST), \r\n    with_info=True, as_supervised=True)\r\nencoder = info.features['text'].encoder\r\npadded_shapes = ([None],())\r\ntrain_batches = train_data.shuffle(10000).padded_batch(100, padded_shapes = padded_shapes)\r\ntest_batches = test_data.shuffle(10000).padded_batch(100, padded_shapes = padded_shapes)\r\n\r\nembedding_dim=16\r\n\r\nmodel = keras.Sequential([\r\n    layers.Embedding(encoder.vocab_size, embedding_dim,mask_zero=True),\r\n    layers.Bidirectional(tf.keras.layers.LSTM(32)),\r\n    layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n# TF_FORCE_GPU_ALLOW_GROWTH=true\r\n\r\nhistory = model.fit(\r\n    train_batches,\r\n    epochs=10,\r\n    validation_data=test_batches, validation_steps=20)\r\n```\r\nThe screenshot:\r\n![image](https://user-images.githubusercontent.com/15049049/68728942-9c968480-0603-11ea-9736-4b6d2c96d0bb.png)\r\n\r\n\r\n## Second\r\nI try to cut down the batchsize form 100 to 10, but it still run error, but this time, my python shutdown <i>**with a diifferent error**</i>.\r\nThe log:\r\n```python\r\n2019-11-13 10:21:45.379517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-11-13 10:21:47.712765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-11-13 10:21:49.089035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\n2019-11-13 10:21:49.094992: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-13 10:21:49.100413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-13 10:21:49.177539: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-11-13 10:21:49.186276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\n2019-11-13 10:21:49.193764: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-13 10:21:49.199434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-13 10:21:49.744023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-13 10:21:49.748786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-11-13 10:21:49.753294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-11-13 10:21:49.756546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4708 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-11-13 10:21:56.728819: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_15158_15757' and '__inference___backward_cudnn_lstm_with_fallback_12868_14345_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_15947' both implement 'lstm_25e519cc-41b2-4ee5-b350-71058bc3955d' but their signatures do not match.\r\n2019-11-13 10:21:56.977714: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2019-11-13 10:21:57.667451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-11-13 10:23:22.374341: E tensorflow/stream_executor/dnn.cc:588] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1776): 'cudnnRNNForwardTrainingEx( cudnn.handle(), rnn_desc.handle(), input_desc.data_handle(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.data_handle(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n2019-11-13 10:23:22.382089: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-11-13 10:23:22.397738: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 32, 1, 950, 10, 32]\r\n2019-11-13 10:23:22.403047: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n2[I 10:23:43.327 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports\r\nWARNING:root:kernel d7b33090-faf7-4c7b-a280-5ede0ac73339 restarted\r\n```\r\nAnd the running result:\r\n```python\r\nEpoch 1/10\r\n    335/Unknown - 17s 98ms/step - loss: 0.6919 - accuracy: 0.523 - 17s 98ms/step - loss: 0.6919 - accuracy: 0.522 - 17s 98ms/step - loss: 0.6918 - accuracy: 0.522 - 17s 98ms/step - loss: 0.6918 - accuracy: 0.522 - 17s 97ms/step - loss: 0.6918 - accuracy: 0.523 - 17s 97ms/step - loss: 0.6917 - accuracy: 0.523 - 17s 97ms/step - loss: 0.6916 - accuracy: 0.525 - 17s 97ms/step - loss: 0.6915 - accuracy: 0.527 - 17s 96ms/step - loss: 0.6914 - accuracy: 0.528 - 17s 96ms/step - loss: 0.6914 - accuracy: 0.529 - 17s 96ms/step - loss: 0.6914 - accuracy: 0.530 - 17s 96ms/step - loss: 0.6914 - accuracy: 0.529 - 17s 96ms/step - loss: 0.6913 - accuracy: 0.530 - 17s 95ms/step - loss: 0.6912 - accuracy: 0.531 - 17s 95ms/step - loss: 0.6912 - accuracy: 0.531 - 17s 95ms/step - loss: 0.6911 - accuracy: 0.533 - 18s 95ms/step - loss: 0.6910 - accuracy: 0.534 - 18s 94ms/step - loss: 0.6909 - accuracy: 0.534 - 18s 94ms/step - loss: 0.6908 - accuracy: 0.535 - 18s 94ms/step - loss: 0.6909 - accuracy: 0.534 - 18s 94ms/step - loss: 0.6909 - accuracy: 0.534 - 18s 94ms/step - loss: 0.6908 - accuracy: 0.534 - 18s 94ms/step - loss: 0.6909 - accuracy: 0.533 - 18s 94ms/step - loss: 0.6909 - accuracy: 0.533 - 18s 94ms/step - loss: 0.6911 - accuracy: 0.532 - 18s 93ms/step - loss: 0.6910 - accuracy: 0.532 - 18s 93ms/step - loss: 0.6907 - accuracy: 0.533 - 18s 93ms/step - loss: 0.6904 - accuracy: 0.535 - 18s 93ms/step - loss: 0.6904 - accuracy: 0.535 - 18s 92ms/step - loss: 0.6903 - accuracy: 0.535 - 18s 92ms/step - loss: 0.6901 - accuracy: 0.536 - 18s 92ms/step - loss: 0.6899 - accuracy: 0.537 - 18s 92ms/step - loss: 0.6897 - accuracy: 0.536 - 19s 92ms/step - loss: 0.6885 - accuracy: 0.538 - 19s 92ms/step - loss: 0.6891 - accuracy: 0.538 - 19s 92ms/step - loss: 0.6900 - accuracy: 0.538 - 19s 91ms/step - loss: 0.6930 - accuracy: 0.537 - 19s 91ms/step - loss: 0.6931 - accuracy: 0.537 - 19s 91ms/step - loss: 0.6930 - accuracy: 0.538 - 19s 91ms/step - loss: 0.6931 - accuracy: 0.538 - 19s 91ms/step - loss: 0.6932 - accuracy: 0.536 - 19s 91ms/step - loss: 0.6930 - accuracy: 0.536 - 19s 90ms/step - loss: 0.6930 - accuracy: 0.537 - 19s 90ms/step - loss: 0.6932 - accuracy: 0.535 - 19s 90ms/step - loss: 0.6933 - accuracy: 0.535 - 19s 90ms/step - loss: 0.6935 - accuracy: 0.534 - 19s 89ms/step - loss: 0.6932 - accuracy: 0.534 - 19s 89ms/step - loss: 0.6932 - accuracy: 0.534 - 19s 89ms/step - loss: 0.6931 - accuracy: 0.534 - 19s 89ms/step - loss: 0.6927 - accuracy: 0.535 - 19s 89ms/step - loss: 0.6924 - accuracy: 0.536 - 19s 89ms/step - loss: 0.6924 - accuracy: 0.535 - 20s 88ms/step - loss: 0.6924 - accuracy: 0.536 - 20s 88ms/step - loss: 0.6923 - accuracy: 0.536 - 20s 88ms/step - loss: 0.6921 - accuracy: 0.537 - 20s 88ms/step - loss: 0.6921 - accuracy: 0.536 - 20s 88ms/step - loss: 0.6920 - accuracy: 0.536 - 20s 88ms/step - loss: 0.6919 - accuracy: 0.536 - 20s 88ms/step - loss: 0.6918 - accuracy: 0.536 - 20s 88ms/step - loss: 0.6916 - accuracy: 0.536 - 20s 88ms/step - loss: 0.6915 - accuracy: 0.536 - 20s 87ms/step - loss: 0.6915 - accuracy: 0.536 - 20s 87ms/step - loss: 0.6938 - accuracy: 0.535 - 20s 87ms/step - loss: 0.6937 - accuracy: 0.534 - 20s 87ms/step - loss: 0.6937 - accuracy: 0.535 - 20s 87ms/step - loss: 0.6936 - accuracy: 0.536 - 20s 87ms/step - loss: 0.6935 - accuracy: 0.536 - 20s 86ms/step - loss: 0.6932 - accuracy: 0.537 - 20s 86ms/step - loss: 0.6930 - accuracy: 0.538 - 21s 86ms/step - loss: 0.6930 - accuracy: 0.538 - 21s 86ms/step - loss: 0.6927 - accuracy: 0.538 - 21s 86ms/step - loss: 0.6924 - accuracy: 0.539 - 21s 86ms/step - loss: 0.6923 - accuracy: 0.539 - 21s 86ms/step - loss: 0.6922 - accuracy: 0.539 - 21s 86ms/step - loss: 0.6921 - accuracy: 0.540 - 21s 86ms/step - loss: 0.6920 - accuracy: 0.541 - 21s 85ms/step - loss: 0.6917 - accuracy: 0.541 - 21s 85ms/step - loss: 0.6918 - accuracy: 0.541 - 21s 85ms/step - loss: 0.6916 - accuracy: 0.541 - 21s 85ms/step - loss: 0.6916 - accuracy: 0.541 - 21s 85ms/step - loss: 0.6914 - accuracy: 0.541 - 21s 85ms/step - loss: 0.6911 - accuracy: 0.542 - 21s 84ms/step - loss: 0.6907 - accuracy: 0.543 - 21s 84ms/step - loss: 0.6904 - accuracy: 0.544 - 21s 84ms/step - loss: 0.6904 - accuracy: 0.544 - 21s 84ms/step - loss: 0.6901 - accuracy: 0.544 - 21s 84ms/step - loss: 0.6902 - accuracy: 0.544 - 21s 84ms/step - loss: 0.6902 - accuracy: 0.543 - 21s 84ms/step - loss: 0.6901 - accuracy: 0.544 - 22s 84ms/step - loss: 0.6900 - accuracy: 0.543 - 22s 83ms/step - loss: 0.6900 - accuracy: 0.543 - 22s 83ms/step - loss: 0.6900 - accuracy: 0.543 - 22s 83ms/step - loss: 0.6899 - accuracy: 0.543 - 22s 83ms/step - loss: 0.6897 - accuracy: 0.544 - 22s 83ms/step - loss: 0.6895 - accuracy: 0.544 - 22s 83ms/step - loss: 0.6893 - accuracy: 0.545 - 22s 83ms/step - loss: 0.6893 - accuracy: 0.545 - 22s 83ms/step - loss: 0.6891 - accuracy: 0.544 - 22s 83ms/step - loss: 0.6893 - accuracy: 0.544 - 22s 83ms/step - loss: 0.6891 - accuracy: 0.544 - 22s 82ms/step - loss: 0.6888 - accuracy: 0.545 - 22s 82ms/step - loss: 0.6888 - accuracy: 0.545 - 22s 82ms/step - loss: 0.6888 - accuracy: 0.545 - 22s 82ms/step - loss: 0.6888 - accuracy: 0.544 - 22s 82ms/step - loss: 0.6886 - accuracy: 0.545 - 22s 82ms/step - loss: 0.6885 - accuracy: 0.545 - 23s 82ms/step - loss: 0.6884 - accuracy: 0.546 - 23s 82ms/step - loss: 0.6882 - accuracy: 0.547 - 23s 82ms/step - loss: 0.6879 - accuracy: 0.548 - 23s 82ms/step - loss: 0.6878 - accuracy: 0.547 - 23s 81ms/step - loss: 0.6879 - accuracy: 0.548 - 23s 81ms/step - loss: 0.6877 - accuracy: 0.548 - 23s 81ms/step - loss: 0.6876 - accuracy: 0.548 - 23s 81ms/step - loss: 0.6877 - accuracy: 0.548 - 23s 81ms/step - loss: 0.6876 - accuracy: 0.549 - 23s 81ms/step - loss: 0.6875 - accuracy: 0.550 - 23s 81ms/step - loss: 0.6874 - accuracy: 0.550 - 23s 81ms/step - loss: 0.6871 - accuracy: 0.550 - 23s 81ms/step - loss: 0.6871 - accuracy: 0.550 - 23s 80ms/step - loss: 0.6868 - accuracy: 0.551 - 23s 80ms/step - loss: 0.6868 - accuracy: 0.550 - 23s 80ms/step - loss: 0.6867 - accuracy: 0.551 - 23s 80ms/step - loss: 0.6866 - accuracy: 0.551 - 23s 80ms/step - loss: 0.6866 - accuracy: 0.551 - 23s 80ms/step - loss: 0.6862 - accuracy: 0.553 - 24s 80ms/step - loss: 0.6860 - accuracy: 0.553 - 24s 80ms/step - loss: 0.6859 - accuracy: 0.554 - 24s 80ms/step - loss: 0.6858 - accuracy: 0.554 - 24s 80ms/step - loss: 0.6855 - accuracy: 0.555 - 24s 79ms/step - loss: 0.6854 - accuracy: 0.556 - 24s 79ms/step - loss: 0.6852 - accuracy: 0.556 - 24s 79ms/step - loss: 0.6851 - accuracy: 0.557 - 24s 79ms/step - loss: 0.6850 - accuracy: 0.557 - 24s 79ms/step - loss: 0.6848 - accuracy: 0.557 - 24s 79ms/step - loss: 0.6844 - accuracy: 0.558 - 24s 79ms/step - loss: 0.6841 - accuracy: 0.558 - 24s 79ms/step - loss: 0.6838 - accuracy: 0.559 - 24s 79ms/step - loss: 0.6837 - accuracy: 0.559 - 24s 79ms/step - loss: 0.6834 - accuracy: 0.560 - 24s 78ms/step - loss: 0.6831 - accuracy: 0.561 - 24s 78ms/step - loss: 0.6826 - accuracy: 0.561 - 24s 78ms/step - loss: 0.6826 - accuracy: 0.561 - 24s 78ms/step - loss: 0.6825 - accuracy: 0.561 - 24s 78ms/step - loss: 0.6822 - accuracy: 0.562 - 24s 78ms/step - loss: 0.6819 - accuracy: 0.562 - 25s 78ms/step - loss: 0.6815 - accuracy: 0.562 - 25s 78ms/step - loss: 0.6809 - accuracy: 0.563 - 25s 78ms/step - loss: 0.6806 - accuracy: 0.563 - 25s 78ms/step - loss: 0.6805 - accuracy: 0.564 - 25s 78ms/step - loss: 0.6803 - accuracy: 0.564 - 25s 78ms/step - loss: 0.6799 - accuracy: 0.565 - 25s 78ms/step - loss: 0.6793 - accuracy: 0.566 - 25s 78ms/step - loss: 0.6795 - accuracy: 0.566 - 25s 77ms/step - loss: 0.6789 - accuracy: 0.567 - 25s 77ms/step - loss: 0.6783 - accuracy: 0.568 - 25s 77ms/step - loss: 0.6775 - accuracy: 0.569 - 25s 77ms/step - loss: 0.6770 - accuracy: 0.569 - 25s 77ms/step - loss: 0.6770 - accuracy: 0.569 - 25s 77ms/step - loss: 0.6772 - accuracy: 0.569 - 25s 77ms/step - loss: 0.6764 - accuracy: 0.570 - 25s 77ms/step - loss: 0.6756 - accuracy: 0.571 - 25s 77ms/step - loss: 0.6761 - accuracy: 0.571 - 25s 77ms/step - loss: 0.6756 - accuracy: 0.571 - 25s 77ms/step - loss: 0.6749 - accuracy: 0.572 - 26s 77ms/step - loss: 0.6738 - accuracy: 0.574 - 26s 77ms/step - loss: 0.6729 - accuracy: 0.575 - 26s 76ms/step - loss: 0.6728 - accuracy: 0.5755\r\n    502/Unknown - 26s 76ms/step - loss: 0.6733 - accuracy: 0.575 - 26s 76ms/step - loss: 0.6729 - accuracy: 0.575 - 26s 76ms/step - loss: 0.6726 - accuracy: 0.576 - 26s 76ms/step - loss: 0.6722 - accuracy: 0.576 - 26s 76ms/step - loss: 0.6726 - accuracy: 0.576 - 26s 76ms/step - loss: 0.6729 - accuracy: 0.576 - 26s 76ms/step - loss: 0.6729 - accuracy: 0.577 - 26s 76ms/step - loss: 0.6730 - accuracy: 0.577 - 26s 76ms/step - loss: 0.6730 - accuracy: 0.577 - 26s 76ms/step - loss: 0.6722 - accuracy: 0.578 - 26s 76ms/step - loss: 0.6718 - accuracy: 0.578 - 26s 76ms/step - loss: 0.6715 - accuracy: 0.579 - 26s 76ms/step - loss: 0.6713 - accuracy: 0.579 - 26s 76ms/step - loss: 0.6712 - accuracy: 0.579 - 26s 76ms/step - loss: 0.6710 - accuracy: 0.580 - 26s 75ms/step - loss: 0.6705 - accuracy: 0.580 - 27s 75ms/step - loss: 0.6703 - accuracy: 0.580 - 27s 75ms/step - loss: 0.6702 - accuracy: 0.581 - 27s 75ms/step - loss: 0.6696 - accuracy: 0.581 - 27s 75ms/step - loss: 0.6695 - accuracy: 0.582 - 27s 75ms/step - loss: 0.6689 - accuracy: 0.583 - 27s 75ms/step - loss: 0.6686 - accuracy: 0.583 - 27s 75ms/step - loss: 0.6682 - accuracy: 0.584 - 27s 75ms/step - loss: 0.6684 - accuracy: 0.584 - 27s 75ms/step - loss: 0.6681 - accuracy: 0.584 - 27s 75ms/step - loss: 0.6679 - accuracy: 0.585 - 27s 75ms/step - loss: 0.6673 - accuracy: 0.585 - 27s 75ms/step - loss: 0.6666 - accuracy: 0.586 - 27s 75ms/step - loss: 0.6661 - accuracy: 0.587 - 27s 75ms/step - loss: 0.6653 - accuracy: 0.587 - 27s 75ms/step - loss: 0.6653 - accuracy: 0.588 - 27s 75ms/step - loss: 0.6650 - accuracy: 0.588 - 27s 75ms/step - loss: 0.6641 - accuracy: 0.589 - 27s 74ms/step - loss: 0.6636 - accuracy: 0.590 - 28s 74ms/step - loss: 0.6638 - accuracy: 0.590 - 28s 74ms/step - loss: 0.6640 - accuracy: 0.590 - 28s 74ms/step - loss: 0.6638 - accuracy: 0.590 - 28s 74ms/step - loss: 0.6628 - accuracy: 0.591 - 28s 74ms/step - loss: 0.6625 - accuracy: 0.592 - 28s 74ms/step - loss: 0.6622 - accuracy: 0.592 - 28s 74ms/step - loss: 0.6623 - accuracy: 0.592 - 28s 74ms/step - loss: 0.6619 - accuracy: 0.592 - 28s 74ms/step - loss: 0.6613 - accuracy: 0.592 - 28s 74ms/step - loss: 0.6606 - accuracy: 0.593 - 28s 74ms/step - loss: 0.6605 - accuracy: 0.593 - 28s 74ms/step - loss: 0.6600 - accuracy: 0.594 - 28s 74ms/step - loss: 0.6592 - accuracy: 0.595 - 28s 74ms/step - loss: 0.6583 - accuracy: 0.595 - 28s 74ms/step - loss: 0.6574 - accuracy: 0.596 - 28s 74ms/step - loss: 0.6572 - accuracy: 0.597 - 28s 74ms/step - loss: 0.6571 - accuracy: 0.597 - 29s 74ms/step - loss: 0.6579 - accuracy: 0.596 - 29s 74ms/step - loss: 0.6576 - accuracy: 0.597 - 29s 74ms/step - loss: 0.6569 - accuracy: 0.598 - 29s 73ms/step - loss: 0.6569 - accuracy: 0.598 - 29s 73ms/step - loss: 0.6563 - accuracy: 0.599 - 29s 73ms/step - loss: 0.6560 - accuracy: 0.599 - 29s 73ms/step - loss: 0.6556 - accuracy: 0.600 - 29s 73ms/step - loss: 0.6560 - accuracy: 0.599 - 29s 73ms/step - loss: 0.6556 - accuracy: 0.600 - 29s 73ms/step - loss: 0.6553 - accuracy: 0.600 - 29s 73ms/step - loss: 0.6551 - accuracy: 0.600 - 29s 73ms/step - loss: 0.6550 - accuracy: 0.601 - 29s 73ms/step - loss: 0.6547 - accuracy: 0.601 - 29s 73ms/step - loss: 0.6540 - accuracy: 0.602 - 29s 73ms/step - loss: 0.6537 - accuracy: 0.602 - 29s 73ms/step - loss: 0.6536 - accuracy: 0.602 - 29s 73ms/step - loss: 0.6530 - accuracy: 0.602 - 29s 73ms/step - loss: 0.6528 - accuracy: 0.603 - 29s 73ms/step - loss: 0.6525 - accuracy: 0.603 - 30s 73ms/step - loss: 0.6517 - accuracy: 0.604 - 30s 73ms/step - loss: 0.6515 - accuracy: 0.604 - 30s 73ms/step - loss: 0.6509 - accuracy: 0.605 - 30s 73ms/step - loss: 0.6502 - accuracy: 0.606 - 30s 73ms/step - loss: 0.6500 - accuracy: 0.606 - 30s 72ms/step - loss: 0.6498 - accuracy: 0.606 - 30s 72ms/step - loss: 0.6494 - accuracy: 0.607 - 30s 72ms/step - loss: 0.6490 - accuracy: 0.607 - 30s 72ms/step - loss: 0.6486 - accuracy: 0.607 - 30s 72ms/step - loss: 0.6487 - accuracy: 0.607 - 30s 72ms/step - loss: 0.6483 - accuracy: 0.607 - 30s 72ms/step - loss: 0.6475 - accuracy: 0.608 - 30s 72ms/step - loss: 0.6468 - accuracy: 0.608 - 30s 72ms/step - loss: 0.6463 - accuracy: 0.609 - 30s 72ms/step - loss: 0.6456 - accuracy: 0.609 - 30s 72ms/step - loss: 0.6457 - accuracy: 0.610 - 30s 72ms/step - loss: 0.6448 - accuracy: 0.610 - 31s 72ms/step - loss: 0.6442 - accuracy: 0.611 - 31s 72ms/step - loss: 0.6435 - accuracy: 0.611 - 31s 72ms/step - loss: 0.6433 - accuracy: 0.612 - 31s 72ms/step - loss: 0.6432 - accuracy: 0.612 - 31s 72ms/step - loss: 0.6429 - accuracy: 0.612 - 31s 72ms/step - loss: 0.6432 - accuracy: 0.612 - 31s 72ms/step - loss: 0.6426 - accuracy: 0.613 - 31s 72ms/step - loss: 0.6422 - accuracy: 0.613 - 31s 72ms/step - loss: 0.6421 - accuracy: 0.613 - 31s 72ms/step - loss: 0.6415 - accuracy: 0.614 - 31s 72ms/step - loss: 0.6415 - accuracy: 0.615 - 31s 72ms/step - loss: 0.6407 - accuracy: 0.615 - 31s 72ms/step - loss: 0.6402 - accuracy: 0.616 - 31s 72ms/step - loss: 0.6399 - accuracy: 0.616 - 31s 72ms/step - loss: 0.6395 - accuracy: 0.616 - 31s 72ms/step - loss: 0.6392 - accuracy: 0.617 - 31s 72ms/step - loss: 0.6395 - accuracy: 0.617 - 31s 71ms/step - loss: 0.6392 - accuracy: 0.617 - 32s 71ms/step - loss: 0.6390 - accuracy: 0.617 - 32s 71ms/step - loss: 0.6387 - accuracy: 0.618 - 32s 71ms/step - loss: 0.6384 - accuracy: 0.619 - 32s 71ms/step - loss: 0.6377 - accuracy: 0.619 - 32s 71ms/step - loss: 0.6374 - accuracy: 0.619 - 32s 71ms/step - loss: 0.6372 - accuracy: 0.620 - 32s 71ms/step - loss: 0.6371 - accuracy: 0.620 - 32s 71ms/step - loss: 0.6368 - accuracy: 0.620 - 32s 71ms/step - loss: 0.6366 - accuracy: 0.620 - 32s 71ms/step - loss: 0.6366 - accuracy: 0.620 - 32s 71ms/step - loss: 0.6367 - accuracy: 0.620 - 32s 71ms/step - loss: 0.6365 - accuracy: 0.620 - 32s 71ms/step - loss: 0.6364 - accuracy: 0.620 - 32s 71ms/step - loss: 0.6362 - accuracy: 0.620 - 32s 71ms/step - loss: 0.6358 - accuracy: 0.621 - 32s 71ms/step - loss: 0.6356 - accuracy: 0.621 - 32s 71ms/step - loss: 0.6350 - accuracy: 0.622 - 33s 71ms/step - loss: 0.6346 - accuracy: 0.622 - 33s 71ms/step - loss: 0.6347 - accuracy: 0.622 - 33s 71ms/step - loss: 0.6345 - accuracy: 0.623 - 33s 71ms/step - loss: 0.6346 - accuracy: 0.623 - 33s 71ms/step - loss: 0.6344 - accuracy: 0.623 - 33s 71ms/step - loss: 0.6341 - accuracy: 0.623 - 33s 71ms/step - loss: 0.6336 - accuracy: 0.624 - 33s 71ms/step - loss: 0.6335 - accuracy: 0.624 - 33s 71ms/step - loss: 0.6331 - accuracy: 0.624 - 33s 71ms/step - loss: 0.6327 - accuracy: 0.625 - 33s 71ms/step - loss: 0.6323 - accuracy: 0.625 - 33s 71ms/step - loss: 0.6319 - accuracy: 0.626 - 33s 71ms/step - loss: 0.6314 - accuracy: 0.626 - 33s 71ms/step - loss: 0.6306 - accuracy: 0.627 - 33s 70ms/step - loss: 0.6299 - accuracy: 0.628 - 33s 70ms/step - loss: 0.6295 - accuracy: 0.628 - 33s 70ms/step - loss: 0.6294 - accuracy: 0.628 - 33s 70ms/step - loss: 0.6289 - accuracy: 0.628 - 33s 70ms/step - loss: 0.6287 - accuracy: 0.629 - 34s 70ms/step - loss: 0.6288 - accuracy: 0.629 - 34s 70ms/step - loss: 0.6293 - accuracy: 0.629 - 34s 70ms/step - loss: 0.6288 - accuracy: 0.629 - 34s 70ms/step - loss: 0.6284 - accuracy: 0.630 - 34s 70ms/step - loss: 0.6275 - accuracy: 0.630 - 34s 70ms/step - loss: 0.6270 - accuracy: 0.631 - 34s 70ms/step - loss: 0.6266 - accuracy: 0.631 - 34s 70ms/step - loss: 0.6259 - accuracy: 0.632 - 34s 70ms/step - loss: 0.6250 - accuracy: 0.632 - 34s 70ms/step - loss: 0.6243 - accuracy: 0.633 - 34s 70ms/step - loss: 0.6234 - accuracy: 0.634 - 34s 70ms/step - loss: 0.6228 - accuracy: 0.634 - 34s 70ms/step - loss: 0.6226 - accuracy: 0.634 - 34s 70ms/step - loss: 0.6226 - accuracy: 0.634 - 34s 70ms/step - loss: 0.6223 - accuracy: 0.634 - 34s 70ms/step - loss: 0.6216 - accuracy: 0.635 - 34s 70ms/step - loss: 0.6216 - accuracy: 0.635 - 34s 70ms/step - loss: 0.6210 - accuracy: 0.635 - 35s 70ms/step - loss: 0.6209 - accuracy: 0.636 - 35s 70ms/step - loss: 0.6202 - accuracy: 0.636 - 35s 70ms/step - loss: 0.6201 - accuracy: 0.636 - 35s 70ms/step - loss: 0.6199 - accuracy: 0.636 - 35s 70ms/step - loss: 0.6201 - accuracy: 0.636 - 35s 70ms/step - loss: 0.6198 - accuracy: 0.637 - 35s 69ms/step - loss: 0.6194 - accuracy: 0.637 - 35s 69ms/step - loss: 0.6188 - accuracy: 0.6380\r\n    669/Unknown - 35s 69ms/step - loss: 0.6187 - accuracy: 0.638 - 35s 69ms/step - loss: 0.6183 - accuracy: 0.638 - 35s 69ms/step - loss: 0.6180 - accuracy: 0.639 - 35s 69ms/step - loss: 0.6174 - accuracy: 0.639 - 35s 69ms/step - loss: 0.6172 - accuracy: 0.639 - 35s 69ms/step - loss: 0.6172 - accuracy: 0.639 - 35s 69ms/step - loss: 0.6168 - accuracy: 0.640 - 35s 69ms/step - loss: 0.6165 - accuracy: 0.640 - 35s 69ms/step - loss: 0.6162 - accuracy: 0.640 - 35s 69ms/step - loss: 0.6159 - accuracy: 0.641 - 35s 69ms/step - loss: 0.6153 - accuracy: 0.641 - 36s 69ms/step - loss: 0.6147 - accuracy: 0.642 - 36s 69ms/step - loss: 0.6139 - accuracy: 0.643 - 36s 69ms/step - loss: 0.6137 - accuracy: 0.643 - 36s 69ms/step - loss: 0.6131 - accuracy: 0.643 - 36s 69ms/step - loss: 0.6128 - accuracy: 0.643 - 36s 69ms/step - loss: 0.6131 - accuracy: 0.643 - 36s 69ms/step - loss: 0.6134 - accuracy: 0.643 - 36s 69ms/step - loss: 0.6134 - accuracy: 0.644 - 36s 69ms/step - loss: 0.6138 - accuracy: 0.644 - 36s 69ms/step - loss: 0.6139 - accuracy: 0.644 - 36s 69ms/step - loss: 0.6134 - accuracy: 0.644 - 36s 69ms/step - loss: 0.6131 - accuracy: 0.645 - 36s 69ms/step - loss: 0.6140 - accuracy: 0.644 - 36s 69ms/step - loss: 0.6144 - accuracy: 0.644 - 36s 69ms/step - loss: 0.6155 - accuracy: 0.644 - 36s 69ms/step - loss: 0.6152 - accuracy: 0.644 - 36s 69ms/step - loss: 0.6154 - accuracy: 0.644 - 36s 69ms/step - loss: 0.6155 - accuracy: 0.644 - 36s 69ms/step - loss: 0.6161 - accuracy: 0.644 - 37s 69ms/step - loss: 0.6162 - accuracy: 0.644 - 37s 69ms/step - loss: 0.6165 - accuracy: 0.644 - 37s 69ms/step - loss: 0.6172 - accuracy: 0.644 - 37s 69ms/step - loss: 0.6173 - accuracy: 0.644 - 37s 69ms/step - loss: 0.6172 - accuracy: 0.644 - 37s 68ms/step - loss: 0.6182 - accuracy: 0.644 - 37s 68ms/step - loss: 0.6181 - accuracy: 0.644 - 37s 68ms/step - loss: 0.6182 - accuracy: 0.644 - 37s 68ms/step - loss: 0.6180 - accuracy: 0.644 - 37s 68ms/step - loss: 0.6178 - accuracy: 0.644 - 37s 68ms/step - loss: 0.6181 - accuracy: 0.644 - 37s 68ms/step - loss: 0.6177 - accuracy: 0.644 - 37s 68ms/step - loss: 0.6178 - accuracy: 0.644 - 37s 68ms/step - loss: 0.6172 - accuracy: 0.644 - 37s 68ms/step - loss: 0.6167 - accuracy: 0.645 - 37s 68ms/step - loss: 0.6162 - accuracy: 0.645 - 38s 68ms/step - loss: 0.6161 - accuracy: 0.645 - 38s 68ms/step - loss: 0.6166 - accuracy: 0.645 - 38s 68ms/step - loss: 0.6166 - accuracy: 0.645 - 38s 68ms/step - loss: 0.6173 - accuracy: 0.645 - 38s 68ms/step - loss: 0.6179 - accuracy: 0.645 - 38s 68ms/step - loss: 0.6175 - accuracy: 0.646 - 38s 68ms/step - loss: 0.6182 - accuracy: 0.645 - 38s 68ms/step - loss: 0.6176 - accuracy: 0.646 - 38s 68ms/step - loss: 0.6180 - accuracy: 0.646 - 38s 68ms/step - loss: 0.6177 - accuracy: 0.646 - 38s 68ms/step - loss: 0.6183 - accuracy: 0.646 - 38s 68ms/step - loss: 0.6188 - accuracy: 0.645 - 38s 68ms/step - loss: 0.6184 - accuracy: 0.646 - 38s 68ms/step - loss: 0.6183 - accuracy: 0.646 - 38s 68ms/step - loss: 0.6180 - accuracy: 0.646 - 38s 68ms/step - loss: 0.6178 - accuracy: 0.646 - 38s 68ms/step - loss: 0.6173 - accuracy: 0.647 - 38s 68ms/step - loss: 0.6168 - accuracy: 0.647 - 39s 68ms/step - loss: 0.6166 - accuracy: 0.647 - 39s 68ms/step - loss: 0.6161 - accuracy: 0.647 - 39s 68ms/step - loss: 0.6160 - accuracy: 0.648 - 39s 68ms/step - loss: 0.6159 - accuracy: 0.648 - 39s 68ms/step - loss: 0.6157 - accuracy: 0.648 - 39s 68ms/step - loss: 0.6153 - accuracy: 0.648 - 39s 68ms/step - loss: 0.6153 - accuracy: 0.648 - 39s 68ms/step - loss: 0.6147 - accuracy: 0.649 - 39s 68ms/step - loss: 0.6148 - accuracy: 0.649 - 39s 68ms/step - loss: 0.6146 - accuracy: 0.649 - 39s 68ms/step - loss: 0.6142 - accuracy: 0.649 - 39s 68ms/step - loss: 0.6139 - accuracy: 0.649 - 39s 68ms/step - loss: 0.6139 - accuracy: 0.650 - 39s 68ms/step - loss: 0.6138 - accuracy: 0.650 - 39s 68ms/step - loss: 0.6135 - accuracy: 0.650 - 39s 68ms/step - loss: 0.6139 - accuracy: 0.650 - 39s 68ms/step - loss: 0.6136 - accuracy: 0.650 - 39s 68ms/step - loss: 0.6137 - accuracy: 0.650 - 40s 68ms/step - loss: 0.6141 - accuracy: 0.649 - 40s 68ms/step - loss: 0.6139 - accuracy: 0.650 - 40s 68ms/step - loss: 0.6134 - accuracy: 0.650 - 40s 68ms/step - loss: 0.6131 - accuracy: 0.650 - 40s 68ms/step - loss: 0.6130 - accuracy: 0.650 - 40s 67ms/step - loss: 0.6125 - accuracy: 0.651 - 40s 67ms/step - loss: 0.6124 - accuracy: 0.651 - 40s 67ms/step - loss: 0.6124 - accuracy: 0.651 - 40s 67ms/step - loss: 0.6122 - accuracy: 0.651 - 40s 67ms/step - loss: 0.6123 - accuracy: 0.651 - 40s 67ms/step - loss: 0.6121 - accuracy: 0.651 - 40s 67ms/step - loss: 0.6120 - accuracy: 0.652 - 40s 67ms/step - loss: 0.6117 - accuracy: 0.652 - 40s 67ms/step - loss: 0.6116 - accuracy: 0.652 - 40s 67ms/step - loss: 0.6115 - accuracy: 0.652 - 40s 67ms/step - loss: 0.6114 - accuracy: 0.652 - 40s 67ms/step - loss: 0.6109 - accuracy: 0.652 - 40s 67ms/step - loss: 0.6109 - accuracy: 0.652 - 40s 67ms/step - loss: 0.6109 - accuracy: 0.652 - 41s 67ms/step - loss: 0.6109 - accuracy: 0.652 - 41s 67ms/step - loss: 0.6108 - accuracy: 0.652 - 41s 67ms/step - loss: 0.6106 - accuracy: 0.652 - 41s 67ms/step - loss: 0.6107 - accuracy: 0.652 - 41s 67ms/step - loss: 0.6105 - accuracy: 0.653 - 41s 67ms/step - loss: 0.6101 - accuracy: 0.653 - 41s 67ms/step - loss: 0.6102 - accuracy: 0.653 - 41s 67ms/step - loss: 0.6097 - accuracy: 0.653 - 41s 67ms/step - loss: 0.6096 - accuracy: 0.653 - 41s 67ms/step - loss: 0.6095 - accuracy: 0.654 - 41s 67ms/step - loss: 0.6093 - accuracy: 0.654 - 41s 67ms/step - loss: 0.6091 - accuracy: 0.654 - 41s 67ms/step - loss: 0.6092 - accuracy: 0.654 - 41s 67ms/step - loss: 0.6086 - accuracy: 0.654 - 41s 67ms/step - loss: 0.6080 - accuracy: 0.655 - 41s 67ms/step - loss: 0.6079 - accuracy: 0.655 - 41s 67ms/step - loss: 0.6075 - accuracy: 0.655 - 41s 67ms/step - loss: 0.6081 - accuracy: 0.655 - 42s 67ms/step - loss: 0.6077 - accuracy: 0.655 - 42s 67ms/step - loss: 0.6074 - accuracy: 0.655 - 42s 67ms/step - loss: 0.6075 - accuracy: 0.655 - 42s 67ms/step - loss: 0.6072 - accuracy: 0.656 - 42s 67ms/step - loss: 0.6070 - accuracy: 0.656 - 42s 67ms/step - loss: 0.6070 - accuracy: 0.656 - 42s 67ms/step - loss: 0.6063 - accuracy: 0.656 - 42s 67ms/step - loss: 0.6059 - accuracy: 0.657 - 42s 67ms/step - loss: 0.6058 - accuracy: 0.657 - 42s 67ms/step - loss: 0.6056 - accuracy: 0.657 - 42s 67ms/step - loss: 0.6051 - accuracy: 0.657 - 42s 67ms/step - loss: 0.6050 - accuracy: 0.658 - 42s 67ms/step - loss: 0.6044 - accuracy: 0.658 - 42s 67ms/step - loss: 0.6044 - accuracy: 0.658 - 42s 67ms/step - loss: 0.6040 - accuracy: 0.659 - 42s 67ms/step - loss: 0.6044 - accuracy: 0.658 - 42s 66ms/step - loss: 0.6041 - accuracy: 0.658 - 42s 66ms/step - loss: 0.6040 - accuracy: 0.659 - 43s 66ms/step - loss: 0.6037 - accuracy: 0.659 - 43s 66ms/step - loss: 0.6034 - accuracy: 0.659 - 43s 66ms/step - loss: 0.6030 - accuracy: 0.660 - 43s 66ms/step - loss: 0.6029 - accuracy: 0.660 - 43s 66ms/step - loss: 0.6023 - accuracy: 0.660 - 43s 66ms/step - loss: 0.6019 - accuracy: 0.660 - 43s 66ms/step - loss: 0.6018 - accuracy: 0.661 - 43s 66ms/step - loss: 0.6020 - accuracy: 0.660 - 43s 66ms/step - loss: 0.6016 - accuracy: 0.661 - 43s 66ms/step - loss: 0.6016 - accuracy: 0.661 - 43s 66ms/step - loss: 0.6016 - accuracy: 0.661 - 43s 66ms/step - loss: 0.6012 - accuracy: 0.661 - 43s 66ms/step - loss: 0.6013 - accuracy: 0.661 - 43s 66ms/step - loss: 0.6011 - accuracy: 0.662 - 43s 66ms/step - loss: 0.6009 - accuracy: 0.662 - 43s 66ms/step - loss: 0.6007 - accuracy: 0.662 - 43s 66ms/step - loss: 0.6001 - accuracy: 0.662 - 44s 66ms/step - loss: 0.5997 - accuracy: 0.662 - 44s 66ms/step - loss: 0.5996 - accuracy: 0.662 - 44s 66ms/step - loss: 0.5992 - accuracy: 0.663 - 44s 66ms/step - loss: 0.5991 - accuracy: 0.663 - 44s 66ms/step - loss: 0.5989 - accuracy: 0.663 - 44s 66ms/step - loss: 0.5985 - accuracy: 0.663 - 44s 66ms/step - loss: 0.5984 - accuracy: 0.664 - 44s 66ms/step - loss: 0.5982 - accuracy: 0.664 - 44s 66ms/step - loss: 0.5977 - accuracy: 0.664 - 44s 66ms/step - loss: 0.5976 - accuracy: 0.664 - 44s 66ms/step - loss: 0.5970 - accuracy: 0.665 - 44s 66ms/step - loss: 0.5974 - accuracy: 0.665 - 44s 66ms/step - loss: 0.5973 - accuracy: 0.6649\r\n    836/Unknown - 44s 66ms/step - loss: 0.5977 - accuracy: 0.664 - 44s 66ms/step - loss: 0.5973 - accuracy: 0.665 - 44s 66ms/step - loss: 0.5967 - accuracy: 0.665 - 44s 66ms/step - loss: 0.5965 - accuracy: 0.665 - 44s 66ms/step - loss: 0.5961 - accuracy: 0.666 - 44s 66ms/step - loss: 0.5959 - accuracy: 0.666 - 44s 66ms/step - loss: 0.5959 - accuracy: 0.666 - 44s 66ms/step - loss: 0.5957 - accuracy: 0.666 - 45s 66ms/step - loss: 0.5956 - accuracy: 0.666 - 45s 66ms/step - loss: 0.5955 - accuracy: 0.666 - 45s 66ms/step - loss: 0.5953 - accuracy: 0.666 - 45s 66ms/step - loss: 0.5950 - accuracy: 0.666 - 45s 66ms/step - loss: 0.5951 - accuracy: 0.666 - 45s 66ms/step - loss: 0.5952 - accuracy: 0.666 - 45s 66ms/step - loss: 0.5951 - accuracy: 0.666 - 45s 66ms/step - loss: 0.5948 - accuracy: 0.667 - 45s 66ms/step - loss: 0.5946 - accuracy: 0.667 - 45s 66ms/step - loss: 0.5943 - accuracy: 0.667 - 45s 66ms/step - loss: 0.5940 - accuracy: 0.667 - 45s 66ms/step - loss: 0.5941 - accuracy: 0.667 - 45s 66ms/step - loss: 0.5942 - accuracy: 0.667 - 45s 66ms/step - loss: 0.5944 - accuracy: 0.667 - 45s 66ms/step - loss: 0.5949 - accuracy: 0.667 - 45s 66ms/step - loss: 0.5951 - accuracy: 0.667 - 46s 66ms/step - loss: 0.5956 - accuracy: 0.667 - 46s 66ms/step - loss: 0.5965 - accuracy: 0.667 - 46s 66ms/step - loss: 0.5964 - accuracy: 0.667 - 46s 66ms/step - loss: 0.5961 - accuracy: 0.667 - 46s 66ms/step - loss: 0.5958 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5955 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5953 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5953 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5953 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5953 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5953 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5950 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5947 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5951 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5947 - accuracy: 0.668 - 46s 66ms/step - loss: 0.5947 - accuracy: 0.668 - 47s 66ms/step - loss: 0.5945 - accuracy: 0.669 - 47s 66ms/step - loss: 0.5942 - accuracy: 0.669 - 47s 65ms/step - loss: 0.5938 - accuracy: 0.669 - 47s 65ms/step - loss: 0.5938 - accuracy: 0.669 - 47s 65ms/step - loss: 0.5936 - accuracy: 0.670 - 47s 65ms/step - loss: 0.5934 - accuracy: 0.670 - 47s 65ms/step - loss: 0.5932 - accuracy: 0.670 - 47s 65ms/step - loss: 0.5933 - accuracy: 0.670 - 47s 65ms/step - loss: 0.5937 - accuracy: 0.670 - 47s 65ms/step - loss: 0.5936 - accuracy: 0.670 - 47s 65ms/step - loss: 0.5934 - accuracy: 0.670 - 47s 65ms/step - loss: 0.5932 - accuracy: 0.670 - 47s 65ms/step - loss: 0.5930 - accuracy: 0.670 - 47s 65ms/step - loss: 0.5929 - accuracy: 0.671 - 47s 65ms/step - loss: 0.5928 - accuracy: 0.671 - 47s 65ms/step - loss: 0.5927 - accuracy: 0.671 - 47s 65ms/step - loss: 0.5926 - accuracy: 0.671 - 47s 65ms/step - loss: 0.5923 - accuracy: 0.671 - 47s 65ms/step - loss: 0.5923 - accuracy: 0.671 - 47s 65ms/step - loss: 0.5923 - accuracy: 0.672 - 48s 65ms/step - loss: 0.5922 - accuracy: 0.672 - 48s 65ms/step - loss: 0.5922 - accuracy: 0.672 - 48s 65ms/step - loss: 0.5920 - accuracy: 0.672 - 48s 65ms/step - loss: 0.5919 - accuracy: 0.672 - 48s 65ms/step - loss: 0.5917 - accuracy: 0.672 - 48s 65ms/step - loss: 0.5912 - accuracy: 0.672 - 48s 65ms/step - loss: 0.5911 - accuracy: 0.673 - 48s 65ms/step - loss: 0.5910 - accuracy: 0.673 - 48s 65ms/step - loss: 0.5909 - accuracy: 0.673 - 48s 65ms/step - loss: 0.5910 - accuracy: 0.673 - 48s 65ms/step - loss: 0.5909 - accuracy: 0.673 - 48s 65ms/step - loss: 0.5910 - accuracy: 0.673 - 48s 65ms/step - loss: 0.5910 - accuracy: 0.673 - 48s 65ms/step - loss: 0.5909 - accuracy: 0.673 - 48s 65ms/step - loss: 0.5909 - accuracy: 0.673 - 48s 65ms/step - loss: 0.5908 - accuracy: 0.673 - 48s 65ms/step - loss: 0.5904 - accuracy: 0.674 - 48s 65ms/step - loss: 0.5905 - accuracy: 0.674 - 48s 65ms/step - loss: 0.5903 - accuracy: 0.674 - 49s 65ms/step - loss: 0.5897 - accuracy: 0.674 - 49s 65ms/step - loss: 0.5895 - accuracy: 0.675 - 49s 65ms/step - loss: 0.5898 - accuracy: 0.674 - 49s 65ms/step - loss: 0.5895 - accuracy: 0.675 - 49s 65ms/step - loss: 0.5896 - accuracy: 0.675 - 49s 65ms/step - loss: 0.5893 - accuracy: 0.675 - 49s 65ms/step - loss: 0.5888 - accuracy: 0.675 - 49s 65ms/step - loss: 0.5888 - accuracy: 0.675 - 49s 65ms/step - loss: 0.5886 - accuracy: 0.676 - 49s 65ms/step - loss: 0.5882 - accuracy: 0.676 - 49s 65ms/step - loss: 0.5884 - accuracy: 0.676 - 49s 65ms/step - loss: 0.5885 - accuracy: 0.676 - 49s 65ms/step - loss: 0.5883 - accuracy: 0.676 - 49s 65ms/step - loss: 0.5880 - accuracy: 0.676 - 49s 65ms/step - loss: 0.5886 - accuracy: 0.676 - 49s 65ms/step - loss: 0.5884 - accuracy: 0.676 - 49s 65ms/step - loss: 0.5884 - accuracy: 0.676 - 50s 65ms/step - loss: 0.5881 - accuracy: 0.676 - 50s 65ms/step - loss: 0.5879 - accuracy: 0.676 - 50s 65ms/step - loss: 0.5876 - accuracy: 0.677 - 50s 65ms/step - loss: 0.5873 - accuracy: 0.677 - 50s 65ms/step - loss: 0.5873 - accuracy: 0.677 - 50s 65ms/step - loss: 0.5872 - accuracy: 0.677 - 50s 65ms/step - loss: 0.5872 - accuracy: 0.677 - 50s 65ms/step - loss: 0.5869 - accuracy: 0.678 - 50s 65ms/step - loss: 0.5868 - accuracy: 0.678 - 50s 65ms/step - loss: 0.5869 - accuracy: 0.678 - 50s 65ms/step - loss: 0.5869 - accuracy: 0.678 - 50s 65ms/step - loss: 0.5868 - accuracy: 0.678 - 50s 65ms/step - loss: 0.5864 - accuracy: 0.678 - 50s 65ms/step - loss: 0.5861 - accuracy: 0.678 - 50s 65ms/step - loss: 0.5858 - accuracy: 0.679 - 50s 65ms/step - loss: 0.5860 - accuracy: 0.678 - 50s 65ms/step - loss: 0.5858 - accuracy: 0.679 - 51s 65ms/step - loss: 0.5855 - accuracy: 0.679 - 51s 65ms/step - loss: 0.5853 - accuracy: 0.679 - 51s 65ms/step - loss: 0.5850 - accuracy: 0.679 - 51s 64ms/step - loss: 0.5852 - accuracy: 0.679 - 51s 64ms/step - loss: 0.5850 - accuracy: 0.679 - 51s 64ms/step - loss: 0.5852 - accuracy: 0.679 - 51s 64ms/step - loss: 0.5851 - accuracy: 0.679 - 51s 64ms/step - loss: 0.5849 - accuracy: 0.679 - 51s 64ms/step - loss: 0.5846 - accuracy: 0.680 - 51s 64ms/step - loss: 0.5843 - accuracy: 0.680 - 51s 64ms/step - loss: 0.5842 - accuracy: 0.680 - 51s 64ms/step - loss: 0.5842 - accuracy: 0.680 - 51s 64ms/step - loss: 0.5839 - accuracy: 0.680 - 51s 64ms/step - loss: 0.5843 - accuracy: 0.680 - 51s 64ms/step - loss: 0.5842 - accuracy: 0.680 - 51s 65ms/step - loss: 0.5839 - accuracy: 0.680 - 52s 64ms/step - loss: 0.5836 - accuracy: 0.681 - 52s 64ms/step - loss: 0.5835 - accuracy: 0.681 - 52s 64ms/step - loss: 0.5833 - accuracy: 0.681 - 52s 64ms/step - loss: 0.5831 - accuracy: 0.681 - 52s 64ms/step - loss: 0.5831 - accuracy: 0.681 - 52s 64ms/step - loss: 0.5834 - accuracy: 0.681 - 52s 65ms/step - loss: 0.5834 - accuracy: 0.681 - 52s 65ms/step - loss: 0.5832 - accuracy: 0.681 - 52s 65ms/step - loss: 0.5833 - accuracy: 0.681 - 52s 64ms/step - loss: 0.5834 - accuracy: 0.681 - 52s 64ms/step - loss: 0.5830 - accuracy: 0.681 - 52s 64ms/step - loss: 0.5831 - accuracy: 0.681 - 52s 64ms/step - loss: 0.5827 - accuracy: 0.682 - 52s 64ms/step - loss: 0.5828 - accuracy: 0.682 - 52s 64ms/step - loss: 0.5829 - accuracy: 0.682 - 52s 64ms/step - loss: 0.5830 - accuracy: 0.681 - 53s 64ms/step - loss: 0.5832 - accuracy: 0.682 - 53s 64ms/step - loss: 0.5831 - accuracy: 0.682 - 53s 64ms/step - loss: 0.5832 - accuracy: 0.682 - 53s 64ms/step - loss: 0.5832 - accuracy: 0.682 - 53s 64ms/step - loss: 0.5830 - accuracy: 0.682 - 53s 64ms/step - loss: 0.5831 - accuracy: 0.682 - 53s 64ms/step - loss: 0.5831 - accuracy: 0.682 - 53s 64ms/step - loss: 0.5827 - accuracy: 0.682 - 53s 64ms/step - loss: 0.5825 - accuracy: 0.682 - 53s 64ms/step - loss: 0.5825 - accuracy: 0.682 - 53s 64ms/step - loss: 0.5822 - accuracy: 0.683 - 53s 64ms/step - loss: 0.5818 - accuracy: 0.683 - 53s 64ms/step - loss: 0.5819 - accuracy: 0.683 - 53s 64ms/step - loss: 0.5821 - accuracy: 0.683 - 53s 64ms/step - loss: 0.5817 - accuracy: 0.683 - 53s 64ms/step - loss: 0.5817 - accuracy: 0.683 - 53s 64ms/step - loss: 0.5817 - accuracy: 0.683 - 53s 64ms/step - loss: 0.5813 - accuracy: 0.684 - 53s 64ms/step - loss: 0.5813 - accuracy: 0.684 - 54s 64ms/step - loss: 0.5808 - accuracy: 0.684 - 54s 64ms/step - loss: 0.5804 - accuracy: 0.685 - 54s 64ms/step - loss: 0.5800 - accuracy: 0.6855\r\n   1003/Unknown - 54s 64ms/step - loss: 0.5801 - accuracy: 0.685 - 54s 64ms/step - loss: 0.5798 - accuracy: 0.685 - 54s 64ms/step - loss: 0.5797 - accuracy: 0.685 - 54s 64ms/step - loss: 0.5794 - accuracy: 0.686 - 54s 64ms/step - loss: 0.5793 - accuracy: 0.686 - 54s 64ms/step - loss: 0.5791 - accuracy: 0.686 - 54s 64ms/step - loss: 0.5790 - accuracy: 0.686 - 54s 64ms/step - loss: 0.5787 - accuracy: 0.686 - 54s 64ms/step - loss: 0.5784 - accuracy: 0.686 - 54s 64ms/step - loss: 0.5781 - accuracy: 0.687 - 54s 64ms/step - loss: 0.5780 - accuracy: 0.687 - 54s 64ms/step - loss: 0.5778 - accuracy: 0.687 - 54s 64ms/step - loss: 0.5776 - accuracy: 0.687 - 54s 64ms/step - loss: 0.5774 - accuracy: 0.687 - 54s 64ms/step - loss: 0.5771 - accuracy: 0.687 - 54s 64ms/step - loss: 0.5767 - accuracy: 0.688 - 55s 64ms/step - loss: 0.5765 - accuracy: 0.688 - 55s 64ms/step - loss: 0.5760 - accuracy: 0.688 - 55s 64ms/step - loss: 0.5759 - accuracy: 0.688 - 55s 64ms/step - loss: 0.5755 - accuracy: 0.689 - 55s 64ms/step - loss: 0.5753 - accuracy: 0.689 - 55s 64ms/step - loss: 0.5751 - accuracy: 0.689 - 55s 64ms/step - loss: 0.5750 - accuracy: 0.689 - 55s 64ms/step - loss: 0.5749 - accuracy: 0.689 - 55s 64ms/step - loss: 0.5747 - accuracy: 0.689 - 55s 64ms/step - loss: 0.5745 - accuracy: 0.690 - 55s 64ms/step - loss: 0.5744 - accuracy: 0.690 - 55s 64ms/step - loss: 0.5749 - accuracy: 0.690 - 55s 64ms/step - loss: 0.5748 - accuracy: 0.690 - 55s 64ms/step - loss: 0.5747 - accuracy: 0.690 - 55s 64ms/step - loss: 0.5746 - accuracy: 0.690 - 55s 64ms/step - loss: 0.5746 - accuracy: 0.690 - 55s 64ms/step - loss: 0.5745 - accuracy: 0.690 - 56s 64ms/step - loss: 0.5744 - accuracy: 0.690 - 56s 64ms/step - loss: 0.5741 - accuracy: 0.690 - 56s 64ms/step - loss: 0.5740 - accuracy: 0.690 - 56s 64ms/step - loss: 0.5740 - accuracy: 0.690 - 56s 64ms/step - loss: 0.5740 - accuracy: 0.690 - 56s 64ms/step - loss: 0.5740 - accuracy: 0.690 - 56s 64ms/step - loss: 0.5736 - accuracy: 0.691 - 56s 64ms/step - loss: 0.5733 - accuracy: 0.691 - 56s 64ms/step - loss: 0.5734 - accuracy: 0.691 - 56s 64ms/step - loss: 0.5735 - accuracy: 0.691 - 56s 64ms/step - loss: 0.5734 - accuracy: 0.691 - 56s 64ms/step - loss: 0.5731 - accuracy: 0.691 - 56s 64ms/step - loss: 0.5731 - accuracy: 0.691 - 56s 64ms/step - loss: 0.5730 - accuracy: 0.691 - 56s 64ms/step - loss: 0.5727 - accuracy: 0.691 - 56s 64ms/step - loss: 0.5727 - accuracy: 0.691 - 56s 64ms/step - loss: 0.5726 - accuracy: 0.692 - 56s 64ms/step - loss: 0.5724 - accuracy: 0.692 - 56s 64ms/step - loss: 0.5723 - accuracy: 0.692 - 57s 64ms/step - loss: 0.5722 - accuracy: 0.692 - 57s 64ms/step - loss: 0.5718 - accuracy: 0.692 - 57s 64ms/step - loss: 0.5716 - accuracy: 0.692 - 57s 64ms/step - loss: 0.5716 - accuracy: 0.693 - 57s 64ms/step - loss: 0.5713 - accuracy: 0.693 - 57s 64ms/step - loss: 0.5712 - accuracy: 0.693 - 57s 64ms/step - loss: 0.5709 - accuracy: 0.693 - 57s 64ms/step - loss: 0.5708 - accuracy: 0.693 - 57s 64ms/step - loss: 0.5705 - accuracy: 0.693 - 57s 64ms/step - loss: 0.5703 - accuracy: 0.694 - 57s 64ms/step - loss: 0.5706 - accuracy: 0.693 - 57s 64ms/step - loss: 0.5704 - accuracy: 0.694 - 57s 63ms/step - loss: 0.5705 - accuracy: 0.694 - 57s 63ms/step - loss: 0.5704 - accuracy: 0.694 - 57s 64ms/step - loss: 0.5702 - accuracy: 0.694 - 57s 64ms/step - loss: 0.5702 - accuracy: 0.694 - 57s 63ms/step - loss: 0.5699 - accuracy: 0.694 - 58s 63ms/step - loss: 0.5696 - accuracy: 0.694 - 58s 63ms/step - loss: 0.5696 - accuracy: 0.694 - 58s 63ms/step - loss: 0.5696 - accuracy: 0.694 - 58s 63ms/step - loss: 0.5693 - accuracy: 0.694 - 58s 63ms/step - loss: 0.5692 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5689 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5690 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5690 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5688 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5686 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5688 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5686 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5685 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5685 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5685 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5685 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5684 - accuracy: 0.695 - 58s 63ms/step - loss: 0.5685 - accuracy: 0.695 - 59s 63ms/step - loss: 0.5687 - accuracy: 0.695 - 59s 63ms/step - loss: 0.5688 - accuracy: 0.695 - 59s 63ms/step - loss: 0.5687 - accuracy: 0.695 - 59s 63ms/step - loss: 0.5688 - accuracy: 0.695 - 59s 63ms/step - loss: 0.5685 - accuracy: 0.695 - 59s 63ms/step - loss: 0.5684 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5685 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5689 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5690 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5690 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5687 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5686 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5682 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5683 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5682 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5682 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5686 - accuracy: 0.696 - 59s 63ms/step - loss: 0.5684 - accuracy: 0.697 - 60s 63ms/step - loss: 0.5682 - accuracy: 0.697 - 60s 63ms/step - loss: 0.5678 - accuracy: 0.697 - 60s 63ms/step - loss: 0.5679 - accuracy: 0.697 - 60s 63ms/step - loss: 0.5678 - accuracy: 0.697 - 60s 63ms/step - loss: 0.5675 - accuracy: 0.697 - 60s 63ms/step - loss: 0.5672 - accuracy: 0.698 - 60s 63ms/step - loss: 0.5671 - accuracy: 0.698 - 60s 63ms/step - loss: 0.5669 - accuracy: 0.698 - 60s 63ms/step - loss: 0.5669 - accuracy: 0.698 - 60s 63ms/step - loss: 0.5665 - accuracy: 0.698 - 60s 63ms/step - loss: 0.5666 - accuracy: 0.698 - 60s 63ms/step - loss: 0.5664 - accuracy: 0.698 - 60s 63ms/step - loss: 0.5664 - accuracy: 0.699 - 60s 63ms/step - loss: 0.5665 - accuracy: 0.698 - 60s 63ms/step - loss: 0.5663 - accuracy: 0.699 - 60s 63ms/step - loss: 0.5661 - accuracy: 0.699 - 60s 63ms/step - loss: 0.5658 - accuracy: 0.699 - 61s 63ms/step - loss: 0.5657 - accuracy: 0.699 - 61s 63ms/step - loss: 0.5655 - accuracy: 0.699 - 61s 63ms/step - loss: 0.5654 - accuracy: 0.699 - 61s 63ms/step - loss: 0.5651 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5651 - accuracy: 0.699 - 61s 63ms/step - loss: 0.5651 - accuracy: 0.699 - 61s 63ms/step - loss: 0.5650 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5648 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5646 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5646 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5645 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5645 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5642 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5641 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5639 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5638 - accuracy: 0.700 - 61s 63ms/step - loss: 0.5639 - accuracy: 0.700 - 62s 63ms/step - loss: 0.5636 - accuracy: 0.701 - 62s 63ms/step - loss: 0.5637 - accuracy: 0.701 - 62s 63ms/step - loss: 0.5636 - accuracy: 0.701 - 62s 63ms/step - loss: 0.5633 - accuracy: 0.701 - 62s 63ms/step - loss: 0.5631 - accuracy: 0.701 - 62s 63ms/step - loss: 0.5628 - accuracy: 0.701 - 62s 63ms/step - loss: 0.5625 - accuracy: 0.702 - 62s 63ms/step - loss: 0.5624 - accuracy: 0.702 - 62s 63ms/step - loss: 0.5621 - accuracy: 0.702 - 62s 63ms/step - loss: 0.5619 - accuracy: 0.702 - 62s 63ms/step - loss: 0.5621 - accuracy: 0.702 - 62s 63ms/step - loss: 0.5619 - accuracy: 0.702 - 62s 63ms/step - loss: 0.5619 - accuracy: 0.702 - 62s 63ms/step - loss: 0.5618 - accuracy: 0.702 - 62s 63ms/step - loss: 0.5616 - accuracy: 0.702 - 62s 63ms/step - loss: 0.5615 - accuracy: 0.702 - 62s 63ms/step - loss: 0.5612 - accuracy: 0.703 - 62s 63ms/step - loss: 0.5612 - accuracy: 0.703 - 63s 63ms/step - loss: 0.5611 - accuracy: 0.703 - 63s 63ms/step - loss: 0.5608 - accuracy: 0.703 - 63s 63ms/step - loss: 0.5609 - accuracy: 0.703 - 63s 63ms/step - loss: 0.5606 - accuracy: 0.703 - 63s 63ms/step - loss: 0.5602 - accuracy: 0.703 - 63s 63ms/step - loss: 0.5601 - accuracy: 0.703 - 63s 63ms/step - loss: 0.5600 - accuracy: 0.703 - 63s 63ms/step - loss: 0.5597 - accuracy: 0.704 - 63s 63ms/step - loss: 0.5594 - accuracy: 0.704 - 63s 63ms/step - loss: 0.5592 - accuracy: 0.7046\r\n   1170/Unknown - 63s 63ms/step - loss: 0.5590 - accuracy: 0.704 - 63s 63ms/step - loss: 0.5590 - accuracy: 0.704 - 63s 63ms/step - loss: 0.5587 - accuracy: 0.704 - 63s 63ms/step - loss: 0.5586 - accuracy: 0.705 - 63s 63ms/step - loss: 0.5583 - accuracy: 0.705 - 63s 63ms/step - loss: 0.5580 - accuracy: 0.705 - 63s 63ms/step - loss: 0.5577 - accuracy: 0.705 - 64s 63ms/step - loss: 0.5579 - accuracy: 0.705 - 64s 63ms/step - loss: 0.5576 - accuracy: 0.705 - 64s 63ms/step - loss: 0.5575 - accuracy: 0.705 - 64s 63ms/step - loss: 0.5571 - accuracy: 0.706 - 64s 63ms/step - loss: 0.5568 - accuracy: 0.706 - 64s 63ms/step - loss: 0.5565 - accuracy: 0.706 - 64s 63ms/step - loss: 0.5563 - accuracy: 0.706 - 64s 63ms/step - loss: 0.5561 - accuracy: 0.706 - 64s 63ms/step - loss: 0.5559 - accuracy: 0.706 - 64s 63ms/step - loss: 0.5557 - accuracy: 0.707 - 64s 63ms/step - loss: 0.5559 - accuracy: 0.707 - 64s 63ms/step - loss: 0.5557 - accuracy: 0.707 - 64s 63ms/step - loss: 0.5555 - accuracy: 0.707 - 64s 63ms/step - loss: 0.5552 - accuracy: 0.707 - 64s 63ms/step - loss: 0.5557 - accuracy: 0.707 - 64s 63ms/step - loss: 0.5556 - accuracy: 0.707 - 64s 63ms/step - loss: 0.5557 - accuracy: 0.707 - 64s 63ms/step - loss: 0.5561 - accuracy: 0.707 - 64s 63ms/step - loss: 0.5556 - accuracy: 0.707 - 65s 63ms/step - loss: 0.5566 - accuracy: 0.707 - 65s 63ms/step - loss: 0.5568 - accuracy: 0.707 - 65s 63ms/step - loss: 0.5573 - accuracy: 0.707 - 65s 63ms/step - loss: 0.5578 - accuracy: 0.706 - 65s 63ms/step - loss: 0.5577 - accuracy: 0.706 - 65s 63ms/step - loss: 0.5585 - accuracy: 0.706 - 65s 63ms/step - loss: 0.5587 - accuracy: 0.706 - 65s 63ms/step - loss: 0.5587 - accuracy: 0.706 - 65s 63ms/step - loss: 0.5594 - accuracy: 0.706 - 65s 63ms/step - loss: 0.5598 - accuracy: 0.706 - 65s 63ms/step - loss: 0.5598 - accuracy: 0.706 - 65s 63ms/step - loss: 0.5597 - accuracy: 0.706 - 65s 63ms/step - loss: 0.5599 - accuracy: 0.706 - 65s 62ms/step - loss: 0.5602 - accuracy: 0.706 - 65s 63ms/step - loss: 0.5602 - accuracy: 0.706 - 65s 62ms/step - loss: 0.5601 - accuracy: 0.706 - 65s 62ms/step - loss: 0.5600 - accuracy: 0.706 - 65s 62ms/step - loss: 0.5597 - accuracy: 0.706 - 65s 62ms/step - loss: 0.5597 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5598 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5599 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5598 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5600 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5599 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5597 - accuracy: 0.707 - 66s 62ms/step - loss: 0.5601 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5602 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5603 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5606 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5607 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5608 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5610 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5608 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5607 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5607 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5606 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5607 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5606 - accuracy: 0.706 - 66s 62ms/step - loss: 0.5607 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5606 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5606 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5606 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5607 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5605 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5604 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5605 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5605 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5605 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5604 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5602 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5601 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5603 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5603 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5603 - accuracy: 0.706 - 67s 62ms/step - loss: 0.5601 - accuracy: 0.707 - 67s 62ms/step - loss: 0.5601 - accuracy: 0.707 - 67s 62ms/step - loss: 0.5600 - accuracy: 0.707 - 68s 62ms/step - loss: 0.5599 - accuracy: 0.707 - 68s 62ms/step - loss: 0.5599 - accuracy: 0.707 - 68s 62ms/step - loss: 0.5598 - accuracy: 0.707 - 68s 62ms/step - loss: 0.5597 - accuracy: 0.707 - 68s 62ms/step - loss: 0.5595 - accuracy: 0.707 - 68s 62ms/step - loss: 0.5594 - accuracy: 0.707 - 68s 62ms/step - loss: 0.5593 - accuracy: 0.707 - 68s 62ms/step - loss: 0.5590 - accuracy: 0.708 - 68s 62ms/step - loss: 0.5589 - accuracy: 0.708 - 68s 62ms/step - loss: 0.5587 - accuracy: 0.708 - 68s 62ms/step - loss: 0.5585 - accuracy: 0.708 - 68s 62ms/step - loss: 0.5582 - accuracy: 0.708 - 68s 62ms/step - loss: 0.5582 - accuracy: 0.708 - 68s 62ms/step - loss: 0.5580 - accuracy: 0.708 - 68s 62ms/step - loss: 0.5580 - accuracy: 0.708 - 68s 62ms/step - loss: 0.5581 - accuracy: 0.708 - 68s 62ms/step - loss: 0.5582 - accuracy: 0.708 - 68s 62ms/step - loss: 0.5578 - accuracy: 0.709 - 68s 62ms/step - loss: 0.5580 - accuracy: 0.709 - 68s 62ms/step - loss: 0.5579 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5579 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5577 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5578 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5577 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5577 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5578 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5578 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5579 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5577 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5577 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5576 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5576 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5576 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5575 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5574 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5574 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5572 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5573 - accuracy: 0.709 - 69s 62ms/step - loss: 0.5572 - accuracy: 0.710 - 69s 62ms/step - loss: 0.5571 - accuracy: 0.710 - 70s 62ms/step - loss: 0.5570 - accuracy: 0.710 - 70s 62ms/step - loss: 0.5570 - accuracy: 0.710 - 70s 62ms/step - loss: 0.5569 - accuracy: 0.710 - 70s 62ms/step - loss: 0.5567 - accuracy: 0.710 - 70s 62ms/step - loss: 0.5567 - accuracy: 0.710 - 70s 62ms/step - loss: 0.5566 - accuracy: 0.710 - 70s 62ms/step - loss: 0.5565 - accuracy: 0.710 - 70s 62ms/step - loss: 0.5564 - accuracy: 0.710 - 70s 62ms/step - loss: 0.5563 - accuracy: 0.710 - 70s 62ms/step - loss: 0.5561 - accuracy: 0.711 - 70s 62ms/step - loss: 0.5560 - accuracy: 0.711 - 70s 62ms/step - loss: 0.5558 - accuracy: 0.711 - 70s 62ms/step - loss: 0.5556 - accuracy: 0.711 - 70s 62ms/step - loss: 0.5556 - accuracy: 0.711 - 70s 62ms/step - loss: 0.5555 - accuracy: 0.711 - 70s 62ms/step - loss: 0.5555 - accuracy: 0.711 - 71s 62ms/step - loss: 0.5555 - accuracy: 0.711 - 71s 62ms/step - loss: 0.5557 - accuracy: 0.711 - 71s 62ms/step - loss: 0.5556 - accuracy: 0.711 - 71s 62ms/step - loss: 0.5555 - accuracy: 0.711 - 71s 62ms/step - loss: 0.5553 - accuracy: 0.711 - 71s 62ms/step - loss: 0.5552 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5550 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5551 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5551 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5550 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5549 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5547 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5546 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5544 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5545 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5544 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5544 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5543 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5543 - accuracy: 0.712 - 71s 62ms/step - loss: 0.5545 - accuracy: 0.712 - 72s 62ms/step - loss: 0.5544 - accuracy: 0.712 - 72s 62ms/step - loss: 0.5542 - accuracy: 0.713 - 72s 61ms/step - loss: 0.5545 - accuracy: 0.712 - 72s 61ms/step - loss: 0.5544 - accuracy: 0.712 - 72s 61ms/step - loss: 0.5541 - accuracy: 0.713 - 72s 61ms/step - loss: 0.5540 - accuracy: 0.713 - 72s 61ms/step - loss: 0.5539 - accuracy: 0.713 - 72s 61ms/step - loss: 0.5538 - accuracy: 0.7135\r\n   1337/Unknown - 72s 61ms/step - loss: 0.5536 - accuracy: 0.713 - 72s 61ms/step - loss: 0.5535 - accuracy: 0.713 - 72s 61ms/step - loss: 0.5535 - accuracy: 0.713 - 72s 61ms/step - loss: 0.5535 - accuracy: 0.713 - 72s 61ms/step - loss: 0.5533 - accuracy: 0.714 - 72s 61ms/step - loss: 0.5534 - accuracy: 0.713 - 72s 61ms/step - loss: 0.5533 - accuracy: 0.714 - 72s 61ms/step - loss: 0.5531 - accuracy: 0.714 - 72s 61ms/step - loss: 0.5531 - accuracy: 0.714 - 72s 61ms/step - loss: 0.5529 - accuracy: 0.714 - 73s 61ms/step - loss: 0.5527 - accuracy: 0.714 - 73s 61ms/step - loss: 0.5527 - accuracy: 0.714 - 73s 61ms/step - loss: 0.5529 - accuracy: 0.714 - 73s 61ms/step - loss: 0.5527 - accuracy: 0.714 - 73s 61ms/step - loss: 0.5526 - accuracy: 0.714 - 73s 61ms/step - loss: 0.5523 - accuracy: 0.714 - 73s 61ms/step - loss: 0.5523 - accuracy: 0.714 - 73s 61ms/step - loss: 0.5523 - accuracy: 0.714 - 73s 61ms/step - loss: 0.5521 - accuracy: 0.715 - 73s 61ms/step - loss: 0.5521 - accuracy: 0.715 - 73s 61ms/step - loss: 0.5520 - accuracy: 0.715 - 73s 61ms/step - loss: 0.5518 - accuracy: 0.715 - 73s 61ms/step - loss: 0.5517 - accuracy: 0.715 - 73s 61ms/step - loss: 0.5516 - accuracy: 0.715 - 73s 61ms/step - loss: 0.5514 - accuracy: 0.715 - 73s 61ms/step - loss: 0.5514 - accuracy: 0.715 - 73s 61ms/step - loss: 0.5511 - accuracy: 0.715 - 74s 61ms/step - loss: 0.5509 - accuracy: 0.716 - 74s 61ms/step - loss: 0.5508 - accuracy: 0.716 - 74s 61ms/step - loss: 0.5506 - accuracy: 0.716 - 74s 61ms/step - loss: 0.5503 - accuracy: 0.716 - 74s 61ms/step - loss: 0.5503 - accuracy: 0.716 - 74s 61ms/step - loss: 0.5501 - accuracy: 0.716 - 74s 61ms/step - loss: 0.5499 - accuracy: 0.716 - 74s 61ms/step - loss: 0.5497 - accuracy: 0.716 - 74s 61ms/step - loss: 0.5496 - accuracy: 0.716 - 74s 61ms/step - loss: 0.5494 - accuracy: 0.716 - 74s 61ms/step - loss: 0.5493 - accuracy: 0.717 - 74s 61ms/step - loss: 0.5491 - accuracy: 0.717 - 74s 61ms/step - loss: 0.5489 - accuracy: 0.717 - 74s 61ms/step - loss: 0.5492 - accuracy: 0.717 - 74s 61ms/step - loss: 0.5492 - accuracy: 0.717 - 74s 61ms/step - loss: 0.5491 - accuracy: 0.717 - 74s 61ms/step - loss: 0.5490 - accuracy: 0.717 - 74s 61ms/step - loss: 0.5489 - accuracy: 0.717 - 74s 61ms/step - loss: 0.5487 - accuracy: 0.717 - 74s 61ms/step - loss: 0.5487 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5485 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5486 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5490 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5489 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5487 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5487 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5489 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5488 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5487 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5485 - accuracy: 0.717 - 75s 61ms/step - loss: 0.5483 - accuracy: 0.718 - 75s 61ms/step - loss: 0.5482 - accuracy: 0.718 - 75s 61ms/step - loss: 0.5481 - accuracy: 0.718 - 75s 61ms/step - loss: 0.5482 - accuracy: 0.718 - 75s 61ms/step - loss: 0.5483 - accuracy: 0.718 - 75s 61ms/step - loss: 0.5481 - accuracy: 0.718 - 75s 61ms/step - loss: 0.5480 - accuracy: 0.718 - 75s 61ms/step - loss: 0.5481 - accuracy: 0.718 - 75s 61ms/step - loss: 0.5479 - accuracy: 0.718 - 75s 61ms/step - loss: 0.5476 - accuracy: 0.718 - 75s 61ms/step - loss: 0.5478 - accuracy: 0.718 - 76s 61ms/step - loss: 0.5478 - accuracy: 0.718 - 76s 61ms/step - loss: 0.5479 - accuracy: 0.718 - 76s 61ms/step - loss: 0.5478 - accuracy: 0.718 - 76s 61ms/step - loss: 0.5477 - accuracy: 0.718 - 76s 61ms/step - loss: 0.5476 - accuracy: 0.718 - 76s 61ms/step - loss: 0.5475 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5473 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5471 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5469 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5468 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5467 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5469 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5467 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5467 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5466 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5464 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5465 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5463 - accuracy: 0.719 - 76s 61ms/step - loss: 0.5461 - accuracy: 0.720 - 76s 61ms/step - loss: 0.5460 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5459 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5459 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5457 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5456 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5455 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5455 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5453 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5452 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5451 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5451 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5450 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5450 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5451 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5450 - accuracy: 0.721 - 77s 61ms/step - loss: 0.5453 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5452 - accuracy: 0.720 - 77s 61ms/step - loss: 0.5450 - accuracy: 0.721 - 78s 61ms/step - loss: 0.5448 - accuracy: 0.721 - 78s 61ms/step - loss: 0.5446 - accuracy: 0.721 - 78s 61ms/step - loss: 0.5445 - accuracy: 0.721 - 78s 61ms/step - loss: 0.5444 - accuracy: 0.721 - 78s 61ms/step - loss: 0.5443 - accuracy: 0.721 - 78s 61ms/step - loss: 0.5441 - accuracy: 0.721 - 78s 61ms/step - loss: 0.5441 - accuracy: 0.721 - 78s 61ms/step - loss: 0.5441 - accuracy: 0.721 - 78s 61ms/step - loss: 0.5439 - accuracy: 0.722 - 78s 61ms/step - loss: 0.5437 - accuracy: 0.722 - 78s 61ms/step - loss: 0.5439 - accuracy: 0.722 - 78s 61ms/step - loss: 0.5437 - accuracy: 0.722 - 78s 61ms/step - loss: 0.5434 - accuracy: 0.722 - 78s 61ms/step - loss: 0.5433 - accuracy: 0.722 - 78s 61ms/step - loss: 0.5433 - accuracy: 0.722 - 78s 61ms/step - loss: 0.5431 - accuracy: 0.722 - 78s 61ms/step - loss: 0.5429 - accuracy: 0.722 - 79s 61ms/step - loss: 0.5431 - accuracy: 0.722 - 79s 61ms/step - loss: 0.5431 - accuracy: 0.722 - 79s 61ms/step - loss: 0.5431 - accuracy: 0.722 - 79s 61ms/step - loss: 0.5430 - accuracy: 0.722 - 79s 61ms/step - loss: 0.5429 - accuracy: 0.722 - 79s 61ms/step - loss: 0.5426 - accuracy: 0.723 - 79s 61ms/step - loss: 0.5425 - accuracy: 0.723 - 79s 61ms/step - loss: 0.5422 - accuracy: 0.723 - 79s 61ms/step - loss: 0.5421 - accuracy: 0.723 - 79s 61ms/step - loss: 0.5421 - accuracy: 0.723 - 79s 61ms/step - loss: 0.5418 - accuracy: 0.723 - 79s 61ms/step - loss: 0.5416 - accuracy: 0.724 - 79s 61ms/step - loss: 0.5415 - accuracy: 0.724 - 79s 61ms/step - loss: 0.5415 - accuracy: 0.724 - 79s 61ms/step - loss: 0.5415 - accuracy: 0.723 - 79s 61ms/step - loss: 0.5418 - accuracy: 0.723 - 79s 61ms/step - loss: 0.5416 - accuracy: 0.724 - 79s 61ms/step - loss: 0.5414 - accuracy: 0.724 - 79s 61ms/step - loss: 0.5416 - accuracy: 0.724 - 80s 61ms/step - loss: 0.5416 - accuracy: 0.724 - 80s 61ms/step - loss: 0.5415 - accuracy: 0.724 - 80s 61ms/step - loss: 0.5413 - accuracy: 0.724 - 80s 61ms/step - loss: 0.5413 - accuracy: 0.724 - 80s 61ms/step - loss: 0.5411 - accuracy: 0.724 - 80s 61ms/step - loss: 0.5408 - accuracy: 0.724 - 80s 61ms/step - loss: 0.5407 - accuracy: 0.724 - 80s 61ms/step - loss: 0.5407 - accuracy: 0.724 - 80s 61ms/step - loss: 0.5406 - accuracy: 0.724 - 80s 61ms/step - loss: 0.5405 - accuracy: 0.725 - 80s 61ms/step - loss: 0.5404 - accuracy: 0.725 - 80s 61ms/step - loss: 0.5403 - accuracy: 0.725 - 80s 61ms/step - loss: 0.5402 - accuracy: 0.725 - 80s 61ms/step - loss: 0.5401 - accuracy: 0.725 - 80s 61ms/step - loss: 0.5402 - accuracy: 0.725 - 80s 61ms/step - loss: 0.5400 - accuracy: 0.725 - 80s 61ms/step - loss: 0.5402 - accuracy: 0.725 - 80s 61ms/step - loss: 0.5401 - accuracy: 0.725 - 81s 61ms/step - loss: 0.5400 - accuracy: 0.725 - 81s 61ms/step - loss: 0.5400 - accuracy: 0.725 - 81s 61ms/step - loss: 0.5399 - accuracy: 0.725 - 81s 60ms/step - loss: 0.5397 - accuracy: 0.725 - 81s 60ms/step - loss: 0.5396 - accuracy: 0.725 - 81s 60ms/step - loss: 0.5394 - accuracy: 0.725 - 81s 60ms/step - loss: 0.5393 - accuracy: 0.725 - 81s 60ms/step - loss: 0.5391 - accuracy: 0.7260\r\n   1504/Unknown - 81s 60ms/step - loss: 0.5388 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5387 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5386 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5386 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5385 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5384 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5382 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5383 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5384 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5383 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5382 - accuracy: 0.726 - 81s 60ms/step - loss: 0.5383 - accuracy: 0.726 - 82s 60ms/step - loss: 0.5381 - accuracy: 0.726 - 82s 60ms/step - loss: 0.5380 - accuracy: 0.726 - 82s 60ms/step - loss: 0.5380 - accuracy: 0.726 - 82s 60ms/step - loss: 0.5379 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5377 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5377 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5376 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5373 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5371 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5370 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5368 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5367 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5366 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5365 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5365 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5364 - accuracy: 0.727 - 82s 60ms/step - loss: 0.5362 - accuracy: 0.728 - 82s 60ms/step - loss: 0.5361 - accuracy: 0.728 - 82s 60ms/step - loss: 0.5358 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5360 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5361 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5361 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5360 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5359 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5359 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5358 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5358 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5356 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5354 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5351 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5352 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5351 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5351 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5348 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5350 - accuracy: 0.728 - 83s 60ms/step - loss: 0.5348 - accuracy: 0.729 - 83s 60ms/step - loss: 0.5348 - accuracy: 0.729 - 83s 60ms/step - loss: 0.5347 - accuracy: 0.729 - 83s 60ms/step - loss: 0.5346 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5349 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5350 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5351 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5351 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5350 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5349 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5346 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5345 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5344 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5347 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5347 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5346 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5347 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5346 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5346 - accuracy: 0.729 - 84s 60ms/step - loss: 0.5345 - accuracy: 0.729 - 85s 60ms/step - loss: 0.5342 - accuracy: 0.729 - 85s 60ms/step - loss: 0.5343 - accuracy: 0.729 - 85s 60ms/step - loss: 0.5341 - accuracy: 0.729 - 85s 60ms/step - loss: 0.5339 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5340 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5339 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5337 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5339 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5338 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5336 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5334 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5333 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5331 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5330 - accuracy: 0.731 - 85s 60ms/step - loss: 0.5331 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5329 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5330 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5328 - accuracy: 0.730 - 85s 60ms/step - loss: 0.5326 - accuracy: 0.731 - 86s 60ms/step - loss: 0.5325 - accuracy: 0.731 - 86s 60ms/step - loss: 0.5322 - accuracy: 0.731 - 86s 60ms/step - loss: 0.5321 - accuracy: 0.731 - 86s 60ms/step - loss: 0.5320 - accuracy: 0.731 - 86s 60ms/step - loss: 0.5319 - accuracy: 0.731 - 86s 60ms/step - loss: 0.5317 - accuracy: 0.731 - 86s 60ms/step - loss: 0.5317 - accuracy: 0.731 - 86s 60ms/step - loss: 0.5316 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5314 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5314 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5313 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5311 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5309 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5307 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5306 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5306 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5304 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5301 - accuracy: 0.733 - 86s 60ms/step - loss: 0.5301 - accuracy: 0.732 - 86s 60ms/step - loss: 0.5300 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5301 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5302 - accuracy: 0.732 - 87s 60ms/step - loss: 0.5302 - accuracy: 0.732 - 87s 60ms/step - loss: 0.5302 - accuracy: 0.732 - 87s 60ms/step - loss: 0.5301 - accuracy: 0.732 - 87s 60ms/step - loss: 0.5300 - accuracy: 0.732 - 87s 60ms/step - loss: 0.5301 - accuracy: 0.732 - 87s 60ms/step - loss: 0.5299 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5298 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5297 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5296 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5295 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5296 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5295 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5294 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5293 - accuracy: 0.733 - 87s 60ms/step - loss: 0.5291 - accuracy: 0.733 - 88s 60ms/step - loss: 0.5288 - accuracy: 0.733 - 88s 60ms/step - loss: 0.5287 - accuracy: 0.733 - 88s 60ms/step - loss: 0.5285 - accuracy: 0.733 - 88s 60ms/step - loss: 0.5284 - accuracy: 0.733 - 88s 60ms/step - loss: 0.5284 - accuracy: 0.733 - 88s 60ms/step - loss: 0.5286 - accuracy: 0.733 - 88s 60ms/step - loss: 0.5284 - accuracy: 0.734 - 88s 60ms/step - loss: 0.5286 - accuracy: 0.733 - 88s 60ms/step - loss: 0.5287 - accuracy: 0.733 - 88s 60ms/step - loss: 0.5286 - accuracy: 0.733 - 88s 60ms/step - loss: 0.5286 - accuracy: 0.734 - 88s 60ms/step - loss: 0.5286 - accuracy: 0.734 - 88s 60ms/step - loss: 0.5286 - accuracy: 0.734 - 88s 60ms/step - loss: 0.5285 - accuracy: 0.734 - 88s 60ms/step - loss: 0.5284 - accuracy: 0.734 - 88s 60ms/step - loss: 0.5284 - accuracy: 0.734 - 88s 60ms/step - loss: 0.5281 - accuracy: 0.734 - 88s 60ms/step - loss: 0.5280 - accuracy: 0.734 - 88s 60ms/step - loss: 0.5281 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5279 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5279 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5277 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5276 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5273 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5274 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5273 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5278 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5279 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5276 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5274 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5271 - accuracy: 0.735 - 89s 60ms/step - loss: 0.5271 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5270 - accuracy: 0.735 - 89s 60ms/step - loss: 0.5272 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5272 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5273 - accuracy: 0.734 - 89s 60ms/step - loss: 0.5272 - accuracy: 0.734 - 90s 60ms/step - loss: 0.5272 - accuracy: 0.734 - 90s 60ms/step - loss: 0.5272 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5270 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5271 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5270 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5269 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5268 - accuracy: 0.7352\r\n   1514/Unknown - 90s 60ms/step - loss: 0.5267 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5265 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5264 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5262 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5260 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5260 - accuracy: 0.736 - 90s 60ms/step - loss: 0.5261 - accuracy: 0.735 - 90s 60ms/step - loss: 0.5261 - accuracy: 0.736 - 90s 60ms/step - loss: 0.5260 - accuracy: 0.736 - 90s 60ms/step - loss: 0.5259 - accuracy: 0.7361\r\n```", "Hi,\r\n\r\nThe `[I 10:23:43.327 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports` error makes this look like an issue with the jupyter notebook you're using.  That could also explain the `CancelledError` you were seeing earlier.\r\n\r\nCan you help us isolate the problem by trying to reproduce it outside a jupyter notebook?", "> Hi,\r\n> \r\n> The `[I 10:23:43.327 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports` error makes this look like an issue with the jupyter notebook you're using. That could also explain the `CancelledError` you were seeing earlier.\r\n> \r\n> Can you help us isolate the problem by trying to reproduce it outside a jupyter notebook?\r\n\r\nHello\uff01I run it on my pycharm\uff0cand show you the error\uff0cthis is pic\r\n![image](https://user-images.githubusercontent.com/15049049/71301306-7c16d280-23d8-11ea-95ce-d1e348340890.png)\r\nAnd this is the error information:\r\n```python\r\n2019-12-21 09:55:35.923410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\nWARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\r\n2019-12-21 09:55:40.903703: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-21 09:55:42.175231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\n2019-12-21 09:55:42.175475: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-21 09:55:42.176787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-21 09:55:42.179852: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-21 09:55:42.184928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\n2019-12-21 09:55:42.185165: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-21 09:55:42.185960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-21 09:55:43.718374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-21 09:55:43.718523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-12-21 09:55:43.718612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-12-21 09:55:43.720381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4708 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nEpoch 1/10\r\n2019-12-21 09:55:51.734039: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_15158_15757' and '__inference___backward_cudnn_lstm_with_fallback_12868_14345_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_15947' both implement 'lstm_5ec36c18-de20-4de0-96f1-9723ca36f23c' but their signatures do not match.\r\n2019-12-21 09:55:52.080664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2019-12-21 09:55:53.038029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n\r\n      1/Unknown - 9s 9s/step - loss: 0.6934 - accuracy: 0.4883\r\n      2/Unknown - 9s 5s/step - loss: 0.6932 - accuracy: 0.4941\r\n      3/Unknown - 9s 3s/step - loss: 0.6933 - accuracy: 0.4870\r\n      4/Unknown - 9s 2s/step - loss: 0.6933 - accuracy: 0.4922\r\n      5/Unknown - 10s 2s/step - loss: 0.6935 - accuracy: 0.4797\r\n      6/Unknown - 10s 2s/step - loss: 0.6933 - accuracy: 0.4883\r\n      7/Unknown - 10s 1s/step - loss: 0.6933 - accuracy: 0.4922\r\n      8/Unknown - 10s 1s/step - loss: 0.6934 - accuracy: 0.4883\r\n      9/Unknown - 10s 1s/step - loss: 0.6933 - accuracy: 0.4926\r\n     10/Unknown - 10s 1s/step - loss: 0.6933 - accuracy: 0.4969\r\n     11/Unknown - 11s 965ms/step - loss: 0.6932 - accuracy: 0.4993\r\n     12/Unknown - 11s 899ms/step - loss: 0.6932 - accuracy: 0.4987\r\n     13/Unknown - 11s 845ms/step - loss: 0.6931 - accuracy: 0.5000\r\n     14/Unknown - 11s 795ms/step - loss: 0.6930 - accuracy: 0.5031\r\n     15/Unknown - 11s 755ms/step - loss: 0.6930 - accuracy: 0.5083\r\n     16/Unknown - 11s 718ms/step - loss: 0.6930 - accuracy: 0.5122\r\n     17/Unknown - 12s 685ms/step - loss: 0.6930 - accuracy: 0.5129\r\n     18/Unknown - 12s 655ms/step - loss: 0.6929 - accuracy: 0.5139\r\n     19/Unknown - 12s 630ms/step - loss: 0.6928 - accuracy: 0.5191\r\n     20/Unknown - 12s 606ms/step - loss: 0.6928 - accuracy: 0.5232\r\n     21/Unknown - 12s 584ms/step - loss: 0.6927 - accuracy: 0.5272\r\n     22/Unknown - 12s 565ms/step - loss: 0.6927 - accuracy: 0.5295\r\n     23/Unknown - 13s 547ms/step - loss: 0.6926 - accuracy: 0.5326\r\n     24/Unknown - 13s 531ms/step - loss: 0.6925 - accuracy: 0.5387\r\n     25/Unknown - 13s 516ms/step - loss: 0.6925 - accuracy: 0.5387\r\n     26/Unknown - 13s 502ms/step - loss: 0.6924 - accuracy: 0.5388\r\n     27/Unknown - 13s 489ms/step - loss: 0.6924 - accuracy: 0.5378\r\n     28/Unknown - 13s 477ms/step - loss: 0.6923 - accuracy: 0.5386\r\n     29/Unknown - 13s 465ms/step - loss: 0.6922 - accuracy: 0.5416\r\n     30/Unknown - 14s 456ms/step - loss: 0.6922 - accuracy: 0.5417\r\n     31/Unknown - 14s 445ms/step - loss: 0.6920 - accuracy: 0.5460\r\n     32/Unknown - 14s 436ms/step - loss: 0.6919 - accuracy: 0.5481\r\n     33/Unknown - 14s 429ms/step - loss: 0.6918 - accuracy: 0.5496\r\n     34/Unknown - 14s 421ms/step - loss: 0.6917 - accuracy: 0.5512\r\n     35/Unknown - 15s 418ms/step - loss: 0.6916 - accuracy: 0.5530\r\n     36/Unknown - 15s 411ms/step - loss: 0.6914 - accuracy: 0.5564\r\n     37/Unknown - 15s 404ms/step - loss: 0.6912 - accuracy: 0.5587\r\n     38/Unknown - 15s 397ms/step - loss: 0.6910 - accuracy: 0.5623\r\n     39/Unknown - 15s 391ms/step - loss: 0.6908 - accuracy: 0.5653\r\n     40/Unknown - 15s 385ms/step - loss: 0.6906 - accuracy: 0.5685\r\n     41/Unknown - 16s 379ms/step - loss: 0.6902 - accuracy: 0.5706\r\n     42/Unknown - 16s 374ms/step - loss: 0.6898 - accuracy: 0.5726\r\n     43/Unknown - 16s 370ms/step - loss: 0.6892 - accuracy: 0.5755\r\n     44/Unknown - 16s 365ms/step - loss: 0.6884 - accuracy: 0.5784\r\n     45/Unknown - 16s 361ms/step - loss: 0.6872 - accuracy: 0.5813\r\n     46/Unknown - 16s 358ms/step - loss: 0.6867 - accuracy: 0.5825\r\n     47/Unknown - 17s 357ms/step - loss: 0.6852 - accuracy: 0.5849\r\n     48/Unknown - 17s 356ms/step - loss: 0.6839 - accuracy: 0.5865\r\n     49/Unknown - 17s 354ms/step - loss: 0.6825 - accuracy: 0.5884\r\n     50/Unknown - 18s 353ms/step - loss: 0.6804 - accuracy: 0.5907\r\n     51/Unknown - 18s 350ms/step - loss: 0.6783 - accuracy: 0.5935\r\n     52/Unknown - 18s 348ms/step - loss: 0.6764 - accuracy: 0.5959\r\n     53/Unknown - 18s 345ms/step - loss: 0.6747 - accuracy: 0.5976\r\n     54/Unknown - 18s 342ms/step - loss: 0.6722 - accuracy: 0.6002\r\n     55/Unknown - 19s 339ms/step - loss: 0.6708 - accuracy: 0.6016\r\n     56/Unknown - 19s 336ms/step - loss: 0.6687 - accuracy: 0.6040\r\n     57/Unknown - 19s 333ms/step - loss: 0.6672 - accuracy: 0.6060\r\n     58/Unknown - 19s 330ms/step - loss: 0.6661 - accuracy: 0.6072\r\n     59/Unknown - 19s 327ms/step - loss: 0.6648 - accuracy: 0.6086\r\n     60/Unknown - 19s 324ms/step - loss: 0.6634 - accuracy: 0.6104\r\n     61/Unknown - 20s 322ms/step - loss: 0.6620 - accuracy: 0.6127\r\n     62/Unknown - 20s 319ms/step - loss: 0.6607 - accuracy: 0.6153\r\n     63/Unknown - 20s 316ms/step - loss: 0.6591 - accuracy: 0.6186\r\n     64/Unknown - 20s 314ms/step - loss: 0.6574 - accuracy: 0.6218\r\n     65/Unknown - 20s 311ms/step - loss: 0.6555 - accuracy: 0.6244\r\n     66/Unknown - 20s 309ms/step - loss: 0.6540 - accuracy: 0.6264\r\n     67/Unknown - 21s 307ms/step - loss: 0.6528 - accuracy: 0.6277\r\n     68/Unknown - 21s 306ms/step - loss: 0.6513 - accuracy: 0.6293\r\n     69/Unknown - 21s 303ms/step - loss: 0.6496 - accuracy: 0.6304\r\n     70/Unknown - 21s 301ms/step - loss: 0.6484 - accuracy: 0.6311\r\n     71/Unknown - 21s 299ms/step - loss: 0.6459 - accuracy: 0.6330\r\n     72/Unknown - 21s 297ms/step - loss: 0.6431 - accuracy: 0.6354\r\n     73/Unknown - 22s 295ms/step - loss: 0.6415 - accuracy: 0.6362\r\n     74/Unknown - 22s 294ms/step - loss: 0.6387 - accuracy: 0.6385\r\n     75/Unknown - 22s 292ms/step - loss: 0.6368 - accuracy: 0.6401\r\n     76/Unknown - 22s 290ms/step - loss: 0.6338 - accuracy: 0.6427\r\n     77/Unknown - 22s 288ms/step - loss: 0.6314 - accuracy: 0.6448\r\n     78/Unknown - 22s 287ms/step - loss: 0.6295 - accuracy: 0.6462\r\n     79/Unknown - 22s 285ms/step - loss: 0.6270 - accuracy: 0.6480\r\n     80/Unknown - 23s 283ms/step - loss: 0.6249 - accuracy: 0.6500\r\n     81/Unknown - 23s 282ms/step - loss: 0.6228 - accuracy: 0.6517\r\n     82/Unknown - 23s 280ms/step - loss: 0.6215 - accuracy: 0.6529\r\n     83/Unknown - 23s 279ms/step - loss: 0.6201 - accuracy: 0.6546\r\n     84/Unknown - 23s 277ms/step - loss: 0.6190 - accuracy: 0.6563\r\n     85/Unknown - 23s 276ms/step - loss: 0.6179 - accuracy: 0.6583\r\n     86/Unknown - 24s 274ms/step - loss: 0.6167 - accuracy: 0.6600\r\n     87/Unknown - 24s 273ms/step - loss: 0.6152 - accuracy: 0.6620\r\n     88/Unknown - 24s 271ms/step - loss: 0.6136 - accuracy: 0.6638\r\n     89/Unknown - 24s 271ms/step - loss: 0.6124 - accuracy: 0.6648\r\n     90/Unknown - 24s 270ms/step - loss: 0.6106 - accuracy: 0.6668\r\n     91/Unknown - 24s 269ms/step - loss: 0.6095 - accuracy: 0.6679\r\n     92/Unknown - 25s 267ms/step - loss: 0.6085 - accuracy: 0.6692\r\n     93/Unknown - 25s 266ms/step - loss: 0.6077 - accuracy: 0.6703\r\n     94/Unknown - 25s 265ms/step - loss: 0.6062 - accuracy: 0.6718\r\n     95/Unknown - 25s 263ms/step - loss: 0.6046 - accuracy: 0.6740\r\n     96/Unknown - 25s 262ms/step - loss: 0.6031 - accuracy: 0.6760\r\n     97/Unknown - 25s 261ms/step - loss: 0.6018 - accuracy: 0.67772019-12-21 09:56:11.420917: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}}]]\r\n\t [[IteratorGetNext/_2]]\r\n\r\n     98/Unknown - 25s 260ms/step - loss: 0.6006 - accuracy: 0.67862019-12-21 09:56:11.421316: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}}]]\r\n2019-12-21 09:56:13.333385: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_16816' and '__inference_cudnn_lstm_with_fallback_16816_specialized_for_sequential_bidirectional_forward_lstm_StatefulPartitionedCall_at___inference_distributed_function_20375' both implement 'lstm_4ab884f3-e58f-48b8-8a5d-1c50699274ed' but their signatures do not match.\r\n\r\n98/98 [==============================] - 29s 300ms/step - loss: 0.6006 - accuracy: 0.6786 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 2/10\r\n\r\n 1/98 [..............................] - ETA: 1:26 - loss: 0.4257 - accuracy: 0.8594\r\n 2/98 [..............................] - ETA: 50s - loss: 0.4496 - accuracy: 0.8359 \r\n 3/98 [..............................] - ETA: 38s - loss: 0.4490 - accuracy: 0.8242\r\n 4/98 [>.............................] - ETA: 32s - loss: 0.4461 - accuracy: 0.8193\r\n 5/98 [>.............................] - ETA: 28s - loss: 0.4339 - accuracy: 0.8281\r\n 6/98 [>.............................] - ETA: 25s - loss: 0.4418 - accuracy: 0.8177\r\n 7/98 [=>............................] - ETA: 23s - loss: 0.4429 - accuracy: 0.8158\r\n 8/98 [=>............................] - ETA: 22s - loss: 0.4431 - accuracy: 0.8169\r\n 9/98 [=>............................] - ETA: 21s - loss: 0.4391 - accuracy: 0.8203\r\n10/98 [==>...........................] - ETA: 20s - loss: 0.4341 - accuracy: 0.8219\r\n11/98 [==>...........................] - ETA: 19s - loss: 0.4345 - accuracy: 0.8200\r\n12/98 [==>...........................] - ETA: 18s - loss: 0.4377 - accuracy: 0.8174\r\n13/98 [==>...........................] - ETA: 18s - loss: 0.4382 - accuracy: 0.8167\r\n14/98 [===>..........................] - ETA: 17s - loss: 0.4360 - accuracy: 0.8175\r\n15/98 [===>..........................] - ETA: 17s - loss: 0.4370 - accuracy: 0.81612019-12-21 09:56:19.159940: E tensorflow/stream_executor/dnn.cc:588] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1887): 'cudnnRNNBackwardDataEx( cudnn.handle(), rnn_desc.handle(), output_desc.data_handle(), output_data.opaque(), output_desc.data_handle(), output_backprop_data.opaque(), nullptr, nullptr, output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.data_handle(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), nullptr, nullptr, workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n2019-12-21 09:56:19.161113: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1899 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 32, 1, 2413, 256, 32] \r\n2019-12-21 09:56:19.161667: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 32, 1, 2413, 256, 32] \r\n\t [[{{node gradients/cond_grad/If/then/_0/gradients/CudnnRNNV3_grad/CudnnRNNBackpropV3}}]]\r\n2019-12-21 09:56:19.169628: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node Reshape_11/_38}}]]\r\n\t [[Adam/Adam/update/AssignSubVariableOp/_41]]\r\n2019-12-21 09:56:19.169887: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node Reshape_11/_38}}]]\r\n\r\n16/98 [===>..........................] - ETA: 19s - loss: 0.4370 - accuracy: 0.8161Traceback (most recent call last):\r\n  File \"D:/Documents/Project/Python/2019/mission2/test.py\", line 33, in <module>\r\n    validation_data=test_batches, validation_steps=20)\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 487, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\sha\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node Reshape_11/_38}}]] [Op:__inference_distributed_function_15947]\r\n\r\nFunction call stack:\r\ndistributed_function\r\n\r\n```", "Hello\uff01 I try tensorflow-gpu-2.1 and cuda10.1\r\n\r\nBut still run error!\r\n\r\nThis is the error information:\r\n```python\r\n2020-02-15 16:56:38.744787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4702 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-02-15 16:57:00.686067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-02-15 16:57:01.345712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-02-15 16:57:18.411493: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n2020-02-15 16:57:18.435792: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1432, 10, 64]\r\n2020-02-15 16:57:18.450200: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1432, 10, 64]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n2020-02-15 16:57:18.465558: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_3894_4070_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_5744}} {{function_node __inference___backward_cudnn_lstm_with_fallback_3894_4070_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_5744}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1432, 10, 64]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n         [[StatefulPartitionedCall]]\r\n         [[Reshape_11/_38]]\r\n2020-02-15 16:57:18.487909: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_3894_4070_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_5744}} {{function_node __inference___backward_cudnn_lstm_with_fallback_3894_4070_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_5744}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1432, 10, 64]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n         [[StatefulPartitionedCall]]\r\n```\r\nWith the same code.", "> Hello\uff01 I try tensorflow-gpu-2.1 and cuda10.1\r\n> \r\n> But still run error!\r\n\r\nUnfortunately the Python program works fine on my local machine (P100): [log](https://gist.github.com/sanjoy/942bef2c1433ddfa721d173c5f539585).\r\n\r\nAre you able to run RNN cells using cuDNN on your machine normally?  And does preventing TF from allocating all of the GPU memory at startup ([instructions](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)) help?", "> > Hello\uff01 I try tensorflow-gpu-2.1 and cuda10.1\r\n> > But still run error!\r\n> \r\n> Unfortunately the Python program works fine on my local machine (P100): [log](https://gist.github.com/sanjoy/942bef2c1433ddfa721d173c5f539585).\r\n> \r\n> Are you able to run RNN cells using cuDNN on your machine normally? And does preventing TF from allocating all of the GPU memory at startup ([instructions](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)) help?\r\n\r\nThis error will turn out on GTX1060 >_<.\r\nI try on google cloab, it will run fluently.", "Can you please check if this helps:\r\n\r\n> And does preventing TF from allocating all of the GPU memory at startup ([instructions](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)) help?", "> Can you please check if this helps:\r\n> \r\n> > And does preventing TF from allocating all of the GPU memory at startup ([instructions](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth)) help?\r\n\r\nok, i will try. Thank you!", "Hello @sanjoy ,\r\n\r\nI encountered the exact same problem when trying the tutorial codes, with the same error logs. \r\n\r\n**Configurations of my system:**\r\n\r\n- **OS**: Windows 10 Enterprise 1809, build 17763.973\r\n- **Tensorflow installed from**: Anaconda\r\n- **Tensorflow version**: tensorflow-gpu 2.0.0\r\n- **Python version**: 3.7.4\r\n- **CUDA/cuDNN version**: cudatoolkit 10.0.130, cudnn 7.6.5\r\n- **GPU model and memory**: GeForce 940MX, 2.0GB\r\n- **Jupyter version**: 1.0.0\r\n\r\nWith the codes completely the same as provided in the tutorial, the error logs are identical to what @shazhongcheng has provided. Meanwhile the logs in the anaconda prompt are as follows:\r\n\r\n    2020-02-19 15:35:49.304828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1384 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:06:00.0, compute capability: 5.0)\r\n    2020-02-19 15:35:55.661516: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_standard_lstm_5125_5608' and '__inference___backward_cudnn_lstm_with_fallback_4298_4480_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_6331' both implement 'lstm_5885b35c-5694-48ee-8982-75fb59ffbf3f' but their signatures do not match.\r\n    2020-02-19 15:35:55.893338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n    2020-02-19 15:35:57.775284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n    2020-02-19 15:36:03.652048: E tensorflow/stream_executor/dnn.cc:588] CUDNN_STATUS_INTERNAL_ERROR in tensorflow/stream_executor/cuda/cuda_dnn.cc(1796): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n    2020-02-19 15:36:03.698306: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1360, 64, 64]\r\n    2020-02-19 15:36:03.730320: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 64, 64, 1, 1360, 64, 64]\r\n         [[{{node CudnnRNN}}]]\r\n    2020-02-19 15:36:03.759823: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.\r\n         [[{{node Reshape_11/_38}}]]\r\n         [[Adam/Adam/update/AssignSubVariableOp/_41]]\r\n    2020-02-19 15:36:03.782996: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.\r\n         [[{{node Reshape_11/_38}}]]\r\n\r\nI then tried adding the following lines:\r\n\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    tf.config.experimental.set_memory_growth(gpus[0],True)\r\n\r\nafter the import statements, but received the same error logs in jupyter and same logs in anaconda prompt.\r\n\r\nIn [issue #33924](https://github.com/tensorflow/tensorflow/issues/33924) as mentioned by @shazhongcheng above, many others have also experienced the same problem\r\n\r\nIs there any other possible solutions?", "Thanks for the detailed information!  It would be helpful to also see what exact cuDNN API call leads to the error.  The built-in API logging in cuDNN (https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#api-logging ) can help here.\r\n\r\nBasically you can just set a few environment variables like the following before running your python script:\r\n```\r\nexport CUDNN_LOGINFO_DBG=1\r\nexport CUDNN_LOGDEST_DBG=\"/tmp/cudnn_api_log.%i\"\r\n```\r\nWe should then be able to look to one of the last items in the API log to find the reproducer.\r\n\r\nThanks, \r\nCliff", "Any update on this?\r\n\r\nAlso, I got some logs for the error.\r\n[cudnn_api_log.txt](https://github.com/tensorflow/tensorflow/files/4533890/cudnn_api_log.txt)\r\n", "@rodrigoruiz - You didn't mention which GPU or which cuDNN version you're using.  But I can confirm that the two LSTM cases in the log you provided pass our unit tests on V100 with cudnn 7.6.5.  If you can provide more information, I can check your combination more closely.  Otherwise,  I suggest we close this issue; if there are further recurrences, please do report them directly to NVIDIA.  Thanks!", "Hi, I'm using Cuda 10.1 with the latest cudnn (can't remember which one and I uninstalled everything to rebuild TensorFlow from scratch as a last attempt) and I have an RTX 2080 Ti.\r\nIt seems the problem is related to Cuda and the RTX, but TensorFlow 2 doesn't work on Cuda 10.2 (which I'm not sure would be enough to fix the problem anyway).\r\n\r\nCuda 10 (or 9, I can't remember), does work, but TensorFlow 1 with Keras is so bad (wrong precision and recall metrics during training) that I'm not sure what to do.", "@shazhongcheng , Install the latest driver for your GPU from [here](https://www.nvidia.com/Download/driverResults.aspx/175403/en-us) and upgrade your Tensorflow version to latest and for CUDA and cuDNN compatibility match you can check [this](https://www.tensorflow.org/install/source_windows#gpu) table and let us know if you are still facing issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34094\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34094\">No</a>\n"]}, {"number": 34093, "title": "[TF2.0] GradientTape.gradient raise SystemError when calling embedding_column layer multiple times with tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tf-nightly-gpu-2.0-preview\r\n- TensorFlow version (use command below): v1.12.1-14959-g9663619 2.0.0-dev20191002\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cudatoolkit=10.0.130  cudnn=7.6.4\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen calling a ```tf.keras.layers.DenseFeatures``` layer created from ```tf.feature_column.embedding_column``` multiple times and calculate the gradients with ```GradientTape.gradient```, using ```@tf.function``` may cause ```GradientTape.gradient``` raise ```SystemError```, depends on the number of times we call the same layer( error raise if we call it more than 4 times ).\r\n\r\nWithout ```@tf.function```, everything is fine. \r\n\r\n**Describe the expected behavior**\r\nThe gradients should be calculated well regardless of how many times we call the layer within a ```@tf.function```.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nft_col_numeric = tf.feature_column.numeric_column(\"test_input\")\r\nft_col_buk     = tf.feature_column.bucketized_column(ft_col_numeric, boundaries=[1, 3, 5, 7])\r\nft_col_embed   = tf.feature_column.embedding_column(ft_col_buk, dimension=4)\r\nft_embed_layer = tf.keras.layers.DenseFeatures(ft_col_embed)\r\n\r\n# crash when the call number is greater than 4\r\nLAYER_CALL_NUM = 5\r\n\r\n@tf.function\r\ndef run(inputs):\r\n    with tf.GradientTape() as tape:\r\n        res_list = []\r\n        for i in range(LAYER_CALL_NUM):\r\n            x = ft_embed_layer(inputs)\r\n            res_list.append(x)\r\n        y = tf.reduce_sum(sum(res_list))\r\n    weights = ft_embed_layer.trainable_variables\r\n    gradients = tape.gradient(y, weights)\r\n    return gradients\r\n\r\ntest_input = tf.constant([0, 2, 4, 6, 8])\r\ninputs = { \"test_input\" : test_input }\r\nrun(inputs)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 26, in <module>\r\n    run(inputs)\r\n  File \"/home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 553, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 599, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 492, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2320, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2628, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2517, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 943, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 434, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 933, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\ntensorflow.python.autograph.impl.api.StagingError: in converted code:\r\n\r\n    test.py:21 run  *\r\n        gradients = tape.gradient(y, weights)\r\n    /home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:1023 gradient\r\n        unconnected_gradients=unconnected_gradients)\r\n    /home/user/anaconda3/envs/tf2_nt_last/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py:76 imperative_grad\r\n        compat.as_str(unconnected_gradients.value))\r\n\r\n    SystemError: <built-in function TFE_Py_TapeGradient> returned a result with an error set\r\n```\r\n", "comments": ["Issue replicating for the given code for TF-2.0, please find the [gist](https://colab.sandbox.google.com/gist/oanush/932cf9d8d1145a5c88ac8cecb84493e5/34093.ipynb) of colab.Thanks!", "@alextp does GradientTape have an owner other than you? I've looked at most of the code but am probably not going to be giving random issues the attention they deserve...", "@jaingaurav could you triage? I'm told the core team owns GradientTape.", "I believe this is resolved by https://github.com/tensorflow/tensorflow/pull/33912. I'll test it out and confirm.", "@GoSz, I tried with Tf-nightly == 2.2.0.dev20200318, but not getting any error.\r\nPlease take a look at the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/7d923164ab88f5e792d7abc1cacf529e/untitled469.ipynb) and confirm. Thanks!", "Seems it will be fixed with TF2.2 , thanks a lot.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34093\">No</a>\n"]}, {"number": 34092, "title": "fix_unit_test_bonus_tests", "body": "Fix this error:https://github.com/tensorflow/tensorflow/issues/34091.", "comments": []}, {"number": 34091, "title": "Failed to run the unit test of bonus_tests", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.0 master branch, commit-id:af019188adc704569b7cce51e50eba489af1a725 \r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.27.1\r\n- GCC/Compiler version (if compiling from source): 6.3\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAfter build from source code:\r\n```\r\nbazel --output_user_root=$build_dir build --config=v2 //tensorflow/tools/pip_package:build_pip_package \r\n```\r\n\r\nI run the unit test:\r\n```\r\nbazel --output_user_root=$build_dir test //tensorflow/core/kernels:bonus_tests\r\n```\r\nIt failed with the error message:\r\n\r\n_ERROR: no such target '//tensorflow/core/kernels:bonus_tests': target 'bonus_tests' not declared in package 'tensorflow/core/kernels' (did you mean 'loss_test'?) defined by /home/lesliefang/debug_failrunbonus_tests/tensorflow/tensorflow/core/kernels/BUILD\r\nINFO: Elapsed time: 0.382s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)_\r\n\r\n\r\n**Describe the expected behavior**\r\nThe unit test should run successfully.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nAs the description above.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["If I change this line:https://github.com/tensorflow/tensorflow/blob/bfce5bae88e01c080c8cc9224b439c4dd26c506b/tensorflow/core/kernels/BUILD#L3163-L3193\r\ntf_cc_tests to tf_cc_test\r\nIt can find the unit test case and show pass result.\r\n\r\nWhat's the difference of **tf_cc_tests** and **tf_cc_test**?", "As you can see [here](https://github.com/tensorflow/tensorflow/blob/7a07f274d65bed12b91b6c6dbe4987a6e1649429/tensorflow/tensorflow.bzl#L1169) tf_cc_tests perform tf_cc_test on multiple ops(components)\r\n\r\n**Source**\r\n```\r\n# Create a cc_test for each of the tensorflow tests listed in \"tests\"\r\ndef tf_cc_tests(\r\n        srcs,\r\n        deps,\r\n        name = \"\",\r\n        linkstatic = 0,\r\n        tags = [],\r\n        size = \"medium\",\r\n        args = None,\r\n        linkopts = [],\r\n        kernels = []):\r\n    for src in srcs:\r\n        tf_cc_test(\r\n            name = src_to_test_name(src),\r\n            size = size,\r\n            srcs = [src],\r\n            args = args,\r\n            kernels = kernels,\r\n            linkopts = linkopts,\r\n            linkstatic = linkstatic,\r\n            tags = tags,\r\n            deps = deps,\r\n        )\r\n```\r\n\r\n**Example**\r\n```\r\ntf_cc_tests(\r\n    name = \"bonus_tests\",\r\n    srcs = [\r\n        \"adjust_contrast_op_test.cc\",\r\n        \"colorspace_op_test.cc\",\r\n        \"crop_and_resize_op_test.cc\",\r\n        \"non_max_suppression_op_test.cc\",\r\n        \"resize_area_op_test.cc\",\r\n        \"resize_bicubic_op_test.cc\",\r\n        \"resize_nearest_neighbor_op_test.cc\",\r\n        \"scale_and_translate_op_test.cc\",\r\n    ],\r\n    linkopts = select({\r\n        \"//tensorflow:macos\": [\"-headerpad_max_install_names\"],\r\n        \"//conditions:default\": [],\r\n    }),\r\n    deps = [\r\n        \":image\",\r\n        \":ops_testutil\",\r\n        \":ops_util\",\r\n        \":sampling_kernels\",\r\n        \"//tensorflow/core:core_cpu\",\r\n        \"//tensorflow/core:framework\",\r\n        \"//tensorflow/core:lib\",\r\n        \"//tensorflow/core:lib_internal\",\r\n        \"//tensorflow/core:protos_all_cc\",\r\n        \"//tensorflow/core:test\",\r\n        \"//tensorflow/core:test_main\",\r\n        \"//tensorflow/core:testlib\",\r\n    ],\r\n)\r\n```\r\n\r\nLet me know if the above explanation helps. Thanks!", "@gowthamkpr Thanks for the explanation. I understand the definition of tf_cc_tests. \r\nThe question is why I run\uff1a\r\n`\r\nbazel --output_user_root=$build_dir test //tensorflow/core/kernels:bonus_tests\r\n`\r\nwhich throws the error:\r\n_ERROR: no such target '//tensorflow/core/kernels:bonus_tests': target 'bonus_tests' not declared in package 'tensorflow/core/kernels' (did you mean 'loss_test'?) defined by /home/lesliefang/debug_failrunbonus_tests/tensorflow/tensorflow/core/kernels/BUILD\r\nINFO: Elapsed time: 0.382s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)_", "`ERROR: no such target '//tensorflow/core/kernels:bonus_tests': target 'bonus_tests' not declared in package 'tensorflow/core/kernels' (did you mean 'loss_test'?) defined by /home/lesliefang/debug_failrunbonus_tests/tensorflow/tensorflow/core/kernels/BUILD`\r\n\r\nCan you take a look at the source in this path in your system and let me know if 'bonus_tests' is declared or no?", "@gowthamkpr Yes, 'bonus_tests' is defined in the file $tensorflow_root/tensorflow/core/kernels/BUILD\r\nI am using master branch, commit-id: af019188adc704569b7cce51e50eba489af1a725\r\nYou can help to reproduce this issue if possible.", "The name of the tf_cc_tests rule is never used to create an executable. If you check the definition, it creates executable tests for each of the entries in srcs (their name derived from name of their source file). So you can for example run:\r\nbazel --output_user_root=$build_dir test //tensorflow/core/kernels:adjust_contrast_op_test\r\n(this is the first of the created tests from the bonus_tests target).", "@akuegel Thanks a lot and It explains clearly. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34091\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34091\">No</a>\n"]}, {"number": 34090, "title": "ResourceScatterNdUpdate bug in graph mode and tape.gradient()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0.130\r\n- GPU model and memory: GeForce RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nRunning the code gets the following error:\r\n```\r\n ValueError: The inner -1 dimensions of input.shape=[] must match the inner 1 dimensions of updates.shape=[4,20]: Shapes must be equal rank, but are 0 and 1 for 'ResourceScatterNdUpdate' (op: 'ResourceScatterNdUpdate') with input shapes: [], [4,1], [4,20].\r\n```\r\nThere is no error if running the code without @tf.function decoration, or without using while_loop, or without using tape.gradient()\r\n\r\n**Describe the expected behavior**\r\nCode should run without error\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n    dim = 20\r\n    capacity = 256\r\n    batch_size = 4\r\n    buffer = tf.Variable(\r\n            initial_value=tf.zeros((capacity, dim)),\r\n            trainable=False)\r\n    w = tf.Variable(\r\n        initial_value=tf.zeros(()), trainable=True)\r\n\r\n    def _loop_body(loss):\r\n        loss += w.value()\r\n        indices = tf.range(batch_size)\r\n        indices = tf.expand_dims(indices, axis=-1)\r\n        # Error disappears if running buffer.scatter_nd_update inside tf.cond\r\n        # tf.cond(tf.shape(indices)[0] > 0, lambda:\r\n        buffer.scatter_nd_update(indices, tf.zeros(shape=(batch_size, dim))),\r\n        # lambda: buffer.value())\r\n        return [loss]\r\n\r\n    # Error disappears if @tf.function is removed\r\n    @tf.function\r\n    def _loop():\r\n        unroll_length = 10\r\n        loss = tf.zeros(())\r\n        with tf.GradientTape() as tape:\r\n            [loss] = tf.while_loop(\r\n                cond=lambda *_: True,\r\n                body=_loop_body,\r\n                loop_vars=[loss],\r\n                back_prop=True,\r\n                maximum_iterations=unroll_length)\r\n            # Error disappears if removing the previous while_loop()\r\n            # and uncomment the following line\r\n            # [loss] = _loop_body(loss)\r\n        # Error disappears if removing tape.gradient()\r\n        tape.gradient(loss, w)\r\n        w.assign_add(loss)\r\n\r\n    _loop()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFull error message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"scatter_nd_bug.py\", line 45, in <module>\r\n    main()\r\n  File \"scatter_nd_bug.py\", line 42, in main\r\n    _loop()\r\n  File \"/home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    scatter_nd_bug.py:39 _loop  *\r\n        tape.gradient(loss, w)\r\n    /home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:1014 gradient\r\n        unconnected_gradients=unconnected_gradients)\r\n    /home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py:76 imperative_grad\r\n        compat.as_str(unconnected_gradients.value))\r\n    /home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py:138 _gradient_function\r\n        return grad_fn(mock_op, *out_grads)\r\n    /home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/while_v2.py:336 _WhileGrad\r\n        body_graph = _get_graph(while_op, \"body\")\r\n    /home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/while_v2.py:557 _get_graph\r\n        func_graph = function_def_to_graph.function_def_to_graph(fdef, input_shapes)\r\n    /home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/function_def_to_graph.py:65 function_def_to_graph\r\n        importer.import_graph_def_for_function(graph_def, name=\"\")\r\n    /home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py:412 import_graph_def_for_function\r\n        graph_def, validate_colocation_constraints=False, name=name)\r\n    /home/weixu/venvs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py:505 _import_graph_def_internal\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: The inner -1 dimensions of input.shape=[] must match the inner 1 dimensions of updates.shape=[4,20]: Shapes must be equal rank, but are 0 and 1 for 'ResourceScatterNdUpdate' (op: 'ResourceScatterNdUpdate') with input shapes: [], [4,1], [4,20].\r\n```", "comments": ["@emailweixu ,\r\nCan you try running the code in latest -` tf-nightly 2.1.0.dev20191110 `version? Issue seemed to be fixed, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/188e307b70eaf4ddbb43fd26e8be1c87/34090.ipynb) of colab for the same.Thanks!\r\n\r\n", "Yes, it works with tf-nightly", "@emailweixu ,\r\nCan you please try using the nightly version as the issue is fixed in the latest version? kindly close the issue if its resolved!", "Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34090\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34090\">No</a>\n"]}, {"number": 34089, "title": "Add `any` support for autograph with dataset", "body": "\r\nThis PR tries to address the issue where `any` in autograph\r\ndoes not work with tf.data:\r\n```\r\nimport tensorflow as tf\r\ndef g():\r\n  dataset = tf.data.Dataset.from_tensor_slices([False, True, False])\r\n  return any(dataset)\r\nprint(\"G: \", g())\r\n@tf.function\r\ndef f():\r\n  dataset = tf.data.Dataset.from_tensor_slices([False, True, False])\r\n  return any(dataset)\r\nprint(\"F: \", f())\r\n..........\r\n    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n```\r\n\r\nThis PR translate `any(dataset)` to\r\n```\r\ndataset.filter(lambda x: x).take(1).reduce\r\n```\r\nwhich works in tf.data's pipeline.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@mdanatg Thanks for the review! I will create a follow up PR once this PR is merged.", "@mdanatg  (update)  I will create a follow up PR  for `all` as well, once this PR is merged.", "@mdanatg The PR has been updated. Please let me know if there are any other issues."]}, {"number": 34088, "title": "metrics = ['accuracy'] and metrics = [tf.metrics.Accuracy()] produces different results", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- **Python version**: Python 3.6.8\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nDeclaring metrics as a list of tensorflow.python.keras.metrics.BinaryAccuracy or tensorflow.python.keras.metrics.Accuracy produces an error. Declaring the metrics as a string list works as expected.\r\n\r\n### Source code / logs\r\n\r\nBelow is the source that can reproduce the issue.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(1)\r\ntf.random.set_seed(1)\r\nBATCH_SIZE = 32\r\n\r\n#Import mnist dataset as numpy arrays\r\n(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()#Import\r\nx_train = x_train / 255.0 #normalizing\r\ny_train = y_train.astype(dtype='float32')\r\nx_train = x_train.astype(dtype='float32')\r\n\r\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1]*x_train.shape[2]))#Reshaping the 2D picture\r\n\r\n##############################################################################################\r\n#THIS BLOCK CREATES A DATASET FROM THE NUMPY ARRAYS. IT WILL BE USED FOR THE CASE OF TF.DATA DATASET INPUTS\r\ntfdata_dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\ntfdata_dataset_train = tfdata_dataset_train.batch(BATCH_SIZE).repeat()\r\n##############################################################################################\r\n\r\n#Create model\r\nkeras_model = tf.keras.models.Sequential([\r\n    tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n    tf.keras.layers.Dropout(0.2, seed=1),\r\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\n\r\nmetrics = [tf.metrics.Accuracy()]\r\n#metrics = ['accuracy']\r\n\r\n#Compile the model\r\nkeras_model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                    loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n                    metrics=metrics)\r\n\r\n\r\n#Train with tf.data datasets\r\nkeras_training_history = keras_model.fit(tfdata_dataset_train,\r\n                epochs=1,\r\n                steps_per_epoch=60000//BATCH_SIZE\r\n                )\r\n########################\r\n```\r\n\r\n### Output using metrics=['accuracy']\r\n\r\n```\r\n2019-11-07 21:47:17.874727: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\r\n2019-11-07 21:47:17.875199: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49747e0 executing computations on platform Host. Devices:\r\n2019-11-07 21:47:17.875253: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-07 21:47:17.876808: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.\r\n2019-11-07 21:47:18.324898: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.\r\nTrain for 1875 steps\r\n2019-11-07 21:47:18.992644: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.\r\n1875/1875 [==============================] - 26s 14ms/step - loss: 0.2265 - accuracy: 0.9329\r\n```\r\n### Output using metrics = [tf.metrics.Accuracy()]\r\n\r\n```\r\n2019-11-07 21:51:01.199548: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\r\n2019-11-07 21:51:01.200090: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a91560 executing computations on platform Host. Devices:\r\n2019-11-07 21:51:01.200143: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-11-07 21:51:01.201813: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.\r\n2019-11-07 21:51:01.721895: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.\r\nTrain for 1875 steps\r\n2019-11-07 21:51:02.375751: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of system memory.\r\n   1/1875 [..............................] - ETA: 23:32Traceback (most recent call last):\r\n  File \"bug.py\", line 41, in <module>\r\n    steps_per_epoch=60000//BATCH_SIZE\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 73, in distributed_function\r\n    per_replica_function, args=(model, x, y, sample_weights))\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 760, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1787, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2132, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 264, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 315, in train_on_batch\r\n    model, outs, targets, sample_weights=sample_weights, masks=masks)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 74, in _eager_metrics_fn\r\n    skip_target_masks=model._prepare_skip_target_masks())\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2063, in _handle_metrics\r\n    target, output, output_mask))\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2014, in _handle_per_output_metrics\r\n    metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 1067, in call_metric_function\r\n    return metric_fn(y_true, y_pred, sample_weight=weights)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\", line 193, in __call__\r\n    replica_local_fn, *args, **kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\", line 1135, in call_replica_local_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\", line 176, in replica_local_fn\r\n    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/metrics_utils.py\", line 75, in decorated\r\n    update_op = update_state_fn(*args, **kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\", line 581, in update_state\r\n    matches = self._fn(y_true, y_pred, **self._fn_kwargs)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/metrics.py\", line 2750, in accuracy\r\n    y_pred.shape.assert_is_compatible_with(y_true.shape)\r\n  File \"/home/developer/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 1115, in assert_is_compatible_with\r\n    raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (None, 10) and (None, 1) are incompatible\r\n```\r\n\r\n", "comments": ["The meaning of `'accuracy'` depends on the loss function. The one that corresponds to `sparse_categorical_crossentropy` is `tf.keras.metrics.SparseCategoricalAccuracy()`, not `tf.metrics.Accuracy()`.", "@canesqui \r\nPlease use `metrics = [tf.keras.metrics.SparseCategoricalAccuracy()] `in place of `metrics = [tf.metrics.Accuracy()]` then you can see the same results.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c31c3ed5a3e35e483fa34ac76b2ed48e/untitled350.ipynb). Thanks!", "Thank you @netw0rkf10w and @ravikyram for pointing that out. @netw0rkf10w, can you please share the documentation where I can find information on declaring metrics as a string array and how it would look for the corresponding metric according to the loss function? Once again, thank you for your help. ", "@canesqui \r\n\r\nCan you please go through the below [link](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy) and see if it helps you.Thanks!", "I stumbled across a very similar issue today, and also earlier (where I noticed that `'mse'` and `tf.keras.metrics.mse` behaved differently, IIRC). Therefore, I have been resorting to using `tf.keras.metric.get()` to sanitize all inputs. However, with `accuracy`, I have now found that `'accuracy'` behaves differently from `tf.keras.metrics.get('accuracy')`. This is super-unexpected, and as far as I can see, this is not documented, either. The link posted by @ravikyram does not address this issue.\r\n\r\nAlso, not only do the two versions behave differently in terms of accepting tensors of different shapes (one does, the other does not, see OP), also the numerical results can differ between the two:\r\n\r\n```\r\n\"\"\"Bug.\"\"\"\r\n# import keras\r\nimport numpy as np\r\nimport tensorflow.keras as keras\r\n\r\nX = np.empty([10, 224, 224, 3])\r\nY = np.empty([10, 2])\r\n\r\nMODEL = keras.applications.vgg16.VGG16(weights=None, classes=2)\r\n\r\nMODEL.compile(optimizer=keras.optimizers.Adam(),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nMODEL.fit(X, Y, epochs=10)\r\n\r\nMODEL.compile(optimizer=keras.optimizers.Adam(),\r\n              loss='categorical_crossentropy',\r\n              metrics=[keras.metrics.get('accuracy')])\r\nMODEL.fit(X, Y, epochs=10)\r\n```\r\ngives (for example)\r\n```\r\nTrain on 10 samples\r\nEpoch 1/10\r\n\r\n10/10 [==============================] - 4s 389ms/sample - loss: inf - accuracy: 0.9000\r\nEpoch 2/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 3/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 4/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 5/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 6/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 7/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 8/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 9/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nEpoch 10/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.9000\r\nTrain on 10 samples\r\nEpoch 1/10\r\n\r\n10/10 [==============================] - 1s 131ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 2/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 3/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 4/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 5/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 6/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 7/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 8/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 9/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\nEpoch 10/10\r\n\r\n10/10 [==============================] - 0s 8ms/sample - loss: nan - accuracy: 0.0000e+00\r\n```\r\n\r\nBy the way, the problem is the same when using `keras` instead of `tf.keras`.", "@bersbersbers \r\n\r\nWill it be possible for you to raise a new issue by filling [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!", "> @netw0rkf10w, can you please share the documentation where I can find information on declaring metrics as a string array and how it would look for the corresponding metric according to the loss function?\r\n\r\nSorry for the late reply. Unfortunately I don't remember how I learned about that, but definitely not from the official documentation or tutorials!", "@canesqui \r\nPlease, let us know if this is still an issue?. Please, close this issue if it is already resolved.Thanks!", "@ravikyram I believe this is still an issue, at the least same issue (that I have been asked to re-file as #34451 and which was closed by accident) is still there in nightly.", "@canesqui I am closing this issue as this issue is being tracked here(#34451). Please follow this issue for more updates. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34088\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34088\">No</a>\n"]}, {"number": 34087, "title": "[TF 2.0] Add batched gradient function to GradientTape", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThere should be a feature for batched gradient computation in GradientTape similar to batched_jacobian.\r\n\r\n**Will this change the current api? How?**\r\n`GradientTape.batched_gradient` would exist.\r\n\r\n**Who will benefit with this feature?**\r\nIf someone wants to compute the second derivative of an output w.r.t. the input. Here is an example:\r\nLet's assume `x=(x_1, x_2, x_3)` and we have some function (network) which gives us a scalar output for every x `f(x)=y`. Computing the second derivative of the `f(x)` w.r.t. `x_i` `(d^2f(x)/dx_i^2)` involves either a gradient computation + jacobian computation or a gradient computation + loop:\r\n1. solution with jacobian:\r\n```\r\nwith tf.GradientTape(True) as g:\r\n  g.watch(x)\r\n  with tf.GradientTape() as gg:\r\n    gg.watch(x)\r\n    y = f(x)\r\n  dy_dx = gg.gradient(y, x)\r\nd2y_dx2 = g.jacobian(y, x) # here some filtering is requires to get only the diagonal elements\r\n```\r\n2. solution with loop of gradients:\r\n```\r\nwith tf.GradientTape(True) as g:\r\n  x_i = [x[i] for i in range(len(x))]\r\n  [g.watch(_x) for _x in x_i]\r\n  x = tf.stack(x_i, 0)\r\n  with tf.GradientTape() as gg:\r\n    gg.watch(x)\r\n    y = f(x)\r\n  dy_dx = gg.gradient(y, x)\r\n  dy_dx_i = [dy_dx[i] for i in range(len(dy_dx))]\r\nd2y_dx2 = tf.stack([g.gradient(grad, _x) for grad, _x in zip(dy_dx_i, x_i)])\r\n```\r\n\r\nThe first solution is expensive due to the jacobian computation and for the second one, one has to split up the tensor before and after and loop over gradient computation.", "comments": ["@n-gao,\r\nSorry for the delayed response. Can you please let us know if this is [the functionality](https://www.tensorflow.org/guide/advanced_autodiff#higher-order_gradients) that you are looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34086, "title": "tf.distriubte.strategy() doesn't support multiple input, multiple input will report an error:TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'", "body": "Hi,I find a bug.I have found that the workaround those not work if the model has multiple inputs. The following code fails:\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n#strategy = tf.distribute.MirroredStrategy()\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\ndef generator():\r\n    while True:\r\n        yield [np.ones([10, 10], np.float32), np.ones([10, 10], np.float32)], np.ones([10, 1], np.float32)\r\nwith strategy.scope():\r\n    inputA = tf.keras.layers.Input(shape=(10,))\r\n    inputB = tf.keras.layers.Input(shape=(10,))\r\n    output = tf.keras.layers.Concatenate()([inputA, inputB])\r\n    output = tf.keras.layers.Dense(1, input_shape=(10,), activation=\"relu\")(output)\r\n    model = tf.keras.models.Model(inputs=[inputA, inputB], outputs=output)\r\n    model.compile('Adam', 'mae')\r\n    model.fit(generator(), steps_per_epoch=1000, epochs=10)\r\n\r\nThe error is as follows\uff1a\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'\r\nHow can I solve this problem?", "comments": ["@lijinze9456yy000 \r\n\r\nCan you please let us know which version of TensorFlow you are using?. Thanks!", "@lijinze9456yy000 \r\n\r\nAny update on this issue please.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I am using TF2.0 with this docker  \"tensorflow/tensorflow:latest-gpu-py3-jupyter\", and have a similar problem, my model has two images and a vector input and it has 14 categorical outputs each with 3 clases. \r\nthis line works but terrible slow\r\n```python\r\nT_history = classification_model.fit_generator(trainGenerator, steps_per_epoch=100,\r\n                                              validation_data=validation_generator,\r\n                                              validation_steps=validation_steps, \r\n                                              callbacks=callbacks_list,\r\n                                              epochs=6,\r\n                                              use_multiprocessing=True,\r\n                                              workers=8,\r\n                                              max_queue_size=50)\r\n```\r\nand this one gives the error reported here:\r\n```python\r\nT_history = classification_model.fit(trainGenerator, steps_per_epoch=100,\r\n                                              validation_data=validation_generator,\r\n                                              validation_steps=validation_steps, \r\n                                              callbacks=callbacks_list,\r\n                                              epochs=6,\r\n                                              use_multiprocessing=True,\r\n                                              workers=8,\r\n                                              max_queue_size=50,\r\n                                              shuffle=False)\r\n```", "I confirmed that I encounter a similar issue.", "use a tuple, not a list, in the return statement of the generator:\r\n\r\nreturn ((x1, x2, ...), y)\r\n\r\nnot:\r\n\r\nreturn [x1, x2, ...], y\r\n\r\n+ shuffle=False in the call to fit\r\n", "I had the same issue and it is indeed solved by the generator not yielding a list but a tuple for the FEATURES. I feel like this is a bit inconsistent, as the features are supposed to be a list when you call model.fit without a generator (and multiple inputs of course)"]}]