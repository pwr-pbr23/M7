[{"number": 11056, "title": "11055: test to reproduce Keras load model raises ValueError when loading optimizer weights", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@fchollet this is a repro case for #11055, not a merge request.", "Thanks for the report. I have identified the cause and will fix it. It's a bit of an edge case. As a temporary workaround, you can skip the optimizer loading step when loading your model, by passing `compile=False` to `load_model`.", "Thank you for looking into it! I look forward to reading the patch.", "This has been fixed in upstream Keras. We will backport the fix to TensorFlow in a few days.", "@fchollet, shall I apply the fix to this PR? I saw in the Keras repo that simply removing the sort of the trainable weights by name fixes the serialization bug.\r\n\r\nAlso, I have noticed that if I load the model with compile=False or delete the optimizer weights before loading the model that the Exception is not raised. However, the loaded model predictions preform as if the model was not trained at all. I don't understand how this could be.. I am retraining the model with this bug fix applied now to see if it's related or not.", "Closing since this is probably better as an issue filed in Keras at this point.  Thanks for the help @kevinjos !!"]}, {"number": 11055, "title": "Keras load model raises ValueError when loading optimizer weights", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I extended models_test.py to reproduce the bug\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 8.8\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.2.0-1106-g1f82b7a', '1.2.0')\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: nvidia Tesla K-80 / 11.17gb\r\n- **Exact command to reproduce**: See test in PR\r\n\r\n### Describe the problem\r\nUpon calling keras.models.load_model(fn) against a file generated by keras.models.save_model() a ValueError is raised. I believe it could be related to using padding='same' in the Conv2D layer.\r\n\r\nTraceback (most recent call last):\r\n  File \"models_test.py\", line 166, in test_saving_cnn\r\n    model = keras.models.load_model(fname)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/keras/python/keras/models.py\", line 316, in load_model\r\n    model.optimizer.set_weights(optimizer_weight_values)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/keras/python/keras/optimizers.py\", line 95, in set_weights\r\n    'provided weight shape ' + str(w.shape))\r\nValueError: Optimizer weight shape (512,) not compatible with provided weight shape (64,)\r\n\r\n### Source code / logs\r\nPlease see PR below.\r\n", "comments": ["The test I wrote here https://github.com/tensorflow/tensorflow/pull/11056 reproduces the model deserialization glitch.\r\n\r\nSimilar issues have been discussed quite a bit in the Keras repo. I haven't seen a comprehensive solution, only workarounds. I wasn't sure if this should be reported here or there, but since I am using the tensorflow packaged Keras and I wrote the test in the tensorflow test-cases I thought to raise the issue here.", "Nagging Assignee @fchollet: It has been 453 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "As mentioned in the PR, it is better to be asked as a Keras issue. Since this is old, this issue might have already been resolved. If not, please create an issue for the Keras team."]}, {"number": 11054, "title": "Update get_started.md", "body": "```\r\ninput_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_train}, y_train, 4, num_epochs=1000)\r\n\r\n# train\r\nestimator.fit(input_fn=input_fn, steps=1000)\r\n# Here we evaluate how well our model did. \r\ntrain_loss = estimator.evaluate(input_fn=input_fn)\r\neval_loss = estimator.evaluate(input_fn=eval_input_fn)\r\n```\r\nThe `eval_input_fn` is not defined here.", "comments": ["Can one of the admins verify this patch?", "Thanks @raoqiyu!"]}, {"number": 11053, "title": "A custom model of Getting Started", "body": "In the section \"A custom model\" of \"Getting Started With TensorFlow \" on [Tensorflow Website](https://www.tensorflow.org/get_started/get_started#a_custom_model)\r\n```\r\ninput_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_train}, y_train, 4, num_epochs=1000)\r\n\r\n# train\r\nestimator.fit(input_fn=input_fn, steps=1000)\r\n# Here we evaluate how well our model did. \r\ntrain_loss = estimator.evaluate(input_fn=input_fn)\r\neval_loss = estimator.evaluate(input_fn=eval_input_fn)\r\n```\r\nThe `eval_input_fn` is not defined here.", "comments": []}, {"number": 11052, "title": " standard_tensorboard_wsgi() missing 1 required positional argument: 'plugins'", "body": "how set plugins ? thanks", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11051, "title": "tensorflow inference in iOS produces EXC_BAD_ACCESS error when memory mapped graph is used", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX Sierra (10.12.5)\r\n- **TensorFlow installed from (source or binary)**: source & binary (tested both)\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: 0.5.1-homebrew\r\n- **CUDA/cuDNN version**: CPU version used\r\n- **GPU model and memory**: CPU version used\r\n- **Exact command to reproduce**: \r\n\r\nApologies if I posted an issue not appropriate here.\r\nI asked this issue on stackoverflow a week ago but seems that no one interested in this issue.\r\n\r\n===========\r\nI tested both iOS simple and camera example in tensorflow GitHub repo and they worked well.\r\n\r\nI checked those projects and recognized that camera example can use memory mapped graph if I modify constant variable \"model_uses_memory_mapping\" to true, while simple example cannot.\r\n\r\nSo I modified simple example source to implement same function as camera example, and it seems that the mmapped graph loaded without problem, but when I run inference with session->Run method, the app gave me EXE_BAD_ACCESS error.\r\n\r\nI think I've done everything what I can do but still same error.\r\n\r\nNo idea what else I can do for I'm not good at iOS nor tensorflow core functions.\r\n\r\nCould someone guide me how can I resolve this?\r\n\r\nFYI, run inference with optimized or quantized graph (with model_uses_memory_mapping set to false) works well.\r\n\r\na) here's message I get when i execute session->Run method:\r\n\r\n```\r\ntf_ios_makefile_example`tensorflow::Env::NewReadOnlyMemoryRegionFromFile:\r\n    0x103157ad0 <+0>:  pushq  %rbp\r\n    0x103157ad1 <+1>:  movq   %rsp, %rbp\r\n    0x103157ad4 <+4>:  pushq  %r15\r\n    0x103157ad6 <+6>:  pushq  %r14\r\n    0x103157ad8 <+8>:  pushq  %rbx\r\n    0x103157ad9 <+9>:  pushq  %rax\r\n    0x103157ada <+10>: movq   %rcx, %r14\r\n    0x103157add <+13>: movq   %rdx, %r15\r\n    0x103157ae0 <+16>: movq   %rdi, %rbx\r\n    0x103157ae3 <+19>: movq   (%rsi), %rax\r\n    0x103157ae6 <+22>: leaq   -0x20(%rbp), %rcx\r\n->  0x103157aea <+26>: callq  *0x10(%rax)  \r\n    0x103157aed <+29>: cmpq   $0x0, (%rbx)\r\n    0x103157af1 <+33>: jne    0x103157b06               ; <+54>\r\n    0x103157af3 <+35>: movq   -0x20(%rbp), %rsi\r\n    0x103157af7 <+39>: movq   (%rsi), %rax\r\n    0x103157afa <+42>: movq   %rbx, %rdi\r\n    0x103157afd <+45>: movq   %r15, %rdx\r\n    0x103157b00 <+48>: movq   %r14, %rcx\r\n    0x103157b03 <+51>: callq  *0x18(%rax)\r\n    0x103157b06 <+54>: movq   %rbx, %rax\r\n    0x103157b09 <+57>: addq   $0x8, %rsp\r\n    0x103157b0d <+61>: popq   %rbx\r\n    0x103157b0e <+62>: popq   %r14\r\n    0x103157b10 <+64>: popq   %r15\r\n    0x103157b12 <+66>: popq   %rbp\r\n    0x103157b13 <+67>: retq   \r\n    0x103157b14 <+68>: nopw   %cs:(%rax,%rax)\r\n```\r\nand the error message shown on the line was: **Thread19: EXC_BAD_ACCESS(code=EXC_I386_GPFLT)**\r\n\r\nb) and here's some of code I added/modified/used in simple example:\r\n\r\n**global variable declaration:**\r\n\r\n```\r\nstatic NSString* model_file_name = @\"mmapped_graph\";\r\nstatic NSString* model_file_type = @\"pb\";\r\nconst bool model_uses_memory_mapping = true;  //use memory mapped graph\r\n\r\nstatic NSString* labels_file_name = @\"retrained_labels\";\r\nstatic NSString* labels_file_type = @\"txt\";\r\n\r\nconst int wanted_width = 299;\r\nconst int wanted_height = 299;\r\nconst int wanted_channels = 3;\r\nconst float input_mean = 128.0f;\r\nconst float input_std = 128.0f;\r\nconst std::string input_layer = \"Mul\";\r\nconst std::string output_layer = \"final_result\";\r\n```\r\nmethod definition to read mmapped graph\r\n\r\n( referenced from camera example)\r\n\r\n```\r\ntensorflow::Status LoadMemoryMappedModel(\r\n    NSString* file_name, NSString* file_type,\r\n    std::unique_ptr<tensorflow::Session>* session,\r\n    std::unique_ptr<tensorflow::MemmappedEnv>* memmapped_env) {\r\n    NSString* network_path = FilePathForResourceName(file_name, file_type);\r\n    memmapped_env->reset(\r\n        new tensorflow::MemmappedEnv(tensorflow::Env::Default())\r\n    );\r\n    tensorflow::Status mmap_status =\r\n    (memmapped_env->get())->InitializeFromFile([network_path UTF8String]);\r\n    if (!mmap_status.ok()) {\r\n        LOG(ERROR) << \"MMap failed with \" << mmap_status.error_message();\r\n        return mmap_status;\r\n    }\r\n\r\n    tensorflow::GraphDef tensorflow_graph;\r\n    tensorflow::Status load_graph_status = ReadBinaryProto(\r\n                                                           memmapped_env->get(),\r\n                                                           tensorflow::MemmappedFileSystem::kMemmappedPackageDefaultGraphDef,\r\n                                                           &tensorflow_graph);\r\n    if (!load_graph_status.ok()) {\r\n        LOG(ERROR) << \"MMap load graph failed with \"\r\n        << load_graph_status.error_message();\r\n        return load_graph_status;\r\n    }\r\n\r\n    tensorflow::SessionOptions options;\r\n    // Disable optimizations on this graph so that constant folding doesn't\r\n    // increase the memory footprint by creating new constant copies of the weight\r\n    // parameters.\r\n    options.config.mutable_graph_options()\r\n    ->mutable_optimizer_options()\r\n    ->set_opt_level(::tensorflow::OptimizerOptions::L0);\r\n    options.env = memmapped_env->get();\r\n\r\n    tensorflow::Session* session_pointer = nullptr;\r\n    tensorflow::Status session_status =\r\n    tensorflow::NewSession(options, &session_pointer);\r\n    if (!session_status.ok()) {\r\n        LOG(ERROR) << \"Could not create TensorFlow Session: \" << session_status;\r\n        return session_status;\r\n    }\r\n\r\n    tensorflow::Status create_status = session_pointer->Create(tensorflow_graph);\r\n    //tensorflow::Status create_status = session_pointer->Create(*(tensorflow::GraphDef *)tensorflow_graph);\r\n\r\n    if (!create_status.ok()) {\r\n        LOG(ERROR) << \"Could not create TensorFlow Graph: \" << create_status;\r\n        return create_status;\r\n    }\r\n\r\n    session->reset(session_pointer);\r\n\r\n\r\n    return tensorflow::Status::OK();\r\n}\r\n```\r\n**and I load the graph and run inference like this**\r\n\r\n```\r\nNSString* RunInferenceOnImage() {\r\n  tensorflow::SessionOptions options;\r\n    std::unique_ptr<tensorflow::Session> session;\r\n\r\n  tensorflow::GraphDef tensorflow_graph;\r\n  LOG(INFO) << \"Graph created.\";\r\n\r\n    tensorflow::Status load_status;\r\n\r\n    if (model_uses_memory_mapping) {\r\n        //use memmapped graph - gives me an error\r\n        std::unique_ptr<tensorflow::MemmappedEnv>  tf_memmapped_env;\r\n        load_status = LoadMemoryMappedModel(model_file_name, model_file_type, &session, &tf_memmapped_env);\r\n    } else {\r\n        // use optimized or quantized graph - this works well\r\n        NSString* network_path = FilePathForResourceName(model_file_name, model_file_type);\r\n        load_status = PortableReadFileToProto([network_path UTF8String],&session, &tensorflow_graph);\r\n    }\r\n\r\n    if (!load_status.ok()) {\r\n        LOG(FATAL) << \"Couldn't load model: \" << load_status;\r\n    }\r\n\r\n  // Read the label list\r\n  NSString* labels_path = FilePathForResourceName(@\"retrained_labels\", @\"txt\");\r\n  std::vector<std::string> label_strings;\r\n  std::ifstream t;\r\n  t.open([labels_path UTF8String]);\r\n  std::string line;\r\n  while(t){\r\n    std::getline(t, line);\r\n    label_strings.push_back(line);\r\n  }\r\n  t.close();\r\n\r\n  // Read the image.\r\n  NSString* image_path = FilePathForResourceName(@\"testimage\", @\"jpg\");\r\n  int image_width;\r\n  int image_height;\r\n  int image_channels;\r\n  std::vector<tensorflow::uint8> image_data = LoadImageFromFile(\r\n    [image_path UTF8String], &image_width, &image_height, &image_channels);\r\n    LOG(INFO) << \"Graph created5.\";\r\n\r\n  // image_channel is set to 4 from LoadImageFromFile method (not modified)\r\n\r\n  assert(image_channels >= wanted_channels);\r\n  tensorflow::Tensor image_tensor(\r\n      tensorflow::DT_FLOAT,\r\n      tensorflow::TensorShape({\r\n          1, wanted_height, wanted_width, wanted_channels}));\r\n  auto image_tensor_mapped = image_tensor.tensor<float, 4>();\r\n\r\n  tensorflow::uint8* in = image_data.data();\r\n  tensorflow::uint8* in_end = (in + (image_height * image_width * image_channels));\r\n  float* out = image_tensor_mapped.data();\r\n  for (int y = 0; y < wanted_height; ++y) {\r\n    const int in_y = (y * image_height) / wanted_height;\r\n    tensorflow::uint8* in_row = in + (in_y * image_width * image_channels);\r\n    float* out_row = out + (y * wanted_width * wanted_channels);\r\n    for (int x = 0; x < wanted_width; ++x) {\r\n      const int in_x = (x * image_width) / wanted_width;\r\n      tensorflow::uint8* in_pixel = in_row + (in_x * image_channels);\r\n      float* out_pixel = out_row + (x * wanted_channels);\r\n      for (int c = 0; c < wanted_channels; ++c) {\r\n        out_pixel[c] = (in_pixel[c] - input_mean) / input_std;\r\n      }\r\n    }\r\n  }\r\n    NSString* result = @\" Graph loaded!\";\r\n  result = [NSString stringWithFormat: @\"%@ - %d, %s - %dx%d\", result,\r\n    label_strings.size(), label_strings[0].c_str(), image_width, image_height];\r\n\r\n  std::vector<tensorflow::Tensor> outputs;\r\n    if(session.get()) {\r\n        LOG(INFO) << \"SESSION OK!!!!!!\";\r\n    }\r\n\r\n\r\n  tensorflow::Status run_status = session->Run({{input_layer, image_tensor}},\r\n                               {output_layer}, {}, &outputs);\r\n  // EXC_BAD_ACCESS error occur when session Run method called\r\n\r\n  if (!run_status.ok()) {\r\n  //  LOG(ERROR) << \"Running model failed: \" << run_status;\r\n    tensorflow::LogAllRegisteredKernels();\r\n    result = @\"Error running model\";\r\n    return result;\r\n  }\r\n  tensorflow::string status_string = run_status.ToString();\r\n  result = [NSString stringWithFormat: @\"%@ - %s\", result,\r\n    status_string.c_str()];\r\n\r\n  tensorflow::Tensor* output = &outputs[0];\r\n  const int kNumResults = 5;\r\n  const float kThreshold = 0.1f;\r\n  std::vector<std::pair<float, int> > top_results;\r\n  GetTopN(output->flat<float>(), kNumResults, kThreshold, &top_results);\r\n\r\n  std::stringstream ss;\r\n  ss.precision(3);\r\n  for (const auto& result : top_results) {\r\n    const float confidence = result.first;\r\n    const int index = result.second;\r\n\r\n    ss << index << \" \" << confidence << \"  \";\r\n\r\n    // Write out the result as a string\r\n    if (index < label_strings.size()) {\r\n      // just for safety: theoretically, the output is under 1000 unless there\r\n      // is some numerical issues leading to a wrong prediction.\r\n      ss << label_strings[index];\r\n    } else {\r\n      ss << \"Prediction: \" << index;\r\n    }\r\n\r\n    ss << \"\\n\";\r\n  }\r\n\r\n  LOG(INFO) << \"Predictions: \" << ss.str();\r\n\r\n  tensorflow::string predictions = ss.str();\r\n  result = [NSString stringWithFormat: @\"%@ - %s\", result,\r\n    predictions.c_str()];\r\n\r\n  return result;\r\n}\r\n```\r\n\r\nYou can find original (unmodified) simple example project here: https://github.com/tensorflow/tensorflow/tree/v1.1.0/tensorflow/contrib/ios_examples/simple\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.1.0/tensorflow/contrib/ios_examples/simple/RunModelViewController.mm\r\n\r\nalso you can find LoadImageFromFile method here: https://github.com/tensorflow/tensorflow/blob/v1.1.0/tensorflow/contrib/ios_examples/simple/ios_image_load.mm", "comments": ["@petewarden ", "From looking at the error, the immediate cause may be the `memmapped_env` variable being destroyed by the time that code is called. I would recommend making sure that the lifetime of the variable you pass into `LoadMemoryMappedModel()` is the same as that of the model. For example, you could make it a global variable, or some other long-lived scope.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11050, "title": "Fix typos", "body": "This PR fixes some typos: `as as`, `is is`, `for for`, `not not`, and `are are`.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11049, "title": "variable name change and documentation clarification", "body": "Addressing #9927", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Could you take a look at the `layers_test` failure?\r\n\r\n```\r\n======================================================================\r\nERROR: testSparsePartialFlatten (__main__.PartialFlattenTest)\r\nTest `_inner_flatten` on `SparseTensor`s.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/bazel_pip/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/bazel_pip/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1434, in testSparsePartialFlatten\r\n    expected_indices, expected_values, _ = _sparsify(reshaped_random_)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/bazel_pip/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/bazel_pip/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1399, in _sparsify\r\n    values = array[non_zero]\r\nIndexError: index 3 is out of bounds for axis 1 with size 3\r\n```", "@drpngx I did. It's unrelated to the patch. It also occurred in a completely separate PR (https://github.com/tensorflow/tensorflow/pull/11012).\r\n\r\nThat error occurs occasionally depending on what random values were generated in the setup of ```testSparsePartialFlatten```. Specifically the ```random_ = np.random.rand(*shape)``` command\r\n```\r\n  def testSparsePartialFlatten(self):\r\n    \"\"\"Test `_inner_flatten` on `SparseTensor`s.\"\"\"\r\n    shape = [4, 3, 11, 6, 1, 3]\r\n    np.random.seed(10301)\r\n    random_ = np.random.rand(*shape)\r\n    indices, values, _ = _sparsify(random_)\r\n    ...\r\n```\r\n\r\n\r\nI isolated the test (https://gist.github.com/jalammar/e560aea1557ea12fec943762f32ab6eb) and ran it a few times. It fails about 40% of the time.\r\n\r\nRunning the test again would likely pass. But that test needs to be addressed as well.\r\n", "/CC: @yifeif if you know about the layers test.", "Thanks @jalammar !! Are you saying this needs a seed?", "Jenkins, test this please.", "@drpngx Thanks! \r\n\r\nActually, upon further investigation, the culprit seems not to be the random value generation in testSparsePartialFlatten. It seems to be inconsistent behavior from numpy.where().", "Are you able to formulate a fix?", "@drpngx I reported the underlying issue to the numpy team (https://github.com/numpy/numpy/issues/9304). This second PR (https://github.com/tensorflow/tensorflow/pull/11069) is my attempt at fixing the test."]}, {"number": 11048, "title": "tensorflow-gpu aborts with CuDNN v6", "body": "### System information\r\n- **The code can be found from the notebook [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb)**:\r\n- **OS Platform and Distribution: Linux Ubuntu 16.04**:\r\n- **TensorFlow installed from pip**:\r\n- **TensorFlow version ('v1.2.0-rc2-21-g12f033d', '1.2.0')**:\r\n- **Bazel version : None**:\r\n- **CUDA/cuDNN version : CUDA v8, CuDNN v6**:\r\n- **GPU:  GeForce GTX 950M (2GB)**:\r\n- **Exact command to reproduce: Run the notebook**:\r\n\r\nI was trying out some code for a convolutional network using tensorflow-gpu but the following messages appear.  \r\n```\r\n2017-06-25 21:53:48.099327: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 21:53:48.099345: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 21:53:48.099361: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 21:53:48.099367: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 21:53:48.099382: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 21:53:48.188830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-06-25 21:53:48.189176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: GeForce GTX 950M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.124\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.96GiB\r\nFree memory: 1.61GiB\r\n2017-06-25 21:53:48.189201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-06-25 21:53:48.189207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-06-25 21:53:48.189224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)\r\n2017-06-25 21:53:48.838272: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 6021 (compatibility version 6000) but source was compiled with 5110 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2017-06-25 21:53:48.838557: F tensorflow/core/kernels/conv_ops.cc:671] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\nAborted (core dumped)\r\n```\r\nThe code runs smoothly if I'm only using CPU.  Is it a bug or feature of tensorflow? How do I get around it?", "comments": ["`E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 6021 (compatibility version 6000) but source was compiled with 5110 (compatibility version 5100)`\r\n\r\nThe error is very clear. The binary you are using was compiled using a different version of CuDNN. Either get that version (v5.1) or compile it yourself with version 6.0.", "Yes, I figured that much out. I reverted to v5.1 and it worked. But how do I compile it with version 6.0?", "@rohitrango follow [the official guide](https://www.tensorflow.org/install/install_sources). `configure `will eventually ask you what version you want to use.", "If you still need assistance on how to compile with CuDNN 6.0, please ask this question on Stack Overflow, for it is neither a feature request nor a bug. Thanks!", "@rohitrango how to revert? I set the soft-link of libcudnn.so to libcudnn.so.5 but it doesn't work."]}, {"number": 11047, "title": "fix get_started custom model", "body": "Original file does not run because eval_input_fn is undefined.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@vorpus Thanks for the pull request (PR). This issue is already fixed by an internal change pending syncing to external. Therefore I will close this PR."]}, {"number": 11046, "title": "tf.contrib.xprof not supported on Windows", "body": "opts = tf.contrib.tfprof.model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS\r\nAttributeError: module 'tensorflow.contrib.tfprof' has no attribute 'model_analyzer'\r\n\r\nI have tensorflow version 1.2 and have made all the other transitioning changes and the above error was thrown later. \r\nHas the contrib support been resolved for Windows? Please help\r\n", "comments": ["The `tf.contrib.tfprof` submodule is not yet supported on Windows. @panyx0718 is currently working to add support for this submodule, so assigning this bug to him as a placeholder.", "I think it supports Windows now. Note: Now the api is: tf.profiler\r\nLet me know if it doesn't work"]}, {"number": 11045, "title": "ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'", "body": "### System information\r\n- **custom code**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro 10.0.15063\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: tensorflow_gpu-1.2.0-cp36-cp36m-win_amd64.wh\r\n- **CUDA/cuDNN version**: 8.0.6.1 / 8.0 \r\n- **GPU model and memory**: Geforce 1080 Ti 11GB\r\n- **Exact command to reproduce**: import tensorflow \r\n\r\n### Describe the problem\r\nImporting tensorflow fails immediately on import with the following log.\r\nI'm running Python 3.6.1 from Anaconda. I installed tensorflow-gpu with `pip install tensorflow-gpu`\r\n\r\nI've installed the CUDA/cuDNN libraries per this guide: https://nitishmutha.github.io/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html\r\n\r\nNot sure what I am missing here. Any help would be most appreciated!\r\nTensorflow CPU works great however. This seems limited to the GPU install.\r\n\r\nThank you in advance\r\n\r\n### Source code / logs\r\n\r\n```python\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\ptpython\\repl.py\", line 117, in _execute\r\n    code = compile_with_flags(line, 'eval')\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\ptpython\\repl.py\", line 105, in compile_with_flags\r\n    dont_inherit=True)\r\n  File \"<stdin>\", line 1\r\n    import tensorflow\r\n         ^\r\nSyntaxError: invalid syntax\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\programdata\\anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\programdata\\anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\ptpython\\repl.py\", line 117, in _execute\r\n    code = compile_with_flags(line, 'eval')\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\ptpython\\repl.py\", line 105, in compile_with_flags\r\n    dont_inherit=True)\r\n  File \"<stdin>\", line 1\r\n    import tensorflow\r\n         ^\r\nSyntaxError: invalid syntax\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\programdata\\anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\programdata\\anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```", "comments": ["I'm having the exact same problem on a late 2011 model mac book pro", "@martinwicke can you look at this or redirect? Thanks.", "Yun, do you know what could be causing this behavior on Win10?", "I encountered the same problem. Solved by add <path4cuDNN>/bin into PATH", "Which /bin would that be? The one under the cuda home path?\n\nI will try that when I get home. Thanks.\n\nOn Tue, Jun 27, 2017, 4:34 PM RaymondY1213 <notifications@github.com> wrote:\n\n> I encountered the same problem. Solved by add /bin into PATH\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11045#issuecomment-311514817>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEPL0qCwOmckJZj2VRkf9o15xuBYkBb3ks5sIZGcgaJpZM4OErGK>\n> .\n>\n", "under cuDNN path @dgovil ", "Unfortunately that didn't work for me @RaymondY1213 . Thanks for the suggestion though.\r\n\r\nHere's my current `PATH` sorted alphabetically for easier reading\r\n\r\n```\r\n['C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\Shared '\r\n 'Libraries\\\\redist\\\\intel64\\\\compiler',\r\n 'C:\\\\Program Files (x86)\\\\GtkSharp\\\\2.12\\\\bin',\r\n 'C:\\\\Program Files (x86)\\\\Microsoft VS Code\\\\bin',\r\n 'C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common',\r\n 'C:\\\\Program Files (x86)\\\\Windows Kits\\\\10\\\\Windows Performance Toolkit\\\\',\r\n 'C:\\\\Program Files (x86)\\\\Yarn\\\\bin',\r\n 'C:\\\\Program Files\\\\CMake\\\\bin',\r\n 'C:\\\\Program Files\\\\Docker\\\\Docker\\\\Resources\\\\bin',\r\n 'C:\\\\Program Files\\\\Git\\\\cmd',\r\n 'C:\\\\Program Files\\\\Microsoft SQL Server\\\\130\\\\Tools\\\\Binn\\\\',\r\n 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v8.0\\\\bin',\r\n 'C:\\\\Program Files\\\\NVIDIA GPU Computing '\r\n 'Toolkit\\\\CUDA\\\\v8.0\\\\extras\\\\CUPTI\\\\libx64',\r\n 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v8.0\\\\lib\\\\x64',\r\n 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v8.0\\\\libnvvp',\r\n 'C:\\\\Program Files\\\\Oculus\\\\Support\\\\oculus-runtime',\r\n 'C:\\\\Program Files\\\\Rust stable GNU 1.18\\\\bin',\r\n 'C:\\\\Program Files\\\\dotnet\\\\',\r\n 'C:\\\\Program Files\\\\nodejs\\\\',\r\n 'C:\\\\ProgramData\\\\Anaconda3',\r\n 'C:\\\\ProgramData\\\\Anaconda3\\\\Library\\\\bin',\r\n 'C:\\\\ProgramData\\\\Anaconda3\\\\Scripts',\r\n 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\tf-gpu',\r\n 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\tf-gpu\\\\Library\\\\bin',\r\n 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\tf-gpu\\\\Library\\\\mingw-w64\\\\bin',\r\n 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\tf-gpu\\\\Library\\\\usr\\\\bin',\r\n 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\tf-gpu\\\\Scripts',\r\n 'C:\\\\ProgramData\\\\chocolatey\\\\bin',\r\n 'C:\\\\Users\\\\Dhruv\\\\.cargo\\\\bin',\r\n 'C:\\\\Users\\\\Dhruv\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps',\r\n 'C:\\\\Users\\\\Dhruv\\\\AppData\\\\Local\\\\Yarn\\\\.bin',\r\n 'C:\\\\Users\\\\Dhruv\\\\AppData\\\\Roaming\\\\npm',\r\n 'C:\\\\WINDOWS',\r\n 'C:\\\\WINDOWS\\\\System32\\\\Wbem',\r\n 'C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\',\r\n 'C:\\\\WINDOWS\\\\system32',\r\n 'C:\\\\Windows',\r\n 'C:\\\\Windows\\\\System32\\\\Wbem',\r\n 'C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\',\r\n 'C:\\\\Windows\\\\system32',\r\n 'C:\\\\tools\\\\cmder',\r\n 'C:\\\\tools\\\\cmder\\\\vendor\\\\conemu-maximus5',\r\n 'C:\\\\tools\\\\cmder\\\\vendor\\\\conemu-maximus5\\\\ConEmu',\r\n 'C:\\\\tools\\\\cmder\\\\vendor\\\\conemu-maximus5\\\\ConEmu\\\\Scripts',\r\n 'c:\\\\programdata\\\\anaconda3\\\\Library\\\\bin']\r\n```", "Hello,\r\n\r\nAny progress on this issue ? I'm having the same one on Windows 7 Pro.\r\n\r\nThank you in advance !", "@dgovil @mopimoi I also believe this is a Path issue.\r\nI have the following relevant directories in PATH on my machine:\r\n```\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\libnvvp\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64\r\nC:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common\r\nc:\\tools\\cuda\\bin   # this is for cudnn64_6.dll\r\nC:\\Program Files\\Anaconda3\r\nC:\\Program Files\\Anaconda3\\Scripts\r\nC:\\Program Files\\Anaconda3\\Library\\bin\r\n```\r\n\r\nCan you please check if you have all corresponding directories in PATH?", "@meteorcloudy I've got all those paths in my PATH, with the exception of `c:\\tools\\cuda\\bin` because that's all inside `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin`\r\n\r\nI get this error still. It looks identical to me, but I'm just pasting it again in case there's anything of value here.\r\n\r\n```python\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\ptpython\\repl.py\", line 117, in _execute\r\n    code = compile_with_flags(line, 'eval')\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\ptpython\\repl.py\", line 105, in compile_with_flags\r\n    dont_inherit=True)\r\n  File \"<stdin>\", line 1\r\n    import tensorflow\r\n         ^\r\nSyntaxError: invalid syntax\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\programdata\\anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\programdata\\anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\ptpython\\repl.py\", line 117, in _execute\r\n    code = compile_with_flags(line, 'eval')\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\ptpython\\repl.py\", line 105, in compile_with_flags\r\n    dont_inherit=True)\r\n  File \"<stdin>\", line 1\r\n    import tensorflow\r\n         ^\r\nSyntaxError: invalid syntax\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\programdata\\anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\programdata\\anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "Hmm,, this looks strange. What version of cudnn do you have? Is there a `cudnn64_6.dll` under  `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin` ?", "@meteorcloudy I have cudnn-8.0-windows10-x64-v6.0 and yes there's a cudnn64_6.dll in that path", "Same problems here. Did anyone find a solution to this?", "Hi,\r\nI just wanted to update this ticket.\r\n\r\nWith the exact same setup as before, I've installed tensorflow-gpu 1.3.0 and it works properly.\r\nMaybe it was an incompatibility with tensorflow 1.2, but anyway just wanted to update the ticket for anyone else hitting it."]}, {"number": 11044, "title": "Tensorflow - No valid folders of images found at XXXXX", "body": "So I have a semi custom code written for a new biomedical program that I am developing. I am trying to get my model to begin retraining and I am getting this problem out of nowhere whenever I run \r\n\r\npython retrain.py \\\r\n  --bottleneck_dir=/tf_files/bottlenecks\\\r\n  --how_many_training_steps=100\\\r\n  --model_dir=/tf_files/ inception \\\r\n  --output_graph=/tf_files/retrained_graph.pb \\\r\n  --output_labels=/tf_files/retrained_labels.txt \\\r\n  --image_dir /tf_files/ct \r\n\r\nThe 'ct' folder in question and giving me issues and has a stockpile of images. I have made sure that the folder does not have any other files in it, creating the issue of tensor not picking up the image first. Furthermore, I have ensured that each file has been converted to a jpg. I have no hyphens or dashes in the subfolders and they are all lowercased/ no spaces. \r\n\r\nBelow are the following inputs that are giving me troubles. I have scoured the net for different answers to this problem and after troubleshooting the above mentioned attributes I still am finding the same error message incur. Thanks in advance. \r\n\r\n\r\nroot@8c16ee553d5a:/tf_files# python retrain.py \\\r\n>   --bottleneck_dir=/tf_files/bottlenecks\\\r\n>   --how_many_training_steps=100\\\r\n>   --model_dir=/tf_files/ inception \\\r\n>   --output_graph=/tf_files/retrained_graph.pb \\\r\n>   --output_labels=/tf_files/retrained_labels.txt \\\r\n>   --image_dir /Users/maisiemullin/tf_files/ct \r\n2017-06-25 16:43:47.433755: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 16:43:47.443444: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-25 16:43:47.443724: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nImage directory '/Users/maisiemullin/tf_files/ct' not found.\r\nTraceback (most recent call last):\r\n  File \"retrain.py\", line 1062, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"retrain.py\", line 784, in main\r\n    class_count = len(image_lists.keys())\r\nAttributeError: 'NoneType' object has no attribute 'keys'\r\nroot@8c16ee553d5a:/tf_files# \r\n\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Good day for everyone\r\nI have the same problem with the script on Windows 7\r\n\r\npython retrain.py \\\r\n  --bottleneck_dir=/tf_files/bottlenecks \\\r\n  --how_many_training_steps=500 \\\r\n  --model_dir=/tf_files/inception \\\r\n  --summaries_dir=/tf_files/training_summaries\r\n  --output_graph=/tf_files/retrained_graph.pb \\\r\n  --output_labels=/tf_files/retrained_labels.txt \\\r\n  --image_dir ???\r\n\r\n\"No valid folders or images found at\"\r\nI have tried different variants, but no successful results\r\n  --image_dir d:\\tfi\\\r\n  --image_dir c:/Users/Alexei/AppData/Local/Programs/Python/Python35/tf_files/bottles/\r\n  --image_dir=/tf_files/bottles\r\n", "@wider2 \r\n\r\nIt seems as though you are attatching the directory for the binary bottleneck files and not your image files. It does not need to be an absolute path either, just ensure that if you 'cd' into the current directory that houses the folders. BE SURE to add within the folder you are adding to image_dir= to be filled with jpg.", "thank you\r\nthe work with tensorflow model is very very tricky and need patience", "No problems! \r\n\r\nTensorflow / AI classifiers are brand new modalities and entail a deep level of python knowledge despite what many guides state. I believe it is possible for one to follow the guides without this dedicated knowledge but it becomes extremely tricky through certain steps. Glad to help!", "@ali01  \r\n\r\nThank you for your fast response. It is deeply appreciated! I have found a fix to the problem as within my custom retrain.py file I wrote an incorrect concated script that loads the image directories and processes them through - as i am currently trying a build that allows the retrain file to multi-classify instead of just binary classification. thanks again for the help. ", "here we go again\r\nI have created model successfully\r\nand got optimized model too\r\n\r\nPython works successfully too (when recognize test image)\r\npython label_image.py\r\n --graph=retrained_graph.pb\r\n  --labels=retrained_labels.txt\r\n  --image=hand2.jpg\r\n\r\n\r\nI setup variables in android studio project:\r\n    private static final int INPUT_SIZE = 224;\r\n    private static final int IMAGE_MEAN = 117;\r\n    private static final float IMAGE_STD = 1f;\r\n\r\nbut when I run in Android, I have got this exception:\r\nE/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[Mul], outputs:[final_result]\r\nW/System.err: java.lang.IllegalArgumentException: computed output size would be negative\r\n\r\n", "solution has ben found\r\nI set these values, and application works more stable now\r\n    private static final int INPUT_SIZE = 299;\r\n    private static final int IMAGE_MEAN = 128;\r\n    private static final float IMAGE_STD = 1f;\r\n", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "https://colab.research.google.com/drive/1hFte7dNUZGVukuR0OY02t0atJgMzn6oc#scrollTo=Bl-yFZqKeezp&forceEdit=true&offline=true&sandboxMode=true\r\n\r\n\r\n**Step 0** \r\n\r\nReplace retrain.py with downloading file from following link and uploading it to same folder in Google drive.\r\n\r\nhttps://raw.githubusercontent.com/tensorflow/hub/master/examples/image_retraining/retrain.py\r\n\r\n\r\n**Step 1**\r\n\r\nGo to this link : \r\n\r\nhttps://colab.research.google.com/drive/1hFte7dNUZGVukuR0OY02t0atJgMzn6oc#scrollTo=Bl-yFZqKeezp&forceEdit=true&offline=true&sandboxMode=true\r\n\r\n**Step 2**\r\n\r\nGo to chapter Two\r\n\r\n**Step 3** \r\n\r\n**Worked by adding two lines as given following before the script section : LOL**\r\n\r\n**It's a LOL that just added following two lines and it worked before script section and it worked.**\r\n\r\n**Two Lines:**\r\n\r\n\r\n    if not os.path.isdir(\"../Pokemons-subset\"):\r\n  \r\n         print('Error')\r\n\r\n\r\n**Script Section:**\r\n\r\n\r\n     %run scripts/retrainn.py   \\\r\n\r\n       --bottleneck_dir=tf_files/bottlenecks   \\\r\n\r\n       --how_many_training_steps=4000   \\\r\n\r\n       --model_dir=tf_files/models/   \\\r\n\r\n       --summaries_dir=tf_files/training_summaries/mobilenet_0.50_160   \\\r\n\r\n       --output_graph=tf_files/retrained_graph.pb   \\\r\n\r\n       --output_labels=tf_files/retrained_labels.txt   \\\r\n\r\n       --learning_rate=0.01   \\\r\n\r\n       --architecture=mobilenet_0.50_160   \\\r\n\r\n       --image_dir=../Pokemons-subset \\\r\n\r\n       --flip_left_right \\\r\n\r\n       --random_crop=10 \\\r\n\r\n       --random_scale=10 \\\r\n\r\n       --random_brightness=10\r\n\r\n\r\n"]}, {"number": 11043, "title": "Error with building clang unknown argument", "body": "I configured project for building and execute next command in project directory:\r\n\r\n`bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\r\n`\r\n\r\nAfter that i get this error:\r\n> ERROR: /home/eugeny/Git/tensorflow/tensorflow/core/kernels/BUILD:3614:1: C++ compilation of rule '//tensorflow/core/kernels:multinomial_op_gpu' failed: clang failed: error executing command /usr/bin/clang -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/multinomial_op_gpu/tensorflow/core/kernels/multinomial_op_gpu.cu.d ... (remaining 142 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n> clang: error: unknown argument: '-nvcc_options=relaxed-constexpr'\r\n> clang: error: unknown argument: '-nvcc_options=ftz=true'\r\n> clang: error: cannot find libdevice for sm_35. Provide path to different CUDA installation via --cuda-path, or pass -nocudalib to build without linking with libdevice.\r\n> clang: error: cannot find CUDA installation.  Provide its path via --cuda-path, or pass -nocudainc to build without CUDA includes.\r\n> clang: error: cannot find libdevice for sm_52. Provide path to different CUDA installation via --cuda-path, or pass -nocudalib to build without linking with libdevice.\r\n> clang: error: cannot find CUDA installation.  Provide its path via --cuda-path, or pass -nocudainc to build without CUDA includes.\r\n> clang: error: cannot find CUDA installation.  Provide its path via --cuda-path, or pass -nocudainc to build without CUDA includes.\r\n> Target //tensorflow/cc:tutorials_example_trainer failed to build\r\n> \r\n\r\nI'm trying to build it with gpu support by using next libs:\r\n\r\n\r\nBazel: 0.5.1\r\ngcc: 7.1.1\r\nCuda: 8 (version of cuda/bin/gcc is 5.4.0)\r\nCUDNN: 6\r\nprotobuf: 3.3.1\r\n\r\nWhat shall i change for normal building?\r\n\r\n", "comments": ["I've got the same error with current version of clang (`clang-5.0`).", "It looks like clang doesn't support those nvcc flags, at least I haven't found information on it anywhere, including [docs](https://clang.llvm.org/docs/ClangCommandLineReference.html)", "Yeah, i tried it with `clang3.5` and `clang4`, noway to build tf with them.", "@skye Why it's closed? ", "I just asked for versions of building tools and no answer for it. Pretty sad you cant help me with it, because i have no clue how deal with it.", "Ah sorry, didn't realize you were still asking for help.\r\n\r\nCan you fill out the new issue template please? In particular what OS are you using?\r\n\r\n@gunan it looks like clang/gcc versions aren't documented, do you know if/when we expect building with clang to work?", "Are you configuring to use when building for GPU?\r\nYou may need a bleeding edge build of clang, that has GPU support.\r\n\r\nAlso, could you fill in the issue template?\r\nI cannot really comment much without knowing your OS version, what commit you guys synced to, etc.", "@gunan I've used Ubuntu 16.04, clang-5.0 (i.e. development branch), and master branch of tensorflow. CUDA 8.0 and cudnn 6.", "@Randl, which version of TensorFlow are you trying to build?\r\nAlso, when configuring, are you trying to use clang as the cuda compiler?\r\nOr is clang simply the default compiler on your system while you use nvcc for GPU code.", "@gunan \r\nI'm trying to build 708cbaf2f6bbbda931e7bf627ce5f840d9c92162\r\nI've configured clang as CUDA compiler and I'm using gcc as system compiler and Tensorflow compiler.", "@ilya-biryukov Could you look into this issue?\r\nWhich version of clang do we need to use to build TF with GPU support?\r\n\r\nMaybe we should add a note in the configure script for the minimum required clang version.", "Hello, thank you all for respond. I use `archlinux` and same build configuration as @Randl, but i can easily switch into `ubuntu 16.04 `\r\n\r\n", "@gunan is it possible to create docker container for building tensorflow somehow?", "I'm using `clang version 5.0.0-svn306258-1~exp1` and it does compile CUDA code (I've tried with a couple of toy examples).", "@4l4l4l our default builds use gcc and nvcc to build.\r\nThere are already docker containers for tensorflow for those, you can use any \"devel\" docker container in our dockerhub.\r\n\r\nclang is still work in progress, so Ill have to wait for a response from @ilya-biryukov to see what the problem you are running into is.\r\n", "Please use `--config=cuda_clang` after configuring to build using clang. (I also think that using bazel build without `--config` flags should work too).\r\nWe should add a check for that and provide a helpful error message.\r\n\r\n", "The build currently also fails because of an error in `configure` script. The fix should be upstream soon, the workaround before it lands is to change the following lines of `.tf_configure.bazelrc` from:\r\n```\r\nbuild --config=cuda\r\ntest --config=cuda\r\n```\r\nto:\r\n```\r\nbuild --config=cuda_clang\r\ntest --config=cuda_clang\r\n```\r\nThis should be done after running `configure`.", "@ilya-biryukov Thanks, with those two fixes I managed to build Tensorflow", "Thanks for the update @Randl! I'm gonna close this, but @4l4l4l please reopen if Ilya's advice doesn't work for you. Thanks.", "@ilya-biryukov my build in progress but i get next error\r\n\r\n```\r\n[eugeny@localhost tensorflow]$ bazel build -c opt //tensorflow/cc:tutorials_example_trainer\r\nINFO: Found 1 target...\r\nERROR: /home/eugeny/Git/tensorflow/tensorflow/core/kernels/BUILD:3036:1: C++ compilation of rule '//tensorflow/core/kernels:depth_space_ops_gpu' failed: clang failed: error executing command /usr/bin/clang -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/depth_space_ops_gpu/tensorflow/core/kernels/spacetodepth_op_gpu.cu.d ... (remaining 141 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nclang: error: cannot find libdevice for sm_50. Provide path to different CUDA installation via --cuda-path, or pass -nocudalib to build without linking with libdevice.\r\nclang: error: cannot find CUDA installation.  Provide its path via --cuda-path, or pass -nocudainc to build without CUDA includes.\r\nclang: error: cannot find CUDA installation.  Provide its path via --cuda-path, or pass -nocudainc to build without CUDA includes.\r\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.632s, Critical Path: 0.23s\r\n\r\n```\r\n\r\nWhere should i insert `--cuda-path` flag?\r\n\r\nI found solution. Guys, you should work better with configuration files. I directly install my cuda sources in` /opt/cuda` and set it in configuration but your code still seeks it in `/usr/local/cuda` path somewhere\r\n\r\nthank you @ilya-biryukov for help and other guys too, all works well", "Thank you for sharing your solution @4l4l4l!", "@4l4l4l, the configure script is not very good at figuring out cuda install paths other than `/usr/local/cuda`. And it should also probably do some sanity-checking (i.e. at least check that the cuda folder exists).\r\n\r\nAnyway, glad you figured it out."]}, {"number": 11042, "title": "Documentation Error in TenserFlow website", "body": "On website page:\r\nhttps://www.tensorflow.org/get_started/get_started#a_custom_model\r\n\r\nThere is a missing line in the sample code provide for Custom Model:\r\n After this line \r\n `input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_train}, y_train, 4, num_epochs=1000)`.\r\nin sample code there should be one more line for variable `eval_input_fn ` which is missing right now in the docs.\r\nsingle line of code to be added in sample code is \r\n`eval_input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_eval}, y_eval,4, num_epochs=1000)`", "comments": ["Thanks for pointing this out, this line seems to be there now."]}, {"number": 11041, "title": "Update logging in DNNClassifier to use tf.summary.scalar", "body": "### System information\r\n- I am using the stock DNNClassifier in contrib/learn. The warning is present in every instance where DNNClassifier is called.\r\n- Present in OSX (Mac OS 10.12.5) and Linux (Ubuntu 16.04)\r\n- **TensorFlow installed from (source or binary)**: Present both on binary and when compiled from source\r\n- **TensorFlow version (use command below)**: v1.2.0 (release) and v1.2.0-1371-g97af82d53 1.2.0\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Call the DNN classifier as indicated in https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier\r\n\r\n### Describe the problem\r\nA warning for a deprecated feature is cluttering the logs when using the DNNClassifier. It is due to the use of the deprecated feature scalar_summary while logging. While this has been deprecated in 2016-11-30, it is still used in tensorflow/contrib/learn/python/learn/estimators/head.py:642\r\nThis bug report request for updating the the current tf.summary.scalar, as indicated. The usability of the product is much improved as a consequence.\r\n\r\n### Logs\r\n```\r\nWARNING:tensorflow:From /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\n\r\n```\r\n### Source code\r\ntensorflow/contrib/learn/python/learn/estimators/head.py:642\r\n```\r\n    # Uses the deprecated API to set the tag explicitly.\r\n    # Without it, training and eval losses will show up in different graphs.\r\n    logging_ops.scalar_summary(\r\n        _summary_key(head_name, mkey.LOSS), weighted_average_loss)\r\n```", "comments": ["From the comments to the line in code in question, the choice of retaining the deprecated feature seems to be intentional:\r\ntensorflow/contrib/learn/python/learn/estimators/head.py:642\r\n```\r\n    # Uses the deprecated API to set the tag explicitly.\r\n    # Without it, training and eval losses will show up in different graphs.\r\n    logging_ops.scalar_summary(\r\n        _summary_key(head_name, mkey.LOSS), weighted_average_loss)\r\n```\r\nWhile I understand the rationale, the resulting cluttering of the log makes it rather difficult to read.\r\n\r\n", "@ispirmustafa can you comment on this or redirect to someone else? Thanks!", "Yes we keep it the deprecated version. This is fixed in the core version of DNNClassifier. (tf.estimator.DNNclassifier) which will be available with 1.3 or at head."]}, {"number": 11040, "title": "Crash:  F tensorflow/core/kernels/conv_ops.cc:659] Check failed: stream\u2011>parent()\u2011>GetConvolveAlgorithms(&algorithms)", "body": ": E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 6021 (compatibility version 6000) but source was compiled with 5110 (compatibility version 5100). If using a binary install, upgrade your CuDNN library to match. If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2017\udae1\udeaa\udae1\udebd 14:02:30.167164: F tensorflow/core/kernels/conv_ops.cc:659] Check failed: stream\u2011>parent()\u2011>GetConvolveAlgorithms(&algorithms)\r\n------------------------\r\n\r\n### System information\r\n-- OS Platform and Distribution: CentOS 7\r\n- TensorFlow installed from source :\r\n- **TensorFlow version: 1.1.0 \r\n- **CUDA/cuDNN version:  CUDA v8.0, cuDNN v6\r\n- **GPU model and memory**: TITAN X, 12 GB\r\n\r\n\r\n### Describe the problem\r\nWhen I run the program I am getting this error. Any idea what may cause this error? \r\n\r\n\r\n\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Can you please help me resolve this same issue? I run into it when I try running the MNIST program ( cnn_mnist.py ) from Tensorflow website. \r\n\r\nOS : Windows 10\r\nCUDA : v9.0\r\nCudnn : v7.1\r\nTensorflow : 1.7.0\r\nGPU : GTX 1060 \r\nI installed tensorflow using conda\r\n\r\nThis is the error log from cmd\r\n`WARNING:tensorflow:From C:\\Users\\rohit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\nWARNING:tensorflow:From cnn_mnist.py:120: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data.\r\nWARNING:tensorflow:From C:\\Users\\rohit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From C:\\Users\\rohit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From C:\\Users\\rohit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From C:\\Users\\rohit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST-data\\train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From C:\\Users\\rohit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST-data\\train-labels-idx1-ubyte.gz\r\nExtracting MNIST-data\\t10k-images-idx3-ubyte.gz\r\nExtracting MNIST-data\\t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From C:\\Users\\rohit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/mnist_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026B78AB40F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2018-04-22 10:43:05.219110: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-04-22 10:43:06.100892: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1344] Found device 0 with properties:\r\nname: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1 memoryClockRate(GHz): 1.3415\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 6.00GiB freeMemory: 4.97GiB\r\n2018-04-22 10:43:06.115814: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-04-22 10:43:07.412164: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-04-22 10:43:07.418200: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:917]      0\r\n2018-04-22 10:43:07.425386: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:930] 0:   N\r\n2018-04-22 10:43:07.433139: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4747 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\n2018-04-22 10:43:10.582393: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:396] Loaded runtime CuDNN library: 7103 (compatibility version 7100) but source was compiled with 7003 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2018-04-22 10:43:10.616071: F T:\\src\\github\\tensorflow\\tensorflow\\core\\kernels\\conv_ops.cc:712] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)\r\n`", "it means the official compiled cudnn version is 7.0.3 .But we use the latest 7.1.x, you can re-download 7.0.x and then replace local 7.1.x", "Thanks a lot !!  It worked! I noticed that but failed to understand how to act on it. "]}, {"number": 11039, "title": "coerce dict keys to strings when sorting for python3 compat", "body": "When tensor dicts have heterogeneous key types python 3 blows up due to the way `sorted` is being used.\r\n\r\nIn python 2 you can sort dicts with heterogeneous types:\r\n```python\r\n   In [1]: d = {\"z\": 1, 1: 42, (\"a\", \"b\"): 100}\r\n   In [2]: sorted(d)\r\n   Out[2]: [1, 'z', ('a', 'b')]\r\n```\r\n\r\nIn python 3 you get an error:\r\n```python\r\n   In [1]: d = {\"z\": 1, 1: 42, (\"a\", \"b\"): 100}\r\n   In [2]: sorted(d)\r\n   ---------------------------------------------------------------------------\r\n   TypeError                                 Traceback (most recent call last)\r\n   <ipython-input-5-01813638448d> in <module>()\r\n   ----> 1 sorted(d)\r\n```\r\nBTW, I ran into this issue when using the train script in the new object detection model/API: https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_locally.md\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11038, "title": "tf.estimator.RunConfig is incompatible with tf.contrib.learn.Experiment.train", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 8.0.44, libcudnn.so.5.1.10\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n`tf.estimator.RunConfig` doesn't have an `environment` attribute, which is incompatible with the current implementation of [`tf.contrib.learn.Experiment.train`](https://github.com/tensorflow/tensorflow/blob/77867318b3c89e38828e787f3948ccae21bc0693/tensorflow/contrib/learn/python/learn/experiment.py#L248). A workaround for local training is simply to set `rc.environment = None`; in general I don't know whether a PR should add the attribute to `tf.estimator.RunConfig` or amend the implementation of `tf.contrib.learn.Experiment.train` to check for its existence.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@martinwicke, thoughts about this?", "The contrib Experiment is expected to work with tf.contrib.learn.RunConfig for now. This is sad. but will be fixed soon. ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "With TF 1.4 release, tf.estimator.train_and_evaluate is added, as a replacement for the tf.contrib.learn.Experiment. tf.estimator.RunConfig is designed to work with the new API. Given this, close this issue. "]}, {"number": 11037, "title": "installation failed with Virtualenv ERROR:The executable is not functioning tensorflow", "body": "Operating System: macOS Sierra 10.12.3\r\nThe version of virtualenv is 15.1.0 \r\nissue like these:\r\n`admindeMacBook-Air:~ admin$ virtualenv --system-site-packages ~/tensorflow`\r\n`Using base prefix '//anaconda'`\r\n`New python executable in /Users/admin/tensorflow/bin/python`\r\n`dyld: Library not loaded: @loader_path/../lib/libpython3.5m.dylib`\r\n`  Referenced from: /Users/admin/tensorflow/bin/python`\r\n`  Reason: image not found`\r\n`ERROR: The executable /Users/admin/tensorflow/bin/python is not functioning`\r\n`ERROR: It thinks sys.prefix is '/Users/admin' (should be '/Users/admin/tensorflow')`\r\n`ERROR: virtualenv is not compatible with this system or executable`\r\n\r\nI have tried many ways like homebrew, upgrade py, but it does not work at all.\r\n", "comments": ["Hi,\r\n     I guess you have to choose either of the following according to your python version.\r\n     `virtualenv --system-site-packages targetDirectory # for Python 2.7`\r\n     `virtualenv --system-site-packages -p python3 targetDirectory # for Python 3.n`\r\n    Also, please try these links,\r\n[https://github.com/tensorflow/tensorflow/issues/2554](https://github.com/tensorflow/tensorflow/issues/2554)\r\n[https://stackoverflow.com/questions/5904319/problem-with-virtualenv-in-mac-os-x](https://stackoverflow.com/questions/5904319/problem-with-virtualenv-in-mac-os-x) \r\n\r\n\r\n     ", "I tried \r\n`virtualenv --system-site-packages -p python3 targetDirectory # for Python 3.5`\r\nand the issue changed:\r\n`OSError: [Errno 62] Too many levels of symbolic links: '/Users/admin/targetDirectory/bin/python'`\r\nwhat should I do next? I tried to remove other files /Users/admin/targetDirectory/bin/python but it still not work.", "Hi,\r\n     Did you tried the links? Also, please execute first `rm -rf env` to remove old virtual environment file.", "I'm getting exactly the same error too.", "@wolffg this looks like a virtualenv problem and not a TensorFlow problem, but if this is a common problem we might need to update our virtualenv install docs. If you think this should work for most people feel free to close and redirect to a different forum. ", "I am also facing the exactly same issue. OS: Mac Sierra 10.12, Python 3.6, Anaconda 4.4. Follwed the instructions specified in https://www.tensorflow.org/install/install_mac. Error happening in step 3. Tried to change the directory too but still the same error.", "I have the exact same error problems on my MacOS Sierra 10.12.6, Python 3.5", "I am also having the same problem.\r\nI am using **Python 3.6.0 :: Anaconda 4.3.1 (64-bit)**\r\n", "any solutions ?\r\n\r\n\r\nOSError: [Errno 62] Too many levels of symbolic links:", "I am also experiencing this. \r\nPython 3.6.1 :: Anaconda 4.4.0\r\n\r\nHere is what happens when I run Step 3 of Installing TensorFlow on macOS:\r\n\r\n```\r\nxxxxxx-xxx-xxx:~ saraquigley$ virtualenv --system-site-packages -p python3 ~/tensorflow\r\nRunning virtualenv with interpreter /anaconda/bin/python3\r\nUsing base prefix '/anaconda'\r\nNew python executable in /Users/saraquigley/tensorflow/bin/python3\r\nNot overwriting existing python script /Users/saraquigley/tensorflow/bin/python \r\n(you must use /Users/saraquigley/tensorflow/bin/python3)\r\ndyld: Library not loaded: @rpath/libpython3.6m.dylib\r\n  Referenced from: /Users/saraquigley/tensorflow/bin/python3\r\n  Reason: image not found\r\nERROR: The executable /Users/saraquigley/tensorflow/bin/python3 is not functioning\r\nERROR: It thinks sys.prefix is '/Users/saraquigley' (should be '/Users/saraquigley/tensorflow')\r\nERROR: virtualenv is not compatible with this system or executable\r\n```\r\n\r\n\r\nThank you for any suggestions.", "Experiencing this while setting up vertualenv in Pychram\r\nPython 3.6.1 :: Anaconda 4.4.0\r\n\r\nUsing base prefix '/home/chandan/anaconda3'\r\nNew python executable in /home/chandan/VirtualEnvironment/bin/python3.6\r\nAlso creating executable in /home/chandan/VirtualEnvironment/bin/python\r\nERROR: The executable /home/chandan/VirtualEnvironment/bin/python3.6 is not functioning\r\nERROR: It thinks sys.prefix is '/tmp/tmp4vk5cyo9pycharm-management/virtualenv-15.1.0' (should be '/home/chandan/VirtualEnvironment')\r\nERROR: virtualenv is not compatible with this system or executable\r\n\r\n/home/chandan/VirtualEnvironment/bin/python3.6: error while loading shared libraries: libpython3.6m.so.1.0: cannot open shared object file: No such file or directory\r\n", "I encountered the issue. Has anyone resolve the issue?  ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I had similar problems. Looks like virtualenv builds it's version of python from the one that is used to run the set up script. Just updated to the latest V3 Python from https://www.python.org/downloads/ closed the terminal (no idea if I needed to do this or not - habit!) deleted the junk from the old install attempts and ran again - flew straight through.\r\nHope that helps someone.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "If you are still struggling, and experiencing this issue: \r\n\r\n`ERROR: virtualenv is not compatible with this system or executable`\r\n\r\nHave you tried installing virtualenv with conda and trying again? \r\n\r\n`conda install virtualenv`", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "conda install virtualenv worked", "What if we don't want to use Anaconda? What if it's a straight Python3.5 install? Has anyone solved this?", "Any solutions to this using Pip3 and not conda?"]}, {"number": 11036, "title": "Issue while backpropagating through sparse_tensor_dense_multiply", "body": "I have a simple network defined as follows:\r\n\r\n    h1 = tf.sparse_tensor_dense_matmul(x, W1)\r\n    h2 = tf.matmul(h1, W2)\r\n    y = tf.matmul(h2, W3)\r\n    loss = tf.nn.l2_loss(y - y_)\r\n    train = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\r\n\r\nwhere x is a sparseTensor, rest are dense. \r\nDimensions (shapes) of W1 = [1000,200], W2 = [200,400] and W3 = [ 400, 500]. \r\n\r\nWhen I run the following:\r\n\r\n    sess.run([train], feed_dict={x:X, y_:Y})\r\n\r\nwhere X is sparseTensor of shape [N, 1000] and Y is a tensor of shape [N, 500]\r\n\r\nI get an error saying: OOM when allocating tensor with shape[3684773,200].\r\nThis is happening while the the the gradient for W is being computed. 3684773 also happens to be the number of non-zero elements in X.\r\n\r\nNote:\r\n\r\n 1. When I compute gradients using tf.gradients, they work completely\r\n    fine.\r\n 2. When I run the same network using dense X and dense multiply( tf.matmul ), it works completely fine.\r\n\r\n    \r\n\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11035, "title": "tensorboard display nothing after update to v1.2.0", "body": "I update to tensorflow 1.2.0 by building from source. \r\nafter that, tensorboard show nothing.\r\nIs the tensorboard changed?\r\nI use command:\r\n    tensorboard --logdir=log", "comments": ["It seems tensor board disappeared in v1.2.0 ? \r\nhow should I install tensor board in v1.2.0", "https://github.com/tensorflow/tensorboard", "When I install tensorflow compiled from source v1.2.0, it automaticly do bellow:\r\n     pip install tensorflow-tensorboard, \r\nThis tensorboard seems to display nothing!\r\nHowever using (https://github.com/tensorflow/tensorboard) this one is OK.\r\nIs it a bug?", "@dandelionmane @jart ", "@cxwx Can you open an issue at github.com/tensorflow/tensorboard with the following information:\r\n- What was the output that TensorBoard printed to console?\r\n- What was the output that TensorBoard printed to web console? (in chrome, right click on the page + click \"inspect\")\r\n- What browser are you using?\r\n- What OS are you using?\r\n- What exactly is TensorBoard displaying?\r\n\r\nI'm closing this issue since this is the wrong place for TensorBoard issues."]}, {"number": 11034, "title": "There should be tf.fill_like", "body": "", "comments": ["Did you mean tf.full_like (Ref: [`np.full_like`](https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.full_like.html))? In case the devs decide to incorporate this functionality, I would like to work on this issue.", "Yes, exactly. Sorry for the lack of details.", "@3rd3 is there really a need for this function? I guess the same effect can be achieved by using `k * tf.ones_like(x)`", "@lakshayg Or `k + tf.zeros_like(x)`. But I am not sure which one is optimal, or whether either of them is.", "Shouldn't it be closed?", "I think this issue can be closed as the functionality can be easily achieved at the python level."]}, {"number": 11033, "title": "Removed unnecessary row in estimators extension tutorial", "body": "", "comments": ["Thanks @terrytangyuan!"]}, {"number": 11032, "title": "Update momentum.py", "body": "fixed a typo in the documentation", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 11031, "title": "Error building bazel \"gcc: unrecognized option '-no-canonical-prefixes'\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Description:\tSUSE Linux Enterprise Server 11 (x86_64)\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:master branch\r\n- **Bazel version (if compiling from source)**:0.5.1\r\n- **CUDA/cuDNN version**:7.5\r\n- **GPU model and memory**:Tesla K20Xm\r\n- **Exact command to reproduce**:`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\n\r\n### Describe the problem\r\nCan't build bazel\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n![screen shot 2017-06-24 at 7 49 54 pm](https://user-images.githubusercontent.com/20103571/27507974-b650c51c-5916-11e7-978c-3a2e5ade2b2c.png)\r\n\r\n\r\n", "comments": ["It looks like you need a newer version of gcc. Unfortunately you're not building on an officially-supported OS, so most likely you'll have to figure out how to upgrade gcc or use Ubuntu. I'm going to close this issue since it's not a TensorFlow bug or feature request."]}, {"number": 11030, "title": "test_session: fix doc formatting", "body": "This is a blind fix following create_local_cluster example below. See broken doc formatting here:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/test/TestCase#test_session", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11029, "title": "install with MKL and OpenCL without locate command", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Description:\tSUSE Linux Enterprise Server 11 (x86_64)\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:master branch\r\n- **Bazel version (if compiling from source)**:0.5.1\r\n- **CUDA/cuDNN version**:7.5\r\n- **GPU model and memory**:Tesla K20Xm\r\n- **Exact command to reproduce**:./configure\r\n\r\n### Describe the problem\r\nI am trying to install tensorflow on Tokyo Institute of Technology Supercomputer TSUBAME.\r\nI want to install with MKL support, but locate command is required.\r\nThe university says that because TSUBAME is a public service and `locate` can be used to see files from other users, the command is not going to be installed. So it means MKL can't be used even it is installed in the supercomputer.\r\nI suggest a feature to install tensorflow with the supports without locate command.\r\nI guess tensorflow is used in a lot of supercomputers, the same problem may occur elsewhere.\r\n\r\n### Source code / logs\r\n`Do you wish to build TensorFlow with MKL support? [y/N] y\r\nMKL support will be enabled for TensorFlow\r\nDo you wish to download MKL LIB from the web? [Y/n] n\r\nPlease specify the location where MKL is installed. [Default is /opt/intel/mklml]: /usr/apps.sp3/isv/intel/xe2013.1.046/composer_xe_2013_sp1.2.144/mkl\r\n./configure: line 279: locate: command not found`\r\n\r\n", "comments": ["can you use whereis command? or find? In TSUBAME", "Locate is not installed by default on some systems, like Gentoo. It's a nice tool to have though (sometimes it's called 'mlocate' though, and it doesn't have a standard, so it's generally not a great idea to use it in scripts.) If you don't want locate though, you can manually change the line to this:\r\n```bash\r\nloc=\"$(find / -mount -name 'libdl.so.2' -print 2>/dev/null | head -n1)\"\r\n```\r\nIt looks like that section is going to change a lot in the future, so it might be better just edit the config instead of merging a change someone else will have to remove.", "MKL build should now not depend on locate command, if you are building from master branch."]}, {"number": 11028, "title": "wrong path for retrain.py", "body": "python /tensorflow/tensorflow/examples/image_retraining/retrain.py \\\r\n  --bottleneck_dir=bottlenecks \\\r\n  --model_dir=inception \\\r\n  --summaries_dir=training_summaries/long \\\r\n  --output_graph=retrained_graph.pb \\\r\n  --output_labels=retrained_labels.txt \\\r\n  --image_dir=flower_photos\r\n\r\nThe path is wrong for retrain.py with 4000 iterations (default)\r\nProvided the reader is following the article, path should be \r\npython retrain.py \\\r\n  --bottleneck_dir=bottlenecks \\\r\n  --model_dir=inception \\\r\n  --summaries_dir=training_summaries/long \\\r\n  --output_graph=retrained_graph.pb \\\r\n  --output_labels=retrained_labels.txt \\\r\n  --image_dir=flower_photos", "comments": ["What article are you referring to?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11027, "title": "Tensorflow 1.2.0 : can not load  graph using tf.train.import_meta_graph", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: self written code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 on Azure VM\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: 8.0 and 6.0.21 respectively\r\n- **GPU model and memory**: NVIDIA Tesla K80\r\n- **Exact command to reproduce**: tf.train.import_meta_graph(\"graph_name\")\r\n\r\n2017-06-24 07:01:44.245235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:\r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 92f4:00:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\n2017-06-24 07:01:44.245278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0\r\n2017-06-24 07:01:44.245291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y\r\n2017-06-24 07:01:44.245304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 92f4:00:00.0)\r\n\r\n### Problem\r\nWhenever the code spawns the loading of meta graph, it does not exit. If the process is cancelled, there is no segmentation fault, and the code exits cleanly. \r\n\r\n### Source code / logs\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as rd\r\n>>>\r\nKeyboardInterrupt\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.2.0'\r\n>>> tf.layers.conv2d_transpose\r\n<function conv2d_transpose at 0x7fe9ccd5e668>\r\n>>> saver = tf.train.import_meta_graph('../results/nepal/Need/wc2_nepal_2_model.ckpt.meta')                                                                                                                \r\n\r\n\r\n\r\n^CTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1686, in import_meta_graph\r\n    **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 504, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 387, in import_graph_def\r\n    op._add_input(source_tensor, dtype=input_type)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1473, in _add_input\r\n    self._recompute_node_def()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1542, in _recompute_node_def\r\n    self._node_def.input.extend([t._as_node_def_input() for t in self._inputs])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 483, in _as_node_def_input\r\n    if not self._op.name:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1420, in name\r\n    return self._node_def.name\r\nKeyboardInterrupt\r\n>>>", "comments": ["Would you be able to provide us with your metagraph files? Otherwise this will be very hard to reproduce and diagnose.\r\n\r\nAlso, how long did you try running the command before cancelling it? I'm wondering if it's truly stuck or taking a long time.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}]