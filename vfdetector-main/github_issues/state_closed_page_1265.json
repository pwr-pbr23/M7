[{"number": 15188, "title": "error: 'isnan' was not declared in this scope", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Master branch commit: `4ad12049`\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**: g++ 5.4.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: `bazel build //tensorflow:libtensorflow.so`\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI'm trying to compile using `bazel build //tensorflow:libtensorflow.so`.\r\nI'm getting this error:\r\n```\r\nERROR: /home/rdi/Workspace/Common/tensorflow/tensorflow/core/BUILD:577:1: C++ compilation of rule '//tensorflow/core:random_ops_op_lib' failed (Exit 1)\r\nIn file included from ./tensorflow/core/framework/allocator.h:23:0,\r\n                 from ./tensorflow/core/framework/tensor.h:20,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:24,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:20,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from tensorflow/core/ops/random_ops.cc:16:\r\n./tensorflow/core/framework/numeric_types.h: In constructor 'tensorflow::bfloat16::bfloat16(float)':\r\n./tensorflow/core/framework/numeric_types.h:49:16: error: 'isnan' was not declared in this scope\r\n     if (isnan(v)) {\r\n                ^\r\n```\r\n", "comments": ["I think replacing `isnan(v)` with `std::isnan(v)` solves the problem."]}, {"number": 15187, "title": "Cherrypicks for 1.4.1", "body": "", "comments": ["Linux Python3 was a flake:\r\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/7470/console\r\n\r\nReran and it passed"]}, {"number": 15186, "title": "Fix assert_called error on Python3", "body": "by replacing it with assertTrue(....called)", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 15185, "title": "[XLA] Add fast path cases for common scatter and gather operations", "body": "This change checks if the indices vector passed to a scatter or gather operation is a constant, and does a fast-path operation when it is filled with a zero-based incrementing set.\r\n\r\nThis is quite a common case because of tensor-array stack and unstack.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "although there are existing unit tests that catch this change, do you think I should add an explicit device-targetted test which demonstrates this working.  It is hard to check that the fast path has been used, without checking the form of the XLA HLO graph - but this could be done in the XLA CC tests.\r\n\r\n", "thanks for the comments.  I will check the coverage.  \r\n\r\nI'm not sure that the tensor_array_ops_test actually tests a lot of XLA device side stuff.  Last time I looked it was mostly being compiled by the CPU constant removal code.  this change certainly passes the tests in that file, but maybe only because the HLO graphs are optimized down to constants before they are compiled.\r\n\r\nlet me get back to you tomorrow....\r\n\r\n", "good news. both the scatter and gather changes are hit by the tensor_array_ops_test set of tests.\r\n\r\n", "i think i'll ignore those last 2 comments \ud83d\ude06", "@tensorflow-jenkins test this please"]}, {"number": 15184, "title": "Add some sparse operators", "body": "", "comments": ["Can one of the admins verify this patch?", "@benoitsteiner WDYT?", "With this `op`, it might be easy to implement the gradient of `SparseReduceSumSparse`.\r\n\r\n```\r\nimport tensorflow.contrib.sparse_ops.python.ops.sparse_ops as spops\r\n@ops.RegisterGradient(\"SparseReduceSumSparse\")\r\ndef _SparseReduceSumSparseGrad(op, *grads):\r\n    output_tensor = sparse_tensor.SparseTensor(op.outputs[0], grads[1], op.outputs[2])\r\n    input_tensor = sparse_tensor.SparseTensor(op.inputs[0], op.inputs[1], op.inputs[2])\r\n\r\n    grad = spops.sparse_tile_like(output_tensor, input_tensor, op.inputs[3]).values\r\n\r\n    return (None, grad,\r\n            None, None)\r\n```\r\n\r\nI was wondering if it is proper adding this gradient directly into the `contrib` package?", "@ebrevdo Thanks for your reply! I'd love to make changes recently.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@ebrevdo I did just as your suggestions. Please remind me if there is anything else to work on. Thank you.", "Please resolve conflicts then we can negotiate like merge. Thanks!", "@chaihua483 could you pull rebase and push again?", "Sorry for the delay.", "@drpngx @ebrevdo -- looks like you both reviewed this but did not explicitly approve; can you take a last look and approve if all is good?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "#14964 It seems to have been resolved."]}, {"number": 15183, "title": "estimator: allow export_outputs to be a Tensor", "body": "When someone wants to use `tf.estimator.Estimator.export_savedmodel` to save the predictions of a model and `tf.contrib.predictor.from_saved_model` to load the prediction function, it is currently necessary to define boilerplate code. This PR reduces the amount of code.\r\n\r\nCurrently:\r\n```python\r\n...\r\nret['export_outputs'] = {\r\n        'predictions': tf.estimator.export.PredictOutput(\r\n            {'predictions': ret['predictions']}\r\n        )\r\n    }\r\ntf.estimator.EstimatorSpec(**ret)\r\n```\r\nWith this PR the following is enough:\r\n```python\r\nret['export_outputs'] = {'predictions': ret['predictions']}\r\nret['export_outputs'] = ret['predictions']\r\n```\r\nThe predictor repr for the first example is equal to the original code\r\n```python\r\nSavedModelPredictor with feed tensors {'x': <tf.Tensor 'Placeholder:0' shape=(1, 10) dtype=float32>} and fetch_tensors {'predictions': <tf.Tensor 'dense/Relu:0' shape=(1, 10) dtype=float32>}\r\n```\r\nand for the second the output name changes\r\n```python\r\nSavedModelPredictor with feed tensors {'x': <tf.Tensor 'Placeholder:0' shape=(1, 10) dtype=float32>} and fetch_tensors {'output': <tf.Tensor 'dense/Relu:0' shape=(1, 10) dtype=float32>}\r\n```\r\n\r\nWhen someone wants to verify the code, here a full example:\r\n```python\r\nfrom types import SimpleNamespace\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import predictor\r\n\r\ndef model_fn(features, labels, mode):\r\n    assert labels is None, labels\r\n    x = features['x']\r\n    y = features['y']\r\n    \r\n    y_hat = tf.layers.Dense(10, tf.nn.relu)(x)\r\n    loss = tf.losses.mean_squared_error(y, y_hat)\r\n    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\r\n    \r\n    ret = dict(\r\n        mode=mode,\r\n        loss=loss,\r\n        predictions=y_hat,\r\n        train_op=train_op,\r\n    )\r\n\r\n    # Boilderplate:\r\n    ret['export_outputs'] = {  # <------------------- old\r\n        'predictions': tf.estimator.export.PredictOutput(\r\n            {'predictions': ret['predictions']}\r\n        )\r\n    }\r\n    ret['export_outputs'] = {'predictions': ret['predictions']}  # <------------------- new\r\n    ret['export_outputs'] = ret['predictions']  # <------------------- new alternate\r\n    \r\n    return tf.estimator.EstimatorSpec(**ret)\r\n        \r\nestimator = tf.estimator.Estimator(\r\n    model_fn=model_fn\r\n)\r\n\r\ndef generator():\r\n    x = np.random.randn(1, 10).astype(np.float32)\r\n    y = np.random.randn(1, 10).astype(np.float32)\r\n    yield {'x': x, 'y': y}\r\n\r\ndef input_fn():\r\n    ds = tf.data.Dataset.from_generator(generator, {'x': tf.float32, 'y': tf.float32}, {'x': [1, 10], 'y': [1, 10]})\r\n    it = ds.make_one_shot_iterator()\r\n    element = it.get_next()\r\n    return element\r\n\r\nestimator.train(input_fn)\r\n\r\ndef serving_input_receiver_fn():\r\n    x = tf.placeholder(tf.float32, [1, 10])\r\n    y = tf.placeholder(tf.float32, [1, 10])\r\n    \r\n    ret = SimpleNamespace()\r\n    ret.features = {'x': x, 'y': y}  # for graph def\r\n    ret.receiver_tensors = {'x': x}  # for serving\r\n    ret.receiver_tensors_alternatives = None\r\n    return ret\r\n\r\n\r\nmodel_path = estimator.export_savedmodel('tmp', serving_input_receiver_fn)\r\n\r\npredict_fn = predictor.from_saved_model(model_path)\r\n# predict_fn({'x': np.random.randn(1, 10)})\r\npredict_fn\r\n```", "comments": ["Can one of the admins verify this patch?", "@davidsoergel WDYT?", "@boeddeker Thanks for your contribution and apologies for the delay in reverting. Can you please sync with latest TensorFlow branch and test again? @martinwicke Can you please take a look? Thanks!", "@ymodak and @martinwicke: I can not rebase to the current master, because the code is moved to `tensorflow_estimator`.\r\nFurthermore, I can not run the tests in `tensorflow_estimator`, so it is difficult for me to write tests.\r\nThe reason for that may be that tensorflow 1.12 does not use `tensorflow_estimator` (I use pip to install tensorflow).", "@case540 is tensorflow_estimator installed separately, or bundled using bazel? I thought it was the former, so installing tensorflow, you should have tensorflow_estimator.\r\n\r\nEither way, it doesn't matter much, since you created this patch as well, without the tensorflow source being installed via pip -- you probably cloned the tensorflow repo, and copied your changes into the cloned tree, right?\r\n\r\nSimply do the same thing with tensorflow_estimator. \r\n\r\nAs to tests: most tests are runnable by just running the script. For final verification, we can trigger the tests on the PR you submit. While it is ideal if you can test in the same environment (using bazel and all), this isn't necessary.", "TensorFlow is installed separately. You may have to \"pip install tensorflow_estimator\", or \"pip install tf-estimator-nightly\" in order to run some TensorFlow tests (some contrib/ tests still depend on estimator).\r\n\r\nJust cloning TensorFlow git repo and building with Bazel will not get you the tensorflow_estimator code.\r\n\r\nSorry for slow repsonse", "Closing this PR as the code is being moved to Estimator repo."]}, {"number": 15182, "title": "GPU: Add Complex kernel for tf.exp()", "body": "Fix #15103.\r\n\r\nBecause it's my first contribution for GPU kernel, the PR might be not good. Welcome to feedback, and any help will be appreciated. Thanks.\r\n\r\n### How to test\r\n\r\n+ [x] add test case.\r\n+ [ ] pass all test.", "comments": ["Can one of the admins verify this patch?", "Thanks for the contribution!", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 15181, "title": "Add un-fed placeholder warning to negative-dimension error", "body": "Sometimes a user runs a tensor and forgets to initialize one of its placeholders in `feed_dict`. If that placeholder has a shape that includes `None`, the `None` gets converted to -1 and rejected at the C++ level, resulting in [confusing](https://stackoverflow.com/q/45059428/1979005) [error](https://stackoverflow.com/a/45480003/1979005) [messages](https://stackoverflow.com/q/44706840/1979005) (each word a separate link). This problem also appears in #11371.\r\n\r\nThis PR is a two-line change that briefly mentions this issue in the exception string. The hope is that it will help developers find the problem more quickly.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15180, "title": "More simpler examples of using dataflow_ops.StagingArea", "body": "### Describe the problem\r\nGetting the below error when using StagingArea.\r\n`ValueError: Fetch argument <tf.Operation 'group_deps' type=NoOp> cannot be interpreted as a Tensor. (Operation name: \"group_deps\"\r\nop: \"NoOp\"`\r\n\r\nThe error happens only after completing a few 100 steps. It would be a great help if someone can place simpler examples of proper usage of StagingArea\r\n\r\n### Source code / logs\r\n`compute_stage_put_op = compute_stage.put(iterator.get_next())\r\n  if compute_stage_put_op.type == 'Stage':\r\n         compute_stage_ops.append(compute_stage_put_op)`\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@mrry do you happen to know what the status is of staging area? Is it still actively maintained?", "Not AFAIK, although it is still used in the CNN benchmarks. @rohan100jain is working on replacing it with a `tf.data`-based version, but it is currently only available in TF Eager.", "@shahinkl I think your best bet is to move to another API. Ideally you would find what you need to do with the `dataset` API. LMK if that works.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Feel free to reopen once you have more information.", "Hi experts, do you mean the `StagingArea` in `data_flow_ops` will be deprecated or not be recommended to use? Is it being replaced by another higher APIs? Thanks."]}, {"number": 15179, "title": "Failure in LMDBReaderTest while reading testdata", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: s390x Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.4.0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc 5.4.0\r\n- **CUDA/cuDNN version**: No GPU\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: bazel test -c opt  //tensorflow/python/kernel_tests:reader_ops_test\r\n\r\n### Describe the problem\r\nWhile executing `reader_ops_test`, came across failure in `LMDBReaderTest`. \r\nSince I am running it on a big endian system, the failure could be because the testdata(data.mdb) is platform specific and hence gets interpreted wrongly. \r\n@bowang, @jhseu , Is my understanding correct? How can I generate the above testdata for s390x(big endian)?\r\n\r\n### Source code / logs\r\n```\r\nF tensorflow/core/kernels/lmdb_reader_op.cc:49] Check failed: mdb_env_open(mdb_env_, current_work().c_str(), flags, 0664) == 0 (-30793 vs. 0)Invalid argument\r\n012\r\n234\r\n012\r\n234\r\n012\r\n234\r\n012\r\n234\r\n012\r\n234\r\n012\r\n234\r\nAborted (core dumped)\r\n```\r\n", "comments": ["/CC @rmlarsen do you know if you can fix this despite not having access to a big endian system?\r\n\r\n/CC @sandipmgiri @Nayana-ibm can you help us fix this?", "@namrata-ibm `data.mdb` is very simple and generated with LMDB's python interface.\r\nIt contains ten entries as below:\r\n```\r\n  '0' : 'a'\r\n  '1' : 'b'\r\n  ...\r\n  '9' : 'j'\r\n```\r\n", "@bowang , Thank you for your inputs. \r\nI generated the `data.mdb` for s390x(big endian) and the test is passing with it.\r\n ", "Closing this issue. "]}, {"number": 15178, "title": "Dataset.from_generator doesn't play nice with feature_column.categorical_column_with_vocabulary_list", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux 4.4.0-98-generic #121~14.04.1-Ubuntu SMP Wed Oct 11 11:54:55 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.4.0-rc1-11-g130a514', '1.4.0')\r\n- **Python version**: Python 2.7.6\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**:  -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: curl -L -o predict_not_working.py 'https://drive.google.com/uc?authuser=0&id=1KHCNOfJOpEFSLzzJqjtP6oy0EvuLvETE&export=download' && python predict_not_working.py\r\n\r\n### Describe the problem\r\n\r\nI have the same code in two versions, [one using Dataset.from_generator](https://drive.google.com/open?id=1KHCNOfJOpEFSLzzJqjtP6oy0EvuLvETE) and [one using Dataset.from_tensor_slices](https://drive.google.com/open?id=1BFmIEnpuRWPeghUfeBD4225BXZqvLcmS). To me it looks like the created Datasets have the exact same content, but feature_column.categorical_column_with_vocabulary_list doesn't work with the `from_generator` one, which ought to be a bug, right?\r\n\r\n### Source code / logs\r\n\r\nSources are in links above.", "comments": ["It looks like the problem stems from `feature_column.categorical_column_with_vocabulary_list()` requiring a defined rank for its input. By default `Dataset.from_generator()` does not know anything about the shapes of the tensors that it generates, but you can work around this by passing `output_shapes` when you create it. The following input function worked for me:\r\n\r\n```python\r\ndef input_fn():\r\n    def gen():\r\n        for i in range(100000):\r\n            for j in range(10):\r\n                yield {\"in\": str(j)}, j+1\r\n    data = tf.data.Dataset.from_generator(gen, ({\"in\": tf.string}, tf.int32),\r\n                                          output_shapes=({\"in\": []}, []))\r\n    data = data.batch(10)\r\n    iterator = data.make_one_shot_iterator()\r\n    return iterator.get_next()\r\n```", "Ah, good workaround, thanks!", "@ispirmustafa could you take a look?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "This is fixed at head."]}, {"number": 15177, "title": "Use argmax output_type argument instead of cast op", "body": "`argmax` supports int32 output type since e3d99f4d8c0fb5479a0e7abb8623eb9ccab7c0ba, so use it instead of cast op inside those helpers.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15176, "title": "fix a problem in tflite custom_operators.md", "body": "s/SinResize/SinPrepare/ \r\nObviously, it should be SinPrepare instead of SinResize.", "comments": ["Can one of the admins verify this patch?", "ping @aselle ", "@aselle WDYT?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "close this, since the this was fixed by a later PR https://github.com/tensorflow/tensorflow/pull/16489/commits/e22c2a59deaff39b92e19d1c66bac701f49c3027"]}, {"number": 15175, "title": "Got \"Attempting to use uninitialized value\" error after variable initalization", "body": "I am working on windows 10, Python 3.6 and tensorflow 1.4.0. I tested the code on two laptops, one with gpu and another without, both of them had this problem.\r\n\r\nThis is my code:\r\n\r\n`def network(self, net_input):\r\n        dense1 = tf.layers.dense(net_input, 64)\r\n        norm1 = tf.contrib.layers.batch_norm(dense1)\r\n        relu1 = tf.nn.relu(norm1)\r\n        dense2 = tf.layers.dense(relu1, 32)\r\n        norm2 = tf.contrib.layers.batch_norm(dense2)\r\n        relu2 = tf.nn.relu(norm2)\r\n        out = tf.layers.dense(relu2, 1)\r\n        return out`\r\n\r\n`def train(self):\r\n        init_global = tf.global_variables_initializer()\r\n        init_local = tf.local_variables_initializer()\r\n        sess = tf.InteractiveSession()\r\n        #sess = tf.Session()\r\n        sess.run(init_global)\r\n        sess.run(init_local)\r\n        data = tf.placeholder(tf.float32, [self.batch_size, 53])\r\n        label = tf.placeholder(tf.float32, [self.batch_size, 1])\r\n        prediction = self.network(data)\r\n        loss = tf.reduce_mean(tf.reduce_sum(tf.square(\r\n                label - prediction),reduction_indices=[1]))\r\n        train_step = tf.train.GradientDescentOptimizer(1e-3).minimize(loss)\r\n    \r\n        for i in range(self.epoch):\r\n            for j in range(20000 // self.batch_size):\r\n                batch_data, batch_label = self.next_batch(self.train_data, self.train_label)\r\n                #batch_label = self.next_batch(self.train_label)\r\n                sess.run(train_step, feed_dict = {data : batch_data, label : batch_label})\r\n                if j % 50 == 0:\r\n                    start = time.time()\r\n                    loss = sess.run(loss, feed_dict = {data : batch_data, label : batch_label})\r\n                    print('Epoch: [%2d/%2d], Iter: [%4d/%4d], Loss: %8f, Time cost: %8f'%\\\r\n                          (i, self.epoch, j, 20000//self.batch_size, loss, time.time()-start))`\r\n\r\nand this is the error message:\r\n\r\n`Traceback (most recent call last):\r\n\r\n  File \"<ipython-input-17-6e018316c758>\", line 1, in <module>\r\n    runfile('C:/Users/System_Error/Downloads/report1/regression_tf.py', wdir='C:/Users/System_Error/Downloads/report1')\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 880, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/System_Error/Downloads/report1/regression_tf.py\", line 99, in <module>\r\n    main()\r\n\r\n  File \"C:/Users/System_Error/Downloads/report1/regression_tf.py\", line 92, in main\r\n    regression.train()\r\n\r\n  File \"C:/Users/System_Error/Downloads/report1/regression_tf.py\", line 76, in train\r\n    sess.run(train_step, feed_dict = {data : batch_data, label : batch_label})\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\n\r\n`FailedPreconditionError:` Attempting to use uninitialized value dense_18/bias\r\n\t [[Node: dense_18/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_18/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_18/bias)]]\r\n\r\nCaused by op 'dense_18/bias/read', defined at:\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\ipython\\start_kernel.py\", line 231, in <module>\r\n    main()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\ipython\\start_kernel.py\", line 227, in main\r\n    kernel.start()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2827, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-17-6e018316c758>\", line 1, in <module>\r\n    runfile('C:/Users/System_Error/Downloads/report1/regression_tf.py', wdir='C:/Users/System_Error/Downloads/report1')\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 880, in runfile\r\n    execfile(filename, namespace)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spyder\\utils\\site\\sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n  File \"C:/Users/System_Error/Downloads/report1/regression_tf.py\", line 99, in <module>\r\n    main()\r\n  File \"C:/Users/System_Error/Downloads/report1/regression_tf.py\", line 92, in main\r\n    regression.train()\r\n  File \"C:/Users/System_Error/Downloads/report1/regression_tf.py\", line 67, in train\r\n    prediction = self.network(data)\r\n  File \"C:/Users/System_Error/Downloads/report1/regression_tf.py\", line 26, in network\r\n    dense1 = tf.layers.dense(net_input, 64)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 250, in dense\r\n    return layer.apply(inputs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 671, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 559, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 145, in build\r\n    trainable=True)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 458, in add_variable\r\n    trainable=trainable and self.trainable)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1203, in get_variable\r\n    constraint=constraint)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1092, in get_variable\r\n    constraint=constraint)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 425, in get_variable\r\n    constraint=constraint)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 394, in _true_getter\r\n    use_resource=use_resource, constraint=constraint)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 805, in _get_single_variable\r\n    constraint=constraint)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 213, in __init__\r\n    constraint=constraint)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 356, in _init_from_args\r\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 125, in identity\r\n    return gen_array_ops.identity(input, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2070, in identity\r\n    \"Identity\", input=input, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_18/bias\r\n\t [[Node: dense_18/bias/read = Identity[T=DT_FLOAT, _class=[\"loc:@dense_18/bias\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense_18/bias)]]`\r\n\r\n\r\nI searched stack overflow, most of the answers told me to add an global_variables_initializer, and I tried tf.Session and tf.InteractiveSession, global and local variables initializer, but still got this error. \r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["I think the problem is that your code has\r\n\r\n```python\r\nsess.run(init_global)\r\nsess.run(init_local)\r\n```\r\n\r\n...before you build the network in this line:\r\n\r\n```python\r\nprediction = self.network(data)\r\n```\r\n\r\nYou must create the initializers and run them *after* you build the network, because building the network creates the variables that you need to initialize."]}, {"number": 15174, "title": "Branch 178185697", "body": "", "comments": ["cc @ankurtaly", "test failure in \"Ubuntu Contrib\" is a known flake. ", "Similar here:\r\n```\r\nERROR: /home/xxx/tensorflow/tensorflow/contrib/boosted_trees/lib/BUILD:31:1: C++ compilation of rule '//tensorflow/contrib/boosted_trees/lib\r\n:utils' failed (Exit 1).\r\nIn file included from ./tensorflow/core/framework/allocator.h:23:0,\r\n                 from ./tensorflow/core/framework/tensor.h:20,\r\n                 from ./tensorflow/contrib/boosted_trees/lib/utils/sparse_column_iterable.h:19,\r\n                 from tensorflow/contrib/boosted_trees/lib/utils/sparse_column_iterable.cc:16:\r\n./tensorflow/core/framework/numeric_types.h: In constructor 'tensorflow::bfloat16::bfloat16(float)':\r\n./tensorflow/core/framework/numeric_types.h:49:16: error: 'isnan' was not declared in this scope\r\n     if (isnan(v)) {\r\n```", "Issue still alive, with nvcc", "Issue is known, and we will try to fix them as a part of https://github.com/tensorflow/tensorflow/pull/14773\r\nThere is nothing we can do as a part of this PR, as this is just a part of our sync processs, and it is already merged."]}, {"number": 15173, "title": "Fix for #12537", "body": "I have built the fix on x86 linux no gpu and it's working.\r\n\r\nThere are some android specific code in logging that I ignored.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/logging.cc#L35-L75", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I am going to defer to API review first, and possibly someone else who works on TF core team should be a real reviewer.", "The concerns I have are; API bloat, and internally our logging infrastructure under the LOG macros do a lot of extra helpful things relevant to Google-infrastructure that std:cerr would lose.  I don't know how to rectify these things easily, even though I like the spirit of your change.\r\n\r\nPerhaps this is a candidate for a custom op; it's not clear that we can make \"Print\" work generically for all environments in the world, and trying will just make Print more complicated than it needs to be.", "Another solution would be to have an environment variable like TF_CPP_MIN_LOG_LEVEL that modifies the underlying fprintf https://github.com/tensorflow/tensorflow/blob/4ad12049d0c93155a8137b7d656136885335374b/tensorflow/core/platform/default/logging.cc#L88-L91\r\n\r\nAlso is it a bug or feature that TF_CPP_MIN_LOG_LEVEL can disable tf.Print?", "@asimshankar and I chatted offline, there may be a simpler solution to this, but he'll have to pass it by the API review folks first.\r\n\r\nIt's a feature to allow users to completely disable all logging via LOG(INFO), and perhaps a bug that tf.Print uses LOG(INFO) in the OSS build.", "@JohanJu : Thanks for the contribution. We feel that instead of adding a new API, it might be okay to just change the implementation of the `Print` kernel to always use `std::cerr` instead of `LOG(INFO)`.\r\n\r\nThoughts?", "@asimshankar This is my first PR so I will just follow your lead, It won't be any problem the Google-infrastructure angel that @vrv talked about? What do you think is better between std::cerr and std::cout in this case?", "@JohanJu : I think it is fine, @vrv 's concerns are valid, but I believe we have a workaround to integrate better with our internal logging system.\r\n\r\nFor now I'd stick with `std::cerr`, since the current behavior with `LOG(INFO)` is to write to stderr as well (IIUC).\r\n\r\nThanks very much!"]}, {"number": 15172, "title": "Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed", "body": "ERROR: /Users/lion/Documents/opensoft/tensorflow/tensorflow/python/BUILD:2749:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o ... (remaining 573 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nclang: warning: argument unused during compilation: '-pthread'\r\nld: library not found for -lgomp\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "@tensorflowbutler \r\nIt is still an issue. I follow the guide: https://gist.github.com/smitshilu/53cf9ff0fd6cdb64cca69a7e2827ed0f\r\n\r\nHave I written custom code?\r\nI removed all align(sizeof(T)) from following files:\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc\r\ntensorflow/core/kernels/concat_lib_gpu.impl.cu.cc\r\n\r\nOS Platform and Distribution\r\nMac OS X 10.13.2\r\n\r\nTensorFlow installed from\r\nsource 1.4, date: 20180104\r\n\r\nTensorFlow version\r\n1.4\r\n\r\nBazel version\r\n0.9.0-homebrew\r\n\r\nCUDA/cuDNN version\r\n9.1/7.0\r\n\r\nGPU model and memory\r\nGTX 1080Ti, 11GB\r\n\r\nExact command to reproduce\r\nbazel build --config=cuda --config=opt --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package\r\n  ", "Interested in this as well, as I am getting:\r\n\r\n![screen shot 2018-01-05 at 11 53 41 am](https://user-images.githubusercontent.com/4885245/34619146-2bfbc582-f20f-11e7-8051-3b15c8bce044.png)\r\n\r\nI am running:\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\non ubuntu 1404 as well @tensorflowbutler ", "Also seeing the same issue:\r\n\r\nSlightly different versions from @sxhysj \r\n\r\nHave I written custom code?\r\nI removed all align(sizeof(T)) from following files:\r\ntensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\ntensorflow/core/kernels/split_lib_gpu.cu.cc\r\ntensorflow/core/kernels/concat_lib_gpu.impl.cu.cc\r\n\r\nOS Platform and Distribution\r\nMac OS X 10.12\r\n\r\nTensorFlow version\r\n1.5\r\n\r\nBazel version\r\n0.5.4\r\n\r\nCUDA/cuDNN version\r\n9.0/7.0\r\n\r\nGPU model and memory\r\nGTX 1080Ti, 11GB", "Solution for me was to comment \r\n\r\n`linkopts = [\u201c-lgomp\u201d]`\r\n\r\nin tensorflow/third_party/gpus/cuda/BUILD.tpl. I now have a working build :)\r\n\r\nVia: https://metakermit.com/2017/compiling-tensorflow-with-gpu-support-on-a-macbook-pro/", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n@JCBrouwer : Thanks for the update, hopefully that helps others. Note that the guide linked to isn't owned by TensorFlow team and in general for GPU support on macOS we rely on community support.", "@Jovonni did you resolve the issue?", "> Interested in this as well, as I am getting:\r\n> \r\n> ![screen shot 2018-01-05 at 11 53 41 am](https://user-images.githubusercontent.com/4885245/34619146-2bfbc582-f20f-11e7-8051-3b15c8bce044.png)\r\n> \r\n> I am running:\r\n> \r\n> bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n> \r\n> on ubuntu 1404 as well @tensorflowbutler\r\n\r\nsame issue. did you resolve it?", "> Solution for me was to comment\r\n> \r\n> `linkopts = [\u201c-lgomp\u201d]`\r\n> \r\n> in tensorflow/third_party/gpus/cuda/BUILD.tpl. I now have a working build :)\r\n> \r\n> Via: https://metakermit.com/2017/compiling-tensorflow-with-gpu-support-on-a-macbook-pro/\r\n\r\nThis fixed the problem for me.\r\nChanged line 160 in the file tensorflow/third_party/gpus/cuda/BUILD.tpl in the section `cc_library(name = \"cusparse\",` from:\r\n`linkopts = [\"-lgomp\"],` to\r\n`#linkopts = [\"-lgomp\"],`\r\nAnd recompiled/continued compiling the build. Build completed successfully. :D", "@ralpha OMG u really save me!\r\nThks a lotttttttt!"]}, {"number": 15171, "title": "[FeatureRequest] Decorator for estimator input_fn", "body": "In the documentation for [Passing input_fn Data to Your Model](https://www.tensorflow.org/get_started/input_fn#passing_input_fn_data_to_your_model) there are a few methods provided for using a function to construct input data and then wrapping that to get a function object. The documentation suggests `functools.partial` or `lambda`.\r\n\r\nWould it be possible to provide a python decorator (maybe under the `tf.estimator` module)? \r\n\r\nLooking at suggested decorator and usage below, I think it makes the code cleaner when making these reusable input functions. \r\n\r\n## Proposed decorator\r\n_n.b. name to be improved/aligned with TF_\r\n```python\r\nimport functools\r\n\r\ndef tf_data_input_fn(old_func):\r\n    def inside(*args, **kwargs):\r\n        return functools.partial(old_func, *args, **kwargs)\r\n    return inside\r\n```\r\n\r\n## Usage\r\n```python\r\n@tf_data_input_fn\r\ndef my_input_fn(data_set):\r\n    # Construct dataset, repeat, maps etc\r\n    features, labels = dataset.make_one_shot_iterator().get_next()\r\n    return features, labels\r\n\r\ntrain_input_fn = my_input_fn(training_set)\r\neval_input_fn = my_input_fn(test_set)\r\n\r\nclassifier.train(input_fn=train_input_fn, steps=2000)\r\nclassifier.evaluate(input_fn= eval_input_fn, steps=2000)\r\n```\r\n\r\nIf this is favourable, I'd be happy to provide a contribution towards such an addition.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: v1.4.0-8-gbca50da6eb 1.4.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A", "comments": ["Looks concise, however I'd prefer to use lambda, which might be straightforward enough. ", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "@martinwicke what do you think of this proposal?", "[unrelated] @av8ramit: Can you look into the template checking and make sure there's a good way to create feature requests? Maybe a field in the template itself, or checking the description for \"feature request\" or \"FR\"? \r\n\r\n@ispirmustafa wdyt?", "we do check the arguments of given `input_fn` within Estimator. `tf_data_input_fn` uses *args, **kwargs which eliminates the check. ", "Correct me if I'm wrong, but the given `input_fn` with the decorator is the same as that if you use `functools.partial` directly as suggested in the docs?\r\n\r\nSomething like `functools.partial(<function my_input_fn at 0x11b078d90>, data_set=[])`", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@damienpontifex Not quite. We explicitly make sure that the arguments of partial objects are properly interpreted. This would be impossible with a decorator such as the one you propose (though in theory I believe one could write a decorator which does the right thing, most likely simply by internally using functools.partial).", "(see fn_args in tensorflow/python/estimator/util.py)", "Looking into it, the addition doesn't seem to be adding sufficient value compared to the explicit declaration as the docs suggest already. Thanks for the assistance and will close"]}, {"number": 15170, "title": "Go: Don't require -std=c99 for the cgo code.", "body": "This should fix the error:\r\ngithub.com/tensorflow/tensorflow/tensorflow/go/graph.go:31:3: error:\r\n'for' loop initial declarations are only allowed in C99 mode\r\n //  for (int i = 0; i < num_shapes; i++) {\r\n    ^\r\n\r\nin some continuous builds like:\r\nhttps://ci.tensorflow.org/job/tensorflow-master-cpu/3297/consoleFull\r\n\r\nAlternative to #15169", "comments": ["Jenkins, test this please", "Jenkins, test this please", "Thanks, @asimshankar !"]}, {"number": 15169, "title": "DO NOT SUBMIT: Temporarily disable go:test", "body": "context: b/70154286", "comments": ["@tensorflow-jenkins test this please", "cc @jhseu @asimshankar ", "Let's try https://github.com/tensorflow/tensorflow/pull/15170 instead of disabling the test?"]}, {"number": 15168, "title": "Adding reading op for hadoop sequence file format (very basic version though).", "body": "This CL allows tensorflow to be able to read tf.Example stored in Hadoop sequence file, which is the one of the well supported output format in the Hadoop eco system (comparing to recordio or sstable).\n\nThis enables one to write Hadoop mapreduce in the familiar way, producing tf.Example, storing them into standard output format (on S3 for example), and invoke tensorflow, avoiding the expensive, and sometimes confusing step of converting from Hadoop format into recordio.\n\nTo keep things simple, there are some limitations in this implementation:\n\n1. no compressed sequence file (not a big issue if the records are already serialized protos)\n\n2. SEQ5 or SEQ6 format.\n\n3. The value in the sequence file must be encoded using org.apache.hadoop.io.BytesWritable, no text or other format, as the primary goal is to read tf.Example protos stored therein.\n\nPlease help me to get this into tensorflow. Thank you.\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Could someone please help me getting the Windows test right? Seems the failure has nothing to do with my change.", "@rosun82 The windows failure doesn't have anything to do with your change.\r\n\r\nGentle ping @jhseu ", "Can we add this as a Dataset instead? https://www.tensorflow.org/programmers_guide/datasets", "(from API review)\r\n\r\n+1 to @jhseu's comment. Let's make this available for DataSets. No concerns adding this type of reader there.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 45 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 15167, "title": "Fix some Bash issues", "body": "Argument mixes string and array. Use `*` or separate argument.\r\nDon't use `$` on the left side of assignments.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15166, "title": "[CMake] Add bazel tests for python file lists", "body": "Progressing #10296 \r\n", "comments": ["Can one of the admins verify this patch?", "I let `tf_python.cmake` at least ensure the existence of all entries in the filetree.\r\nAs suggested by @gunan I wrote a `BUILD` file to invoke the following:\r\n- `python_modules_test.py` to report invalid and missing/expected entries\r\n- `python_proto_test.py` to report invalid entries\r\n- `python_proto_cc_test.py` to report invalid entries", "Thanks for getting this started! I have a few high-level comments about the structure:\r\n\r\n* Can you rewrite the Python tests to use `tf.test.TestCase` (similar to other Python `*_test.py` files)? This will make the tests work better with our testing infrastructure, which in turn makes failures easier to track down.\r\n* In their current form,  I think those tests will fail when running on Bazel, because they don't have the appropriate `data` dependencies in the `BUILD` files. FWIW, I expected the test would have dependencies on the appropriate `*.txt` files and a generated PIP package (built using the Bazel target), and it would compare the set of directories. (See [here](https://github.com/tensorflow/tensorflow/blob/3a05c119d8499c3b1e0a3ef6520af3e62738a80c/tensorflow/python/BUILD#L2559) for an example of a test that uses `data` dependencies.)\r\n* Just to check, are the changes in `python_modules.txt` due to code that's been removed?", "- I rewrote the tests using `tf.test.TestCase`.\r\n- The tests depend on the `.txt` files and the filetree.\r\n- The PIP package shouldn't be needed here?\r\n- I have put together the changes to `python_modules.txt` in [this comment](https://github.com/tensorflow/tensorflow/pull/15368#issuecomment-351777543).", "@tensorflow-jenkins test this please.", "Seems like blacklisting this dependency was not enough:\r\n```\r\nERROR: /workspace/bazel_pip/tensorflow/contrib/cmake/BUILD:7:1:\r\n\tTarget '//tensorflow:all_opensource_files' is not visible from\r\n\ttarget '//bazel_pip/tensorflow/contrib/cmake:python_module_test'.\r\n\tCheck the visibility declaration of the former target\r\n\tif you think the dependency is legitimate.\r\nERROR: /workspace/bazel_pip/tensorflow/contrib/cmake/BUILD:20:1:\r\n\tTarget '//tensorflow:all_opensource_files' is not visible from\r\n\ttarget '//bazel_pip/tensorflow/contrib/cmake:python_proto_test'.\r\n\tCheck the visibility declaration of the former target\r\n\tif you think the dependency is legitimate.\r\nERROR: /workspace/bazel_pip/tensorflow/contrib/cmake/BUILD:33:1:\r\n\tTarget '//tensorflow:all_opensource_files' is not visible from\r\n\ttarget '//bazel_pip/tensorflow/contrib/cmake:python_proto_cc_test'.\r\n\tCheck the visibility declaration of the former target\r\n\tif you think the dependency is legitimate.\r\n```", "@gunan On behalf of @mrry I did instead modify the set of included scripts [`glob([\"**/*.py\"])`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/BUILD#L12) to exclude [`*_test.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L127) like is done here. Please see #15316 for details.", "@gunan What I said about runfiles was about same [issue faced when trying to use TensorFlow from within the source directory](https://stackoverflow.com/questions/35953210/error-running-basic-tensorflow-example).\r\nBecause the `runfiles` directory has the same issue, I had to introduce [this minor hack](https://github.com/tensorflow/tensorflow/pull/15166/files#diff-627a2a073a8f26eae4124d586f145cc5R30) to make it work.\r\nNot ideal of course, but should be fine. If you wanna suggest a different solution, I keep an open ear.", "@jart @wicke I am out for a few days. Could you take a look?\n\nOn Dec 13, 2017 8:44 AM, \"Robin Richtsfeld\" <notifications@github.com>\nwrote:\n\n> @gunan <https://github.com/gunan> What I said about runfiles was about\n> same issue faced when trying to use TensorFlow from within the source\n> directory\n> <https://stackoverflow.com/questions/35953210/error-running-basic-tensorflow-example>\n> .\n> Because the runfiles directory has the same issue, I had to introduce this\n> minor hack\n> <https://github.com/tensorflow/tensorflow/pull/15166/files#diff-627a2a073a8f26eae4124d586f145cc5R30>\n> to make it work.\n> Not ideal of course, but should be fine. If you wanna suggest a different\n> solution, I keep an open ear.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15166#issuecomment-351448537>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOQ80EL-89n9dFL_jaNYB29wLMEKEks5s__7ZgaJpZM4Q4hZe>\n> .\n>\n", "Waiting for #15368 and #15315 while redoing bazel dependencies etc.", "@jart It doesn't seem like we can remove `//tensorflow:all_opensource_files` as dependency from [`tensorflow/contrib/makefile/BUILD`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/BUILD#L21) and I can't figure out how to have these tests access at least the directory structure without this either.", "You might be able to do something similar to what our pip package building process does, where we just depend on the things we need, and then we copy everything we want from the foo.runfiles/ symlink tree structure. There are other ways of doing it.", "> where we just depend on the things we need\r\n\r\nThere is no way of knowing what (let's say) [`python_modules.txt`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/python_modules.txt) contains upfront. They are not even guaranteed to be valid which is what these tests are for.\r\n\r\nAlso, some of the tests depend on knowing about the filetree structure.\r\n\r\n> similar to what our pip package building process\r\n\r\nThis PR tests directory entries \"similar\" to `pip_smoke_test` testing dependencies. As long as [it depends on `//tensorflow:all_opensource_files`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L51) as well, why would this need to work without?\r\n\r\n> There are other ways of doing it.\r\n\r\nUntil we find a better alternative for `cmake` (this PR) and `pip_smoke_test` and `makefile` and all other things that depend on `//tensorflow:all_opensource_files`, I don't see why having it would be an issue.\r\n\r\nEven if we were to write a script to dump a summary of the filetree structure into a file and use that, we would still need some way of requiring \"all files\" for its `runfiles` directory.", "This makes we wonder: How does e.g. `tensorflow/contrib/cmake/tf_python.cmake` get access to all files?", "Waiting for #15315 and #15368 to get merged.", "@gunan Are you okay with these changes?\r\n@mrry Do you have further suggestions?", "> They can be merged together, and then you won't need to worry about the init file hack, either.\r\n\r\nThe init file hack is about the imports from `tensorflow.python.platform`.\r\nIf it were not to duplicate logic, I could of course get rid of most of them.\r\nBut I'm sure I cannot remove `tensorflow.python.platform.test`.", "Maybe I need to clarify things about the init file hack:\r\n\r\nWhen TensorFlow is used from within the repository directory, it finds the `__init__.py` files and resolves all imports the the repository directory instead of the PIP package.\r\nThe problem is that `tensorflow/__init__.py` imports all submodules including generated ops, which it can't find here.\r\n\r\nThe same happens when `all_opensource_files` are linked to `runfiles`.", "@gunan @martinwicke Any updates concerning Sanity Checks?\r\nDo you want to merge this for now and turn it into a sanity check in a follow-up?", "@Androbin the sanity check appears to have passed.", "@drpngx I was referring to [#15368#issuecomment-353682976](https://github.com/tensorflow/tensorflow/pull/15368#issuecomment-353682976)", "I see. @martinwicke @gunan WDYT?", "If you say `data = [\"//tensorflow/python\", \"//tensorflow/contrib:contrib_py\"]` then pretty much everything you should need, including the `__init__.py` files, will be available in the runfiles directory.", "@jart These tests will become Sanity Check together with most other targets that use `//tensorflow:all_opensource_files`. See https://github.com/tensorflow/tensorflow/pull/15368#issuecomment-354156560 for reference.\r\nThis should be the preferred solution.", "Closing this in favor of #15670."]}, {"number": 15165, "title": "LSTM layer in consistent with tf.keras v2.0.8-tf and keras 2.1.2", "body": "It looks like there are some inconsistencies with the output shape of the LSTM layer. \r\n\r\nRunning the following code does not produce an error in `keras 2.1.2`:\r\n```python\r\nmodel = Sequential()\r\n\r\nconv_layer = Conv1D(filters=320,\r\n                    kernel_size=26,\r\n                    strides=1,\r\n                    padding='valid',\r\n                    activation='relu',\r\n                    input_shape=(1000,4))\r\n\r\nmodel.add(conv_layer)\r\nmodel.add(MaxPooling1D(pool_size=13,\r\n                       strides=13))\r\n\r\nmodel.add(LSTM(320, return_sequences=True))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(925,\r\n                activation='relu'))\r\nmodel.add(Dense(919,\r\n                activation='sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\n\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv1d_1 (Conv1D)            (None, 975, 320)          33600     \r\n_________________________________________________________________\r\nmax_pooling1d_1 (MaxPooling1 (None, 75, 320)           0         \r\n_________________________________________________________________\r\nlstm_1 (LSTM)                (None, 75, 320)           820480    \r\n_________________________________________________________________\r\nflatten_1 (Flatten)          (None, 24000)             0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 925)               22200925  \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 919)               850994    \r\n=================================================================\r\nTotal params: 23,905,999\r\nTrainable params: 23,905,999\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nbut produces this error in `keras v2.0.8-tf`:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-389-956501e5fb90> in <module>()\r\n     16 model.add(Flatten())\r\n     17 model.add(Dense(925,\r\n---> 18                 activation='relu'))\r\n     19 model.add(Dense(919,\r\n     20                 activation='sigmoid'))\r\n\r\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py in add(self, layer)\r\n    499           output_tensors=self.outputs)\r\n    500     else:\r\n--> 501       output_tensor = layer(self.outputs[0])\r\n    502       if isinstance(output_tensor, list):\r\n    503         raise TypeError('All layers in a Sequential model '\r\n\r\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in __call__(self, inputs, **kwargs)\r\n    250     \"\"\"\r\n    251     # Actually call the layer (optionally building it).\r\n--> 252     output = super(Layer, self).__call__(inputs, **kwargs)\r\n    253 \r\n    254     # Update learning phase info.\r\n\r\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    557           input_shapes = [x.get_shape() for x in input_list]\r\n    558           if len(input_shapes) == 1:\r\n--> 559             self.build(input_shapes[0])\r\n    560           else:\r\n    561             self.build(input_shapes)\r\n\r\n~/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/core.py in build(self, input_shape)\r\n    125     input_shape = tensor_shape.TensorShape(input_shape)\r\n    126     if input_shape[-1].value is None:\r\n--> 127       raise ValueError('The last dimension of the inputs to `Dense` '\r\n    128                        'should be defined. Found `None`.')\r\n    129     self.input_spec = base.InputSpec(min_ndim=2,\r\n\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n```\r\n\r\nIf I keep return_sequences = True and remove Flatten() after the LSTM I get the following:\r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv1d_49 (Conv1D)           (None, 975, 320)          33600     \r\n_________________________________________________________________\r\nmax_pooling1d_49 (MaxPooling (None, 75, 320)           0         \r\n_________________________________________________________________\r\nlstm_33 (LSTM)               (None, None, 320)         820480    \r\n_________________________________________________________________\r\ndense_92 (Dense)             (None, None, 925)         296925    \r\n_________________________________________________________________\r\ndense_93 (Dense)             (None, None, 919)         850994    \r\n=================================================================\r\nTotal params: 2,001,999\r\nTrainable params: 2,001,999\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\nMore on the discussion in https://github.com/uci-cbcl/DanQ/issues/9#issuecomment-348377899", "comments": ["/CC @fchollet, can you look into this?", "Please print `model.layers[-1].output.get_shape()` after every line of your model definition. Some layer causes a TF-side static shape inference interruption and we'd need to know which one.", "```python\r\n# keras.__version__\r\n# '2.1.2'\r\n\r\nmodel = Sequential()\r\nconv_layer = Conv1D(filters=320,\r\n                    kernel_size=26,\r\n                    strides=1,\r\n                    padding='valid',\r\n                    activation='relu',\r\n                    input_shape=(1000,4))\r\nmodel.add(conv_layer)\r\nprint(model.layers[-1].output.get_shape())\r\n# '2.1.2'\r\n# (?, 975, 320)\r\n\r\n# '2.0.8-tf'\r\n# (?, 975, 320)\r\n\r\nmodel.add(MaxPooling1D(pool_size=13,\r\n                       strides=13))\r\nprint(model.layers[-1].output.get_shape())\r\n# '2.1.2'\r\n# (?, 75, 320)\r\n\r\n# '2.0.8-tf'\r\n# (?, 75, 320)\r\n\r\nmodel.add(LSTM(320, return_sequences=True))\r\nprint(model.layers[-1].output.get_shape())\r\n# '2.1.2'\r\n# (?, ?, 320)\r\n\r\n# '2.0.8-tf'\r\n# (?, ?, 320)\r\n\r\nmodel.add(Flatten())\r\nprint(model.layers[-1].output.get_shape())\r\n# '2.1.2'\r\n# (?, ?)\r\n\r\n# '2.0.8-tf'\r\n# ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n\r\nmodel.add(Dense(925,\r\n                activation='relu'))\r\nprint(model.layers[-1].output.get_shape())\r\n# '2.1.2'\r\n# (?, 925)\r\n\r\n# '2.0.8-tf'\r\n# It errored in previous layer\r\n\r\nmodel.add(Dense(919,\r\n                activation='sigmoid'))\r\nprint(model.layers[-1].output.get_shape())\r\n# '2.1.2'\r\n# (?, 919)\r\n\r\n# '2.0.8-tf'\r\n# It errored in previous layer\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\n\r\n\r\n```", "For keras, I think it uses its `_keras_shape`, which seems smarter for lstm layer.\r\n\r\n```python\r\nconv1d_1: output: (?, 975, 320), keras_shape: (None, 975, 320)\r\nmax_pooling1d_1: output: (?, 75, 320), keras_shape: (None, 75, 320)\r\nlstm_1: output: (?, ?, 320), keras_shape: (None, 75, 320)\r\nflatten_1: output: (?, ?), keras_shape: (None, 24000)\r\ndense_1: output: (?, 925), keras_shape: (None, 925)\r\ndense_2: output: (?, 919), keras_shape: (None, 919)\r\n```", "The issue is with the LSTM output. In `tf.keras/backend.py`, at the end of the call to `rnn`, we need to call `.set_shape(shape)` on the outputs, with a shape that's basically `(inputs.get_shape[0], inputs.get_shape[1], None)` (batch dim and time dim -- no need to specify the feature dim since it's already correctly inferred).\r\n\r\nFeel free to open a PR.", "In general we need to make sure that TF static shape inference (in backend methods) and Keras static shape inference (at the layer level) are in sync.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied."]}, {"number": 15164, "title": "minor docs addition to fixed_size_partitioner", "body": "Clarifying whether fixed_size_partitioner is equivalent to partition_strategy='div' or ='mod', as described in https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup (to hopefully help use downstream with embedding_lookup).", "comments": ["Can one of the admins verify this patch?", "ps, I manually verified that this is the apparent behavior by iterating through the PartitionedVariable via the Iterator (as I was trying to figure out whether this was 'mod' or 'div', as you might imagine).", "OK so, are these equivalent?  Perhaps I am misunderstanding partition_strategy.  \r\n\r\nIf they are not equivalent, could (per your comment on things being confusing as I suggested) docs should be added to embedding_lookup to clarify how to set partition_strategy, given a PartitionedVariable?\r\n\r\n```\r\nmy_embedding_var = tf.get_variable(..., partitioner=fixed_size_partitioner, ...)\r\n\r\nx = tf.embedding_lookup(my_embedding_var, ..., partition_strategy='mod', ...)\r\n```\r\n\r\n```\r\nmy_embedding_var = tf.get_variable(..., partitioner=fixed_size_partitioner, ...)\r\n\r\nx = tf.embedding_lookup(my_embedding_var, ..., partition_strategy='div', ...)\r\n```\r\n\r\n", "@ebrevdo WDYT?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Reviewer @ebrevdo: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 16 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 151 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 166 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 181 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @ebrevdo: It has been 196 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Closing this because doesn't look like this is going to be addressed either way."]}, {"number": 15163, "title": "Fix problem with camera on Android TV", "body": "For a scenario of using a usb external camera, we should use Camera API 2 according to the definition:\r\nCamera1 API is framework API that had been created to support HALv1.x\r\nCamera2 API is a new API that is meant for HALv3.x.\r\n\r\nHowever, **CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_FULL)** should return false as USB camera doesn' support all the necessary capabilities. \r\nhttps://developer.android.com/reference/android/hardware/camera2/CameraMetadata.html#INFO_SUPPORTED_HARDWARE_LEVEL_FULL\r\n\r\nThis is a work around to use **facing == CameraCharacteristics.LENS_FACING_EXTERNAL** to detect usb external camera and use Camera2 API instead.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins Test this, please.", "Merging as the failures have no dependency on the Android demo"]}, {"number": 15162, "title": "tf.data.Dataset.from_generator creates too many threads throwing \"thread constructor failed\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Adapted an example from documentation\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Mac OS Siera (10.12.6)\r\n- **TensorFlow installed from (source or binary)**:pip install tensorflow \r\n- **TensorFlow version (use command below)**: tensorflow-1.4.0-cp27-cp27m-macosx_10_11_x86_64.whl\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: CPU\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport itertools\r\nimport tensorflow as tf\r\n\r\ndef gen():\r\n  for i in itertools.count(1):\r\n    yield (i, [1] * 5)\r\n\r\nds = tf.data.Dataset.from_generator(\r\n    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))\r\nds = ds.make_one_shot_iterator()\r\n\r\nwith tf.Session() as sess:\r\n    while True:\r\n        sess.run(ds.get_next())  # (1, array([1, 1, 1, 1, 1]))...\r\n```\r\n\r\n### Describe the problem\r\nIf you run the code above, which is adapted from tf.data.Dataset.from_generator docstring, the program will crash with an error: `libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: thread constructor failed: Resource temporarily unavailable`.\r\n\r\nI see the number of threads increasing in the activity monitor of the mac OS and when it reaches ~3K threads the program crashes. It takes several seconds. \r\n\r\nPlease let me know if this is not intended use of this API, it is OS releated issue or there is a bug involved.\r\n", "comments": ["The problem arises from  calling `Iterator.get_next()` *inside* the loop, which creates a new operation (and, in this case, a new background thread) in each iteration. The following code should not suffer from the same problem:\r\n\r\n```python\r\nimport itertools\r\nimport tensorflow as tf\r\n\r\ndef gen():\r\n  for i in itertools.count(1):\r\n    yield (i, [1] * 5)\r\n\r\nds = tf.data.Dataset.from_generator(\r\n    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))\r\nds = ds.make_one_shot_iterator()\r\n\r\nnext_element = ds.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    while True:\r\n        sess.run(next_element)\r\n```", "Makes sense now!\r\n\r\n--\r\nLinas"]}, {"number": 15161, "title": "Create Simple DNNClassifier ", "body": "Hi, I'm beginner in Machine Learning and Tensorflow.\r\nI edited [this](https://www.tensorflow.org/get_started/estimator) to adapt it to my dataset (102 features-input and 4 classes-output).\r\n\r\n**Question:**\r\nThis simple approach is correct for a simple classifier?\r\nWhy can't I change the number of nodes and levels (compilier gives errors)? \r\n\r\n**Code:**\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport os\r\nfrom six.moves.urllib.request import urlopen\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport EstraiFeature as ef\r\nfrom pyAudioAnalysis import audioBasicIO\r\n# Datasets (Training and Testing set doesn't refer to iris)\r\nIRIS_TRAINING = \"/Users/giuseppeaccardo/Documents/python/Depressione/TrainingSet.csv\" \r\nIRIS_TEST = \"/Users/giuseppeaccardo/Documents/python/Depressione/TestingSet.csv\" \r\n\r\ndef main():\r\n    # Load datasets.\r\n    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n        filename=IRIS_TRAINING,\r\n        target_dtype=np.int,\r\n        features_dtype=np.float32)\r\n    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n        filename=IRIS_TEST,\r\n        target_dtype=np.int,\r\n        features_dtype=np.float32)\r\n\r\n    # Specify that all features have real-value data\r\n    feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[102])]\r\n    \r\n    # Build 3 layer DNN with 10, 20, 10 units respectively. How can I change this values? \r\n    classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\r\n                                          hidden_units=[10,20, 10],\r\n                                          n_classes=4,\r\n                                          model_dir=\"/tmp/depres_model\")\r\n    \r\n   # Define the TRAINING inputs, includes both the feature (DNN input end) and target (DNN output end)\r\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": np.array(training_set.data)}, #training_set.data\r\n        y=np.array(training_set.target), #training_set.target\r\n        num_epochs=None,\r\n        shuffle=True)\r\n    \r\n    # Fit model.\r\n    print(\"Training classfier...\")\r\n    classifier.train(\r\n        input_fn = train_input_fn,\r\n        steps = 2000)\r\n\r\n    #Define the TEST inputs, both feature and target\r\n    test_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\":np.array(test_set.data)},\r\n        y=np.array(test_set.target),\r\n        num_epochs=1,\r\n        shuffle=False)\r\n\r\n    #Evaluate accuracy after training\r\n    accuracy_score = classifier.evaluate(\r\n        input_fn=test_input_fn)[\"accuracy\"]\r\n\r\n    print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\r\n    # Bag of words  approach -> Conta le label pi\u00f9 ripetute per ogni mid range\r\n    # PREDIZIONE\r\n    #Predict with new data\r\n    filename = \"/Users/giuseppeaccardo/Documents/python/Depressione/dataset/audio/426_AUDIO.wav\"\r\n    [Fs, signal] = audioBasicIO.readAudioFile(filename)  # read audio signal\r\n    [mtFeatures, _] = ef.estraiFeatureMt(signal, Fs)\r\n    mtF = mtFeatures.T       \r\n    new_samples = np.array(\r\n        mtF, dtype=np.float32\r\n    )        \r\n\r\n    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\":new_samples},\r\n        num_epochs=1,\r\n        shuffle=False\r\n    )\r\n\r\n    predictions = list(classifier.predict(input_fn=predict_input_fn))\r\n    countLabel = [0,0,0,0]\r\n    classPredict = 0\r\n    \r\n    for p in predictions:\r\n        countLabel[int(p[\"class_ids\"])] +=1\r\n        \r\n    print(countLabel)\r\n    print(\"Prevision is \"+ str(countLabel.index(max(countLabel))) ) \r\n    \r\nif __name__ == \"__main__\":\r\n    main()\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Can you please add more details like what the error code is? Where is it? Other details like what changes you made? Can you also post the date you are using here?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 15160, "title": "Installing Tensorflow from source", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRedhat\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.4.1\r\n- **Python version**: \r\n2.7.14\r\n- **Bazel version (if compiling from source)**:\r\n0.7.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc 4.8.5\r\n\r\n- **CUDA/cuDNN version**:\r\n8/6\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI am trying to install TF. Its the last part of building TF from source.\r\n\r\npip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n[amalik@node05 tensorflow]$ pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl \r\nProcessing /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl\r\nRequirement already satisfied: enum34>=1.1.6 in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)\r\nRequirement already satisfied: backports.weakref>=1.0rc1 in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)\r\nRequirement already satisfied: wheel in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)\r\nRequirement already satisfied: mock>=2.0.0 in /lfs1/software7/anaconda2/lib/python2.7/site-packages (from tensorflow==1.4.1)\r\nCollecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.1)\r\n  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a2261d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/\r\n  Retrying (Retry(total=3, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a226350>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/\r\n  Retrying (Retry(total=2, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a2264d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/\r\n  Retrying (Retry(total=1, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a226650>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/\r\n  Retrying (Retry(total=0, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f384a2267d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-tensorboard/\r\n  Could not find a version that satisfies the requirement tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.1) (from versions: )\r\nNo matching distribution found for tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.1)\r\n\r\n\r\n\r\n", "comments": ["Closing until we resolve your previous error."]}, {"number": 15159, "title": "Unable access S3 using the S3 filesystem", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  docker container based on centos7\r\n- **TensorFlow installed from (source or binary)**: binary, from pip\r\n- **TensorFlow version (use command below)**: ('v1.4.0-rc1-11-g130a514', '1.4.0')\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**:\r\n```\r\nfrom tensorflow.python.lib.io import file_io\r\nfile_io.stat('s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data')\r\n```\r\n\r\n\r\n### Describe the problem\r\nI'm unable to read files using the s3 filesystem. I believe my example above should work, please correct my usage of the api if not. I tried using both an object in a public bucket and one in a private bucket that I have credentials in my ~/.aws/config for. \r\n\r\nI'm not quite sure what the error is, or how to diagnose it further. Setting the log level to Debug had no effect. `tf.logging.set_verbosity(tf.logging.DEBUG)` \r\n\r\n### Source code / logs\r\n\r\nTraceback from file_io.stat\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 540, in stat\r\n    return file_statistics\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Object s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data does not exist\r\n```\r\n\r\nUsing the aws cli, you can verify the object exists \r\n```\r\naws s3 ls s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data\r\n```", "comments": ["Hmmm, the 2-line program works when I try it on my workstation. Did you try the aws cli command line in the docker container, or outside of it?\r\n\r\n/CC @drpngx, any ideas?", "The cli command works both from without and outside the container. It might be worthwhile to try and reproduce on a common docker image. Do you have one that you'd recommend? \r\n\r\nIn my current setup, I'm running the container using docker for mac with a mounted volume for my aws credentials ", "Good idea. The TensorFlow docker images are [here](https://hub.docker.com/r/tensorflow/tensorflow/tags/). The following commands worked for me:\r\n\r\n```\r\ndocker run -it tensorflow/tensorflow:1.4.0 bash\r\npython\r\n>>> from tensorflow.python.lib.io import file_io\r\n>>> file_io.stat('s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data')\r\n```\r\nI ran this on Ubuntu 14.04 and didn't use any AWS credentials. I don't think the host OS matters, but try without the credentials to match what I did.", "Right, I'm guessing it's an auth issue. Some of these things are env variable controlled. The cli also drops files in the home directory. Is it a public bucket?", "I can confirm official tensorflow docker image works correctly for both public and private buckets. \r\n\r\nThe below Dockerfile is the smallest repro of the issue. Are there additional dependencies I need to install? Does the pip installation ship with the s3 filesystem? \r\n\r\n```\r\ndocker build -t tf-15159:latest .\r\ndocker run --rm -it tf-15159:latest\r\n```\r\n\r\n```\r\nFROM centos:7\r\n\r\nRUN yum -y update && \\\r\n    yum -y install epel-release && \\\r\n    yum -y install python2-pip\r\n\r\nRUN pip install 'tensorflow == 1.4.0'\r\n\r\nCMD python -c \"from tensorflow.python.lib.io import file_io; file_io.stat('s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data')\"\r\n```", "Can you try `env` to see what you have defined (please don't paste your secret key here).", "Heres my environment variables within the container \r\n\r\n```\r\n[root@af2f2940e30a /]# env\r\nHOSTNAME=af2f2940e30a\r\nTERM=xterm\r\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:\r\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\r\nPWD=/\r\nSHLVL=1\r\nHOME=/root\r\nno_proxy=*.local, 169.254/16\r\n_=/usr/bin/env\r\n[root@af2f2940e30a /]#\r\n```", "For comparison, the env inside the tensorflow/tensorflow:1.4.0 container\r\n\r\n```\r\nHOSTNAME=2b58ca0af29e\r\nTERM=xterm\r\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\r\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\r\nPWD=/notebooks\r\nSHLVL=1\r\nHOME=/root\r\nno_proxy=*.local, 169.254/16\r\n_=/usr/bin/env\r\n```", "Please note, that without credentials you must pass the `--no-sign-request` option to the cli. So that would look like this\r\n\r\n```\r\npip install awscli\r\n\r\naws s3 ls s3://datasets.elasticmapreduce/ngrams/books/20090715/eng-1M/1gram/data --region us-east-1 --no-sign-request\r\n```\r\n\r\nThis works correctly in both containers. \r\n\r\nIs there a way to enable debug logging for the s3 filesystem? It would be useful to look at the actual requests it is making\r\n", "The code is [here](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L404) but there is no debugging info, unless the AWS toolkit prints something.\r\n\r\nIf memory serves, there was a perl script cli that you could tweak to dump the header. If you fixed up the date/time field then you could compare side-by-side with your implementation.\r\n\r\nThe code does use `.WithKey()`. @yongtang any suggestion?", "@drpngx (sorry if this is what you're saying - being a pedant to assure clarity) `.WithKey()` refers to the object key which is basically the path & file name within the bucket, as opposed to being a key related to authentication / credentials.", "@chris-smith-zocdoc it is something peculiar to CentOS 7 or the Python dot-dot version therein.\r\n\r\nThis Dockerfile:\r\n```Dockerfile\r\nFROM ubuntu:16.04\r\n\r\nRUN apt-get update && apt-get install -y --no-install-recommends \\\r\n        ca-certificates \\\r\n        curl \\\r\n        python \\\r\n        && \\\r\n    apt-get clean && \\\r\n    rm -rf /var/lib/apt/lists/*\r\n\r\nRUN curl -O https://bootstrap.pypa.io/get-pip.py && \\\r\n    python get-pip.py && \\\r\n    rm get-pip.py\r\n\r\nRUN pip install 'tensorflow == 1.4.0'\r\n\r\nCMD sleep infinity\r\n```\r\nwhen built and run (attaching a bash shell, running Python:\r\n```\r\nPython 2.7.12 (default, Nov 20 2017, 18:23:56) \r\n[GCC 5.4.0 20160609] on linux2\r\n```\r\n) works fine stat'ing the s3 file you cite, where as this Dockerfile\r\n```Dockerfile\r\nFROM centos:7\r\n\r\nRUN yum -y update && \\\r\n    yum -y install \\\r\n        ca-certificates \\\r\n        curl \\\r\n        python\r\n\r\nRUN curl -O https://bootstrap.pypa.io/get-pip.py && \\\r\n    python get-pip.py && \\\r\n    rm get-pip.py\r\n\r\nRUN pip install 'tensorflow == 1.4.0'\r\n\r\nCMD sleep infinity\r\n```\r\nwhen built and run (attaching a bash shell, running Python:\r\n```\r\nPython 2.7.5 (default, Aug  4 2017, 00:39:18) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2\r\n```\r\n) produces the error you cite.\r\n", "Hmm, not sure what you want to do with this issue, but I'll move over to an ubuntu based image", "I think adding AWS logging to TensorFlow's logging system might help. Will take a look and see if I can do something about it.", "Added a PR #15493 for the S3 logging. Please take a look if interested.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "PR merged. Thank you!"]}]