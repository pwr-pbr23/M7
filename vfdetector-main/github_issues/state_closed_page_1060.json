[{"number": 21485, "title": "Keras save_weights/load_weights error for custom layer", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: Quardo M4000\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI create a simple custom layer performing a multiply. However, if I do not define \"get_config()\" function for my custom layer, after saving and loading weights, the layer's weights are not loaded. If I implemented the \"get_config()\" function for my custom layer as suggested by many online discussions, the program crashed when calling \"save_weights\" with the error message \"TypeError: can't pickle _thread.RLock objects\". According to some online discussions, I change the \"self.add_variable\" by \"K.variable\", but the crash still happened. I have went through many online discussions on Github and Stackoverflow, but none worked for me. Therefore, I doubt whether there are some bugs for \"save_weights\" or I use it in a wrong way. Thank you.\r\n\r\nMy codes are as follows:\r\n\r\n\tclass MaskLayer(Layer):\r\n\t  def __init__(self, data_format=None, **kwargs):\r\n\t\tself.data_format = data_format\r\n\t\tsuper(MaskLayer, self).__init__(trainable=False, **kwargs)\r\n\r\n\t  def build(self, input_shape):\r\n\t\tif self.data_format == 'channels_first':\r\n\t\t  channel_axis = 1\r\n\t\telse:\r\n\t\t  channel_axis = -1\r\n\t\tdimension = input_shape[channel_axis]\r\n\t\t# Either definition of self.mask below lead to the same results\r\n\t\tself.mask = self.add_variable(name='mask', shape=dimension, \r\n                                                            initializer='ones', trainable=False, dtype=self.dtype)\r\n\t#    self.mask = K.variable(np.ones(dimension), name='mask')\r\n\t\tsuper(MaskLayer, self).build(input_shape)\r\n\r\n\t  def call(self, inputs):\r\n\t\treturn inputs * self.mask\r\n\r\n\t  def set_weights(self, weights):\r\n\t\tK.batch_set_value([(self.mask, weights)])\r\n\r\n\t  # If this get_config is enalbed, the program crashed when calling \"save_weights\"\r\n\t#  def get_config(self):\r\n\t#    config = {'mask': self.mask}\r\n\t#    base_config = super(MaskLayer, self).get_config()\r\n\t#    return dict(list(base_config.items()) + list(config.items()))\r\n\t\t\r\n\tdef test():\r\n\t  data = np.ones((1,1,4,5), dtype=float)\r\n\r\n\t  model = keras.Sequential()\r\n\t  mask_layer = MaskLayer()\r\n\t  model.add(mask_layer)\r\n\r\n\t  for layer in model.layers:\r\n\t\tprint(layer.name)\r\n\t\tweights = layer.get_weights()\r\n\t\tprint(weights)\r\n\t  results = model.predict(data)\r\n\t  print(results)\r\n\t  print('-------------------------------')\r\n\t  mask_layer.set_weights([1,-1,0,2,-2])\r\n\r\n\t  for layer in model.layers:\r\n\t\tprint(layer.name)\r\n\t\tweights = layer.get_weights()\r\n\t\tprint(weights)\r\n\t  results = model.predict(data)\r\n\t  print(results)\r\n\t  print('-------------------------------')\r\n\t  \r\n\t  model.save_weights('models/test/test')\r\n\r\n\t  new_model = keras.Sequential()\r\n\t  mask_layer = MaskLayer()\r\n\t  new_model.add(mask_layer)\r\n\r\n\t  new_model.load_weights('models/test/test')\r\n\t  for layer in new_model.layers:\r\n\t\tprint(layer.name)\r\n\t\tweights = layer.get_weights()\r\n\t\tprint(weights)\r\n\r\n\t  results_new = new_model.predict(data)\r\n\t  print(results_new)\r\n", "comments": ["`get_config` is for Python state associated with the Layer. TensorFlow variables are saved separately, so the error is because there's no pickling defined for them.\r\n\r\nOtherwise I assume this is the same issue as https://github.com/tensorflow/tensorflow/issues/21460 ? Or can you reproduce on a more recent build?", "@allenlavoie Thank you for your response. For #21460, the bug is from loading weights. For this one, if I enable \"get_config()\", the crash happened during save_weights(). Therefore, I submitted this new issue. \r\n\r\nI do not quite understand the issue. Do you mean that I cannot use get_config() for my custom layer? However, I see some online discussions claiming they use \"get_config()\" without any issues. Did I use it wrong? Thank you.", "I just mean that including variables in get_config() (`{'mask': self.mask}`, where `self.mask` looks like a variable) is unusual and not supported. Putting `dimension` and defining a `from_config` method which re-creates `self.mask` is the usual path; `load_weights` will then load the variable automatically. But you can load the weights without defining `get_config` or `from_config`.", "@allenlavoie Thank you for your explanation. If get_config() should not work with a variable, then the problem seems to be same with #21460. I will close this issue then. If the problem still exists after trying the new build of Tensorflow, I will reopen this later. Thank you for your help.", "After trying the new nightly built version, the program runs correctly if I implement a \"compute_output_shape\" function in the MaskLayer class. "]}, {"number": 21484, "title": "the url is invalid, maybe you need to update it", "body": "The answer to the question is invalid. Can you update it?", "comments": []}, {"number": 21483, "title": "[Intel MKL] Adding Quantized Convolution (Int8) - Part 1", "body": "This PR is part 1 of four PRs that add quantized version of convolution using MKL-DNN. This part implements the quantized convolution kernel.\r\n\r\nPlease note that the below PRs replace this one #19425\r\nPart 1: #21483 \r\nPart 2: #21456\r\nPart 3: #21465\r\nPart 4: #21466", "comments": ["Nagging Reviewer @tatianashp, @raghuraman-k: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 21 days with no activity and the `awaiting review` label has been applied.", "@tatianashp @raghuraman-k Any update/comments on this PR? ", "@mahmoud-abuzaina could you run clang-format?", "@drpngx Ran clang-format.", "@drpngx I ran the following from my side clang-format [LLVM version 3.4.2]. Could you please suggest me if I'm not doing correctly.\r\n\r\n> clang-format -i -style=Google tensorflow/core/kernels/mkl_conv_ops.cc", "Yeah unfortunately clang-format does not produce completely stable results.", "@drpngx Tests looks fine. Ready to go?", "I just noticed I hadn't sent the review. Sorry.", "@drpngx changed enum to enum class. Thanks!", "@drpngx  Thanks for taking care of the other int8/quarantined convolution PRs. Can you approve this also?", "@drpngx Sorry to bug you. Can you take a look at this PR? Thanks a lot.", "I'm not sure I have the expertise to review that. Maybe @tatianashp has someone in mind that would be better suited?", "@drpngx  thanks. @tatianashp @raghuraman-k can you approve the changes we made since @raghuraman-k first approved the PR?", "@drpngx @tatianashp @raghuraman-k Any update on this PR?\r\n", "@tatianashp @raghuraman-k @drpngx Any update on this PR?", "@tatianashp Thanks for the approval. The XLA errors are not related to our work, for example, similar xla failures are there for other PR #23414 (https://source.cloud.google.com/results/invocations/1756d954-ba6d-4235-b728-fe1a70ab53b4/targets)\r\n\r\nLet us know if you need anything else for merging the PR. Thanks!", "@drpngx It looks like the failures are not related to this PR. Can you start the merge process? Thanks.", "@drpngx Looks like the tests are all passing now. Can you merge the PR?", "Merge initiated. It'll go to internal tests and approval.", "@drpngx Can you take a look at this PR, it is stuck in the \"ready to pull\" state?", "Yes, it's stuck because of #22515. The last time this happened, it automatically got unstuck while I was debugging. Let's wait until the end of the day.", "/CC @yifeif ", "The PR got imported and is pending for internal testing and approval now FYI.", "Could you remove the `CHECK_EQ` reformatting? There is an internal check we don't add any new `CHECK`s in the code.", "@drpngx Do you suggest any function call like `TF_RETURN_IF_ERROR`? The problem is `CHECK_EQ` is inside some function whose return type is void and is outside OpKernel so we can't use `OP_REQUIRES` either.", "Either you fix it like this, which would be my preference without looking\nat the code, it seems like you haven't actually changed the code, so what I\nwas suggesting is that you just undo the formatting change.\n\nOn Thu, Nov 8, 2018, 1:05 PM FAIJUL <notifications@github.com wrote:\n\n> @drpngx <https://github.com/drpngx> Do you suggest any function call like\n> TF_RETURN_IF_ERROR? The problem CHECK_EQ is inside some function whose\n> return type is void and is outside OpKernel so we can't use OP_REQUIRES\n> either.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21483#issuecomment-437154244>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbZ6N4HF3blbeta4eDaZtw49kQEAfks5utJyqgaJpZM4V0TsA>\n> .\n>\n", "@drpngx This `CHECK_EQ` is new code added in this PR. Is `TF_CHECK_OK` fine? Or  do you have any suggestion?", "So, you have two options:\r\n\r\n1. You can make the function and its calling chain return `Status`.\r\n2. If it's a true assertion, and if it's performance critical code, then you can use `DCHECK` which will only check with debug mode.\r\n", "Sorry, could you `DCHECK_EQ` for everything that's not an bool? `DCHECK(is_true)` is fine, but please use `DCHECK_EQ(num_value, 4)` gives a better error message.", "@drpngx Changed DCHECK to DCHECK_EQ.", "Thank you!", "@drpngx Is it passing all tests now?", "There is a problem with\r\n`//tensorflow/python/autograph/pyct/static_analysis:reaching_definitions_test`. No log. Let me try again.", "@drpngx Yes, it may be a flaky test. It is passing for me locally. Also looks like other PRs are failing because of this test too.", "@drpngx Is the testing results ok now? Can you merge the PR?", "I just kicked off the tests again, but it needs internal approval from another googler. @tatianashp could you have a look?", "@drpngx thanks. @tatianashp can you approve again?", "@drpngx are the tests all passing now?", "It fails again on the same test.", "@drpngx It is not failing in our internal testing, can you provide a log? Thanks."]}, {"number": 21482, "title": "tf.nn.embedding_lookup_sparse without combiner, to get result similar to tf.nn.embedded_lookup using dense input params", "body": "I would like to undrestand if `tf.nn.embedding_lookup_sparse` with sparse tensor can be used to get result similar to `tf.nn.embedded_lookup ` which requires dense representation of sparse tensor.\r\n\r\nCurrently im converting the sparse tensor to dense tensor (using `tf.sparse_tensor_to_dense`) and suppling the same as input to  `tf.nn.embedded_lookup `. This works well as expected\r\n\r\nHowever from the naming convention it seemed `tf.nn.embedding_lookup_sparse` is doing the same process using the sparse tensor without the need to convert to dense representation. But the documentation specifies the method tries to combine the values in the dense representation based on some strategy. \r\n\r\nis there any way we can get the embeddng lookup on the sparse tensor, without combining the row values using the `tf.nn.embedding_lookup_sparse` \r\n\r\nOne relavent question on stackoverlfow is copied for reference\r\n[https://stackoverflow.com/questions/45789532/embedding-lookup-sparse-of-tensorflow-without-combiner](url)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code: Yes\r\n\r\n>                     dense = tf.sparse_tensor_to_dense(ws,name='dense')\r\n>                     char_vec = tf.nn.embedding_lookup(embeddings, dense,name='embedded_lookup')\r\n>                     # Output is Batch x  Max Words x Max Char x Char Feature Size\r\n\r\n`vs`\r\n\r\n>                     char_vec = tf.nn.embedding_lookup(embeddings, dense,name='embedded_lookup')\r\n>                     # Output is Batch x  Max Words x Char Feature Size\r\n\r\nI would like the output shape to be similar to the embedding lookup\r\n\r\n`Batch x  Max Words x Max Char x Char Feature Size`\r\n\r\n\r\n\r\n\r\n\r\n\r\nOS Platform and Distribution: Windows\r\nTensorFlow installed from: pip install tensorflow\r\nTensorFlow version: 1.9.0\r\nBazel version: Unknown\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: NA\r\nMobile device: NA\r\n", "@amadupu Hi, the stackoverflow link is broken. Could you explain your problem in more detials? I think both `embedding_lookup_sparse` and `embedding_lookup` require a dense tensor for `params`.  The difference is, for `id` argument, `embedding_lookup` require a dense id while `embedding_looup_sparse` receives sparse id.", "Both of them use combiners to combine values, so I'm not sure what your question is here.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@amadupu  -  Hi , could you please respond to the above ?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 21481, "title": "enable double precision for SparseTensorDenseMatMul on GPU", "body": "Trivial changes (just adding some extra macro calls)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@bendavid it looks like there are build errors. Could you take a look?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 21480, "title": "ppc64le: CI/CD test scripts ", "body": "This PR enables unit test to be run on ppc64le by invoking ci_build.sh. For example:\r\n./tensorflow/tools/ci_build/ci_build.sh gpu \\\r\n    --docker file tensorflow/tools/ci_build/Dockerfile.gpu.ppc64le \\\r\n    ./tensorflow/tools/ci_build/linux/ppc64le/gpu/run_py3_core.sh\r\n\r\nThis PR also fixes two other problems found while testing the scripts:\r\n1) Since configure.py defaults to NCCL2 and NCCL is not available for ppc64le and CUDA 9.0, in the docker file we set TF_NCCL_VERSION to 1.\r\n\r\n2) Test on python3 were seg faulting when using h5py, I modified the ppc64le specific script that installs h5py to install pip and then install h5py using pip instead of easy_install. ", "comments": ["@wdirons could you push the commit with the fixes to the branch?\r\nI can then approve and merge the PR right away.", "@gunan, I added the fixes this pull request. Thank you."]}, {"number": 21479, "title": "Fix op_scope warning in adjust_gamma", "body": "While running the following op_scope causes the warning:\r\n```\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01)\r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\ni>>> import numpy as np\r\n>>> tf.image.adjust_gamma(np.random.uniform(0.0, 255.0, (8, 8)), gamma=1)\r\nWARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\r\n<tf.Tensor 'adjust_gamma/mul_1:0' shape=(8, 8) dtype=float32>\r\n>>>\r\n```\r\n\r\nThis fix fixes the warning by switching op_scope to name_scope.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@tombstone could you take a look?", "Nagging Reviewer : You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied."]}, {"number": 21478, "title": "Implement \"quantize_graph\" will print \"tf.estimator package not installed.\"", "body": "Making quantization by using command \"sudo bazel-bin/tensorflow/tools/quantization/quantize_graph .......\" in the directory of tensorflow, and it will input \"tf.estimator package not installed.\" And in this directory, implemt \"python->import tensorflow\", it will return error message. But in other directory, implemt \"python->import tensorflow\", it will be normally. The folllowing is my envirnment information: \r\n OS Platform and Distribution: Linux Ubuntu 18.04\r\n TensorFlow installed from: using TF source code (CPU) for graph conversion, not using binary-python(GPU) for inference\r\nTensorFlow version:  using r1.10.0-rc1\r\nPython version: 3.6\r\nBazel version: 0.16.0\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code: N/A\r\nCUDA/cuDNN version: Not supported when confugurate\r\nGPU model and memory : Not supported GPU\r\nExact command to reproduce : N/A\r\nMobile device: In PC not in mobile device\r\nThank you.", "To complement,\r\nin the directory of tensorflow/: \r\nxxxx@edward-ubuntu-18:~/tensorflow$ python\r\nPython 3.6.5 (default, Apr  1 2018, 05:46:30)\r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/home/xx/tensorflow/tensorflow/python/platform/self_check.py\", line 25, in <module>\r\n    from tensorflow.python.platform import build_info\r\nImportError: cannot import name 'build_info'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/xx/tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/xx/tensorflow/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/xx/tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\n  File \"/home/xx/tensorflow/tensorflow/python/platform/self_check.py\", line 27, in <module>\r\n    raise ImportError(\"Could not import tensorflow. Do not import tensorflow \"\r\nImportError: Could not import tensorflow. Do not import tensorflow from its source directory; change directory to outside the TensorFlow source tree, and relaunch your Python interpreter from there.\r\n\r\nBut when implemet the same operation not in the directory of /tensorflow, it can successfully import tensorflow", "Hi, I have the same problem with the \"optimize_for_inference\" script : \"tf.estimator package not installed\"... I'm using TF 1.9 ", "i found that it may because the version of tensorflow is not fit to the version of bazel.", "What do you mean by \"not fit\" ? What is the version that we need to use so ?", "I have the same problem with the \"optimize_for_inference\"  and \" quantize_graph\" scripts in TF 1.8 / 1.9 ", "I have the same problem with the \"optimize_for_inference\" script in TF 1.10", "same problem with \"quantize_graph\" scripts in latest TF", "I had the same issue with optimize_for_inference, commenting out the following lines\r\n`from tensorflow.python.estimator import api as estimator_api  # pylint: disable=g-import-not-at-top  \r\n__path__ += [os.path.dirname(estimator_api.__file__)]  \r\ndel estimator_api` from the top of this file (inside `try..except` block)\r\n`bazel-bin/tensorflow/python/tools/optimize_for_inference.runfiles/org_tensorflow/tensorflow/__init__.py` seems to solve it though getting some official comments would be great", "I have the same problem when compile\r\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\\r\n    --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\r\n    //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n    --crosstool_top=//external:android/crosstool --cpu=arm64-v8a", "@case540 There probably is some additional untangling to be done?", "I'll take a look", "> I have the same problem when compile\r\n> bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \r\n> --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \r\n> //tensorflow/contrib/android:libtensorflow_inference.so \r\n> --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \r\n> --crosstool_top=//external:android/crosstool --cpu=arm64-v8a\r\n\r\nI also see this with 1.11 when building selective registration for iOS: \r\nbazel-bin/tensorflow/python/tools/print_selective_registration_header \\\r\n  --graphs=\"${MODEL_PATHS}\" > tensorflow/core/framework/ops_to_register.h", "@lauriebyrum Please feel free to open a new issue explaining your problem since your system configuration is different. It will help us to focus on your case closely.  Thanks!", "Hi There,\r\n\r\nWe are moving this issue to closed status, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.\r\nPlease open a new issue for any help you need against 2.x, and we will get you the right help.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21478\">No</a>\n"]}, {"number": 21477, "title": "Correct links to documentation pages", "body": "(redo of https://github.com/tensorflow/tensorflow/pull/21457)", "comments": []}, {"number": 21476, "title": "Transforming a dataset pre-processing to an inference graph", "body": "If we are working with data with fixed dimensions we can easily load the data in a dataset using from_tensor_slices. Thus, when working on inference, we can use a placeholder that will give the dataset the numpy array.\r\n\r\nWhen working with data with varying dimensions (text and a like) most of the answers I've seen suggest using the from_generator function and it works. However there doesn't seem to be a way to make if work in inference.\r\n\r\nSeveral solutions I though of are running the dataset ops separately from the inference graph or serialize the data and have the inference graph read it.\r\n\r\nIs there a recommended way of doing this? I want to reuse the dataset preprocessing codes in the training rather than implement them again with in numpy so that I can feed it to the inference graph. If not I think there is a value in having it in tensorflow.\r\n\r\nHave I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: master branch\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\nMobile device: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "There are several ways to handle data transformation during inference. The best method depends on the model you are using and the context for inference. Are you running inference using a serving system like tf-serving? In Python? Using Estimator or Keras? Any other relevant details you can provide?", "I'm using the low level tensorflow API. As recommended I create 3 graphs for training, evaluation, and inference and I'm using the Dataset APIs to load my data.\r\n\r\nIf we are working with model inputs with similar lengths or dimension, you can do this.\r\n\r\n```python\r\n# x and y are uniformly sized numpy arrays\r\ndataset = tf.data.Dataset.from_tensor_slices((x, y), ...)\r\ndataset = dataset.map(...)\r\ndataset = dataset.padded_batch(...)\r\niterator = dataset.make_one_shot_iterator()\r\ninputs = iterator.get_next()\r\n\r\n# training graph\r\ncreate_training_graph(inputs)\r\n```\r\n\r\nFor inference, you can simply remove y and replace x with a placeholder so you can give the data to the network as feed dict.\r\n\r\nWhen working with data with difference input lengths (text, and other sequences), most of the issues posted here that I've seen recommend to use generators\r\n\r\n```python\r\n# Creating a generator since sequence lengths are different\r\ndef data_generator(x, y):\r\n    ...\r\n\r\ndataset = tf.data.Dataset.from_generator(data_generator(x, y), ...)\r\ndataset = dataset.map(...)\r\ndataset = dataset.padded_batch(...)\r\niterator = dataset.make_one_shot_iterator()\r\ninputs = iterator.get_next()\r\n\r\n# training graph\r\ncreate_training_graph(inputs)\r\n```\r\n\r\nIn this case, how do we try to add the placeholders in order to perform inference without making a large changes in the dataset pre-processing?", "In general, I would recommend using the high-level APIs, which have made this sort of task easier. In this case, it's a little hard to advise, because it's not exactly clear to me what about your approach isn't working-- can you provide the code and error message you are getting? In theory, as long as you build your dataset ops in the inference graph, separate from the training graph, you should be fine, if I understand your use-case correctly.", "Doesn't matter even if I use the Estimator APIs since my problem is not with the network but just with the way I can load the data.\r\n\r\nFor the training graph, the data I want to load is fixed (this is my training set) so it doesn't matter if the data is fixed in the graph. For inference graph, the data I want to load is different since this is based on real world data. Hence, I would like to use place holder (or other techniques to vary my input, if any).\r\n\r\nThere is no problem with the code when using data with consistent dimensions. For example [[1, 2], [3, 4]] has a shape of (2,2). This can be fed to a placeholder without issues. If the data however has some inconsistency with its shape like [[1, 2], [3,4,5]], there is no easy way to feed it to the network.\r\n\r\nBefore this doesn't bring any issues since most tensorflow ops requires your shape to be consistent. However the dataset API allows creation of dataset with varying lengths.\r\n\r\nThis works \r\n\r\n```python\r\ndata = [[1, 2], [3, 4]]\r\nplaceholder = tf.placeholder(tf.int32)\r\nwith tf.Session() as sess:\r\n    sess.run(placeholder, feed_dict={placeholder:data})\r\n```\r\n\r\nBut this will have an error\r\n\r\n```python\r\ndata = [[1, 2], [3, 4, 5]]\r\nplaceholder = tf.placeholder(tf.int32)\r\nwith tf.Session() as sess:\r\n    sess.run(placeholder, feed_dict={placeholder:data})\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-1f6abd9e9211> in <module>()\r\n      1 with tf.Session() as sess:\r\n----> 2     sess.run(placeholder, feed_dict={placeholder:data})\r\n\r\nc:\\users\\raffae~1\\docume~1\\virtua~1\\machin~1\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    898     try:\r\n    899       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 900                          run_metadata_ptr)\r\n    901       if run_metadata:\r\n    902         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nc:\\users\\raffae~1\\docume~1\\virtua~1\\machin~1\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1102             feed_handles[subfeed_t] = subfeed_val\r\n   1103           else:\r\n-> 1104             np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\r\n   1105 \r\n   1106           if (not is_tensor_handle_feed and\r\n\r\nc:\\users\\raffae~1\\docume~1\\virtua~1\\machin~1\\lib\\site-packages\\numpy\\core\\numeric.py in asarray(a, dtype, order)\r\n    490 \r\n    491     \"\"\"\r\n--> 492     return array(a, dtype, copy=False, order=order)\r\n    493 \r\n    494 \r\n\r\nValueError: setting an array element with a sequence.\r\n```\r\n\r\nOf course there might be ways to make it work without using a placeholder. And if there are I'll settle for those.", "Nagging Assignee @karmel: It has been 21 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "For uneven sequences, you might use a serialized tf.Example or tf.SequenceExample; that would allow you to deserialize and parse at run time, adding the padding you want. Alternatively, you can pad out your data as you desire in numpy to match the target representation internally. Would either of those solutions work?", "So far I explored using the tf.SequenceExample I was able to produce the result I wanted. The only problem I have is the lack of documentation about the subject. As far as this issue is concerned, I think I'll close the issue. Thanks."]}, {"number": 21475, "title": "FusedBatchNorm problem during benchmark test", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None\r\n- **TensorFlow installed from (source or binary)**: 1.9\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.11.1 (for covert and benchmark the model)\r\n- **GCC/Compiler version (if compiling from source)**: None, installed from build\r\n- **CUDA/cuDNN version**: Not used\r\n- **GPU model and memory**: Not used\r\n- **Exact command to reproduce**:\r\n\r\n### Problem description\r\nI'm trying to convert a piece of mobilenetV1 (using slim model) from the master repository [https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md](url); In particular I have added 2 simple convolution after the _MobilenetV1/Conv2d_8_pointwise/Conv2D_. Obviously the number of parameters is more less than the original MobilenetV1. Without train the model, I have initialized all the variables and save them into a freezed model. Now If I visualize the model with tensorboard is the same of that downloaded from the link above until the _MobilenetV1/Conv2d_8_pointwise/Conv2D_ (It compare the Conv2D and the FusedBatchNorm, Gamma, Beta, var and so on). The problems cames when I try to benchmark the model using bazel: the original one is more faster than mine _(FLOPs estimated 1.14B VS 3.31B)_ and also in summaries of the original model the _FusedBatchNorm_ Node type doesn't compare instead in mine model yes. Where am I doing wrong? And there is also the possibility of combining the conv2D and the batch norm layer?\r\n\r\n### logs\r\n\r\n\r\n------------------------- ORIGINAL MOBILENETV1 ---------------------------------------------------------------\r\n\r\n>...stat_summarizer.cc:468] \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n...stat_summarizer.cc:468] \t                  Conv2D\t       15\t    52.027\t    56.168%\t    56.168%\t 12447.652\t       15\r\n...stat_summarizer.cc:468] \t   DepthwiseConv2dNative\t       13\t    15.413\t    16.640%\t    72.808%\t  7905.664\t       13\r\n...stat_summarizer.cc:468] \t                     **Mul**\t       27\t    12.018\t    12.975%\t    85.783%\t     0.000\t       27\r\n...stat_summarizer.cc:468] \t                     **Add**\t       27\t    11.806\t    12.746%\t    98.529%\t     0.000\t       27\r\n...stat_summarizer.cc:468] \t                   **Relu6**\t       27\t     1.163\t     1.256%\t    99.784%\t     0.000\t       27\r\n...stat_summarizer.cc:468] \t                   Const\t       83\t     0.120\t     0.130%\t    99.914%\t     0.000\t       83\r\n...stat_summarizer.cc:468] \t                 Softmax\t        1\t     0.027\t     0.029%\t    99.943%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                 AvgPool\t        1\t     0.022\t     0.024%\t    99.967%\t     4.096\t        1\r\n...stat_summarizer.cc:468] \t                    NoOp\t        1\t     0.007\t     0.008%\t    99.974%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                 BiasAdd\t        1\t     0.007\t     0.008%\t    99.982%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                 Squeeze\t        1\t     0.006\t     0.006%\t    99.988%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                 _Retval\t        1\t     0.005\t     0.005%\t    99.994%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                    _Arg\t        1\t     0.004\t     0.004%\t    99.998%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                Identity\t        1\t     0.002\t     0.002%\t   100.000%\t     0.000\t        1\r\n--------------------------------------------------------------------------------------------------------------\r\n\r\n---------------------------- MY MODEL ------------------------------------------------------------------------\r\n>...stat_summarizer.cc:468] \t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n...stat_summarizer.cc:468] \t          **FusedBatchNorm**\t       18\t   151.477\t    48.975%\t    48.975%\t    72.704\t       18\r\n...stat_summarizer.cc:468] \t                  Conv2D\t       11\t   138.036\t    44.630%\t    93.605%\t 15457.344\t       11\r\n...stat_summarizer.cc:468] \t   DepthwiseConv2dNative\t        8\t    18.148\t     5.868%\t    99.472%\t  9300.352\t        8\r\n...stat_summarizer.cc:468] \t                    Relu\t       18\t     1.581\t     0.511%\t    99.984%\t     0.000\t       18\r\n...stat_summarizer.cc:468] \t                   Const\t       29\t     0.038\t     0.012%\t    99.996%\t     0.000\t       29\r\n...stat_summarizer.cc:468] \t                 _Retval\t        1\t     0.005\t     0.002%\t    99.997%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                    _Arg\t        1\t     0.004\t     0.001%\t    99.999%\t     0.000\t        1\r\n...stat_summarizer.cc:468] \t                    NoOp\t        1\t     0.004\t     0.001%\t   100.000%\t     0.000\t        1\r\n--------------------------------------------------------------------------------------------------------------\r\nOK, I have noted the differen Relu activation but I do not think that's the problem for the fusedbatchnorm layer..", "comments": ["Nagging Assignee @tatatodd: It has been 120 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nobody reply, so I think we can close the issue."]}, {"number": 21474, "title": "Fix incorrect check logic for state_forget_sig gate", "body": "State_forget_sig does not have Split operation as an input.\r\nInstead it has Add operation and Add has a Split.\r\n\r\nSigned-off-by: Jiyoung Yun <jy910.yun@samsung.com>\r\n\r\nRelated issue : #21473 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Is there anyone to review this pr? ", "@jyoungyun could you add a test perhaps? That would make it easier to make sure it's right.", "@drpngx @aselle I added new patch for verifying my changes using python script. You can test this python script with below commands.\r\n```\r\n$ python lstm_test.py --toco [toco location] --use_frozen_graph\r\n```\r\n\r\nWhen I tested this python script using original toco, it failed to find `LstmCell`:\r\n```\r\ntensorflow/contrib/lite/testing$ python lstm_test.py --toco [Original TOCO]/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco --use_frozen_graph\r\n========================\r\nNot found 'LstmCell'\r\n========================\r\n```\r\n\r\nBut when I used this PR, the result was like below:\r\n```\r\ntensorflow/contrib/lite/testing$ python lstm_test.py --toco [applied this PR]/bazel-bin/tensorflow/contrib/lite/toco/toco --use_frozen_graph\r\n========================\r\nFound 'LstmCell'\r\n========================\r\n```\r\n\r\nAlso you can save the Graphviz dot or tflite file if you use `--save_dot` or `--save_tflite`.\r\nPlease take a look and leave comments.", "Using original toco:\r\n```\r\nNumber of all operator types: 7\r\n\tCONCATENATION                         :    2 \t (instrs: 0)\r\n\tFULLY_CONNECTED                       :    2 \t (instrs: 1,629,729,558,528)\r\n\tSPLIT                                 :    2 \t (instrs: ???)\r\n\tLOGISTIC                              :    5 \t (instrs: ???)\r\n\tTANH                                  :    4 \t (instrs: ???)\r\n\tMUL                                   :    5 \t (instrs: ???)\r\n\tADD                                   :    2 \t (instrs: ???)\r\nNumber of all operators                       :   22 \t (total instrs: 1,629,729,558,528)\r\n```\r\n\r\nUsing this PR:\r\n```\r\nNumber of all operator types: 7\r\n\tCONCATENATION                         :    1 \t (instrs: 0)\r\n\tFULLY_CONNECTED                       :    1 \t (instrs: 814,864,779,264)\r\n\tSPLIT                                 :    1 \t (instrs: ???)\r\n\tLOGISTIC                              :    2 \t (instrs: ???)\r\n\tTANH                                  :    2 \t (instrs: ???)\r\n\tMUL                                   :    2 \t (instrs: ???)\r\n\tLSTM                                  :    1 \t (instrs: ???)\r\nNumber of all operators                       :   10 \t (total instrs: 814,864,779,264)\r\n```", "@drpngx Could you review this PR? Thanks in advance.", "@aselle could you take another look?", "I rebased this pr.", "@achowdhery @rthadur Do you have a plan to review this PR?", "Could you please add another reviewer? I won't likely have time to do this\nin the near future\n\nOn Wed, May 29, 2019, 9:28 AM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Reviewer @achowdhery <https://github.com/achowdhery>: You have\n> been added as a reviewer to this pull request. Please add your review or\n> reassign. It has been 14 days with no activity and the awaiting review\n> label has been applied.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21474?email_source=notifications&email_token=AE75E3PWGADOZILHO6SQ5R3PX2AHLA5CNFSM4FOOPBCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWPJXDA#issuecomment-496933772>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE75E3JH4IJU6KSQRIHTK2TPX2AHLANCNFSM4FOOPBCA>\n> .\n>\n", "I succeeded in converting to `LSTM` op without this patch by using `BasicRNNCell`, but `LSTMCell` is not.", "Sure thing. I will make a change soon.\n\nOn Fri, 28 Jun 2019 at 15:37 Yu-Cheng Ling <notifications@github.com> wrote:\n\n> *@miaout17* requested changes on this pull request.\n>\n> I haven't noticed this pull request until now.\n>\n> @jyoungyun <https://github.com/jyoungyun> Sorry for the long review\n> process.\n> If you're still pursuing this pull request, could you rebase the change\n> into tensorflow/lite/ (all the code was moved out of contrib/)\n> ------------------------------\n>\n> In tensorflow/contrib/lite/testing/lstm_test.py\n> <https://github.com/tensorflow/tensorflow/pull/21474#discussion_r298466793>\n> :\n>\n> > @@ -0,0 +1,274 @@\n> +import argparse\n>\n> Instead of adding a whole new file, could you add the test case into the\n> existing generated examples test?\n>\n>\n> https://github.com/tensorflow/tensorflow/blob/e0d9dfd54b0f2f4a2a6637268aa176d1701ee8e9/tensorflow/lite/testing/generate_examples_lib.py#L3320\n>\n> The test is runnable by\n>\n> bazel test //tensorflow/lite/testing:zip_test_lstm\n>\n> ------------------------------\n>\n> In tensorflow/contrib/lite/testing/lstm_test.py\n> <https://github.com/tensorflow/tensorflow/pull/21474#discussion_r298467367>\n> :\n>\n> > +        input_tensors = [(input_tensor.name.split(\":\")[0],\n> +                          input_tensor.get_shape(), input_tensor.dtype)\n> +                         for input_tensor in inputs]\n> +        output_tensors = [normalize_output_name(out.name) for out in outputs]\n> +\n> +        graph_def = freeze_graph(\n> +            sess,\n> +            tf.global_variables() + inputs + outputs) if FLAGS.use_frozen_graph else sess.graph_def\n> +        model_binary = toco_convert(\n> +            graph_def.SerializeToString(), input_tensors, output_tensors)\n> +\n> +        # Check if `LstmCell` is contained\n> +        print(\"========================\")\n> +        if 'label=\\\"LstmCell\\\"' in model_binary.decode('utf-8'):\n> +          print(\"Found 'LstmCell'\")\n> +        else:\n>\n> It's not always safe to decode the model to UTF8.\n> Doesn't the generated example test cover this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21474?email_source=notifications&email_token=ABXDSS4LSMWKYOM5MZAJAULP4WWRFA5CNFSM4FOOPBCKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOB46CWIY#pullrequestreview-255601443>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABXDSSZMNXVBXWKCRFGSFD3P4WWRFANCNFSM4FOOPBCA>\n> .\n>\n", "Can one of the admins verify this patch?", "Hey guys, it's best if someone else reviews this. It seems that it's fine\nin principle. @yifeif do you want to take a crack at it?\n\nOn Tue, Aug 20, 2019 at 6:30 AM Mellanox Build Bot <notifications@github.com>\nwrote:\n\n> Can one of the admins verify this patch?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21474?email_source=notifications&email_token=AE75E3MTTGJ3BX56DFZPSC3QFPWXNA5CNFSM4FOOPBCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4WIELQ#issuecomment-523010606>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE75E3MR2RJGSFTHJQGFVCTQFPWXNANCNFSM4FOOPBCA>\n> .\n>\n", "@drpngx I think we're waiting for the author to address the comments. ", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 21473, "title": "Do not generate LSTM op by incorrect param check logic", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: x\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.10\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.16.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: 9.2\r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n**Related to input param of state forget gate activation**\r\n\r\nI made a simple RNN test code with below 2 lines. I attached the full code as a file. [rnn.py.txt](https://github.com/tensorflow/tensorflow/files/2269704/rnn.py.txt)\r\n```\r\nlstm_layer=tf.nn.rnn_cell.BasicLSTMCell(num_units)\r\noutputs,_=tf.nn.static_rnn(lstm_layer,input,dtype=tf.float32)\r\n```\r\nAnd I tried to generate tflite file from frozen pb via toco. But it failed to generate LSTM operation because `state_forget_sig`(State forget gate activation) did not have `Split` as an input parameter. Instead it had a `Add` operation as an input and the `Add` operation had a `Split` operation. It generates the same graph when I tried to use `dynamic_rnn`. I attached the tensorboard image. \r\n\r\n![state_forget_sig](https://user-images.githubusercontent.com/7223627/43870358-09d57a3c-9bb2-11e8-9d7d-2f6dcd8dd5ab.png)\r\n\r\nI fixed some codes and it succesfully generated LSTM operations.\r\n```\r\nNumber of all operator types: 5\r\n\tEXPAND_DIMS                           :    2 \t (instrs: ???)\r\n\tCONCATENATION                         :    2 \t (instrs: 0)\r\n\tCUSTOM(Fill)                          :    2 \t (instrs: ???)\r\n\tLSTM                                  :   28 \t (instrs: ???)\r\n\tFULLY_CONNECTED                       :    1 \t (instrs: 7,307,045,943,451,530,714)\r\nNumber of all operators                       :   35 \t (total instrs: 7,307,045,943,451,530,714)\r\n```\r\n\r\nIf I understand it correctly, I will make a PR soon.\r\n", "comments": ["> ... I attached the tensorboard image. state_forget_sig\r\n\r\n@jyoungyun , the attached image was broken. Would you please update it?", "@lemmaa Thanks for letting me know. I update image now. ", "@drpngx I couldn't find a good way to share a test for this issue, so for now I shared a test scenario here with test file. ([rnn_optimized.pb.txt](https://github.com/tensorflow/tensorflow/files/2304231/rnn_optimized.pb.txt), this is a pb file so please remove the txt extension before using it.)\r\n\r\nAnd command is :\r\n```\r\nbazel run tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=\"$HOME/git/ml/test/rnn_mnist/save/rnn_optimized.pb\" \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=\"$HOME/git/ml/test/rnn_mnist/save/rnn_optimized.tflite\" \\\r\n--input_arrays=\"input_tensor\" \\\r\n--input_shapes=\"1,28\" \\\r\n--output_arrays=\"output\" \\\r\n--allow_custom_ops \\\r\n--toco_compatible=True\r\n```\r\n\r\nBefore applying my changes(#21474), the generated tflite file does not have a `LSTM` operator.\r\n```\r\nNumber of all operator types: 7\r\n\tCONCATENATION                         :   28 \t (instrs: 0)\r\n\tFULLY_CONNECTED                       :   29 \t (instrs: 90,520,366,340,588,690,046)\r\n\tSPLIT                                 :   28 \t (instrs: ???)\r\n\tLOGISTIC                              :   83 \t (instrs: ???)\r\n\tTANH                                  :   56 \t (instrs: ???)\r\n\tMUL                                   :   83 \t (instrs: ???)\r\n\tADD                                   :   55 \t (instrs: ???)\r\nNumber of all operators                       :  362 \t (total instrs: 90,520,366,340,588,690,046)\r\n```\r\nHowever, when it tests with my changing, it makes a `LSTM` operator successfully.\r\n```\r\nNumber of all operator types: 8\r\n\tCONCATENATION                         :    1 \t (instrs: 0)\r\n\tFULLY_CONNECTED                       :    2 \t (instrs: 7,307,046,758,316,309,978)\r\n\tSPLIT                                 :    1 \t (instrs: ???)\r\n\tLOGISTIC                              :    2 \t (instrs: ???)\r\n\tTANH                                  :    2 \t (instrs: ???)\r\n\tMUL                                   :    2 \t (instrs: ???)\r\n\tADD                                   :    1 \t (instrs: ???)\r\n\tLSTM                                  :   27 \t (instrs: ???)\r\nNumber of all operators                       :   38 \t (total instrs: 7,307,046,758,316,309,978)\r\n```\r\n", "So, here are your options, in decreasing order of preference:\r\n\r\n1. Have a small `C++` test that generates the binary, run the conversion, and checks that the output does indeed have an LSTM.\r\n2. Same, but where the binary is checked in into `testdata`, with clear instructions of how to re-create it manually.\r\n3. Check in the binary pb, and run the toco converter in an `sh_test`. Do this only in the last resort.", "@drpngx Thanks for guideline. I will add a test soon.", "@jyoungyun , this is really awesome. Thanks for working on this.", "I will update the test case for this patch asap. Thesedays I was too busy to implement test case. Sorry and I will update it soon.", "I added lstm_test.py file into tensorflow/contrib/lite/testing directoy. This file will check whether LSTM operator is generated or not.", "We're working on some usability updates for LSTMs. Stay tuned."]}, {"number": 21472, "title": "RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7", "body": "**Environment:**\r\n-  MacOS (High Sierra)\r\n-  Python 3.7\r\n-  Tensorflow 1.9.0\r\n\r\nAfter I've imported tensorflow, I've got warning message link this\r\n`RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7`\r\n\r\nAnd I've tried to run demo.py from this repo https://github.com/mystic123/tensorflow-yolo-v3\r\nstill got error message\r\n `SystemError: <built-in function AppendFloat32ArrayToTensorProto> returned NULL without setting an error`\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "**OS Platform**\r\n\r\n- MacOS (High Sierra) Version 10.13.4\r\n\r\n**TensorFlow installed from**\r\n\r\n-  pip install tensorflow  \r\nEdit: Update correct info.\r\n\r\n**TensorFlow version**\r\n\r\n- 1.9.0 \r\nEdit: Update correct info.\r\n\r\n**Bazel version**\r\n\r\n- N/A (I don't know how to check this).\r\n\r\n**CUDA/cuDNN version**\r\n\r\n- Not installed because I use CPU version\r\n\r\n**GPU model and memory**\r\n\r\n- GPU Intel Iris Plus Graphics 640 1536 MB\r\n- Memory 8 GB 2133 MHz LPDDR3\r\n- *Additional CPU 2.3 GHz Intel Core i5\r\n\r\n**Exact command to reproduce**\r\n\r\n- I don't understand this question, but I just install via pip and run python script and fail\r\n(following demo test in tensorflow website is fine).\r\n\r\n**Mobile device**\r\n\r\n- No\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Can you try installing a non-custom build of TF from our repo and see if you still get the problem? I suspect it's a problem in the build provided by https://github.com/lakshayg/tensorflow-build.", "Sorry for my mistake info, I've installed via pip.", "Can you try the latest release, 1.10?", "@av8ramit this looks to be the same problem as #14182", "We do not have a native binary for python3.7 yet. My recommendation until we have python3.7 support is that you [build from sources](https://www.tensorflow.org/install/install_sources) or use python3.5 or 3.6.", "@av8ramit I got it, I'll try to build from source."]}, {"number": 21471, "title": "N_class from DNN estimator Problem", "body": "`assertion failed: [Label IDs must >= 0]`\r\nI am getting this class again and again and i can not understand why.\r\nPlease tell me what is label and n_class?\r\ni have 3 labels and they are asking me to make n_class greater than labels. When i do it, it does not do anything.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Is it possible to put a range numbers of values under one label?\r\nHow to do that? I have research on internet but i could not find it. Any hint?\r\nlets suppose i want to put 10,000 data under \"+\" operator. Now there are two ways, either i put 10,000 labels to 10,000 values which is a stupid approach or i can put all values under \"+\" operator. How to do that? And these values are totally different. They have no link with each other.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21470, "title": "NCCL is not supported on Windows ", "body": "------------------------\r\n### System information\r\nWindows 10 x64 pro 17314.365, i7 7900X R6E, TTxp (*4), 16G DDR4 3000@2666 (*6) \r\nChannel .\r\nTensorflow 1.9.0 with AVX2 simd, CUDA9.2.148, Cudnn7.1.4, py3.6.6 , VS2017 15.7(with cmake), CP/SM6.1\r\n\r\n### Describe the problem\r\nOp type not registered 'NcclAllReduce' in binary running on Windows10.\r\nWith onedevicestrategy no problems. I noticed that NCCL only works in Linux.\r\n\r\n### Source code / logs\r\ntensorflow/contrib/distribute/python/examples/simple_tfkeras_example.py\r\n\r\n\r\n> `\r\n> from __future__ import absolute_import\r\n> from __future__ import division\r\n> from __future__ import print_function\r\n> \r\n> import sys\r\n> \r\n> import numpy as np\r\n> import tensorflow as tf\r\n> \r\n> \r\n> def input_fn():\r\n>   x = np.random.random((1024, 10))\r\n>   y = np.random.randint(2, size=(1024, 1))\r\n>   x = tf.cast(x, tf.float32)\r\n>   dataset = tf.data.Dataset.from_tensor_slices((x, y))\r\n>   dataset = dataset.repeat(10)\r\n>   dataset = dataset.batch(32)\r\n>   return dataset\r\n> \r\n> \r\n> def main(args):\r\n> \r\n>   model_dir = './'\r\n>   print('Using %s to store checkpoints.' % model_dir)\r\n> \r\n>   # Define tf.keras Model.\r\n>   model = tf.keras.Sequential()\r\n>   model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)))\r\n>   model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\n> \r\n>   # Compile tf.keras Model.\r\n>   optimizer = tf.train.GradientDescentOptimizer(0.2)\r\n>   model.compile(loss='binary_crossentropy', optimizer=optimizer)\r\n>   model.summary()\r\n>   tf.keras.backend.set_learning_phase(True)\r\n> \r\n>   # Define a DistributionStrategy and convert the tf.keras Model to a\r\n>   # tf.Estimator that utilizes the DistributionStrategy.\r\n>   strategy = tf.contrib.distribute.MirroredStrategy()\r\n>   config = tf.estimator.RunConfig(\r\n>       train_distribute=strategy)\r\n>   keras_estimator = tf.keras.estimator.model_to_estimator(\r\n>       keras_model=model, config=config, model_dir=model_dir)\r\n> \r\n>   # Train and evaluate the tf.Estimator.\r\n>   keras_estimator.train(input_fn=input_fn, steps=10)\r\n>   eval_result = keras_estimator.evaluate(input_fn=input_fn)\r\n>   print('Eval result: {}'.format(eval_result))\r\n> \r\n> if __name__ == '__main__':\r\n>   tf.app.run(argv=sys.argv)`\r\n\r\nLogs:\r\n\r\n\r\n> D:\\Anaconda3\\envs\\tensorflow\\python.exe Z:/PyProj/leaf/othertest.py\r\n> Using ./ to store checkpoints.\r\n> _________________________________________________________________\r\n> Layer (type)                 Output Shape              Param #   \r\n> =================================================================\r\n> dense (Dense)                (None, 16)                176       \r\n> _________________________________________________________________\r\n> dense_1 (Dense)              (None, 1)                 17        \r\n> =================================================================\r\n> Total params: 193\r\n> Trainable params: 193\r\n> Non-trainable params: 0\r\n> _________________________________________________________________\r\n> 2018-08-08 16:28:32.965695: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1392] Found device 0 with properties: \r\n> name: TITAN Xp COLLECTORS EDITION major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\n> pciBusID: 0000:17:00.0\r\n> totalMemory: 12.00GiB freeMemory: 9.93GiB\r\n> 2018-08-08 16:28:33.068708: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1392] Found device 1 with properties: \r\n> name: TITAN Xp COLLECTORS EDITION major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\n> pciBusID: 0000:18:00.0\r\n> totalMemory: 12.00GiB freeMemory: 9.93GiB\r\n> 2018-08-08 16:28:33.170068: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1392] Found device 2 with properties: \r\n> name: TITAN Xp COLLECTORS EDITION major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\n> pciBusID: 0000:65:00.0\r\n> totalMemory: 12.00GiB freeMemory: 9.93GiB\r\n> 2018-08-08 16:28:33.280326: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1392] Found device 3 with properties: \r\n> name: TITAN Xp COLLECTORS EDITION major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\n> pciBusID: 0000:b3:00.0\r\n> totalMemory: 12.00GiB freeMemory: 9.93GiB\r\n> 2018-08-08 16:28:33.281257: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3\r\n> 2018-08-08 16:28:35.779554: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2018-08-08 16:28:35.779811: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:958]      0 1 2 3 \r\n> 2018-08-08 16:28:35.779990: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   N N N N \r\n> 2018-08-08 16:28:35.780385: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 1:   N N N N \r\n> 2018-08-08 16:28:35.780639: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 2:   N N N N \r\n> 2018-08-08 16:28:35.783787: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 3:   N N N N \r\n> 2018-08-08 16:28:35.784232: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9612 MB memory) -> physical GPU (device: 0, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:36.220466: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9612 MB memory) -> physical GPU (device: 1, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:18:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:36.655982: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 9612 MB memory) -> physical GPU (device: 2, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.091894: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 9612 MB memory) -> physical GPU (device: 3, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:b3:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.529496: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3\r\n> 2018-08-08 16:28:37.530168: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2018-08-08 16:28:37.530377: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:958]      0 1 2 3 \r\n> 2018-08-08 16:28:37.530527: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   N N N N \r\n> 2018-08-08 16:28:37.530675: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 1:   N N N N \r\n> 2018-08-08 16:28:37.530822: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 2:   N N N N \r\n> 2018-08-08 16:28:37.530971: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 3:   N N N N \r\n> 2018-08-08 16:28:37.531268: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9612 MB memory) -> physical GPU (device: 0, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.531988: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9612 MB memory) -> physical GPU (device: 1, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:18:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.532651: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 9612 MB memory) -> physical GPU (device: 2, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.533329: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 9612 MB memory) -> physical GPU (device: 3, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:b3:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.570852: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3\r\n> 2018-08-08 16:28:37.571420: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2018-08-08 16:28:37.571620: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:958]      0 1 2 3 \r\n> 2018-08-08 16:28:37.571766: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   N N N N \r\n> 2018-08-08 16:28:37.571910: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 1:   N N N N \r\n> 2018-08-08 16:28:37.572056: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 2:   N N N N \r\n> 2018-08-08 16:28:37.572200: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 3:   N N N N \r\n> 2018-08-08 16:28:37.572479: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9612 MB memory) -> physical GPU (device: 0, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.573133: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9612 MB memory) -> physical GPU (device: 1, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:18:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.573824: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 9612 MB memory) -> physical GPU (device: 2, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.574554: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 9612 MB memory) -> physical GPU (device: 3, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:b3:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.902576: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1471] Adding visible gpu devices: 0, 1, 2, 3\r\n> 2018-08-08 16:28:37.903108: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2018-08-08 16:28:37.903311: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:958]      0 1 2 3 \r\n> 2018-08-08 16:28:37.903456: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   N N N N \r\n> 2018-08-08 16:28:37.903604: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 1:   N N N N \r\n> 2018-08-08 16:28:37.903751: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 2:   N N N N \r\n> 2018-08-08 16:28:37.903896: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 3:   N N N N \r\n> 2018-08-08 16:28:37.904189: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 9612 MB memory) -> physical GPU (device: 0, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.904886: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:1 with 9612 MB memory) -> physical GPU (device: 1, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:18:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.905868: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:2 with 9612 MB memory) -> physical GPU (device: 2, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n> 2018-08-08 16:28:37.906462: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:3 with 9612 MB memory) -> physical GPU (device: 3, name: TITAN Xp COLLECTORS EDITION, pci bus id: 0000:b3:00.0, compute capability: 6.1)\r\n> Traceback (most recent call last):\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1589, in _create_c_op\r\n>     c_op = c_api.TF_FinishOperation(op_desc)\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Op type not registered 'NcclAllReduce' in binary running on DESKTOP-K1BEKCL. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'NcclAllReduce'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"Z:/PyProj/leaf/othertest.py\", line 66, in <module>\r\n>     tf.app.run(argv=sys.argv)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n>     _sys.exit(main(argv))\r\n>   File \"Z:/PyProj/leaf/othertest.py\", line 61, in main\r\n>     keras_estimator.train(input_fn=input_fn, steps=10)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 366, in train\r\n>     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1117, in _train_model\r\n>     return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 1160, in _train_model_distributed\r\n>     self.config)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 794, in call_for_each_tower\r\n>     return self._call_for_each_tower(fn, *args, **kwargs)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 269, in _call_for_each_tower\r\n>     coord.join(threads)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 389, in join\r\n>     six.reraise(*self._exc_info_to_raise)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\six.py\", line 693, in reraise\r\n>     raise value\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 297, in stop_on_exception\r\n>     yield\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 263, in _call_for_each_tower\r\n>     self, *merge_args, **merge_kwargs)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 652, in _distributed_apply\r\n>     reduced_grads = distribution.batch_reduce(\"sum\", grads_and_vars)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\distribute.py\", line 840, in batch_reduce\r\n>     return self._batch_reduce(method_string, value_destination_pairs)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\mirrored_strategy.py\", line 310, in _batch_reduce\r\n>     value_destination_pairs)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\cross_tower_ops.py\", line 177, in batch_reduce\r\n>     return self._batch_reduce(method_string, value_destination_pairs)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\cross_tower_ops.py\", line 475, in _batch_reduce\r\n>     [v[0] for v in value_destination_pairs])\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\cross_tower_ops.py\", line 520, in _batch_all_reduce\r\n>     device_grad_packs)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\distribute\\python\\cross_tower_utils.py\", line 37, in aggregate_gradients_using_nccl\r\n>     agg_grads = nccl.all_sum(single_grads)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\nccl\\python\\ops\\nccl_ops.py\", line 47, in all_sum\r\n>     return _apply_all_reduce('sum', tensors)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\nccl\\python\\ops\\nccl_ops.py\", line 228, in _apply_all_reduce\r\n>     shared_name=shared_name))\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\nccl\\ops\\gen_nccl_ops.py\", line 58, in nccl_all_reduce\r\n>     num_devices=num_devices, shared_name=shared_name, name=name)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n>     op_def=op_def)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\r\n>     op_def=op_def)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1756, in __init__\r\n>     control_input_ops)\r\n>   File \"D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1592, in _create_c_op\r\n>     raise ValueError(str(e))\r\n> ValueError: Op type not registered 'NcclAllReduce' in binary running on DESKTO Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'NcclAllReduce'\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code\r\nYes\r\nOS Platform and Distribution\r\nWindows 10 x64 pro 17314.365, i7 7900X R6E\r\nChannel .\r\nTensorFlow installed from Self Compiled,TensorFlow version\r\nTensorflow 1.9.0 with AVX2 simd, \r\nBazel version\r\nWith Cmake\r\nCUDA/cuDNN version\r\nCUDA9.2.148, Cudnn7.1.4, py3.6.6 , VS2017 15.7(with cmake), CP/SM6.1\r\nGPU model and memory\r\nTTxp (*4), 16G DDR4 3000@2666 (*6)\r\n\r\n", "@tfboyd - do you know what needs to be done for getting nccl to run with windows?", "I am not aware of a nccl binary for Windows from NVIDIA.  That does not\nmean it does not exist.  You can use hierarchical copy and get the same or\nvery similar performance.\n\nOn Mon, Sep 17, 2018, 7:07 PM guptapriya <notifications@github.com> wrote:\n\n> @tfboyd <https://github.com/tfboyd> - do you know what needs to be done\n> for getting nccl to run with windows?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21470#issuecomment-422228329>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZestJVbBYt62pOmvE3XZS1veFTYd9Tks5ucFVbgaJpZM4Vzgrp>\n> .\n>\n", "@Windaway nccl is only useful if there are GPU to GPU connections available in your setup. Is that the case? If yes, you could try finding a nccl binary for Windows.\r\n\r\nIf not, then it would be better to try some of the non nccl options. To get those, try the following:\r\n\r\nOption 1:\r\nTry using hierarchical copy. \r\n```\r\ncross_tower_ops = tf.contrib.distribute.AllReduceCrossTowerOps(\r\n    'hierarchical_copy', num_packs=num_gpus))\r\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n```\r\n\r\nOption 2: \r\nReduce to first GPU:\r\n```\r\ncross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps()\r\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n```\r\n\r\nOption 3: \r\nReduce to CPU:\r\n```\r\ncross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps(\r\n    reduce_to_device=\"/device:CPU:0\")\r\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n```\r\n\r\nYou will have to try out the 2 approaches and see which one works and gives the best performance for your use case. \r\n\r\n@yuefengz - for use cases like this, perhaps we should detect if nccl is not available, give a warning, and default to something else that will work for sure? \r\n", "This echoes what I found when using CPU-only binary on *NIX (#22274), on which NCCL is not available as well.", "Does this work? https://github.com/tbennun/nccl-windows", "This seems more recent ? https://github.com/MyCaffe/NCCL", "> @Windaway nccl is only useful if there are GPU to GPU connections available in your setup. Is that the case? If yes, you could try finding a nccl binary for Windows.\r\n> \r\n> If not, then it would be better to try some of the non nccl options. To get those, try the following:\r\n> \r\n> Option 1:\r\n> Try using hierarchical copy.\r\n> \r\n> ```\r\n> cross_tower_ops = tf.contrib.distribute.AllReduceCrossTowerOps(\r\n>     'hierarchical_copy', num_packs=num_gpus))\r\n> strategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n> ```\r\n> \r\n> Option 2:\r\n> Reduce to first GPU:\r\n> \r\n> ```\r\n> cross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps()\r\n> strategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n> ```\r\n> \r\n> Option 3:\r\n> Reduce to CPU:\r\n> \r\n> ```\r\n> cross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps(\r\n>     reduce_to_device=\"/device:CPU:0\")\r\n> strategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n> ```\r\n> \r\n> You will have to try out the 2 approaches and see which one works and gives the best performance for your use case.\r\n> \r\n> @yuefengz - for use cases like this, perhaps we should detect if nccl is not available, give a warning, and default to something else that will work for sure?\r\n\r\nThe API for the first solution has changed from `AllReduceCrossTowerOps` to `AllReduceCrossDeviceOps`. Hope it help you guys :)", "Any update on this thread. For now, I have to use 'hierarchical copy' as reduce algorithm in place of 'nccl' in tf 1.14 CUDA 10. Can't find official binaries for nccl windows 10.\r\n\r\n\r\n```\r\ncross_tower_ops = tf.contrib.distribute.AllReduceCrossDeviceOps(\r\n    'hierarchical_copy', num_packs=num_gpus))\r\nstrategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n```", "> @Windaway nccl is only useful if there are GPU to GPU connections available in your setup. Is that the case? If yes, you could try finding a nccl binary for Windows.\r\n> \r\n> If not, then it would be better to try some of the non nccl options. To get those, try the following:\r\n> \r\n> Option 1:\r\n> Try using hierarchical copy.\r\n> \r\n> ```\r\n> cross_tower_ops = tf.contrib.distribute.AllReduceCrossTowerOps(\r\n>     'hierarchical_copy', num_packs=num_gpus))\r\n> strategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n> ```\r\n> \r\n> Option 2:\r\n> Reduce to first GPU:\r\n> \r\n> ```\r\n> cross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps()\r\n> strategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n> ```\r\n> \r\n> Option 3:\r\n> Reduce to CPU:\r\n> \r\n> ```\r\n> cross_tower_ops = tf.contrib.distribute. ReductionToOneDeviceCrossTowerOps(\r\n>     reduce_to_device=\"/device:CPU:0\")\r\n> strategy = tf.contrib.distribute.MirroredStrategy(cross_tower_ops=cross_tower_ops)\r\n> ```\r\n> \r\n> You will have to try out the 2 approaches and see which one works and gives the best performance for your use case.\r\n> \r\n> @yuefengz - for use cases like this, perhaps we should detect if nccl is not available, give a warning, and default to something else that will work for sure?\r\n\r\nIn TensorFlow 2, option 1 should be used as follow:\r\n```\r\ncross_tower_ops = tf.distribute.HierarchicalCopyAllReduce(num_packs=num_gpus)\r\nstrategy = tf.distribute.MirroredStrategy(cross_device_ops=cross_tower_ops)\r\n```", "You can follow the installations from https://github.com/MyCaffe/NCCL/blob/master/INSTALL.md and test it on your Windows machine with  [NcclAllReduce](https://www.tensorflow.org/api_docs/python/tf/distribute/NcclAllReduce) which is basically a inherited class from [CrossDeviceOps](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps).\r\n\r\nFeel free to reopen the issue, if the issue still persists.Thank You."]}, {"number": 21469, "title": "ci.tensorflow.org is now deprecated.  Where can I get nightly builds for pi-zero AND regular pis when I only have a pi3?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Raspbian Stretch 9\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:Raspberry Pi 3 Model B\r\n- **TensorFlow installed from (source or binary)**:latest pi-zero nightly binary that was on jenkins.\r\n- **TensorFlow version (use command below)**:1.9.0 (ultimately not relevant so this should be good enough)\r\n- **Python version**:3.5.3\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:VideoCore IV\r\n- **Exact command to reproduce**:\r\npip3 install tf-nightly\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nNow, that ci.tensorflow.org is gone.  I'm struggling to find an easy way to view changelogs between nightly builds AND download both pi-zero and regular raspberry pi nightly builds.  The reason why this is useful is because recently #21198 armv7 optimizations interferred with the tflite interpreter.  The current most practical work around was to go to ci.tensorflow.org and grab the pi-zero binary instead of the default one you get when installing tf-nightly or tensorflow on the raspberry pi.  I can see how this wouldn't be a problem for other tensorflow versions that only have one binary that works for only that specific machine.  I understand that you can use the 2.7 tensorflow vs 3.n tensorflow on other machines, but those you can do `pip install tensorflow` or `pip3 install tensorflow`.  This isn't possible with the raspberry pi as far as I can tell, or at least it's not documented clearly anywhere that I can.  Furthermore, it was great being able to roll back a couple of days and just download the .whl from yesterday with ease.  I saved many hours by just playing around with the available .whl's instead of having to cross-compile each one.  \r\ntl;dr I miss ci.tensorflow.org.  Please let me choose whether to install the tf nightly binaries of either pi-zero or regular raspberry-pi by choice since they both work on the raspberry pi.\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This should be easy now with the latest effort to make raspberry pi officially supported?\r\nDoes this address your needs? (not sure if it makes it easy to get the nightly?)\r\nhttps://medium.com/tensorflow/tensorflow-1-9-officially-supports-the-raspberry-pi-b91669b0aa0\r\n\r\n", "No, there are two ways to compile for Raspberry Pi Non-zeroes (I'm talking 2,3,3b,3b+ etc): \r\n\r\n1. tensorflow/tools/ci_build/ci_build.sh PI tensorflow/tools/ci_build/pi/build_raspberry_pi.sh PI_ONE\r\n2. CI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\" \\\r\ntensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n\r\nThese commands are taken directly from [the official tensorflow on raspbian webpage](https://www.tensorflow.org/install/install_raspbian).  The 2nd one has armv7 optimizations that currently break tflite's python interpreter_wrapper.  The 1st one which is what I believe ci.tensorflow.org used to compile for their nightly pi-zero build does not have those optimizations and thus can use tflite's interpreter.  Let's be honest, there really is no point to pip install tensorflow on the raspberry pi other than to use tflite.  Regular tensorflow is just way too bulky; you're better off just running it over wifi.  \r\ntl;dr: ci.tensorflow.org offered two DIFFERENT binaries for raspberry pis: one without optimizations and one with.  Currently, optimizations do not work.  I could grab no optimization version as a prebuilt binary off of ci.tensorflow.org.  ci.tensorflow.org is down.  pip and github don't give me clear ways to pick and choose the binaries I need.  \r\n\r\nps.  I lost Jenkins once, I can't lose him a second time :crying_cat_face: my heart can't take it.", "@gragundier if TF Lite related stuff compiled with optimization flags is what you need, I think something like\r\n```\r\nbazel build --config opt \\\r\n--local_resources 1024.0,0.5,0.5 \\\r\n--copt=-mfpu=neon-vfpv4 \\\r\n--copt=-ftree-vectorize \\\r\n--copt=-funsafe-math-optimizations \\\r\n--copt=-ftree-loop-vectorize \\\r\n--copt=-fomit-frame-pointer \\\r\n--copt=-DRASPBERRY_PI \\\r\n--host_copt=-DRASPBERRY_PI \\\r\n//tensorflow/contrib/lite/tools/benchmark:benchmark_model\r\n```\r\non your Pi devices works. Building the whole TensorFlow pip package natively on Pi devices is painful. But building only TF Lite related code should be fine.", "@freedomtan \r\nI noticed that you changed from cxxopt to copt is there a difference?", "@gragundier I guess NO for TF Lite code. When building the pip package of TF natively on RPI 3, it occurred to me that there is non-C++ code.", "The changes are about to be pushed to github.\r\nWe will have these under the README in the repository root.", "Hello, could anyone please provide the current URL for TF nightly builds, unrelated to raspberry PI?\r\nThank you!", "Please see the README.md page at the repository root, as recommended above.", "thank you!"]}, {"number": 21468, "title": "typeerror", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n"]}, {"number": 21467, "title": "FP16 Batch Matmul still does not run", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Slightly.\r\n\r\nOS Platform and Distribution : centos6.3\r\n\r\nTensorFlow installed from: conda\r\n\r\nTensorFlow version (use command below): v1.8.0(it already has the code TF_CALL_half(REGISTER_BATCH_MATMUL_GPU);)\r\n\r\nPython version: 3.6\r\n\r\nCUDA/cuDNN version: 9.2\r\n\r\nGPU model and memory: V100 \r\n\r\nExact command to reproduce:\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    a = tf.random_normal(dtype=tf.float16, shape=[5, 2, 3], name='a')\r\n    b = tf.random_normal(dtype=tf.float16, shape=[5, 3, 2], name='b')\r\n    c = tf.matmul(a, b)\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=False))\r\nprint(sess.run(c).shape)\r\n\r\n\r\nthe problem:\r\n\r\nSource code / logs\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'MatMul_1': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n\r\n         [[Node: MatMul_1 = BatchMatMul[T=DT_HALF, adj_x=false, adj_y=false, _device=\"/devic\r\n\r\n\r\n\r\n", "comments": ["I meet the same problem, with code:\r\n```\r\nimport tensorflow as tf\r\na = tf.random_normal(dtype=tf.float16, shape=[5, 2, 3], name='a')\r\nb = tf.random_normal(dtype=tf.float16, shape=[5, 3, 2], name='b')\r\nc = tf.matmul(a, b)\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=False))\r\nprint(sess.run(c).shape)\r\ntf.__version__\r\n```\r\n\r\n\r\n, tf 1.8.0 shows:\r\n```\r\n>>> print(sess.run(c).shape)\r\nb/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832338: I tensorflow/core/common_runtime/placer.cc:886] b/RandomStandardNormal: (RandomStandardNormal)/job:localhost/replica:0/task:0/device:GPU:0\r\nb/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832374: I tensorflow/core/common_runtime/placer.cc:886] b/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\r\nb: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832389: I tensorflow/core/common_runtime/placer.cc:886] b: (Add)/job:localhost/replica:0/task:0/device:GPU:0\r\na/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832404: I tensorflow/core/common_runtime/placer.cc:886] a/RandomStandardNormal: (RandomStandardNormal)/job:localhost/replica:0/task:0/device:GPU:0\r\na/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832417: I tensorflow/core/common_runtime/placer.cc:886] a/mul: (Mul)/job:localhost/replica:0/task:0/device:GPU:0\r\na: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832431: I tensorflow/core/common_runtime/placer.cc:886] a: (Add)/job:localhost/replica:0/task:0/device:GPU:0\r\nMatMul: (BatchMatMul): /job:localhost/replica:0/task:0/device:CPU:0\r\n2018-08-08 11:36:17.832439: I tensorflow/core/common_runtime/placer.cc:886] MatMul: (BatchMatMul)/job:localhost/replica:0/task:0/device:CPU:0\r\nb/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832454: I tensorflow/core/common_runtime/placer.cc:886] b/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nb/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832468: I tensorflow/core/common_runtime/placer.cc:886] b/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\nb/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832482: I tensorflow/core/common_runtime/placer.cc:886] b/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\na/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832495: I tensorflow/core/common_runtime/placer.cc:886] a/stddev: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\na/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832509: I tensorflow/core/common_runtime/placer.cc:886] a/mean: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\na/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2018-08-08 11:36:17.832522: I tensorflow/core/common_runtime/placer.cc:886] a/shape: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n(5, 2, 2)\r\n```\r\n\r\nNote This Line:\r\n\r\n**2018-08-08 11:36:17.832439: I tensorflow/core/common_runtime/placer.cc:886] MatMul: (BatchMatMul)/job:localhost/replica:0/task:0/device:CPU:0**\r\n\r\nMatrix of fp16 does multiplication on CPU.\r\n\r\n\r\n\r\nEnvironment:\r\npython 2.7\r\ntf 1.8.0 installed from pip (pip install tensorflow-gpu==1.8.0 -t $PYTHONHOME/lib/python2.7/)\r\nV100,  driver 396.26\r\ncuda9.2,  cudnn7  \r\n", "This issue seems to be the same as #21226 and there is a on going PR #21231 to address it.", "Nagging Assignee @robieta: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Duplicate of #21226 "]}, {"number": 21466, "title": "[Intel MKL] Adding Quantized Convolution (Int8) - Part 4", "body": "This PR is part 4 of four PRs that add quantized version of convolution using MKL-DNN. This part adds the registration of MKL quantized and fused operations. \r\n\r\nPlease note that the below PRs replace this one #19425 \r\nPart 1: #21483 \r\nPart 2: #21456 \r\nPart 3: #21465 \r\nPart 4: #21466 ", "comments": ["Nagging Reviewer @tatianashp, @raghuraman-k: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 51 days with no activity and the `awaiting review` label has been applied.", "@mahmoud-abuzaina could you look into the failures?", "@drpngx Looks like failures are not related to linux CPU. Could you please point me which failure should we look into?", "Can you see the invocation log [here](https://source.cloud.google.com/results/invocations/2fb64cb0-c711-4f79-9fd6-e859d68752f5/targets)? It says \"broken\", which means that the `C++` failed to build.", "@drpngx There is no \"--config=mkl\" in this test invocation command and all the codes in this PR is wrapped ifdef INTEL_MKL\r\n\r\n> bazel\u00a0test\u00a0--config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_lang_filters=py --local_test_jobs=32 --remote_accept_cached=true --spawn_strategy=standalone --remote_local_fallback=false --remote_timeout=600 --strategy=Javac=standalone --strategy=Closure=standalone --genrule_strategy=standalone --copt=-DPOISON_CACHE=TRUE --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute --auth_enabled=true --auth_credentials=/tmpfs/src/keystore/71384_tensorflow-testing-service-account --auth_scope=https://www.googleapis.com/auth/cloud-source-tools --tls_enabled=true --bes_backend=buildeventservice.googleapis.com --bes_best_effort=false --bes_timeout=600s --project_id=tensorflow-testing --remote_cache=remotebuildexecution.googleapis.com --remote_instance_name=projects/tensorflow-testing/instances/default_instance --remote_timeout=3600\u00a0--\u00a0//tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/...\r\n\r\n\r\n\r\n", "OK, maybe it's unrelated. Trying again.", "@drpngx I think this failure (MacOS Contrib) too is not related to our contribution.", "@drpngx @tatianashp I don't think failures related to our contribution. Could you please let us know.", "Could you run clang-format? I think we use `3.6.0`. Thanks.", "@drpngx Ran clang-format 3.6.0\r\n", "@drpngx It looks like the tests are passing, can you initiate the merging?", "@drpngx We don't have access to ```feedback/copybara``` issue", "It's running. Stay tuned.", "@drpngx Any update on this PR?", "Waiting for internal approval...", "@drpngx  Sorry to keep bugging you. Any updates on the results of the internal testing?", "@tatianashp could you approve internally?", "@tatianashp @drpngx Any update."]}, {"number": 21465, "title": "[Intel MKL] Adding Quantized Convolution (Int8) - Part 3", "body": "This PR is part 3 of four PRs that add quantized version of convolution using MKL-DNN. This part adds changes that are required to quantize a graph and it enables fusions of MKL quantized convolution. \r\n\r\nPlease note that the below PRs replace this one #19425 \r\nPart 1: #21483 \r\nPart 2: #21456 \r\nPart 3: #21465 \r\nPart 4: #21466 ", "comments": ["\r\n\r\n\u5728 2018\u5e748\u67088\u65e5\uff0c08:28\uff0cMahmoud Abuzaina <notifications@github.com<mailto:notifications@github.com>> \u5199\u9053\uff1a\r\n\r\n\r\nThis PR is part 3 of four PRs that add quantized version of convolution using MKL-DNN. This part adds changes that are required to quantize a graph and it enables fusions of MKL quantized convolution.\r\n\r\n________________________________\r\nYou can view, comment on, or merge this pull request online at:\r\n\r\n  https://github.com/tensorflow/tensorflow/pull/21465\r\n\r\nCommit Summary\r\n\r\n  *   Adding int8 conv part3\r\n\r\nFile Changes\r\n\r\n  *   M tensorflow/tools/graph_transforms/BUILD<https://github.com/tensorflow/tensorflow/pull/21465/files#diff-0> (1)\r\n  *   A tensorflow/tools/graph_transforms/fuse_quantized_convolution.cc<https://github.com/tensorflow/tensorflow/pull/21465/files#diff-1> (220)\r\n  *   M tensorflow/tools/quantization/quantize_graph.py<https://github.com/tensorflow/tensorflow/pull/21465/files#diff-2> (348)\r\n\r\nPatch Links:\r\n\r\n  *   https://github.com/tensorflow/tensorflow/pull/21465.patch\r\n  *   https://github.com/tensorflow/tensorflow/pull/21465.diff\r\n\r\n\u2014\r\nYou are receiving this because you are subscribed to this thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/21465>, or mute the thread<https://github.com/notifications/unsubscribe-auth/ARHDlNRwr6aBck0dOv5vV2agM9YqImu7ks5uOjCzgaJpZM4VzFan>.\r\n", "Nagging Reviewer @tatianashp, @raghuraman-k: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 50 days with no activity and the `awaiting review` label has been applied.", "@raghuraman-k @drpngx removed quantize_graph.py", "@drpngx Tests looks fine. Ready to go?", "@suharshs Can you take a look? Is this PR still relevant?", "@mahmoud-abuzaina please return an error (invalid argument) for when the tensor parsing fails and not implemented if the dtype is wrong:\r\n```\r\nCHECK(float_tensor.FromProto(float_tensor_proto));\r\nCHECK_EQ(float_tensor.dtype(), DT_FLOAT);\r\n```\r\nWe do not allow `CHECK` in new code.", "@drpngx Removed CHECK and CHECK_EQ, added errors::InvalidArgument and errors::Unimplemented. Could you please check?"]}, {"number": 21464, "title": "StridedSlice (OpKernel was found, but attributes didn't match)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Docker ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary - docker\r\n- **TensorFlow version (use command below)**: nightly version as of August 6th 2018\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:   V9.0.176\r\n- **GPU model and memory**: 4 * GTX 1080 Ti \r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI have written code to train a boosted trees estimator. The code gives an error even before it starts training if run on the GPU. On the other hand, it runs without any issues on the CPU. \r\n\r\nThe issue seems to be with the StridedSlice op and an incompatible op. \r\n\r\nI have attached the log of the error below. \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py:993: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nShapes are always computed; don't use the compute_shapes as it has no effect.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nINFO:tensorflow:Saving checkpoints for 0 into GBDT_multi/GPU/vLat-vLon/model.ckpt.\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1291     try:\r\n-> 1292       return fn(*args)\r\n   1293     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1276       return self._call_tf_sessionrun(\r\n-> 1277           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1278 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1366         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1367         run_metadata)\r\n   1368 \r\n\r\nNotFoundError: No registered 'StridedSlice' OpKernel for GPU devices compatible with node {{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_BOOL, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](is_active, strided_slice/stack, strided_slice/stack_1, strided_slice/stack)\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_VARIANT]\r\n  device='CPU'; T in [DT_RESOURCE]\r\n  device='CPU'; T in [DT_STRING]\r\n  device='CPU'; T in [DT_BOOL]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n\r\n\t [[{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_BOOL, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](is_active, strided_slice/stack, strided_slice/stack_1, strided_slice/stack)]]\r\n\t [[{{node head/historyyawDiff5/dense_make_stats_update_pTOiIS60uTM}} = dense_make_stats_update_pTOiIS60uTM_specialized_for_head_currentspeed1_dense_make_stats_update_pTOiIS60uTM[_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ConstantFolding/head/unstack_13-folded-0/_377, head/unstack_11/_379, gbdt/transform_features_13/history_yawDiff_5/ExpandDims, head/split:25, gbdt_1/GradientTreesPartitionExamples/_381, head/Gradients/head/mean_squared_error/SquaredDifference_grad/Reshape, head/stack, head/Sum)]]\r\n\t [[{{node head/cond_1/cond/split/_1463}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3016_head/cond_1/cond/split\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-18-5992a8bb32bf> in <module>()\r\n     13             }\r\n     14 \r\n---> 15 _ = train_GBDT_regressor()\r\n\r\n<ipython-input-15-c864123abd09> in train_GBDT_regressor()\r\n    128         learn_runner.run(experiment_fn=create_gbdt_experiment, \r\n    129                          output_dir=TRAIN_DATA['model_dir'],\r\n--> 130                          schedule=\"train_and_evaluate\"\r\n    131                         )      \r\n    132 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    304               'in a future version' if date is None else ('after %s' % date),\r\n    305               instructions)\r\n--> 306       return func(*args, **kwargs)\r\n    307     return tf_decorator.make_decorator(\r\n    308         func, new_func, 'deprecated',\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py in run(experiment_fn, output_dir, schedule, run_config, hparams)\r\n    223   schedule = schedule or _get_default_schedule(run_config)\r\n    224 \r\n--> 225   return _execute_schedule(experiment, schedule)\r\n    226 \r\n    227 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/learn_runner.py in _execute_schedule(experiment, schedule)\r\n     50     logging.error('Allowed values for this experiment are: %s', valid_tasks)\r\n     51     raise TypeError('Schedule references non-callable member %s' % schedule)\r\n---> 52   return task()\r\n     53 \r\n     54 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py in train_and_evaluate(self)\r\n    670                   hooks=self._eval_hooks)\r\n    671           ]\r\n--> 672       self.train(delay_secs=0)\r\n    673 \r\n    674     # If the checkpoint_and_export flag and appropriate estimator configuration\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py in train(self, delay_secs)\r\n    387         max_steps=self._train_steps,\r\n    388         hooks=self._train_monitors + extra_hooks,\r\n--> 389         saving_listeners=self._saving_listeners)\r\n    390 \r\n    391   def evaluate(self, delay_secs=None, name=None):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/experiment.py in _call_train(self, _sentinel, input_fn, steps, hooks, max_steps, saving_listeners)\r\n    881           max_steps=max_steps,\r\n    882           hooks=hooks,\r\n--> 883           saving_listeners=saving_listeners)\r\n    884     else:\r\n    885       return self._estimator.fit(\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    341 \r\n    342       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 343       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    344       logging.info('Loss for final step: %s.', loss)\r\n    345       return self\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1127       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1128     else:\r\n-> 1129       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1130 \r\n   1131   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1161       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n   1162                                              hooks, global_step_tensor,\r\n-> 1163                                              saving_listeners)\r\n   1164 \r\n   1165   def _train_model_distributed(self, input_fn, hooks, saving_listeners):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\r\n   1369       loss = None\r\n   1370       while not mon_sess.should_stop():\r\n-> 1371         _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n   1372     return loss\r\n   1373 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    581                           feed_dict=feed_dict,\r\n    582                           options=options,\r\n--> 583                           run_metadata=run_metadata)\r\n    584 \r\n    585   def run_step_fn(self, step_fn):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1057                               feed_dict=feed_dict,\r\n   1058                               options=options,\r\n-> 1059                               run_metadata=run_metadata)\r\n   1060       except _PREEMPTION_ERRORS as e:\r\n   1061         logging.info('An error was raised. This may be due to a preemption in '\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1148         raise six.reraise(*original_exc_info)\r\n   1149       else:\r\n-> 1150         raise six.reraise(*original_exc_info)\r\n   1151 \r\n   1152 \r\n\r\n/usr/local/lib/python3.5/dist-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n   1133   def run(self, *args, **kwargs):\r\n   1134     try:\r\n-> 1135       return self._sess.run(*args, **kwargs)\r\n   1136     except _PREEMPTION_ERRORS:\r\n   1137       raise\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n   1205                                   feed_dict=feed_dict,\r\n   1206                                   options=options,\r\n-> 1207                                   run_metadata=run_metadata)\r\n   1208 \r\n   1209     for hook in self._hooks:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n    985 \r\n    986   def run(self, *args, **kwargs):\r\n--> 987     return self._sess.run(*args, **kwargs)\r\n    988 \r\n    989   def run_step_fn(self, step_fn, raw_session, run_with_hooks):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    885     try:\r\n    886       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 887                          run_metadata_ptr)\r\n    888       if run_metadata:\r\n    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1109       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1110                              feed_dict_tensor, options, run_metadata)\r\n   1111     else:\r\n   1112       results = []\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1284     if handle is None:\r\n   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1286                            run_metadata)\r\n   1287     else:\r\n   1288       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1306           self._config.experimental.client_handles_error_formatting):\r\n   1307         message = error_interpolation.interpolate(message, self._graph)\r\n-> 1308       raise type(e)(node_def, op, message)\r\n   1309 \r\n   1310   def _extend_graph(self):\r\n\r\nNotFoundError: No registered 'StridedSlice' OpKernel for GPU devices compatible with node {{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_BOOL, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](is_active, strided_slice/stack, strided_slice/stack_1, strided_slice/stack)\r\n\t (OpKernel was found, but attributes didn't match)\r\n\t.  Registered:  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_VARIANT]\r\n  device='CPU'; T in [DT_RESOURCE]\r\n  device='CPU'; T in [DT_STRING]\r\n  device='CPU'; T in [DT_BOOL]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n\r\n\t [[{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_BOOL, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](is_active, strided_slice/stack, strided_slice/stack_1, strided_slice/stack)]]\r\n\t [[{{node head/historyyawDiff5/dense_make_stats_update_pTOiIS60uTM}} = dense_make_stats_update_pTOiIS60uTM_specialized_for_head_currentspeed1_dense_make_stats_update_pTOiIS60uTM[_device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ConstantFolding/head/unstack_13-folded-0/_377, head/unstack_11/_379, gbdt/transform_features_13/history_yawDiff_5/ExpandDims, head/split:25, gbdt_1/GradientTreesPartitionExamples/_381, head/Gradients/head/mean_squared_error/SquaredDifference_grad/Reshape, head/stack, head/Sum)]]\r\n\t [[{{node head/cond_1/cond/split/_1463}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3016_head/cond_1/cond/split\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```", "comments": ["@karmel Is there any other information that I should provide to help you locate the issue? ", "Thanks, @amtagrwl , that gives me a place to start. Did you install docker on your machine with nvidia-docker as per the [install instructions](https://www.tensorflow.org/install/install_linux#InstallingDocker)? Which Docker image did you use?", "Yes, docker was installed using nvidia-docker according to the install instructions provided. \r\n\r\nI have been using the **nightly-gpu-py3** image from the tensorflow docker hub for the past 2 weeks. I keep updating my image every few days. Before that, I was using the **1.9.0-gpu-py3** image. The issues persists regardless of the image. ", "Re-reading the error, it looks like the op in question is found, but the data it is receiving is a boolean tensor rather than an int. Further, the warnings above the error indicate that there are other type mismatches encountered. Are you switching between CPU and GPU with a single checkpoint? Alternatively, is there something in your model_fn that might yield this sort of type mismatch?", "@amtagrwl , I just spoke to @tanzhenyu and @nataliaponomareva about Boosted Trees, and it looks like they do not currently work on GPU. Assigning this to @tanzhenyu , as the ops should just be forced onto CPU with a warning for now.", "@karmel @tanzhenyu @nataliaponomareva Thanks for the update. Is GPU support for Boosted Trees in the roadmap? Will GPU training be supported in the next few months? ", "We anticipate not breaking on GPUs in the next few months, but not adding the ops to actually run on GPUs. Can you tell us a little bit more about how you are using the model? Data size and type? Contributions are welcome if you are interested.", "My data consists of around 200k data points with around 25 features each at this point. Right now, all the features are float32. In the future, I am hoping to train with 10s-100s million data points with over 100s of features with different types such as float32, one-hot vectors, and multi-hot vectors. The model uses a multi-target regression head. I am predicting 20 targets using a single model. Currently I am using a simple MSE loss. In the future, I hope to incorporate custom loss functions to model some domain specific regularization losses. \r\n\r\nI am a beginner at using tensorflow at this point and am only well-versed with the high level APIs. I am hoping to learn more in the coming months. I can attempt to contribute once I have a better understanding of more complex workings of TF. ", "Nagging Assignee @tanzhenyu: It has been 47 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I have created a code review to raise error when users attempts to train Boosted Trees using GPU. Just FYI @amtagrwl contribution welcomed (supporting gpu currently not coming soon).", "@amtagrwl Can you try the latest tf version? It looks like the bug has been resolved.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 21463, "title": "Keras model with SparseTensor as inputs", "body": "I'm trying to setup a Keras model with sparse input:\r\n\r\n```\r\ninput_layer = tf.keras.layers.Input(shape=(10, ), sparse=True)\r\nweights = tf.get_variable(name='weights', shape=(10, 1))\r\n\r\nweights_mult = lambda x: tf.sparse_tensor_dense_matmul(x, weights)\r\noutput_layer=  tf.keras.layers.Lambda(weights_mult)(input_layer)\r\nmodel = tf.keras.Model([input_layer], output_layer)\r\nmodel.compile(loss='binary_crossentropy',\r\n                  optimizer=tf.keras.optimizers.Adam(lr=0.0001),\r\n                  metrics=['accuracy'])\r\n\r\n```\r\nThis fails with \r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-822e68cb2d64> in <module>()\r\n      7 model.compile(loss='binary_crossentropy',\r\n      8                   optimizer=tf.keras.optimizers.Adam(lr=0.0001),\r\n----> 9                   metrics=['accuracy'])\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\r\n    457       # Add regularization penalties\r\n    458       # and other layer-specific losses.\r\n--> 459       for loss_tensor in self.losses:\r\n    460         total_loss += loss_tensor\r\n    461\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in losses(self)\r\n    635       else:\r\n    636         relevant_inputs.append(inputs)\r\n--> 637     reachable = tf_utils.get_reachable_from_inputs(relevant_inputs, losses)\r\n    638     relevant_conditional_losses = [x for x in losses if x in reachable]\r\n    639     unconditional_losses = [\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in get_reachable_from_inputs(inputs, targets)\r\n    115       outputs = [x.op]\r\n    116     else:\r\n--> 117       raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))\r\n    118\r\n    119     for y in outputs:\r\n\r\nTypeError: Expected Operation, Variable, or Tensor, got SparseTensor(indices=Tensor(\"input_1/indices:0\", shape=(?, 2), dtype=int64), values=Tensor(\"input_1/values:0\", shape=(?,), dtype=float32), dense_shape=Tensor(\"input_1/shape:0\", shape=(2,), dtype=int64))\r\n```\r\n\r\nIt seems to me that tensorflow/python/keras/utils/tf_utils.py:get_reachable_from_inputs doesn't recognize SparseTensor as a correct input type. Is there anyway to setup a Keras model with sparse input?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform and Distribution: unix\r\nTensorFlow installed from: pip install \r\nTensorFlow version: 1.9.0\r\nBazel version: NA\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: pasted above\r\nMobile device: NA", "Hi @chtran, thanks for submitting this issue. I was able to recreate the error and have a fix pending, I'll let you know when it is available in the nightly build", "Nagging Assignee @omalleyt12: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@chtran, this fix should be in the latest nightly build, please ```pip install tf-nightly``` (or ```pip install tf-nightly-gpu``` for GPU support) and let me know if your issue is fixed"]}, {"number": 21462, "title": "RNN.call should get initial state from full input spec", "body": "Pick the fix at https://github.com/keras-team/keras/pull/10845 to ```tf.keras```, to fix a critical bug when running ```RNN``` with ```multi_gpu_model```.", "comments": []}, {"number": 21461, "title": "Bazel: use Tensorflow as external dependency", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  1.9.0\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n$ bazel run :hello_world\r\nINFO: Build options have changed, discarding analysis cache.\r\nINFO: Analysed target //:hello_world (29 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //:hello_world up-to-date:\r\n  bazel-bin/hello_world\r\nINFO: Elapsed time: 7.108s, Critical Path: 0.32s\r\nINFO: 1 process, local.\r\nINFO: Build completed successfully, 4 total actions\r\nINFO: Build completed successfully, 4 total actions\r\nTraceback (most recent call last):\r\n  File \"/home/nmerino/.cache/bazel/_bazel_nmerino/daaf1ed794b21b41620d835ce6042e7a/execroot/__main__/bazel-out/k8-fastbuild/bin/hello_world.runfiles/__main__/hello_world.py\", line 1, in <module>\r\n    import tensorflow as tf\r\nImportError: No module named tensorflow\r\n```\r\n### Describe the problem\r\nHey there! I'm trying to import Tensorflow in my current project, which use Bazel as its build tool. For some reason Bazel cannot find Tensorflow even though it is specified in the requirements file. As Bazel is the building tool for Tensorflow, and also gaining popularity in the community I would like if you could help me solving this issue.\r\n_Just in case you are wondering I'm not trying to use Tensorflow to infer stuffs, rather to serialize data to TFRecords._\r\n\r\n### Source code / logs\r\n\r\n`hello_world.py`\r\n```\r\nimport tensorflow as tf\r\n\r\nmsg = tf.constant('Hello World!')\r\n\r\nwith tf.Session() as sess:\r\n\tprint sess.run(msg)\r\n```\r\n`WORKSPACE`\r\n```\r\ngit_repository(\r\n  name = \"io_bazel_rules_python\",\r\n  remote = \"https://github.com/bazelbuild/rules_python.git\",\r\n  commit = \"44711d8ef543f6232aec8445fb5adce9a04767f9\")\r\n\r\nload(\r\n    \"@io_bazel_rules_python//python:pip.bzl\",\r\n    \"pip_repositories\",\r\n    \"pip_import\")\r\npip_repositories()\r\npip_import(\r\n    name = \"pip\",\r\n    requirements = \"//:requirements.txt\")\r\nload(\r\n    \"@pip//:requirements.bzl\",\r\n    pip_install = \"pip_install\")\r\npip_install()\r\n```\r\n`BUILD.bazel`\r\n```\r\npackage(default_visibility = [\"//visibility:public\"])\r\nload(\r\n    \"@pip//:requirements.bzl\",\r\n    \"requirement\")\r\nload(\r\n    \"@io_bazel_rules_python//python:python.bzl\",\r\n    \"py_binary\",\r\n    \"py_library\")\r\n\r\npy_binary(\r\n    name = \"hello_world\",\r\n    srcs = [\r\n        \"hello_world.py\"\r\n    ],\r\n    deps = [\r\n        requirement(\"tensorflow\"),\r\n    ]\r\n)\r\n```\r\n`requirements.txt`\r\n```\r\ntensorflow==1.9.0\r\n```", "comments": ["That's neat, /CC @jonathanasdf .\r\n\r\n@duggelz is there any way to debug this?", "@nmerino just checking, does `python -c 'import tensorflow'` work?", "@drpngx when the virtualenv is active, it does.\r\nI came up with a solution to this issue based on how TF Serving manage TF as an external dependency.\r\n\r\n```\r\n$ bazel build --define=grpc_no_ares=true :hello_world\r\n$ bazel-bin/hello_world\r\n2018-08-18 13:58:37.648246: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX\r\nHello World!\r\n```\r\n### Source code\r\n`hello_world.py`\r\n```\r\nimport tensorflow as tf\r\n\r\nmsg = tf.constant('Hello World!')\r\n\r\nwith tf.Session() as sess:\r\n\tprint sess.run(msg)\r\n```\r\n`WORKSPACE`\r\n```\r\nworkspace(name=\"hello_world\")\r\n\r\nload(\":tf_repo.bzl\", \"tensorflow_http_archive\")\r\n\r\ntensorflow_http_archive(\r\n    name = \"org_tensorflow\",\r\n    sha256 = \"696c4906d6536ed8d9f8f13c4927d3ccf36dcf3e13bb352ab80cba6b1b9038d4\",\r\n    git_commit = \"25c197e02393bd44f50079945409009dd4d434f8\",\t# TF 1.9.0\r\n)\r\n\r\n# TensorFlow depends on \"io_bazel_rules_closure\" so we need this here.\r\n# Needs to be kept in sync with the same target in TensorFlow's WORKSPACE file.\r\nhttp_archive(\r\n    name = \"io_bazel_rules_closure\",\r\n    sha256 = \"a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae\",\r\n    strip_prefix = \"rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1\",\r\n    urls = [\r\n        \"https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz\",\r\n        \"https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz\",  # 2018-04-13\r\n    ],\r\n)\r\n\r\nload(\"@org_tensorflow//tensorflow:workspace.bzl\", \"tf_workspace\")\r\n\r\ntf_workspace(path_prefix = \"\", tf_repo_name = \"org_tensorflow\")\r\n```\r\n`BUILD.bazel`\r\n```\r\npackage(default_visibility = [\"//visibility:public\"])\r\n\r\npy_binary(\r\n    name = \"hello_world\",\r\n    srcs = [\r\n        \"hello_world.py\"\r\n    ],\r\n    deps = [\r\n        \"@org_tensorflow//tensorflow:tensorflow_py\"\r\n    ],\r\n)\r\n```\r\n`tf_repo.bzl`\r\n```\r\n\"\"\" TensorFlow Http Archive\r\nModified http_arhive that allows us to override the TensorFlow commit that is\r\ndownloaded by setting an environment variable. This override is to be used for\r\ntesting purposes.\r\nAdd the following to your Bazel build command in order to override the\r\nTensorFlow revision.\r\nbuild: --action_env TF_REVISION=\"<git commit hash>\"\r\n  * `TF_REVISION`: tensorflow revision override (git commit hash)\r\n\"\"\"\r\n\r\n_TF_REVISION = \"TF_REVISION\"\r\n\r\ndef _tensorflow_http_archive(ctx):\r\n  git_commit = ctx.attr.git_commit\r\n  sha256 = ctx.attr.sha256\r\n\r\n  override_git_commit = ctx.os.environ.get(_TF_REVISION)\r\n  if override_git_commit:\r\n    sha256 = \"\"\r\n    git_commit = override_git_commit\r\n\r\n  strip_prefix = \"tensorflow-%s\" % git_commit\r\n  urls = [\r\n      \"https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/%s.tar.gz\" % git_commit,\r\n      \"https://github.com/tensorflow/tensorflow/archive/%s.tar.gz\" % git_commit,\r\n  ]\r\n  ctx.download_and_extract(\r\n      urls,\r\n      \"\",\r\n      sha256,\r\n      \"\",\r\n      strip_prefix)\r\n\r\ntensorflow_http_archive = repository_rule(\r\n    implementation=_tensorflow_http_archive,\r\n    attrs={\r\n        \"git_commit\": attr.string(mandatory=True),\r\n        \"sha256\": attr.string(mandatory=True),\r\n    })\r\n```\r\nThis solution works, even though I think it would be great to perform the installation via pip, because building TF from source takes time.\r\n\r\nPS: If you try this with TF 1.10.0, it won't work with due to issue https://github.com/tensorflow/tensorflow/issues/21518. \r\n", "Yes, we had this problem with `keras_applications` that is a new dependency, it now works.\r\n\r\nWe are open-sourcing a repo where we use an existing pip-installed binary, relying on `tf.sysconfig.*` to do the autoconf. Stay tuned.", "@drpngx could you update this thread when that repo is open sourced?", "See [`repo.bzl`](https://github.com/tensorflow/lingvo/blob/master/lingvo/repo.bzl), called by [`WORKSPACE`](https://github.com/tensorflow/lingvo/blob/master/WORKSPACE) and additional rules in [`lingvo.bzl`](https://github.com/tensorflow/lingvo/blob/master/lingvo/lingvo.bzl). Hope this helps.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@nmerino  Is this still an issue ?", "@harshini-gadige this is no longer an issue, thank you all.", "> @harshini-gadige this is no longer an issue, thank you all.\r\n\r\n@nmerino Isn't this still an issue? The `tf_repo.bzl` thing is just a workaround.", "Is there a solution to this that doesn't involve building TF from source?", "You can depend on pre-installed tensorflow if you use [custom_op](https://github.com/tensorflow/custom-op) or [lingvo](https://github.com/tensorflow/lingvo). Both use similar techniques to provide a bazel environment where you can depend on a pre-installed tensorflow.", "I'll look at going down that path.  It's unfortunate this approach introduces another dependency management mechanism (ie as opposed to rules_python/rules_pip) and that it couples build results to the local system environment"]}, {"number": 21460, "title": "Keras load weights does not work", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: Quardo M4000\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nI try a simple Keras model. After saving the model weights, I tried to reload it. However, the \"load_weights\" function does not really load the weights, but still use the initializer's setting in the model.\r\n\r\n### Source code / logs\r\n\tdef test_load():\r\n\t  data = np.ones((1,1,4,5), dtype=float)\r\n\t  \r\n\t  model = keras.Sequential()\r\n\t  model.add(Conv2D(5, 1, use_bias=False, kernel_initializer='ones', trainable=False))\r\n\r\n\t  results = model.predict(data)\r\n\t  print(results)\r\n\r\n\t  for layer in model.layers:\r\n\t    weights = layer.get_weights()\r\n\t    layer.set_weights(np.zeros_like(weights))\r\n\r\n\t  results = model.predict(data)\r\n\t  print(results)\r\n\r\n\t  model.save_weights('./temp')\r\n\r\n\t  new_model = keras.Sequential()\r\n\t  new_model.add(Conv2D(5, 1, use_bias=False, kernel_initializer='uniform', trainable=False))\r\n\t  \r\n\t  new_model.load_weights('./temp')\r\n\r\n\t  results = new_model.predict(data)\r\n\t  print(results)\r\n\r\nBefore saving the model, everything works as expected. However, after loading, the loaded weights are always random according to the initializer. I wonder whether there is some bugs in the codes or I used it in a wrong way. Thank you.\r\n", "comments": ["Sorry you ran into this. There was a bug with when restore ops got run when graph building; the correct behavior was to mimic eager execution (run restore ops in the Session as they came in), but they were getting run just once instead (which probably interacts with Sequential building immediately here).\r\n\r\nYour snippet works for me in the 2018-06-28 nightly. The fix was https://github.com/tensorflow/tensorflow/commit/21d94be838f7a7917f072e5ec0bbe6e3593177b9#diff-7b3f8757e2c5c405c3d056aa0a301c8f\r\n\r\nCould you give a nightly a try?", "@allenlavoie Thank you for your response. As I am working on Windows, I found the latest binary for Windows was 0722 version. However, this version still produces the same bug. The loaded weights are still random. In addition, I do not find a 2018-06-28 nightly at https://pypi.org/project/tf-nightly-gpu/#history.", "You're right, 20180802 was the one that worked. 20180628 was broken. I don't expect a regression, so anything from August should work.\r\n\r\nLooks like the only August Windows builds are CPU: https://pypi.org/project/tf-nightly/1.10.0.dev20180802/#files", "Thank you for the clarification. Do you know whether there be any Windows builds for tensorflow GPU version soon? ", "@av8ramit are we planning to do Windows GPU nightly builds?", "@allenlavoie we are in the process of migrating to bazel from cmake. Hopefully that fixes that build.", "@av8ramit Thank you for your update. Once the new build is available, I will test it and report the results here.", "Nagging Assignee @allenlavoie: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We now have Windows tf-nightly-gpu builds.", "I tested the new nightly built version, and the program now works well. But the Tensorflow 1.10 version does not work. ", "Hello @allenlavoie, sorry to bother you again. Would you please help have a look at https://github.com/tensorflow/tensorflow/issues/22062? Could this be another similar bug in Keras save/load? Or it was just my mis-use? Thank you."]}, {"number": 21459, "title": "Changing optimizer of restored network messes up training output", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-130-generic x86_64)\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n\r\n- **TensorFlow version (use command below)**:\r\nv1.4.0-rc1-11-g130a514 1.4.0\r\n\r\n- **Python version**:\r\nPython 3.5.2\r\n\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\nCuda compilation tools, release 8.0, V8.0.61\r\n\r\n- **GPU model and memory**:\r\nGeForce GTX 1080 Ti (11GB)\r\n\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI am trying to train a model to do face detection, using Resnet_v1_50 as a backbone net and restoring it from the checkpoint provided in tf-slim. I am following the paper \"MB-FCN for Face Detection\" in building my network. Initially trained with gradient descent, the model was converging slowly. When the optimizer was changed from Gradient Descent (which ResNet was trained with) to a different optimizer (momentum, adam) there were wildly different outputs, that were nowhere near correct even though the training and validation losses were lower and care was taken to initialize all weights for new variables (from momentum for example). There seems to perhaps be an issue with the tf Saver class (restoring variables goes wrong) or an issue changing the optimizer of a restored network from the one it was originally trained with. I also had this issue when using a different model for text detection (changing optimizer messing with output). Restoring the weights from the fine-tuned model with another optimizer goes wrong.\r\n\r\n### Source code / logs\r\nCode while training from ResNet:\r\n    global_step = tf.train.get_or_create_global_step()\r\n    with slim.arg_scope(resnet_v1.resnet_arg_scope()):\r\n        vars_to_restore = slim.get_variables_to_restore(['resnet_v1_50'])\r\n\r\n    optimizer = tf.train.GradientDescentOptimizer(0.001)\r\n    optimizer = optimizer.minimize(loss, global_step=global_step) \r\n\r\n    config = tf.ConfigProto(allow_soft_placement=True)\r\n    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n    overall_saver = tf.train.Saver(slim.get_variables_to_restore())\r\n\r\n    with tf.Session(config=config) as sess:\r\n        resnet_model_path = 'backbone_net/resnet_v1_50.ckpt'\r\n        vars_to_initialize = []\r\n        all_vars = slim.get_variables_to_restore()\r\n        restored_vars = set(vars_to_restore)\r\n        for var in all_vars:\r\n            if var not in restored_vars:\r\n                vars_to_initialize.append(var)\r\n\r\n        restorer = tf.train.Saver(vars_to_restore)\r\n        restorer.restore(sess, resnet_model_path)\r\n        sess.run(tf.variables_initializer(vars_to_initialize))\r\n\r\nLine to save weights while training:\r\n        overall_saver.save(sess, 'weights/model.ckpt', global_step=global_step)\r\n\r\nCode to restore weights once fine-tuned:\r\n    iterator, init_op = create_dataset('test_imgs.csv', is_training=False)\r\n    b1_cls, b2_cls, b1_reg, b2_reg, ratio_h, ratio_w = mbfcn_model(iterator, is_training=False)\r\n\r\n    config = tf.ConfigProto(allow_soft_placement=True)\r\n    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n\r\n    with tf.Session(config=config) as sess:\r\n        chkpt = 'weights/'\r\n        restorer = tf.train.Saver(slim.get_variables_to_restore())\r\n        restorer.restore(sess, tf.train.latest_checkpoint(chkpt))\r\n", "comments": ["What do you results look like if you infer using `is_training=True`? Obviously that's not what you -want- to do, but I've found I have similar issues with transfering `slim` model weights to different applications that are resolved by inferring in training mode, which would indicate the problem is likely related to `slim`'s batch normalization - either not saving moving averages, or not loading them correctly.\r\n\r\nI haven't got to the bottom of the issue yet, but I can't make a minimal working example which exhibits similar behaviour either... but it sounds like I might not be the only one with such issues.", "After extensive testing I found my issue was related to the very high value of `decay` used in batch normalization. Lowering the decay term to `0.9` resulted in almost immediate consistency between training performance and evaluation performance. The best explanation I have is that update operations were being run, but the decay value so close to 1 (0.997 for my case using mobilenet) meant moving averages weren't keeping up with the changes to the batch statistics as a result of faster training.\r\n\r\nI find this very surprising, but it's the best I've got...", "One more update: I've found lowering the learning rate/switching to a simpler optimizer (momentum, from Adam) resolved my issues. Again I find this incredibly surprising, but it lends support to the idea that these networks might not have convergent activations, even if the final inferences are relatively stable... Alternatively I'm completely wrong and there's a bug somewhere. /shrug", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21459\">No</a>\n"]}, {"number": 21458, "title": "speech recognition tutorial bug", "body": "Have I written custom code no\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from N/A\r\nTensorFlow version N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A\r\nMobile device N/A\r\n\r\nI'm doing the Speech Commands tutorial and looking at the low_latency_conv architecture here:\r\nhttps://github.com/tensorflow/tensorflow/blob/23c218785eac5bfe737eec4f8081fd0ef8e0684d/tensorflow/examples/speech_commands/models.py#L283-L302\r\n\r\nThe architecture is supposed to be cnn-one-fstride4 from this paper: https://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf\r\n\r\nThe paper says \"The baseline DNN architecture consists of 3 hidden layers with 128 hidden units/layer and a softmax layer. Each hidden layer uses a rectified linear unit (ReLU) nonlinearity.\"\r\n\r\nHowever the network generated by the code only contains one ReLU. Subsequently, it just has three stacked MatMul operations with bias, and no interleaved nonlinearities. This is really equivalent to just one MatMul with bias, since you can compose such stacked operations.\r\n\r\nWhy are the additional ReLUs not included in this network?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "In general the tutorials strive for pedagogical simplicity rather than best-of-breed implementations.  If you'd like to discuss improvements to this model for your purposes, stackoverflow is a better venue.", "Similar issue here shows that several of these networks are wrong:\r\nhttps://github.com/tensorflow/tensorflow/issues/19853\r\n\r\nI'm fixing them on my end and perhaps will do a pull request depending on how testing goes.", "Thanks for tracking down the related issue.\r\n", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@ibeling This is a stale issue. \r\n\r\nSince the issue opened, more models were added with two and three `relu` layers along with the initial low-latency model. Depending on the application, different model can be selected.\r\n\r\nPlease check updated code in the `master` \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/840afe09496ce89ff23f2136c881993d957abec8/tensorflow/examples/speech_commands/models.py#L207-L237\r\n\r\nI think this was resolved. I am closing this issue. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21458\">No</a>\n"]}, {"number": 21457, "title": "Correct links to documentation pages", "body": "I'm not 100% sure these are the correct links (like, there might be a secret shorthand I don't know about), but the current links (on [this page](https://www.tensorflow.org/api_docs/python/tf/keras/Model), under `compile` and `fit`) lead to [404s](https://www.tensorflow.org/losses).", "comments": ["Oh, you have to send this against the `master` branch. Please open a new PR. Thanks."]}, {"number": 21456, "title": "[Intel MKL] Adding Quantized Convolution (Int8) - Part 2", "body": "This PR is part 2 of four PRs that add quantized version of convolution using MKL-DNN. This part enables MKL quantized convolution kernel in the graph pass.\r\n\r\nPlease note that the below PRs replace this old one #19425 \r\nPart 1: #21483 \r\nPart 2: #21456 \r\nPart 3: #21465 \r\nPart 4: #21466 \r\n", "comments": ["@tatianashp could you take a look?", "Nagging Reviewer @tatianashp, @raghuraman-k: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 35 days with no activity and the `awaiting review` label has been applied.", "@mahmoud-abuzaina could you look into the failures?", "@drpngx ran clang-format.", "@raghuraman-k good to go?", "@drpngx Any update on this PR?", "@drpngx @tatianashp I don't think failures related to our contribution. Could you please let us know.", "Yes, it appears to be passing. Initiating merge.", "@drpngx We can't access the details for the feedback/copybara. Can you provide us more info on the failure?", "Waiting for check & approval from @tatianashp ", "I reviewed and approved. All checks pass. Let me know if you need anything else."]}]