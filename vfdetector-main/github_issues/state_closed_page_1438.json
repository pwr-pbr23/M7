[{"number": 9827, "title": "tf.abs() isn't documented to handle complex, but it does appear to work as tf.complex_abs() used to.", "body": "Issue #7405 was a bug filed that tf.complex_abs() was removed in 1.0.  At the bottom it says that tf.abs() now does that work, but the docs for tf.abs() only mention float.  I confirmed that tf.abs() does in fact do as the comment on the issue describes (https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/ops/math_ops.py#L225)\r\n\r\nPlease incorporate the info from https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/complex_number_functions#complex_abs \r\n\r\ninto\r\nhttps://www.tensorflow.org/api_docs/python/tf/abs\r\n\r\n... specifically the parts about it computing sqrt(a^2 + b^2) for complex numbers.\r\n", "comments": ["@rmlarsen Do you mind fixing this?", "BTW- I would be happy to regularly make PRs for various documentation fixes.  However, I'm a little confused by the code.  I can grep for some of the descriptions to find current docs in the code, but I find that there is quite a bit of duplicate documentation.   Is this because there is separate documentation for the C++ and python code?  \r\n\r\nSince I assume you want the same behavior in both, could this be changed to use \\copydoc from doxygen to avoid having to put the docs in two places?\r\n\r\nOtherwise, If I were to make a change, do I change all the places or is some of it generated and I only need to change one?\r\n\r\nSo that I can be a contributing citizen, generally, what is the rule for where to change things when fixing documentation?", "@ddurham2 (cc @martinwicke): I don't think we have a good story for avoid code duplication between C++ and Python for ops with wrappers.  In this case (and many others) I don't think there's a good substitute for a human merging the docs, since they're different ops at the C++ level (though they probably shouldn't be).\r\n\r\n@ddurham2 Did I interpret correctly that you're volunteering to send a PR?  If so, thank you!", "Sure.. I'll give it a whirl. Could you give me a pointer on how to build and view the documentation (HTML or otherwise) to make sure my changes would look presentable?", "The documentation on documentation is here. If you have questions or if something doesn't work as expected, please ask. Note that the doc generator currently only works for Python 2.7.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md"]}, {"number": 9826, "title": "C API Exception", "body": "Hi,\r\n\r\nThe C API method \"TF_GraphGetTensorNumDims\" throws an exception with message \"Node X was not found in the graph\" even for valid nodes that are in the graph. This tends to happen with the outputs of particular ops (e.g., reshape and matmul). I think it may have to do with shape inference after looking at the C API implementation but I'm not sure what's wrong. Is there something I need to do to enable shape inference when compiling the shared library? I thought that was enabled by default. Or is there something else that's broken?\r\n\r\nThank you,\r\nAnthony", "comments": ["Shape inference should be happening. Please do fill out the full issue template, in particular the library version being used and a code snippet that reproduces the problem. \r\n\r\nWhat you're saying does sound like a bug, but a reproducible example will go a long way :)\r\n(FWIW, the [basic unittest passes](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/c/c_api_test.cc#L430), so if there is a bug, we'd love to update the unittests to catch that)\r\n\r\nThanks!", "@asimshankar I'll try to create a reproducible example using the Java API. Truth be told, I came about this problem while developing the Scala API and don't have a simple code snippet you can use to reproduce this. One thing that might help for your unit tests is that the problem doesn't occur for many ops. A minimal example that causes the problem for me (using Scala syntax but trivial to translate) is this:\r\n\r\n```scala\r\nval images = placeholder(TFUInt8, Shape(-1, 28, 28))\r\nval vectorizedImages = reshape(images, Shape(-1, 784))\r\n```\r\n\r\n`vectorizedImages` is an `Output` with Java API semantics. If you now try to call `vectorizedImages.shape`, you will get the exception I am describing, saying that `Node Reshape was not found in the graph`. I hope this can help you reproduce the problem, but if it doesn't I can look into creating a code sample using the Java API.\r\n\r\nAs for the version, I tried this with both the master branch version (current one) and the r1.1 branch. I compiled without `--config=opt` as this currently causes another problem for which I am currently creating another issue. :)\r\n\r\nThanks!", "Small update: Compiling the current master branch code with the `--config=opt` flag does not resolve the issue.", "@asimshankar After trying to replicate the problem with a simple example in the Java API I found out the following:\r\n\r\n1. The reason the previous example was failing was that I was passing a tensor with shape [2, 1] as the second input to `Reshape`. This needs to be a one-dimensional tensor. I assume that this made [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L1108) in the C API fail silently. Thus the shape refiner never registered the op.\r\n2. I noticed the same happening with other ops. For example, passing a one-dimensional tensor to `MatMul` results in the same behavior.\r\n3. Shouldn't we have an error thrown in the native library whenever invalid inputs or attributes are provided for the ops being constructed?\r\n\r\nThanks!", "Ah, thanks for tracing this down @eaplatanios ! Sounds like we forgot to clean this up (the precondition mentioned in the comment in the line you pointed to has been met I believe, so we just have to remove that).\r\n\r\nI'll try to get that cleared up.\r\n\r\nThanks!", "@asimshankar No worries. I'm actually curious. What is the desired behavior for when somebody creates a \"Reshape\" op using a 2-D tensor provided for the shape? That's just an example, but in general, shouldn't the native library check the op arguments and throw an exception at graph construction time if something is wrong?", "@eaplatanios : Yes, the desired behavior would be to have the addition of the \"Reshape\" operator (which was provided invalid arguments) throw an exception. If we didn't ignore the error, that is what would happen. The reason we ignored the error was a temporary one to ease transition, that's taken care of, so after my fix, the Java call to `OperationBuilder.build()` will throw an exception.\r\n\r\nHope that makes sense.", "@asimshankar That makes total sense! Thanks for the prompt responses! :)"]}, {"number": 9825, "title": "Fix a broken link", "body": "Fix a broken link due to new line.", "comments": ["Can one of the admins verify this patch?", "Jenskins, test this please.", "Jenkins, test this please.\r\n", "Can one of the admins verify this patch?", "Jenkins, test this please.\r\n", "Jenkins, test this please.\r\n"]}, {"number": 9824, "title": "possible bug - LSTMCell and GRUCell have different variable reuse behavior", "body": "[tf_env.txt](https://github.com/tensorflow/tensorflow/files/999124/tf_env.txt)\r\n### System information\r\n\r\nCapture script output is attached.\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes -- see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  OS X 10.12.4\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**:  NA\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nclass ToyModel(object):\r\n\tdef __init__(self):\r\n\t\tx = tf.get_variable(\"x\", shape=[10,32,3], initializer=tf.random_uniform_initializer(), trainable=False)\r\n\t\tcell = tf.contrib.rnn.LSTMCell(3)\r\n\t\tself.y, _ = tf.nn.dynamic_rnn(cell, inputs=x, dtype=tf.float32)\r\n\r\ngraph_context = tf.Graph()\r\nwith graph_context.as_default():\r\n\twith tf.name_scope(\"Train\"):\r\n\t\twith tf.variable_scope(\"Model\", reuse=None):\r\n\t\t\tm1 = ToyModel()\r\n\twith tf.name_scope(\"Valid\"):\r\n\t\twith tf.variable_scope(\"Model\", reuse=True):\r\n\t\t\tm2 = ToyModel()\r\n\r\n\ttf_init = tf.global_variables_initializer()\r\n\r\n\t# sv = tf.train.Supervisor(logdir=save_dir)\r\n\t# with sv.managed_session() as session:\r\n\tsession = tf.Session(graph=graph_context)\r\n\twith session as sess:\r\n\t\tsess.run(tf_init)\r\n\t\ty1 = m1.y.eval()\r\n\r\n\t\ty2 = m2.y.eval()\r\n\r\n\t\tprint(y1 == y2)\r\n\r\nTraceback (most recent call last):\r\n  File \"src/minimal_tf.py\", line 30, in <module>\r\n    m2 = ToyModel()\r\n  File \"src/minimal_tf.py\", line 21, in __init__\r\n    self.y, _ = tf.nn.dynamic_rnn(cell, inputs=x, dtype=tf.float32)\r\n  File \"/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 553, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 720, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 705, in _time_step\r\n    (output, new_state) = call_cell()\r\n  File \"/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 691, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 398, in __call__\r\n    reuse=self._reuse) as unit_scope:\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/Users/delkind/Desktop/whd/venv/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 93, in _checked_scope\r\n    \"the argument reuse=True.\" % (scope_name, type(cell).__name__))\r\nValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'Model/rnn/lstm_cell'; and the cell was not constructed as LSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.\r\n```\r\n### Describe the problem\r\n\r\nWhen cell is an LSTMCell instance, this code raises the the ValueError above, which I believe to be a bug. My expectation is that invoking the second model in a scope with reuse=True will mean that m2 uses the already-existing variables in m1. The error message indicates that this is not happening, apparently because the LSTMCell is not aware of the reuse flag set in m2's scope.\r\n\r\nBy contrast, if you swap the LSTMCell for GRUCell, no errors are raised, and the code completes as expected. Indeed, the outputs of y1 and y2 are equal when the graph is evaluated.\r\n\r\nLikewise, if you use the LSTMCell but skip creating m2 at all, y1 can be evaluated without any errors.", "comments": ["@ebrevdo can you comment?", "The way that cells create and store variables has changed recently (in the TF nightlies); can you retry your computation there and ensure that the variables are created and reused as you expect?", "@ebrevdo I'm not an engineer, but it doesn't seem to make a lot of sense to attempt to recreate an error from a nightly. For reproducibility, I think one would attempt to reconstruct the error from a stable version.\r\n\r\nBut if I were to build from a nightly, which nightly would I use?", "If you're not using a nightly then we don't know if the bug's been fixed\nalready or not (and bugfixes don't get backported to old versions).\n\nYou can download a nightly build by following one of the links here\n<https://github.com/tensorflow/tensorflow/#installation>.\n\nOn Wed, May 24, 2017 at 1:42 PM, David J. Elkind <notifications@github.com>\nwrote:\n\n> I'm not an engineer, but it doesn't seem to make a lot of sense to attempt\n> to recreate an error from a nightly. For reproducibility, I think one would\n> attempt to reconstruct the error from a stable version.\n>\n> But if I do build from a nightly, which nightly should I use?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9824#issuecomment-303845513>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxxjngCwsDlcpPlXY5kJ-Ob1IuV8ks5r9JY9gaJpZM4NXdhS>\n> .\n>\n", "This works as expected in TF 1.2rc0."]}, {"number": 9823, "title": "Tensorflow consumes much more memory than expected", "body": "My model has four CPU variables:\r\n[500M, 3] tf.int32\r\n[500M] tf.float32\r\n[500M] tf.float32 (FTRL accumulate slot)\r\n[500M] tf.float32 (FTRL linear slot)\r\nexpected memory consumption should be (500M * 6) * 4 = 12G, however tensorflow used 20G memory.\r\n\r\nWhen I increased 500M to 1B, total memory usage is 40G, seems tensorflow do allocate much more memory than needed, any idea? By the way I am not using any tcmalloc stuff.\r\nI also used timeline show_memory to print allocated tensor size, everything is consistent with my calculating.", "comments": ["I traced it using strace and found two mmap called by different two threads, so actually memory consumption is 2x than needed.\r\n\r\nHere is the code:\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\n\r\nbucket_size = 1000000000\r\n\r\na = tf.get_variable(\"W\", [bucket_size], initializer=tf.zeros_initializer, dtype=tf.float32, trainable=False)\r\n\r\nwith tf.Session() as sess:\r\n    print(\"start to initialize variable\")\r\n    time.sleep(10)\r\n    sess.run(a.initializer)\r\n    print(\"initialize variable done\")\r\n\r\ntime.sleep(10)\r\n```\r\n\r\nHere is the strace output\r\n```\r\n[pid 25768] write(1, \"start to initialize variable\\n\", 29start to initialize variable\r\n <unfinished ...>\r\n[pid 25817] futex(0x26c92e4, FUTEX_WAIT_PRIVATE, 1, NULL <unfinished ...>\r\n[pid 25768] <... write resumed> )       = 29\r\n[pid 25768] select(0, NULL, NULL, NULL, {10, 0}) = 0 (Timeout)\r\n[pid 25768] mmap(NULL, 4294967296, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f2ec8000000\r\n[pid 25768] futex(0x26c92e4, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0x26c92e0, {FUTEX_OP_SET, 0, FUTEX_OP_CMP_GT, 1}) = 1\r\n[pid 25817] <... futex resumed> )       = 0\r\n[pid 25768] futex(0x7ffdfa482a9c, FUTEX_WAIT_PRIVATE, 1, NULL <unfinished ...>\r\n[pid 25817] futex(0x26c92b8, FUTEX_WAKE_PRIVATE, 1) = 0\r\n[pid 25817] futex(0x26c9264, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0x26c9260, {FUTEX_OP_SET, 0, FUTEX_OP_CMP_GT, 1}) = 1\r\n[pid 25816] <... futex resumed> )       = 0\r\n[pid 25817] mmap(NULL, 2097152, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0 <unfinished ...>\r\n[pid 25816] futex(0x26c9238, FUTEX_WAKE_PRIVATE, 1 <unfinished ...>\r\n[pid 25817] <... mmap resumed> )        = 0x7f307f9ff000\r\n[pid 25816] <... futex resumed> )       = 0\r\n[pid 25817] munmap(0x7f307f9ff000, 2097152 <unfinished ...>\r\n[pid 25816] futex(0x26c9264, FUTEX_WAIT_PRIVATE, 3, NULL <unfinished ...>\r\n[pid 25817] <... munmap resumed> )      = 0\r\n[pid 25817] mmap(NULL, 4190208, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f307f800000\r\n[pid 25817] munmap(0x7f307fa00000, 2093056) = 0\r\n[pid 25817] mmap(NULL, 4294967296, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f2dc8000000\r\n[pid 25817] futex(0x26c9264, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0x26c9260, {FUTEX_OP_SET, 0, FUTEX_OP_CMP_GT, 1}) = 1\r\n[pid 25816] <... futex resumed> )       = 0\r\n[pid 25817] futex(0x7ffdfa482a9c, FUTEX_CMP_REQUEUE_PRIVATE, 1, 2147483647, 0x7ffdfa482a70, 2) = 1\r\n[pid 25816] futex(0x26c9238, FUTEX_WAKE_PRIVATE, 1 <unfinished ...>\r\n[pid 25817] futex(0x26c92e4, FUTEX_WAIT_PRIVATE, 3, NULL <unfinished ...>\r\n[pid 25816] <... futex resumed> )       = 0\r\n[pid 25768] <... futex resumed> )       = 0\r\n[pid 25816] futex(0x26c9264, FUTEX_WAIT_PRIVATE, 5, NULL <unfinished ...>\r\n[pid 25768] futex(0x7ffdfa482a70, FUTEX_WAKE_PRIVATE, 1) = 0\r\n[pid 25768] write(1, \"initialize variable done\\n\", 25initialize variable done\r\n```\r\n\r\nBoth 25768 25817 called mmap to allocate 4G memory. Why?", "It seems that this code actually initialized two variables, one is zero const with 4G, the other is W variable, then assign zero const to W. I will check the python code to see what I can do to avoid it.", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variables.py#L276 If initializer is provided and callable, how about passing variable to initializer instead of initialize and then assign it?\r\nCodebase now has one zero initializer operator to initialize variables, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/kernels/zero_initializer_op.h, we need to add more.\r\nAny idea?", "You can already do this, `tf.get_variable(\"a\",another_variable)`, I'm not sure that will avoid a copy. ", "BTW, easier way than strace is to look at tensor allocation/deallocation messages with https://github.com/yaroslavvb/memory_util", "@benoitsteiner Would it be feasible to add a graph rewriting optimization to do in-place assignment for simple cases?", "It's on our to-do list, we just lacked a good reason to prioritize this ahead of other rewrites. ", "@benoitsteiner  I am training super sparse model, no memory left due to this 2x memory issue. This is very general case, I don't want to spend effort to fix it for my own, and  several weeks after my own fix you guys fix and commit it, what if I make the change and send pull request?", "@rmlarsen: Your name is on comments in `AssignOp` that buffer forwarding rarely triggers.  Looks like @fesun is offering to submit a PR; can we direct him to the right change to make?", "@rmlarsen ping...", "@girving @rmlarsen @benoitsteiner  Could you guys make some decision? It's our blocking issue, long time no response.", "Sorry for not noticing this thread earlier. Let me take a look.", "@rmlarsen any update? If you guys don't have resource now, I can make the change, but we need to talk and agree the design.", "Since googler didn't response for very long time, I made one minimal change on my codebase to solve this, and post my solution here in case others are also confused by this:\r\n1) Tensorflow provides ones_initializer/zeros_initializer for initializing variables to 1/0, what tensorflow actually does is creating one const tensor and then assign it to variable, however constant operator holds tensor as its instance member, so its memory will not be freed unless the operator instance is deleted. Tensorflow AssignOp intent to try it's best to avoid memory copy(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/assign_op.h#L47), this happens only when input tensor refcount is 1, while const tensor refcount is always greater than 1.\r\n**Solution: write custom operator to create normal 0/1 tensor**\r\n2) As AssignOp comments said, conservative constraints make buffer forwarding unlikely to happen very often, my model doesn't leverage GPU and any other advanced communication tech, so I set both gpu and nic compatible to false.\r\n3) patch desired optimizer to use custom 0/1 initializer(slot creation)", "@fesun Good job figuring this out. My apologies for not getting back to you. I think we can probably implement a more general solution by doing a graph analysis and strip off the buffer constraints when they are not needed, or perhaps propagate them to dependencies, so the bufferes are compatible, perhaps similar to \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc#L718", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@rmlarsen any updates on the more general solution?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@fesun Do you happen to have a github link handy to that custom operator?", "Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@rmlarsen Was the general solution that you mentioned ever implemented?\r\n", "@tatianashp Yes, this was implemented about a month ago:\r\n\r\nThe graph pass is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/memory_optimizer.cc#L1226\r\n\r\nAnd the code using the result here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/assign_op.h#L58", "Great. Thank you!\r\n\r\nI am closing the issue."]}, {"number": 9822, "title": "OK", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "\u62c9\u53d6\u6700\u65b0\u4ee3\u7801", "You are submitting to the wrong repo."]}, {"number": 9821, "title": "tf.constant_initializer does not accept Tensor as input", "body": "I'd like to load multiple copies of VGG on each of 4 different GPUs.. rather than copy the weights down to each GPU every time, I'd like a local copy of the weights. Unfortunately using constant_initializer and a numpy array explodes the graph def (as all the n copies of the weights are stored). \r\n\r\nInstead, I convert the weights to a tensor once, and want to use this tensor to initialize the variable. this keeps the graphdef small and allows me to be super flexible. Unfortunately it looks like tf.constant_initializer doesn't accept Tensors as values.", "comments": ["It's pretty inefficient to large numpy arrays into Graph. A work-around is to create Variables with  `tf.placeholder` initial value and then feed numpy array during initialization. Unfortunately `get_variable` doesn't have `placeholder_initializer` either.\r\n\r\nI get around it this by using a simplified replacement for `get_variable` mechanism that creates variables with `tf.placeholder` initial value, and puts corresponding `placeholder,numpy` pairs into global init_dict, so then I can call `sess.run(tf.global_variables_initializer(), feed_dict=init_dict)`\r\n\r\nHere's an example: https://github.com/yaroslavvb/stuff/blob/master/numpy_initializers/kfac_cifar.py#L245", "@lukaszkaiser what do you think about supporting following: `tf.get_variable(name, tf.tensor_initializer(tensor))` which accepts a `tf.Tensor` object, and creates variable as `tf.Variable(tensor, name=name)`? That will cover the placeholder use-case as well", "We already support just passing values to initializer in get_variable, they just get passed over to tf.Variable, so you can, e.g., do tf.get_variable(\"x\", initializer=0.5). I'd hope it'd work with tf.Tensor already, doesn't it?", "Good point, that does seem to work already\r\n\r\n```\r\npl = tf.placeholder(tf.float32, shape=())\r\na = tf.get_variable(\"test\", initializer=pl)\r\nsess = tf.Session()\r\nsess.run(a.initializer, feed_dict={pl: 1})\r\n```\r\n\r\n@el3ment does that cover your use-case? You could be even more efficient by initializing the first variable `var` on GPU from `tf.placeholder`, and the rest from `var.initial_value()`", "@yaroslavvb The issue there is that I would need to create a placeholder for all the weights in VGG which is impractical.\r\n\r\n@lukaszkaiser This is true for constants, but I think when I tried it with a tensor object it complained -- to be honest though, I am not 100% sure that I tried that and I should have.", "@el3ment it should work with a Tensor, can you double check that you have latest version"]}, {"number": 9820, "title": "Unable to build TensorFlow Java native libraries for arm64-v8a Android devices", "body": "### Problem\r\n\r\nI'm trying to build TensorFlow Java and the relevant native libraries for an Android device (I'm targeting a Pixel).\r\n\r\n### What I've tried\r\n\r\nFollowing the instructions on building TensorFlow Java from source [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md) and on Android TensorFlow support [here](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/android/README.md), (relative to my checked out TensorFlow repo at `~/Android/tensorflow-master`) I updated my `WORKSPACE` to include the following\r\n\r\n```\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 25,\r\n    build_tools_version = \"25.0.3\",\r\n    path = \"~/Android/Sdk/\",\r\n)\r\n\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"~/Android/android-ndk-r12b/\",\r\n    api_level=24)\r\n```\r\n\r\nI ran `./configure` and supplied the following options\r\n\r\n```\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\n  /usr/local/buildtools/current/sitecustomize\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] n\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: -std=c++11 -march=armv8-a\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] n\r\njemalloc disabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n\r\nNo XLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] n\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] n\r\nNo CUDA support will be enabled for TensorFlow\r\n```\r\n\r\nand then I attempted to build the relevant libraries with the following command\r\n\r\n```\r\nbazel build -c opt --config opt \\\r\n //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni \\\r\n --crosstool_top=//external:android/crosstool \\\r\n --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n --cpu=arm64-v8a\r\n```\r\n\r\n### What happens\r\n\r\nI get a bunch of warnings (stdout from the above command: [log.txt](https://github.com/tensorflow/tensorflow/files/991284/log.txt)) and the following linking error\r\n\r\n`ERROR: /usr/local/google/home/tsamson/Android/tensorflow-master/tensorflow/java/BUILD:142:1: Linking of rule '//tensorflow/java:libtensorflow_jni.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/linux-x86_64/bin/aarch64-linux-android-gcc ... (remaining 38 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.`\r\n\r\n(I get the same errors building for `armeabi-v7a`. I am able to build the above targets for my Ubuntu machine by doing the above (instead using `-march=native` and the default `--crosstool_top`, `--host_crosstool_top`, and `--cpu` flags), but I want to compile for Android \u263a.)\r\n\r\n### Environment\r\n\r\nHere's the output of `tools/tf_env_collect.sh` (which had to be run from a directory other than `tensorflow-master` to keep from getting a tensorflow import error):\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nVERSION=\"14.04.5 LTS, Trusty Tahr\"\r\nVERSION_ID=\"14.04\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\r\nCopyright (C) 2013 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nprotobuf (3.3.0)\r\ntensorflow (1.1.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.1.0\r\ntf.GIT_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\ntf.COMPILER_VERSION = v1.1.0-rc0-61-g1ec6ed5\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed May 10 13:42:21 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro K1200        On   | 0000:01:00.0      On |                  N/A |\r\n| 39%   45C    P8     1W /  35W |    553MiB /  4016MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      3426    G   /usr/lib/xorg/Xorg                             331MiB |\r\n|    0      4959    G   cinnamon                                        61MiB |\r\n|    0      8578    G   /proc/self/exe                                 158MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n\r\n```\r\n", "comments": ["The Java API build instructions and libtensorflow_jni are irrelevant to building for Android. You'll want to follow the instructions in [examples/android/README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md) and [contrib/android/README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/README.md) instead. The target you'll want to build for Android containing the native code for TF + Java APIs + TensorFlowInferenceInterface wrapper is `tensorflow/contrib/android:libtensorflow_inference.so`", "I'd like to build a TensorFlow Graph and update its parameters, in addition to doing inference, from within an Android app. How do you suggest doing this / why is it not possible to use the Java API directly instead of the wrapper?", "You can definitely use the Java API on Android (the TFII wrapper is just provided for convenience), but graph-building and training on mobile devices is not supported by TF at this time."]}, {"number": 9819, "title": "Importing tensorflow fails.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: I can't import, but the wheel file is \"tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl (48.5MB)\"\r\n- **Bazel version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: cuda_8.0.61_win10.exe / cudnn-8.0-windows10-x64-v5.1.zip\r\n- **GPU model and memory**: NVIDIA GeForce GTX 1070\r\n- **Exact command to reproduce**: import tensorflow\r\n\r\n### Describe the problem\r\nImporting tensorflow fails.\r\n\r\n### Source code / logs\r\nInstalling tensorflow-gpu:\r\n```\r\n\u03bb pip3 install tensorflow-gpu --upgrade\r\nCollecting tensorflow-gpu\r\n  Downloading tensorflow_gpu-1.1.0-cp35-cp35m-win_amd64.whl (48.5MB)\r\n    100% |################################| 48.6MB 16.9MB/s\r\nRequirement already up-to-date: wheel>=0.26 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: protobuf>=3.2.0 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: werkzeug>=0.11.10 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: numpy>=1.11.0 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: six>=1.10.0 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: setuptools in c:\\users\\arthu\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from protobuf>=3.2.0->tensorflow-gpu)\r\nRequirement already up-to-date: packaging>=16.8 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from setuptools->protobuf>=3.2.0->tensorflow-gpu)\r\nRequirement already up-to-date: appdirs>=1.4.0 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from setuptools->protobuf>=3.2.0->tensorflow-gpu)\r\nRequirement already up-to-date: pyparsing in c:\\users\\arthu\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from packaging>=16.8->setuptools->protobuf>=3.2.0->tensorflow-gpu)\r\nInstalling collected packages: tensorflow-gpu\r\nSuccessfully installed tensorflow-gpu-1.1.0\r\n```\r\n\r\nImporting tensorflow:\r\n```\r\n\u03bb python\r\nPython 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n", "comments": ["Could you try the dependency walker on the tensorflow DLL to see what are the missing DLLs? I am suspecting CUDA.", "Where should this DLL be located in a default installation?", "I'm having the same problem as @acburigo \r\n\r\n> pip show tensorflow-gpu\r\n...\r\nLocation:  C:\\Users\\[USER NAME]\\appdata\\local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\r\n...\r\n\r\nThere's a bunch of stuff there but I don't see a DLL.  There are `tensorflow` and `tensorflow_gpu-1.1.0.dist-info` directories in there.  `tensorflow/python` seems to be the location of the pywrap_tensorflow_internal.py that's generating the error.\r\n\r\n@drpngx  do you know where there is supposed to be a tensorflow DLL?  The only DLLs in that whole tensorflow directory appear to be `_gru_ops.dll` and `_lstm_ops.dll`", "I was able to open the _python_tensorflow_internal.pyd with Dependency Walker and it complained about cudnn64_5.dll not being found.  I tried it on my command prompt and found that adding it to my path hadn't worked.\r\n\r\nFix for me was:\r\n1. Type \"cudnn64_5.dll\" into command prompt and get \"not recognized\" to verify that DLL wasn't in my path.\r\n2. Go back and add the location of the cudnn DLL to the path again (wherever you unzipped it to), making sure I clicked \"OK\" on the dialogs so that they close out correctly.\r\n3. Close my open command prompts.\r\n4. Open a new command prompt and type \"cudnn64_5.dll\", for me this opens NUnit since that is what opens DLLs on my setup.\r\n5. Activate my anaconda environment & run again.\r\n6. Bask in the fastness of GPU tensorflow!\r\n\r\nHope that helps @acburigo ", "Thanks, @SeanColombo . I think my Alienware machine was messing with the GPU libraries.\r\nI ended up deciding it was best installing Ubuntu in it and now I have everything working well."]}, {"number": 9818, "title": "fatal error: 'infiniband/verbs.h' file not found", "body": "When building Tensorflow from Source\r\n\r\n```\r\nERROR: /Users/dendisuhubdy/dev/dlframeworks/tensorflow/tensorflow/contrib/verbs/BUILD:104:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 135 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:\r\nIn file included from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:21:\r\nIn file included from ./tensorflow/contrib/verbs/rdma_mgr.h:24:\r\n./tensorflow/contrib/verbs/rdma.h:21:10: fatal error: 'infiniband/verbs.h' file not found\r\n#include <infiniband/verbs.h>\r\n         ^\r\n1 error generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 4656.328s, Critical Path: 2662.37s\r\n```", "comments": ["You need verbs. Do you intend to use RDMA?", "Yes, oh shit I forgot verbs in my build, thanks @drpngx !", "@drpngx is there any tutorial about how to use this on a centos system ?", "I have installed libibverbs-1.1.8 and libibverbs-devel-1.1.8, but I still can't locate the infiniband/verbs.h, any advice?", "You should have it in libibverbs-devel (/usr/include/infiniband/verbs.h).\r\nwhat is the output of: dpkg-query -L libibverbs-dev | grep verbs.h  ?", "I installed the libibverbs-devel-1.1.8 from the local repo before, and problem solved after downloading another version of libibverbs-devel from the Internet. Thank you!!", "@CynthiaProtector hello, i encountered the same problem and i haven't found the solution. I want to know if you run on MacOS? because when i try to install  libibverbs-dev on mac, it turns:\r\n$ pip3 install libibverbs-dev\r\nCollecting libibverbs-dev\r\n  Could not find a version that satisfies the requirement libibverbs-dev (from versions: )\r\nNo matching distribution found for libibverbs-dev\r\nthanks very much.", "> I installed the libibverbs-devel-1.1.8 from the local repo before, and problem solved after downloading another version of libibverbs-devel from the Internet. Thank you!!\r\n\r\nHello Cynthia,\r\nCould you please tell the internet source to download libibverbs-devel as I am facing the same issue.", "RDMA is only provided for Linux now, and you could install them with libibverbs-dev (Debian/Ubuntu) or libibverbs-devel (RHEL/CentOS/Fedora)."]}, {"number": 9817, "title": "Tests that BasicLSTMCell with layer_norm=False.", "body": "I think BasicLSTMCell should be tested when layer_norm is False.", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "Pretty sure the basiclstmcell is so basic we don't want to add layer norm\nto it.\n\nOn May 11, 2017 9:24 PM, \"Benoit Steiner\" <notifications@github.com> wrote:\n\n@benoitsteiner <https://github.com/benoitsteiner> requested your review on:\ntensorflow/tensorflow#9817\n<https://github.com/tensorflow/tensorflow/pull/9817> Tests that\nBasicLSTMCell with layer_norm=False..\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/9817#event-1079368087>, or mute\nthe thread\n<https://github.com/notifications/unsubscribe-auth/ABtim4dbMPWzehzx4iXrrmGteqDTrHQZks5r497_gaJpZM4NW8oN>\n.\n", "@ebrevdo this just adds additional test coverage. Are you opposed to that?", "@tensorflow-jenkins test this please", "No, I'm opposed to BasicLSTMCell having layer norm in the first place.\nLet's fix that in the other PR and then we don't need this one.\n\nOn Fri, May 19, 2017 at 12:13 PM, Rasmus Munk Larsen <\nnotifications@github.com> wrote:\n\n> @tensorflow-jenkins <https://github.com/tensorflow-jenkins> test this\n> please\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9817#issuecomment-302787847>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9PA7CPojUu03MiSE_XrEE0aiJ9tks5r7enegaJpZM4NW8oN>\n> .\n>\n", "Can one of the admins verify this patch?", "Ah I see - you were comparing BasicLSTMCell to LayerNormLSTMCell.  \r\n\r\nLGTM assuming tests pass", "Jenkins test this please."]}, {"number": 9816, "title": "Variable validate_shape not honored", "body": "A Variable with validate_shape=True does not seem to be honored and throws a error.  I am using two Variables to store the contents of a SparseTensor and the assign fails because the sizes do not match on one, but not the other Variable.  \r\n\r\nPlatform:  Ubuntu 14.04\r\nCode:  Both build from source and binary release\r\nVersion: 1.1.0\r\n\r\n2017-05-10 14:36:12.973654: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [10]\r\n\t [[Node: Assign = Assign[T=DT_INT32, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, SparseAdd:1)]]\r\n2017-05-10 14:36:12.973661: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [10]\r\n\t [[Node: Assign = Assign[T=DT_INT32, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, SparseAdd:1)]]\r\n2017-05-10 14:36:12.973657: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [10]\r\n\t [[Node: Assign = Assign[T=DT_INT32, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, SparseAdd:1)]]\r\ndone\r\nTraceback (most recent call last):\r\n  File \"parse.py\", line 52, in <module>\r\n    print(sess.run([st2, asop2]))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [10]\r\n\t [[Node: Assign = Assign[T=DT_INT32, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, SparseAdd:1)]]\r\n\r\nCaused by op u'Assign', defined at:\r\n  File \"parse.py\", line 39, in <module>\r\n    asop2 = tf.assign(var_feature_count_cnt, st2.values)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 270, in assign\r\n    validate_shape=validate_shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [10]\r\n\t [[Node: Assign = Assign[T=DT_INT32, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, SparseAdd:1)]]\r\n\r\n\r\n\r\nCode:\r\nimport tensorflow as tf\r\n\r\n\r\n# parse logic\r\n# start with example feature list\r\n# parse into distinct tokens\r\n# collect counts\r\nfile_queue = tf.FIFOQueue(100, [tf.string])\r\n\r\n# the feature count that we maintain between batches\r\n# must be kept in a variable, but variables dont support sparse tensors\r\n# so we have to keep the source index and value arrays\r\nvar_feature_count_idx = tf.Variable(tf.zeros([1,1],dtype=tf.int64), validate_shape=False)\r\nvar_feature_count_cnt = tf.Variable(tf.zeros([1],dtype=tf.int32), validate_shape=False)\r\n\r\n#read a chunk of features\r\nreader = tf.TextLineReader()\r\n_, line = reader.read_up_to(file_queue, 1000)\r\n\r\n#parse into tokens\r\ntokens = tf.string_split(line,delimiter='\\t')\r\nvals = tf.string_to_hash_bucket_fast(tokens.values, 1024) # hash\r\ny, idx, count = tf.unique_with_counts(vals)  # get distinct\r\ny2 = tf.expand_dims(y,1)\r\n\r\n# now create a sparse array with the hashbucket of as the index\r\nfcount = tf.SparseTensor(indices=y2,values=count,dense_shape=[1024]) # running count of features\r\n\r\n# update the global count\r\nst = tf.SparseTensor(indices=var_feature_count_idx, values=tf.identity(var_feature_count_cnt), dense_shape=[1024])\r\nst2 = tf.sparse_add(st, fcount)\r\n#asop1 = tf.assign(var_feature_count_idx, st2.indices)\r\nasop2 = tf.assign(var_feature_count_cnt, st2.values)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  sess.run(file_queue.enqueue_many((['features.txt'],)))\r\n  sess.run(file_queue.close())\r\n\r\n  try:\r\n    while True:\r\n      print(sess.run([st2, asop2]))\r\n  except tf.errors.OutOfRangeError:\r\n    print 'load finished!'\r\n  finally:\r\n    print 'done'\r\n\r\n\r\n", "comments": ["Can you try to isolate a shorter repro program so that we can look into this?  (It'd be nice if the Github code block syntax can be fixed too :))", "Hey Zongheng,\nHere is a simpler example:Notice that the \"validate_shape=False\" does not seem to be honored. \u00a0\nThanks.-Doug\n\n# sparse variable\nimport tensorflow as tf\n\n# the feature count that we maintain between batches# must be kept in a variable, but variables dont support sparse tensors# so we have to keep the source index and value arraysvar_feature_count_idx = tf.Variable(tf.zeros([1,1],dtype=tf.int64), validate_shape=False)var_feature_count_cnt = tf.Variable(tf.zeros([1],dtype=tf.int32), validate_shape=False)\nvals = tf.constant([2,3,4,2,5,2,5],dtype=tf.int64)y, idx, count = tf.unique_with_counts(vals)\u00a0 # get distinct\n# now create a sparse array with the hashbucket of as the indexy2 = tf.expand_dims(y,1)fcount = tf.SparseTensor(indices=y2,values=count,dense_shape=[1024]) # running count of features\n# update the global countst = tf.SparseTensor(indices=var_feature_count_idx, values=tf.identity(var_feature_count_cnt), dense_shape=[1024])st2 = tf.sparse_add(st, fcount)asop1 = tf.assign(var_feature_count_idx, st2.indices)asop2 = tf.assign(var_feature_count_cnt, st2.values)\n\nwith tf.Session() as sess:\u00a0 sess.run(tf.global_variables_initializer())\u00a0 print(sess.run([asop1, asop2]))\n\n\nIt will fail with:Traceback (most recent call last):\u00a0 File \"sparsevar.py\", line 28, in <module>\u00a0 \u00a0 print(sess.run([asop1, asop2]))\u00a0 File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 789, in run\u00a0 \u00a0 run_metadata_ptr)\u00a0 File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 997, in _run\u00a0 \u00a0 feed_dict_string, options, run_metadata)\u00a0 File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\u00a0 \u00a0 target_list, options, run_metadata)\u00a0 File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\u00a0 \u00a0 raise type(e)(node_def, op, message)tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [5]  [[Node: Assign_1 = Assign[T=DT_INT32, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, SparseAdd:1)]]\nCaused by op u'Assign_1', defined at:\u00a0 File \"sparsevar.py\", line 23, in <module>\u00a0 \u00a0 asop2 = tf.assign(var_feature_count_cnt, st2.values)\u00a0 File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 271, in assign\u00a0 \u00a0 validate_shape=validate_shape)\u00a0 File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\u00a0 \u00a0 use_locking=use_locking, name=name)\u00a0 File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\u00a0 \u00a0 op_def=op_def)\u00a0 File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2359, in create_op\u00a0 \u00a0 original_op=self._default_original_op, op_def=op_def)\u00a0 File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1242, in __init__\u00a0 \u00a0 self._traceback = _extract_stack()\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [1] rhs shape= [5]  [[Node: Assign_1 = Assign[T=DT_INT32, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, SparseAdd:1)]]\n\n      From: Zongheng Yang <notifications@github.com>\n To: tensorflow/tensorflow <tensorflow@noreply.github.com> \nCc: Doug Loyer <dloyer123@yahoo.com>; Author <author@noreply.github.com>\n Sent: Thursday, May 11, 2017 5:11 PM\n Subject: Re: [tensorflow/tensorflow] Variable validate_shape not honored (#9816)\n   \nCan you try to isolate a shorter repro program so that we can look into this? (It'd be nice if the Github code block syntax can be fixed too :))\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.  \n\n   ", "Hmmm...   \r\n\r\nIt looks like the value_shape=False on the Variable is not honored, but it is honored on assign.\r\n\r\nMaybe the documentation should be updated", "Hey Doug, your code sample is very hard to read. It looks like a bunch of the line breaks were lost somehow. I think replying by email is screwing up github, would you mind opening the issue and repasting in your code + error into a comment directly? You can use the \"<>\" button above the comment edit box to insert a code block. Thanks!", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 9815, "title": "Add control input dependency to Java bindings", "body": "Add a builder method to OperationBuilder to allow adding\r\ncontrol inputs to Operations.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Can one of the admins verify this patch?", "Is there perhaps a flaky test in the windows CMake build?\r\n\r\nThe only time the ci build ran it reported an error in a seemingly [unrelated area](https://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/2249/console) of the code; and I'm not able to connect the dots between the reported failure and this change.", "@tensorflow-jenkins test this please"]}, {"number": 9814, "title": "failed to load the native TensorFlow runtime", "body": "Hi! \r\n\r\nI'm worked with Tensorflow on a CPU without. No I'm trying to run it on a device with following specs:\r\nCPU: Intel Xeon(E5-2670)  and win7 64bit and NVIDIA GeForce GTX 980 Ti.\r\nI've installed python3.5 and Tensorflow for GPU just as described in TF homepage.   when I run a test program here what I get when  I try to import Tensorflow : \r\n\r\n\r\n```\r\n(C:\\Users\\Engine>python\r\nPython 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AM\r\nD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):](url)\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_h\r\nelper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\r\n\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_h\r\nelper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\r\n\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\python\\__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_h\r\nelper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\r\n\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-pack\r\nages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_h\r\nelper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Engine\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\r\n\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n```", "comments": ["Do you have VS2015 installed? If not, try installing it or install Microsoft Visual C++ 2015 Redistributable.\r\nThen. try to install and configure Tensorflow as specified in the Installation guide (Requirements to run TensorFlow with GPU support): https://www.tensorflow.org/install/install_windows\r\n\r\nSomeone has a similar issue in this thread:\r\nhttps://github.com/tensorflow/tensorflow/issues/6055\r\n", "@aelimame  thanks for reply. I've got the info on SO that it's  known issue 42011070", "@aelimame thanks. it worked."]}, {"number": 9813, "title": "Failed to load the native TensorFlow runtime.", "body": "", "comments": []}, {"number": 9812, "title": "Implement strided slice (stride > 2) in tf2xla", "body": "Before, strides are not acceptable in strided_slice where the stride > 1 (or < -1).\r\n\r\nThis uses a concatenation of normal slices to implement strided_slice.\r\n\r\nFor backends where all tensor slicing/padding/concating/striding are just index manipulation, this implementation is as efficient as any other.\r\n", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "agreed that Slice should probably have strides as a parameter in the long run", "@tensorflow-jenkins test this please"]}, {"number": 9811, "title": "Tensorflow Session Memory Leak", "body": "### System information\r\n- **Have I written custom code :                                     YES\r\n- **OS Platform and Distribution :                                   Mac OS\r\n- **TensorFlow installed from (source or binary)**:      From Source\r\n- **TensorFlow version (use command below)**:         tensorflow (1.1.0)\r\n- **Bazel version (if compiling from source)**:             bazel-0.4.5\r\n- **CUDA/cuDNN version**:                                            not used\r\n- **GPU model and memory**:                                        not used\r\n- **Exact command to reproduce**:                               used in an app\r\n\r\n\r\n### Describe the problem\r\n\r\nI am using the tensorlfow C++ API and I have linked the tensor flow framework to perform a prediction using an inference.pb file.\r\n\r\nThe inference works but I have a Memory leak which (according to instruments) is linked to the session->Close().\r\n\r\nMy function is:\r\n\r\n```\r\n- (double) MLPredictionObjC: (float[24]) inputFeatures {\r\n    double results[]={0.0};\r\n\r\n    NSString *path = [[NSBundle mainBundle] pathForResource:@\"inference84\" ofType:@\"pb\"];\r\n\r\n    if ([self loadGraphFromPath:path] && [self createSession]) {\r\n        double resultsP = [self predictML:inputFeatures];\r\n        session->Close();\r\n        return resultsP;\r\n    }\r\n    return *results;\r\n}\r\n```\r\n\r\n// the other functions associated with loading and creating the session are:\r\n\r\n```\r\n- (BOOL)loadGraphFromPath:(NSString *)path  \r\n{\r\n    ReadBinaryProto(tensorflow::Env::Default(), path.fileSystemRepresentation, &graph);\r\n    return YES;\r\n}\r\n\r\n- (BOOL)createSession\r\n{\r\n        tensorflow::SessionOptions options;\r\n        tensorflow::NewSession(options, &session);\r\n        session->Create(graph);\r\n        return YES;\r\n}\r\n```\r\n\r\nI am not sure how to resolve this? should I be using std::unique_ptr session?\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9810, "title": "how to install the previous tf version on windows (based python2.7)? like 0.9 or 0.1....", "body": "", "comments": ["We added support for Windows in version 0.12. Previous versions do not work on Windows."]}, {"number": 9809, "title": "Bug of CPU detection?", "body": "### System information\r\nLinux 4.9.0-kali4-686-pae #1 SMP Debian 4.9.25-1kali1 (2017-05-04) i686 GNU/Linux\r\nTensorFlow installed from source code\r\nTensorFlow version : 1.0.1\r\nBazel version : 0.4.2\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\n\r\n> \r\n> == cat /etc/issue ===============================================\r\n> Linux 4.9.0-kali4-686-pae #1 SMP Debian 4.9.25-1kali1 (2017-05-04) i686 GNU/Linux\r\n> VERSION=\"2017.1\"\r\n> VERSION_ID=\"2017.1\"\r\n> \r\n> == are we in docker =============================================\r\n> No\r\n> \r\n> == compiler =====================================================\r\n> c++ (Debian 6.3.0-16) 6.3.0 20170425\r\n> Copyright (C) 2016 Free Software Foundation, Inc.\r\n> This is free software; see the source for copying conditions.  There is NO\r\n> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n> \r\n> \r\n> == uname -a =====================================================\r\n> Linux 4.9.0-kali4-686-pae #1 SMP Debian 4.9.25-1kali1 (2017-05-04) i686 GNU/Linux\r\n> \r\n> == check pips ===================================================\r\n> numpy (1.12.1)\r\n> \r\n> == check for virtualenv =========================================\r\n> False\r\n> \r\n> == tensorflow import ============================================\r\n> Traceback (most recent call last):\r\n>   File \"<string>\", line 1, in <module>\r\n>   File \"tensorflow/__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import *\r\n>   File \"tensorflow/python/__init__.py\", line 72, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"tensorflow/python/__init__.py\", line 61, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n> ImportError: cannot import name pywrap_tensorflow\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> == env ==========================================================\r\n> LD_LIBRARY_PATH is unset\r\n> DYLD_LIBRARY_PATH is unset\r\n> \r\n> == nvidia-smi ===================================================\r\n> \r\n> == cuda libs  ===================================================\r\n\r\n### Describe the problem\r\nI compiled it from source,  and got this fatal error when try to import tensorflow in python3.5.3:\r\n\r\n\r\n```\r\nPython 3.5.3 (default, Jan 19 2017, 14:11:04)\r\n[GCC 6.3.0 20170118] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nF tensorflow/core/platform/cpu_feature_guard.cc:35] The TensorFlow library was compiled to use SSE instructions, but these aren't available on your machine.\r\nAborted\r\n```\r\n\r\nAnd here is the information of my CPU:\r\n\r\n```\r\n.......\r\nprocessor    : 3\r\nvendor_id    : GenuineIntel\r\ncpu family    : 6\r\nmodel        : 37\r\nmodel name    : Intel(R) Core(TM) i3 CPU       M 370  @ 2.40GHz\r\nstepping    : 5\r\nmicrocode    : 0x4\r\ncpu MHz        : 933.000\r\ncache size    : 3072 KB\r\nphysical id    : 0\r\nsiblings    : 4\r\ncore id        : 2\r\ncpu cores    : 2\r\napicid        : 5\r\ninitial apicid    : 5\r\nfdiv_bug    : no\r\nf00f_bug    : no\r\ncoma_bug    : no\r\nfpu        : yes\r\nfpu_exception    : yes\r\ncpuid level    : 11\r\nwp        : yes\r\nflags        : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe nx rdtscp lm constant_tsc arch_perfmon pebs bts xtopology nonstop_tsc aperfmperf eagerfpu pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm arat\r\nbugs        :\r\nbogomips    : 4787.91\r\nclflush size    : 64\r\ncache_alignment    : 64\r\naddress sizes    : 36 bits physical, 48 bits virtual\r\n......\r\n\r\n```\r\n\r\nthat's all.", "comments": ["@petewarden Any idea what's happening here?  Core i3 certainly has SSE, as indicated in the flags.", "FYI I'm also seeing this (and I'm also using i686).\r\n\r\nI don't understand these things so could be wrong, but it seems like PLATFORM_IS_X86 isn't set when building on i686 with gcc [1][2], and if PLATFORM_IS_X86 isn't set then the CPU info checks fail fast [3]. \r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/platform.h#L59\r\n[2] https://sourceforge.net/p/predef/wiki/Architectures/\r\n[3] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/cpu_info.cc#L313", "Unfortunately we don't test on 32-bit x86 here, since it's not an officially supported platform and we don't have machines set up, so it's hard for us to do a fix. We'd welcome a PR with an update though, I'm guessing adding the right macro check at [1] would get things running?", "Actually, I take that back. We do have a supported platform we can test on, older iPhone emulators use 32-bit x86, as shown in bug #8914. Since we're tracking this issue there, closing this as a duplicate."]}, {"number": 9808, "title": "multiple kernel_constraint error: keyword argument repeated", "body": "When I did a conv2d operation like this:\r\n`    model.add(Convolution2D(k_size_0, 3, 3, init='he_normal', border_mode='valid', \r\n                        input_shape=(image_width, image_height, 1), \r\n                        kernel_constraint=max_norm(3., axis=0), \r\n                        kernel_constraint=nonneg(),\r\n                         activity_regularizer = keras.regularizers.l1(0.0000008)))`\r\n\r\nI put **two** constraints for `kernel_constraint` and there was an error:\r\n`SyntaxError: keyword argument repeated`\r\n\r\nSo can I pass a **list** with multiple items to the kernel_constraint argument instead of only one?\r\nThank you", "comments": []}, {"number": 9807, "title": "Fixes #9654: Allow model_fn being a member function of a class", "body": "Fixes #9654 ", "comments": ["@tensorflow-jenkins Test this please", "Should be fine. Classic Jenkins issue: `ERROR: Couldn't find any revision to build. Verify the repository and branch configuration for this job.` in `cpu-cmake` test. ", "CI is offline. Hopefully back soon, but no point trying to run tests right now.", "Thanks. Fixed! @ispirmustafa @martinwicke ", "@tensorflow-jenkins Test this please ", "Can one of the admins verify this patch?"]}, {"number": 9806, "title": "(pandas) read_csv(compression='gzip') fails while reading compressed file with tf.gfile.GFile in Python 2", "body": "Originally I opened an issue on pandas, apparently it maybe some bug in tensorflow side and they asked to be verified here. The original issue is pandas-dev/pandas#16241\r\n\r\nI'm pasting bellow the same steps I used to replicate the behavior in the original issue. The last comment from pandas was about: \"At a glance, it looks like gfile.GFile doesn't follow python's IO interface for seek\" (please see more details on the original issue linked above)\r\n\r\n#### Code Sample, a copy-pastable example if possible\r\n\r\nSample (1)\r\n```python\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nwith tf.gfile.GFile('test.csv') as f: pd.read_csv(f)\r\n```\r\n\r\nSample (2)\r\n```python\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nwith tf.gfile.GFile('test.csv.gz') as f: pd.read_csv(f, compression='gzip')\r\n```\r\n\r\n#### Problem description\r\n\r\nI'm converting some code to run on Google Cloud, and in the process I'm changing the way my datasets are read. I started using tf.gfile.GFile implementation from Tensorflow, as it is portable and can read both local files and files from storage buckets.\r\n\r\nAlso in the process I'm changing my code to work with Python 2 instead of Python 3.\r\n\r\nNot sure if it is a bug in Pandas or Tensorflow code, but this issue seems similar to #14222, so I'm opening an issue here first.\r\n\r\nTo reproduce, create two files: test.csv and test.csv.gz locally. Run both samples in python 2. The sample (1) works fine, but sample (2) crashes with an error: \"AttributeError: 'NoneType' object has no attribute 'Tell'\"\r\n\r\nStrangely, both samples work fine in python 3. I'm using the same library versions in python 2 and 3: pandas 0.19.2 and tensorflow 1.0.\r\n\r\n#### Expected Output\r\n\r\nBoth samples should work in python 2.\r\n\r\n#### Output of ``pd.show_versions()``\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 2.7.13.final.0\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 16.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.None\r\n\r\npandas: 0.19.2\r\nnose: None\r\npip: 9.0.1\r\nsetuptools: 32.1.0\r\nCython: None\r\nnumpy: 1.12.0\r\nscipy: 0.19.0\r\nstatsmodels: None\r\nxarray: None\r\nIPython: 5.3.0\r\nsphinx: None\r\npatsy: None\r\ndateutil: 2.5.2\r\npytz: 2016.10\r\nblosc: None\r\nbottleneck: None\r\ntables: None\r\nnumexpr: None\r\nmatplotlib: 2.0.0\r\nopenpyxl: None\r\nxlrd: None\r\nxlwt: None\r\nxlsxwriter: None\r\nlxml: None\r\nbs4: None\r\nhtml5lib: None\r\nhttplib2: 0.10.3\r\napiclient: None\r\nsqlalchemy: None\r\npymysql: None\r\npsycopg2: None\r\njinja2: None\r\nboto: None\r\npandas_datareader: None\r\n</details>\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: see sample code above\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macos\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.0.0\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: see above\r\n\r\n", "comments": ["I'm not sure what file interface you are expecting from panda. It is looking for `Tell`, but we have `tell`, which is the python `FileIO` interface.\r\n\r\n```\r\n>>> f = tf.gfile.GFile('/tmp/foo.txt')\r\n>>> print dir(f)\r\n['_FileIO__mode', '_FileIO__name', '__class__', '__delattr__', '__dict__', '__doc__', '__enter__', '__exit__', '__format__', '__getattribute__', '__hash__', '__init__', '__iter__', '__module__', '__new__', '__next__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_binary_mode', '_prepare_value', '_preread_check', '_prewrite_check', '_read_buf', '_read_check_passed', '_writable_file', '_write_check_passed', 'close', 'flush', 'mode', 'name', 'next', 'read', 'readline', 'readlines', 'seek', 'size', 'tell', 'write']\r\n>>> ff = open('/tmp/foo.txt', 'r')\r\n>>> print dir(ff)\r\n['__class__', '__delattr__', '__doc__', '__enter__', '__exit__', '__format__', '__getattribute__', '__hash__', '__init__', '__iter__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'close', 'closed', 'encoding', 'errors', 'fileno', 'flush', 'isatty', 'mode', 'name', 'newlines', 'next', 'read', 'readinto', 'readline', 'readlines', 'seek', 'softspace', 'tell', 'truncate', 'write', 'writelines', 'xreadlines']\r\n```", "With tensorflow 1.1.0, the exception is a bit different. Running\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndf = pd.DataFrame(np.random.randn(10, 2))\r\ndf.to_csv(\"foo.csv.gz\", compression=\"gzip\")\r\n\r\npd.read_csv(\"foo.csv.gz\", compression=\"gzip\")\r\n\r\nwith tf.gfile.GFile(\"foo.csv.gz\") as gf:\r\n    pd.read_csv(gf, compression=\"gzip\")\r\n```\r\n\r\ngives,\r\n\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 11, in <module>\r\n    pd.read_csv(gf, compression=\"gzip\")\r\n  File \"/Users/taugspurger/Envs/py27/lib/python2.7/site-packages/pandas/pandas/io/parsers.py\", line 655, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"/Users/taugspurger/Envs/py27/lib/python2.7/site-packages/pandas/pandas/io/parsers.py\", line 405, in _read\r\n    parser = TextFileReader(filepath_or_buffer, **kwds)\r\n  File \"/Users/taugspurger/Envs/py27/lib/python2.7/site-packages/pandas/pandas/io/parsers.py\", line 762, in __init__\r\n    self._make_engine(self.engine)\r\n  File \"/Users/taugspurger/Envs/py27/lib/python2.7/site-packages/pandas/pandas/io/parsers.py\", line 966, in _make_engine\r\n    self._engine = CParserWrapper(self.f, **self.options)\r\n  File \"/Users/taugspurger/Envs/py27/lib/python2.7/site-packages/pandas/pandas/io/parsers.py\", line 1582, in __init__\r\n    self._reader = parsers.TextReader(src, **kwds)\r\n  File \"pandas/_libs/parsers.pyx\", line 562, in pandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:6175)\r\n  File \"pandas/_libs/parsers.pyx\", line 751, in pandas._libs.parsers.TextReader._get_header (pandas/_libs/parsers.c:9268)\r\n  File \"pandas/_libs/parsers.pyx\", line 953, in pandas._libs.parsers.TextReader._tokenize_rows (pandas/_libs/parsers.c:11755)\r\n  File \"pandas/_libs/parsers.pyx\", line 2173, in pandas._libs.parsers.raise_parser_error (pandas/_libs/parsers.c:28589)\r\nTypeError: seek() takes exactly 2 arguments (3 given)\r\n```\r\n\r\n```\r\nIn [15]: gf.seek?\r\nSignature: gf.seek(position)\r\n```", "I *think* this should be fixed in master.  Here's the `seek()` signature from HEAD:\r\n```python\r\n  def seek(self, offset=None, whence=0, position=None):\r\n```\r\n[Link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py#L131).  Can you retry with a nightly build?", "It is working as expected with the nightly build.", "Yay!", "I'm seeing this again for python2 `pandas (0.20.1)` and would love to see zlib used instead of gzip because when i handle the files myself with zlib the issue is fixed but when pandas `pd.read_csv(filename, compression='gzip', error_bad_lines=False, chunksize=chunksize)` is used it fails miserably too often."]}, {"number": 9805, "title": "Squelch warnings related to API deprecation in mnist.py example.", "body": "For reference:\r\n\r\n```\r\nWARNING:tensorflow:From /home/.../head.py: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nWARNING:tensorflow:From mnist.py: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\r\n```\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please\r\n", "@khan-faiz this appears to break //bazel_pip/tensorflow/contrib/learn:head_test. https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/4827/consoleFull\r\n\r\nCan you take a look?"]}, {"number": 9804, "title": "OpenCL support", "body": "When will TensorFlow be supporting OpenCL?", "comments": ["Closing this out as a duplicate of #22 , which tracks OpenCL support."]}, {"number": 9803, "title": "Add reference for crelu and relu6.", "body": "Change the reference for crelu to a clickable link.\r\nAdd reference paper for relu6.", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?"]}, {"number": 9802, "title": "DOC: formats momentum calculation for web", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.\r\n", "Is this change intended for 1.1 branch? We don't usually accept changes to released branches. Could you make the change in master branch instead?", "Changed.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Yes, I'm okay with this PR being merged into master regardless of any other commits authored by others.", "@tensorflow-jenkins test this please", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9801, "title": "ValueError: Cannot feed value of shape (1, 40, 3) for Tensor u'cls1_fc_pose_xyz_target:0', which has shape '(?, ?)", "body": "I have installed keras using conda with tensorflow backend. I am experimenting with the keras version of posenet architecture. I have introduced a few LSTM layers into the original architecture and now my input should be of 5 dim(n_batch,n_frame,row,col,channel) and my model compiles correctly but when I call the fit function, it throws following **error:**\r\n\r\n    Train on 129 samples, validate on 129 samples\r\n\r\n    Epoch 1/800\r\n\r\n    Traceback (most recent call last):\r\n\r\n    File \"train.py\", line 72, in <module>\r\n\r\n    callbacks=[checkpointer])\r\n\r\n    File \"/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/keras/engine/training.py\", line 1485, in fit\r\n\r\n    initial_epoch=initial_epoch)\r\n\r\n    File \"/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/keras/engine/training.py\", line 1140, in _fit_loop\r\n\r\n    outs = f(ins_batch)\r\n\r\n    File \"/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2073, in __call__\r\n\r\n    feed_dict=feed_dict)\r\n\r\n    File \"/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n\r\n    run_metadata_ptr)\r\n\r\n    File \"/u/some_user/conda-envs/my_root2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 944, in _run\r\n\r\n    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\r\n\r\n    ValueError: Cannot feed value of shape (1, 40, 3) for Tensor u'cls1_fc_pose_xyz_target:0', which has shape '(?, ?)'\r\n`\r\n\r\nand here the piece of code that I am using in **training**:\r\n\r\n    X_train=np.squeeze(np.array(dataset_train.images,dtype=float))\r\n\r\n    X_test=np.squeeze(np.array(dataset_test.images,dtype=float))\r\n\r\n    y_train=np.squeeze(np.array(dataset_train.poses,dtype=float))\r\n\r\n    print(\"X_train shape:\"+str(X_train.shape))#X_train shape:(129, 40, 224, 224, 3)\r\n\r\n    print(\"y_train shape:\"+str(y_train.shape))#y_train shape:(129, 40, 7)\r\n\r\n    y_train_x = y_train[:,:,0:3]\r\n\r\n    y_train_q = y_train[:,:,3:7]\r\n\r\n    y_test = np.squeeze(np.array(dataset_test.poses))\r\n\r\n    print(\"X_test shape:\"+str(X_test.shape))#X_test shape:(129, 40, 224, 224, 3)\r\n\r\n    print(\"y_test shape:\"+str(y_test.shape))#y_test shape:(129, 40, 7)\r\n\r\n    y_test_x = y_test[:,:,0:3]\r\n\r\n    y_test_q = y_test[:,:,3:7]\r\n\r\n    #Setup checkpointing\r\n    checkpointer = ModelCheckpoint(filepath=\"checkpoint_weights.h5\", verbose=1, save_best_only=True, save_weights_only=True)\r\n\r\n    model.fit(X_train, [y_train_x, y_train_q, y_train_x, y_train_q, y_train_x, y_train_q],\r\n      batch_size=batch_size,\r\n      nb_epoch=800,\r\n      verbose=1,\r\n      validation_data=(X_test, [y_test_x, y_test_q, y_test_x, y_test_q, y_test_x, y_test_q]),\r\n      callbacks=[checkpointer])\r\nand here is the **input** of my model:\r\n\r\n    input = Input(shape=(helper.frames_per_sequence,224, 224, 3))\r\n\r\nand here is the **lines causing error**:\r\n\r\n    cls1_fc1_flat = Flatten()(cls1_reduction_pose)\r\n\r\n    cls1_fc1_pose = Dense(1024,activation='relu',name='cls1_fc1_pose')(cls1_fc1_flat)\r\n\r\n    cls1_fc_pose_xyz = Dense(3,name='cls1_fc_pose_xyz')(cls1_fc1_pose)\r\n\r\n    cls1_fc_pose_wpqr = Dense(4,name='cls1_fc_pose_wpqr')(cls1_fc1_pose)", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9800, "title": "TensorBoard relies on a version of \"tsify\" which includes an incompatible TypeScript version", "body": "The version of \"tsify\" in the package.json is:\r\n\r\n\"tsify\": \"^0.14.8\"\r\n\r\nbut this package includes a 1.x version of typescript. Therefore, compiling certain constructs e.g. \"number | null\" fails.\r\n\r\nThis can be (seemingly) resolved by changing the version of tsify to a more recent version not relying on its own TypeScript package.\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from**: Source (master)\r\n- **TensorFlow version**: last commit to tensorboard @ 9dd8e7aec9d3a8ddc458af01c2e51541ad876fb8\r\n- **Exact command to reproduce**: \"gulp\"\r\n", "comments": ["@dandelionmane can you take a look or reassign?", "Thanks to hard work by @jart, we no longer use gulp or package.json."]}, {"number": 9799, "title": "TensorBoard \"gulp\" assigns to const", "body": "The TensorBoard build is all kinds of broken, but the latest introduction to the tree includes an assignment to a const within the gulpfile.\r\n\r\nIn particular:\r\n\r\ntensorflow/tensorboard/gulp_tasks/compiler.js:51 reassigns to the variable \"entries\" which is marked with const.\r\n\r\nThe same occurs on the next line for the variable \"deps\".\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from**: Source (master)\r\n- **TensorFlow version**: last commit to tensorboard @ 9dd8e7aec9d3a8ddc458af01c2e51541ad876fb8\r\n- **Exact command to reproduce**: \"gulp\"\r\n", "comments": ["@dandelionmane can you take a look or reassign?", "We've deleted the gulp build! Now everything builds using bazel. Please check it out at our new repo: https://github.com/tensorflow/tensorboard.\r\n\r\nh/t to @jart who did a ton of awesome work to get rid of gulp and switch us to bazel."]}, {"number": 9798, "title": "What will dynamic_rnn reuse when set reuse of variable_scope as true", "body": "I'm sorry but I havn't found a clear answer in StackOverflow\r\n\r\nI wonder what will dynamic_rnn reuse when set reuse of variable_scope as true, only the gate parameters will be share or including the output & status?\r\n\r\nThe example is that my code wanna process two sequence with different length by one bidirectional LSTM network, and then compare them for further use. Will I set reuse as True when process the second sequence at one step? Do I need to clear the state in this situation?\r\n", "comments": ["Short answer is `reuse=True` only affects the `tf.Variable` objects.  @ebrevdo on the `dynamic_rnn()` part.", "Depends on if you're using tf 1.1 or the tf nighties.  If you're using tf\n1.1 then you need to reuse the variable scope from the first call to\ndynamic rnn to ensure you're using the same parameters in the second call.\nIn the nighties, so long as you use the same cell object in the second\ncall, your parameters will be shared regardless of the variable scope/\nreuse value.\n\nOn May 9, 2017 4:17 PM, \"Zongheng Yang\" <notifications@github.com> wrote:\n\n> Short answer is reuse=True only affects the tf.Variable objects. @ebrevdo\n> <https://github.com/ebrevdo> on the dynamic_rnn() part.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9798#issuecomment-300328486>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzQTplyY9WIYJzFjD9U6RNVHrEG4ks5r4PQPgaJpZM4NVk7U>\n> .\n>\n", "@ebrevdo \r\nThx for your answer. I've take a quick glimpse at the rnn_ops.py, it seems the dynamic_rnn call will auto clear the cells' state unless I wanna keep them isn't? So I can just set the reuse as True in the second call thus the code can run as I want regardless of the TF version?", "That's right. Furthermore, If you want to keep state between two calls to\ndynamic_rnn, you'd have to take the final state from the first call and\npass it in as the initial state in the second call. This of very rarely\ndone in practice.\n\nOn May 9, 2017 11:24 PM, \"ruiann\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo>\n> Thx for your answer. I've take a quick glimpse at the rnn_ops.py, it seems\n> the dynamic_rnn call will auto clear the cells' state unless I wanna keep\n> them isn't? So I can just set the reuse as True in the second call thus the\n> code can run as I want regardless of the TF version?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9798#issuecomment-300387379>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7jljxrmp4YzHhsVIBHzjACRNW1Cks5r4VgpgaJpZM4NVk7U>\n> .\n>\n"]}]