[{"number": 47056, "title": "Why is snappy compression output buffer size so small?", "body": "Hi @frankchn,\r\nThe snappy compression buffer size is hard coded to 262144 bytes:\r\nhttps://github.com/tensorflow/tensorflow/blob/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/core/lib/io/snappy/snappy_compression_options.h#L30. This is a quite small number leading to high chance of hitting the error:\r\nhttps://github.com/tensorflow/tensorflow/blob/516ae286f6cc796e646d14671d94959b129130a4/tensorflow/core/lib/io/snappy/snappy_inputstream.cc#L113\r\n\r\nI wonder why it is set to thus small. \r\n\r\nFurthermore, `output_buffer_` is a fixed sized array https://github.com/tensorflow/tensorflow/blob/516ae286f6cc796e646d14671d94959b129130a4/tensorflow/core/lib/io/snappy/snappy_inputstream.h#L73\r\n\r\nWhy can't it be a vector so that we don't need the `output_buffer_size` at all? I'm happy to submit a PR to make the according change if needed.\r\n\r\nThanks.\r\n", "comments": ["1) The maximum block size in the current snappy implementation is 64KB, so in practice users are never going to hit that error. See https://github.com/google/snappy/blob/master/snappy.h#L190-L198\r\n\r\n2) We are using a raw pointer into the `char[]` array in the `next_out_` variable (snappy_inputstream.cc:121 and snappy_inputstream.cc:136). If we were using `std::vector` and extracting the raw pointer using `std::vector::data()`, then we need to account for `std::vector` performing reallocations, and thus invalidating the existing `next_out_` pointer. A raw memory allocation avoids this issue altogether.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47056\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47056\">No</a>\n", "@frankchn thanks for the detailed explanation.\r\n1. It means SnappyInputStream relies on an external lib's internal implementation to determine its behavior and it's tempting to think it's an anti-pattern.\r\n\r\n2. the 64KB seems not taking effect as we are hitting the error https://github.com/tensorflow/tensorflow/blob/516ae286f6cc796e646d14671d94959b129130a4/tensorflow/core/lib/io/snappy/snappy_inputstream.cc#L113", "Do you have a test case / reproduction of the error you are encountering?\r\n\r\nThe implementation of the snappy library is also controlled by Google, so I don't think it is a problem in practice."]}, {"number": 47055, "title": "Added a run_ makefile target for non-test binaries.", "body": "Also, the x86 test behavior has changed:\r\n  * removed the unnecessary testing script for x86 with Make and bazel.\r\n  * This change means that we no longer need a special skylark rule for tflite_micro_cc_test and can instead directly use cc_test.\r\n  * All the logs from the test are now visible on the terminal (previously we would only see logs on errors, which can be annoying\r\n    for debugging)\r\n\r\nTested that the following commands:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_keyword_benchmark -j8\r\nmake -f tensorflow/lite/micro/tools/make/Makefile run_keyword_benchmark -j8\r\n```\r\nexecute without any error.\r\n\r\nThe output is someting like:\r\n```\r\nInitializeKeywordRunner() took 39 ticks (0 ms)\r\nKeywordRunNIerations(1) took 42 ticks (0 ms)\r\nKeywordRunNIerations(10) took 187 ticks (0 ms)\r\n```\r\n\r\nPrior to this change, benchmarks would have to be run with\r\n`make test_keyword_benchmark` which would give a confusing output:\r\n\r\n```\r\nInitializeKeywordRunner() took 85 ticks (0 ms)\r\nKeywordRunNIerations(1) took 27 ticks (0 ms)\r\nKeywordRunNIerations(10) took 276 ticks (0 ms)\r\nmake: *** [tensorflow/lite/micro/benchmarks/Makefile.inc:28: test_keyword_benchmark] Error 1\r\n```\r\n\r\nFixes http://b/168123200\r\nAnother relevant bug (for the asan failures): http://b/179930607", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> Just to make sure, this has no effect on existing tests right? like if you run kernel_conv_test the output and behavior is the same?\r\n\r\ncorrect. everything stays the same for the tests."]}, {"number": 47054, "title": "tf.keras.initializers.zeros causes model.save to fail, while tf.keras.initializers.Zeros() works great", "body": "[Here](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Zeros) it's written that `tf.keras.initializers.zeros` is a shortcut for `tf.keras.initializers.Zeros()`.\r\nIf it is a shortcut, then both should work same\r\n\r\n\r\nWhile saving the model, if I use `tf.keras.initializers.zeros` , model save failes, but using `tf.keras.initializers.Zeros()` works great.\r\n\r\nSame issue raised by someone at [Stackoverflow](https://stackoverflow.com/questions/57154799/keras-model-saving-erroring-typeerror-get-config-missing-1-required-position) \r\n\r\n[Colab](https://colab.research.google.com/drive/1E0P-aBU9B7RO_QUtDrlRDPcOWqd6UfkD?usp=sharing#scrollTo=weBjeZAFJOP4)\r\n\r\n", "comments": ["I'm new to open source, but to clarify, are you calling or referencing the function directly? If you reference the function object rather than initialize a new one (i.e. `tf.keras.initializers.zeros` instead of `tf.keras.initializers.zeros()`) without calling it, it will not work.", "@fawazahmed0 \r\nPlease provide with minimal stand alone indented code to replicate the issue reported or if possible share a colab gist with the error.\r\nThe usage of these depends on the context they are used in,Zeros behavior is more suitable for including it inside models and serializing them.", "Well, I was going through, [timeseries forecasting tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series) and when I tried to save the model it failed, I just had to change `tf.initializers.zeros` to `tf.initializers.zeros()` to make it working.\r\n\r\nHere is the [colab](https://colab.research.google.com/drive/1E0P-aBU9B7RO_QUtDrlRDPcOWqd6UfkD?usp=sharing#scrollTo=weBjeZAFJOP4)\r\n\r\n ", "Minimal code sample which reproduces the error ([colab](https://colab.research.google.com/drive/1ehWxx6PaDKP6nFMaI2Nf84O3-Xyv3-Sm?usp=sharing) link):\r\n\r\n```python\r\nimport tensorflow as tf   # issue was reproduced with tf.__version__ == 2.4.1\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Reshape((-1, 784)),\r\n    tf.keras.layers.Lambda(lambda x: tf.divide(tf.cast(x, tf.float32), 255.)),\r\n    tf.keras.layers.Dense(256, activation='relu', bias_initializer=tf.initializers.zeros),\r\n    tf.keras.layers.Dense(10, activation='softmax',)\r\n])\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nmodel.compile('adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\nhistory = model.fit(x_train, y_train, validation_data=(x_test, y_test))\r\nmodel.save('test')\r\n```\r\n\r\nexception:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<...>\r\n\r\nTypeError: get_config() missing 1 required positional argument: 'self'\r\n```\r\n\r\nThe same code with tf.initializers.Zeros() would work fine `tf.initializers.Zeros()`, as @fawazahmed0 mentioned:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Reshape((-1, 784)),\r\n    tf.keras.layers.Lambda(lambda x: tf.divide(tf.cast(x, tf.float32), 255.)),\r\n    tf.keras.layers.Dense(256, activation='relu', bias_initializer=tf.initializers.Zeros()),\r\n    tf.keras.layers.Dense(10, activation='softmax',)\r\n])\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nmodel.compile('adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\nhistory = model.fit(x_train, y_train, validation_data=(x_test, y_test))\r\nmodel.save('test')\r\n```\r\n", "I guess it could be simplified to the following gist ([colab](https://colab.research.google.com/drive/1jqONjLpZe1V6DYt9XulC9YFVifetUnFm?usp=sharing)):\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nlayer = tf.keras.layers.Dense(256, bias_initializer=tf.initializers.zeros)\r\nlayer.get_config()\r\n\r\nOut: \r\n...\r\nTypeError: get_config() missing 1 required positional argument: 'self'\r\n```\r\n\r\nBut the following snippet works just fine:\r\n```python\r\nimport tensorflow as tf\r\n\r\nlayer = tf.keras.layers.Dense(256, bias_initializer=tf.initializers.Zeros())\r\nlayer.get_config()\r\nOut: \r\n{'activation': 'linear',\r\n 'activity_regularizer': None,\r\n 'bias_constraint': None,\r\n 'bias_initializer': {'class_name': 'Zeros', 'config': {}},,\r\n ...\r\n 'units': 256,\r\n 'use_bias': True}\r\n```\r\n\r\nThe same logic holds for other initializers. For example, `tf.initializers.he_uniform` will fail with the same error if `get_config` is called, but  `tf.initializers.HeUniform` will not.\r\n\r\nI'm not sure if it is a problem or it's by design. But it's at least confusing because models trained with \"`tf.initializers.zeros` - like\" initializers use them properly except for the model serialization.  \r\n", "@mishc9 To me, it appears that the `tf.initializers.zeros` shortcut function was intended to be _called_ rather than referenced as-is, i.e. it generates an object of the `tf.keras.initializers.Zeros` class. The class and the shortcut function are exported with the same `@keras_export` decorator in the source code. \r\n\r\nIf you run the colab with `tf.initializers.zeros()` in place of `tf.initializers.zeros`, it works just fine.", "@MaanasArora you're right, it should be provided to the layer constructor as `tf.initializers.zeros()` (better way to do this), not  `tf.initializers.zeros`. But the strange thing is that it still would work even in the later case ([colab](https://colab.research.google.com/drive/1R1K3olMovqhX7z7qW1AwbmhbK12WeEjq#scrollTo=In65cIrYpBst)).\r\n\r\nI think this behaviour should be treated as an issue if the way to provide an initializer in the form of `tf.initializers.zeros` is allowed and it initializers weights the right way (because it's the main purpose of initializers). \r\n\r\nOn the other hand, if this way to use initializers is not a 'regular' way to use them and trained model could fail, there should be a warning (at least, may be exception) during the model build/compile stage in my opinion. Not sure about backward compatibility - raise an exception could break some old code. It is 'correct' way to initialize weights & biases in the [standalone](https://github.com/keras-team/keras) `keras` API. ", "@mishc9 Yes, it oddly works in both cases and it's inconsistent because `tf.initializers.zeros` only creates issues during serialization.\r\n\r\nFurther, I noticed that when I passed `tf.initializers.Zeros` instead of `tf.initializers.Zeros`, it _also_ worked, and it also had the same error as `tf.initializers.zeros` when serializing the model. So I think that the issue is not with the shortcut function but that the class itself can be used directly without initializing an instance but breaks when serializing the model.\r\n\r\nLooking at the source, I suspect that the class itself was not intended to be passed to the layer constructor. I will review the source further to be sure.", "@MaanasArora that is interesting. I looked here and there in the source code too and still do not understand what's the reason of this issue :( \r\n[Function](https://github.com/tensorflow/tensorflow/blob/132437408620c947aaa43db31ce442a2b30dec12/tensorflow/python/keras/initializers/__init__.py#L150) `initializers.get` dispatches initializers:\r\n\r\n```python \r\n@keras_export('keras.initializers.get')\r\ndef get(identifier):\r\n  if identifier is None:\r\n    return None\r\n  if isinstance(identifier, dict):\r\n    return deserialize(identifier)\r\n  elif isinstance(identifier, six.string_types):\r\n    identifier = str(identifier)\r\n    return deserialize(identifier)\r\n  elif callable(identifier):\r\n    return identifier\r\n  else:\r\n    raise ValueError('Could not interpret initializer identifier: ' +\r\n                     str(identifier))\r\n```\r\n\r\nIt is invoked [here](https://github.com/tensorflow/tensorflow/blob/132437408620c947aaa43db31ce442a2b30dec12/tensorflow/python/keras/layers/core.py#L1164) in `__init__` of the layer and later, during 'build' stage, in `add_weight` [method](https://github.com/tensorflow/tensorflow/blob/132437408620c947aaa43db31ce442a2b30dec12/tensorflow/python/keras/engine/base_layer.py#L591). But it should return the identity of the provided object, because the `tf.initializers.zeros`/`tf.initializers.Zeros` is callable. And we couldn't use this object to initialize weights - it is just initializer class. So perhaps I missed something, probably would use debugger to localise the error. \r\n\r\n**Edit**\r\n\r\nJust found instantiation of type object initializer [here](https://github.com/tensorflow/tensorflow/blob/132437408620c947aaa43db31ce442a2b30dec12/tensorflow/python/keras/engine/base_layer_utils.py#L121):\r\n\r\n```python\r\n  else:\r\n    # Instantiate initializer if provided initializer is a type object.\r\n    if tf_inspect.isclass(initializer):\r\n      initializer = initializer()\r\n```\r\n\r\nSo, it seems like we could provide an initializer as a type object by design.\r\n\r\nFrames (`tf` of the older version `2.2.1`, so lines could differ now):\r\n<img width=\"405\" alt=\"image\" src=\"https://user-images.githubusercontent.com/15159090/107704354-28e39b80-6cce-11eb-9d34-3150363dc18c.png\">\r\n", "@mishc9 Note that the variable is instantiated in the `add_weight` method, so when `get_config` is called, the initializer attribute of the layer object is still a class. So, the problem is that the `get_config` method is not accessing the object that is created in `add_weight`, but rather the (class type) attribute itself.\r\n\r\nI'm not sure if it fits the design or function very well, but a possible solution would be to check if an attribute is a class during serialization and instantiate it if it is. Should I create a pull request with these changes?", "@MaanasArora yes, exactly. In a layer object initializer is still a class, because `getter` function called in `add_weight` does not change the layer object itself. \r\n\r\nI've tested locally following changes to the function `initialisers.get`: \r\n\r\n```python\r\n@keras_export('keras.initializers.get')\r\ndef get(identifier):\r\n  if identifier is None:\r\n    return None\r\n  if isinstance(identifier, dict):\r\n    return deserialize(identifier)\r\n  elif isinstance(identifier, six.string_types):\r\n    identifier = str(identifier)\r\n    return deserialize(identifier)\r\n  elif callable(identifier):\r\n    if inspect.isclass(identifier):  # Additional check copied from the snippet above\r\n      identifier = identifier()\r\n    return identifier\r\n  else:\r\n    raise ValueError('Could not interpret initializer identifier: ' +\r\n                     str(identifier))\r\n```\r\nand it worked. This function transforms type object initializer to the object initializer, as the `getter` in `add_weight` method does. \r\n\r\nI thought of changes in the serialisation function too. But it seems this solution could affect other serialised objects: activations, layers, regularizers etc. It's hards to handle all this cases carefully.  But `get` function is only for initializers. There's a cons to the `initializers.get` change too: type of provided argument will be changed implicitly. But it is true for `string` and `dict` initializers now.\r\n\r\nI would make a PR with this change, but have had a problem with tensorflow local unit testing \ud83e\udd37\u200d\u2642\ufe0f. Just could not run them. I would send PR without any additional unit tests covering this issue though. By the way, have you seen any additional intstructions how to dev & tests tensorflow on the local machine, except the contributing guidelines? Than would be great :) ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47054\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47054\">No</a>\n"]}, {"number": 47053, "title": "Weight becomes untrainable after multiplying with constant ", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macos\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**: tensorflow 2.4\r\n-   **Python version**: python 3.7\r\n\r\n### Describe the problem\r\nI want to mask the weights of my network (set them to zero and freeze them, i.e. no gradients). I tried creating a custom layer and multiply the weight matrices by a constant matrix, but this makes the weight matrices no longer trainable. Note that this approach used to work in tensorflow 1, and I can't figure out why it doesn't work anymore in tf2. I've attached an example in this [colab](https://colab.research.google.com/drive/1E1IdLuAFbQUKzrxoqN0XaZ79wb3au0nY?usp=sharing).\r\n", "comments": ["Try this\r\n\r\nhttps://colab.research.google.com/drive/1T7dxrXyaeO05aBqpOyonDyCMRSPTywaa?usp=sharing\r\n\r\nYour approach is not working because `self.kernel` is computed outside `GradientTape` scope.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47052, "title": "Prefer std::copy_n to naive for loop", "body": "Modern compiler is able to optimize (with -O1 or higher) std::copy_n to memmove for TriviallyCopyable type, which is usually faster, and fall back to naive for-loop for others. There is no reason to use naive for loop when trying to copy contiguous memory.", "comments": ["Edward is the right reviewer here."]}, {"number": 47051, "title": "java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model", "body": "Please, in desperate need for help, has been trying to solve for 10 days. the tensorflow lite model I trained is here  [here](uhttps://drive.google.com/file/d/1fYay6FXNlAXi-migvGRtaeqNXB8gKZXk/view?usp=sharingrl). I ran python inference test and it worked. However, no way it is working on Android sample object detection app here https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\n\r\nby debugging the issue this  long modelHandle = createModelWithBuffer(this.modelByteBuffer, errorHandle); specifically this part in  NativeInterpreterWrapper.class\r\n NativeInterpreterWrapper(ByteBuffer buffer, Options options) {\r\n        this.inferenceDurationNanoseconds = -1L;\r\n        this.isMemoryAllocated = false;\r\n        this.delegates = new ArrayList();\r\n        this.ownedDelegates = new ArrayList();\r\n        TensorFlowLite.init();\r\n        if (buffer != null && (buffer instanceof MappedByteBuffer || buffer.isDirect() && buffer.order() == ByteOrder.nativeOrder())) {\r\n            this.modelByteBuffer = buffer;\r\n            long errorHandle = createErrorReporter(512);\r\n            long modelHandle = createModelWithBuffer(this.modelByteBuffer, errorHandle);\r\n            this.init(errorHandle, modelHandle, options);\r\n        } else {\r\n            throw new IllegalArgumentException(\"Model ByteBuffer should be either a MappedByteBuffer of the model file, or a direct ByteBuffer using ByteOrder.nativeOrder() which contains bytes of model content.\");\r\n        }\r\n    }\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform MACOS sieera 10.13\r\n- Android studio 4\r\n- I have tried every possible solution and updated the NDK\r\nI used in the gradle\r\n\r\nbuildscript {\r\n    repositories {\r\n        google()\r\n        jcenter()\r\n        mavenLocal()\r\n    }\r\n\r\naaptOptions {\r\n        noCompress \"tflite\"\r\n        noCompress \"lite\"\r\n    }\r\n\r\n  implementation 'org.tensorflow:tensorflow-lite-metadata:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\nalso tried\r\n implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.2-nightly'  and no difference\r\n\r\nplease help me I don't know if it is the file it self or the android libraries ", "comments": ["Hi, please describe error in detail.", "Hello, I have solved it. I tried the following \r\ncomment the \r\n//apply from:'download_model.gradle'\r\nin app gridle. Don't try to download the model from a link.\r\nAlso, I removed that part from TFLiteObjectDetectionAPIModel\r\n\r\ntry (BufferedReader br =\r\n    new BufferedReader(\r\n        new InputStreamReader(\r\n            metadata.getAssociatedFile(labelFilename), Charset.defaultCharset()))) {\r\n  String line;\r\n  while ((line = br.readLine()) != null) {\r\n    Log.w(TAG, line);\r\n    d.labels.add(line);\r\n  }\r\n}\r\n\r\ninstead I added labels to the \"d\" directly and those labels are the classes of the custom trained model\r\nd.labels.add(\"driver\");\r\nd.labels.add(\"passenger\");\r\n\r\n ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47051\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47051\">No</a>\n"]}, {"number": 47050, "title": "ValueError: Cannot iterate over a shape with unknown rank", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Ubuntu 20.04\r\n- TensorFlow installation (pip package or built from source): pip (python 3.7)\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow-2.4.1\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n```python\r\nimport sys\r\nfrom pathlib import Path\r\n\r\nimport tensorflow as tf\r\n\r\n# Specify the model.\r\nsaved_model_dir = Path('training/Model/admin/test2/1/exported-model/1/')\r\n\r\nif saved_model_dir.exists():\r\n    print(f'Converting model: {str(saved_model_dir)}')\r\nelse:\r\n    print(f'Could not find model: {str(saved_model_dir)}')\r\n    sys.exit(1)\r\n\r\n# Convert the model.\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(str(saved_model_dir))\r\ntflite_model = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\nprint('Ready.')\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nError message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"convert-to-tflite.py\", line 17, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1947, in convert\r\n    return super(TFLiteConverter, self).convert()\r\n  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1304, in convert\r\n    **converter_kwargs)\r\n  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 606, in toco_convert_impl\r\n    input_tensors, output_tensors, *args, **kwargs)\r\n  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 497, in build_toco_convert_protos\r\n    for dim in shape:\r\n  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 861, in __iter__\r\n    raise ValueError(\"Cannot iterate over a shape with unknown rank.\")\r\nValueError: Cannot iterate over a shape with unknown rank.\r\n```\r\n\r\n### 4. Any other info / logs\r\n\r\nStartup log:\r\n\r\n```\r\n2021-02-09 20:09:36.646605: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/Qt/5.14.1/lib\r\n2021-02-09 20:09:36.646636: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nConverting model: training/Model/admin/test2/1/exported-model/1\r\n2021-02-09 20:09:38.125156: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-09 20:09:38.125298: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/Qt/5.14.1/lib\r\n2021-02-09 20:09:38.125310: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-02-09 20:09:38.125331: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (snowblower): /proc/driver/nvidia/version does not exist\r\n2021-02-09 20:09:38.125557: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-02-09 20:09:38.125891: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nWARNING:tensorflow:From /home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\n2021-02-09 20:09:38.792020: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n2021-02-09 20:09:38.858416: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3593310000 Hz\r\n2021-02-09 20:09:38.954802: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-09 20:09:39.889489: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2021-02-09 20:09:39.889750: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2021-02-09 20:09:39.890025: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-09 20:09:39.953928: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n\r\nWARNING:tensorflow:From /home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/util.py:327: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py:856: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\n2021-02-09 20:09:40.561332: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-09 20:09:41.338828: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-09 20:09:42.358175: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2021-02-09 20:09:42.358313: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2021-02-09 20:09:42.358532: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-09 20:09:42.426773: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\n```\r\n", "comments": ["@thijstriemstra is it possible to share your saved model to us to reproduce your problem on our side? Or, could you make reproducible steps in a gist?", "thanks for the feedback @abattery.\r\n\r\nTried to attach the model (created with tensorflow 1.15 using the https://github.com/emedvedev/attention-ocr project) but it's too big for github size (29,1Mb).\r\n\r\n\r\n\r\n", "Could you share it through google drive? @thijstriemstra ", "@abattery see https://we.tl/t-1LR2WA5myY", "Thanks @thijstriemstra We recommend using TFLiteConverter in TF v2 like the below one:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"...\")\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\nmodel = converter.convert()\r\n```\r\n\r\nHowever, it also failed with the following log and I will take a look at why the error happens:\r\n\r\n```\r\n<unknown>:0: error: loc(callsite(\"map_2/while/foldr/while/LoopCond@_functionalize_body_3\" at \"map_2/while/LoopCond\")): 'tfl.cast' op operand #0 must be tensor of 32-bit float or 1-bit signless integer or 16-bit signless integer or 32-bit signless integer or 64-bit signless integer or TFLite quint8 type or 8-bit unsigned integer or complex type with 32-bit float elements values, but got 'tensor<*xf64>'\r\n```", "thanks @abattery, fingers crossed you get it to work!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47050\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47050\">No</a>\n", "The fix is now available at HEAD. You can try the tomorrow version of tf-nightly if you depend the PIP version.", "thanks @abattery, that's awesome! I tried to digest your code fixes but it looks like black magic to me. I will give it a try and let you know.", "> We recommend using TFLiteConverter in TF v2 like the below one:\r\n\r\nI tested just now with this script as you suggested:\r\n\r\n```python\r\nimport sys\r\nfrom pathlib import Path\r\n\r\nimport tensorflow as tf\r\n\r\n# Specify the model.\r\nsaved_model_dir = Path('training/Model/admin/test2/1/exported-model/1/')\r\n\r\nif saved_model_dir.exists():\r\n    print(f'Converting model: {str(saved_model_dir)}')\r\nelse:\r\n    print(f'Could not find model: {str(saved_model_dir)}')\r\n    sys.exit(1)\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_dir))\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\nmodel = converter.convert()\r\n\r\n# Save the model.\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(model)\r\n\r\nprint('Ready.')\r\n```\r\n\r\n> You can try the tomorrow version of tf-nightly if you depend the PIP version.\r\n\r\nand these versions:\r\n\r\n```\r\ntb-nightly              2.5.0a20210223\r\ntensorboard-data-server 0.3.0\r\ntensorboard-plugin-wit  1.8.0\r\ntermcolor               1.1.0\r\ntf-estimator-nightly    2.5.0.dev2021022301\r\ntf-nightly-cpu          2.5.0.dev20210223\r\ntyping-extensions       3.7.4.3\r\n```\r\n\r\nBut getting this traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"convert-to-tflite.py\", line 20, in <module>\r\n    model = converter.convert()\r\n  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 788, in convert\r\n    result = _convert_saved_model(**converter_kwargs)\r\n  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 723, in convert_saved_model\r\n    enable_mlir_converter=True)\r\n  File \"/home/thijs/.virtualenvs/tflite/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 293, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"MutableHashTable\"): 'tf.MutableHashTableV2' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"MutableHashTable_lookup_table_insert/LookupTableInsertV2\"): 'tf.LookupTableInsertV2' op is neither a custom op nor a flex op\r\n<unknown>:0: error: failed while converting: 'main': \r\nSome ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom \r\nCustom ops: LookupTableInsertV2, MutableHashTableV2\r\nDetails:\r\n\ttf.LookupTableInsertV2 {_class = [\"loc:@MutableHashTable\"], device = \"\"}\r\n\ttf.MutableHashTableV2 {container = \"\", device = \"\", key_dtype = i64, shared_name = \"MutableHashTable\", use_node_name_sharing = true, value_dtype = !tf.string}\r\n```\r\n\r\nFull log attached: [log.txt](https://github.com/tensorflow/tensorflow/files/6029795/log.txt)", "I think this problem is fixed at HEAD. Could you try again with the recent tf-nightly?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47050\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47050\">No</a>\n", "FYI, https://github.com/tensorflow/tensorflow/commit/f77a48ea4807046996a84b476e56e1feba10ae69 fixed the issue.", "thanks @abattery, that worked!\r\n\r\n```\r\n$ ls -lha model.tflite \r\n-rw-rw-r-- 1 thijs thijs 31M mrt 19 15:44 model.tflite\r\n```\r\n\r\nI would expect the filesize to be considerably smaller (tensorflow **lite**?) than the original model but it seems it's the same size?", "Loading the converted model and [trying inference](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python) results in a segmentation fault. I opened #47996 for this."]}, {"number": 47049, "title": "\"no checkpoint found\" tensorboard projector", "body": "I want to load a custom embedding file and a metadata file into tensorboard's projector plugin. I've been following this tutorial mostly: https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin. I get the \"no checkpoint found\" error in spite of the checkpoint files being present in the folder. I also have used tensorboard's inspect function on the log directory and tensorboard can't find any event files. Here's my code:\r\n\r\n`import os\r\nfrom tensorboard.plugins import projector\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\nweights = pd.read_csv (\"test/vectors_test.tsv\", sep = '\\t')\r\n\r\nlog_dir = \"test/logs/\"\r\nweights = tf.Variable(weights)\r\ncheckpoint = tf.train.Checkpoint(embedding=weights)\r\ncheckpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\r\n\r\n# Set up config\r\nconfig = projector.ProjectorConfig()\r\nconfig.model_checkpoint_path = os.path.join(log_dir, \"embedding.ckpt\")\r\nembedding = config.embeddings.add()\r\nembedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\r\nembedding.metadata_path = \"test/metadata_test.tsv\"\r\nprojector.visualize_embeddings(log_dir, config)\r\n\r\n%load_ext tensorboard\r\n\r\n%tensorboard --logdir test/logs/ --host localhost --port=1003\r\n%tensorboard --inspect --logdir test/logs\r\n`", "comments": ["@AnnBkrv \r\n\r\nThis issue is more suitable for Tensorboard repo. Please post it on Tensorboard repo from [here](https://github.com/tensorflow/tensorboard/issues/new/choose). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47049\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47049\">No</a>\n"]}, {"number": 47048, "title": "Allow cloning of tf.keras.Model subclass", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs far as I know, there is no way to clone a class-based model currently. For functional models, we have `tf.keras.models.clone_model`, but this doesn't work for subclasses of `tf.keras.Model`. For example, if I define some model like so:\r\n```python\r\nclass MySequentialModel(tf.keras.Model):\r\n    def __init__(self, name=None, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense_1 = FlexibleDense(out_features=3)\r\n        self.dense_2 = FlexibleDense(out_features=2)\r\n\r\n    def call(self, x):\r\n        x = self.dense_1(x)\r\n        return self.dense_2(x)\r\n```\r\nThen train, save and load the model, when I try to clone it like this:\r\n```model = tf.keras.models.clone_model(loaded_model)```\r\nI get:\r\n```ValueError: Expected `model` argument to be a functional `Model` instance, but got a subclass model instead.```\r\nIs there another way to do this or am I missing something?\r\n**Will this change the current api? How?**\r\nNot really, you'd just be allowing an additional input type in the `tf.keras.models.clone_model` function.\r\n**Who will benefit with this feature?**\r\nPeople who want to clone class-based models.", "comments": ["Why is this issue closed? I'm facing the same problem, is there some solution?", "@emadboctorx,\r\nThis is not the issue that is closed.   ", "@rmothukuru yeah, I just noticed. Both issues are addressing the same topic anyway.", "Any updates on this? Also facing the same issue.", "tried 2.6 version but not solved...\r\nwaiting for a solution...", "Indeed, I am also facing this issue, even after ensuring that I build the model.", "issue is still there, any updates?", "@Lucianod28 ! It is still replicating in [2.8](https://colab.sandbox.google.com/gist/mohantym/313d5ffc85b215b1d23d927bb23f92d0/github_47048.ipynb#scrollTo=5Ds8QUcrZ56q) version. Could you please post this on [Keras](https://github.com/keras-team/keras/issues) repo and close the feature request here?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47046, "title": "Deprecate s3 file system", "body": "This PR is part of the effort to switch to modular file system support\r\nby deprecates s3 file system and direct user to use modular file system\r\nfrom tensorflow-io instead.\r\n\r\nA `TF_ENABLE_LEGACY_FILESYSTEM=1` will allow the usage of legacy hdfs file\r\nsystem the same way as before (a warning will be displayed).\r\n\r\n/cc @mihaimaruseac @vnvo2409  @tensorflow/sig-io-maintainers @burgerkingeater \r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Also cc @mihaimaruseac @vnvo2409"]}, {"number": 47045, "title": "Support RaggedTensors in categorical_crossentropy.", "body": "Follow up to #46283.\r\n\r\nTagging @tomerk for review.", "comments": []}, {"number": 47043, "title": "TF_SessionRun with multiple inputs gives Segmentation Fault", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RHEL\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 2.4\r\nPython version: 3.6\r\nBazel version (if compiling from source): 0.16.1\r\nExact command to reproduce: compile and execute my program\r\n\r\n\r\nProblem description:\r\ni have created a logistic regression model using tf.estimators.linearClassifier and exported the model to serve it using C API.\r\n\r\nthe saved_model cli command has the following output:\r\nThe given SavedModel SignatureDef contains the following input(s):\r\ninputs['a'] tensor_info:\r\ndtype: DT_INT64\r\nshape: (-1, 1)\r\nname: Placeholder:0\r\ninputs['b'] tensor_info:\r\ndtype: DT_INT64\r\nshape: (-1, 1)\r\nname: Placeholder_1:0\r\nThe given SavedModel SignatureDef contains the following output(s):\r\noutputs['all_class_ids'] tensor_info:\r\ndtype: DT_INT32\r\nshape: (-1, 2)\r\nname: head/predictions/Tile:0\r\noutputs['all_classes'] tensor_info:\r\ndtype: DT_STRING\r\nshape: (-1, 2)\r\nname: head/predictions/Tile_1:0\r\noutputs['class_ids'] tensor_info:\r\ndtype: DT_INT64\r\nshape: (-1, 1)\r\nname: head/predictions/ExpandDims:0\r\noutputs['classes'] tensor_info:\r\ndtype: DT_STRING\r\nshape: (-1, 1)\r\nname: head/predictions/str_classes:0\r\noutputs['logistic'] tensor_info:\r\ndtype: DT_FLOAT\r\nshape: (-1, 1)\r\nname: head/predictions/logistic:0\r\noutputs['logits'] tensor_info:\r\ndtype: DT_FLOAT\r\nshape: (-1, 1)\r\nname: linear/linear_model/linear/linear_model/linear/linear_model/weighted_sum:0\r\noutputs['probabilities'] tensor_info:\r\ndtype: DT_FLOAT\r\nshape: (-1, 2)\r\nname: head/predictions/probabilities:0\r\nMethod name is: tensorflow/serving/predict\r\n\r\nit has two input tensors and 5 outputs, the following is the c code use to run this  model:\r\n\r\n#include <stdlib.h>\r\n#include <stdio.h>\r\n#include \"tensorflow/c/c_api.h\"\r\n\r\nvoid NoOpDeallocator(void* data, size_t a, void* b) {}\r\n\r\nint main()\r\n{\r\n    //********* Read model\r\n    TF_Graph* Graph = TF_NewGraph();\r\n    TF_Status* Status = TF_NewStatus();\r\n\r\n    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();\r\n    TF_Buffer* RunOpts = NULL;\r\n\r\n    const char* saved_model_dir = \"/nobackup/rajassub/scripts/C-Programs/tensorflow/logicalRegression/\";\r\n    const char* tags = \"serve\"; // default model serving tag; can change in future\r\n    int ntags = 1;\r\n\r\n    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);\r\n    if(TF_GetCode(Status) == TF_OK)\r\n    {\r\n        printf(\"TF_LoadSessionFromSavedModel OK\\n\");\r\n    }\r\n    else\r\n    {\r\n        printf(\"%s\",TF_Message(Status));\r\n    }\r\n\r\n    //****** Get input tensor\r\n    //TODO : need to use saved_model_cli to read saved_model arch\r\n    int NumInputs = 2;\r\n\r\n    TF_Output* Input = (TF_Output*)malloc(sizeof(TF_Output) * NumInputs);\r\n\r\n    TF_Output t0 = {TF_GraphOperationByName(Graph, \"Placeholder\"), 0};\r\n    if(t0.oper == NULL)\r\n        printf(\"ERROR: Failed TF_GraphOperationByName placeholder\\n\");\r\n    else\r\n\tprintf(\"TF_GraphOperationByName placeholder is OK\\n\");\r\n    \r\n\t\r\n    TF_Output t1 = {TF_GraphOperationByName(Graph, \"Placeholder_1\"), 0};\r\n    if(t1.oper == NULL)\r\n        printf(\"ERROR: Failed TF_GraphOperationByName placeholder_1\\n\");\r\n    else\r\n\tprintf(\"TF_GraphOperationByName placeholder_1 is OK\\n\");\r\n\t\r\n    Input[0] = t0;\r\n\tInput[1] = t1;\r\n    \r\n    //********* Get Output tensor\r\n    int NumOutputs = 5;\r\n    TF_Output* Output = (TF_Output*)malloc(sizeof(TF_Output) * NumOutputs);\r\n\r\n    TF_Output t2 = {TF_GraphOperationByName(Graph, \"head/predictions/ExpandDims\"), 0};\r\n    if(t2.oper == NULL)\r\n        printf(\"ERROR: Failed TF_GraphOperationByName linear/head/predictions/ExpandDims\\n\");\r\n    else\t\r\n\tprintf(\"TF_GraphOperationByName linear/head/predictions/ExpandDims is OK\\n\");\r\n\t\r\n\tTF_Output t3 = {TF_GraphOperationByName(Graph, \"head/predictions/str_classes\"), 0};\r\n    if(t3.oper == NULL)\r\n        printf(\"ERROR: Failed TF_GraphOperationByName head/predictions/str_classes\\n\");\r\n    else\t\r\n\tprintf(\"TF_GraphOperationByName linear/head/predictions/str_classes is OK\\n\");\r\n\t\r\n    TF_Output t4 = {TF_GraphOperationByName(Graph, \"head/predictions/logistic\"), 0};\r\n    if(t4.oper == NULL)\r\n        printf(\"ERROR: Failed TF_GraphOperationByName linear/head/predictions/logistic\\n\");\r\n    else\t\r\n\tprintf(\"TF_GraphOperationByName linear/head/predictions/logistic is OK\\n\");\r\n\t\r\n\tTF_Output t5 = {TF_GraphOperationByName(Graph, \"linear/linear_model/linear/linear_model/linear/linear_model/weighted_sum\"), 0};\r\n    if(t5.oper == NULL)\r\n        printf(\"ERROR: Failed TF_GraphOperationByName linear/linear_model/weighted_sum\\n\");\r\n    else\t\r\n\tprintf(\"TF_GraphOperationByName linear/linear_model/weighted_sum is OK\\n\");\r\n\t\r\n\tTF_Output t6 = {TF_GraphOperationByName(Graph, \"head/predictions/probabilities\"), 0};\r\n    if(t6.oper == NULL)\r\n        printf(\"ERROR: Failed TF_GraphOperationByName linear/head/predictions/probabilities\\n\");\r\n    else\t\r\n\tprintf(\"TF_GraphOperationByName linear/head/predictions/probabilities is OK\\n\");\r\n\t\r\n    Output[0] = t2;\r\n\tOutput[1] = t3;\r\n\tOutput[2] = t4;\r\n\tOutput[3] = t5;\r\n\tOutput[4] = t6;\r\n\r\n    //********* Allocate data for inputs & outputs\r\n    TF_Tensor** InputValues = (TF_Tensor**)malloc(sizeof(TF_Tensor*)*NumInputs);\r\n    TF_Tensor** OutputValues = (TF_Tensor**)malloc(sizeof(TF_Tensor*)*NumOutputs);\r\n\r\n    int ndims = 2;\r\n    int64_t dims[] = {1,1};\r\n    int64_t data1[] = {2};\r\n\r\n    int ndata = sizeof(int64_t)*1 ;// This is tricky, it number of bytes not number of element\r\n\r\n    TF_Tensor* int_tensor1 = TF_NewTensor(TF_FLOAT, dims, ndims, data1, ndata, &NoOpDeallocator, 0);\r\n    if (int_tensor1 != NULL)\r\n    {\r\n        printf(\"TF_NewTensor is OK for int_tensor1\\n\");\r\n    }\r\n    else\r\n\tprintf(\"ERROR: Failed TF_NewTensor for int_tensor1\\n\");\r\n    \r\n\tint64_t data2[] = {3};\r\n\tTF_Tensor* int_tensor2 = TF_NewTensor(TF_FLOAT, dims, ndims, data2, ndata, &NoOpDeallocator, 0);\r\n    if (int_tensor2 != NULL)\r\n    {\r\n        printf(\"TF_NewTensor is OK for int_tensor2\\n\");\r\n    }\r\n    else\r\n\tprintf(\"ERROR: Failed TF_NewTensor for int_tensor2\\n\");\r\n    InputValues[0] = int_tensor1;\r\n\tInputValues[1] = int_tensor2;\r\n    \r\n    // //Run the Session\r\n    TF_SessionRun(Session, NULL, Input, InputValues, NumInputs, Output, OutputValues, NumOutputs, NULL, 0,NULL , Status);\r\n\r\n    if(TF_GetCode(Status) == TF_OK)\r\n    {\r\n        printf(\"Session is OK\\n\");\r\n    }\r\n    else\r\n    {\r\n        printf(\"%s\",TF_Message(Status));\r\n    }\r\n\r\n    // //Free memory\r\n    TF_DeleteGraph(Graph);\r\n    TF_DeleteSession(Session, Status);\r\n    TF_DeleteSessionOptions(SessionOpts);\r\n    TF_DeleteStatus(Status);\r\n\r\n\r\n    void* buff = TF_TensorData(OutputValues[0]);\r\n    float* offsets = buff;\r\n    printf(\"Result Tensor :\\n\");\r\n    printf(\"%f\\n\",offsets[0]);\r\n    return 0; \r\n    \r\n}\r\n\r\n\r\nBut it is crashing at Session run step, it would be really helpful if someone could identify where the issue is.\r\n\r\noutput obtained:\r\nTF_LoadSessionFromSavedModel OK\r\nTF_GraphOperationByName placeholder is OK\r\nTF_GraphOperationByName placeholder_1 is OK\r\nTF_GraphOperationByName linear/head/predictions/ExpandDims is OK\r\nTF_GraphOperationByName linear/head/predictions/str_classes is OK\r\nTF_GraphOperationByName linear/head/predictions/logistic is OK\r\nTF_GraphOperationByName linear/linear_model/weighted_sum is OK\r\nTF_GraphOperationByName linear/head/predictions/probabilities is OK\r\nTF_NewTensor is OK for int_tensor1\r\nTF_NewTensor is OK for int_tensor2\r\nSegmentation fault (core dumped)\r\n\r\n", "comments": ["@Vaishu-Jeeva,\r\n\r\nCan you take a look at this similar [issue1](https://github.com/tensorflow/tensorflow/issues/19347), [issue2](https://github.com/tensorflow/tensorflow/issues/2034) which is about segmentation fault? We also recommend you to update the tensorflow to latest stable version and let us know if the issue still persists. Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47043\">No</a>\n"]}, {"number": 47042, "title": "Error when Building Tensorflow 2.4/2.4.1 for C++ with cuda support", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0 / 2.4.1\r\n- Python version: 3.8.6\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11.0 /8.0.5.39\r\n- GPU model and memory: Nvidia GTX 1070\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen building tenmsorflow for C++ with gpu support the build failed with the following error:\r\n![grafik](https://user-images.githubusercontent.com/78606066/107390473-0730bb80-6af8-11eb-8454-3b651682eb16.png)\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI first cloned the repository and switched to branch r2.4\r\n\r\nafter that i configured with configure.py.\r\n\r\n![grafik](https://user-images.githubusercontent.com/78606066/107390779-537bfb80-6af8-11eb-8f62-bc4ca3d7b297.png)\r\n\r\nthen I started the build with bazel:\r\n\r\n`bazel build --config=opt --config=cuda tensorflow:tensorflow.dll`\r\n\r\nIt ended up with the error above.\r\n\r\n**Any other info / logs**\r\n\r\n\r\nBuilding without cuda support works just fine\r\n\r\nI dont know where the error is maybe its me maybe Tensorflow. I hope i can get some Help here. It would be great if the Tensorflow Site would have a proper instruction. I had to make a long journey to get to the point of where i am here with building. ", "comments": ["@daschuchmann \r\nplease share error logs as text, it would be easy for us and any user to search for the error.\r\nAlso you may refer to : [link](https://github.com/tensorflow/tensorflow/issues/24549#issuecomment-454464310), [link1](https://github.com/tensorflow/tensorflow/issues/11859#issuecomment-475044408), [link2](https://stackoverflow.com/questions/57503584/cannot-build-the-tensorflow-c-api-in-windows)", "@Saduf2019 thank you solved it. I havent set the environment to mysys. After setting the environment to mysys it started building successfully. In future issues I will share the logs as text.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47042\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47042\">No</a>\n", "@daschuchmann\r\nTahnk you for your update, glad we could help."]}, {"number": 47040, "title": "change tensorfloew file", "body": "this ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47040) for more info**.\n\n<!-- need_sender_cla -->", "@anonymous-313  Can you please sign CLA. Thanks!", "Closing as spam. The edits are invalid, there is no issue with those lines."]}, {"number": 47039, "title": "TF-TRT Dynamic shape mode test for Softmax op converter", "body": "This PR adds explicit batch and dynamic shape mode test for the softmax op converter. \r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility\r\n\r\nTracker: #45481", "comments": ["@tfeher Can you please check @bixia1's comments and keep us posted ? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "The PR is still valid, will update this in a few days."]}, {"number": 47038, "title": "AttributeError: module 'tensorflow' has no attribute 'contrib' -->> alterative", "body": "**I want alternative for the following code in TF2**\r\n\r\ntf.contrib.cloud.configure_gcs(session, credentials=auth_info)\r\n\r\n\r\n", "comments": ["@knightperfectionist,\r\nCould you please provide a minimal code snippet of the use-case you're trying to implement, so that we can look into this. Thanks!", "> @knightperfectionist,\r\n> Could you please provide a minimal code snippet of the use-case you're trying to implement, so that we can look into this. Thanks!\r\n\r\nimport datetime\r\nimport json\r\nimport os\r\nimport pprint\r\nimport random\r\nimport string\r\nimport sys\r\nimport tensorflow as tf\r\n\r\nassert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\r\nTPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\nprint('TPU address is', TPU_ADDRESS)\r\n\r\nfrom google.colab import auth\r\nauth.authenticate_user()\r\nwith tf.Session(TPU_ADDRESS) as session:\r\n  print('TPU devices:')\r\n  pprint.pprint(session.list_devices())\r\n\r\n  # Upload credentials to TPU.\r\n  with open('/content/adc.json', 'r') as f:\r\n    auth_info = json.load(f)\r\n  tf.contrib.cloud.configure_gcs(session, credentials=auth_info) #here i am getting the error \r\n  # Now credentials are set for all future sessions on this TPU.\r\n\r\n\r\nIt's a compatibility issue of v1 and v2. The notebook needs to be updated it seems.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47038\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47038\">No</a>\n"]}, {"number": 47036, "title": "SIGSEGV - error when using large convolution - GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04, Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): anaconda\r\n- TensorFlow version (use command below): 2.2.0-rc3 and Colab Version\r\n- Python version: 3.7 Colab\r\n- CUDA/cuDNN version: 11.2, Colab\r\n- GPU model and memory: 2080Ti, Colab\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe script should generate a map of random values, smoothed with a Gauss filter. For the value sigma = 60 everything works fine, above this value the error \"Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)\" is returned. I got the same error on different workstations and also on colab. \r\n\r\nThe problem only occurs when using the GPU.\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom matplotlib import pyplot as plt\r\n\r\nrandom_mask = tf.random.uniform((1, 400, 400, 1),\r\n                                        minval=-1,\r\n                                        maxval=1,\r\n                                        dtype=tf.float32)\r\n\r\nsigma = 61\r\nsigmas_per_radius = 3\r\nradius = tf.cast(sigma * sigmas_per_radius, tf.int32)\r\n\r\nx = tf.cast(tf.range(-radius, radius + 1),dtype=tf.float32)\r\n\r\ngauss_1d = tf.exp(-0.5 * tf.square(x / sigma))\r\ngauss_1d = gauss_1d / tf.reduce_sum(gauss_1d)\r\n\r\nkernel = tf.expand_dims(gauss_1d, 1) * gauss_1d\r\nkernel = kernel[..., tf.newaxis, tf.newaxis]\r\n\r\npoint_wise_filter = tf.eye(1, batch_shape=[1, 1])\r\n\r\ndef_map = tf.nn.separable_conv2d(\r\n    random_mask,\r\n    kernel,\r\n    point_wise_filter,\r\n    strides=[1, 1, 1, 1],\r\n    padding='SAME'\r\n)\r\n\r\ndef_map = tf.squeeze(def_map)\r\n\r\nplt.imshow(def_map.numpy())\r\nplt.show()\r\n\r\n```", "comments": ["I have tried in colab with TF-GPU version 2.4 and i am not seeing any issue. Please. find the gist [here](https://colab.research.google.com/gist/ravikyram/fd8d3bb4c9fc82f592c951273ea7e9aa/untitled667.ipynb). Thanks!", "I am using the default tensorflow library from Colab, this is 2.4.1. Can you check it ? \r\n(https://colab.research.google.com/drive/1YqnLaeWm_1EXw9PAnOf90tlcDy6PoF6H?usp=sharing)", "I am not seeing any  issue in colab with TF-GPU version 2.4.1. Please,find the gist [here](https://colab.research.google.com/gist/ravikyram/4980c9d435449e7b27ed8552c659685c/untitled669.ipynb).\r\nPlease,verify once and close this issue.Thanks!", "Can you check it again without installing another version of tensorflow than the one on the colab? I asked some of my friends about verification and they all confirm the error. Please try to do the same as on the notebook I provided. Without installing the new library :) ", "> I am using the default tensorflow library from Colab, this is 2.4.1. Can you check it ?\r\n> (https://colab.research.google.com/drive/1YqnLaeWm_1EXw9PAnOf90tlcDy6PoF6H?usp=sharing)\r\n\r\n@Emilon1928,\r\nOn running the Colab notebook you have provided, I did not face issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/6007f14d3e178781a1cc48eb90002d08/untitled2.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47036\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47036\">No</a>\n"]}, {"number": 47035, "title": "Failing to Cross-Compile TensorFlow Lite C++ for Raspberry Pi", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS\r\n- TensorFlow installed from (source or binary): Attempting to build from source as per [here](https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi_with_make)\r\n- TensorFlow version: TensorFlow Lite\r\n- Python version: Python 3.8.5\r\n- Installed using virtualenv? pip? conda?: No\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n\r\n\r\n**Describe the problem**\r\nAttempting to [cross-compile for Raspberry Pi with Make](https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi_with_make) for a Raspberry Pi 4. I tried building on my machine first, but that failed so I tried the Docker-based installation. This failed with the same error.\r\n\r\nI haven't been able to find this in the issue tracker, appologies if I've missed something there. Many thanks in advance!\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nInside the Docker `tensorflow/tensorflow:devel` container:\r\n\r\n```sh\r\n# Step 1\r\ngit clone https://github.com/raspberrypi/tools.git rpi_tools\r\n\r\n# Step 3 (step 2 skipped as per instructions)\r\ncd tensorflow_src && ./tensorflow/lite/tools/make/download_dependencies.sh\r\n\r\n# Step 4a\r\nPATH=../rpi_tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/bin:$PATH \\\r\n  ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n```\r\n\r\nThe final step failed with the log indicated below\r\n\r\n**Any other info / logs**\r\nIt looks like the error might be:\r\n```\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: 'is_trivially_copyable' is not a member of 'std'\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                 ^\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before '>' token\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                                                         ^\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: '::value' has not been declared\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n```\r\nThe full log (including the above extract) is:\r\n\r\n<details>\r\n<summary>Click here for full log</summary>\r\n\r\n```\r\n+ set -e\r\n+++ dirname ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n++ cd ./tensorflow/lite/tools/make\r\n++ pwd\r\n+ SCRIPT_DIR=/tensorflow_src/tensorflow/lite/tools/make\r\n+ TENSORFLOW_DIR=/tensorflow_src/tensorflow/lite/tools/make/../../../..\r\n++ free -m\r\n++ awk '/^Mem/ {print $2}'\r\n+ FREE_MEM=15803\r\n+ [[ FREE_MEM -gt 2000 ]]\r\n+ NO_JOB=4\r\n+ make -j 4 TARGET=rpi -C /tensorflow_src/tensorflow/lite/tools/make/../../../.. -f tensorflow/lite/tools/make/Makefile\r\nmake: Entering directory '/tensorflow_src'\r\ntensorflow/lite/tools/make/Makefile:358: warning: overriding recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a'\r\ntensorflow/lite/tools/make/Makefile:355: warning: ignoring old recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a'\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/allocation.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/allocation.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/arena_planner.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/arena_planner.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/c/c_api.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/c/c_api.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/c/c_api_experimental.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/c/c_api_experimental.o\r\narm-linux-gnueabihf-gcc -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/c/common.c -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/c/common.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/core/api/error_reporter.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/error_reporter.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/core/api/flatbuffer_conversions.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/flatbuffer_conversions.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/core/api/op_resolver.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/op_resolver.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/core/api/tensor_utils.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/tensor_utils.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/core/subgraph.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/subgraph.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/create_op_resolver_with_builtin_ops.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/create_op_resolver_with_builtin_ops.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/delegates/interpreter_utils.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/delegates/interpreter_utils.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/experimental/resource/resource_variable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/resource/resource_variable.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/experimental/resource/static_hashtable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/resource/static_hashtable.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/external_cpu_backend_context.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/external_cpu_backend_context.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/graph_info.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/graph_info.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/interpreter.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/interpreter.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/interpreter_builder.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/interpreter_builder.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/activations.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/add.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/add.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/add_n.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/add_n.o\r\nIn file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function 'static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)':\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:490:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:493:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:496:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:499:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:502:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:505:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\n                 from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\n                 from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\n                 from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function 'void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: 'is_trivially_copyable' is not a member of 'std'\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                 ^\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before '>' token\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                                                         ^\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: '::value' has not been declared\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                                                          ^\r\nIn file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/integer_ops/add.h:26,\r\n                 from tensorflow/lite/kernels/add.cc:15:\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function 'static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)':\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:490:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:493:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:496:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:499:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:502:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:505:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\n                 from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\n                 from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\n                 from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/integer_ops/add.h:26,\r\n                 from tensorflow/lite/kernels/add.cc:15:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function 'void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: 'is_trivially_copyable' is not a member of 'std'\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                 ^\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before '>' token\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                                                         ^\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: '::value' has not been declared\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                                                          ^\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/arg_min_max.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/arg_min_max.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/assign_variable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/assign_variable.o\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/audio_spectrogram.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/audio_spectrogram.o\r\nIn file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n                 from tensorflow/lite/kernels/arg_min_max.cc:23:\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function 'static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)':\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:490:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:493:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:496:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:499:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:502:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:505:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\n                 from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\n                 from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\n                 from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n                 from tensorflow/lite/kernels/arg_min_max.cc:23:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function 'void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: 'is_trivially_copyable' is not a member of 'std'\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                 ^\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before '>' token\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                                                         ^\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: '::value' has not been declared\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                                                          ^\r\nIn file included from ./tensorflow/lite/kernels/cpu_backend_gemm.h:23:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n                 from tensorflow/lite/kernels/audio_spectrogram.cc:24:\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h: In static member function 'static void tflite::cpu_backend_gemm::detail::CustomGemvImpl<LhsScalar, RhsScalar, int, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<int, DstScalar, quantization_flavor>&, int, int)':\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:490:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:493:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:496:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:499:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:502:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\n./tensorflow/lite/kernels/cpu_backend_gemm_custom_gemv.h:505:13: warning: attributes at the beginning of statement are ignored [-Wattributes]\r\n             [[clang::fallthrough]];\r\n             ^\r\nIn file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,\r\n                 from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,\r\n                 from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,\r\n                 from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,\r\n                 from tensorflow/lite/kernels/audio_spectrogram.cc:24:\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function 'void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: 'is_trivially_copyable' is not a member of 'std'\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                 ^\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before '>' token\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                                                         ^\r\n/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: '::value' has not been declared\r\n   static_assert(std::is_trivially_copyable<MulParamsType>::value, \"\");\r\n                                                          ^\r\ntensorflow/lite/tools/make/Makefile:334: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/add.o' failed\r\nmake: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/add.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\ntensorflow/lite/tools/make/Makefile:334: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o' failed\r\nmake: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o] Error 1\r\ntensorflow/lite/tools/make/Makefile:334: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/arg_min_max.o' failed\r\nmake: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/arg_min_max.o] Error 1\r\ntensorflow/lite/tools/make/Makefile:334: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/audio_spectrogram.o' failed\r\nmake: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/audio_spectrogram.o] Error 1\r\nmake: Leaving directory '/tensorflow_src'\r\n```\r\n</details>", "comments": ["@miklasr,\r\nPlease take a look at the comments from issue [#42731](https://github.com/tensorflow/tensorflow/issues/42731) with a similar error log and let us know if it helps. Thanks!", "Makefile build path could be outdated. Could you try CMake instead?\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm\r\n", "Thank you both for your comments!\r\n\r\n@amahendrakar based on those comments I did some a bit of looking around and found [this SO post](https://stackoverflow.com/a/58559140). The post mentions how to install a different RPi toolchain. It sounds like the compiler version used is old in the official RPi tools, so the asserts in the TF code are possibly not supported for that reason. By following the steps in the post I was able to get further through the TF build script, but it still resulted in an error (a different one this time).\r\n\r\n@terryheo the CMake build was successful. I haven't had time to actually test the built library file, but at least it built.\r\n\r\nFor your information I also tried installing natively on the Pi and that worked well and fairly quickly, so that seems the best approach. It may be worth adding words to that effect on the [cross-compile for Raspberry Pi with Make](https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi_with_make) page so others give the native build a go first. At least a warning that the steps may no longer work and a link to the CMake instructions might be good.\r\n\r\nThanks again for your quick responses.", "The page has a note says \"Cross-compile ARM with CMake is available. Please check this.\"\r\nAnyway, we'll remove the page when Makefile build process gets deprecated.\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47035\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47035\">No</a>\n"]}, {"number": 47034, "title": "Conv2D feeding into LSTM breaks model for inference", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nHaving a `Conv2D` layer with a `filter_size=(1, F)`, where `F` is the number of features, delivers inconsistent results while feeding a chunked stream to a model.\r\n\r\nI identified this `Conv2D` layer as the breaking-point in my model for speech-recognition. The model works fine _without_ the `Conv2D` layer using chunking and non-chunking. However, if `Conv2D` is present, chunking is not working anymore.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe error between the chunked and non-chunked model output should _always_ be exactly `0.0` or at least *very* close.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe code below generates plot, as shown below, to illustrate the (potential) issue.\r\n\r\nEssentially, it generates a toy-model which feeds inputs, taking the form `(batch, time, features, channels)` into the `Conv2D` layer and project this down to `(batch, time, 1, filters)`. Note the `tf.concat` before the `Conv2D` layer which artificially creates this input in the example. \r\n\r\nAfter the convolution we just drop the third axis and feed the result into the LSTM layers.\r\n\r\n```python\r\nimport string\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n\r\ndef get_model(\r\n    vocab_size: int,\r\n    embedding_size: int,\r\n    hidden_size: int,\r\n    conv2d_kwargs: dict = None\r\n):\r\n    inputs = layers.Input(shape=(None,), dtype=tf.int32)\r\n\r\n    embedding = layers.Embedding(\r\n        vocab_size + 1,\r\n        embedding_size,\r\n        mask_zero=True\r\n    )\r\n    x = embedding(inputs)\r\n\r\n    if conv2d_kwargs is not None:\r\n        x = layers.Lambda(lambda l: tf.expand_dims(l, axis=-1))(x)\r\n        x = layers.Lambda(lambda t: tf.concat([t, t, t], axis=-1))(x)\r\n        x = layers.Conv2D(**conv2d_kwargs)(x)\r\n        x = layers.Lambda(lambda l: tf.squeeze(l, axis=2))(x)\r\n\r\n    lstm_out = layers.LSTM(hidden_size, return_sequences=True, return_state=True)(x)\r\n    x = lstm_out[0]\r\n    lstm_out = layers.LSTM(hidden_size, return_sequences=False, return_state=True)(x)\r\n    x = lstm_out[0]\r\n    outputs = layers.Dense(vocab_size, activation='softmax')(x)\r\n    return keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n\r\ndef infer(model, inputs, state=None):\r\n    x = inputs\r\n    new_states = list()\r\n\r\n    # We're just interested in the output of the Conv2D layer and the succeeding LSTM-layer\r\n    layers_of_interest = model.layers[:-2]\r\n\r\n    for layer in layers_of_interest:\r\n        if isinstance(layer, layers.LSTM):\r\n            idx = len(new_states)\r\n            outputs = layer(x, initial_state=state[idx] if state is not None else None)\r\n            x, new_state = outputs[0], outputs[1:]\r\n            new_states.append(new_state)\r\n        else:\r\n            x = layer(x)\r\n\r\n    return x, new_states\r\n\r\n\r\ndef eval_plot(model, sample_text: str, char2idx: dict, chunk_size: int):\r\n\r\n    enc_chunks = list()\r\n    state_trained = None\r\n    nb_chunks = int(np.ceil(len(sample_text) / chunk_size))\r\n    for i in range(nb_chunks):\r\n        s = i * chunk_size\r\n        e = s + chunk_size\r\n        text_chunk = sample_text[s:e]\r\n        test_inputs = list(map(lambda c: char2idx[c], text_chunk))\r\n        test_inputs = tf.constant([test_inputs], dtype=tf.int32)\r\n        encoded, state_trained = infer(model, test_inputs, state=state_trained)\r\n        enc_chunks.append(encoded)\r\n\r\n    test_inputs = list(map(lambda c: char2idx[c], sample_text))\r\n    test_inputs = tf.constant([test_inputs], dtype=tf.int32)\r\n    enc_full, _ = infer(model, test_inputs)\r\n    enc_full = enc_full.numpy()[0]\r\n\r\n    chunks_concat = tf.concat(enc_chunks, axis=1).numpy()[0]\r\n    diff = chunks_concat - enc_full\r\n    rmspe = (np.sqrt(np.mean(np.square(diff / enc_full)))) * 100\r\n    import matplotlib.pyplot as plt\r\n    fix, axes = plt.subplots(1, 3, figsize=(16, 9), sharex='all', sharey='all')\r\n    ax1, ax2, ax3 = axes\r\n    ax1.matshow(chunks_concat.T)\r\n    ax1.set_title('Chunked')\r\n    ax2.matshow(enc_full.T)\r\n    ax2.set_title('Full')\r\n    ax3.matshow(diff.T)\r\n    ax3.set_title(f'Diff (RMSPE: {rmspe:.8f})')\r\n    for ax in axes:\r\n        ax.set_xlabel('Time [T]')\r\n    ax1.set_ylabel('Features [F]')\r\n    plt.tight_layout()\r\n    plt.show()\r\n    return rmspe\r\n\r\n\r\ndef main():\r\n\r\n    vocab = set(' ' + string.ascii_lowercase)\r\n    vocab_size = len(vocab)\r\n    idx2char = {(idx + 1): char for idx, char in enumerate(vocab)}\r\n    char2idx = {char: idx for idx, char in idx2char.items()}\r\n\r\n    units = 32\r\n    embedding_size = 16\r\n    filters = units * 2\r\n\r\n    model_conv_params = dict(\r\n        vocab_size=vocab_size,\r\n        embedding_size=embedding_size,\r\n        hidden_size=units,\r\n        conv2d_kwargs=dict(filters=filters, kernel_size=(1, embedding_size), use_bias=False)\r\n    )\r\n\r\n    model_conv = get_model(**model_conv_params)\r\n\r\n    sample_text = 'how are you buddy'\r\n\r\n    for chunk_size in range(1, 11):\r\n        print(f'Creating plots for chunk size {chunk_size}')\r\n        rmspe = eval_plot(model_conv, sample_text, char2idx, chunk_size)\r\n        print(f'  RMSPE: {rmspe:.4f}')\r\n\r\n    print('All done')\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\n### Toy-example plots\r\n\r\n![image](https://user-images.githubusercontent.com/43335432/107357656-a93dad00-6ad2-11eb-9e15-5ce79c3a7f21.png)\r\n\r\n![image](https://user-images.githubusercontent.com/43335432/107357740-bfe40400-6ad2-11eb-9897-a350652e3ba4.png)\r\n\r\n### \"Real\" Model Plots\r\n\r\nThe following are plots from a speech recognition model which uses this simple `Conv2D` operation before it is send into LSTM layers. One plot is after 1000 steps of training, the other after 200. As you can see, **the MSE, computed for each frame in the last image, increases over time** (note the y-axis range). This model is \"broken\" **using chunking** and does not produce any meaningful output anymore after training it until early stopping. It does work *without* chunking though.\r\n\r\n![image](https://user-images.githubusercontent.com/43335432/107615058-c9dd4280-6c4b-11eb-9b7b-2cf8169cfbf2.png)\r\n\r\n![image](https://user-images.githubusercontent.com/43335432/107615064-ce096000-6c4b-11eb-8da7-519df0e33219.png)\r\n\r\nOn the other hand, using the same model but removing the `Conv2D` layer gives an MSE for each frame somewhere below `6e-14` which is (basically) zero:\r\n\r\n![image](https://user-images.githubusercontent.com/43335432/107615288-293b5280-6c4c-11eb-837e-857c6773efb2.png)\r\n\r\n\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47034\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47034\">No</a>\n", "@stefan-falk \r\nI ran the code shared and face  a different [issue](https://colab.research.google.com/gist/Saduf2019/dafb8e5520c958cb8dfb5473f82357aa/untitled529.ipynb), please share all dependencies for us to replicate the issues.", "@Saduf2019 The code in my first post is a standalone and should run. Why do you have a call to `image_dataset_from_directory()`?\r\n\r\nHowever, the interesting thing I noticed is that running the code in Colab does _not_ show the same results..", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47034\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47034\">No</a>\n", "@ymodak\r\nI ran the code on tf 2.4 and nightly, please find the [gist here]( \r\nhttps://colab.research.google.com/gist/Saduf2019/6bf97a0ebe14110f15813eefe6161856/untitled529.ipynb)", "fyi: I was able to fix my original problem. In turns out that I had a bug elsewhere which broke my model. For this matter, this issue is not relevant anymore.\r\n\r\nHowever, it would be interesting to know why the example above sometimes produces sometimes an error of 0 and sometime one that's > 0.\r\n\r\nIt's very likely that these are just some rounding/floating errors but it's kind of interesting to see that there's no difference at all in at least some cases.", "Closing this issue since the original query is resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47034\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47034\">No</a>\n"]}, {"number": 47033, "title": "gradient computation fails in graph mode with numpy_function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary (conda-forge)\r\n- TensorFlow version (use command below): 'v2.3.0-rc2-23-gb36436b087', 'v2.4.0-49-g85c8b2a817f'\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Titan RTX 24GB\r\n\r\n**Describe the current behavior**\r\n\r\nI'm trying to use a function implemented in cython in a loss function. The (int64) output of this function is used as index for tf.gather. When computing the loss in eager execution mode gradients are computed fine, but when run in graph mode, an exception is raised.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect the same result in graph execution mode.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nA tiny sample where the cython function simply implements argmin:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport assign\r\n\r\nmodel = tf.keras.applications.ResNet50(False, weights=None, input_shape=(224, 224, 3))\r\ndata = tf.random.normal((8,224,224,3), 128, 64)\r\n\r\ndef f(x):\r\n\ty = model(x)\r\n\tidx = tf.numpy_function(assign.assign_objects_to_cells, (-y,), np.int64)\r\n\t#idx = tf.argmin(-y, -1)\r\n\t#idx = tf.numpy_function(np.argmin, (-y, -1), np.int64)\r\n\tvalues = tf.gather(y, idx, batch_dims=3)\r\n\treturn tf.reduce_sum(values)\r\n\r\n\r\ndef grad(x):\r\n\twith tf.GradientTape() as tape:\r\n\t\ty = f(x)\r\n\treturn tape.gradient(y, model.trainable_variables)\r\ntf_grad = tf.function(grad)\r\n\r\nprint('eager execution')\r\ng = grad(data)\r\nprint('graph execution')\r\ng = tf_grad(data)\r\n```\r\n\r\nAnd the **assign.pyx**:\r\n\r\n```\r\nimport numpy as np\r\ncimport numpy as np\r\ncimport cython\r\n\r\n@cython.boundscheck(False) # turn off bounds-checking for entire function\r\n@cython.wraparound(False)  # turn off negative index wrapping for entire function\r\ncdef void _assign_objects_to_cells(float[:,:,:,:] error, np.int64_t[:,:,:] argmax) nogil:\r\n\tcdef int N = error.shape[0], H = error.shape[1], W = error.shape[2], C = error.shape[3]\r\n\tcdef int y, x, c, best_id\r\n\tcdef float best, e\r\n\t\r\n\tfor n in range(N):\r\n\t\tfor y in range(H):\r\n\t\t\tfor x in range(W):\r\n\t\t\t\tbest_id = 0\r\n\t\t\t\tbest = error[n,y,x,0]\r\n\t\t\t\tfor c in range(1, C):\r\n\t\t\t\t\te = error[n,y,x,c]\r\n\t\t\t\t\tif e<best:\r\n\t\t\t\t\t\tbest = e\r\n\t\t\t\t\t\tbest_id = c\r\n\t\t\t\targmax[n,y,x] = best_id\r\n\r\ndef assign_objects_to_cells(np.ndarray[np.float32_t, ndim=4] error not None):\r\n\tcdef int N = error.shape[0], H = error.shape[1], W = error.shape[2], C = error.shape[3]\r\n\tout = np.empty((N,H,W), np.int64)\r\n\t_assign_objects_to_cells(error, out)\r\n\t\r\n\treturn out\r\n```\r\n\r\n**Other info / logs** \r\n\r\nEager execution runs fine, but graph execution fails:\r\n\r\n```\r\neager execution\r\ngraph execution\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-eafa53e19dc9>\", line 25, in <module>\r\n    g = tf_grad(data)\r\n\r\n  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n\r\n  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 823, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n\r\n  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 697, in _initialize\r\n    *args, **kwds))\r\n\r\n  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n\r\n  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3213, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n\r\n  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3075, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n\r\n  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n\r\n  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 600, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n\r\n  File \"d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 973, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\n\r\nTypeError: in user code:\r\n\r\n    <ipython-input-1-eafa53e19dc9>:19 grad  *\r\n        return tape.gradient(y, model.trainable_variables)\r\n    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1073 gradient  **\r\n        unconnected_gradients=unconnected_gradients)\r\n    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:77 imperative_grad\r\n        compat.as_str(unconnected_gradients.value))\r\n    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:162 _gradient_function\r\n        return grad_fn(mock_op, *out_grads)\r\n    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:678 _GatherV2Grad\r\n        batch_dims, params_shape[axis])\r\n    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:603 _BatchGatherGrad\r\n        indices = _GetBatchIndices(params_shape, indices, batch_dims)\r\n    d:\\conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:582 _GetBatchIndices\r\n        [1] * (dim - 1) + [dim_value] + [1] * (indices_ndims - dim), axis=0)\r\n\r\n    TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\r\n```\r\n\r\nNote, that both eager and graph modes work if I replace tf.numpy_function with tf.argmin and the results are the same (up to rounding errors). Graph execution also fails when using np.argmin.\r\n", "comments": ["@akkiss \r\n\r\nI have tried in colab with TF -GPU version 2.4 and i am seeing the below error message.(`AttributeError: module 'assign' has no attribute 'assign_objects_to_cells'`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/13dd352805733aebbebab17bae61c4c8/untitled668.ipynb).\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "Assign is not a library but a cython function (the \"And the assign.pyx:\" part). This is not required anyway, the problem appears with np.argmin as well. Unfortunately I cannot make my colab public, but this code reproduced the problem:\r\n\r\n```\r\n!pip install tensorflow-gpu==2.4.0\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n#import assign\r\n\r\nmodel = tf.keras.applications.ResNet50(False, weights=None, input_shape=(224, 224, 3))\r\ndata = tf.random.normal((8,224,224,3), 128, 64)\r\n\r\ndef f(x):\r\n\ty = model(x)\r\n\t#idx = tf.numpy_function(assign.assign_objects_to_cells, (-y,), np.int64)\r\n\t#idx = tf.argmin(-y, -1)\r\n\tidx = tf.numpy_function(np.argmin, (-y, -1), np.int64)\r\n\tvalues = tf.gather(y, idx, batch_dims=3)\r\n\treturn tf.reduce_sum(values)\r\n\r\n\r\ndef grad(x):\r\n\twith tf.GradientTape() as tape:\r\n\t\ty = f(x)\r\n\treturn tape.gradient(y, model.trainable_variables)\r\ntf_grad = tf.function(grad)\r\n\r\nprint('eager execution')\r\ng = grad(data)\r\nprint('graph execution')\r\ng = tf_grad(data)\r\n```\r\n", "The problem is that `tf.numpy_functions` returns completely undefined static shape; i.e.,\r\n```python\r\n  print(idx.shape)\r\n```\r\nreturns `(8, 7, 7)` in Eager mode, but `<unknown>` in Graph mode (i.e., not even the rank of the result is known). This behaviour is expected (it would be difficult to perform some static analysis of the resulting shape).\r\n\r\nWhat you can do is to provide the information about the output shape (and then check at runtime that it holds). You can do so quite easily by using the following just after `idx = tf.numpy_function(np.argmin, (-y, -1), np.int64)`:\r\n```python\r\n  idx = tf.ensure_shape(idx, y.shape[:-1])\r\n```\r\nThe value of `idx` is not modified, but it has the same static shape as `y`, apart from the last dimension (which is eradicated by the `np.argmin`); the `tf.ensure_shape` even checks at runtime that the dynamic shape matches.", "BTW, the gist with the added line: https://colab.research.google.com/drive/1iOwlwiETazXwkHt0SvNgZP29KOXKDjQg?usp=sharing", "OMG, so obvious, how did I not think of this... Thanks @foxik for the solution.\r\n\r\nIf someone ever reads this might be happy to hear this works also with variable tensor shapes:\r\n```\r\nidx = tf.ensure_shape(idx, 3*[None])\r\n```\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47033\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47033\">No</a>\n"]}, {"number": 47031, "title": "update tensorflow", "body": "tensorflow-update", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47031) for more info**.\n\n<!-- need_sender_cla -->", "@nidhalrahali  Can you please sign CLA. Thanks!", "This is spam"]}, {"number": 47030, "title": "Merge pull request #1 from tensorflow/master", "body": "tensorflow-update", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47030) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 47029, "title": "INFO:tensorflow:Saver not created because there are no variables in the graph to restore", "body": "I am using\r\nbert-tensorflow==1.0.1\r\ntensorflow version = 2.4.1\r\nkeras - 2.4.3\r\n\r\nCode\r\nBERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\r\n\r\ndef create_tokenizer_from_hub_module():\r\n\"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\r\nwith tf.Graph().as_default():\r\nbert_module = hub.Module(BERT_MODEL_HUB)\r\ntokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\r\nwith tf.Session() as sess:\r\nvocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\r\ntokenization_info[\"do_lower_case\"]])\r\n\r\nreturn bert.tokenization.FullTokenizer(\r\nvocab_file=vocab_file, do_lower_case=do_lower_case)\r\n\r\ntokenizer = create_tokenizer_from_hub_module()\r\n\r\nAfter running above cell in Colab I'm getting the following msg\r\n\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore", "comments": ["@knightperfectionist \r\nCan you check this link and let us know: [link](https://stackoverflow.com/questions/46104525/how-to-restore-a-partial-graph-in-tensorflow)"]}, {"number": 47028, "title": "unable to run tensorflow - error ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Tlevi16 \r\n\r\nPlease, fill issue template. Please, elaborate the issue and provide the exact sequence of commands / steps that you executed before running into the problem\r\nIt will be great if you share complete error log so it will be helpful for us in debugging further. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "No template filled, closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47028\">No</a>\n"]}, {"number": 47027, "title": "post_training_integer_quantization issue", "body": "I used a simple transfer learning code on my windows computer, downloading the cat-dog dataset.\r\nThe transfer learning and conversion to TFLite sections are OK, but I can't convert the input to int8 from float 32 following TF guidline.\r\n\r\n**My Code;**\r\nhttps://colab.research.google.com/drive/1hlTykr4-rUYqI-n10icVEywxUhyar364\r\n\r\n**The Guide to do the conversion;**\r\nhttps://www.tensorflow.org/lite/performance/post_training_integer_quant\r\n\r\n**Issue**; ValueError: Unbatching a dataset is only supported for rank >= 1\r\n\r\nThanks in advance for your help,\r\nMoZen\r\n", "comments": ["@Paryavi,\r\nI do not have access to the Colab notebook you have linked. Could you please provide the required permissions to view the files. Thanks!", "@amahendrakar  and @abattery \r\nThank you, here is the link; https://colab.research.google.com/drive/1hlTykr4-rUYqI-n10icVEywxUhyar364?usp=sharing", "Hi @Paryavi,\r\n\r\nThe train_dataset variable in the colab is already a dataset in TF. You don't need to re-insert into the tf.dataset API.\r\n\r\n```\r\ndef representative_data_gen():\r\n  for input_value in train_dataset.batch(1).take(100):\r\n    # Model has only one input so each data point has one element.\r\n```\r\n\r\nHowever, you need to preprocess the data from dataset correctly to feed the correct input value to your model.\r\n\r\nYou can rely on the conversion without the representative dataset like the below one and I verified that it is working with your colab:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n#converter.representative_dataset = representative_data_gen\r\n\r\ntflite_model_quant = converter.convert()\r\n```", "For int8 input and output type overriding in the target model, you can use the following code snippet but you need a representative_data_gen method.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\n\r\ntflite_model_quant = converter.convert()\r\n```", "Hi, @abattery and @amahendrakar,\r\nThanks for your time,\r\nI modified the code to get the images from the URL and added the code snippet. Still, it has the error. It's quantized but not with int8 input.\r\nColab link again; https://colab.research.google.com/drive/1hlTykr4-rUYqI-n10icVEywxUhyar364?usp=sharing", "@Paryavi,\r\nOn running the Colab notebook, I see that the input and output are `int8`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/60de6ba4bb568f813987d23f46c295a8/47027.ipynb#scrollTo=zMByxhf97oVQ&line=4&uniqifier=1). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thanks, @amahendrakar, Yes I was able to fix it using @abattery help. Thank you both!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47027\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47027\">No</a>\n"]}, {"number": 47026, "title": "TPU, model.fit : 2GB of RAM limit. tf.version 2.4", "body": "**Describe the current behavior**\r\n\r\nThere is an error (\"\"Session crashed for unknown reason\"\"), when trying to use a dataset with the size more than 2GB of RAM.\r\nError is reproducible for different step size (256,1024,8192).\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error when feeding a dataset which is more than 2GB of RAM.\r\n\r\n**Code to reproduce the issue**\r\n\r\nTo reproduce, just copy the following code to Colab with TPU enabled.\r\n\r\nThe bug can happen due to 2GB limit in protobuf (since tensorflow relies on it).\r\nhttps://stackoverflow.com/questions/34128872/google-protobuf-maximum-size\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimport distutils\r\nif distutils.version.LooseVersion(tf.__version__) < '1.14':\r\n    raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/fashion_mnist.ipynb')\r\n\r\nimport os\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_cluster(resolver)\r\n# This is the TPU initialization code that has to be at the beginning.\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))\r\n\r\n# optimizer = tf.tpu.CrossShardOptimizer(tf.train.GradientDescentOptimizer(0.01))\r\n\r\nstrategy = tf.distribute.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.applications.VGG16(input_shape=(32,32,3),classes=10, weights=None)#  create_model()\r\n  optimizer = tf.keras.optimizers.Adam()\r\n  model.compile(\r\n      optimizer=optimizer,\r\n      loss='mse',\r\n      metrics=['mse'])\r\n  \r\n  training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\r\n  training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n      'training_accuracy', dtype=tf.float32)\r\n\r\n\r\n# 1024*192 * 32*32*3 * 4bytes = 2 415 919 104 bytes\r\n# ~ 2GB. \"Session crashed for unknown reason\"\r\nX = np.zeros((1024*192, 32,32,3),dtype=np.float32)\r\ny = np.ones((1024*192, 10),dtype=np.float32)\r\n\r\n# ~ 1GB. Works\r\n# X = np.zeros((1024*192//2, 32,32,3),dtype=np.float32)\r\n# y = np.ones((1024*192//2, 10),dtype=np.float32)\r\n\r\nmodel.fit(\r\n    X,y,\r\n    epochs=10,\r\n    steps_per_epoch=1024, #batch size is 192 per TPU\r\n)\r\n```\r\n```\r\nprint(tf.version.VERSION)\r\nprint(tf.version.GIT_VERSION)\r\n2.4.1\r\nv2.4.1-0-g85c8b2a817f\r\n```\r\n\r\n**Logs**\r\n![106828116-a4958500-66dd-11eb-9ae6-06606e561bc4](https://user-images.githubusercontent.com/27484172/107322586-d735e800-6af8-11eb-9be7-5035a2bda197.png)", "comments": ["This looks like pretty much the same one you filed a while back:\r\n\r\nhttps://github.com/tensorflow/tpu/issues/533\r\n\r\nNothing in particular has changed yet -- the large numpy tenosr is pushing the rpc to 2GB boundary.\r\n\r\nYou could try wrap it up with tf.dataset.from_tensors or from_tensor_slices to feed data in a smaller batch fashion.\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensors", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47026\">No</a>\n"]}, {"number": 47025, "title": "Update sqlite to the latest sqlite-amalgamation-3340100", "body": "This PR updates sqlite to the latest sqlite-amalgamation-3340100\r\n(released in 2021-01-20)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 47024, "title": "Issue in converting input output from float 32 to int8(Convert using float fallback quantization),", "body": "Hi There, I have an issue in converting a model with float 32 input to int 8;\r\nMy code; https://colab.research.google.com/drive/1EGMqQlos_NovF3qakNVo0PLgvvZukLtB#scrollTo=TqOt6Sv7AsMi\r\n\r\nDetails:\r\nI used standard transfer learning code;\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb\r\n\r\n just instead of loading data from the website, I loaded data folders from my computer (instead of cat I wanna detect an invasive beetle!), then I  tried to convert the input of the model from float 32 to int 8 using the link below guideline;\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#scrollTo=FiwiWU3gHdkW\r\n\r\nSpecifically this cell from abovementioned quantization guidline generates the following issue;\r\ndef representative_data_gen():\r\n  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\r\n    # Model has only one input so each data point has one element.\r\n    yield [input_value]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_model_quant = converter.convert()\r\n\r\n I get the following issue;  ValueError: Unbatching a dataset is only supported for rank >= 1\r\n\r\n\r\n-----------------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-167-9f553cafd0bd> in <module>\r\n      8 converter.representative_dataset = representative_data_gen\r\n      9 \r\n---> 10 tflite_model_quant = converter.convert()\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    871           graph=frozen_func.graph)\r\n    872 \r\n--> 873     return super(TFLiteKerasModelConverterV2,\r\n    874                  self).convert(graph_def, input_tensors, output_tensors)\r\n    875 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    630     calibrate_and_quantize, flags = quant_mode.quantizer_flags()\r\n    631     if calibrate_and_quantize:\r\n--> 632       result = self._calibrate_quantize_model(result, **flags)\r\n    633 \r\n    634     flags_modify_model_io_type = quant_mode.flags_modify_model_io_type(\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type, activations_type, allow_float)\r\n    457       return _mlir_quantize(calibrated)\r\n    458     else:\r\n--> 459       return calibrate_quantize.calibrate_and_quantize(\r\n    460           self.representative_dataset.input_gen, inference_input_type,\r\n    461           inference_output_type, allow_float, activations_type)\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float, activations_type, resize_input)\r\n     91     \"\"\"\r\n     92     initialized = False\r\n---> 93     for sample in dataset_gen():\r\n     94       if not initialized:\r\n     95         initialized = True\r\n\r\n<ipython-input-167-9f553cafd0bd> in representative_data_gen()\r\n      1 def representative_data_gen():\r\n----> 2   for input_value in tf.data.Dataset.from_tensor_slices(train_dataset).batch(1).take(20):\r\n      3     # Model has only one input so each data point has one element.\r\n      4     yield [input_value]\r\n      5 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in from_tensor_slices(tensors)\r\n    689       Dataset: A `Dataset`.\r\n    690     \"\"\"\r\n--> 691     return TensorSliceDataset(tensors)\r\n    692 \r\n    693   class _GeneratorState(object):\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in __init__(self, element)\r\n   3155     element = structure.normalize_element(element)\r\n   3156     batched_spec = structure.type_spec_from_value(element)\r\n-> 3157     self._tensors = structure.to_batched_tensor_list(batched_spec, element)\r\n   3158     self._structure = nest.map_structure(\r\n   3159         lambda component_spec: component_spec._unbatch(), batched_spec)  # pylint: disable=protected-access\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py in to_batched_tensor_list(element_spec, element)\r\n    362   # pylint: disable=protected-access\r\n    363   # pylint: disable=g-long-lambda\r\n--> 364   return _to_tensor_list_helper(\r\n    365       lambda state, spec, component: state + spec._to_batched_tensor_list(\r\n    366           component), element_spec, element)\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py in _to_tensor_list_helper(encode_fn, element_spec, element)\r\n    337     return encode_fn(state, spec, component)\r\n    338 \r\n--> 339   return functools.reduce(\r\n    340       reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\r\n    341 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py in reduce_fn(state, value)\r\n    335   def reduce_fn(state, value):\r\n    336     spec, component = value\r\n--> 337     return encode_fn(state, spec, component)\r\n    338 \r\n    339   return functools.reduce(\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py in <lambda>(state, spec, component)\r\n    363   # pylint: disable=g-long-lambda\r\n    364   return _to_tensor_list_helper(\r\n--> 365       lambda state, spec, component: state + spec._to_batched_tensor_list(\r\n    366           component), element_spec, element)\r\n    367 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\TF2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in _to_batched_tensor_list(self, value)\r\n   3322   def _to_batched_tensor_list(self, value):\r\n   3323     if self._dataset_shape.ndims == 0:\r\n-> 3324       raise ValueError(\"Unbatching a dataset is only supported for rank >= 1\")\r\n   3325     return self._to_tensor_list(value)\r\n   3326 \r\n\r\nValueError: Unbatching a dataset is only supported for rank >= 1\r\n--------------------------------------", "comments": ["The error is coming from ```tf.data.Dataset.from_tensor_slices(train_dataset).batch(1).take(20)``` not from the TFLite converter API.\r\n\r\nCould you make sure that the above code is correct and if not, try to fix it in your side?", "Right, I know, the error is from tf.data.Dataset.from_tensor_slices(train_dataset).batch(1).take(20), \r\nbecause I was able to convert it to tflite but still as you know input/ouput is float32, so I need to convert it to int8 so I used tf.data.Dataset.from_tensor_slices(train_dataset).batch(1).take(20), but I do not know how to fix it; In take I put take(1) for example but still says \"Unbatching a dataset is only supported for rank >= 1\"", "The slice code comes from TF guideline; https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb#scrollTo=FiwiWU3gHdkW\r\nUnfortunately, I do not know more details about the code they wrote.\r\n", "If you don't have correct training data set, you can remove the line, `converter.representative_dataset = representative_data_gen`.\r\n\r\nFor fixing the error, you can refer to the following document.\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices", "Thanks abattery, I will check it out!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47024\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47024\">No</a>\n", "Duplicate issue of https://github.com/tensorflow/tensorflow/issues/47027", "Dear Chung,\n\nI tried with \"correct dataset\" in another simpler post that you just\nclosed, but it didn't work either!\n\nPlease open that again to see what should we do with it.\n\nI removed that line you mentioned too, it was not working!\n\nOn Mon, Feb 8, 2021, 6:23 PM Jae sung Chung <notifications@github.com>\nwrote:\n\n> If you don't have correct training data set, you can remove the line, converter.representative_dataset\n> = representative_data_gen.\n>\n> For the error, you can refer to the following document to fix.\n>\n> https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47024#issuecomment-775653215>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AO7JQRH5VSHPG2EJMYURFEDS6C2FVANCNFSM4XKGL4AA>\n> .\n>\n", "You can comment back on new thread with your colab.", "Please don't create duplicate issues. It just creates work for the triage team.", "Ok, thanks.\n\nOn Mon, Feb 8, 2021, 11:44 PM Jae sung Chung <notifications@github.com>\nwrote:\n\n> Please don't create duplicate issues. It just creates work for the triage\n> team.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47024#issuecomment-775804710>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AO7JQRCTJJWRRSBIV3EWE3DS6D7WNANCNFSM4XKGL4AA>\n> .\n>\n"]}, {"number": 47023, "title": "C++ compilation of rule '//tensorflow/core/kernels/image:extract_image_patches_op' failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tensorflow-2.4.0.tar.gz\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n**Describe the problem**\r\n[root@tf-build-asr tensorflow-2.4.0]# bazel build -c opt  //tensorflow:libtensorflow_cc.so\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=236\r\nINFO: Reading rc options for 'build' from /opt/swan/tensorflow-2.4.0/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /opt/swan/tensorflow-2.4.0/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /opt/swan/tensorflow-2.4.0/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/python3/lib/python3.7/site-packages --python_path=/usr/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /opt/swan/tensorflow-2.4.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /opt/swan/tensorflow-2.4.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /opt/swan/tensorflow-2.4.0/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /opt/swan/tensorflow-2.4.0/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /opt/swan/tensorflow-2.4.0/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow:libtensorflow_cc.so (0 packages loaded, 9 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /opt/swan/tensorflow-2.4.0/tensorflow/core/kernels/image/BUILD:229:18: C++ compilation of rule '//tensorflow/core/kernels/image:extract_image_patches_op' failed (Exit 1): gcc failed: error executing command /usr/local/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 142 argument(s) skipped)\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:117:0,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/kernels/image/extract_image_patches_op.h:19,\r\n                 from tensorflow/core/kernels/image/extract_image_patches_op.cc:21:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h: In static member function 'static void Eigen::internal::EvalRange<Evaluator, StorageIndex, true>::run(Evaluator*, StorageIndex, StorageIndex) [with Evaluator = Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 4, 1, int>, 16, Eigen::MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<int, 4>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const std::complex<float>, 4, 1, int>, 16, Eigen::MakePointer> > > >, Eigen::ThreadPoolDevice>; StorageIndex = int]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h:546:7: internal compiler error: in emit_move_insn, at expr.c:3698\r\n       values[i] = coeff(index+i);\r\n       ^~~~~~\r\n0x899fba emit_move_insn(rtx_def*, rtx_def*)\r\n\t../.././gcc/expr.c:3697\r\n0x88a34d store_bit_field_1\r\n\t../.././gcc/expmed.c:814\r\n0x88a9b8 store_bit_field(rtx_def*, unsigned long, unsigned long, unsigned long, unsigned long, machine_mode, rtx_def*, bool)\r\n\t../.././gcc/expmed.c:1122\r\n0x8a4a1e store_field\r\n\t../.././gcc/expr.c:6974\r\n0x8a204b expand_assignment(tree_node*, tree_node*, bool)\r\n\t../.././gcc/expr.c:5209\r\n0x7b3c01 expand_call_stmt\r\n\t../.././gcc/cfgexpand.c:2656\r\n0x7b3c01 expand_gimple_stmt_1\r\n\t../.././gcc/cfgexpand.c:3571\r\n0x7b3c01 expand_gimple_stmt\r\n\t../.././gcc/cfgexpand.c:3737\r\n0x7b4ddf expand_gimple_basic_block\r\n\t../.././gcc/cfgexpand.c:5744\r\n0x7b9f46 execute\r\n\t../.././gcc/cfgexpand.c:6357\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nPlease include the complete backtrace with any bug report.\r\nSee <https://gcc.gnu.org/bugs/> for instructions.\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 931.265s, Critical Path: 45.73s\r\nINFO: 945 processes: 6 internal, 939 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build -c opt  //tensorflow:libtensorflow_cc.so\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@wangkaisine,\r\nCould you please check if you are facing the same issue with TF v2.4.1 and TF-nightly as well?\r\n\r\nAlso, please provide all the commands that you executed before running into the error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47023\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47023\">No</a>\n"]}, {"number": 47021, "title": "text_dataset_from_directory doesn't work if there isn't at least one subdirectory", "body": "**System information**\r\n- OS Platform: Windows 10\r\n- TensorFlow version: 2.3.2\r\n- Python version: 3.8.5\r\n\r\n**Issue**\r\n`tf.keras.preprocessing.text_dataset_from_directory` doesn't work if you have a directory structure like this:\r\n\r\ndata_dir/\r\n--text_1.txt\r\n--text_2.txt\r\n--text_3.txt\r\n````\r\nds = tf.keras.preprocessing.text_dataset_from_directory(\r\n    data_dir, labels=[0,0,1], label_mode=\"int\", \r\n    batch_size=NUM_FILES_PER_TENSOR, validation_split=None, \r\n    subset=None, shuffle=False, seed=seed\r\n)\r\n````\r\nAccording to the documentation, the above should work since \"inferred\" is not passed to the `labels` argument. But instead you get this error:\r\n\r\n````\r\nValueError: Expected the lengths of `labels` to match the number of files in the target directory. len(labels) is 3 while we found 0 files\r\n````\r\n\r\nBut if you change the structure to this and run the exact same code, it works:\r\n\r\ndata_dir/\r\n--dummy/\r\n----text_1.txt\r\n----text_2.txt\r\n----text_3.txt\r\n\r\nEven worse is that the labels you passed are simply ignored. It prints this message: `Found 3 files belonging to 1 classes`. It feels like the \"inferred\" option (which is the default) was properly implemented and the rest are broken.", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly(`2.5.0-dev20210208`). Please find the gist of it [here](https://colab.research.google.com/gist/ravikyram/e15a8f99bee2ca0b1103c675e526ea66/untitled666.ipynb). Thanks!", "@princyok,\r\nI was able to run the code without any issues by passing the the parent directory as the `directory` argument, instead of `data_dir`.\r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2a5feff0651cd748e9129eab69d68069/47021.ipynb). Thanks!", "@amahendrakar \r\n\r\nThat's exactly the problem, just as the title says. It leads to other issues as explained in the main post above.", "Was able to replicate the issue in TF v2.5 ,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/a9d04923f81d685319797d29f504e794/untitled159.ipynb?authuser=1)..Thanks !", "Thanks for your issue. I think this is working as intended. \r\nThe directory structure is expected to like - \r\n```python\r\nmain_directory/\r\n...class_a/\r\n......a_text_1.txt\r\n......a_text_2.txt\r\n...class_b/\r\n......b_text_1.txt\r\n......b_text_2.txt\r\n```\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory#used-in-the-notebooks\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47021\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47021\">No</a>\n"]}]