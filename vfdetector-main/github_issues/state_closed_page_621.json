[{"number": 35016, "title": "Adding- \"COntinuous COin Betting\" Optimization Algorithm", "body": "Adding a new optimization algorithm named- COntinuous COin Betting(https://arxiv.org/pdf/1705.07795.pdf)\r\nThis algorithm gives a faster convergence rate compared to some of the other algorithms.", "comments": ["I believe this really belongs to https://github.com/tensorflow/addons repository.\r\n\r\nAlso, you should use `optimizer_v2` as a base class, and probably add some tests (see the structure of `optimizer_v2` subdirectory).", "+1 to addons. There is even a dedicated section for optimizers.", "Closing this as this PR belongs to addons. Thank you for your contribution."]}, {"number": 35015, "title": "Adding- \"COntinuous COin Betting\" Optimization Algorithm", "body": "Adding a new optimization algorithm named- COntinuous COin Betting(https://arxiv.org/pdf/1705.07795.pdf)\r\nThis algorithm gives a faster convergence rate compared to some of the other algorithms.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35015) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35015) for more info**.\n\n<!-- ok -->"]}, {"number": 35014, "title": "Build error due to missing dependency declarations in double-conversion", "body": "**System information**\r\n- OS Platform and Distribution : CentOS-8.0\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: from source using spack\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): gcc-7.4.0\r\n- CUDA/cuDNN version: CUDA@10.1.243/cuDNN@7.6.5.32-10.1\r\n- GPU model and memory: 2x NVIDIA Quadro P4000\r\n\r\n**Describe the problem**\r\n\r\nI used [`spack`](https://github.com/spack/spack/) to build all dependencies for tensorflow including bazel. As per the build logs from spack, the following procedure was attempted to install tensorflow from source : \r\n```\r\n'./configure'\r\n```\r\n```\r\n'bazel' '--nohome_rc' '--nosystem_rc' '--output_user_root=/tmp/spack/tf' 'build' '--color=no' '--jobs=16' '--config=opt' '--config=cuda' '--config=noaws' '--config=nohdfs' '--config=noignite' '--config=nokafka' '--config=v2' '--cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0' '//tensorflow/tools/pip_package:build_pip_package'\r\n```\r\nFull error log from spack : [error_log](https://pastebin.com/n6qfsrVr)\r\n\r\nThe error comes from `'@double_conversion//:double-conversion': ` : \r\n```\r\n[sajid@xrmlite ~]$ cat error_log | grep -a3b3 ERROR\r\n80532-INFO: Deleting stale sandbox base /tmp/spack/tf/ba2957b26f360b54b039cbd58767fa4e/sandbox\r\n80621-[6 / 38] [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\n80683-[48 / 1,263] [Prepa] Executing genrule @local_config_nccl//:nccl-files [for host] ... (6 actions, 5 running)\r\n80792:ERROR: /tmp/spack/tf/ba2957b26f360b54b039cbd58767fa4e/external/double_conversion/BUILD.bazel:12:1: undeclared inclusion(s) in rule '@double_conversion//:double-conversion':\r\n80965-this rule is missing dependency declarations for the following files included by 'external/double_conversion/double-conversion/double-conversion.cc':\r\n81115-  '/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.4.0/gettext-0.20.1-yjkttko6qwne6igwp2cabop2d4p2g3ff/include/libintl.h'\r\n81256-[75 / 1,297] Executing genrule @local_config_cuda//cuda:cuda-lib [for host]; 2s local ... (14 actions, 2 running)\r\n[sajid@xrmlite ~]$\r\n```\r\n\r\nAdditional Info : \r\n\r\nThe full dependency tree for building `py-tensoflow` is show below. As can be seen, `gettext` is a dependency for `python` built from source. This `python` is both a dependency of `bazel` and the `python` we want to eventually link `py-tensorflow` against. \r\n\r\n```\r\n[sajid@xrmlite ~]$ spack spec -I  py-tensorflow cuda_arch=61  ^/ppi2muw  ^cuda@10.1.243 ^python@3.7.4 %gcc@7.4.0\r\n\r\n.....\r\n\r\nConcretized\r\n--------------------------------\r\n -   py-tensorflow@2.0.0%gcc@7.4.0~android~aws~computecpp+cuda cuda_arch=61 ~dynamic_kernels+gcp~gdr~hdfs~ignite~ios~jemalloc~kafka~mkl~monolithic~mpi+nccl~ngraph~numa~opencl patches=c49766a976e5c25c3827036828df0a2630e511bd783ac9cdcc6fc13068b22fac ~rocm~tensorrt~verbs~xla arch=linux-centos8-broadwell\r\n[+]      ^bazel@0.26.1%gcc@7.4.0 patches=3e6448a0dde42bbd72568d29c5646d370dd62ca300cdd10a630908c086844167,75fad08d2a118372ed86696832942c7903bb716af28e5af969d8e20857655cf4,aa926467d3fc2bcd338ccc6355bc9f56adfd18dd3b4e1813a4ce8daee9e34600 arch=linux-centos8-broadwell\r\n[+]          ^jdk@1.8.0_131-b11%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]          ^python@3.7.4%gcc@7.4.0+bz2+ctypes+dbm+lzma~nis~optimizations+pic+pyexpat+pythoncmd+readline+shared+sqlite3+ssl~tix~tkinter~ucs4~uuid+zlib arch=linux-centos8-broadwell\r\n[+]              ^bzip2@1.0.8%gcc@7.4.0+shared arch=linux-centos8-broadwell\r\n[+]              ^expat@2.2.9%gcc@7.4.0+libbsd arch=linux-centos8-broadwell\r\n[+]                  ^libbsd@0.10.0%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]              ^gdbm@1.18.1%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]                  ^readline@8.0%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]                      ^ncurses@6.1%gcc@7.4.0~symlinks~termlib arch=linux-centos8-broadwell\r\n[+]              ^gettext@0.20.1%gcc@7.4.0+bzip2+curses+git~libunistring+libxml2+tar+xz arch=linux-centos8-broadwell\r\n[+]                  ^libxml2@2.9.9%gcc@7.4.0~python arch=linux-centos8-broadwell\r\n[+]                      ^libiconv@1.16%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]                      ^xz@5.2.4%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]                      ^zlib@1.2.11%gcc@7.4.0+optimize+pic+shared arch=linux-centos8-broadwell\r\n[+]                  ^tar@1.32%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]              ^libffi@3.2.1%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]              ^openssl@1.1.1d%gcc@7.4.0+systemcerts arch=linux-centos8-broadwell\r\n[+]              ^sqlite@3.30.1%gcc@7.4.0~column_metadata+fts~functions~rtree arch=linux-centos8-broadwell\r\n[+]      ^cuda@10.1.243%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^cudnn@7.6.5.32-10.1-linux-x64%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^curl@7.63.0%gcc@7.4.0~darwinssl~gssapi~libssh~libssh2~nghttp2 arch=linux-centos8-broadwell\r\n[+]      ^nccl@2.4.8-1%gcc@7.4.0 patches=42778c78eb9875dacddf5eca20f7f6a077773fcbee41e51174f81b3143684b6d arch=linux-centos8-broadwell\r\n[+]      ^py-absl-py@0.7.0%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]          ^py-setuptools@41.4.0%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]          ^py-six@1.12.0%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^py-astor@0.8.0%gcc@7.4.0 patches=edc5eeddabe153b08e938f52edaeb2d880ee3128082967f310db0f98510fe6e0 arch=linux-centos8-broadwell\r\n[+]      ^py-gast@0.2.2%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^py-google-pasta@0.1.8%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^py-grpcio@1.25.0%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]          ^c-ares@1.15.0%gcc@7.4.0 build_type=RelWithDebInfo arch=linux-centos8-broadwell\r\n[+]              ^cmake@3.15.5%gcc@7.4.0~doc+ncurses+openssl+ownlibs patches=3387faf4a71efe81c0fa17410b270ca7d352081ac88d2322df3da9bb6a6a3f2d ~qt arch=linux-centos8-broadwell\r\n[+]          ^py-cython@0.29.13%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^py-keras-applications@1.0.8%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^py-keras-preprocessing@1.1.0%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^py-numpy@1.17.4%gcc@7.4.0+blas+lapack arch=linux-centos8-broadwell\r\n[+]          ^openblas@0.3.7%gcc@7.4.0+avx2~avx512 cpu_target=auto ~ilp64+pic+shared threads=none ~virtual_machine arch=linux-centos8-broadwell\r\n[+]      ^py-opt-einsum@3.1.0%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^py-protobuf@3.11.0%gcc@7.4.0~cpp arch=linux-centos8-broadwell\r\n[+]      ^py-termcolor@1.1.0%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^py-wheel@0.33.1%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^py-wrapt@1.11.2%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]      ^swig@4.0.0%gcc@7.4.0 arch=linux-centos8-broadwell\r\n[+]          ^pcre@8.42%gcc@7.4.0~jit+multibyte+utf arch=linux-centos8-broadwell\r\n[+]          ^pkgconf@1.6.3%gcc@7.4.0 arch=linux-centos8-broadwell\r\n```\r\n\r\nThe environment for building `py-tensorflow` is described in [`spack-build-env.txt`](https://pastebin.com/MuWi6BAK). \r\n\r\nContents of [`.bazelrc`](https://pastebin.com/6TTEMMSu)\r\n\r\nThe spack [build recipe](https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/py-tensorflow/package.py) for `py-tensorlfow`. \r\n\r\nDownstream tracker : https://github.com/spack/spack/issues/14105\r\n\r\n", "comments": ["I attempted this again after adding a few debug flags in the build recipe as per https://github.com/spack/spack/pull/14177/files. This time, I see a similar error where `nasm` complains about a `PyObject` in the header `eval.h` from the `python` standard library. I'm mystified as to how building `nasm`, which doesn't even depend on `python` according to the [BUILD.bazel](https://github.com/tensorflow/tensorflow/blob/master/third_party/nasm/BUILD.bazel) could raise this error.\r\n\r\nI'm sharing the following debug files : \r\n- `command.log`\r\n- `java.log`\r\n- `spack-build.out`\r\n- `BUILD` from the following locations  :\r\n- `local_config_python` \r\n- `local_config_cc`\r\n- `local_config_cuda/cuda`\r\n- `local_config_cuda/crosstool`\r\n- `cc_toolchain_config.bzl` and `cc_wrapper.sh` from `local_config_cc`\r\n- `cc_toolchain_config.bzl` from `local_config_cuda/crosstool`\r\n\r\navailable at this link : `https://northwestern.box.com/s/yxhh3uzj72nj3nqn81xcvkozwzqslrhs`\r\n\r\nI asked for an explanation of Bazel's behavior via `--explain=explainlogfile.txt'` but I didn't see any such file in the build directory. Where should I look for this file ?\r\n\r\nIs there a way to not let Bazel build any of the dependencies for tensorflow and use local installations instead ?\r\n", "I recommend reaching out to spack developers, or the spack package maintainer.\r\nWe do not maintain any spack related documentation.\r\n\r\nFor using system installed dependencies, @perfinion may have some pointers.", "Hi @gunan, I'm a Spack developer and I personally maintain the TensorFlow and PyTorch packages in Spack.\r\n\r\nI don't believe the problem above is specific to Spack, and should be possible to reproduce outside of Spack. If we are doing something wrong in the Spack package, let me know and I can fix it.", "```\r\nthis rule is missing dependency declarations for the following files included by \r\n```\r\nThe above error (which references a system header) usually means the bazel c++ toolchain was misconfigured. Bazel did not detect the path, in this case `/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.4.0/gettext-0.20.1-yjkttko6qwne6igwp2cabop2d4p2g3ff/` a part of the C++ toolchain.\r\nThe way we make bazel use a different toolchain than system default gcc is to create a toolchain definition here: https://github.com/tensorflow/tensorflow/tree/master/third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010\r\n\r\nYou may need to do that, and direct the bazel executions under spack to use that, or there may be an easier way to redirect bazel using some environment variables (which you have to check with bazel team).\r\n", "I added the `cc_toolchain_config.bzl` in the folder as well, [available here](https://northwestern.box.com/s/yxhh3uzj72nj3nqn81xcvkozwzqslrhs). Looking closely at the toolchain config, I do see that it includes a bunch of libraries : \r\n```\r\n\r\n    tool_paths = [\r\n        tool_path ( name= \"ar\", path= \"/usr/bin/ar\" ),\r\n        tool_path ( name= \"ld\", path= \"/home/sajid/packages/spack/lib/spack/env/ld\" ),\r\n        tool_path ( name= \"cpp\", path= \"/home/sajid/packages/spack/lib/spack/env/cpp\" ),\r\n        tool_path ( name= \"gcc\", path= \"/home/sajid/packages/spack/lib/spack/env/gcc/gcc\" ),\r\n        tool_path ( name= \"dwp\", path= \"/usr/bin/dwp\" ),\r\n        tool_path ( name= \"gcov\", path= \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-9.2.0-yaowtafvg5f7v3f2en47gpta5b6ucvlq/bin/gcov\" ),\r\n        tool_path ( name= \"nm\", path= \"/usr/bin/nm\" ),\r\n        tool_path ( name= \"objcopy\", path= \"/usr/bin/objcopy\" ),\r\n        tool_path ( name= \"objdump\", path= \"/usr/bin/objdump\" ),\r\n        tool_path ( name= \"strip\", path= \"/usr/bin/strip\" ),\r\n    ]\r\n\r\n    cxx_builtin_include_directories = [\r\n\"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/lib/gcc/x86_64-pc-linux-gnu/7.3.0/include\",\r\n    \"/usr/local/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/lib/gcc/x86_64-pc-linux-gnu/7.3.0/include-fixed\",\r\n    \"/usr/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/openssl-1.1.1d-4bo5gg2eookuk2ubczsa4ciaz5c4mbwz/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/bzip2-1.0.8-5zisvxvepuimx27rqxi53srnkqj6jqlk/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cudnn-7.6.5.32-10.1-linux-x64-sabespboxtmnfvkohzmmackhv7tjvbqb/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cuda-10.1.243-wciaq2rrrkc4w6vcraocheah55wrekha/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/gdbm-1.18.1-ek2j6ymdsfdhwtcsgm5typyeqr2d427s/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/readline-8.0-sdz3prqngfpcyiooqriil7iyy3mp77nr/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libxml2-2.9.9-s5wjt2encrutai46rptb2qdbuqm74rcj/include/libxml2\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libffi-3.2.1-mhdz37flhbznp5bla7dxtvtuwqxlzdpr/lib/libffi-3.2.1/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/gettext-0.20.1-3x3th3xdaosnjd7ttly2mym5hukdqln5/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/include/python3.7m\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/ncurses-6.1-bnt5ehudftgrg5smastj4ejhjvyxfjmj/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/xz-5.2.4-oc7pfgyjo477pshkef56rkv4ovz7ykei/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/nccl-2.4.8-1-phuasbwfolmvjlo5mmizcuz4vqeuayhk/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/expat-2.2.9-v4k636wrhszysgefq2sijry72mj5l2au/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libbsd-0.10.0-jhowpdxutwgnbbpqv5dl3ifkfmrmuck5/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/curl-7.63.0-jrvggtmsfzn64chuvcenb7a4g7lznfvj/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/sqlite-3.30.1-kifllx2du5fe2yqreol2ldfk7zzqrhm2/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libiconv-1.16-qeeepkzh7y5dnj75fidj2abx23x6nwbl/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/zlib-1.2.11-ah6qvpjr2sam2rpn527ivwmz4vyfcqbf/include\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/include/c++/7.3.0\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/include/c++/7.3.0/x86_64-pc-linux-gnu\",\r\n    \"/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/include/c++/7.3.0/backward\"\r\n    ]\r\n```\r\n\r\nIf I understand correctly, these include paths shouldn't be in the compiler toolchain definition, right ?\r\n ", "What order are those include paths checked? Maybe `/usr/include` or `/usr/local/include` are overriding the other directories we add.\r\n\r\nFor the record, we use the following patch to bazel (and corresponding environment variable) to inject these include paths into the build:\r\n```patch\r\n--- a/tools/cpp/unix_cc_configure.bzl\r\n+++ b/tools/cpp/unix_cc_configure.bzl\r\n@@ -145,11 +145,18 @@ def get_escaped_cxx_inc_directories(repository_ctx, cc, lang_flag, additional_fl\r\n     else:\r\n         inc_dirs = result.stderr[index1 + 1:index2].strip()\r\n \r\n-    return [\r\n+    default_inc_directories = [\r\n         _prepare_include_path(repository_ctx, _cxx_inc_convert(p))\r\n         for p in inc_dirs.split(\"\\n\")\r\n     ]\r\n \r\n+    env = repository_ctx.os.environ\r\n+    if \"SPACK_INCLUDE_DIRS\" in env:\r\n+        for path in env[\"SPACK_INCLUDE_DIRS\"].split(\":\"):\r\n+            default_inc_directories.append(path)\r\n+\r\n+    return default_inc_directories\r\n+\r\n def _is_compiler_option_supported(repository_ctx, cc, option):\r\n     \"\"\"Checks that `option` is supported by the C compiler. Doesn't %-escape the option.\"\"\"\r\n     result = repository_ctx.execute([\r\n```", "Looking at this more closely, I see that the exact command that is failing is \r\n```\r\n (cd /tmp/spack/tf/3d4affddb55a47900a919f49b073413c/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/lib/spack/env/gcc:/home/sajid/packages/spack/lib/spack/env/case-insensitive:/home/sajid/packages/spack/lib/spack/env:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cuda-10.1.243-wciaq2rrrkc4w6vcraocheah55wrekha/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/py-wheel-0.33.1-op2hwc4hqsgmc4qhevj3tq3tvw7civj2/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/py-numpy-1.17.4-uxmerwuy33filyrx4sw5rkfawty2vlup/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/curl-7.63.0-jrvggtmsfzn64chuvcenb7a4g7lznfvj/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/swig-4.0.0-mvastxbkyr3gnidrgierbrulpj6eys5u/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/py-numpy-1.17.4-uxmerwuy33filyrx4sw5rkfawty2vlup/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/jdk-1.8.0_231-b11-ynnz6lmhhw5mngtkw74hwcnn5ungicak/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/bazel-0.26.1-fgoxd6mcj3iyn7z43j7fq7dyjb5okpm7/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/py-setuptools-41.4.0-ctklysjoeronag3sfh44ptqfrcs6cuov/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/py-setuptools-41.4.0-ctklysjoeronag3sfh44ptqfrcs6cuov/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cuda-10.1.243-wciaq2rrrkc4w6vcraocheah55wrekha/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/py-wheel-0.33.1-op2hwc4hqsgmc4qhevj3tq3tvw7civj2/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/py-numpy-1.17.4-uxmerwuy33filyrx4sw5rkfawty2vlup/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/curl-7.63.0-jrvggtmsfzn64chuvcenb7a4g7lznfvj/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/swig-4.0.0-mvastxbkyr3gnidrgierbrulpj6eys5u/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/bazel-0.26.1-fgoxd6mcj3iyn7z43j7fq7dyjb5okpm7/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/py-setuptools-41.4.0-ctklysjoeronag3sfh44ptqfrcs6cuov/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/bin:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-9.2.0-yaowtafvg5f7v3f2en47gpta5b6ucvlq/bin:/home/sajid/packages/spack/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin \\\r\n    PWD=/proc/self/cwd \\\r\n    SPACK_CC=/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/bin/gcc \\\r\n    SPACK_CC_RPATH_ARG=-Wl,-rpath, \\\r\n    SPACK_COMPILER_IMPLICIT_RPATHS=/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/lib/gcc/x86_64-pc-linux-gnu/7.3.0:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/lib64 \\\r\n    SPACK_COMPILER_SPEC=gcc@7.3.0 \\\r\n    SPACK_CXX=/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/bin/g++ \\\r\n    SPACK_CXX_RPATH_ARG=-Wl,-rpath, \\\r\n    SPACK_DEBUG_LOG_DIR=/home/sajid \\\r\n    SPACK_DEBUG_LOG_ID=py-tensorflow-a73jdej \\\r\n    SPACK_DTAGS_TO_ADD=--disable-new-dtags \\\r\n    SPACK_DTAGS_TO_STRIP=--enable-new-dtags \\\r\n    SPACK_ENV_PATH=/home/sajid/packages/spack/lib/spack/env:/home/sajid/packages/spack/lib/spack/env/case-insensitive:/home/sajid/packages/spack/lib/spack/env/gcc \\\r\n    SPACK_F77=/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/bin/gfortran \\\r\n    SPACK_F77_RPATH_ARG=-Wl,-rpath, \\\r\n    SPACK_FC=/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-8.2.1/gcc-7.3.0-uhzsamzf47moqf6gu2exxqsyg5gw2pbx/bin/gfortran \\\r\n    SPACK_FC_RPATH_ARG=-Wl,-rpath, \\\r\n    SPACK_INCLUDE_DIRS=/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/openssl-1.1.1d-4bo5gg2eookuk2ubczsa4ciaz5c4mbwz/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/bzip2-1.0.8-5zisvxvepuimx27rqxi53srnkqj6jqlk/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cudnn-7.6.5.32-10.1-linux-x64-sabespboxtmnfvkohzmmackhv7tjvbqb/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cuda-10.1.243-wciaq2rrrkc4w6vcraocheah55wrekha/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/gdbm-1.18.1-ek2j6ymdsfdhwtcsgm5typyeqr2d427s/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/readline-8.0-sdz3prqngfpcyiooqriil7iyy3mp77nr/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libxml2-2.9.9-s5wjt2encrutai46rptb2qdbuqm74rcj/include/libxml2:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libffi-3.2.1-mhdz37flhbznp5bla7dxtvtuwqxlzdpr/lib/libffi-3.2.1/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/gettext-0.20.1-3x3th3xdaosnjd7ttly2mym5hukdqln5/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/include/python3.7m:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/ncurses-6.1-bnt5ehudftgrg5smastj4ejhjvyxfjmj/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/xz-5.2.4-oc7pfgyjo477pshkef56rkv4ovz7ykei/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/nccl-2.4.8-1-phuasbwfolmvjlo5mmizcuz4vqeuayhk/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/expat-2.2.9-v4k636wrhszysgefq2sijry72mj5l2au/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libbsd-0.10.0-jhowpdxutwgnbbpqv5dl3ifkfmrmuck5/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/curl-7.63.0-jrvggtmsfzn64chuvcenb7a4g7lznfvj/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/sqlite-3.30.1-kifllx2du5fe2yqreol2ldfk7zzqrhm2/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libiconv-1.16-qeeepkzh7y5dnj75fidj2abx23x6nwbl/include:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/zlib-1.2.11-ah6qvpjr2sam2rpn527ivwmz4vyfcqbf/include \\\r\n    SPACK_LINKER_ARG=-Wl, \\\r\n    SPACK_LINK_DIRS=/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/openssl-1.1.1d-4bo5gg2eookuk2ubczsa4ciaz5c4mbwz/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/bzip2-1.0.8-5zisvxvepuimx27rqxi53srnkqj6jqlk/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cudnn-7.6.5.32-10.1-linux-x64-sabespboxtmnfvkohzmmackhv7tjvbqb/lib64:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cuda-10.1.243-wciaq2rrrkc4w6vcraocheah55wrekha/targets/x86_64-linux/lib/stubs:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cuda-10.1.243-wciaq2rrrkc4w6vcraocheah55wrekha/lib64:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/gdbm-1.18.1-ek2j6ymdsfdhwtcsgm5typyeqr2d427s/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/readline-8.0-sdz3prqngfpcyiooqriil7iyy3mp77nr/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libxml2-2.9.9-s5wjt2encrutai46rptb2qdbuqm74rcj/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libffi-3.2.1-mhdz37flhbznp5bla7dxtvtuwqxlzdpr/lib64:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libffi-3.2.1-mhdz37flhbznp5bla7dxtvtuwqxlzdpr/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/gettext-0.20.1-3x3th3xdaosnjd7ttly2mym5hukdqln5/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/ncurses-6.1-bnt5ehudftgrg5smastj4ejhjvyxfjmj/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/xz-5.2.4-oc7pfgyjo477pshkef56rkv4ovz7ykei/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/nccl-2.4.8-1-phuasbwfolmvjlo5mmizcuz4vqeuayhk/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/expat-2.2.9-v4k636wrhszysgefq2sijry72mj5l2au/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libbsd-0.10.0-jhowpdxutwgnbbpqv5dl3ifkfmrmuck5/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/curl-7.63.0-jrvggtmsfzn64chuvcenb7a4g7lznfvj/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/sqlite-3.30.1-kifllx2du5fe2yqreol2ldfk7zzqrhm2/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libiconv-1.16-qeeepkzh7y5dnj75fidj2abx23x6nwbl/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/zlib-1.2.11-ah6qvpjr2sam2rpn527ivwmz4vyfcqbf/lib \\\r\n    SPACK_ROOT=/home/sajid/packages/spack \\\r\n    SPACK_RPATH_DIRS=/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/py-tensorflow-2.0.0-a73jdejmdsiybrq3ocyrzxhq4zbv2cjz/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/py-tensorflow-2.0.0-a73jdejmdsiybrq3ocyrzxhq4zbv2cjz/lib64:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/openssl-1.1.1d-4bo5gg2eookuk2ubczsa4ciaz5c4mbwz/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/bzip2-1.0.8-5zisvxvepuimx27rqxi53srnkqj6jqlk/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cudnn-7.6.5.32-10.1-linux-x64-sabespboxtmnfvkohzmmackhv7tjvbqb/lib64:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cuda-10.1.243-wciaq2rrrkc4w6vcraocheah55wrekha/targets/x86_64-linux/lib/stubs:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/cuda-10.1.243-wciaq2rrrkc4w6vcraocheah55wrekha/lib64:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/gdbm-1.18.1-ek2j6ymdsfdhwtcsgm5typyeqr2d427s/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/readline-8.0-sdz3prqngfpcyiooqriil7iyy3mp77nr/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libxml2-2.9.9-s5wjt2encrutai46rptb2qdbuqm74rcj/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libffi-3.2.1-mhdz37flhbznp5bla7dxtvtuwqxlzdpr/lib64:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libffi-3.2.1-mhdz37flhbznp5bla7dxtvtuwqxlzdpr/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/gettext-0.20.1-3x3th3xdaosnjd7ttly2mym5hukdqln5/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/python-3.7.4-ihubkpw2r66wyuqpyelixazj4v76jd7i/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/ncurses-6.1-bnt5ehudftgrg5smastj4ejhjvyxfjmj/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/xz-5.2.4-oc7pfgyjo477pshkef56rkv4ovz7ykei/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/nccl-2.4.8-1-phuasbwfolmvjlo5mmizcuz4vqeuayhk/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/expat-2.2.9-v4k636wrhszysgefq2sijry72mj5l2au/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libbsd-0.10.0-jhowpdxutwgnbbpqv5dl3ifkfmrmuck5/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/curl-7.63.0-jrvggtmsfzn64chuvcenb7a4g7lznfvj/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/sqlite-3.30.1-kifllx2du5fe2yqreol2ldfk7zzqrhm2/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/libiconv-1.16-qeeepkzh7y5dnj75fidj2abx23x6nwbl/lib:/home/sajid/packages/spack/opt/spack/linux-centos8-broadwell/gcc-7.3.0/zlib-1.2.11-ah6qvpjr2sam2rpn527ivwmz4vyfcqbf/lib \\\r\n    SPACK_SHORT_SPEC='py-tensorflow@2.0.0%gcc@7.3.0~android~aws~computecpp+cuda cuda_arch=61 ~dynamic_kernels+gcp~gdr~hdfs~ignite~ios~jemalloc~kafka~mkl~monolithic~mpi+nccl~ngraph~numa~opencl patches=c49766a976e5c25c3827036828df0a2630e511bd783ac9cdcc6fc13068b22fac ~rocm~tensorrt~verbs~xla arch=linux-centos8-broadwell/a73jdej' \\\r\n    SPACK_SYSTEM_DIRS=/bin:/usr/bin:/usr/local/bin:/bin64:/usr/bin64:/usr/local/bin64:/include:/usr/include:/usr/local/include:/lib:/usr/lib:/usr/local/lib:/lib64:/usr/lib64:/usr/local/lib64:/:/usr:/usr/local \\\r\n    SPACK_TARGET_ARGS='-march=broadwell -mtune=broadwell' \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/nasm/_objs/nasm/outelf.d '-frandom-seed=bazel-out/host/bin/external/nasm/_objs/nasm/outelf.o' -DHAVE_SNPRINTF -DHAVE_SYS_TYPES_H -iquote external/nasm -iquote bazel-out/host/bin/external/nasm -iquote external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -isystem external/nasm/asm -isystem bazel-out/host/bin/external/nasm/asm -isystem external/nasm/include -isystem bazel-out/host/bin/external/nasm/include -isystem external/nasm/output -isystem bazel-out/host/bin/external/nasm/output -isystem external/nasm/x86 -isystem bazel-out/host/bin/external/nasm/x86 -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -w '-std=c99' -c external/nasm/output/outelf.c -o bazel-out/host/bin/external/nasm/_objs/nasm/outelf.o)\r\n```\r\n\r\nThis is failing because in attempting to compile `nasm/output/outelf.c` which has a `#include eval.h`, bazel tries to link it to the wrong header. Now, as per the `nasm` build recipe at `tensorflow/third_party/nasm/BUILD.bazel` shows that the includes for `nasm` are :\r\n```\r\n    includes = [\r\n        \"asm\",\r\n        \"include\",\r\n        \"output\",\r\n        \"x86\",\r\n    ],\r\n``` \r\nand the folder `asm` does indeed have the relevant header `eval.h`. Despite being given the right definitions in the `BUILD.bazel` recipe, for some reason `bazel` completely ignores this header, searches either `$PATH` or `cxx_builtin_include_directories` (from the compiler toolchain definition) to try and include a completely irrelevant header (that of `eval.h` from `python` standard library instead !). \r\n\r\nPS: Even the [official bazel docs say](https://docs.bazel.build/versions/master/cpp-use-cases.html#adding-include-paths) that one should explicitly list the include locations which is the case here.", "@s-sajid-ali,\r\n\r\nCan you try installing the latest stable version of tensorflow i.e `2.6.0` and lets us know if the issue still persists. You can follow this [guide](https://www.tensorflow.org/install/source) to build from source. Thanks!\r\n", "This issue was fixed in spack: https://github.com/spack/spack/pull/16077. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35014\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35014\">No</a>\n"]}, {"number": 35013, "title": "ValueError while passing a SparseTensor input to the Dense layer.", "body": "**System information**\r\n- OS Platform and Distribution: MacOS Mojave\r\n- TensorFlow installed from: conda install tensorflow\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6\r\n\r\nI have generated a smaller version of my issue:\r\n```\r\nfrom tensorflow.keras import layers\r\ninputs = layers.Input(shape=(256,), sparse=False, name='name_sparse')\r\nx = layers.Dense(32, name=\"my_layer\")(inputs)\r\nprint(x)\r\n```\r\nOutput\r\n`Tensor(\"my_layer/Identity:0\", shape=(None, 32), dtype=float32)`\r\n\r\nBut if I change `sparse=True`, I get a ValueError: \r\n`The last dimension of the inputs to Dense should be defined. Found None.`\r\n\r\n**Other info / logs**\r\nI am able to run it on TF1.14 with Keras 2.2.4.\r\n", "comments": []}, {"number": 35009, "title": "[S3 PR 4] Use transfer manager for uploads", "body": "", "comments": []}, {"number": 35008, "title": "convert to tflite failed using tf.lite.TFLiteConverter.from_saved_model and validate failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nwhen i convert saved lstm tensorflow model to the tflite file and meets the error.\r\nthis is piece of code :\r\ndef create_LSTM_model(inputs):\r\n    W = {\r\n        \r\n        'hidden': tf.Variable(tf.keras.backend.random_normal([N_FEATURES, N_HIDDEN_UNITS])),\r\n        'output': tf.Variable(tf.keras.backend.random_normal([N_HIDDEN_UNITS, N_CLASSES]))\r\n        'output': tf.Variable(init_w_output)\r\n    }\r\n    biases = {\r\n        'hidden': tf.Variable(tf.keras.backend.random_normal([N_HIDDEN_UNITS], mean=1.0))\r\n        'output': tf.Variable(tf.keras.backend.random_normal([N_CLASSES]))\r\n    }\r\n\r\n    X = tf.transpose(inputs, [1, 0, 2])\r\n    X = tf.reshape(X, [-1, N_FEATURES])\r\n    hidden = tf.nn.relu(tf.matmul(X, W['hidden']) + biases['hidden'])\r\n    hidden = tf.split(hidden, N_TIME_STEPS, 0)\r\n\r\n    def get_a_cell(lstm_size, keep_prob):\r\n        lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(lstm_size)\r\n        drop = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\r\n        return drop\r\n\r\n    # lstm_layers = [tf.compat.v1.nn.rnn_cell.BasicLSTMCell(N_HIDDEN_UNITS, forget_bias=1.0) for _ in range(2)]\r\n    lstm_size = N_HIDDEN_UNITS\r\n    keep_prob = 0.9\r\n    num_layers = 2\r\n    # lstm_layers = tf.compat.v1.nn.rnn_cell.MultiRNNCell([get_a_cell(lstm_size, keep_prob) for _ in range(num_layers)])\r\n    lstm_layers = tf.compat.v1.nn.rnn_cell.MultiRNNCell([get_a_cell(lstm_size, keep_prob) for _ in range(num_layers)])\r\n\r\n    outputs, _ = tf.compat.v1.nn.static_rnn(lstm_layers, hidden, dtype=tf.float32)\r\n    # keras.layers.RNN  outputs, _ = tf.keras.layers.RNN(lstm_layers, hidden, dtype=tf.float32)\r\n\r\n    # Get output for the last time step\r\n    lstm_last_output = outputs[-1]\r\n\r\n    return tf.matmul(lstm_last_output, W['output']) + biases['output']\r\n\r\n\r\ntf.compat.v1.reset_default_graph()\r\n\r\nX = tf.compat.v1.placeholder(tf.float32, [None, N_TIME_STEPS, N_FEATURES], name=\"input_X\")\r\nY = tf.compat.v1.placeholder(tf.float32, [None, N_CLASSES], name='input_Y')\r\n\r\npred_Y = create_LSTM_model(X)\r\ntf.identity(pred_Y, name=\"pred_Y\")\r\n\r\npred_softmax = tf.nn.softmax(pred_Y, name=\"pred_softmax\")\r\n\r\ntrain_count = len(X_train)\r\nL2_LOSS = 0.0015\r\n\r\nl2 = L2_LOSS * \\\r\n     sum(tf.nn.l2_loss(tf_var) for tf_var in tf.compat.v1.trainable_variables())\r\n\r\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_Y, labels=Y)) + l2\r\n# loss=tf.reduce_mean(tf.square(tf.reshape(pred,[-1,2])-tf.reshape(Y, [-1,2])))\r\ntf.identity(loss, name=\"saved_loss\")\r\n\r\n\r\nglobal_step = tf.Variable(0, trainable=False, name='global_step')\r\n\r\nlearning_rate = tf.compat.v1.train.exponential_decay(\r\n    LEARNING_RATE_BASE,\r\n    global_step,\r\n    train_count / BATCH_SIZE, LEARNING_RATE_DECAY)\r\n\r\ntf.identity(learning_rate, name=\"learning_rate\")\r\n\r\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\r\ntf.identity(loss, name=\"saved_optimizer\")\r\n\r\ncorrect_pred = tf.equal(tf.argmax(pred_softmax, 1), tf.argmax(Y, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32), name='output_accuracy')\r\n\r\nhistory = dict(train_loss=[],\r\n               train_acc=[],\r\n               test_loss=[],\r\n               test_acc=[])\r\n\r\nsess = tf.compat.v1.InteractiveSession()\r\nsess.run(tf.compat.v1.global_variables_initializer())\r\n\r\nexport_simple_path = 'model_simple/'\r\nif os.path.exists(export_simple_path):\r\n    shutil.rmtree(export_simple_path)\r\n\r\ntf.compat.v1.saved_model.simple_save(sess,\r\n            export_simple_path,\r\n            inputs={\"input_X\": X},\r\n            outputs={\"pred_softmax\": pred_softmax})\r\nuse this to save the model\r\nand then i convert the .pb model to tflite with python API\r\n\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\nbut got the error:\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, FULLY_CONNECTED, GREATER_EQUAL, LOGISTIC, MUL, SOFTMAX, SPLIT, TANH. Here is a list of operators for which you will need custom implementations: RandomUniform.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\d\\.conda\\envs\\ai\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\d\\.conda\\envs\\ai\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\d\\.conda\\envs\\ai\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\d\\.conda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\d\\.conda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\d\\.conda\\envs\\ai\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\d\\.conda\\envs\\ai\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\d\\.conda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, FULLY_CONNECTED, GREATER_EQUAL, LOGISTIC, MUL, SOFTMAX, SPLIT, TANH. Here is a list of operators for which you will need custom implementations: RandomUniform.\r\n\r\nand then i add the\r\nconverter.allow_custom_ops=True\r\n\r\nthen i can convert to tflite (lstm.tflite) successfully\r\nbut then i use below command to validate my model then i got the errors:\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"lstm.tflite\")\r\n\r\ninterpreter.allocate_tensors()\r\n\r\n>>> interpreter = tf.lite.Interpreter(model_path=\"D:\\Feature\\model_simple_1207\\lstm_har_1207.tflite\")\r\nINFO: Initialized TensorFlow Lite runtime.\r\n>>> interpreter.allocate_tensors()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\d\\.conda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter.py\", line 244, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"C:\\Users\\d\\.conda\\envs\\ai\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter_wrapper\\tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n**R**untimeError: Encountered unresolved custom op: RandomUniform.Node number 14 (RandomUniform) failed to prepare.****\r\n\r\ncould anyone  hlep with the issue?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\n\r\n", "comments": ["@glevenc Can you please format your code and share a simple standalone code to reproduce the issue? You could use colab gist for sharing the standalone code. Thanks!", "> \r\n> \r\n> @glevenc Can you please format your code and share a simple standalone code to reproduce the issue? You could use colab gist for sharing the standalone code. Thanks!\r\n\r\n@jvishnuvardhan \r\nif i remove the dropoutwrapper opratation then the convert will be successfuly \r\ndrop = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\r\nmaybe this opration is not supported yet?\r\n\r\nhope this can help you to give you some clue\r\n\r\nthx", "@jvishnuvardhan  the result is repoduced here:\r\nhttps://colab.research.google.com/drive/1vRsTiKk8CAClShGjwyLmRP7VAqXxAIeq#scrollTo=xNQwn_eZVB4n ", "@glevenc I cannot access your colab (permission issue). Can you please open the gist  --> go to `File` --> press `Save a copy as a GitHub gist`---> another page opens up with the shareable gist --> copy the link and share with me. Thanks! ", "@jvishnuvardhan \r\nsorry for that and the link :\r\nhttps://colab.research.google.com/gist/glevenc/20ecb878b520653257f4d290dd79fec0/test.ipynb", "@glevenc I am not able to reproduce the exact error you are getting with your colab. When I ran your colab, i see the following error\r\n\r\n```\r\nWARNING:tensorflow:Issue encountered when serializing variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\nTensor.name is meaningless when eager execution is enabled.\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-15-79344999a2d5> in <module>()\r\n      2 converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n      3 converter.allow_custom_ops=True\r\n----> 4 tflite_model = converter.convert()\r\n      5 open(\"lstm.tflite\", \"wb\").write(tflite_model)\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py in numpy(self)\r\n    585       return self.read_value().numpy()\r\n    586     raise NotImplementedError(\r\n--> 587         \"numpy() is only available when eager execution is enabled.\")\r\n    588 \r\n    589   @deprecated(None, \"Prefer Dataset.range instead.\")\r\n\r\nNotImplementedError: numpy() is only available when eager execution is enabled.\r\n```\r\n\r\nAre you using `TF2.0`? Can you please check the error one more time. Thanks! ", "@jvishnuvardhan \r\nhi, i print the version in the colab\r\nand it  shows 1.15.0\r\nAlso , in my own pc env. the version is TF2.0\r\nboth version have the same issues as list belows:\r\ni just re-run the colab as i shared above link: it can always be reproduced;\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n\r\n<ipython-input-16-c0f7628eb24c> in <module>()\r\n      1 interpreter = tf.lite.Interpreter(model_path=\"lstm.tflite\")\r\n----> 2 interpreter.allocate_tensors()\r\n      3 \r\n      4 # Get input and output tensors.\r\n      5 input_details = interpreter.get_input_details()\r\n\r\n1 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py in AllocateTensors(self)\r\n    104 \r\n    105     def AllocateTensors(self):\r\n--> 106         return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\n    107 \r\n    108     def Invoke(self):\r\n\r\nRuntimeError: Encountered unresolved custom op: RandomUniform.Node number 13 (RandomUniform) failed to prepare.\r\n", "we are adding `RandomUniform` as a built-in op for TF Lite. Assigning this issue to Jaeyoo  for investigation now. thanks.", "@glevenc We recently announced a new feature in dogfood to convert TF 2.0 Keras LSTM to TFLite fused LSTM (which can then be also quantized if needed). You can see the announcement here:\r\nhttps://groups.google.com/a/tensorflow.org/g/tflite/c/Ub4apUvblN8\r\n\r\nWould you be interested in trying this out?\r\n\r\n ", "Hi @glevenc,\r\n\r\nPlease use this at the front of your colab\r\n```\r\n!pip install tf-nightly\r\n```\r\nthen it succeeds without any errors.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35008\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35008\">No</a>\n"]}, {"number": 35007, "title": "Attention/AdditiveAttention Issue", "body": "I have the following functions to return two models:\r\n\r\n\r\n```\r\ndef get_question_model(self, embedding):\r\n        question_input = Input(shape=(None,), name='question_input')\r\n        question_embedding = embedding(question_input)\r\n\r\n        cnn_1d = Conv1D(128, 4, padding='same', activation='relu', strides=1)(question_embedding)\r\n        cnn_1d = AveragePooling1D(pool_size=3)(cnn_1d)\r\n\r\n        model = Model(question_input, cnn_1d)\r\n        return model\r\n\r\ndef get_sentence_model(self, embedding, question_model):\r\n        sentence_input = Input(shape=(None,),  name='sentence_input')\r\n        sentence_embedding = embedding(sentence_input)\r\n        cnn_1d = Conv1D(128, 4, padding=\"same\", activation='relu', strides=1)(sentence_embedding)\r\n        cnn_1d = AveragePooling1D(pool_size=3)(cnn_1d)\r\n\r\n        sentence_attention = AdditiveAttention()([cnn_1d, question_model])\r\n        model = Model(sentence_input, sentence_attention)\r\n        model.shape = model.output_shape\r\n        return model\r\n```\r\n\r\nAnd the functions are called:\r\n```\r\n   self.question_model = self.get_question_model(embedding)\r\n   self.sentence_model = self.get_sentence_model(embedding, self.question_model)\r\n```\r\n#####\r\n\r\nI get the following issue:\r\n\r\n\r\n```\r\n2019-12-10 18:17:01.848676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9371 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:83:00.0, compute capability: 7.0)\r\nTraceback (most recent call last):\r\n  File \"D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py\", line 376, in <module>\r\n    qa_model.fit(input=input, output=y)\r\n  File \"D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py\", line 152, in fit\r\n    self.model = self.get_model()\r\n  File \"D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py\", line 123, in get_model\r\n    self.sentence_model = self.get_sentence_model(embedding, self.question_model)\r\n  File \"D:/Development/competitions/Kaggle/Tensorflow 2.0 - Q_a_A/tensorflow_2.0/QuestionAnswerContextAttention.py\", line 84, in get_sentence_model\r\n    sentence_attention = AdditiveAttention()([cnn_1d, question_model])\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 887, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 2141, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\dense_attention.py\", line 406, in build\r\n    v_shape = tensor_shape.TensorShape(input_shape[1])\r\n**TypeError: 'NoneType' object is not subscriptable**\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@nectario \r\n\r\nLooks like code is incomplete. Request you to provide complete code to reproduce the issue in our environment. It helps us in localizing the issue faster.Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Thanks!", "Hi,\r\n\r\nI have attached a zip file that contains the complete code with the data. Just unzip and run it as:\r\n\r\npython QAModel.py\r\n\r\nAlso, for the information you requested:\r\n\r\n**Version:** TensorFlow 2.0  (GPU Version)\r\n**Operating System:** Windows 10\r\n**Graphics Card:** Nvidia Titan V\r\n\r\n[AttentionProblem.zip](https://github.com/tensorflow/tensorflow/files/3958469/AttentionProblem.zip)\r\n\r\nLet me know if you need anything else.\r\n\r\nThanks,\r\n\r\nNektarios\r\n", "I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/2e0fe01fd916adcc8f011abf3ea2c2d2/untitled476.ipynb) Thanks!", "@nectario I see the shapes of ```X.dat```and ```Y.dat``` are of ```(5, None, None, None)```, and ```(5, None)```. Is that what you intended?  The error clearly throws ```TypeError: 'NoneType' object is not subscriptable```. Please let me know what you think. Thanks!", "Both X and y consist of variable length lists and hence, are ragged tensors. RaggedTensors is an exciting new feature of Tesnorflow 2.0 and this is exactly what I want!", "I do not want to pad!", "I tried `tf-nightly` instead of `TF2.0` that was used in the gist. I still get the same error as you are getting. Thanks!\r\n\r\n", "This error happens because Model class does not have a shape field. In the code if no shape is present, it initializes it to None. Here is a snippet in base_layer:\r\n\r\n```\r\n input_shapes = None\r\n      if all(hasattr(x, 'shape') for x in input_list):\r\n        input_shapes = nest.map_structure(lambda x: x.shape, inputs)\r\n      # Only call `build` if the user has manually overridden the build method.\r\n      if not hasattr(self.build, '_is_default'):\r\n        # Any setup work performed only once should happen in an `init_scope`\r\n        # to avoid creating symbolic Tensors that will later pollute any eager\r\n        # operations.\r\n        with tf_utils.maybe_init_scope(self):\r\n          self.build(input_shapes)\r\n```", "I have tried in colab with TF GPU version 2.2, nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/c69ad703c9307d2fff4fa05eda9b316b/untitled30.ipynb).Thanks!", "Was able to reproduce your issue in Tensorflow GPU 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/9647b4ee94fa0a18f68708eff3b17772/35007.ipynb). Thanks!", "Hi There,\r\n\r\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \r\n\r\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Issue can be closed. Don't experience this now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35007\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35007\">No</a>\n"]}, {"number": 35006, "title": "Fix version 2.1 release note regarding TF_DETERMINISTIC_OPS", "body": "**This current pull request is intentionally aimed at the r2.1 branch.**\r\n\r\nThis current pull request is a follow-up to [PR 34887](https://github.com/tensorflow/tensorflow/pull/34887). Unfortunately, I didn't review the rendered markdown for those changes before submitting the PR.\r\n\r\nThis PR:\r\n  * Changes the indent of the note about `TF_DETERMINISTIC_OPS` so that it doesn't appear to be related to TensorRT.\r\n  * Escapes the use of \\* twice in the markdown, which was previously interpreted as intending to signal emphasis.\r\n  * Adds some improvements to clarity.\r\n\r\nRequesting review by @sanjoy, who reviewed the original PR.", "comments": ["This PR has been ready to merge for 9 days. Please will someone merge it?", "Apologies, we didn't merge any PRs on the branch this week as we were debugging several segfaults caused by scipy/scipy#11237\r\n\r\nWe will start merging PRs in a while, when we finish this debugging process", "@duncanriach sorry for the delay in merging the PR's. we have been debugging seg fault issues with scipy and hence the delay. Should be merged in soon.", "@mihaimaruseac, thanks for the update.\r\n@goldiegadde, thanks for the merge."]}, {"number": 35005, "title": "[r2.1:Cherrypick] Expose ndtri and erfinv under tf.math.ndtri and tf.math.erfinv.", "body": "PiperOrigin-RevId: 281816005\nChange-Id: Idded0bb39c0d32288f1bfa3d0288ba5847aa6fc1", "comments": []}, {"number": 35004, "title": "BinaryCrossentropy incorrect partial reduction of loss when reduction='none'", "body": "**System information**\r\n custom code\r\n- OS Platform and Distribution\r\nubuntu 18.04\r\n- TensorFlow installed from (source or binary):\r\npip install tensorflow-gpu\r\n- TensorFlow version (use command below):\r\nv2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version:\r\n3.6.9\r\n\r\n**Describe the current behavior**\r\nBinaryCrossentropy does a reduction on the last dimension even if you pass in reduction='none'\r\n\r\n**Describe the expected behavior**\r\nIt should not do any reduction\r\n\r\n**Workaround**\r\nuse tf.nn.sigmoid_cross_entropy_with_logits()\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n    y = np.array([0., 0., 1., 1.]).reshape((1, 1, 1, -1))\r\n    x = np.array([1., 1., 1., 0.]).reshape((1, 1, 1, -1))\r\n    print(y.shape)\r\n    print(x.shape)\r\n\r\n    bce_reduce = tf.keras.losses.BinaryCrossentropy()\r\n    loss = bce_reduce(y, x)\r\n    print('correct: fully reduced loss (reduction=default)', loss.numpy())\r\n\r\n    bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\r\n    loss = bce(y, x)\r\n    print('incorrect: should be not be reduced (reduction=none): ', loss.numpy())  # Loss: 11.522857\r\n    print('incorrect: reduced along last dimension:', loss.shape)\r\n\r\n    # should be the same as:\r\n    correct_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=x)\r\n    print('correct: unreduced_loss (using sigmoid_cross_entropy_with_logits)', correct_loss)\r\n    print('correct: unreduced_loss_shape', correct_loss.shape)\r\n\r\n\r\nmain()\r\n```\r\noutput:\r\n```\r\ncorrect: fully reduced loss (reduction=default) 11.568711280822754\r\nincorrect: should be not be reduced (reduction=none):  [[[11.56871128]]]\r\nincorrect: reduced along last dimension: (1, 1, 1)\r\ncorrect: unreduced_loss (using sigmoid_cross_entropy_with_logits) tf.Tensor([[[[1.31326169 1.31326169 0.31326169 0.69314718]]]], shape=(1, 1, 1, 4), dtype=float64)\r\ncorrect: unreduced_loss_shape (1, 1, 1, 4)\r\n\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.0 ,2.1.0-rc0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9f1986fd00ea719785ffd0f790eb7330/untitled469.ipynb). Thanks!", "@chahld The behavior is as expected. Please see the docs here: https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy#returns. The inputs are always reduced by one dimension. `reduction` indicates whether you are reducing the per sample losses - For example if you input is 2D (samples, features), if reduction is none, you will get (samples,) values - ie. one loss value per sample.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35004\">No</a>\n", "Sorry I misread the documentation.\r\n\r\nIn my use case I'm trying to weight the loss per filter (setting it to zero in some circumstances). \r\n\r\nWhat is the use case for this partial reduction? I understand if the output is dense it would still allow me to weight the loss differently for each sample. But I wonder why it is useful for the convolutional case where it returns the loss per pixel per sample?\r\n\r\nSome further suggestions (all somewhat minor):\r\n- it would be useful to have a 'really none' option.\r\n- the option called 'none' is misnamed and should be something like \"filter'. \r\n- Also the return value documentation says the reduction axis is 'usually' -1, but for this function there is no way to pass in an axis, so it is always -1.  \r\n\r\n"]}, {"number": 35003, "title": "The label_image example fails on VGG19 network", "body": "I call it with the 224x224 image of orange:\r\n```\r\n./label_image \\\r\n        --image orange.bmp \\\r\n        --labels imagenet_class_index.txt \\\r\n        --tflite_model vgg19_tf1_14.tflite\r\n```\r\nand it outputs this:\r\n```\r\nNFO: Initialized TensorFlow Lite runtime.\r\ninvoked \r\naverage time: 1180.25 ms \r\n0.0352545: 669 mosquito_net\r\n0.0291345: 794 shower_curtain\r\n0.0180968: 549 envelope\r\n0.0168044: 999 toilet_tissue\r\n0.0155437: 314 cockroach\r\n```\r\n\r\nIt might be that input normalization is a problem, but ```label_image -h``` doesn't say anything about normalization.\r\n", "comments": []}, {"number": 35002, "title": "Inconsistent loading of saved model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Linux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\n\r\nI have several models saved using `model.save` as hdf5 objects. However, when I try to load them from a script via:\r\n\r\n```python\r\ntf.keras.models.load_model(DATA_DIR + \"models/{}.h5\".format(filename))\r\n```\r\n\r\nI get an I/O error:\r\n\r\n```\r\n\"Unable to open object (file read failed: time = Tue Dec 10 16:36:14 2019\\n, filename = '/home/fonnesbeck/models/Slider-rhb-rhp-attack.h5', file descriptor = 14, errno = 5, error message = 'Input/output error', buf = 0x7f642c733da0, total read size = 272, bytes this sub-read = 272, bytes actually read = 18446744073709551615, offset = 8443360)\"\r\n```\r\n\r\nYet, when I manually go in and import the file manually from the terminal, it works fine:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.keras.models.load_model('Slider-rhb-rhp-attack.h5')\r\n2019-12-10 16:34:45.948071: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions tha\r\nt this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-10 16:34:46.355657: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-12-10 16:34:46.356406: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d96a7f470 executing \r\ncomputations on platform Host. Devices:\r\n2019-12-10 16:34:46.356451: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, De\r\nfault Version\r\n<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa474307400>\r\n```\r\n\r\nEven stranger, after having done the manual import, running the script works fine. this seems to happen with every new file.\r\n", "comments": ["I notice that if I use the SavedModel format rather than HDF5, this issue does not come up, however the model takes a lot longer to load and run, so I'd prefer to get HDF5 working.", "@fonnesbeck  You may try using TF 2.1.0.rc0 version for H5 model saving and loading.", "I've tried this with 2.1.0.rc1, but now get the following:\r\n\r\n```\r\nreturn tf.keras.models.load_model(DATA_DIR + \"models/{}.h5\".format(filename))\r\n\r\nOSError: SavedModel file does not exist at: ./models/my_model.h5/{saved_model.pbtxt|saved_model.pb}\r\n```\r\n\r\nWhy is it now looking for a SavedModel when I've passed it a `h5` extension in the string? The documentation says it should recognize this.", "OK, I noticed in the release notes that under 2.1 the model needs to be saved with a `save_format=\"h5\"` argument.\r\n\r\nI've gone ahead and done this; unfortunately, the same error comes up: a file read failure. Not sure why its a I/O error, as again it works fine when executed manually.", "More info: this only seems to occur when the models are stored on a GCS bucket (which has been mounted using `gcsfuse`). If I move the models to a local directory, they open fine.", "There are issues with using HDF5 with GCS. I'm not sure when we'll be able to get to this, but please use the workaround where the model is copied locally before loading. ", "@fonnesbeck We see that you are using older version of tensorflow .Many bug have been fixed in latest version. We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35002\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35002\">No</a>\n"]}, {"number": 35001, "title": "Added Updated command", "body": "I have added how to **upgrade tensorflow version** in .Readme file", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35001) for more info**.\n\n<!-- need_sender_cla -->", "@VersatileVishal please sign CLA", "CLA sign part is Done.", "@googlebot I signed it!\n\nOn Wed, Dec 11, 2019 at 10:12 AM gbaned <notifications@github.com> wrote:\n\n> @VersatileVishal <https://github.com/VersatileVishal> please sign CLA\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35001?email_source=notifications&email_token=AJGODIBQAC6DMIXR4PLJWRLQYBVSXA5CNFSM4JZBLXTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGR3LPQ#issuecomment-564377022>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJGODIF3CAFS4KRU5DXPY2TQYBVSXANCNFSM4JZBLXTA>\n> .\n>\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35001) for more info**.\n\n<!-- ok -->", "@mihaimaruseac could you please review my PR", "@mihaimaruseac  can you tell me who is going to merge my PR.", "@tomasyany  i have made changes. Please review my PR.", "There is going to be an automated process. Tests are going to be run, if all is green PR is going to be merged automatically.", "Can you please tell me how long will it take.", "Please have some patience. The CI process is long but should not take more than a day if everything goes smoothly", "Thanks for the information really helpful.", "@mihaimaruseac  it says 1 errored, 2 failing and 6 successful checks. So, what i have to do now.", "It should be merged soon(ish)"]}, {"number": 35000, "title": "Add tests in TFLite micro for Float/Uint8/Int8 Tanh activation", "body": "", "comments": ["@giorgio-arenarm Can you please check reviewer comments and keep us posted. Thanks!", "There is an internal discussion on whether the lookup table implementation is needed for ensuring bit-accurateness. Will update the issue once this is resolved. Sorry for the delay!", "@giorgio-arenarm Any update on this PR, please. Thanks!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35000) for more info**.\n\n<!-- need_author_consent -->", "Removed the LuT implementation and rebased the patch", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35000) for more info**.\n\n<!-- ok -->", "@petewarden gentle ping for review", "@petewarden gentle ping for review", "Sorry about the delay, changes have been pushed", "@gbaned @njeffrie Gentle ping for review", "@giorgio-arenarm Can you please resolve conflicts? Thanks!", "Done rebasing, thanks.", "Done moving functions to their own headers", "@giorgio-arenarm Can you please check @njeffrie comments and keep us posted. Thanks!", "Done changing the shape's format. I should point out that while rebasing this patch another implementation of the TanH test (only for float type) popped out. I went on and removed it, please let me know if that's the right course of action. Thanks", "@giorgio-arenarm Can you please fix build failures ? Thanks!", "Done. Please let me know if that fixed the issues", "@giorgio-arenarm we still see build failures , can you please check again ", "Hi @rthadur I have rebased against master and didn't get any failures running this command\r\n`CC=clang BAZEL_COMPILER=llvm bazel-real run --copt=-DADDRESS_SANITIZER --copt=-fsanitize=address --linkopt=-fsanitize=address tensorflow/lite/micro/kernels:activations_test_binary`\r\n\r\nCould you please tell me how you run these tests so that I can reproduce them locally? Thanks", "@giorgio-arenarm sorry for the delay ,I see some build error [here](https://source.cloud.google.com/results/invocations/f35c02ba-eff2-4a74-bf3f-5df800ff69c1/log) can you please check \r\n\r\n`ERROR: T:/src/github/tensorflow/tensorflow/lite/kernels/BUILD:563:1: C++ compilation of rule '//tensorflow/lite/kernels:builtin_op_kernels' failed (Exit 2)\r\ncl : Command line warning D9002 : ignoring unknown option '-O3'\r\ntensorflow/lite/kernels/activations.cc(858): error C2039: 'Tanh': is not a member of 'tflite::reference_ops'\r\n.\\tensorflow/lite/kernels/internal/reference/reference_ops.h(67): note: see declaration of 'tflite::reference_ops'\r\ntensorflow/lite/kernels/activations.cc(1315): note: see reference to function template instantiation 'TfLiteStatus tflite::ops::builtin::activations::TanhEval<tflite::ops::builtin::activations::kReference>(TfLiteContext *,TfLiteNode *)' being compiled\r\ntensorflow/lite/kernels/activations.cc(858): error C3861: 'Tanh': identifier not found\r\ntensorflow/lite/kernels/activations.cc(892): error C2039: 'Tanh16bitPrecision': is not a member of 'tflite::optimized_ops'\r\n.\\tensorflow/lite/kernels/internal/optimized/optimized_ops.h(60): note: see declaration of 'tflite::optimized_ops'\r\ntensorflow/lite/kernels/activations.cc(892): error C3861: 'Tanh16bitPrecision': identifier not found\r\ntensorflow/lite/kernels/activations.cc(907): error C2039: 'Tanh16bitPrecision': is not a member of 'tflite::optimized_ops'\r\n.\\tensorflow/lite/kernels/internal/optimized/optimized_ops.h(60): note: see declaration of 'tflite::optimized_ops'\r\ntensorflow/lite/kernels/activations.cc(907): error C3861: 'Tanh16bitPrecision': identifier not found\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: T:/src/github/tensorflow/tensorflow/lite/python/BUILD:56:1 C++ compilation of rule '//tensorflow/lite/kernels:builtin_op_kernels' failed (Exit 2)\r\nINFO: Elapsed time: 158.018s, Critical Path: 14.33s\r\nINFO: 3889 processes: 3821 remote cache hit, 68 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully`\r\n", "@giorgio-arenarm Can you please check @rthadur's comments and keep us posted. Thanks!", "Hi @rthadur @gbaned sorry about the delay. I wasn't able to reproduce the build failure on my local machine, but from the log that you provided it looks like the new headers hadn't been included in the non-micro path for the test. Please let me know if fixing that removed the failures. Thanks", "@giorgio-arenarm Can you please check build failures. Thanks!", "@giorgio-arenarm can you please check below errors \r\n\r\n`/tensorflow/lite/kernels/activations.cc:15:\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:64:36: error: use of undeclared identifier 'CalculateUnsignedClampingWithRangeBitMasks'\r\n   uint8x16x2_t masks_clamp_0_1 = CalculateUnsignedClampingWithRangeBitMasks(\r\n\r\n                                  ^\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:66:36: error: use of undeclared identifier 'CalculateUnsignedClampingWithRangeBitMasks'\r\n   uint8x16x2_t masks_clamp_2_3 = CalculateUnsignedClampingWithRangeBitMasks(\r\n\r\n                                  ^\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:69:38: error: use of undeclared identifier 'SaturatingRounding'\r\n   int16x8x4_t input_val_rescaled = SaturatingRounding(\r\n\r\n                                    ^\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:74:34: error: use of undeclared identifier 'FixedPoint4Tanh'\r\n   int16x8x4_t output_val_s16 = FixedPoint4Tanh(input_val_rescaled);\r\n\r\n                                ^\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:92:5: error: use of undeclared identifier 'ClampWithRangeAndStore'\r\n   ClampWithRangeAndStore(output_data + c, output_val_u8_0_1, masks_clamp_0_1);\r\n   ^\r\n\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:93:5: error: use of undeclared identifier 'ClampWithRangeAndStore'\r\n   ClampWithRangeAndStore(output_data + c + 16, output_val_u8_2_3,\r\n\r\n   ^\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:168:36: error: use of undeclared identifier 'CalculateSignedClampingWithRangeBitMasks'\r\n   uint8x16x2_t masks_clamp_0_1 = CalculateSignedClampingWithRangeBitMasks(\r\n\r\n                                  ^\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:170:36: error: use of undeclared identifier 'CalculateSignedClampingWithRangeBitMasks'\r\n   uint8x16x2_t masks_clamp_2_3 = CalculateSignedClampingWithRangeBitMasks(\r\n\r\n                                  ^\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:173:38: error: use of undeclared identifier 'SaturatingRounding'\r\n   int16x8x4_t input_val_rescaled = SaturatingRounding(\r\n\r\n                                    ^\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:178:34: error: use of undeclared identifier 'FixedPoint4Tanh'\r\n   int16x8x4_t output_val_s16 = FixedPoint4Tanh(input_val_rescaled);\r\n\r\n                                ^\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:186:5: error: use of undeclared identifier 'ClampWithRangeAndStore'\r\n   ClampWithRangeAndStore(output_data + c, output_val_s8_0_1, masks_clamp_0_1);\r\n\r\n   ^\r\n.//tensorflow/lite/kernels/internal/optimized/activations.h:187:5: error: use of undeclared identifier 'ClampWithRangeAndStore'\r\n   ClampWithRangeAndStore(output_data + c + 16, output_val_s8_2_3,\r\n\r\n   ^\r\n12 errors generated.\r\n`", "Hi @rthadur these functions are defined inside `kernels/internal/optimized/optimized_ops.h` but including that header would defeat the whole purpose of isolating the optimized TanH function from that file as it was suggested in a previous review. How should I proceed?", "@njeffrie could you take a look at this? Thanks!", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 34999, "title": "Extend tests in TFLite micro for conv and depth_conv to support dilat\u2026", "body": "\u2026ion != 1", "comments": ["Done rebasing, I just needed to add a couple of parameters indicating in/out zero_points when calling the PerChannel tests. It should be good to go now", "@giorgio-arenarm Could you please check reviewer comments and keep us posted. Thanks!", "Thanks for the review, I have added TfLiteDepthwiseConvParams in the test's interface and rebased the commit :)", "@petewarden  Gentle ping for review", "@giorgio-arenarm Here are internal failures , can you please check.\r\n\r\nthird_party/tensorflow/lite/micro/kernels/depthwise_conv_test.cc:887:18: error: no matching function for call to 'ValidateDepthwiseConvGoldens'\r\n      kTfLiteOk, tflite::testing::ValidateDepthwiseConvGoldens(\r\n                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./third_party/tensorflow/lite/micro/testing/micro_test.h:112:17: note: expanded from macro 'TF_LITE_MICRO_EXPECT_EQ'\r\n    if ((x) != (y)) {                                                          \\\r\n                ^\r\nthird_party/tensorflow/lite/micro/kernels/depthwise_conv_test.cc:30:14: note: candidate function template not viable: no known conversion from 'TfLiteFusedActivation' to 'TfLiteDepthwiseConvParams *' for 6th argument\r\nTfLiteStatus ValidateDepthwiseConvGoldens(\r\n             ^\r\nthird_party/tensorflow/lite/micro/kernels/depthwise_conv_test.cc:887:18: error: no matching function for call to 'ValidateDepthwiseConvGoldens'\r\n      kTfLiteOk, tflite::testing::ValidateDepthwiseConvGoldens(\r\n                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./third_party/tensorflow/lite/micro/testing/micro_test.h:114:62: note: expanded from macro 'TF_LITE_MICRO_EXPECT_EQ'\r\n                                   __FILE__, __LINE__, (x), (y));              \\\r\n                                                             ^\r\nthird_party/tensorflow/lite/micro/kernels/depthwise_conv_test.cc:30:14: note: candidate function template not viable: no known conversion from 'TfLiteFusedActivation' to 'TfLiteDepthwiseConvParams *' for 6th argument\r\nTfLiteStatus ValidateDepthwiseConvGoldens(\r\n             ^\r\n2 errors generated.", "Hi @gbaned I've had a look at the error log but again it looks like maybe something is gone wrong while cherry picking the changes? `tensorflow/lite/micro/kernels/depthwise_conv_test.cc` file goes up to line 796 and the error seem to appear at line 887, moreover it complains about the 6th argument of `ValidateDepthwiseConvGoldens` being a `TfLiteFusedActivation` while throughout the file I checked that every time it's a `TfLiteDepthwiseConvParams *`. The code compiles on my side and the test pass. Any more info on the problem will be appreciated, thanks :)", "@petewarden Can you please assist with the above error ? Thanks!", "Rebased. Could you please check whether the issue has been resolved?", "@giorgio-arenarm, apologies for the delayed response.\r\n\r\nI just checked out your branch locally and am seeing the test fail with this command:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_kernel_conv_test -j8\r\n```\r\n\r\nError log:\r\n```\r\nTesting SimpleTestDilatedQuantized\r\nexpected_output_data[i] (1.8984370*2^7) near output_data[i] (1.3906247*2^7) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nexpected_output_data[i] (1.6015621*2^7) near output_data[i] (1.1249999*2^7) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nexpected_output_data[i] (1.1328123*2^7) near output_data[i] (1.312499*2^7) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nexpected_output_data[i] (1.9921868*2^7) near output_data[i] (1.1406248*2^7) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nexpected_output_data[i] (1.1484372*2^7) near output_data[i] (1.1796871*2^7) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nexpected_output_data[i] (1.9921868*2^7) near output_data[i] (1.8906246*2^6) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nTesting SimpleTestQuantizedPerChannel\r\nTesting SimpleTestDilatedQuantizedPerChannel\r\nexpected_output_data[i] (1.4374997*2^4) near output_data[i] (1.5624998*2^5) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nexpected_output_data[i] (1.4999999*2^2) near output_data[i] (1.0*2^1) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nexpected_output_data[i] (1.0*2^1) near output_data[i] (1.0*2^2) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nexpected_output_data[i] (1.1249999*2^3) near output_data[i] (1.0*2^1) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nexpected_output_data[i] (1.4999999*2^1) near output_data[i] (-1.7499998*2^2) failed at tensorflow/lite/micro/kernels/conv_test.cc:106\r\nTesting SimpleTestQuantizedPerChannelRelu6\r\nTesting Kernel1x1QuantizedPerChannel\r\nTesting Kernel1x1QuantizedPerChannelRelu6\r\nTesting FilterDimsNotMatchingAffineQuantization\r\ntensorflow/lite/micro/kernels/conv.cc:238 affine_quantization->scale->size == 1 || affine_quantization->scale->size == filter->dims->data[kConvQuanti was not true.\r\ntensorflow/lite/micro/kernels/conv.cc:238 affine_quantization->scale->size == 1 || affine_quantization->scale->size == filter->dims->data[kConvQuanti was not true.\r\nTesting BroadcastPerLayerQuantizationToPerChannelShouldMatchGolden\r\n9/11 tests passed\r\n~~~SOME TESTS FAILED~~~\r\n```\r\n\r\nIf you could take a look, that would be great.", "Hi @advaitjain sorry but I haven't been able to reproduce the issue by checking out this branch and issuing the same command; everything passes for me. Could you please tell me what binary are your running after make?", "I get the error with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile clean clean_downloads\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_kernel_conv_test -j8\r\n```\r\n\r\nand also if I run this binary:\r\n```\r\ntensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/kernel_conv_test\r\n```\r\n\r\nThe internal CI builds are also failing with this same error.\r\n\r\nI also get an error with asan:\r\n```\r\nCC=clang BAZEL_COMPILER=llvm bazel run --copt=-DADDRESS_SANITIZER     --copt=-fsanitize=address --linkopt=-fsanitize=address tensorflow/lite/micro/kernels:conv_test_binary\r\n```\r\n\r\nError log:\r\n```\r\nTesting SimpleTestDilatedQuantized\r\n=================================================================\r\n==44784==ERROR: AddressSanitizer: stack-buffer-overflow on address 0x7ffe456a810c at pc 0x0000005a5dd7 bp 0x7ffe456a4990 sp 0x7ffe456a4988\r\nWRITE of size 1 at 0x7ffe456a810c thread T0\r\n    #0 0x5a5dd6 in tflite::reference_ops::Conv(tflite::ConvParams const&, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, unsigned char*, tflite::RuntimeShape const&, unsigned char*, void*) (/home/advaitjain/.cache/bazel/_bazel_advaitjain/d870524cf2b6c91d24e6d0f78ed0142b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/conv_test_binary+0x5a5dd6)\r\n    #1 0x5a481a in tflite::ops::micro::conv::EvalQuantized(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::micro::conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) (/home/advaitjain/.cache/bazel/_bazel_advaitjain/d870524cf2b6c91d24e6d0f78ed0142b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/conv_test_binary+0x5a481a)\r\n    #2 0x5a9ce7 in tflite::ops::micro::conv::Eval(TfLiteContext*, TfLiteNode*) (/home/advaitjain/.cache/bazel/_bazel_advaitjain/d870524cf2b6c91d24e6d0f78ed0142b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/conv_test_binary+0x5a9ce7)\r\n    #3 0x4f8e46 in TfLiteStatus tflite::testing::(anonymous namespace)::ValidateConvGoldens<unsigned char>(TfLiteTensor*, int, unsigned char const*, unsigned char*, int, TfLiteConvParams*, float) (/home/advaitjain/.cache/bazel/_bazel_advaitjain/d870524cf2b6c91d24e6d0f78ed0142b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/conv_test_binary+0x4f8e46)\r\n    #4 0x4f6c94 in tflite::testing::(anonymous namespace)::TestConvQuantizedPerLayer(int const*, float const*, unsigned char*, float, int const*, float const*, unsigned char*, float, int const*, float const*, int*, int const*, float const*, unsigned char*, unsigned char*, float, TfLiteConvParams*) (/home/advaitjain/.cache/bazel/_bazel_advaitjain/d870524cf2b6c91d24e6d0f78ed0142b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/conv_test_binary+0x4f6c94)\r\n    #5 0x4f3a96 in main (/home/advaitjain/.cache/bazel/_bazel_advaitjain/d870524cf2b6c91d24e6d0f78ed0142b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/conv_test_binary+0x4f3a96)\r\n    #6 0x7fb7db481bba in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x26bba)\r\n    #7 0x41f1e9 in _start (/home/advaitjain/.cache/bazel/_bazel_advaitjain/d870524cf2b6c91d24e6d0f78ed0142b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/conv_test_binary+0x41f1e9)\r\n\r\nAddress 0x7ffe456a810c is located in stack of thread T0 at offset 620 in frame\r\n    #0 0x4f30bf in main (/home/advaitjain/.cache/bazel/_bazel_advaitjain/d870524cf2b6c91d24e6d0f78ed0142b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/conv_test_binary+0x4f30bf)\r\n\r\n  This frame has 115 object(s):\r\n    [32, 40) 'error_reporter'\r\n    [64, 112) 'output_data'\r\n    [144, 152) 'output_data13'\r\n    [176, 196) 'kFilterShape'\r\n    [240, 272) 'filter_values'\r\n    [304, 312) 'kBiasShape'\r\n    [336, 340) 'bias_values'\r\n    [352, 372) 'kOutputShape'\r\n    [416, 424) 'expected_output'\r\n    [448, 460) 'output_data38'\r\n    [480, 496) 'input_quantized'\r\n    [512, 524) 'filter_quantized'\r\n    [544, 556) 'bias_quantized'\r\n    [576, 588) 'golden_quantized'\r\n    [608, 620) 'output_data61' <== Memory access at offset 620 overflows this variable\r\n    [640, 660) 'input_shape'\r\n    [704, 896) 'input_data'\r\n    [960, 980) 'output_shape'\r\n    [1024, 1120) 'golden_data'\r\n    [1152, 1200) 'input_quantized65'\r\n    [1232, 1244) 'filter_quantized66'\r\n    [1264, 1276) 'bias_quantized67'\r\n    [1296, 1320) 'golden_quantized68'\r\n    [1360, 1384) 'conv_params'\r\n    [1424, 1436) 'output_data95'\r\n    [1456, 1472) 'input_quantized98'\r\n    [1488, 1500) 'filter_quantized99'\r\n    [1520, 1532) 'bias_quantized100'\r\n    [1552, 1564) 'golden_quantized101'\r\n    [1584, 1600) 'zero_points'\r\n    [1616, 1632) 'scales'\r\n    [1648, 1660) 'output_data126'\r\n    [1680, 1700) 'input_shape132'\r\n    [1744, 1936) 'input_data133'\r\n    [2000, 2020) 'output_shape135'\r\n    [2064, 2160) 'golden_data136'\r\n    [2192, 2240) 'input_quantized137'\r\n    [2272, 2284) 'filter_quantized138'\r\n    [2304, 2316) 'bias_quantized139'\r\n    [2336, 2360) 'golden_quantized140'\r\n    [2400, 2416) 'zero_points141'\r\n    [2432, 2448) 'scales142'\r\n    [2464, 2488) 'conv_params143'\r\n    [2528, 2540) 'output_data175'\r\n    [2560, 2572) 'bias_values176'\r\n    [2592, 2640) 'golden_data177'\r\n    [2672, 2688) 'input_quantized182'\r\n    [2704, 2716) 'filter_quantized183'\r\n    [2736, 2748) 'bias_quantized184'\r\n    [2768, 2780) 'golden_quantized185'\r\n    [2800, 2816) 'zero_points186'\r\n    [2832, 2848) 'scales187'\r\n    [2864, 2888) 'conv_params213'\r\n    [2928, 2948) 'kInputShape'\r\n    [2992, 3056) 'kInputData'\r\n    [3088, 3108) 'kFilterShape219'\r\n    [3152, 3200) 'kFilterData'\r\n    [3232, 3240) 'kBiasShape228'\r\n    [3264, 3276) 'kBiasData'\r\n    [3296, 3316) 'kOutputShape229'\r\n    [3360, 3372) 'output_data230'\r\n    [3392, 3440) 'kGoldenData'\r\n    [3472, 3484) 'golden_quantized237'\r\n    [3504, 3528) 'conv_params266'\r\n    [3568, 3588) 'kInputShape267'\r\n    [3632, 3696) 'kInputData276'\r\n    [3728, 3748) 'kFilterShape277'\r\n    [3792, 3840) 'kFilterData286'\r\n    [3872, 3880) 'kBiasShape289'\r\n    [3904, 3916) 'kBiasData290'\r\n    [3936, 3956) 'kOutputShape291'\r\n    [4000, 4012) 'output_data293'\r\n    [4032, 4080) 'kGoldenData294'\r\n    [4112, 4124) 'golden_quantized303'\r\n    [4144, 4156) 'output_data333'\r\n    [4176, 4192) 'input_quantized336'\r\n    [4208, 4220) 'filter_quantized337'\r\n    [4240, 4252) 'bias_quantized338'\r\n    [4272, 4284) 'golden_quantized339'\r\n    [4304, 4320) 'zero_points340'\r\n    [4336, 4352) 'scales341'\r\n    [4368, 4388) 'filter_zero_points'\r\n    [4432, 4452) 'filter_scales'\r\n    [4496, 4520) 'filter_quant'\r\n    [4560, 4584) 'bias_quant'\r\n    [4624, 4736) 'input_tensor'\r\n    [4768, 4880) 'filter_tensor'\r\n    [4912, 5024) 'bias_tensor'\r\n    [5056, 5168) 'output_tensor'\r\n    [5200, 5208) 'input_scales'\r\n    [5232, 5240) 'input_zero_points'\r\n    [5264, 5288) 'input_quant'\r\n    [5328, 5776) 'tensors'\r\n    [5840, 5852) 'output_data464'\r\n    [5872, 5888) 'input_quantized468'\r\n    [5904, 5916) 'filter_quantized469'\r\n    [5936, 5948) 'bias_quantized470'\r\n    [5968, 5980) 'golden_quantized471'\r\n    [6000, 6112) 'input_tensor488'\r\n    [6144, 6152) 'input_zero_points492'\r\n    [6176, 6184) 'input_scales493'\r\n    [6208, 6232) 'input_quant494'\r\n    [6272, 6384) 'filter_tensor509'\r\n    [6416, 6424) 'filter_zero_points513'\r\n    [6448, 6456) 'filter_scales514'\r\n    [6480, 6504) 'filter_quant515'\r\n    [6544, 6656) 'bias_tensor532'\r\n    [6688, 6696) 'bias_zero_points'\r\n    [6720, 6728) 'bias_scales'\r\n    [6752, 6776) 'bias_quant536'\r\n    [6816, 6928) 'output_tensor551'\r\n    [6960, 6968) 'output_zero_points'\r\n    [6992, 7000) 'output_scales'\r\n    [7024, 7048) 'output_quant'\r\n    [7088, 7536) 'tensors572'\r\nHINT: this may be a false positive if your program uses some custom stack unwind mechanism, swapcontext or vfork\r\n      (longjmp and C++ exceptions *are* supported)\r\nSUMMARY: AddressSanitizer: stack-buffer-overflow (/home/advaitjain/.cache/bazel/_bazel_advaitjain/d870524cf2b6c91d24e6d0f78ed0142b/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/micro/kernels/conv_test_binary+0x5a5dd6) in tflite::reference_ops::Conv(tflite::ConvParams const&, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, unsigned char*, tflite::RuntimeShape const&, unsigned char*, void*)\r\nShadow bytes around the buggy address:\r\n  0x100048accfd0: 00 00 00 00 f1 f1 f1 f1 00 f2 f2 f2 f8 f8 f8 f8\r\n  0x100048accfe0: f8 f8 f2 f2 f2 f2 f8 f2 f2 f2 f8 f8 f8 f2 f2 f2\r\n  0x100048accff0: f2 f2 f8 f8 f8 f8 f2 f2 f2 f2 f8 f2 f2 f2 f8 f2\r\n  0x100048acd000: f8 f8 f8 f2 f2 f2 f2 f2 f8 f2 f2 f2 f8 f8 f2 f2\r\n  0x100048acd010: f8 f8 f2 f2 f8 f8 f2 f2 f8 f8 f2 f2 f8 f8 f2 f2\r\n=>0x100048acd020: 00[04]f2 f2 00 00 04 f2 f2 f2 f2 f2 00 00 00 00\r\n  0x100048acd030: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x100048acd040: 00 00 00 00 f2 f2 f2 f2 f2 f2 f2 f2 00 00 04 f2\r\n  0x100048acd050: f2 f2 f2 f2 00 00 00 00 00 00 00 00 00 00 00 00\r\n  0x100048acd060: f2 f2 f2 f2 00 00 00 00 00 00 f2 f2 f2 f2 00 04\r\n  0x100048acd070: f2 f2 00 04 f2 f2 00 00 00 f2 f2 f2 f2 f2 00 00\r\nShadow byte legend (one shadow byte represents 8 application bytes):\r\n  Addressable:           00\r\n  Partially addressable: 01 02 03 04 05 06 07 \r\n  Heap left redzone:       fa\r\n  Freed heap region:       fd\r\n  Stack left redzone:      f1\r\n  Stack mid redzone:       f2\r\n  Stack right redzone:     f3\r\n  Stack after return:      f5\r\n  Stack use after scope:   f8\r\n  Global redzone:          f9\r\n  Global init order:       f6\r\n  Poisoned by user:        f7\r\n  Container overflow:      fc\r\n  Array cookie:            ac\r\n  Intra object redzone:    bb\r\n  ASan internal:           fe\r\n  Left alloca redzone:     ca\r\n  Right alloca redzone:    cb\r\n  Shadow gap:              cc\r\n==44784==ABORTING\r\n```\r\n", "@giorgio-arenarm Can you please check @advaitjain's comments and keep us posted. Thanks!", "Done. The bazel command revealed the error; I wasn't getting it because my machine was allocating extra memory even though the code was allocating less than needed. Could you please double check that this works now? Thanks", "@giorgio-arenarm, if possible please add new commits during the review process so that it is easy to see what the incremental changes are to the PR. Right now there is a single commit which makes it hard to follow along the review process.", "@giorgio-arenarm Can you please check @advaitjain's comments and keep us posted. Thanks!", "@gbaned All required changes have been carried out. Thanks", "@gbaned, this PR is ready to go. My comments were only for future PRs.", "Confirming that conv_test passes the internal CI builds.\r\n\r\n@giorgio-arenarm , I should have mentioned this in my earlier comment that depthwise_conv_test also fails with asan and will likely need a similar fix.\r\n\r\n```\r\nCC=clang BAZEL_COMPILER=llvm bazel run --copt=-DADDRESS_SANITIZER     --copt=-fsanitize=address --linkopt=-fsanitize=address tensorflow/lite/micro/kernels:depthwise_conv_test_binary\r\n```\r\n\r\n", "Hi @advaitjain I've had a look at the depthwise_conv failures and it doesn't seem to be the same error as conv. This time the amount of memory allocated for the vectors is right but the values are wrong. Since this patch is quite old, a lot might have changed in the implementation of the functions, so I just changed the golden values, matching what the reference kernels expect. Please let me know if this is ok, thanks!", "Odd that the output values changed so significantly. It is what it is -- let's get this PR merged.\r\n", "sorry one more test failure, portable_optimized_depthwise_conv_test.\r\n\r\nLet's make that work with asan too, just to be sure.\r\n\r\nHopefully after this we're done. Apologies for the long round trip times with the CI failures.\r\n\r\n```\r\nCC=clang BAZEL_COMPILER=llvm bazel run --copt=-DADDRESS_SANITIZER     --copt=-fsanitize=address --linkopt=-fsanitize=address tensorflow/lite/micro/kernels:portable_optimized_depthwise_conv_test_binary\r\n```", "Hi @advaitjain these errors are very interesting since they confirm my concern from before. Portable optimized depth_conv works fine with the golden values of the previous commit, which were the ones that were correct some versions ago. However, the regular depth_conv now fails with those values and I had to change them. In my opinion the regular depth_conv has had some changes recently that made it yield wrong result, but really either one could be the problem, it needs more investigation. Could you please double check what version I should go with? Thanks", "@giorgio-arenarm, we'll need to look into this in more detail. I have an internal bug filed and will get back to you once we figure out what is happening here.", "@advaitjain Any update on this PR? Please. Thanks!", "@giorgio-arenarm seeing this error internally , can you please check \r\n\r\n`tensorflow/lite/micro/kernels/depthwise_conv_test.cc:1038:25: error: no matching function for call to 'ValidateDepthwiseConvGoldens'\r\n  TfLiteStatus status = tflite::testing::ValidateDepthwiseConvGoldens(`", "@giorgio-arenarm: thanks for your patience.\r\n\r\nPlease merge master one more time.\r\n\r\n934df8d has removed the portable optimized kernels (thanks to the great work from you guys on the CMSIS-NN implementations) so you as long as the reference kernels pass, we should be good to go.\r\n\r\nThere may very well be an issue with the reference kernels, but we will tackle that separately.", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 34998, "title": "Classification of Forum for Microcontrollers Vs Android/other OS Apps", "body": "Hi Experts,\r\n\r\nI found the issues reported are mixed of platforms used like Desktop, Android, micontroller and other OS. Is there feature to classify it only based on micro-controller based APPS.\r\n\r\nIt will help better understand the types of problems and issues.\r\n", "comments": ["@techguyzz You may use any of the [templates](https://github.com/tensorflow/tensorflow/issues/new/choose) to post a issue, make sure you provide all relevant information as much as you can. Once you post the issue we can label appropriately after interpreting the content. We can use label ```micro``` on GitHub for your case.\r\n![image](https://user-images.githubusercontent.com/42785357/70650070-3c473280-1c03-11ea-8d91-20207155bf21.png)\r\n"]}, {"number": 34997, "title": "ValueError: You are trying to load a weight file containing 5 layers into a model with 0 layers", "body": "I have defined a custom model and custom train using tf2.0\r\nNot able to load weights of the model from checkpoint or .h5 file.", "comments": ["@manish-kumar-garg can you please post some sample code of yours about how are you loading your model. "]}, {"number": 34996, "title": "Compilation issue on Example program of TF Lite", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Desktop\r\n- TensorFlow installed from (source or binary): Source https://github.com/tensorflow/tensorflow.git\r\n- TensorFlow version: Release 1.15.0\r\n- Python version:\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nDefault compilation of example folder (Hello World) leads to missing header file reference and shows error.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Install Ubuntu 18.04 OS\r\n2. Download git using command \"git clone https://github.com/tensorflow/tensorflow.git\"\r\n3. Install build essentials using command \"sudo apt install build-essential\"\r\n4. Install curl library using command \"sudo apt install curl\"\r\n5. Issue command \"make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test\"\r\n\r\n**Any other info / logs**\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test\r\ng++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -fno-rtti -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/logical.cc -o tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/logical.o\r\nIn file included from ./tensorflow/lite/kernels/internal/reference/binary_function.h:18:0,\r\n                 from tensorflow/lite/micro/kernels/logical.cc:16:\r\n./tensorflow/lite/kernels/internal/common.h:24:10: fatal error: fixedpoint/fixedpoint.h: No such file or directory\r\n #include \"fixedpoint/fixedpoint.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\ntensorflow/lite/micro/tools/make/Makefile:257: recipe for target 'tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/logical.o' failed\r\nmake: *** [tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/logical.o] Error 1", "comments": ["I've tried locally and am not seeing the same issue you see.  I'm wondering if one of the downloads may have failed for you.\r\nPlease try running\r\nmake -f tensorflow/lite/micro/tools/make/Makefile clean_downloads\r\nmake -f tensorflow/lite/micro/tools/make/Makefile clean\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test", "Hi Nat,\r\n\r\nIts working fine after Executing mentioned steps. I believe it should be download failure of Header file \"tensorflow/lite/micro/tools/make/downloads/gemmlowp/fixedpoint/fixedpoint.h\"\r\n\r\n"]}, {"number": 34995, "title": "Use PEP508 environment markers (r2.0)", "body": "PEP508 environment markers should always be used for conditional dependencies.\r\n\r\nUsing environment markers is especially important for package managers with static dependency resolution such as Poetry. The current master branch consistently uses environment markers, but TF 2.1 hasn't been released. I would appreciate a backport of this fix to TF 2.0.", "comments": ["Note that even if merging this, we won't do a patch release unless we find a security issue that we patch."]}, {"number": 34994, "title": "Use PEP508 environment markers (r1.15)", "body": "PEP508 environment markers should always be used for conditional dependencies.\r\n\r\nUsing environment markers is especially important for package managers with static dependency resolution such as Poetry. The current master branch consistently uses environment markers, but I'm still relying on TensorFlow 1.x and, thus, would appreciate a backport of this fix.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34994) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34994) for more info**.\n\n<!-- ok -->", "@googlebot I consent.", "The `size:XL` tag should actually be `size:S`, but it's `XL` because I initially requested a merge into `master` by accident."]}, {"number": 34993, "title": "Use PEP508 environment markers (r1.14)", "body": "PEP508 environment markers should always be used for conditional dependencies.\r\n\r\nUsing environment markers is especially important for package managers with static dependency resolution such as Poetry. The current master branch consistently uses environment markers, but I'm still relying on TensorFlow 1.x and, thus, would appreciate a backport of this fix.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34993) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34993) for more info**.\n\n<!-- ok -->", "We won't issue patch releases for 1.14 or before due to reaching end of life."]}, {"number": 34991, "title": "_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): [Ubuntu 18.04.3 LTS](https://wiki.odroid.com/odroid-c2/os_images/ubuntu/v3.1) on ODROID C2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not a mobile but the hardware is ODROID C2\r\n- TensorFlow installed from (source or binary): source (cross-compiled for ODROID C2)\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: Using virtualenv, installed using pip, with a bazel built .whl file\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: Mali450MP3 (But GPU not used)\r\n\r\n\r\n\r\n**Describe the problem**\r\nBazel build from source and pip install successful, but doing `import tensorflow` results in following error -\r\n```\r\nImportError: /home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory\r\n```\r\nThe file is not missing, it exists.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n- Setup a docker container for Ubuntu 18.04.3 and open a terminal into it\r\n- Follow all instructions from [official build from source docs](https://www.tensorflow.org/install/source)\r\n    - Use `python3-dev  python3-pip`\r\n    - Use `virtualenv` to install python dependencies\r\n    - Install Bazel binary directly (no bazelisk), version 0.26.0 and add it to your $PATH\r\n    - Do not use GPU support\r\n    - Clone Tensorflow from official Github repo, and use tag v2.0.0\r\n    - During `./configure`, say Yes to `XLA JIT` and No to everything else\r\n    - During `./configure`, Set the optimization flag to `-march=armv8-a+crc`\r\n    - Use Bazel build command : `bazel build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package`\r\n- Transfer the built `.whl` to ODROID C2 and rename `tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl` to `tensorflow-2.0.0-cp36-cp36m-linux_aarch64.whl`\r\n- Setup Python on the device using `sudo apt-get install python3-dev python3-pip`\r\n- Setup a virtualenv using `virtualenv -p python3.6 ./venv` and activate it\r\n- Install Tensorflow using `pip install tensorflow-2.0.0-cp36-cp36m-linux_aarch64.whl`\r\n    - Install HDF5 system package dependency using `sudo apt-get install pkg-config libhdf5-100 libhdf5-dev`\r\n- Run Python interpreter and execute `import tensorflow`\r\n- You will get the above error regarding `_pywrap_tensorflow_internal.so`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n- The `/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so` file exists and is 223M in size\r\n- Output of `ldd /home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so` gives `not a dynamic executable`\r\n- Full error Traceback\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/rmc/code/venv/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/rmc/code/venv/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow/__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/home/rmc/code/venv/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/rmc/code/venv/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/rmc/code/venv/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "comments": ["Also sharing the Bazel logs as attachment.\r\n\r\n[Bazel logs.zip](https://github.com/tensorflow/tensorflow/files/3944213/Bazel.logs.zip)\r\n\r\n", "I just had a similar error. The error message is misleading but it may be that `_pywrap_tensorflow_internal.so` is not compiled for the architecture you would expect i.e. the cross compilation went wrong. Can you check what is the output of `file /home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so`?", "@Rbiessy I tried to inspect the file `/home/rmc/code/venv/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so`. Sharing the details -\r\n\r\n- Output of `ldd _pywrap_tensorflow_internal.so` gives \r\n`not a dynamic executable`\r\n- The file size is 223M.\r\n- Output of `readelf -a _pywrap_tensorflow_internal.so` gives\r\n```\r\nELF Header:\r\n  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 \r\n  Class:                             ELF64\r\n  Data:                              2's complement, little endian\r\n  Version:                           1 (current)\r\n  OS/ABI:                            UNIX - System V\r\n  ABI Version:                       0\r\n  Type:                              DYN (Shared object file)\r\n  Machine:                           Advanced Micro Devices X86-64\r\n  Version:                           0x1\r\n  Entry point address:               0x0\r\n  Start of program headers:          64 (bytes into file)\r\n  Start of section headers:          233778704 (bytes into file)\r\n  Flags:                             0x0\r\n  Size of this header:               64 (bytes)\r\n  Size of program headers:           56 (bytes)\r\n  Number of program headers:         9\r\n  Size of section headers:           64 (bytes)\r\n  Number of section headers:         36\r\n  Section header string table index: 35\r\n\r\nSection Headers:\r\n  [Nr] Name              Type             Address           Offset\r\n       Size              EntSize          Flags  Link  Info  Align\r\n  [ 0]                   NULL             0000000000000000  00000000\r\n       0000000000000000  0000000000000000           0     0     0\r\n  [ 1] .note.gnu.build-i NOTE             0000000000000238  00000238\r\n       0000000000000024  0000000000000000   A       0     0     4\r\n  [ 2] .dynsym           DYNSYM           0000000000000260  00000260\r\n       000000000026cf40  0000000000000018   A       3     5     8\r\n  [ 3] .dynstr           STRTAB           000000000026d1a0  0026d1a0\r\n       0000000000ede6c5  0000000000000000   A       0     0     1\r\n  [ 4] .gnu.hash         GNU_HASH         000000000114b868  0114b868\r\n       00000000000c4940  0000000000000000   A       2     0     8\r\n  [ 5] .gnu.version      VERSYM           00000000012101a8  012101a8\r\n       0000000000033bf0  0000000000000002   A       2     0     2\r\n  [ 6] .gnu.version_d    VERDEF           0000000001243d98  01243d98\r\n       0000000000000038  0000000000000000   A       3     2     4\r\n  [ 7] .gnu.version_r    VERNEED          0000000001243dd0  01243dd0\r\n       0000000000000350  0000000000000000   A       3     8     4\r\n  [ 8] .rela.dyn         RELA             0000000001244120  01244120\r\n       00000000005f6718  0000000000000018   A       2     0     8\r\n  [ 9] .rela.plt         RELA             000000000183a838  0183a838\r\n       00000000001179d8  0000000000000018  AI       2    11     8\r\n  [10] .init             PROGBITS         0000000001952210  01952210\r\n       0000000000000017  0000000000000000  AX       0     0     4\r\n  [11] .plt              PROGBITS         0000000001952230  01952230\r\n       00000000000ba6a0  0000000000000010  AX       0     0     16\r\n  [12] .text             PROGBITS         0000000001a0c900  01a0c900\r\n       000000000722ca3f  0000000000000000  AX       0     0     64\r\n  [13] text_env          PROGBITS         0000000008c39340  08c39340\r\n       0000000000002c7f  0000000000000000  AX       0     0     16\r\n  [14] malloc_hook       PROGBITS         0000000008c3bfc0  08c3bfc0\r\n       0000000000000282  0000000000000000  AX       0     0     16\r\n  [15] .fini             PROGBITS         0000000008c3c244  08c3c244\r\n       0000000000000009  0000000000000000  AX       0     0     4\r\n  [16] .rodata           PROGBITS         0000000008c3d000  08c3d000\r\n       00000000010d5390  0000000000000000   A       0     0     4096\r\n  [17] .gcc_except_table PROGBITS         0000000009d12390  09d12390\r\n       00000000001ab801  0000000000000000   A       0     0     4\r\n  [18] .eh_frame         PROGBITS         0000000009ebdb98  09ebdb98\r\n       00000000007ed994  0000000000000000   A       0     0     8\r\n  [19] .eh_frame_hdr     PROGBITS         000000000a6ab52c  0a6ab52c\r\n       000000000016a54c  0000000000000000   A       0     0     4\r\n  [20] .tbss             NOBITS           000000000a8170e0  0a8160e0\r\n       0000000000000160  0000000000000000 WAT       0     0     8\r\n  [21] .data.rel.ro.loca PROGBITS         000000000a8170e0  0a8160e0\r\n       00000000001cf850  0000000000000000  WA       0     0     32\r\n  [22] .fini_array       FINI_ARRAY       000000000a9e6930  0a9e5930\r\n       0000000000000008  0000000000000008  WA       0     0     8\r\n  [23] .init_array       INIT_ARRAY       000000000a9e6938  0a9e5938\r\n       0000000000004d08  0000000000000008  WA       0     0     8\r\n  [24] .data.rel.ro      PROGBITS         000000000a9eb640  0a9ea640\r\n       000000000016ef78  0000000000000000  WA       0     0     32\r\n  [25] .dynamic          DYNAMIC          000000000ab5a5b8  0ab595b8\r\n       00000000000002b0  0000000000000010  WA       3     0     8\r\n  [26] .got              PROGBITS         000000000ab5a868  0ab59868\r\n       0000000000037428  0000000000000000  WA       0     0     8\r\n  [27] .got.plt          PROGBITS         000000000ab91c90  0ab90c90\r\n       000000000005d360  0000000000000000  WA       0     0     8\r\n  [28] .tm_clone_table   PROGBITS         000000000abef000  0abee000\r\n       0000000000000000  0000000000000000  WA       0     0     8\r\n  [29] .data             PROGBITS         000000000abef000  0abee000\r\n       0000000000020f4c  0000000000000000  WA       0     0     32\r\n  [30] .bss              NOBITS           000000000ac0ff80  0ac0ef4c\r\n       0000000000065ce8  0000000000000000  WA       0     0     64\r\n  [31] .comment          PROGBITS         0000000000000000  0ac0ef4c\r\n       000000000000002c  0000000000000001  MS       0     0     1\r\n  [32] .note.gnu.gold-ve NOTE             0000000000000000  0ac0ef78\r\n       000000000000001c  0000000000000000           0     0     4\r\n  [33] .symtab           SYMTAB           0000000000000000  0ac0ef98\r\n       0000000000744618  0000000000000018          34   211534     8\r\n  [34] .strtab           STRTAB           0000000000000000  0b3535b0\r\n       0000000002b9f6f0  0000000000000000           0     0     1\r\n  [35] .shstrtab         STRTAB           0000000000000000  0def2ca0\r\n       0000000000000170  0000000000000000           0     0     1\r\nKey to Flags:\r\n  W (write), A (alloc), X (execute), M (merge), S (strings), I (info),\r\n  L (link order), O (extra OS processing required), G (group), T (TLS),\r\n  C (compressed), x (unknown), o (OS specific), E (exclude),\r\n  l (large), p (processor specific)\r\n\r\nThere are no section groups in this file.\r\n\r\nProgram Headers:\r\n  Type           Offset             VirtAddr           PhysAddr\r\n                 FileSiz            MemSiz              Flags  Align\r\n  PHDR           0x0000000000000040 0x0000000000000040 0x0000000000000040\r\n                 0x00000000000001f8 0x00000000000001f8  R      0x8\r\n  LOAD           0x0000000000000000 0x0000000000000000 0x0000000000000000\r\n                 0x000000000a815a78 0x000000000a815a78  R E    0x1000\r\n  LOAD           0x000000000a8160e0 0x000000000a8170e0 0x000000000a8170e0\r\n                 0x00000000003f8e6c 0x000000000045eb88  RW     0x1000\r\n  DYNAMIC        0x000000000ab595b8 0x000000000ab5a5b8 0x000000000ab5a5b8\r\n                 0x00000000000002b0 0x00000000000002b0  RW     0x8\r\n  NOTE           0x0000000000000238 0x0000000000000238 0x0000000000000238\r\n                 0x0000000000000024 0x0000000000000024  R      0x4\r\n  GNU_EH_FRAME   0x000000000a6ab52c 0x000000000a6ab52c 0x000000000a6ab52c\r\n                 0x000000000016a54c 0x000000000016a54c  R      0x4\r\n  GNU_STACK      0x0000000000000000 0x0000000000000000 0x0000000000000000\r\n                 0x0000000000000000 0x0000000000000000  RW     0x10\r\n  TLS            0x000000000a8160e0 0x000000000a8170e0 0x000000000a8170e0\r\n                 0x0000000000000000 0x0000000000000160  R      0x8\r\n  GNU_RELRO      0x000000000a8160e0 0x000000000a8170e0 0x000000000a8170e0\r\n                 0x00000000003d7f20 0x00000000003d7f20  RW     0x20\r\n\r\n Section to Segment mapping:\r\n  Segment Sections...\r\n   00     \r\n   01     .note.gnu.build-id .dynsym .dynstr .gnu.hash .gnu.version .gnu.version_d .gnu.version_r .rela.dyn .rela.plt .init .plt .text text_env malloc_hook .fini .rodata .gcc_except_table .eh_frame .eh_frame_hdr \r\n   02     .data.rel.ro.local .fini_array .init_array .data.rel.ro .dynamic .got .got.plt .tm_clone_table .data .bss \r\n   03     .dynamic \r\n   04     .note.gnu.build-id \r\n   05     .eh_frame_hdr \r\n   06     \r\n   07     .tbss \r\n   08     .data.rel.ro.local .fini_array .init_array .data.rel.ro .dynamic .got .got.plt \r\n\r\nDynamic section at offset 0xab595b8 contains 38 entries:\r\n  Tag        Type                         Name/Value\r\n 0x0000000000000003 (PLTGOT)             0xab91c90\r\n 0x0000000000000002 (PLTRELSZ)           1145304 (bytes)\r\n 0x0000000000000017 (JMPREL)             0x183a838\r\n 0x0000000000000014 (PLTREL)             RELA\r\n 0x0000000000000007 (RELA)               0x1244120\r\n 0x0000000000000008 (RELASZ)             6252312 (bytes)\r\n 0x0000000000000009 (RELAENT)            24 (bytes)\r\n 0x000000006ffffff9 (RELACOUNT)          129300\r\n 0x0000000000000006 (SYMTAB)             0x260\r\n 0x000000000000000b (SYMENT)             24 (bytes)\r\n 0x0000000000000005 (STRTAB)             0x26d1a0\r\n 0x000000000000000a (STRSZ)              15591109 (bytes)\r\n 0x000000006ffffef5 (GNU_HASH)           0x114b868\r\n 0x0000000000000001 (NEEDED)             Shared library: [libtensorflow_framework.so.2]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libdl.so.2]\r\n 0x0000000000000001 (NEEDED)             Shared library: [librt.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libm.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libpthread.so.0]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6]\r\n 0x0000000000000001 (NEEDED)             Shared library: [ld-linux-x86-64.so.2]\r\n 0x000000000000000e (SONAME)             Library soname: [_pywrap_tensorflow_internal.so]\r\n 0x000000000000000c (INIT)               0x1952210\r\n 0x000000000000000d (FINI)               0x8c3c244\r\n 0x000000000000001a (FINI_ARRAY)         0xa9e6930\r\n 0x000000000000001c (FINI_ARRAYSZ)       8 (bytes)\r\n 0x0000000000000019 (INIT_ARRAY)         0xa9e6938\r\n 0x000000000000001b (INIT_ARRAYSZ)       19720 (bytes)\r\n 0x000000000000001d (RUNPATH)            Library runpath: [$ORIGIN/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:$ORIGIN/:$ORIGIN/..]\r\n 0x000000000000001e (FLAGS)              BIND_NOW\r\n 0x000000006ffffffb (FLAGS_1)            Flags: NOW\r\n 0x000000006ffffff0 (VERSYM)             0x12101a8\r\n 0x000000006ffffffc (VERDEF)             0x1243d98\r\n 0x000000006ffffffd (VERDEFNUM)          2\r\n 0x000000006ffffffe (VERNEED)            0x1243dd0\r\n 0x000000006fffffff (VERNEEDNUM)         8\r\n 0x0000000000000000 (NULL)               0x0\r\n\r\nRelocation section '.rela.dyn' at offset 0x1244120 contains 260513 entries:\r\n  Offset          Info           Type           Sym. Value    Sym. Name + Addend\r\n00000a8170e8  000000000008 R_X86_64_RELATIVE                    a9eb868\r\n00000a8170f0  000000000008 R_X86_64_RELATIVE                    1ce5530\r\n00000a8170f8  000000000008 R_X86_64_RELATIVE                    1ce4b10\r\n00000a817100  000000000008 R_X86_64_RELATIVE                    1ceeb80\r\n00000a817120  000000000008 R_X86_64_RELATIVE                    8c4416b\r\n00000a817130  000000000008 R_X86_64_RELATIVE                    8c4365a\r\n00000a817140  000000000008 R_X86_64_RELATIVE                    8c433c1\r\n00000a817148  000000000008 R_X86_64_RELATIVE                    8c4416d\r\n00000a817150  000000000008 R_X86_64_RELATIVE                    8c433e6\r\n00000a817158  000000000008 R_X86_64_RELATIVE                    8c44176\r\n00000a817160  000000000008 R_X86_64_RELATIVE                    8c43335\r\n00000a817168  000000000008 R_X86_64_RELATIVE                    8c44188\r\n00000a817170  000000000008 R_X86_64_RELATIVE                    8c43436\r\n00000a817178  000000000008 R_X86_64_RELATIVE                    8c4418e\r\n00000a817180  000000000008 R_X86_64_RELATIVE                    8c4346a\r\n00000a817188  000000000008 R_X86_64_RELATIVE                    8c4419d\r\n00000a817190  000000000008 R_X86_64_RELATIVE                    8c43488\r\n00000a817198  000000000008 R_X86_64_RELATIVE                    8c441a5\r\n00000a8171a0  000000000008 R_X86_64_RELATIVE                    8c434c0\r\n00000a8171a8  000000000008 R_X86_64_RELATIVE                    8c441b6\r\n00000a8171b0  000000000008 R_X86_64_RELATIVE                    8c434dc\r\n00000a8171b8  000000000008 R_X86_64_RELATIVE                    8c441bd\r\n00000a8171c0  000000000008 R_X86_64_RELATIVE                    8c4351d\r\n00000a8171c8  000000000008 R_X86_64_RELATIVE                    8c441cd\r\n00000a8171d0  000000000008 R_X86_64_RELATIVE                    8c43512\r\n00000a8171d8  000000000008 R_X86_64_RELATIVE                    8c441d8\r\n00000a8171e0  000000000008 R_X86_64_RELATIVE                    8c43564\r\n00000a8171e8  000000000008 R_X86_64_RELATIVE                    8d05a18\r\n00000a8171f0  000000000008 R_X86_64_RELATIVE                    8c43582\r\n00000a8171f8  000000000008 R_X86_64_RELATIVE                    8d05a40\r\n00000a817200  000000000008 R_X86_64_RELATIVE                    8c435ac\r\n00000a817208  000000000008 R_X86_64_RELATIVE                    8c441ec\r\n00000a817210  000000000008 R_X86_64_RELATIVE                    8c435cc\r\n00000a817218  000000000008 R_X86_64_RELATIVE                    8d05a80\r\n00000a817220  000000000008 R_X86_64_RELATIVE                    8c435d9\r\n00000a817228  000000000008 R_X86_64_RELATIVE                    8d05aa8\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n.\r\n19d08:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d0c:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d10:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d14:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d18:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d1c:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d20:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d24:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d28:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d2c:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d30:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d34:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d38:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d3c:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d40:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d44:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d48:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d4c:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d50:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d54:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d58:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d5c:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d60:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d64:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d68:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d6c:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d70:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d74:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d78:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d7c:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d80:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d84:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d88:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d8c:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d90:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d94:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d98:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19d9c:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19da0:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19da4:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19da8:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19dac:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19db0:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19db4:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19db8:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19dbc:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19dc0:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19dc4:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19dc8:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19dcc:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19dd0:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19dd4:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19dd8:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19ddc:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19de0:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19de4:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19de8:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19dec:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19df0:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n19df4:   2 (tensorflow)    2 (tensorflow)    2 (tensorflow)    2 (tensorflow) \r\n\r\nVersion definition section '.gnu.version_d' contains 2 entries:\r\nAddr: 0x0000000001243d98  Offset: 0x1243d98  Link: 3 (.dynstr)\r\n000000: Rev: 1  Flags: BASE  Index: 1  Cnt: 1  Name: _pywrap_tensorflow_internal.so\r\n0x001c: Rev: 1  Flags: none  Index: 2  Cnt: 1  Name: tensorflow\r\n\r\nVersion needs section '.gnu.version_r' contains 8 entries:\r\nAddr: 0x0000000001243dd0  Offset: 0x1243dd0  Link: 3 (.dynstr)\r\n000000: Version: 1  File: libc.so.6  Cnt: 15\r\n0x0010:   Name: GLIBC_2.9  Flags: none  Version: 3\r\n0x0020:   Name: GLIBC_2.17  Flags: none  Version: 4\r\n0x0030:   Name: GLIBC_2.2.5  Flags: none  Version: 5\r\n0x0040:   Name: GLIBC_2.10  Flags: none  Version: 6\r\n0x0050:   Name: GLIBC_2.3.3  Flags: none  Version: 7\r\n0x0060:   Name: GLIBC_2.16  Flags: none  Version: 8\r\n0x0070:   Name: GLIBC_2.3  Flags: none  Version: 9\r\n0x0080:   Name: GLIBC_2.6  Flags: none  Version: 10\r\n0x0090:   Name: GLIBC_2.7  Flags: none  Version: 11\r\n0x00a0:   Name: GLIBC_2.15  Flags: none  Version: 12\r\n0x00b0:   Name: GLIBC_2.3.2  Flags: none  Version: 13\r\n0x00c0:   Name: GLIBC_2.3.4  Flags: none  Version: 14\r\n0x00d0:   Name: GLIBC_2.11  Flags: none  Version: 15\r\n0x00e0:   Name: GLIBC_2.4  Flags: none  Version: 16\r\n0x00f0:   Name: GLIBC_2.14  Flags: none  Version: 17\r\n0x0100: Version: 1  File: libpthread.so.0  Cnt: 4\r\n0x0110:   Name: GLIBC_2.3.3  Flags: none  Version: 18\r\n0x0120:   Name: GLIBC_2.2.5  Flags: none  Version: 19\r\n0x0130:   Name: GLIBC_2.12  Flags: none  Version: 20\r\n0x0140:   Name: GLIBC_2.3.2  Flags: none  Version: 21\r\n0x0150: Version: 1  File: libm.so.6  Cnt: 3\r\n0x0160:   Name: GLIBC_2.23  Flags: none  Version: 22\r\n0x0170:   Name: GLIBC_2.2.5  Flags: none  Version: 23\r\n0x0180:   Name: GLIBC_2.27  Flags: none  Version: 24\r\n0x0190: Version: 1  File: libstdc++.so.6  Cnt: 18\r\n0x01a0:   Name: GLIBCXX_3.4.21  Flags: none  Version: 25\r\n0x01b0:   Name: GLIBCXX_3.4  Flags: none  Version: 26\r\n0x01c0:   Name: GLIBCXX_3.4.15  Flags: none  Version: 27\r\n0x01d0:   Name: CXXABI_1.3  Flags: none  Version: 28\r\n0x01e0:   Name: GLIBCXX_3.4.17  Flags: none  Version: 29\r\n0x01f0:   Name: CXXABI_1.3.3  Flags: none  Version: 30\r\n0x0200:   Name: GLIBCXX_3.4.14  Flags: none  Version: 31\r\n0x0210:   Name: CXXABI_1.3.11  Flags: none  Version: 32\r\n0x0220:   Name: GLIBCXX_3.4.11  Flags: none  Version: 33\r\n0x0230:   Name: GLIBCXX_3.4.9  Flags: none  Version: 34\r\n0x0240:   Name: CXXABI_1.3.2  Flags: none  Version: 35\r\n0x0250:   Name: GLIBCXX_3.4.18  Flags: none  Version: 36\r\n0x0260:   Name: CXXABI_1.3.7  Flags: none  Version: 37\r\n0x0270:   Name: GLIBCXX_3.4.19  Flags: none  Version: 38\r\n0x0280:   Name: GLIBCXX_3.4.22  Flags: none  Version: 39\r\n0x0290:   Name: CXXABI_1.3.8  Flags: none  Version: 40\r\n0x02a0:   Name: GLIBCXX_3.4.20  Flags: none  Version: 41\r\n0x02b0:   Name: CXXABI_1.3.5  Flags: none  Version: 42\r\n0x02c0: Version: 1  File: libdl.so.2  Cnt: 1\r\n0x02d0:   Name: GLIBC_2.2.5  Flags: none  Version: 43\r\n0x02e0: Version: 1  File: libgcc_s.so.1  Cnt: 2\r\n0x02f0:   Name: GCC_3.3  Flags: none  Version: 44\r\n0x0300:   Name: GCC_3.0  Flags: none  Version: 45\r\n0x0310: Version: 1  File: librt.so.1  Cnt: 1\r\n0x0320:   Name: GLIBC_2.2.5  Flags: none  Version: 46\r\n0x0330: Version: 1  File: ld-linux-x86-64.so.2  Cnt: 1\r\n0x0340:   Name: GLIBC_2.3  Flags: none  Version: 47\r\n\r\nDisplaying notes found in: .note.gnu.build-id\r\nOwner                 Data size\tDescription\r\nGNU                  0x00000014\tNT_GNU_BUILD_ID (unique build ID bitstring)\r\n  Build ID: 62b20b479c2f22f9ff04b7e42712e87bb3286629\r\n\r\nDisplaying notes found in: .note.gnu.gold-version\r\nOwner                 Data size\tDescription\r\nGNU                  0x00000009\tNT_GNU_GOLD_VERSION (gold version)\r\n  Version: gold 1.15\r\n\r\n```\r\n\r\nComparing the output of other `.so` files present on the system. The `Machine` part of the ELF Header which is currently `Advanced Micro Devices X86-64` should be `AArch64`.", "@gswebspace ,\r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!", "@tilakrayal I don't have access to the odroid hardware anymore, so can't really do much here. That also means that this issue is no longer relevant (for me at least) and can be closed.\r\n\r\nI will close this issue for now. If anybody feels this issue is relevant to them, please feel free to open again. Thank you !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34991\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34991\">No</a>\n"]}, {"number": 34990, "title": "Tensorboard: Can'r reach localhost:6006 after \"fuser 6006/tcp -k\"", "body": "Hi!\r\n\r\nI just installed Tensorboard and everything worked fine. I noticed that the old loss-curves where kept in the plots and after reading online I took the suggestion to kill the process by:\r\n\r\nfuser 6006/tcp -k\r\n\r\nWhen I nor run Tensorboard and get:\r\nReusing TensorBoard on port 6006 (pid 48869), started 1:40:42 ago. (Use '!kill 48869' to kill it.)\r\nPlease visit http://localhost:6006 in a web browser.\r\n\r\nThe webpage cannot be reached (Unable to connect). I have probably screwed something up that a reboot did not solve... Can someone help me to un-screw myself?\r\n\r\nThanks!\r\nbtw, first post so sorry if the format is off!", "comments": ["Please delete this, I didn't follow the new issue template and it does seem to work if don't run it via ipython.\r\n\r\nIf the problem persists I'll resubmit according to the template. \r\n\r\nSorry!", "@Krutang ,\r\nClosing the issue as per your comment, please reopen the issue if it still persists.Thank you!"]}, {"number": 34989, "title": "Dataset from TFRecord has unknown shapes.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: 1080Ti 12G * 4\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI follow the [tf2.0 tutorial](https://www.tensorflow.org/tutorials/load_data/tfrecord) to generate and read the dataset into tf.Dataset. However, the read serialized record data has unknown shapes and unable to call ```strategy.experimental_distribute_dataset``` to use mirror strategy.\r\n\r\nNote that I can load data normally using \r\n```for f0, f1, f2, f3 in train_dataset:```\r\n for single GPU training.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe loaded tf.Dataset should have correct shapes and be able to be distributed using ```strategy.experimental_distribute_dataset```. The features all have fixed shapes, but I don't know how to define them.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nraw_train_dataset = tf.data.TFRecordDataset('path/to/the/record')\r\n\r\ndef read_tfrecord(serialized_example):\r\n    feature_description = {\r\n        'feature0': tf.io.FixedLenFeature([], tf.string),\r\n        'feature1': tf.io.FixedLenFeature([], tf.string),\r\n        'feature2': tf.io.FixedLenFeature([], tf.string),\r\n        'feature3': tf.io.FixedLenFeature([], tf.string)\r\n    }\r\n\r\n    example = tf.io.parse_single_example(serialized_example, feature_description)\r\n    f0 = tf.io.parse_tensor(example['feature0'], tf.uint8)\r\n    f1 = tf.io.parse_tensor(example['feature1'], tf.float32)\r\n    f2 = tf.io.parse_tensor(example['feature2'], tf.int16)\r\n    f3 = tf.io.parse_tensor(example['feature3'], tf.uint8)\r\n\r\n    return f0, f1, f2, f3\r\n\r\ntrain_dataset = raw_train_dataset.map(read_tfrecord)\r\ntrain_dataset = strategy.experimental_distribute_dataset(train_dataset)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nBefore running the last line of code, the printed ```train_dataset``` is \r\n```<MapDataset shapes: (<unknown>, <unknown>, <unknown>, <unknown>), types: (tf.uint8, tf.float32, tf.int16, tf.uint8)>```. \r\nAnd after running the last line to distribute the dataset for mirror strategy, it raises an error ```ValueError: Cannot take the length of shape with unknown rank.```.\r\n", "comments": ["@zhangjh915 \r\n\r\nLooks like code is incomplete.Can you please help us with the simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram Thanks for the reply. I have created a toy program to reproduce the same issue as follows.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# generate toy data\r\nF0 = np.random.randint(2000, size=(100,20,1)).astype('uint8')\r\nF1 = np.random.randint(1500, size=(100,3,5)).astype('float32')\r\nF2 = np.random.randint(1600, size=(100,4,4)).astype('int16')\r\nF3 = np.random.randint(2000, size=(100,20,1)).astype('uint8')\r\n\r\n# this part is based on the tensorflow tutorial.\r\ndef _bytes_feature(value):\r\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\"\"\r\n    if isinstance(value, type(tf.constant(0))):\r\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\ndef serialize_example(feature0, feature1, feature2, feature3):\r\n    \"\"\"\r\n    Creates a tf.Example message ready to be written to a file.\r\n    \"\"\"\r\n    # Create a dictionary mapping the feature name to the tf.Example-compatible\r\n    # data type.\r\n    feature0 = tf.io.serialize_tensor(feature0)\r\n    feature1 = tf.io.serialize_tensor(feature1)\r\n    feature2 = tf.io.serialize_tensor(feature2)\r\n    feature3 = tf.io.serialize_tensor(feature3)\r\n    feature = {\r\n               'feature0': _bytes_feature(feature0),\r\n               'feature1': _bytes_feature(feature1),\r\n               'feature2': _bytes_feature(feature2),\r\n               'feature3': _bytes_feature(feature3)\r\n              }\r\n\r\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\r\n    return example_proto.SerializeToString()\r\n\r\ndef tf_serialize_example(f0, f1, f2, f3):\r\n    tf_string = tf.py_function(\r\n            serialize_example,\r\n            (f0, f1, f2, f3),\r\n            tf.string\r\n            )\r\n    return tf.reshape(tf_string, ())\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((F0,F1,F2,F3))\r\nserialized_train_dataset = train_dataset.map(tf_serialize_example)\r\n\r\ndef generator_train():\r\n    for features in train_dataset:\r\n        yield serialize_example(*features)\r\n\r\nserialized_train_dataset = tf.data.Dataset.from_generator(\r\n        generator_train, output_types=tf.string, output_shapes=())\r\n\r\n# write data into tfrecord, please modify the path\r\nwriter_train = tf.data.experimental.TFRecordWriter('/tfrecord_path/sample.tfrecord')\r\nwriter_train.write(serialized_train_dataset)\r\n\r\n# read saved tfrecord\r\nraw_train_dataset = tf.data.TFRecordDataset('/tfrecord_path/sample.tfrecord')\r\n\r\n# this part is based on the tutorial as well\r\ndef read_tfrecord(serialized_example):\r\n    feature_description = {\r\n            'feature0': tf.io.FixedLenFeature([], tf.string),\r\n            'feature1': tf.io.FixedLenFeature([], tf.string),\r\n            'feature2': tf.io.FixedLenFeature([], tf.string),\r\n            'feature3': tf.io.FixedLenFeature([], tf.string)\r\n            }\r\n\r\n    example = tf.io.parse_single_example(serialized_example, feature_description)\r\n    f0 = tf.io.parse_tensor(example['feature0'], tf.uint8)\r\n    f1 = tf.io.parse_tensor(example['feature1'], tf.float32)\r\n    f2 = tf.io.parse_tensor(example['feature2'], tf.int16)\r\n    f3 = tf.io.parse_tensor(example['feature3'], tf.uint8)\r\n\r\n    return f0, f1, f2, f3\r\n\r\ntrain_dataset = raw_train_dataset.map(read_tfrecord)\r\nprint(train_dataset)  # here the shapes would be printed out as <unknown>\r\nstrategy = tf.distribute.MirroredStrategy()\r\ntrain_dataset = strategy.experimental_distribute_dataset(train_dataset)  # it will raise error here\r\n```", "I have tried on colab with TF version 2.0,2.1.0-rc0 and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/572ac2bf82b49dcc57cbad82a0ca0e35/untitled463.ipynb) Thanks!", "@aaudiber could you please take a look", "Hi @zhangjh915,\r\n\r\nThe shapes are unknown because the `parse_tensor` function can't statically determine the shape of the parsed tensor. If you know the shape, you can use [ensure_shape](https://www.tensorflow.org/api_docs/python/tf/ensure_shape?version=stable) to help with shape inference.\r\n\r\n```\r\ntensor = tf.ensure_shape(tensor, shape)\r\n```\r\n\r\nIn your code it would look like\r\n```\r\n    f0 = tf.ensure_shape(tf.io.parse_tensor(example['feature0'], tf.uint8), ())\r\n    f1 = tf.ensure_shape(tf.io.parse_tensor(example['feature1'], tf.float32), ())\r\n    f2 = tf.ensure_shape(tf.io.parse_tensor(example['feature2'], tf.int16), ())\r\n    f3 = tf.ensure_shape(tf.io.parse_tensor(example['feature3'], tf.uint8), ())\r\n```\r\n\r\nSetting the shapes reveals the real problem; that [experimental_distribute_dataset](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?version=stable#experimental_distribute_dataset) expects the input dataset to be batched. \r\n\r\nThe docs for `experimental_distribute_dataset` mention this. \r\n> We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34989\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34989\">No</a>\n", "I'm not sure this must be filed as a separate issue:\r\nIf using TFRecords to store training data is to be simple and robust, having to manually serialize and parse tensors is already getting our hands quite dirty with the code. Using tf.ensure_shape every time as suggested by @zhangjh915 only worsens this. Will the whole functionality of saving and loading datasets get a major revamp to take care of such checks automatically and overall, make the process of loading and saving datasets simpler?", "@dinesh110598 I like the idea of improving type/shape inference so that users don't need to specify types/shapes if they don't want to. However, this doesn't seem possible for TFRecords since they can contain arbitrary bytes. If we want to automatically infer types/shapes, we need a new format that stores type/shape information at the start of the file."]}, {"number": 34988, "title": "whl is not a supported wheel on this platform", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 19.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): checked out from git, tag r1.15\r\n- TensorFlow version: 1.15\r\n- Python version: python 3.6.8\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0 / 7.6.3.30-1+cuda10.0\r\n- GPU model and memory: GTX 1080 Ti\r\n- CPU model: Intel i9-9900K\r\n\r\n\r\n**Describe the problem**\r\nI have compiled successfully without any error, but when I try to install the wheel package I got the following error.\r\n`ERROR: tensorflow-1.15.0-cp36-cp36m-linux_x86_64-optimized.whl is not a supported wheel on this platform.`\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`bazel build -c opt --copt=-march=native --copt=-mfpmath=both --copt=-mfma --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-mavx2 --config=cuda -k //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n\r\n`python3 -m venv tf`\r\n`. tf/bin/activate`\r\n`pip install --upgrade pip`\r\n`pip install /tmp/tensorflow_pkg/tensorflow-1.15.0-cp36-cp36m-linux_x86_64.whl`\r\n\r\n`ERROR: tensorflow-1.15.0-cp36-cp36m-linux_x86_64-optimized.whl is not a supported wheel on this platform.`", "comments": ["It looks like the problem is with name of wheel package. I renamed original name \r\ntensorflow-1.15.0-cp36-cp36m-linux_x86_64.whl \r\ntensorflow-1.15.0-cp36-cp36m-linux_x86_64-**optimized**.whl\r\n\r\nBut it looks like the platform requirements are taken from the wheel name!\r\nhttps://stackoverflow.com/a/52661110/2663593"]}, {"number": 34987, "title": "The quantized model has low accuracy on Android", "body": "I use these code `https://github.com/tensorflow/models/tree/master/research/slim` train mobilenet V2 model. Then I freeze graph `mobilenet_v2.pb`. The last run this command to get  tensorflow lite model.\r\n```shell\r\nbazel build tensorflow/lite/toco:toco\r\n\r\nbazel-bin/tensorflow/lite/toco/toco \\\r\n  --input_file=/mnt/d/tmp/mobilenet_v2.pb \\\r\n  --output_file=/mnt/d/tmp/mobilenet_v2.tflite \\\r\n  --inference_type=QUANTIZED_UINT8 \\                                                                    \r\n  --input_arrays=input \\\r\n  --output_arrays=MobilenetV2/Predictions/Reshape_1 \\\r\n  --input_shapes=1,224,224,3 \\\r\n  --mean_values=128 \\\r\n  --std_values=128 \\\r\n  --change_concat_input_ranges=false \\\r\n  --allow_custom_ops\r\n```\r\n\r\n```java\r\nimgData = ByteBuffer.allocateDirect(ddims[2] * ddims[3] * 3);\r\n\r\nInterpreter.Options options = new Interpreter.Options();\r\noptions.setNumThreads(NUM_THREADS);\r\ntflite = new Interpreter(file, options);\r\n\r\n    private ByteBuffer getScaledMatrix(Bitmap bitmap) {\r\n        imgData.rewind();\r\n        bitmap.getPixels(intValues, 0, ddims[2], 0, 0, ddims[2], ddims[3]);\r\n        for (int i = 0; i < ddims[2]; ++i) {\r\n            for (int j = 0; j < ddims[3]; ++j) {\r\n                int pixelValue = intValues[i * ddims[2] + j];\r\n                imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n                imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n                imgData.put((byte) (pixelValue & 0xFF));\r\n            }\r\n        }\r\n        if (bitmap.isRecycled()) {\r\n            bitmap.recycle();\r\n        }\r\n        return imgData;\r\n    }\r\n\r\n\r\nByteBuffer inputData = getScaledMatrix(bmp);\r\nbyte[][] labelProbArray = new byte[1][NUM_CLASS];\r\ntflite.run(inputData, labelProbArray);\r\n```\r\n**But the accuracy of the results is very low.**\r\n\r\nThe accuracy of the non-quantized model did not decrease.", "comments": ["> But the accuracy of the results is very low.\r\n> The accuracy of the non-quantized model did not decrease.\r\n\r\nCan you be more specific, or quantify this? Have you run the model against ImageNet to get a TopK result? Note that there will generally be *some* accuracy degradation, but it shouldn't be significant.", "@jdduke \r\n>Can you be more specific, or quantify this? Have you run the model against ImageNet to get a TopK result? Note that there will generally be some accuracy degradation, but it shouldn't be significant.\r\n\r\n - I used tensorflow 1.14\r\n - Windows 10\r\n\r\nI used my dataset for the training model. On the PC, the accuracy rate is 98 percent. But using the quantization model on Android is only 10 percent accurate.\r\n\r\nI use these code `https://github.com/tensorflow/models/tree/master/research/slim` train mobilenet V2 model.\r\n\r\n - step 1: `python3 train_image_classifier.py --quantize=True`\r\n - step 2: `python3 export_inference_graph.py`\r\n - step 3: freeze graph\r\n```\r\nbazel build tensorflow/python/tools:freeze_graph\r\n\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n  --input_graph=/mnt/d/tmp/mobilenet_v2/mobilenet_v2_inf_graph.pb \\\r\n  --input_checkpoint=/mnt/d/tmp/mobilenet_v2/model.ckpt-90000 \\\r\n  --input_binary=true \\\r\n  --output_graph=/mnt/d/tmp/mobilenet_v2/mobilenet_v2.pb \\\r\n  --output_node_names=MobilenetV2/Predictions/Reshape_1\r\n```\r\n\r\n - step 4: transformation quantization model\r\n```\r\nbazel build tensorflow/lite/toco:toco\r\n\r\nbazel-bin/tensorflow/lite/toco/toco \\\r\n  --input_file=/mnt/d/tmp/mobilenet_v2.pb \\\r\n  --output_file=/mnt/d/tmp/mobilenet_v2.tflite \\\r\n  --inference_type=QUANTIZED_UINT8 \\                                                                    \r\n  --input_arrays=input \\\r\n  --output_arrays=MobilenetV2/Predictions/Reshape_1 \\\r\n  --input_shapes=1,224,224,3 \\\r\n  --mean_values=128 \\\r\n  --std_values=128 \\\r\n  --change_concat_input_ranges=false \\\r\n  --allow_custom_ops\r\n```\r\n\r\n - step 5: use in Android\r\n```java\r\nByteBuffer imgData = ByteBuffer.allocateDirect(ddims[2] * ddims[3] * 3 * 1);\r\n\r\nprivate ByteBuffer getScaledMatrix(Bitmap bitmap) {\r\n    imgData.rewind();\r\n    bitmap.getPixels(intValues, 0, ddims[2], 0, 0, ddims[2], ddims[3]);\r\n    for (int i = 0; i < ddims[2]; ++i) {\r\n        for (int j = 0; j < ddims[3]; ++j) {\r\n            int pixelValue = intValues[i * ddims[2] + j];\r\n            imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n            imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n            imgData.put((byte) (pixelValue & 0xFF));\r\n        }\r\n    }\r\n    if (bitmap.isRecycled()) {\r\n        bitmap.recycle();\r\n    }\r\n    return imgData;\r\n}\r\n\r\nByteBuffer inputData = getScaledMatrix(b);\r\nbyte[][] labelProbArray = new byte[1][NUM_CLASS];\r\ntflite.run(inputData, labelProbArray);\r\n```", "Use Tensorflow2.x train model.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34987\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34987\">No</a>\n"]}, {"number": 34986, "title": "how to understand \" Do not call this op with the   output of `softmax`, as it will produce incorrect results. \"", "body": "So why?\r\n[tf.nn.softmax_cross_entropy_with_logits_v2](https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/ops/nn_ops.py#L3108-L3224)", "comments": ["@showintime \r\n\r\nCan you please go through [link](https://stackoverflow.com/questions/34240703/what-is-logits-softmax-and-softmax-cross-entropy-with-logits) and see if it helps you. Thanks!", "thanks a lot,and i learned it.\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:&nbsp;\"ravikyram\"<notifications@github.com&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2019\u5e7412\u670811\u65e5(\u661f\u671f\u4e09) \u4e0b\u53482:23\r\n\u6536\u4ef6\u4eba:&nbsp;\"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;;\r\n\u6284\u9001:&nbsp;\"\u8bb0\u5f97\u5237\u901a\u8bc6\u7406\u8bba\u9009\u4fee\"<974200701@qq.com&gt;;\"Mention\"<mention@noreply.github.com&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] how to understand \" Do not call this op with the   output of `softmax`, as it will produce incorrect results. \" (#34986)\r\n\r\n\r\n\r\n\r\n@showintime\r\n \r\nCan you please go through link and see if it helps you. Thanks!\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "@showintime \r\n\r\nI am closing this thread as it solves your question. Thanks!"]}, {"number": 34985, "title": "NFC - minor spelling tweaks under python directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/python` directory.   \r\nfollow-on of #34958", "comments": ["cc @mihaimaruseac", "I will try to create multiple PRs for `lite`, `core`, and others directories.", "@kiszk: I don't think that'll be necessary. We can review this as one bulk PR.", "@kiszk Can you please resolve conflicts? Thanks!", "Sure, I will address the conflict.", "@jaingaurav I am afraid that one future big PR (currently more than 400 files) may increase the possibility of the conflict. What do you think?", "@gbaned I have just resolved conflict", "> @jaingaurav I am afraid that one future big PR (currently more than 400 files) may increase the possibility of the conflict. What do you think?\r\n\r\nWhatever you think is best. Either way we can make sure we get all the necessary owner approvals.", "@jaingaurav I have just created three PRs (#35285, #35286, and #35287). They are all on my side now.", "El 10 dic. 2019 07:05, \"Kazuaki Ishizaki\" <notifications@github.com>\nescribi\u00f3:\n>\n> This PR addresses minor spelling tweaks under tensorflow/python directory.\n> follow-on of #34958\n>\n> ________________________________\n> You can view, comment on, or merge this pull request online at:\n>\n>   https://github.com/tensorflow/tensorflow/pull/34985\n>\n> Commit Summary\n> minor spelling tweaks\n> File Changes\n> M tensorflow/python/autograph/converters/conditional_expressions.py (2)\n> M tensorflow/python/autograph/converters/logical_expressions.py (2)\n> M tensorflow/python/autograph/converters/return_statements.py (2)\n> M tensorflow/python/autograph/converters/return_statements_test.py (2)\n> M tensorflow/python/autograph/core/converter.py (4)\n> M tensorflow/python/autograph/core/function_wrappers.py (2)\n> M tensorflow/python/autograph/core/function_wrappers_test.py (2)\n> M tensorflow/python/autograph/core/naming.py (2)\n> M tensorflow/python/autograph/impl/api.py (2)\n> M tensorflow/python/autograph/impl/api_test.py (2)\n> M tensorflow/python/autograph/impl/conversion.py (4)\n> M tensorflow/python/autograph/operators/control_flow.py (6)\n> M tensorflow/python/autograph/operators/py_builtins.py (6)\n> M tensorflow/python/autograph/pyct/cfg.py (2)\n> M tensorflow/python/autograph/pyct/common_transformers/anf_test.py (4)\n> M tensorflow/python/autograph/pyct/origin_info.py (2)\n> M tensorflow/python/autograph/pyct/parser.py (2)\n> M tensorflow/python/autograph/pyct/static_analysis/activity.py (2)\n> M\ntensorflow/python/autograph/pyct/static_analysis/reaching_definitions.py (2)\n> M tensorflow/python/autograph/pyct/transformer.py (2)\n> M tensorflow/python/client/virtual_gpu_test.py (2)\n> M tensorflow/python/compat/compat.py (2)\n> M tensorflow/python/compat/compat_test.py (2)\n> M tensorflow/python/compat/disable_v2_behavior_test.py (2)\n> M tensorflow/python/compiler/tensorrt/test/biasadd_matmul_test.py (2)\n> M tensorflow/python/compiler/tensorrt/test/dynamic_input_shapes_test.py\n(2)\n> M tensorflow/python/compiler/tensorrt/test/identity_output_test.py (2)\n> M tensorflow/python/compiler/tensorrt/test/int32_test.py (2)\n> M tensorflow/python/compiler/tensorrt/trt_convert.py (4)\n> M tensorflow/python/compiler/tensorrt/trt_convert_test.py (2)\n> M tensorflow/python/data/benchmarks/map_benchmark.py (2)\n> M\ntensorflow/python/data/experimental/kernel_tests/bucket_by_sequence_length_test.py\n(2)\n> M tensorflow/python/data/experimental/ops/readers.py (2)\n> M tensorflow/python/data/kernel_tests/from_generator_test.py (2)\n> M tensorflow/python/data/ops/dataset_ops.py (6)\n> M tensorflow/python/data/util/sparse.py (2)\n> M tensorflow/python/data/util/structure_test.py (2)\n> M tensorflow/python/debug/cli/curses_ui.py (2)\n> M tensorflow/python/debug/cli/debugger_cli_common.py (2)\n> M tensorflow/python/debug/cli/debugger_cli_common_test.py (8)\n> M tensorflow/python/debug/cli/evaluator.py (2)\n> M tensorflow/python/debug/cli/profile_analyzer_cli_test.py (4)\n> M tensorflow/python/debug/cli/tensor_format_test.py (2)\n> M tensorflow/python/debug/lib/check_numerics_callback.py (2)\n> M tensorflow/python/debug/lib/check_numerics_callback_test.py (6)\n> M tensorflow/python/debug/lib/debug_data.py (2)\n> M tensorflow/python/debug/lib/debug_gradients.py (6)\n> M tensorflow/python/debug/lib/debug_graph_reconstruction_test.py (2)\n> M tensorflow/python/debug/lib/debug_v2_ops_test.py (2)\n> M tensorflow/python/debug/lib/dumping_callback.py (2)\n> M tensorflow/python/debug/lib/dumping_callback_test.py (2)\n> M tensorflow/python/debug/lib/dumping_callback_test_lib.py (4)\n> M tensorflow/python/debug/lib/session_debug_grpc_test.py (4)\n> M tensorflow/python/debug/lib/source_remote.py (2)\n> M tensorflow/python/debug/lib/source_utils.py (2)\n> M tensorflow/python/debug/lib/source_utils_test.py (2)\n> M tensorflow/python/debug/wrappers/dumping_wrapper.py (2)\n> M tensorflow/python/debug/wrappers/framework.py (4)\n> M tensorflow/python/debug/wrappers/local_cli_wrapper.py (2)\n> M tensorflow/python/debug/wrappers/local_cli_wrapper_test.py (2)\n> M tensorflow/python/distribute/all_reduce.py (2)\n> M tensorflow/python/distribute/cluster_resolver/cloud_tpu_client.py (2)\n> M tensorflow/python/distribute/cluster_resolver/slurm_cluster_resolver.py\n(4)\n> M tensorflow/python/distribute/collective_all_reduce_strategy.py (4)\n> M tensorflow/python/distribute/combinations.py (4)\n> M tensorflow/python/distribute/cross_device_ops.py (2)\n> M tensorflow/python/distribute/cross_device_utils.py (2)\n> M tensorflow/python/distribute/device_util.py (2)\n> M tensorflow/python/distribute/distribute_coordinator.py (8)\n> M tensorflow/python/distribute/distribute_coordinator_test.py (4)\n> M tensorflow/python/distribute/distribute_lib.py (2)\n> M tensorflow/python/distribute/estimator_training.py (2)\n> M tensorflow/python/distribute/input_lib.py (2)\n> M tensorflow/python/distribute/mirrored_strategy.py (2)\n> M tensorflow/python/distribute/mirrored_strategy_test.py (2)\n> M tensorflow/python/distribute/model_collection/model_collection_base.py\n(2)\n> M tensorflow/python/distribute/model_collection/simple_models.py (2)\n> M tensorflow/python/distribute/parameter_server_strategy.py (2)\n> M tensorflow/python/distribute/parameter_server_strategy_test.py (4)\n> M tensorflow/python/distribute/reduce_util.py (2)\n> M tensorflow/python/distribute/saved_model_test_base.py (2)\n> M tensorflow/python/distribute/values_test.py (2)\n> M tensorflow/python/eager/benchmarks_test.py (2)\n> M tensorflow/python/eager/def_function.py (6)\n> M tensorflow/python/eager/forwardprop_test.py (2)\n> M tensorflow/python/eager/forwardprop_util.py (2)\n> M tensorflow/python/eager/function.py (16)\n> M tensorflow/python/eager/function_test.py (2)\n> M tensorflow/python/eager/pywrap_tensor.cc (8)\n> M tensorflow/python/eager/pywrap_tfe.h (4)\n> M tensorflow/python/eager/pywrap_tfe_src.cc (2)\n> M tensorflow/python/eager/tape.py (2)\n> M tensorflow/python/eager/wrap_function.py (6)\n> M tensorflow/python/feature_column/feature_column_test.py (6)\n> M tensorflow/python/feature_column/feature_column_v2.py (2)\n> M tensorflow/python/feature_column/feature_column_v2_test.py (6)\n> M tensorflow/python/feature_column/serialization.py (2)\n> M tensorflow/python/framework/config_test.py (2)\n> M tensorflow/python/framework/convert_to_constants.py (2)\n> M tensorflow/python/framework/device_spec.py (4)\n> M tensorflow/python/framework/func_graph.py (4)\n> M tensorflow/python/framework/function.py (2)\n> M tensorflow/python/framework/function_def_to_graph.py (2)\n> M tensorflow/python/framework/graph_util_impl.py (2)\n> M tensorflow/python/framework/meta_graph.py (2)\n> M tensorflow/python/framework/op_callbacks.py (2)\n> M tensorflow/python/framework/op_callbacks_test.py (8)\n> M tensorflow/python/framework/op_def_library_test.py (2)\n> M tensorflow/python/framework/ops.py (12)\n> M tensorflow/python/framework/random_seed.py (2)\n> M tensorflow/python/framework/smart_cond.py (2)\n> M tensorflow/python/framework/subscribe.py (2)\n> M tensorflow/python/framework/tensor_like.py (2)\n> M tensorflow/python/framework/tensor_shape.py (2)\n> M tensorflow/python/framework/tensor_shape_test.py (4)\n> M tensorflow/python/framework/tensor_util.py (2)\n> M tensorflow/python/framework/test_combinations.py (2)\n> M tensorflow/python/framework/test_util.py (8)\n> M tensorflow/python/framework/test_util_test.py (8)\n> M tensorflow/python/framework/type_spec.py (4)\n> M tensorflow/python/grappler/cost_analyzer.cc (4)\n> M tensorflow/python/grappler/item_test.py (2)\n> M tensorflow/python/keras/applications/applications_test.py (2)\n> M tensorflow/python/keras/backend_config_test.py (2)\n> M tensorflow/python/keras/distribute/distributed_training_utils.py (4)\n> M tensorflow/python/keras/distribute/keras_correctness_test_base.py (6)\n> M tensorflow/python/keras/distribute/multi_worker_callback_tf1_test.py (2)\n> M tensorflow/python/keras/distribute/multi_worker_fault_tolerance_test.py\n(8)\n> M tensorflow/python/keras/engine/base_layer.py (2)\n> M tensorflow/python/keras/engine/data_adapter.py (4)\n> M tensorflow/python/keras/engine/network.py (2)\n> M tensorflow/python/keras/engine/network_test.py (4)\n> M tensorflow/python/keras/engine/training.py (4)\n> M tensorflow/python/keras/engine/training_distributed.py (2)\n> M tensorflow/python/keras/engine/training_utils.py (4)\n> M tensorflow/python/keras/engine/training_v1.py (6)\n> M tensorflow/python/keras/estimator/__init__.py (8)\n> M tensorflow/python/keras/integration_test.py (2)\n> M tensorflow/python/keras/layers/normalization.py (4)\n> M tensorflow/python/keras/layers/preprocessing/image_preprocessing.py (2)\n> M tensorflow/python/keras/layers/preprocessing/text_vectorization.py (8)\n> M tensorflow/python/keras/layers/preprocessing/text_vectorization_test.py\n(2)\n> M tensorflow/python/keras/layers/recurrent.py (2)\n> M tensorflow/python/keras/layers/recurrent_test.py (4)\n> M tensorflow/python/keras/layers/recurrent_v2.py (2)\n> M tensorflow/python/keras/layers/rnn_cell_wrapper_v2_test.py (2)\n> M tensorflow/python/keras/layers/serialization_test.py (2)\n> M tensorflow/python/keras/metrics.py (4)\n> M tensorflow/python/keras/optimizer_v2/optimizer_v2.py (4)\n> M tensorflow/python/keras/optimizer_v2/optimizer_v2_test.py (2)\n> M tensorflow/python/keras/premade/wide_deep.py (2)\n> M tensorflow/python/keras/saving/hdf5_format_test.py (2)\n> M tensorflow/python/keras/saving/saved_model/save_impl.py (2)\n> M tensorflow/python/keras/saving/saved_model/saved_model_test.py (2)\n> M tensorflow/python/keras/saving/saved_model/serialized_attributes.py (4)\n> M tensorflow/python/keras/utils/conv_utils.py (2)\n> M tensorflow/python/keras/utils/io_utils_test.py (2)\n> M tensorflow/python/keras/utils/metrics_utils.py (2)\n> M tensorflow/python/kernel_tests/bias_op_deterministic_test.py (2)\n> M tensorflow/python/kernel_tests/boosted_trees/stats_ops_test.py (2)\n> M tensorflow/python/kernel_tests/clip_ops_test.py (4)\n> M tensorflow/python/kernel_tests/critical_section_test.py (2)\n> M tensorflow/python/kernel_tests/distributions/special_math_test.py (4)\n> M tensorflow/python/kernel_tests/eig_op_test.py (2)\n> M tensorflow/python/kernel_tests/einsum_op_test.py (2)\n> M tensorflow/python/kernel_tests/fractional_max_pool_op_test.py (4)\n> M tensorflow/python/kernel_tests/linalg/linear_operator_diag_test.py (2)\n> M\ntensorflow/python/kernel_tests/linalg/linear_operator_full_matrix_test.py\n(2)\n> M\ntensorflow/python/kernel_tests/linalg/linear_operator_low_rank_update_test.py\n(2)\n> M\ntensorflow/python/kernel_tests/linalg/linear_operator_lower_triangular_test.py\n(2)\n> M tensorflow/python/kernel_tests/linalg_grad_test.py (4)\n> M tensorflow/python/kernel_tests/metrics_test.py (4)\n> M tensorflow/python/kernel_tests/numerics_test.py (6)\n> M tensorflow/python/kernel_tests/pooling_ops_test.py (4)\n> M tensorflow/python/kernel_tests/py_func_test.py (2)\n> M tensorflow/python/kernel_tests/rnn_test.py (2)\n> M tensorflow/python/kernel_tests/scatter_nd_ops_test.py (2)\n> M tensorflow/python/kernel_tests/segment_reduction_ops_test.py (2)\n> M tensorflow/python/kernel_tests/self_adjoint_eig_op_test.py (2)\n> M tensorflow/python/kernel_tests/svd_op_test.py (2)\n> M tensorflow/python/kernel_tests/template_test.py (2)\n> M tensorflow/python/kernel_tests/while_v2_test.py (2)\n> M tensorflow/python/layers/base_test.py (2)\n> M tensorflow/python/layers/utils.py (4)\n> M tensorflow/python/lib/core/py_func.cc (2)\n> M tensorflow/python/lib/core/pybind11_lib.h (2)\n> M tensorflow/python/ops/array_grad.py (4)\n> M tensorflow/python/ops/clip_ops.py (4)\n> M tensorflow/python/ops/clip_ops_test.py (6)\n> M tensorflow/python/ops/collective_ops.py (2)\n> M tensorflow/python/ops/control_flow_ops.py (2)\n> M tensorflow/python/ops/control_flow_util.py (2)\n> M tensorflow/python/ops/control_flow_util_v2.py (2)\n> M tensorflow/python/ops/control_flow_v2_toggles.py (2)\n> M tensorflow/python/ops/ctc_ops.py (2)\n> M tensorflow/python/ops/data_flow_ops.py (2)\n> M tensorflow/python/ops/distributions/bijector_impl.py (2)\n> M tensorflow/python/ops/distributions/distribution.py (4)\n> M tensorflow/python/ops/distributions/special_math.py (2)\n> M tensorflow/python/ops/embedding_ops.py (4)\n> M tensorflow/python/ops/image_grad.py (6)\n> M tensorflow/python/ops/image_ops_impl.py (2)\n> M tensorflow/python/ops/linalg/linear_operator_circulant.py (2)\n> M tensorflow/python/ops/linalg/linear_operator_test_util.py (2)\n> M tensorflow/python/ops/linalg/linear_operator_util.py (2)\n> M tensorflow/python/ops/linalg_grad.py (2)\n> M tensorflow/python/ops/linalg_ops.py (2)\n> M tensorflow/python/ops/logging_ops.py (2)\n> M tensorflow/python/ops/lookup_ops.py (4)\n> M tensorflow/python/ops/math_ops.py (2)\n> M tensorflow/python/ops/metrics_impl.py (6)\n> M tensorflow/python/ops/nn_impl.py (4)\n> M tensorflow/python/ops/nn_ops.py (6)\n> M tensorflow/python/ops/nn_test.py (10)\n> M tensorflow/python/ops/op_selector_test.py (2)\n> M tensorflow/python/ops/parallel_for/gradients.py (2)\n> M tensorflow/python/ops/parallel_for/pfor.py (8)\n> M tensorflow/python/ops/ragged/ragged_concat_op_test.py (2)\n> M tensorflow/python/ops/ragged/ragged_concat_ops.py (2)\n> M tensorflow/python/ops/ragged/ragged_config.py (2)\n> M tensorflow/python/ops/ragged/ragged_conversion_ops.py (2)\n> M tensorflow/python/ops/ragged/ragged_getitem.py (2)\n> M tensorflow/python/ops/ragged/ragged_tensor.py (2)\n> M tensorflow/python/ops/ragged/ragged_tensor_shape.py (4)\n> M tensorflow/python/ops/ragged/ragged_tensor_test.py (2)\n> M tensorflow/python/ops/ragged/ragged_to_tensor_op_test.py (4)\n> M tensorflow/python/ops/random_ops.py (4)\n> M tensorflow/python/ops/rnn_cell_wrapper_impl.py (4)\n> M tensorflow/python/ops/script_ops.py (4)\n> M tensorflow/python/ops/special_math_ops_test.py (2)\n> M tensorflow/python/ops/stateful_random_ops.py (2)\n> M tensorflow/python/ops/structured/structured_tensor.py (2)\n> M tensorflow/python/ops/summary_ops_v2.py (2)\n> M tensorflow/python/ops/template.py (2)\n> M tensorflow/python/ops/tensor_array_grad.py (2)\n> M tensorflow/python/ops/variables.py (2)\n> M tensorflow/python/ops/while_v2.py (2)\n> M tensorflow/python/ops/while_v2_indexed_slices_rewriter.py (8)\n> M tensorflow/python/profiler/internal/flops_registry.py (4)\n> M tensorflow/python/profiler/model_analyzer.py (2)\n> M tensorflow/python/profiler/model_analyzer_test.py (2)\n> M tensorflow/python/saved_model/builder_impl.py (2)\n> M tensorflow/python/saved_model/function_deserialization.py (6)\n> M tensorflow/python/saved_model/load_test.py (2)\n> M tensorflow/python/saved_model/model_utils/export_test.py (4)\n> M tensorflow/python/saved_model/utils_impl.py (4)\n> M tensorflow/python/tools/freeze_graph.py (4)\n> M tensorflow/python/tools/saved_model_cli.py (2)\n> M tensorflow/python/tpu/bfloat16_test.py (2)\n> M tensorflow/python/tpu/feature_column.py (2)\n> M tensorflow/python/tpu/ops/tpu_ops.py (2)\n> M tensorflow/python/tpu/session_support.py (2)\n> M tensorflow/python/tpu/tensor_tracer.py (4)\n> M tensorflow/python/tpu/tensor_tracer_flags.py (2)\n> M tensorflow/python/tpu/tpu.py (2)\n> M tensorflow/python/tpu/tpu_embedding.py (8)\n> M tensorflow/python/tpu/tpu_embedding_gradient.py (4)\n> M tensorflow/python/tpu/tpu_system_metadata.py (2)\n> M tensorflow/python/training/checkpoint_management.py (2)\n> M tensorflow/python/training/experimental/loss_scale_optimizer.py (12)\n> M tensorflow/python/training/monitored_session_test.py (4)\n> M tensorflow/python/training/optimizer_test.py (2)\n> M tensorflow/python/training/saver.py (2)\n> M tensorflow/python/training/saver_test.py (2)\n> M tensorflow/python/training/sync_replicas_optimizer.py (4)\n> M tensorflow/python/training/sync_replicas_optimizer_test.py (2)\n> M tensorflow/python/training/tracking/data_structures.py (4)\n> M tensorflow/python/training/tracking/tracking_test.py (2)\n> M tensorflow/python/training/tracking/util_with_v1_optimizers_test.py (2)\n> M tensorflow/python/util/example_parser_configuration.py (2)\n> M tensorflow/python/util/tf_inspect.py (2)\n> M tensorflow/python/util/util.h (2)\n> Patch Links:\n> https://github.com/tensorflow/tensorflow/pull/34985.patch\n> https://github.com/tensorflow/tensorflow/pull/34985.diff\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "@kiszk Can you please resolve conflicts? Thanks!", "@gbaned Thank you for pinging me. I have just resolved the conflicts.", "@kiszk Can you please resolve conflicts? Thanks!", "@gbaned Sure, done again.", "@kiszk Can you please resolve conflicts? Thanks!", "@gbaned Thank you for pinging me, done.", "@gbaned Just resolved the conflict, could you handle this?", "@kiszk \r\n\r\nI see still some conflicts to be resolved.Can you please resolve conflicts? Thanks!", "@ravikyram Thank you. Just resolved the conflict, could you review this?", "Resolved conflict again.", "@kiszk: Seems like there are still some conflicts. Are you seeing any on your end?", "@jaingaurav. There is no conflict on github site. Also no conflict on my end.", "Manually imported it", "There are some conflicts to solve, please.", "@mihaimaruseac I see. Now, I saw a new conflict. Resolved, thank you for pinging me.", "There are new conflicts, can you please rebase on master again?", "@mihaimaruseac Just resolved conflicts, again"]}, {"number": 34984, "title": "DLL Load Failed in tensorflow-gpu", "body": "Whenever i try to run my python script, i keep getting this error. My environment specifications are:\r\n\r\nPython: 3.7.4\r\nCUDA: 9.0\r\nTensorflow-gpu: 1.14.0\r\n\r\nError:\r\n\r\nTraceback (most recent call last):\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"demo_cli.py\", line 3, in <module>\r\n    from synthesizer.inference import Synthesizer\r\n  File \"C:\\Users\\DELL\\Desktop\\Project\\Voice_Cloning\\Real-Time-Voice-Cloning-master\\synthesizer\\inference.py\", line 1, in <module>\r\n    from synthesizer.tacotron2 import Tacotron2\r\n  File \"C:\\Users\\DELL\\Desktop\\Project\\Voice_Cloning\\Real-Time-Voice-Cloning-master\\synthesizer\\tacotron2.py\", line 3, in <module>\r\n    from synthesizer.models import create_model\r\n  File \"C:\\Users\\DELL\\Desktop\\Project\\Voice_Cloning\\Real-Time-Voice-Cloning-master\\synthesizer\\models\\__init__.py\", line 1, in <module>\r\n    from .tacotron import Tacotron\r\n  File \"C:\\Users\\DELL\\Desktop\\Project\\Voice_Cloning\\Real-Time-Voice-Cloning-master\\synthesizer\\models\\tacotron.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"O:\\WinPython\\WPy64-3741\\python-3.7.4.amd64\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["@monilshah98, \r\nDoes your CPU support AVX instruction sets?\r\nCheck, MSVC 2015 redistributable update 3. It is listed here under windows:\r\nhttps://www.tensorflow.org/install/pip. \r\nAnd also make sure you have installed cuDNN 7.4. Thanks!", "@monilshah98 Please use cuda 10.0 for TF 1.14.0\r\nSee https://www.tensorflow.org/install/gpu#software_requirements\r\nAlso add cuda, cudnn, cupti variables to your environment path.\r\nhttps://www.tensorflow.org/install/gpu#windows_setup", "@monilshah98, Is this still an issue!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34984\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34984\">No</a>\n"]}, {"number": 34983, "title": "ValueError: tf.function-decorated function tried to create variables on non-first call", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version:  3.6.7\r\n- CUDA/cuDNN version: 7.6.0\r\n- GPU model and memory: 1660Ti\r\n\r\n**Describe the current behavior**\r\nError raised when I tried to train a model with function decorated with @tf.function by two different optimizer sequentially\r\nValueError: tf.function-decorated function tried to create variables on non-first call\r\n\r\n**Describe the expected behavior**\r\nThe model can be trained by two different optimizer sequentially\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n    import tensorflow as tf\r\n    from tensorflow import keras\r\n    from tensorflow.keras.layers import Flatten, Dense\r\n    from tensorflow.keras import Model\r\n    \r\n    try: \r\n        gpus= tf.config.experimental.list_physical_devices('GPU')\r\n        tf.config.experimental.set_memory_growth(gpus[0], True)\r\n    except:\r\n        print('Failed to configure GPU')\r\n    \r\n    cifar10 = tf.keras.datasets.cifar10\r\n    \r\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n    x_train = x_train / 255.0\r\n    \r\n    \r\n    class MyModel(Model):\r\n        def __init__(self):\r\n            super(MyModel, self).__init__()\r\n            self.flatten = Flatten()\r\n            self.d1 = Dense(20, activation='relu')\r\n            self.d2 = Dense(10, activation='softmax')\r\n    \r\n        def call(self, x):\r\n            x = self.flatten(x)\r\n            x = self.d1(x)\r\n            return self.d2(x)\r\n    \r\n    \r\n    optimizer = keras.optimizers.Adam(0.001)\r\n    optimizer2 = keras.optimizers.SGD(0.01)\r\n    \r\n    model = MyModel()\r\n    \r\n    \r\n    @tf.function  # it will work correctly if this line is disabled but the training becomes very slow\r\n    def train_step(model, data, labels, optimizer):\r\n        with tf.GradientTape() as tape:\r\n            pred = model(data)\r\n            loss = keras.losses.SparseCategoricalCrossentropy()(labels, pred)\r\n    \r\n        grads = tape.gradient(loss, model.trainable_variables)\r\n    \r\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n    \r\n    \r\n    train_step(model, x_train[:10], y_train[:10], optimizer)  # this works\r\n    train_step(model, x_train[:10], y_train[:10], optimizer2)  # it can't work\r\n\r\n\r\n**Other info / logs**\r\n\r\n    ---------------------------------------------------------------------------\r\n    ValueError                                Traceback (most recent call last)\r\n    <ipython-input-7-f4e941ca2116> in <module>\r\n          1 train_step(model, x_train[:10], y_train[:10], optimizer)\r\n    ----> 2 train_step(model, x_train[:10], y_train[:10], optimizer2)\r\n    \r\n    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n        455 \r\n        456     tracing_count = self._get_tracing_count()\r\n    --> 457     result = self._call(*args, **kwds)\r\n        458     if tracing_count == self._get_tracing_count():\r\n        459       self._call_counter.called_without_tracing()\r\n    \r\n    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n        485       # In this case we have created variables on the first call, so we run the\r\n        486       # defunned version which is guaranteed to never create variables.\r\n    --> 487       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n        488     elif self._stateful_fn is not None:\r\n        489       # Release the lock early so that multiple threads can perform the call\r\n    \r\n    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n       1820   def __call__(self, *args, **kwargs):\r\n       1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n    -> 1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n       1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n       1824 \r\n    \r\n    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n       2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n       2149         if graph_function is None:\r\n    -> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n       2151           self._function_cache.primary[cache_key] = graph_function\r\n       2152         return graph_function, args, kwargs\r\n    \r\n    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n       2039             arg_names=arg_names,\r\n       2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n    -> 2041             capture_by_value=self._capture_by_value),\r\n       2042         self._function_attributes,\r\n       2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n    \r\n    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n        913                                           converted_func)\r\n        914 \r\n    --> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n        916 \r\n        917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n    \r\n    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n        356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n        357         # the function a weak reference to itself to avoid a reference cycle.\r\n    --> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n        359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n        360 \r\n    \r\n    ~/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n        903           except Exception as e:  # pylint:disable=broad-except\r\n        904             if hasattr(e, \"ag_error_metadata\"):\r\n    --> 905               raise e.ag_error_metadata.to_exception(e)\r\n        906             else:\r\n        907               raise\r\n    \r\n    ValueError: in converted code:\r\n    \r\n        <ipython-input-2-db5119c2db0a>:27 train_step  *\r\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:433 apply_gradients\r\n            _ = self.iterations\r\n        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:541 __getattribute__\r\n            return super(OptimizerV2, self).__getattribute__(name)\r\n        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:655 iterations\r\n            aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\r\n        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:805 add_weight\r\n            aggregation=aggregation)\r\n        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py:744 _add_variable_with_custom_getter\r\n            **kwargs_for_getter)\r\n        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py:139 make_variable\r\n            shape=variable_shape if variable_shape else None)\r\n        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:258 __call__\r\n            return cls._variable_v1_call(*args, **kwargs)\r\n        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:219 _variable_v1_call\r\n            shape=shape)\r\n        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:65 getter\r\n            return captured_getter(captured_previous, **kwargs)\r\n        /home/wilson/venv_tf2.0/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py:413 invalid_creator_scope\r\n            \"tf.function-decorated function tried to create \"\r\n    \r\n        ValueError: tf.function-decorated function tried to create variables on non-first call.\r\n\r\nHow should I correct it if possible? Or it's still a bug?\r\nrelated issue  #27120\r\nThanks!", "comments": ["You should call the train_step model by wrapping it with another function and call it 2 number of time like mentioned in the related issue thread.\r\nhttps://github.com/tensorflow/tensorflow/issues/27120#issuecomment-540071844\r\nThis should solve your issue.", "@Athul8raj , thanks for replying. It did work. But I also wonder it's an official solution or just an workaround. It seems like that many people face the same issue but it's not documented in the tutorial. If it's a bug, will it be fixed so that I could write the codes in an easier way. I'm just a newbie to DL and making a difficult decision between TF2 and pytorch. Thanks", "I think it should be solved in the future releases of TF 2.0.x. Right now I think it's a workaround ,that's why that issue is still open. Alextp who is mentioned in that thread is a core Tensorflow developer, so I guess the fix should be released soon :)", "I have tried the latest version TF2.1.0rc today, but it still raises errors. Maybe we need to wait longer... XD", "Same issue here, 2.1-rc0 and 2.1-rc1", "Issue replicating for given code, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/88a01bec4ee8665666e104e9da270b0b/34983.ipynb).TThanks!", "@hgffly The reason for this error is mentioned in this [comment](https://github.com/tensorflow/tensorflow/issues/27120#issuecomment-500971183)\r\nAnd this exists in 2.1rc2 as well.\r\n\r\nAlso it is a duplicate of the issue #27120 \r\n\r\nIt will be a good idea if we track the issue at onc place. Can I close this issue @hgffly?\r\n", "> @hgffly The reason for this error is mentioned in this [comment](https://github.com/tensorflow/tensorflow/issues/27120#issuecomment-500971183)\r\n> And this exists in 2.1rc2 as well.\r\n> \r\n> Also it is a duplicate of the issue #27120\r\n> \r\n> It will be a good idea if we track the issue at onc place. Can I close this issue @hgffly?\r\nit's ok to close the issue if it's definitely an duplicate of issue #27120\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34983\">No</a>\n", "I fix this problem by adding `tf.config.experimental_run_functions_eagerly(True)` after import tensorflow.\r\n\r\n the following links may help:    \r\n- https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio.\r\n\r\n- https://www.tensorflow.org/guide/effective_tf2#use_tfconfigexperimental_run_functions_eagerly_when_debugging.   ", "> I fix this problem by adding `tf.config.experimental_run_functions_eagerly(True)` after import tensorflow.\r\n> \r\n> the following links may help:\r\n> \r\n> * https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio.\r\n> * https://www.tensorflow.org/guide/effective_tf2#use_tfconfigexperimental_run_functions_eagerly_when_debugging.\r\n\r\nTested v2.1 and fixes the issue ", "> I fix this problem by adding `tf.config.experimental_run_functions_eagerly(True)` after import tensorflow.\r\n> \r\n> the following links may help:\r\n> \r\n> * https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio.\r\n> * https://www.tensorflow.org/guide/effective_tf2#use_tfconfigexperimental_run_functions_eagerly_when_debugging.\r\n\r\nThis is equivalent to remove the `tf.function` decorator, this is unwanted, cf. hgffly's comment: \r\n`@tf.function  # it will work correctly if this line is disabled but the training becomes very slow`", "> > I fix this problem by adding `tf.config.experimental_run_functions_eagerly(True)` after import tensorflow.\r\n> > the following links may help:\r\n> > \r\n> > * https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio.\r\n> > * https://www.tensorflow.org/guide/effective_tf2#use_tfconfigexperimental_run_functions_eagerly_when_debugging.\r\n> \r\n> This is equivalent to remove the `tf.function` decorator, this is unwanted, cf. hgffly's comment:\r\n> `@tf.function # it will work correctly if this line is disabled but the training becomes very slow`\r\n\r\neagerly = True  equals  do not use @tf.function\r\n", "> I fix this problem by adding `tf.config.experimental_run_functions_eagerly(True)` after import tensorflow.\r\n> \r\n> the following links may help:\r\n> \r\n> * https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio.\r\n> * https://www.tensorflow.org/guide/effective_tf2#use_tfconfigexperimental_run_functions_eagerly_when_debugging.\r\n\r\nThis is great, it worked! Thanks.", "> I fix this problem by adding `tf.config.experimental_run_functions_eagerly(True)` after import tensorflow.\r\n> \r\n> the following links may help:\r\n> \r\n> * https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio.\r\n> * https://www.tensorflow.org/guide/effective_tf2#use_tfconfigexperimental_run_functions_eagerly_when_debugging.\r\n\r\nThank you. It's works well!", "Issue still exists in latest 2.3.1. \r\n\r\nRunning function eagerly is not a solution.", "If you don't want to run functions eagerly, add a wrapper, or remove the tf.function decorator, you may find the following a better suggestion:\r\n```\r\noptimizer = tf.keras.optimizers.Adam(0.001)\r\noptimizer2 = tf.keras.optimizers.SGD(0.01)\r\n\r\n@tf.function \r\ndef train_step_with_opt_1(model, data, labels):\r\n    with tf.GradientTape() as tape:\r\n        pred = model(data)\r\n        loss = tf.keras.losses.SparseCategoricalCrossentropy()(labels, pred)\r\n\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n@tf.function \r\ndef train_step_with_opt_2(model, data, labels):\r\n    with tf.GradientTape() as tape:\r\n        pred = model(data)\r\n        loss = tf.keras.losses.SparseCategoricalCrossentropy()(labels, pred)\r\n\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n\r\n    optimizer2.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\ntrain_step_with_opt_1(model, x_train[:10], y_train[:10]) \r\ntrain_step_with_opt_2(model, x_train[:10], y_train[:10]) \r\n```\r\nThe optimizer creates variables when `apply_gradients` is called for the first time on it. `tf.keras.optimizers.SGD` which is a \"stateless\" optimizer still creates variables (you can check by calling `optimizer2.variables()` before and after training, and you will see that there is a `iter` tf.Variable). Thus, you should create two tf.functions, one for each optimizer, if you would like to train your model with both optimizers.\r\n\r\nI hope this is helpful!\r\n"]}]