[{"number": 3588, "title": "Branch 128894163", "body": "", "comments": ["@tensorflow-jenkins test this please\n"]}, {"number": 3587, "title": "In my town people obsess about digital life.", "body": "In my town people **obsess** about digital life. Every passing conversation is couched in **software vernacular**. Enthusiasm for the unrealized potential of information automation is explained as an ever expanding frontier. Technology has brought us great fortune but even greater things in common. For every fan of Larry Ellison there are twenty aspiring Zuckerbergs. People who visualize technology not just as intellectual property, but as an expanding commons. While tech serfdom abounds in the libertarian haven of Silicon Valley there are those whose fantasy is a digital utopia. \n", "comments": ["Is this an issue with TensorFlow, or possibly an issue with Intellectual Property legislation, or are you just trying to rock on some Tech Hate?  \n\nTech-Hate is counter-productive, and speaking for the technologists, especially the open source software, hardware and culture technologists, we do everything we can to fight \"tech serfdom\" -- that's the entire purpose of the open software movement in general.  \n- Wanna solve some problems?  \n  - Change the government, and change closed tech organizations.  \n  - Do something!  Create!  Build!  Generate!  \n- Wanna give google some shit for being google?  \n  - Whine in a forum that isn't one of their really great collectively owned open source software projects-- it IS an expanding commons.  \n  - Complain about things that google actually isn't doing right, as opposed to being vague as hell in a shining example of \"google's pretty great.\"\n"]}, {"number": 3586, "title": "Bug involving tf.train.Saver and num_epochs parameter in tf.train.string_input_producer", "body": "I've run into a strange problem involving both `tf.train.Saver` and an input pipeline based on `tf.train.string_input_producer()`. It seems as though the `num_epochs` parameter to `tf.train.string_input_producer()` stops working as intended after loading a saved model.\n\nI'm currently using v0.8 but I suspect this is still an issue on v0.9 and above.\n### Environment info\n\nOperating System: Mac OSX (10.11.6)\nTensorflow version: 0.8\npip version: 8.1.1\n### Steps to reproduce\n\n1 - Create a new directory and add to it the files from the following gist: https://gist.github.com/6d730e1b3d331c4be34c9a57b0a39ccf . The `1.txt` and `2.txt` files simulate training data. The `training.py` file implements a simple input pipeline, taking in text files, and printing them line by line. Notice that the `num_epochs` parameter on line 28 is set to 1.\n\n2 - Run `$ python training.py test 1.txt`. The output should resemble the following: \n\n```\nThe multiplying villanies of nature\nDo swarm upon him--from the western isles\nAnd fortune, on his damned quarrel smiling,\nShow'd like a rebel's whore: but all's too weak:\nFor brave Macbeth--well he deserves that name--\nModel saved to ./model-test-0\nAs two spent swimmers, that do cling together\nDisdaining fortune, with his brandish'd steel,\nAnd fix'd his head upon our battlements.\nLike valour's minion carved out his passage\nWhich smoked with bloody execution,\nModel saved to ./model-test-1\nTill he unseam'd him from the nave to the chaps,\nAnd choke their art. The merciless Macdonwald--\nOf kerns and gallowglasses is supplied;\nWorthy to be a rebel, for to that\nDoubtful it stood;\nModel saved to ./model-test-2\nDone training!\nModel saved to ./model-test-3\n```\n\n3 - Run `$ python training.py test 2.txt`. The output should be:\n\n```\nDone training!\nModel saved to ./model-test-3\n```\n\n4 - Now, change the `num_epochs` parameter on line 28 from 1 to 2, and run `$ python training.py test 2.txt` again. In this case, we have something that resembles:\n\n```\nLetting 'I dare not' wait upon 'I would,'\nAs thou art in desire? Wouldst thou have that\nSuch I account thy love. Art thou afeard\nWhich thou esteem'st the ornament of life,\nAnd wakes it now, to look so green and pale\nModel saved to ./model-test-3\nAnd live a coward in thine own esteem,\nAt what it did so freely? From this time\nLike the poor cat i' the adage?\nWherein you dress'd yourself? hath it slept since?\nTo be the same in thine own act and valour\nModel saved to ./model-test-4\nDone training!\nModel saved to ./model-test-5\n```\n\n5 - To cleanup, run `$ rm checkpoint model-test-*`.\n\nThere are two problems: First, the model does not continue to \"train\" in step 3. Second, when the `num_epochs` parameter is modified and it does continue to train, it doesn't reproduce the input twice, as it should (step 4).\n\nPlaying around with this, I notice that the initial value of `num_epochs` works as expected, e.g. if at step 1, `num_epochs` is set to 3, three copies of the input file will be produced. Further, step 4 yields similar results when the new value for `num_epochs` is larger than the previous value.\n", "comments": ["I've just upgraded to Tensorflow v0.10.0 and this problem seems to be fixed. However, using `num_epochs` requires that you run `tf.initialize_local_variables()` in addition (!) to `tf.initialize_all_variables()`. See this gist https://gist.github.com/e45174fbf60a4df174af4a5d95a293f1 for an updated version of the dummy `training.py` file above. Commenting out line 55 and running `$ python training-2.py test 1.txt` gives an error:\n\n```\nE tensorflow/core/client/tensor_c_api.cc:485] Attempting to use uninitialized value input/input_producer/limit_epochs/epochs\n```\n\nCleaning up and leaving line 55 in fixes the issue.\n", "@vrv it seems as if the documentation of all_variables and local_variables could be improved. Could you verify that this behavior is expected, and assign someone to update the doc?\n", "Duplicate of https://github.com/tensorflow/tensorflow/issues/1045\n"]}, {"number": 3585, "title": "Bidirectional encoder for seq2seq in Tensorflow", "body": "All methods in `seq2seq.py` library supports encoder-decoder system with the input in forward sequence. How can we construct a bi-directional encoder-decoder using the libraries or else, much like the concept in bi-directional LSTM?\n", "comments": ["This kind of usage question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). Github issues are for TensorFlow bugs and installation problems. Thanks!\n", "@michaelisard: I did not receive any answer on Stackoverflow,  hence I decided to put a query over here. \n", "you'll need to implement your own encoder,\nfor decoder part, you could use tf provided ones : rnn_decoder or attention_decoder\nin tf0.9, they are implemented in \nlib/python2.7/site-packages/tensorflow/python/ops/seq2seq.py\nencoders already implemented in this file are good start points for implementing your own encoder. \n"]}, {"number": 3584, "title": "Removed unused imports from examples", "body": "Minor code cleanup.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3583, "title": "Dying Threads?", "body": "### Environment info\n\nOperating System: Ubuntu trusty\n\nInstalled version of CUDA and cuDNN: Cuda compilation tools, release 7.5, V7.5.17\n1. Which ~~pip~~ package you installed.  Docker gpu-devel image\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.9.0\n### Steps to reproduce\n1. Run inception model on training with inception_train.py\n   \n   bazel-bin/inception/(pick your data)_train \\\n     --train_dir=\"${TRAIN_DIR}\" \\\n     --data_dir=\"${DATA_DIR}\" \\\n     --pretrained_model_checkpoint_path=\"${PRETRAINED_DIR}\" \\\n     --fine_tune=True \\\n     --initial_learning_rate=0.01 \\\n     --input_queue_memory_factor=12 \\\n     --batch_size=64 \\\n     --max_steps=100000 \\\n     --num_epochs_per_decay=30 \\\n     --num_preprocess_threads=4 \\\n     --num_readers=4 \\\n     --log_device_placement=True\n### What have you tried?\n1. This is ironic, because I have been wrestling with the problem for a few days, and serendipitously came to a conclusion. I was training the model and testing it out, and always\n   after 1000~ steps the training slows to a crawl utilizing my GPU only every 10s - 15s and the CPU seems to be only utilizing 2 threads to 100% at this point for preprocessing , when i specified 4 & 4 readers, but all threads kick in right before/at the GPU utilization. I tried restarted and playing with the threads and readers and other things, no luck, it still slowed after 1000~ steps. I left it running for whatever reason and later started running a Tensorflow bazel build, which used significant CPU for some time and after/during the build my training magically picked up to the original speed at the beginning, and by picked up i mean went from 1-4 exp/s not steady to 15 exp/s steady, hmm thats strange, because it had been crawling for hours at 1-4 exp/s? \n### Logs or other output that would be helpful\n\nNone \n", "comments": ["Could you share a timeline trace? There are some tips for collecting timelines on [this StackOverflow thread](http://stackoverflow.com/questions/34293714/tensorflow-can-i-measure-the-execution-time-of-individual-operations).\n", "Now I get the following error when trying to run.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcupti.so. LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:\nF tensorflow/core/platform/default/gpu/cupti_wrapper.cc:58] Check failed: f != nullptr could not find cuptiActivityRegisterCallbacksin libcupti DSO; dlerror: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cuptiActivityRegisterCallbacks\n```\n", "Hi, it seems likely that your second and first issues are unrelated.  Let's tackle the second issue first, since it's a blocker.  What changed between when you had the slow runs, and when the library could no longer be found?  It seems that one must be careful when using gpus with docker to use the correct image.  There's some recent discussion in #808 \n", "Nothing changed, but the code for the time tracer if I comment out the tracer code it will run, but if I uncomment the tracer code I get the above error. \n", "OK, thanks for clarifying.  It would be nice to get op times somehow, but I'm less optimistic that will be strongly helpful.   Weird slowdown errors can often be caused by memory problems, especially if they gradually build up over time, like you report.   Can you watch use of virtual memory and swap on your machine while this is going on?  I wonder whether you might be using up too much vmem for file buffers or similar.  I know that bazel is a memory hog, and maybe when you run that it forces some kind of GC that cleans things up temporarily.  \n", "That sounds likely, I do recall massive vmem usage when I was running I will double check and report, back with my findings. I'm not sure what the issue is with the time tracer might be a docker issue, I will try again on a local install as well.\n", "Had the same issue, how I fixed it:\nI'm running zsh on ubuntu 16.04, with cuda 7.5.18 and cudNN 4.0.7\n\nrunning $LD_LIBRARY_PATH showed no path for the library, and so I had to manually add two lines to my .zshrc file:\n\n```\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"\nexport CUDA_HOME=/usr/local/cuda\n```\n\nIt seems strange they weren't there before, but once I did that and ran source `.zshrc`, the path updated and it worked\n", "Closing since @Pcres91 provided an answer.\n"]}, {"number": 3582, "title": "Loss not decreasing, GTX 1070, cuDNN 5.0 for RC 8.0, Cuda 8.0 RC", "body": "I've spent about 12 hours monkeying around before throwing in the towel here. I've checked the other similar threads on this and done the steps there. This includes #3068 and #3507. \n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \nCuda 8.0 RC\ncuDNN 5.0 RC\nOutput of libcudn*:\n/usr/local/cuda/lib64/libcudadevrt.a       /usr/local/cuda/lib64/libcudnn.so\n/usr/local/cuda/lib64/libcudart.so         /usr/local/cuda/lib64/libcudnn.so.5\n/usr/local/cuda/lib64/libcudart.so.8.0     /usr/local/cuda/lib64/libcudnn.so.5.0.5\n/usr/local/cuda/lib64/libcudart.so.8.0.27  /usr/local/cuda/lib64/libcudnn_static.a\n/usr/local/cuda/lib64/libcudart_static.a\n\nBuilt from source\n1. The commit hash: 5161e4c51b994b3feb93cdb851479c29a3450f31\n2. The output of `bazel version`:\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n### Notes\n\nI've built and thrown out, built and thrown out, cleaned the slate and reinstalled in about as many permutations as I can think. However, with training, the loss freezes. I simplified my network all the way down to a shallow autoencoder where there is only one image in the training set, and still the loss freezes. This worked completely fine using the CPU for similar code from a regular old pip distro. I'm using the MSE loss on a 224*224 image scaled between 0 and 1 to try to keep things as regular as possible.\n\nSomething odd to note is that no matter what my loss starts at, it always tends to 11893.53125. Ironically, if it starts below there, it goes up to it. Example of several 40 epoch runs: 11893.52930, 11893.53223, 11893.52930, 11893.52930, 11893.52930.\n\nMy theory was that it was outputting 0s -- assuming an even distribution, the summed MSE loss would be 224x224x.5^2 = 12544, which is pretty close to our 11893.52930. Since regular images won't have an even pixel distribution, this makes sense. In practice, after testing the network, it outputs all 1s.\n\nA side note is that when running the trainer test, the lambda value was not always exactly 2 -- every few iterations it was off by a fraction.\n\nI'm really scratching my head here -- any folks with combat experience see any telltale signs of a solution? Thank you very much in advance for your time.\n", "comments": ["I've got exactly the same symptom... \nKeras with Tensorflow backend... \nLoss not decrease\nAccuracy not increase... \nHave been several days....\n\nI'll try TF 0.10.0 tonight...\n", "Alright, sounds good. Let me know what you find. It's been almost a week for me.\n\nFrom what I've seen, I suspect the interaction between CUDA 8.0 and Tensorflow is our primary culprit. Which is sad because that's required for the 10 series cards.\n", "@TylerBalsam Just arriving at this thread now; can you explain why you think this isn't just a model convergence issue?  Nothing in the thread implies there is a bug in TensorFlow to my eyes.\n", "Closing automatically due to lack of recent response. Glancing at the thread history, this looks like it is more of a stack overflow question because it is concerning usage of TensorFlow rather than any type of bug. The stackoverflow community is generally experienced and helpful for these types of questions. Good luck!\n", "Is this issue resolved?\r\n\r\nI had my CNN network in tensorflow which was working fine when I used either CPU or GPU.\r\nAfter I updated my Cuda to Cuda 8.0.61, my code doesn't function properly with GPU and the loss value freezes after 2-3 epochs!! (doesn't decrease).\r\nWhen I use CPU, my code works fine and I don't have any problem and everything is fine!\r\n\r\nPlease let me know if there is any solution to this issue! \r\nThanks"]}, {"number": 3581, "title": "0.10.0rc release", "body": "Merge release branch back into master to get all the bugfixes.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "branch merge, CLA ok to ignore.\n"]}, {"number": 3580, "title": "Image retraining example: unable to run", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 LTS 64-bit\n\nInstalled version of CUDA and cuDNN: none (not using GPU)\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): fc9162975e52978d3af38549b570cc3cc5f0ab66\n2. The output of `bazel version`\n\n```\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n```\n### Steps to reproduce\n\n(instructions from [here](https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html))\n1. Run `cd ~`\n2. Run `curl -O http://download.tensorflow.org/example_images/flower_photos.tgz\ntar xzf flower_photos.tgz`\n3. Move to root of TensorFlow source directory\n4. Run `bazel build tensorflow/examples/image_retraining:retrain`\n5. Run `bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos`\n### What have you tried?\n1. The StackOverflow question [here](http://stackoverflow.com/questions/33622842/error-in-python-after-import-tensorflow-typeerror-init-got-an-unexpect) suggested uninstalling protobuf and reinstalling it with a version greater than or equal to 3.0.0a3. However, when I ran `pip uninstall protobuf`, I got this output:\n   `Can't uninstall 'protobuf'. No files were found to uninstall.`\n   I then tried to just run `pip install protobuf`, however, this was the output:\n\n```\nRequirement already satisfied (use --upgrade to upgrade): protobuf in /usr/local/lib/python2.7/dist-packages\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf)\nCleaning up...\n```\n\nI then tried to run `pip install 'protobuf>=3.0.0a3'` as suggested on the StackOverflow thread, but this was the output:\n\n```\nDownloading/unpacking protobuf>=3.0.0a3\n  Downloading protobuf-3.0.0-py2.py3-none-any.whl (342kB): 342kB downloaded\nRequirement already satisfied (use --upgrade to upgrade): six>=1.9 in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.0.0a3)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.0.0a3)\nInstalling collected packages: protobuf\n  Found existing installation: protobuf 2.6.1\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Can't roll back protobuf; was not uninstalled\nCleaning up...\nException:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 283, in run\n    requirement_set.install(install_options, global_options, root=options.root_path)\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 1436, in install\n    requirement.install(install_options, global_options, *args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 672, in install\n    self.move_wheel_files(self.source_dir, root=root)\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 902, in move_wheel_files\n    pycompile=self.pycompile,\n  File \"/usr/lib/python2.7/dist-packages/pip/wheel.py\", line 206, in move_wheel_files\n    clobber(source, lib_dir, True)\n  File \"/usr/lib/python2.7/dist-packages/pip/wheel.py\", line 193, in clobber\n    os.makedirs(destsubdir)\n  File \"/usr/lib/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/dist-packages/protobuf-3.0.0.dist-info'\n\nStoring debug log for failure in /home/me/.pip/pip.log\n```\n\nSome answers have suggested uninstalling and reinstalling TensorFlow along with protobuf, but I haven't tried that yet, since I wanted to avoid possibly messing anything up.\n### Logs or other output that would be helpful\n\n(output when I tried to run `bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos`)\n\n```\nTraceback (most recent call last):\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 78, in <module>\n    import tensorflow as tf\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 52, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>2. \n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n  File \"/home/me/tf_m/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB2\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3')\nTypeError: __init__() got an unexpected keyword argument 'syntax'\n```\n", "comments": ["Are you sure you have protobuf installed?\n\nThis output suggests you don't:\n1. `Can't uninstall 'protobuf'. No files were found to uninstall`\n2. `Permission denied: '/usr/local/lib/python2.7/dist-packages/protobuf-3.0.0.dist-info'` suggests it can't be installed for python 2.7\n\nYou can try to install with sudo.  Also, you can check what the output of `import google.protobuf` is in the python interpreter of the version of python you're trying to install to (seems like it's `2.7`).  There shouldn't be any output actually.  That should not throw an error when you have it installed.\n", "My guess would be that I actually do have protobuf installed, as suggested by this when I ran `pip install 'protobuf>=3.0.0a3'`:\n`Found existing installation: protobuf 2.6.1`\nbut neither uninstallation nor installation seem to work properly. \n\nAs per your suggestion, I tried both `sudo apt-get install protobuf` and `sudo pip install protobuf`. The output of sudo pip is as follows:\n\n```\nDownloading/unpacking protobuf\n  Downloading protobuf-2.6.1.tar.gz (188kB): 188kB downloaded\n  Running setup.py (path:/tmp/pip_build_root/protobuf/setup.py) egg_info for package protobuf\n\n    Installed /tmp/pip_build_root/protobuf/.eggs/google_apputils-0.4.2-py2.7.egg\n    Searching for pytz>=2010\n    Reading https://pypi.python.org/simple/pytz/\n    Best match: pytz 2016.6.1\n    Downloading https://pypi.python.org/packages/79/a7/74bf57f5b10bebb8e605dc277484bddc5559ad8cf5082c90fcd64ab5dcc6/pytz-2016.6.1-py2.7.egg#md5=3b1dd4a944e92e84a6510faf81abd655\n    Processing pytz-2016.6.1-py2.7.egg\n    Moving pytz-2016.6.1-py2.7.egg to /tmp/pip_build_root/protobuf/.eggs\n\n    Installed /tmp/pip_build_root/protobuf/.eggs/pytz-2016.6.1-py2.7.egg\n    Searching for python-gflags>=1.4\n    Reading https://pypi.python.org/simple/python-gflags/\n    Best match: python-gflags 3.0.5\n    Downloading https://pypi.python.org/packages/1f/01/3ca6527f51b7ff26abb635d4e7e2fa8413c7cf191564cc2c1e535f50dec7/python-gflags-3.0.5.tar.gz#md5=0e3a06cb99b9883bf6abacc489f31297\n    Processing python-gflags-3.0.5.tar.gz\n    Writing /tmp/easy_install-TKfuvj/python-gflags-3.0.5/setup.cfg\n    Running python-gflags-3.0.5/setup.py -q bdist_egg --dist-dir /tmp/easy_install-TKfuvj/python-gflags-3.0.5/egg-dist-tmp-rkay2G\n    zip_safe flag not set; analyzing archive contents...\n    Moving python_gflags-3.0.5-py2.7.egg to /tmp/pip_build_root/protobuf/.eggs\n\n    Installed /tmp/pip_build_root/protobuf/.eggs/python_gflags-3.0.5-py2.7.egg\n    Searching for python-dateutil>=1.4\n    Reading https://pypi.python.org/simple/python-dateutil/\n    Best match: python-dateutil 2.5.3\n    Downloading https://pypi.python.org/packages/b7/9f/ba2b6aaf27e74df59f31b77d1927d5b037cc79a89cda604071f93d289eaf/python-dateutil-2.5.3.zip#md5=52b3f339f41986c25c3a2247e722db17\n    Processing python-dateutil-2.5.3.zip\n    Writing /tmp/easy_install-O0nWd9/python-dateutil-2.5.3/setup.cfg\n    Running python-dateutil-2.5.3/setup.py -q bdist_egg --dist-dir /tmp/easy_install-O0nWd9/python-dateutil-2.5.3/egg-dist-tmp-TXb00f\n    warning: no previously-included files matching '__pycache__' found anywhere in distribution\n    warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n    Moving python_dateutil-2.5.3-py2.7.egg to /tmp/pip_build_root/protobuf/.eggs\n\n    Installed /tmp/pip_build_root/protobuf/.eggs/python_dateutil-2.5.3-py2.7.egg\n\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf)\nInstalling collected packages: protobuf\n  Running setup.py install for protobuf\n    Skipping installation of /usr/local/lib/python2.7/dist-packages/google/__init__.py (namespace package)\n\n    Installing /usr/local/lib/python2.7/dist-packages/protobuf-2.6.1-py2.7-nspkg.pth\n  Could not find .egg-info directory in install record for protobuf\nSuccessfully installed protobuf\nCleaning up...\n```\n\nIt says that protobuf was successfully installed, but I'm still getting the same error with `bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos`. It seems that the version installed is 2.6.1, which isn't the right version, but I can't upgrade it with `pip install 'protobuf>=3.0.0a3'` (it gives the same error as in my original post).\n\nAlso, `sudo apt-get install protobuf` gives this output:\n\n```\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nE: Unable to locate package protobuf\n```\n\n `import google.protobuf` doesn't have any output in my interpreter (IPython Notebook).\n", "Also, `import tensorflow` in my interpreter gives a similar error to when I tried to run the bazel command:\n\n```\nTypeError                                 Traceback (most recent call last)\n<ipython-input-1-c653d7eaed8c> in <module>()\n      4 from __future__ import print_function\n      5 import numpy as np\n----> 6 import tensorflow as tf\n      7 from six.moves import cPickle as pickle\n      8 from six.moves import range\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py in <module>()\n     21 from __future__ import print_function\n     22 \n---> 23 from tensorflow.python import *\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py in <module>()\n     47 \n     48 try:\n---> 49   from tensorflow.core.framework.graph_pb2 import *\n     50 except ImportError:\n     51   msg = \"\"\"%s\\n\\nError importing tensorflow.  Unless you are using bazel,\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py in <module>()\n     14 \n     15 \n---> 16 from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n     17 from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2\n     18 from tensorflow.core.framework import versions_pb2 as tensorflow_dot_core_dot_framework_dot_versions__pb2\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py in <module>()\n     14 \n     15 \n---> 16 from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n     17 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n     18 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/tensor_pb2.py in <module>()\n     14 \n     15 \n---> 16 from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n     17 from tensorflow.core.framework import types_pb2 as tensorflow_dot_core_dot_framework_dot_types__pb2\n     18 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py in <module>()\n     20   package='tensorflow',\n     21   syntax='proto3',\n---> 22   serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB/\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\x62\\x06proto3')\n     23 )\n     24 _sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTypeError: __init__() got an unexpected keyword argument 'syntax'\n```\n", "Try `sudo pip install 'protobuf>=3.0.0a3'`. In your comment it looks like you have successfully installed protobuf 2.6.1, which is not what you want.\n", "It looks like that did the trick, though I also had to run `sudo pip install mock`. Thanks for your help!\n"]}, {"number": 3579, "title": "Shape resizing of input in tf.batch_self_adjoint_eig", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Linux\n### Steps to reproduce\n1. Create any tensor (tf.Variable or tf.constant) containing batches of self adjoint matrices (call this B) and note its shape\n2. run something like alpha_U = tf.batch_self_adjoint_eig(B)\n3. look at B's shape again - the second to last dimension increments by 1\n### What have you tried?\n1. selecting all but the last element of the second to last dimension solves the problem, but is hacky\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n B\n<tf.Tensor 'Const_15:0' shape=(1, 3, 3) dtype=float32>\nsess.run(B)\narray([[[ 8.,  5.,  3.],\n        [ 5.,  2.,  4.],\n        [ 3.,  4.,  4.]]], dtype=float32)\nalpha_U = tf.batch_self_adjoint_eig(B)\n B\n<tf.Tensor 'Const_15:0' shape=(1, 4, 3) dtype=float32>\n sess.run(B)\narray([[[ 8.,  5.,  3.],\n        [ 5.,  2.,  4.],\n        [ 3.,  4.,  4.]]], dtype=float32)\n", "comments": ["@rmlarsen would you take a look? I don't see how the LinearAlgebra op would do this.\n", "This was intentional. The self_adjoint_eig and batch_self_adjoint_eig ops pack the eigenvalues and eigenvectors together in a single output, such that the first row contains the eigenvalues while remaining rows contain the eigenvectors. \n\nI recently updated these ops, such that they return the eigenvalues and vectors as separate outputs.\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py#L408\n\n I also added a wrappers (*self_adjoint_eigvals) that only returns the eigenvalues, which can be significantly faster. \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py#L449\n", "@rmlarsen It seems like you misunderstood what I was pointing out - I understand that the dimensions of the _output_ are (N+1) x N for both batch_self_adjoint_eig and self_adjoint_eig since it is packing both the eigenvectors and values together. I was pointing out that the batch_self_adjoint_eig function is currently modifying the shape of the _input_ tensor to become (N+1) x N for no reason at all. Take a look at the example above that I posted for more clarity, and thanks for your help. \n", "It seems that the issue lies in _BatchSelfAdjointEigShape(op) in linalg_ops.py. In line 95 (from https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/python/ops/linalg_ops.py), dlist[-2] += 1\nalso increments input_shape.dims (see line 94), causing the input dimensions to change. A deepcopy fixes the problem, but is again a hack solution.\n"]}, {"number": 3578, "title": "Branch 128859117", "body": "", "comments": []}, {"number": 3577, "title": "Branch 128853679", "body": "", "comments": []}, {"number": 3576, "title": "Hard-code libcuda version number to \"1\". Fixes #2865.", "body": "As per comments from nvidia-docker dev @3XX0, hardcoding \"1\" should be\nreasonably safe. The TF_CUDA_VERSION variable from the configure script\nis not appropriate here (it will contain something like \"7.0\" or \"7.5\",\nwhile the libcuda soname major version number should be \"1\").\n\n/CC @rdadolf  and @martinwicke \n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "@tensorflow-jenkins test this please\n\n(CLA is fine, this is a cherrypick)\n", "(Martin: does it matter that this is not cherrypicking from the master branch?)\n", "er, @martinwicke \n", "That's fine. A commit is a commit.\n", "ping?\n", "@tensorflow-jenkins test this please (not sure why tests failed, trying again)\n", "We have test failures :(\n", "I think there's been flakiness\nlet's try again\n\n@tensorflow-jenkins test this please\n", "Screw it, the GPU tests passed, let's just merge.\n"]}, {"number": 3575, "title": "Update Mac GPU setup instructions", "body": "", "comments": []}, {"number": 3574, "title": "Numerical differences between self.test_session and regular sessions?", "body": "I'm trying to use the test suite to create tests that verify various aspects of my model. Strangely, I'm getting numerical differences between the exact same model run on the exact same machine if I use `self.test_session` inside `tf.test.TestCase` versus if I just run the same piece of code in a regular session. Most models work fine, it's just that certain flavors that use dropout show big discrepancies, usually after 4 or 5 steps of training. Is this expected behavior? If so what's the cause of the discrepancy and can it be removed?\n", "comments": ["The code for test_session isn't too long: https://github.com/tensorflow/tensorflow/blob/73ced9d797056c7e67a06ed2098dd809d85ec44a/tensorflow/python/framework/test_util.py#L239 \n\nmostly involves making device placement a little bit easier in tests.  The only thing that comes to mind is that we turn off some of the compiler optimizations in tests, and so perhaps the numerical values are slightly different.  Try commenting out the corresponding line in test_session and let us know if that's the problem!\n", "Are you referring to this line?\n\nhttps://github.com/tensorflow/tensorflow/blob/73ced9d797056c7e67a06ed2098dd809d85ec44a/tensorflow/python/framework/test_util.py#L251\n\nWhere the optimizer options are set? I did comment it out but it had no effect.\n", "Okay, that's good to know.   The only other thing that the function does is deal with device placement, so it would be good to see the difference between your examples by finding a difference using log_device_placement=True.  That is, test_session() and tf.Session() with log_device_placement=True might be different in your use cases, and that might be highlighting either a bug, or numerical differences.\n\nCan you provide a small test case that we can use to validate?\n", "There's also \"enable_soft_placement\" -- in 0.9,\ntf.Session(tf.ConfigProto()) gave session without soft placement,\ntf.Session() gave session with soft placement\n\nOn Fri, Jul 29, 2016 at 4:01 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> Okay, that's good to know. The only other thing that the function does is\n> deal with device placement, so it would be good to see the difference\n> between your examples by finding a difference using\n> log_device_placement=True. That is, test_session() and tf.Session() with\n> log_device_placement=True might be different in your use cases, and that\n> might be highlighting either a bug, or numerical differences.\n> \n> Can you provide a small test case that we can use to validate?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3574#issuecomment-236314643,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHKTQNfmS_Cn3kmHl0AHUYY1jdt2Iks5qaoZjgaJpZM4JYmBH\n> .\n", "That's true, but code that runs with hard placement successfully should place ops no differently than code that runs with soft placement (soft placement only does something different if the ops are not implemented on the device the user specified).\n", "I'm running a Mac version and only with CPU support, so device placement can't be the reason, can it?\n\nThe model is big and hairy. I'll see about localizing it, but I don't think it'll be easy. Maybe this is helpful: if I set input dropout (in an LSTM) to some value, say 0.2, it fails, but then other nearby values, like 0.25, pass just fine (well up to 5 training steps).\n", "Hmm, then no :). Not really sure where differences can come from then. We\nwill need a repro\n\nOn Fri, Jul 29, 2016, 5:55 PM Mohammed AlQuraishi notifications@github.com\nwrote:\n\n> I'm running a Mac version and only with CPU support, so device placement\n> can't be the reason, can it?\n> \n> \u2014\n> You are receiving this because you commented.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3574#issuecomment-236328069,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAcTeTYMVc03YtnhImPOnC0bSqXQWvESks5qaqEFgaJpZM4JYmBH\n> .\n", "I figured out what was going on. I was setting `inter_op_parallelism_threads` and `intra_op_parallelism_threads` to specific values and that wasn't being reflected in `test_session`. I see now that it's possible to pass it an explicit `ConfigProto` object and that solves the problem.\n"]}, {"number": 3573, "title": "update os_setup for r0.10", "body": "mac directories are changed to be similar to linux and to avoid collision of cpu/gpu mac wheel files\n", "comments": ["@gunan @caisq \n"]}, {"number": 3572, "title": "Update documentation for r0.10 release.", "body": "", "comments": []}, {"number": 3571, "title": "Building tensorflow from source", "body": "Operating System: CentOS-7\nCUDA: 8.0\ncuDNN: 5.0\nbazel: 0.3.0-2016-07-29\ngcc: 4.8.5\n\nWhen I try to build tensorflow from source following the instructions in https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html, in particular, when executing:\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n\nI get the following error:\n\nERROR: /home/fjrodriguez/tensorflow/tensorflow/core/kernels/BUILD:854:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:crop_and_resize_op_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/crop_and_resize_op_gpu.cu.cc':\n\nI have tried many times, the error is always similar, undeclared inclusion(s) in rule xxxx, but the xxxx is changing depending on the execution.\n\nI've tried the solution proposed at https://github.com/tensorflow/tensorflow/issues/2413 without any luck\n\nThanks in advance!\n", "comments": ["CUDA 8 is not yet fully supported. Does it work with 7.5?\n", "I haven't tried since GTX1080 is not supported (I think) by CUDA 7.5\n", "Yes sorry I didn't realize you had GTV1080 I have seen several people who can't get that to work with 7.5. @martinwicke is there a required bazel version mismatch here?\n", "Want to second this, cuda 7.5 has issues with GTX 1080 (random generation, etc.) and cuda 8.0RC is not supported to tensorflow. I only manged to get torch ml framework working.\n", "I've also had this issue with \nOperating System: Ubuntu 16.04\nCUDA: 8.0\ncuDNN: 5.0\nbazel: 0.3.0-2016-07-29\ngcc: 4.9\n", "CUDA 8.0 also has a 50% speed improvement on matmul for GTX 980 over CUDA 7.5, I hit similar issue and solution is to modify CROSSTOOL file -- https://github.com/tensorflow/tensorflow/issues/3431\n", "I just tried the solution in https://github.com/tensorflow/tensorflow/issues/3431 and it works. Thanks!\n", "In summary, setting the version explicitly should work.\n"]}, {"number": 3570, "title": "word2vec_basic.py : to prevent possible issue with libpng", "body": "Following error message stopped with this pull request.\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 libpng warning: Application was compiled with png.h from libpng-1.6.17\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 libpng warning: Application\u00a0 is\u00a0 running with png.c from libpng-1.2.53\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 libpng error: Incompatible libpng version in application and library\n\nI was running this example on\n    \\* Ubuntu 16.04\n    \\* python 2.7.11\n    \\* Anaconda 4.0.0 +\n    \\* TensorFlow 0.8.0\n    \\* matplotlib 1.5.1\n    \\* numpy 1.11.1\n    \\* nomlk 1.0\n\nLet me know any questions or comments.\n\nKW\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Hi @autodrive, can you sign the CLA please?\n", "Closing due to inactivity.\n"]}, {"number": 3569, "title": "Tensorflow testing script cannot import numpy whereas it is installed", "body": "Hello,\n\nI have the following error when I try to compile the pip package from the source code.\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\nls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Feb  9 18:48 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Feb  9 18:48 /usr/local/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxrwxr-x 1 root root 61453024 Feb  8 23:12 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-rw-r-- 1 root root 62025862 Feb  8 23:12 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from sources, provide the commit hash: 5c44302d778f43a13bf1b97bffd0b2b6be46ae2f\n### Steps to reproduce\n1. git clone https://github.com/tensorflow/tensorflow\n2. cd tensorflow\n3. ./configure\n4. bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n5. bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n6. bazel test -c opt --config=cuda //tensorflow/python:graph_util_test\n### Logs or other output that would be helpful\n\n```\nbazel test -c opt --config=cuda //tensorflow/python:graph_util_test\nWARNING: Output base '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7' is on NFS. This may lead to surprising failures and undetermined behavior.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/protobuf/WORKSPACE:1: Workspace name in /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/re2/WORKSPACE:1: Workspace name in /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/highwayhash/WORKSPACE:1: Workspace name in /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/gemmlowp/WORKSPACE:1: Workspace name in /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/gemmlowp/WORKSPACE (@__main__) does not match the name given in the repository's definition (@gemmlowp); this will cause a build error in future versions.\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/bit_depth.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/gemmlowp.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/map.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/public/output_stages.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/instrumentation.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/external/gemmlowp/BUILD:102:12: in hdrs attribute of cc_library rule @gemmlowp//:eight_bit_int_gemm: Artifact 'external/gemmlowp/profiling/profiler.h' is duplicated (through '@gemmlowp//:eight_bit_int_gemm_public_headers' and '@gemmlowp//:gemmlowp_headers').\nWARNING: /home/plu/git/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nINFO: Found 1 test target...\nSlow read: a 160721045-byte read from /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/_pywrap_tensorflow.so took 13761ms.\nFAIL: //tensorflow/python:graph_util_test (see /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/testlogs/tensorflow/python/graph_util_test/test.log).\nTarget //tensorflow/python:graph_util_test up-to-date:\n  bazel-bin/tensorflow/python/graph_util_test\nINFO: Elapsed time: 57.357s, Critical Path: 44.71s\n//tensorflow/python:graph_util_test                                      FAILED in 0.5s\n  /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/testlogs/tensorflow/python/graph_util_test/test.log\n```\n\nAnd here the content of the `test.log` file:\n\n```\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n-----------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/org_tensorflow/tensorflow/python/framework/graph_util_test.py\", line 21, in <module>\n    import tensorflow as tf\n  File \"/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/org_tensorflow/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 45, in <module>\n    import numpy as np\nImportError: No module named numpy\n```\n\nBut, I do have numpy installed:\n\n```\npython \nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import numpy;numpy.version.version\n'1.11.1'\n>>>\n```\n\nI do not understand why I get this Python error. Any idea of what is going wrong?\n\nThanks in advance for any help!\n", "comments": ["@martinwicke would you take a look?\n", "In the configure step, did you specify the same python you're usually using? My guess would be that you're getting a different installation of python, without numpy installed.\n", "Oh this is a good guess! Unfortunately the one specified in the configure is the good one :-( \n\nMy own guess is that it is somehow related to the issue #2703, I think my $PYTHONPATH variable is not loaded in the test environment and then it cannot find all my libs. I will do some test to check that and keep you posted.\n", "My guess was the good one, my Python environment is not loaded properly. I modified the file\n\n```\nlocal_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/org_tensorflow/tensorflow/python/__init__.py\n```\n\nTo add these two lines at the beginning:\n\n```\nimport sys\nprint(sys.path)\n```\n\nAnd this is what I have in the log file:\n\n```\n['/home/plu/git/tensorflow/tensorflow/python/framework', '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles', '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/protobuf/python', '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/org_tensorflow', '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/six_archive', '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/protobuf', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/PILcompat', '/usr/lib/python2.7/dist-packages/gtk-2.0', '/usr/lib/python2.7/dist-packages/ubuntu-sso-client']\n```\n\nThese three paths are missing:\n\n```\n'/home/plu/git/deepdetect/clients/python', '/usr/local/python/lib/python2.7/site-packages', '/home/plu/git/caffe/python'\n```\n\nWhich correspond to my $PYTHONPATH environment variable.\n", "@damienmg we have seen this before right? I couldn't find a bug open for bazel, and I forget whether this is intended behavior (it doesn't look like intended behavior to me).\n", "@martinwicke this is an issue with the environment being stripped, @aehlig is working on a principle solution to it.\n", "@aehlig let me know when a fix is available in bazel and when that would hit a release so we can recommend a new minimum version.\n", "> @aehlig let me know when a fix is available in bazel\n\n@martinwicke: The \"principle solution\" @dmarting was talking about\nis the environment variable design\nhttps://www.bazel.io/designs/2016/06/21/environment.html\n\nThe first working implementation of the --action_env just hit the\nsource tree, https://github.com/bazelbuild/bazel/commit/6f33a1c54e517d7343c36d0479713655a19f3224\n\n> and when that would hit a release so we can recommend a new minimum version.\n\nIt will be included in the 0.4 release, expected to be released in October.\n\nRegards,\nKlaus\n\n## \n\nKlaus Aehlig\nGoogle Germany GmbH, Erika-Mann-Str. 33, 80636 Muenchen\nRegistergericht und -nummer: Hamburg, HRB 86891\nSitz der Gesellschaft: Hamburg\nGeschaeftsfuehrer: Matthew Scott Sucherman, Paul Terence Manicle\n", "Ok. We'll wait for 0.4, make that the minimum version and add the `--action_env` to the bazelrc.\n", "FYI this flag is in 0.3.2 release candidate, so should be out in the next week\n", "jplu@, can you try this again? We are now building with bazel 0.3.2, so this may work now, or at least we have the opportunity to make it work if it doesn't.\n", "Thanks!! I will try today with bazel 0.3.2 and let you know if everything is now ok.\n", "Hello,\n\nI do have an issue to compile tensorflow with 0.3.2 now:\n\n```\n./configure\n/home/plu/git/tensorflow /home/plu/git/tensorflow\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\nNo Hadoop File System support will be enabled for TensorFlow\nFound possible Python library paths:\n  /usr/local/lib/python2.7/dist-packages\n  /home/plu/git/caffe/python\n  /usr/lib/python2.7/dist-packages\n  /usr/local/python/lib/python2.7/site-packages\n  /home/plu/git/deepdetect/clients/python\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n\n/usr/local/lib/python2.7/dist-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nlibcudnn.so resolves to libcudnn.4\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \nWARNING: Output base '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7' is on NFS. This may lead to surprising failures and undetermined behavior.\nExtracting Bazel installation...\n..............\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\nERROR: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/.nfs000000000000e93b00000021 (Device or resource busy).\n```\n\nI have this error every time I try to run configure even after doing `rm -rf /homes/plu/.cache/bazel/_bazel_plu/*`.\n\nIs the problem is that my desktop machine is fully on a remote file system?\n", "I believe that may be the problem (see warning in the output). You should\ncheck on the bazel issue tracker.\n\nOn Friday, October 28, 2016, Julien Plu notifications@github.com wrote:\n\n> Hello,\n> \n> I do have an issue to compile tensorflow with 0.3.2 now:\n> \n> ./configure\n> /home/plu/git/tensorflow /home/plu/git/tensorflow\n> Please specify the location of python. [Default is /usr/bin/python]:\n> Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\n> No Google Cloud Platform support will be enabled for TensorFlow\n> Do you wish to build TensorFlow with Hadoop File System support? [y/N] N\n> No Hadoop File System support will be enabled for TensorFlow\n> Found possible Python library paths:\n>   /usr/local/lib/python2.7/dist-packages\n>   /home/plu/git/caffe/python\n>   /usr/lib/python2.7/dist-packages\n>   /usr/local/python/lib/python2.7/site-packages\n>   /home/plu/git/deepdetect/clients/python\n> Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\n> \n> /usr/local/lib/python2.7/dist-packages\n> Do you wish to build TensorFlow with GPU support? [y/N] y\n> GPU support will be enabled for TensorFlow\n> Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\n> Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]:\n> Please specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\n> Please specify the Cudnn version you want to use. [Leave empty to use system default]:\n> Please specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\n> libcudnn.so resolves to libcudnn.4\n> Please specify a list of comma-separated Cuda compute capabilities you want to build with.\n> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\n> Please note that each additional compute capability significantly increases your build time and binary size.\n> [Default is: \"3.5,5.2\"]:\n> WARNING: Output base '/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7' is on NFS. This may lead to surprising failures and undetermined behavior.\n> Extracting Bazel installation...\n> ..............\n> INFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n> ERROR: /homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/.nfs000000000000e93b00000021 (Device or resource busy).\n> \n> I have this error every time I try to run configure even after doing rm\n> -rf /homes/plu/.cache/bazel/_bazel_plu/*.\n> \n> Is the problem is that my desktop machine is fully on a remote file system?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3569#issuecomment-256861265,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_dnM0JcaXbURYbh4YKcChczHPn0qks5q4a2kgaJpZM4JYL1m\n> .\n", "Apparently it is indeed a known issue https://github.com/bazelbuild/bazel/issues/1970 I gonna try the fix and let you know.\n", "The fix in the linked issue solved the compilation error but I still have the same issue than at the beginning :(\n\n```\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n-----------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/org_tensorflow/tensorflow/python/framework/graph_util_test.py\", line 21, in <module>\n    import tensorflow as tf\n  File \"/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/org_tensorflow/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/execroot/tensorflow/bazel-out/local_linux-opt/bin/tensorflow/python/graph_util_test.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 44, in <module>\n    import numpy as np\nImportError: No module named numpy\n/homes/plu/.cache/bazel/_bazel_plu/5fecb6612d2e95475ff53a54e377c3f7/execroot/tensorflow/bazel-out/local_linux-opt/testlogs/tensorflow/python/graph_util_test/test.log (END)\n```\n\nI think it is due to the configure, as I have selected only one Python library path, how is it possible to select multiple paths?\n", "This is odd -- we are using several paths from PYTHONPATH. Is PYTHONPATH different between running configure and at runtime?", "> This is odd -- we are using several paths from PYTHONPATH.\n> Is PYTHONPATH different between running configure and at runtime?\n\nAny variable that should be taken from the invocation environment to\nbazel's build action must be declared with --action_env, any variable\nthat should be visible in tests must be declared with --test_env. This\ncan be done by either specifying these options on the command line, or\nin one of the rc-files.\n\n", "@martinwicke What I can see is that my PYTHONPATH is missing when I do the same process than in my previous comment https://github.com/tensorflow/tensorflow/issues/3569#issuecomment-238164088", "Hello, any update on this issue?", "So the bazel fix you tried did not work?", "@jplu feel free to open a new issue if the problem persists."]}, {"number": 3568, "title": "Fixing crosstool config file to build tensorflow on PowerPC architect\u2026", "body": "\u2026ures\n", "comments": ["Can one of the admins verify this patch?\n", "@keveman could you take a look?\n", "@damienmg Damien, can you take a look? I am not 100% sure about the syntax and semantics of that file.\n", "LGTM\n", "@tensorflow-jenkins test this please\n", "@yifeif - I wanted to know if this change would be part of Tensorflow 0.10.0 release. It actually missed 0.10.0rc0 by just one day or even less than that. Request to kindly include this change in the next RC(if any) or final release of 0.10.0.\n"]}, {"number": 3567, "title": "KeyError Tensorflow", "body": "Traceback (most recent call last):\n  File \"ptb_word_lm.py\", line 326, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"ptb_word_lm.py\", line 291, in main\n    raw_data = reader.ptb_raw_data(FLAGS.data_path)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/ptb/reader.py\", line 76, in ptb_raw_data\n    valid_data = _file_to_word_ids(valid_path, word_to_id)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/ptb/reader.py\", line 48, in _file_to_word_ids\n    return [word_to_id[word] for word in data]\nKeyError: 'FLOOR\\n<eos>\\n//'\n", "comments": ["Could you give more details as indicated in the issue template that appears when you click 'New Issue', e.g. OS, repro steps, etc.? Thanks!\n", "Hi @michaelisard I encountered a similar error when I changed the contents of 'ptb.train.txt' or 'ptb.valid.txt' or 'ptb.test.txt' files while running the [LSTM tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py) script. It might be something really silly, so I'm trying to workaround it, but any hint would be greatly appreciated. Thanks!\n", "the problem is that I'm trying to train rnn model to sentence tokenize with ptb example on tensorflow official webiste. \n\nI have a string defined as follows: \n\n**\"\"\"AVL LIST OF APTS High End 4bhk @ STEESHA Mount Mary Bandra Higher Floor Avl Approx 2500 sqft carpet 2 parkings Full Sea Facing Quote : Rs 30 Cr Spacious 3bhk @ PALACIO 16th Rd Bandra West Brand New Bldng Higher floor Approx 1700 sqft carpet 2 parkings Quote : Rs 9.25 Cr\"\"\"**\n\nI need to train rnn tokenizer to tokenize sentences as \nmessage1 = \"AVL LIST OF APTS 1) High End 4bhk @ STEESHA Mount Mary Bandra Higher Floor Avl Approx 2500 sqft carpet 2 parkings Full Sea Facing Quote : Rs 30 Cr\"\n\nmessage2 = \" Spacious 3bhk @ PALACIO 16th Rd Bandra West Brand New Bldng Higher floor Approx 1700 sqft carpet 2 parkings Quote : Rs 9.25 Cr \"\n\nWould you please help me in solving problem I'm really stucked. \nThanks, @martinwicke for looking into this issue!\n", "It's easier to help find a problem if you are willing to say what OS and versions of tensorflow etc. you are using and a repro case, e.g. the command line you are using. If you click on 'New Issue' you will see a template which shows you the right details to fill in. Thanks!\n", "Closing for now due to lack of response.  We are happy to reopen if more details emerge!\n"]}, {"number": 3566, "title": "Extended adjust_hue and adjust_saturation to work on 4D tensors", "body": "I noticed that adjust_hue and adjust_saturation only worked on single images (3D tensors), but I couldn't really see a reason for that, so I extended it to work on 4D. I don't know if you want this in the main repo?\n\nI based the work on the r0.9 branch because that's what I'm using, but I can do it for the master branch if that is more suitable.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "CLA signed.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "I just noticed pretty much all of the methods has a check for 3D image in them. So is this a design choice or?\n", "@martinwicke could you take a look at this one?\n", "This should be a patch against master -- we don't really want to backport features at this point. The code hasn't changed much so it will probably work unaltered against master. I have some comments on the patch itself (use tf.rank), otherwise, could you write some unit tests that exercise the newly enabled behavior?\n", "I've changed the appropriate lines with calls to tf.rank instead. I will have to assume there will always be three channels, then -- but I guess that's fine?\n\nI don't have much experience with unit testing, but I will take a look at it later this week or start of next week. Any recommendations are welcome.\n\nShould I also change the other ops to be usable on batches of images as well?\n\nI will continue working on this branch for now. Once we agree it's ready I'll close this pull request, merge with master, and create a new one.\n", "Sounds good. \n\nI left a comment about the 3 channels issue.\n\nIf you would remove the restriction to 3D from other ops, that would be amazing.\n\nAs for unit tests: Please add tests that make sure that these ops actually work as expected when applied to batches. You can probably simply extend the existing tests to add cases for higher input rank. The key is to use predictable input data with known outputs, and keep the tests fast.\n", "Just to give an update: I'm working on removing the 3D restriction from the other ops, but I'm having difficulty doing it in a way that would work with tensors whose shape is not statically defined. Right now I'm thinking something like creating a flat tensor of ones (or whatever fits the op) of same size as the rank of the input tensor, and then change whatever index needs to be different for that op with tf.scatter_update, but I'm not sure how to get scatter_update to work with constant tensors. I'm sure I will figure it out though, just wanted to inform you I haven't forgotten about this. \n", "I'm closing this, since I've opened a new pull request for the master branch instead of r0.9 ( #4150).\n"]}, {"number": 3565, "title": "Tensorboard unable to generate result, IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/TAG' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/TAG", "body": "I use tensorboard in docker, when I type the command\uff1a \n\ntensorboard --logdir='logs/'\n\nthe following error showed up:\n\nIOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/TAG' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/TAG\n\nis there anyone can help me?\n", "comments": ["problem solved! \nwhen run tensorboard in docker, it is need to map port from container to host. For example, run\ndocker run -it -P tensorflow tensorboard --logdir='/notebooks/logs/'\n"]}, {"number": 3564, "title": "Update index.md", "body": "I just test it as I want to assign a, b and c to gpu3. Something wrong happened.\nif the code \"c = tf.matmul(a, b)\" is not include in \"with tf.device('/cpu:0')\", then even you find b and a are assigned to cpu0\uff0cthey just be assigned default. Because c is assigned to cpu0 default. a and b are used by c, so they are assigned to cpu0. \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it !!!!!!!!!!!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks @ssyrain!\n"]}, {"number": 3563, "title": "[BUG?] Memory overflow (?) on Nvidia Quadro M6000 using feed dictionary with conv2d", "body": "Hi everybody,\n\nwe just got several machines with Quadro M6000 (12G RAM) for our lab, and they all seem to suffer the same problem when feeding too much data to a convolution via a feed dictionary. The problem does not appear when using CPU on the same machine, and neither when using an equivalent machine with a different GPU (GeForce GTX Titan X).\n\nIt looks like when using a feed dictionary to provide data for the evaluation of a graph, part of the weights in the graph may be overwritten depending on the size of the data fed through the dictionary.\n#### Two ways to reproduce\n##### A minimal example\n\n```\nimport tensorflow as tf\nimport numpy as np\nx = tf.placeholder(tf.float32, shape=(None, 1024))\nx_r = tf.reshape(x, (-1, 32, 32, 1))\nW = tf.Variable(tf.ones((5, 5, 1, 32)))\nW_sum = tf.reduce_sum(W)\nconv = tf.nn.conv2d(x_r, W, strides=(1, 1, 1, 1), padding='SAME')\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\ndata = np.ones((5001, 1024), dtype='float32')\nprint(sess.run(W_sum))\nc = sess.run(conv, feed_dict={x: data})\nprint(sess.run(W_sum))\nsess.close()\n```\n\nIt outputs\n\n```\n800.0\n0.0\n```\n\nwhich indicates that `W` has changed through the feed-dictionary evaluation of the convolution.\n\nNote that\n- By reducing `data.shape[0]` to 5000 or less, the problem disappears\n- Going up to 5070, the second value stays at 0.0. \n- at exactly 5080 the problem seems to disappear (we get `800.0 800.0`)\n- from 5100 the second number can take arbitrary values (not zero)\n- The problem does not arise when replacing the `tf.nn.conv2d` operation by an equivalently sized  `matmul` operation (same dimensions of weight vector, not same dimensionality of output)\n- EDIT: (that said, see below, in the MNIST tutorial all weight vectors are affected, not only ones pertaining to convolution. But that doesn't necessarily contradict the convolution being the culprit)\n##### The expert MNIST tutorial\n\nThe problem also arises in one of the very first tutorials https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html\n\nEverything works perfect until the last line of the tutorial, where a large array is fed by feed-dictionary for test accuracy. This modifies all the weight vectors of the convnet!\n\nAgain, the error disappears when feeding only 5000 test examples instead of 10000 to the architecture.\n#### Environment\n- OS: Ubuntu 1504\n- CUDA version 7.5\n- cuDNN version 4\n- library files\n\n```\n(tensorflow) meickenb@g-1504:~/code/tensorflow$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug  15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192 Aug  15  2015 /usr/local/cuda/lib64/libcudart_static.a\n\n(tensorflow) meickenb@g-1504:/usr/lib/x86_64-linux-gnu$ ls -l libcudnn*\nlrwxrwxrwx 1 root root       13 Jul 21 18:22 libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17 Jul 21 18:22 libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024 Jul 21 18:22 libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Jul 21 18:22 libcudnn_static.a\n```\n- Anaconda 3\n- pip install from wheel `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp35-cp35m-linux_x86_64.whl`\n- tf version 0.9.0\n\n```\n(tensorflow) meickenb@g-1504:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.9.0\n\n```\n\nPlease let me know if you need any other info, or what else I could try to narrow down the reason for this overflow.\n\nCC @tmanglesl @CarmineCella @andreuxmath @beedotkiran\n", "comments": ["Thanks for the detailed report. @zheng-xq would you take a look?\n", "Closing due to inactivity. Feel free to open a new bug if the problem persists with more recent versions."]}, {"number": 3562, "title": "Locate operations on parameter server", "body": "Hi,\n\nI'm working on a distributed tensorflow, and I want to do some preprocess on the variables before the workers retrieve the variable. The preprocessing will trim the model and consequently reduce the amount of network traffic that transfer the variable from parameter server to workers. \n\nIs that possible to put operations on parameter servers? Thank you\n", "comments": ["Yes, there's no distinction between parameter servers and other workers in TensorFlow, and Ops can be placed in the most appropriate location. [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) is the right place for this kind of usage question: if you search for 'device placement' or 'device scope' you will see discussions about placing ops.\n"]}, {"number": 3561, "title": "R0.9", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 3560, "title": "Inception retraining / transfer learning fails when running with GPU", "body": "Thanks so much for releasing TensorFlow.  We're experimenting with the image retraining example as described here: https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html\n\nEverything in TensorFlow has worked perfectly for us, including the test and GPU setup validation samples.  However, when running the Inception retraining code and using the GPU, TensorFlow errors.  Since the error occurs in `check_numerics_op.cc`, I thought it might be worth reporting.  \n\nFor example, CPU-only bottleneck generation for 600 classes on a recent 36-core machine takes nearly a month, so working multi-GPU support for bottleneck creation would be really great.  It would help us learn faster and hopefully contribute to the project sooner.\n\nAbbreviated output (full output is attached): \n[TensorFlow_Retraining_Error.txt](https://github.com/tensorflow/tensorflow/files/389942/TensorFlow_Retraining_Error.txt)\n\npython tensorflow/examples/image_retraining/retrain.py --image_dir ~/flower_photos\nMon Jul 25 00:54:39 PDT 2016\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 1080\n...\nCreating bottleneck at /tmp/bottleneck/dandelion/3365850019_8158a161a8_n.jpg.txt\n**E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x1000e000300 = {1, 0} activation input is not finite.**\nTraceback (most recent call last):\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 824, in <module>\n    tf.app.run()\n### Environment info\n\nOperating System: Ubuntu 14.04\nGPU: GTX 1080 (two cards)\n\nInstalled version of CUDA and cuDNN: \n /usr/local/cuda/lib/libcudadevrt.a\n /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\n /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n /usr/local/cuda/lib/libcudart.so.7.5.18\n /usr/local/cuda/lib/libcudart_static.a\n1. The commit hash (`git rev-parse HEAD`)\n   v0.9.0 25023df\n2. The output of `bazel version`\n   Build label: 0.2.3\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Tue May 17 14:21:13 2016 (1463494873)\n   Build timestamp: 1463494873\n   Build timestamp as int: 1463494873\n### Steps to reproduce\n1. Build tensorflow from source with GPU support; all demos and tests working properly\n2. `bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos` runs properly but only uses CPU when generating bottleneck files\n3. `python tensorflow/examples/image_retraining/retrain.py --image_dir ~/flower_photos` utilizes the GPU, but crashes during bottleneck file generation due to \"irregular\" response from GPU.  If the /tmp/bottlenecks directory has not yet been generated, image retraining using the gpu will fail almost immediately after processing a few files.\n### What have you tried?\n1.  If cached bottleneck files already exist in /tmp/bottlenecks, then the retraining on GPU works properly.  However, building those bottleneck files in the first place depends on the CPU to generate those bottlenecks, which takes forever (nearly a month for a dual socket E5-2699 v3 36-core machine for around 600 classes)\n", "comments": ["Thank you for the detailed bug report! We have heard of other people having problems with GTX 1080 #3507 which have been fixed by switching to CUDA 8.0. Is it easy for you to try either using a different GPU or building with 8.0? Unfortunately 8.0 isn't fully supported by TensorFlow yet so that isn't an ideal solution, but it has unblocked other people and will be supported before long.\n", "Thanks for reading it, Michaelisard!  We are very excited by the possibilities opened up by TensorFlow.  I can give CUDA 8.0RC a try.\n\nWe could try other GPUs, though these GTX 1080s were bought specifically for TensorFlow.  On a somewhat related question, is the 8GB of GDDRAM going to be a major limiting factor in your opinion?  Our training sets typically run around 60GB in size (actual total image file size; the bottlenecks are much smaller).  The new Titan X Pascal cards have 12GB; I wonder if we'll be handicapped in the long run by the 8GB in terms of batch sizes and other problems.\n", "CUDA 8.0 is working for me using a GTX 1080 and Ubuntu 16.04.  Can confirm creating bottlenecks takes forever, a few seconds an image or so for me.\n", "@theclifbar I was only suggesting you try another GPU as a debugging aid to help pinpoint the problem: TensorFlow should work on GTX 1080 but others have had trouble on that specific card with earlier versions of CUDA and that may be the issue here. Please report back if CUDA8 doesn't fix it.\n\nI can't comment on the bottleneck question I'm afraid since I am not familiar with the details of the model implementation: @shlens does this sound expected?\n", "thanks @michaelisard, upgrading to CUDA 8.0 Release Candidate and cuDNN 5 and building from source again with GPUs enabled has resolved this issue!\n\n@JohnAllen, thanks for your information, as well.  We've found that running the retrainer when built for GPUs takes about 0.1 seconds per image bottleneck; previously, on a 36-core machine, it would take around 3 seconds per bottleneck with all cores maxed out.  If your setup is still taking 3 seconds per image bottleneck, I'm happy to help debug your setup with you as it really should be 20-30x faster on the GTX 1080.  You might simply need to rebuild the trainer with CUDA enabled.\n\nThe retrainer / transfer learning example does not seem to support multiple GPUs, unfortunately, even with the --num_gpus flag.  I can open up another issue for that if that's not by design nor a known issue.  Thanks again, you have literally saved us weeks of processing time.\n", "Hello everyone. I'm having some issue regarding running the bottlenecks on GPU. Currently, it takes about 1s per bottleneck and when I check the GPU utilizaiton with nvidia-smi it appears that it is fluctuating between 0-20%.\r\n\r\nAlso, I noticed that this gets printed to my screen everytime sess.run() is called\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\r\nMaybe this is the cause of overhead? I also tried to check the device placement logs and here are the first few lines from the log file\r\n\r\n\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GRID K520, pci bus id: 0000:00:03.0\r\nCreating bottleneck at ./newmodel/bottleneck/baby_in_crib/1_136.jpg.txt\r\nDecodeJpeg_1: (DecodeJpeg): /job:localhost/replica:0/task:0/cpu:0\r\nShape: (Shape): /job:localhost/replica:0/task:0/cpu:0\r\nDecodeJpeg: (DecodeJpeg): /job:localhost/replica:0/task:0/cpu:0\r\nCast: (Cast): /job:localhost/replica:0/task:0/gpu:0\r\nExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/gpu:0\r\nResizeBilinear: (ResizeBilinear): /job:localhost/replica:0/task:0/gpu:0\r\nSub: (Sub): /job:localhost/replica:0/task:0/gpu:0\r\nMul: (Mul): /job:localhost/replica:0/task:0/gpu:0\r\nconv/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/gpu:0\r\nconv/batchnorm: (BatchNormWithGlobalNormalization): /job:localhost/replica:0/task:0/gpu:0\r\nconv/CheckNumerics: (CheckNumerics): /job:localhost/replica:0/task:0/gpu:0\r\nconv/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nconv: (Relu): /job:localhost/replica:0/task:0/gpu:0\r\nconv_1/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/gpu:0\r\nconv_1/batchnorm: (BatchNormWithGlobalNormalization): /job:localhost/replica:0/task:0/gpu:0\r\nconv_1/CheckNumerics: (CheckNumerics): /job:localhost/replica:0/task:0/gpu:0\r\nconv_1/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\nconv_1: (Relu): /job:localhost/replica:0/task:0/gpu:0\r\nconv_2/Conv2D: (Conv2D): /job:localhost/replica:0/task:0/gpu:0\r\nconv_2/batchnorm: (BatchNormWithGlobalNormalization): /job:localhost/replica:0/task:0/gpu:0\r\n...\r\n\r\n\r\nAs I understand, the operations in the first few layers are being run in the cpu and the rest on gpu. IS the data transfer between cpu and gpu the cause for the slow execution? @theclifbar did you have any such issues while running your model? \r\nThis is my first post here and please let me know if I have to open another discussion for this. Any help is much appreciated. Thanks!\r\n\r\n", "Can anyone provide the multi-GPU version of image re-training would be great"]}, {"number": 3559, "title": "Can I add a py_func to a queue?", "body": "Is it possible to add a custom py_func to load non-image data in a FIFOQueue? It appears that the Queue does not support py_func objects. Specifically can you enqueue a py_func object? When trying this Tensor Flow gives the following error:\n\n`tensorflow/core/framework/op_kernel.cc:909] Unimplemented: Unsupported object type Tensor`\n\nI've tried calling `set_shape` on the `py_func` output\n", "comments": ["Sorry I am not understanding immediately. Are you asking whether or not you can use higher-order functions, i.e. send functions down a queue, and then dequeue and apply them? Or are you asking if you can process images from a queue with a custom processor?\n", "Yes I'd like to process other data (such as 16-bit png images and .npy files) from a queue with a custom processor. \n\nIdeally I'd like to avoid using TFRecord as an intermediate format.\n", "You could encode arbitrary data into a string tensor and send the string tensor into the queue, then process the strings at the other end with a custom function. I'm still not sure I'm understanding though: is the queue relevant, or are you just asking how to store and process binary data in tensors?\n\nIf this is a usage question it's probably better suited to [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow): this forum is intended for TensorFlow bugs and installation issues.\n"]}]