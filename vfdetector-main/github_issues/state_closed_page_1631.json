[{"number": 3978, "title": "Tensorflow hangs when enqueueing with session timeout specified", "body": "I am creating a simple `FIFOQueue` and enqueueing elements to it. Everything works fine when the session timeout is not specified (code as below, with timeout commented out):\n\n```\n$ time python ex.py\nenqueue\nenqueue\nenqueue\nenqueue\nenqueue\n1.56user 0.24system 0:01.70elapsed 105%CPU \n```\n\nEverything finishes correctly. But when I run the same code with timeout specified (to 60 seconds):\n\n```\nimport tensorflow as tf\nimport numpy as np\n\ntf.reset_default_graph()\ndata = np.array([1, 2])\nnum_epochs = 5\nqueue1 = tf.FIFOQueue(capacity=50, dtypes=[tf.int32], shapes=[()])\n\ndef create_session():\n    config = tf.ConfigProto()\n    config.operation_timeout_in_ms=60000\n    return tf.InteractiveSession(config=config)\n\nenqueue_op = queue1.enqueue_many(data)\nsess = create_session()\ntf.train.start_queue_runners()\nfor i in range(num_epochs):\n    print(\"enqueue\")\n    sess.run(enqueue_op)\n```\n\n```\n$ time python ex.py\nenqueue\nenqueue\nE tensorflow/core/client/tensor_c_api.cc:485] Timed out waiting for notification\nTraceback (most recent call last):\n  File \"ex.py\", line 21, in <module>\n    sess.run(enqueue_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.DeadlineExceededError: Timed out waiting for notification\nCommand exited with non-zero status 1\n1.57user 0.24system 1:01.71elapsed 2%CPU\n```\n\nThe number of \"enqueue\"s is not deterministic, but the code never finishes. Note that with timeout specified the code didn't manage to finish in a minute (in comparison to less than 2 seconds with no timeout).\n### Environment info\n\nOperating System: lubuntu 14.04, kernel 3.13.0-32-generic\n\nInstalled version of CUDA and cuDNN: no CUDA, using just CPU\nIf installed from binary pip package, provide:\n1. Which pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`: 0.10.0rc0\n", "comments": ["PS, I tried .10rc0 (build with --cuda) on mac and ubuntu, and that code snippet always finishes in a couple of seconds\n", "Thanks for reporting - I was able to reproduce with the nightly version, and should have a fix soon.\n", "@sygi   This should be fixed now in the nightly builds.  (thanks @mrry!)  \nPlease can you reopen if the problem persists.\n"]}, {"number": 3977, "title": "Installation failed on Mac OS X", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nOS X EI Capitan 10.11.6\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nls: /path/to/cuda/lib/libcud*: No such file or directory\n\nIf installed from binary pip package, provide:\ninstall from pip package\n# Mac OS X, CPU only, Python 2.7:\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py2-none-any.whl\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   File \"/usr/local/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n     from tensorflow.python import *\n   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 52, in <module>\n     from tensorflow.core.framework.graph_pb2 import *\n   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\n     from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\n     from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\n     from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n   File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 22, in <module>\n     serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB2\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3')\n   TypeError: __init__() got an unexpected keyword argument 'syntax'\n### Steps to reproduce\n1. Install tensorflow from pip\n2. Test installation.\n### What have you tried?\n1. uninstall protobuf and reinstall tensorflow again.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["This means you have an older version of protobuf (version 2) installed.  See https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#mac-os-x-typeerror-init-got-an-unexpected-keyword-argument-syntax\n"]}, {"number": 3976, "title": "Add support for complex SVD.", "body": "In response to my issue https://github.com/tensorflow/tensorflow/issues/3874, I decided to get it working. I might still need to write some tests, but preliminary testing seems good.\n", "comments": ["Can one of the admins verify this patch?\n", "Please update the tests in tensorflow/python/kernel_tests/svd_op_test.py accordingly.\n", "@AidanGG Please update the tests in tensorflow/python/kernel_tests/svd_op_test.py accordingly.\n", "I modified the tests and they do pass, but I still have to clean up the file. Are the tolerance values appropriate?\n", "@AidanGG Looks good to me! Thanks!\n", "@tensorflow-jenkins test this please\n", "@AidanGG Could you please sync your repo with the master and resolve the merge conflicts? The second template argument was removed from all the linear algebra ops to avoid duplicating the code. This reduces compile time and binary size.\n", "I think this is ready now.\n", "@tensorflow-jenkins test this please\n", "I don't know what's going on with the failed check.\n", "The test failure is in a flaky test. Unrelated to your PR, which I'm merging now. Thanks for your contribution.\n", "Let's give the testing another go. @tensorflow-jenkins test this please.\n", "@rmlarsen Tests look good. Ready to merge.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3975, "title": "Install numpy from pip", "body": "- Pip is installed using `get-pip`\n- The version of numpy in apt repos is ancient (1.8) AND a newer version seems to be pulled in from pypi anyway.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "@cancan101 , can you please make the same changes to Dockefile, Dockefile.devel and Dockerfile.gpu in the same directory? Thanks.\n", "@caisq done\n", "@tensorflow-jenkins , test this please.\n", "Can you squash the two commits into one, @cancan101 ?\n", "done\n", "@tensorflow-jenkins , test this please.\n", "Merged. Thanks!\n"]}, {"number": 3974, "title": "Regression in Performance between r0.9.0 and r0.10.0rc0.", "body": "### Environment info\n\nOperating System:\n\n```\nLinux 4.4.11-23.53\n```\n### Observations\n\nI am testing on a Tesla K80 (details below) using the following lines:\n- `nvidia-docker run --rm -it -v /tmp/cifar10_data:/tmp/cifar10_data tensorflow/tensorflow:0.9.0-devel-gpu  bash -c 'ln -s /usr/local/nvidia/lib64/libcuda.so.1 /usr/lib/x86_64-linux-gnu/libcuda.so && python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py'`\n- `nvidia-docker run --rm -it -v /tmp/cifar10_data:/tmp/cifar10_data tensorflow/tensorflow:0.10.0rc0-devel-gpu python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py`\n\nOn r0.0.9, I get: ~`575 examples/sec`.\nOn r0.10.0rc0 I get ~`425 examples/sec`.\n\nHere are the device stats:\n\n```\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:00:04.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\n```\n\nI have the: Nvidia Driver Version: 367.35\n", "comments": ["(Can you try the nightly pip at HEAD?  it's probably related to an eigen regression that we're in the process of cherry-picking into 0.10.0)\n", "It looks like tf nightly images are not getting built for `nightly-devel-gpu`: https://hub.docker.com/r/tensorflow/tensorflow/tags/\n", "Indeed.  Can you try a non-docker version here: https://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/ ?\n\ncc @jendap and @gunan since I don't know how our dockers are built or pushed\n", "Probably related to #3603 ?\n", "@caisq has recently set up nightly-docker-\\* jobs to build and push nightly docker images for all our builds.\nHowever, that job seems to be failing for a while now.\nWe will look into the docker failure.\n\nI also cherrypicked Benoit's fix yesterday into r0.10. \nOnce we fix the jobs that build docker images, I will ping this issue to try again.\nI am assigning this to myself for now.\n", "This might be related to the nightlies failing: https://github.com/tensorflow/tensorflow/issues/3989.\n\nI think the issue is cudnn 5.1 but am investigating.\n", "@cancan101 This seems to be fixed in the nightly actually. This is the performance I got on a Titan X:\ntensorflow/tensorflow:0.9.0-devel-gpu:\n1497.08\n\ntensorflow/tensorflow:0.10.0rc0-devel-gpu:\n600.573\n\ntensorflow/tensorflow:nightly-devel-gpu:\n1934.54\n", "Are you able to get the Dockerfile on master to build?\n", "@cancan101 nope, I'm getting the same error as you.\n", " Well actually the fix was easy, see #3989 \n", "Closing this issue since after the fix is applied, the performance regression is gone.\nPlease reopen if I am wrong.\n"]}, {"number": 3973, "title": "cifar10 train runs 50% slower than r0.9 in r0.10rc0", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": []}, {"number": 3972, "title": "tf.gradients returns None in tf.map_fn", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nMac OS 10.10\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nNone\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0rc0\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### Steps to reproduce\n\nI want to evaluate the diagonal of Hessian with `tf.map_fn`, which takes a function that maps each dimension (scalar) to its Hessian\n\n```\nimport tensorflow as tf\n\n\ndef hessian_factory(f):\n    def hessian1(x):\n        g = tf.gradients(f, x)[0]\n        h = tf.gradients(g, x)[0]\n        return h\n\n    return hessian1\n\nsess = tf.Session()\nx = tf.Variable(1.0)\nsess.run(tf.initialize_all_variables())\n\nf = 1. / (1 + tf.exp(-x))\nfunc = hessian_factory(f)\nh = tf.map_fn(func, x)\n\nprint(sess.run(h))\n```\n\nHowever, it seems `tf.gradients(f, x)` produces `None`.\n\nEven the following statements gives `None`\n\n```\nimport tensorflow as tf\n\nsess = tf.Session()\nx = tf.Variable(1.0)\nsess.run(tf.initialize_all_variables())\n\nf = 1. / (1 + tf.exp(-x))\nh = tf.map_fn(lambda x: tf.gradients(f, x)[0], x)\n```\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["It is a known issue that tf.gradients() can't be called inside map_fn, due to some tricky problems with TensorArray.  However you can call tf.gradients() inside tf.while_loop so try to use tf.while_loop directly.\n", "Is that still a problem with recent versions?", "`None`s in Hessians are expected for reasons unrelated to `map_fn` -- for implementation reasons, TensorFlow returns `None` instead of `0` when the gradient is not a function of the variable being differentiated against. Hence off-diagonal terms are None's, see discussion here for Hessians -- https://www.google.com/url?hl=en&q=https://groups.google.com/a/tensorflow.org/d/msgid/discuss/d83404b7-793a-4201-ad73-fcecbb83eaac%2540tensorflow.org?utm_medium%3Demail%26utm_source%3Dfooter&source=gmail&ust=1485411137547000&usg=AFQjCNEhO37F5hU6GDwsfv8BBbAariqCFQ\r\n\r\nAlso see https://github.com/tensorflow/tensorflow/issues/783 for why it's None's instead of 0's", "You need to do something like this to get Hessians\r\n\r\n```\r\ndef replace_none_with_zero(l):\r\n  return [0 if i==None else i for i in l] \r\n\r\ntf.reset_default_graph()\r\n\r\nx = tf.Variable(1.)\r\ny = tf.Variable(1.)\r\nloss = tf.square(x) + tf.square(y)\r\nsess = create_session()\r\ngrads = tf.gradients([loss], [x, y])\r\nhess0 = replace_none_with_zero(tf.gradients([grads[0]], [x, y]))\r\nhess1 = replace_none_with_zero(tf.gradients([grads[1]], [x, y]))\r\nhessian = tf.pack([tf.pack(hess0), tf.pack(hess1)])\r\nprint hessian.eval()\r\n\r\n```", "Closing since @yaroslavvb comment seems like a reasonable answer.", "@aselle, @drpngx or @yuanbyu , can something about the interaction between tf.map_fn and gradients be added to the API documentation? I don't think anything is currently mentioned.", "So there was a convention where gradients would return none (and not zero)\nif integers were involved. Then we changed some case for correctness. That\nhappened maybe a couple of months ago. Do your computations involve\nintegers?\n\nOn Sun, Jun 10, 2018, 8:52 AM Rylan Schaeffer <notifications@github.com>\nwrote:\n\n> @aselle <https://github.com/aselle> and @drpngx\n> <https://github.com/drpngx> , can something about the interaction between\n> tf.map_fn and gradients be added to the API documentation? I don't think\n> anything is currently mentioned.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/3972#issuecomment-396059390>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbdrH-l10xL1grY_-NXncGFE6JZE8ks5t7UDKgaJpZM4Jqhtc>\n> .\n>\n", "@drpngx , I'm actually dealing with a different use case (I was using `tf.map_fn` to feed a sequence of 3d images through a few `conv2d` layers and just discovered that no gradients were reaching my convolutional layers). Was your question intended for me?\r\n\r\nAlso, as an aside, what is the preferred way of feeding a sequence of images through convolutional layers? Using a while loop seems unnecessarily complicated.", "I created a separate issue for my question/suggestion. https://github.com/tensorflow/tensorflow/issues/19896", "I see. I don't really see why `map_fn` can't propagate the gradients. Maybe file a feature request and CC @skye.", "Sounds good! I opened a new feature request."]}, {"number": 3971, "title": "tf.contrib.metrics.streaming_auc evaluation exception", "body": "I used `tf.contrib.metrics.streaming_auc` to evaluate DNN rank model, \n\n```\nauc,  opts = tf.contrib.metrics.streaming_auc(logits, labels)\n```\n\n`logits` are my predicted results, `labels` are true labels, they have format:\n\n```\nTensor(\"Sigmoid:0\", shape=(100, 1), dtype=float32)\nTensor(\"labels_placeholder:0\", shape=(100, ?), dtype=float32)\n```\n\nErrors as following:\n\n```\nTraceback (most recent call last):\n  File \"/Users/nali/Workspace/tensorflow_example/search_click/MLP.py\", line 200, in <module>\n    dnn.run(training_data=[x, y], epochs=10)\n  File \"/Users/nali/Workspace/tensorflow_example/search_click/MLP.py\", line 135, in run\n    feed_dict={self.x: x[samples], self.y: y[samples]})\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value auc/true_positives\n     [[Node: auc/true_positives/read = Identity[T=DT_FLOAT, _class=[\"loc:@auc/true_positives\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](auc/true_positives)]]\nCaused by op u'auc/true_positives/read', defined at:\n  File \"/Users/nali/Workspace/tensorflow_example/search_click/MLP.py\", line 200, in <module>\n    dnn.run(training_data=[x, y], epochs=10)\n  File \"/Users/nali/Workspace/tensorflow_example/search_click/MLP.py\", line 126, in run\n    cross_entropy, accuracy, logits = self.training()\n  File \"/Users/nali/Workspace/tensorflow_example/search_click/MLP.py\", line 100, in training\n    accuracy = self.evaluation(logits, self.y)\n  File \"/Users/nali/Workspace/tensorflow_example/search_click/MLP.py\", line 146, in evaluation\n    auc, opt = tf.contrib.metrics.streaming_auc(logits, labels)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 718, in streaming_auc\n    predictions, labels, thresholds, ignore_mask)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 600, in _tp_fn_tn_fp\n    true_positives = _create_local('true_positives', shape=[num_thresholds])\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 75, in _create_local\n    collections=collections)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 211, in __init__\n    dtype=dtype)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 319, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 831, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n```\n\nThanks in advance.\n", "comments": ["This looks like you failed to initialize some of the variables in your model?\n\nIt seems like the uninitialized variable is a LOCAL_VARIABLE created [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/metrics/python/ops/metric_ops.py#L708)\n\nI think this requires you to call tf.initialize_all_variables() and tf.initialize_local_variables()   (see #1045)\n\n@wicke  Is there somebody who can comment on usage of this op?  \n", "@prb12 is correct, you have to call tf.initialize_local_variables(), since local variables are not part of all variables. Yes, we know how silly that sounds, and we're working on it.\n\nIf you use an Estimator, the initialization should be taken care of for you.\n", "@martinwicke So what exactly is the fix for this?  Do i call tf.initialize_local_variables() before calling auc,  opts = tf.contrib.metrics.streaming_auc(logits, labels)?", "You have to call tf.initialize_local_variables().eval() once before you start calling streaming_auc. Every time you call initialize_local_variables().eval() it will reset your auc.", "Python is telling me Operation object has no attribute eval for \r\ntf.initialize_local_variables().eval()", "sorry, .run()\u200b\r\n", "K.get_session().run(tf.initialize_local_variables().run())\r\n\r\nGives me\r\nbuiltins.ValueError: Cannot execute operation using `run()`: No default session is registered. Use `with sess.as_default():` or pass an explicit session to `run(session=sess)`\r\n", "I don't know what K.get_session() does, but have you tried making and\npassing a session (or using a with statement) as suggested?\n", "You may try the following hack:\r\n\r\n1. Define a `callback` which runs `keras.backend.get_session().run(tf.initialize_local_variables())` on each epoch begin.:\r\n\r\n       class ResetLocalVariables(keras.callbacks.Callback):\r\n            def on_epoch_begin(self, logs={}):\r\n                keras.backend.get_session().run(tf.initialize_local_variables())\r\n\r\n2. Add it to your loss function:\r\n\r\n        model.fit(..., callbacks=[.... , ResetLocalVariables()], ...)", "@marimarimarimari I dont think this will work because update_op does the accumalation step and there's no way to get that into the metrics function in keras because y_pred and y_true are not given to you before execution.", "You need to call tf.initialize_local_variables().run() AFTER you call tf.contrib.metrics.streaming_auc() and BEFORE you call a session.run().\r\n\r\nI hope it helps."]}, {"number": 3970, "title": "update instructions for Raspberry Pi", "body": "Hi Tensorflow team,\nwhen installing on a fresh Raspberry Pi, I noticed that I also had to install a couple additional packages. These two lines were taken from the Linux instructions, and were verified by me (and additionally a friend of mine), when we've both started building it on a new Raspbian system.\n\nEverything else worked perfectly, thanks for the excellent documentation!\nJennifer\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "This looks good to me, thanks @naturegirl!\n", "Thank you for reviewing and merging @petewarden !\n"]}, {"number": 3969, "title": "update instructions for Raspberry Pi", "body": "Hi Tensorflow team,\nwhen installing on a fresh Raspberry Pi, I noticed that I also had to install a couple additional packages. These two lines were taken from the Linux instructions, and were verified by me (and additionally a friend of mine), when we've both started building it on a new Raspbian system.\n\nEverything else worked perfectly, thanks for the excellent documentation!\nJennifer\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I'm closing this, as I ran into some CLA issues.\nThis is the updated PR, after I got the CLA setup correctly: https://github.com/tensorflow/tensorflow/pull/3970\n"]}, {"number": 3968, "title": "More tutorial updates for r0.10 release", "body": "Possible to fold this commit into the r0.10 release, as it contains a fix for a broken URL in the Logging and Monitors tutorial?\n", "comments": ["Can one of the admins verify this patch?\n", "LGTM\n", "Jenkins, test this please.\n"]}, {"number": 3967, "title": "Document checkpoint file format", "body": "Hi,\nAs far I can find, the internal structure of checkpoint files is not documented on tensorflow.org. I'm trying to write bindings to TensorFlow which can load checkpoints without making use of the Python API, so I was wondering if that information is available somewhere.\n", "comments": ["At the moment the checkpoint format isn't public for a good reason: it's changing to a more efficient format and remains an internal implementation detail of the Saver class in python.  But as part of this improvement, we'll likely make the checkpoint format public through some C++ helper classes, and a C-API to go with it.\n\n@concretevitamin can offer more insight.\n", "+1 to what @vrv has said.  For now, you could use the [`TensorSliceReader::GetTensor()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/tensor_slice_reader.h#L102) function to obtain a tensor from a checkpoint.\n", "Thanks guys.\n"]}, {"number": 3966, "title": "Add cuda_configure repository rule to autodetect cuda.", "body": "This change reimplements the CUDA autoconfiguration mechanism in Skylark,\nproviding a `cuda_configure` workspace rule. We keep the same user interface,\nthe ./configure script, but rather than modifying source files within the\nsource tree, `cuda_configure` generates a `@local_config_cuda` workspace\ncontaining:\n- Symlinks to the CUDA headers and libraries\n- BUILD files generated with the correct CUDA and cuDNN versions\n- CROSSTOOL config with CUDA include dirs populated\n- crosstool_wrapper_driver_is_not_gcc wrapper script with compiler paths and\n  CUDA compute capabilities set.\n- cuda_config.h header file with CUDA versions and compute capabilities set,\n  which can be `#include`d by source files.\n\nThis change also makes the following fixes to `Dockerfile.gpu`:\n- Change the `CUDNN_INSTALL_PATH` to point to `/usr/lib/x86_64-linux-gnu`\n  rather than `/usr/local/cuda` since NVIDIA's image installs `libcudnn.so`\n  under `/usr/lib/x86_64-linux-gnu`.\n- Add env variable to set the minimum compute capability to 3.0.\n\nFixes #2873\n", "comments": []}, {"number": 3965, "title": "Conv2d issue with GPU", "body": "I have extracted the activations of a neural net at a certain layer with size [1,28,28,128] and I have a corresponding kernel that I want to convolve with these activations. The kernel shape is [3,3,128,256]. \nI am using:\n- 2 GTX-1080 GPUs\n- Cuda-8.0 (The problem persists with Cuda-7.5 as well)\n- Graphics Driver: NVIDIA-367\n- Python 2.7\n\nWhen I perform 2D convolution using the CPU I get the correct answer but when I run it through the GPU I get all zeros. Note that, there is no ReLu operator involved here and the output should not be zero (as confirmed by the CPU output). In addition, I do not think this is a numerical precision error. \n\nI have included the codes, activations and the kernel files for reproducing the problem here: \n\nhttps://github.com/skolouri/GTX-1080_CNN_Issue\n### Environment info\n\nOperating System: Linux 14.04\n\nInstalled version of CUDA and cuDNN: Cuda-8.0.27 and cudNN-4.0.7\n### Steps to reproduce\n\nThe steps are included here:\n\nhttps://github.com/skolouri/GTX-1080_CNN_Issue\n### What have you tried?\n1. I have tried Cuda-7.5 \n2. CPU works fine, GPU returns all zeros! \n", "comments": ["Hi,\n\nI posted a reply at: https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/FTcjjGhZOkA/BHH1j9mWAgAJ\n\nThere is a little more detail in this message.  You need to use the cuDNN 5.0 that works with CUDA 8.0, that is certain.  I am not sure if you need to run Ubuntu 16.04 for the GTX1080 driver, but keep that in mind if you have trouble after the cuDNN update.\n\nHere is my execution of your test script:\n\n![screenshot 2016-08-22 17 11 10](https://cloud.githubusercontent.com/assets/18412448/17873198/16b0f01a-6892-11e6-975c-097f6fee4890.png)\n", "Thanks Greg! CudNN-5.0 solved the issue.\n", "Great, so it looks like 14.0X is okay, but need the CUDA 8.0 and cuDNN 5.0.\n", "Looks to be fixed. Closing this issue.  \n"]}, {"number": 3964, "title": "Branch 130964761", "body": "", "comments": ["@tensorflow-jenkins test this please\n", "The one test that failed in Python 3 \"sequence_queueing_state_saver_test\" looks like a flaky test. Let's merge the PR and we will deal with the flakiness later.\n", "Also ignore the mac status. PR Mac jobs have been disabled temporarily due to resource crunch. This CL is ready to be merged @rmlarsen \n", "The mac test will be run after merge (in master jobs).\n", "@tensorflow-jenkins test this please\n", "@caisq  Thanks. I'll merge.\n"]}, {"number": 3963, "title": "add threadpool.h to the files exported to the sysconfig.get_include() directory with a binary install", "body": "### Environment info\n\nOperating System: ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: 7.5, v4\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so.4\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\n   I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n   0.10.0rc0\n### Steps to reproduce\n\n1.pip install tensorflow version 0.10.0rc0\n2.ls /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/lib/core\nerror_codes.pb.h  errors.h  notification.h  refcount.h  status.h  stringpiece.h\n\nthreadpool.h is not included in the exported header files with a binary install. This means this one can't implement a multi-threaded custom operator using the tensorflow threadpool. \n\nNote, the files that threadpool.h includes, \n# include \"tensorflow/core/platform/env.h\"\n# include \"tensorflow/core/platform/macros.h\"\n# include \"tensorflow/core/platform/types.h\"\n\nARE installed and the threadpool functionality is included in the core library. \n\nI can work around this by copying threadpool.h into my own custom operator library include directory, and everything works fine, but this is a fragile solution. \n", "comments": ["https://github.com/tensorflow/tensorflow/blob/c99b9bef4cdce8f1724095c77a9620ccf76045be/tensorflow/core/BUILD#L997 probably need to add the file to the list there (we probably need to do this for a bunch more headers that are public too).\n\n@kbrems do you want to send us a change that adds this and any other headers that might be missing for you?\n", "threadpool.h is the only file we are missing. Maybe others are missing some more?\n\nI can create a PR, but it may take a while to get the CLA through our corporate legal dept.\n", "I believe you work at HP Labs and I see an \"HP Inc.\" that's already signed the Google Corporate CLA, so assuming your email is part of your company's CLA list, it should already work. :)  Let me know if you'd rather me do it though -- I'd rather give you credit for finding this :)\n\nEdit: also \"Hewlett Packard Enterprise\" has signed the CLA.\n", "HP split into 2 companies last year: HP Inc. and Hewlett Packard Enterprise - I'm at Hewlett Packard Enterprise Labs, so we need our own CLA. I've sent the CLA to legal - let me see if I can get a response within a week. Otherwise maybe you will need to do it. \n", "(see my edit above, is Enterprise labs separate from Enterprise?)\n", "@vrv Yes, HPE Labs is part of HPE. Awaiting legal approval to get added to the contributor list for tensorflow...\n", "Fixed by #4108."]}, {"number": 3962, "title": "strip_unused on TensorFlow 0.10.0rc0 outputs ImportError: No module named tools", "body": "### Environment info\n\nOperating System: Mac OS X El Capitan\n\nInstalled version of CUDA and cuDNN: N/A\n\nInstalled from binary pip package, TensorFlow version is 0.10.0rc0.\n\nPython 2.7.11\n### Steps to reproduce\n1. `git clone https://github.com/tensorflow/tensorflow.git`\n2. `cd tensorflow`\n3. `bazel build tensorflow/python/tools:strip_unused`\n4. I got the same error as in step 3 (shown in Logs below) when running the full command with the Inception V3 or V1 model. This is with the V3 model: \n\n`bazel-bin/tensorflow/python/tools/strip_unused --input_graph=/tmp/tensorflow_inception_graph_v3.pb --output_graph=/tmp/stripped_graph.pb --input_node_names=Mul --output_node_names=softmax --input_binary=true` \n### What have you tried?\n1. I tried the same procedure previously on TensorFlow 0.8.0 on a different Mac (also running OS X El Capitan), and the above commands work perfectly: the `strip_unused` itself without any parameters outputs `Input graph file '' does not exist!` and the full command outputs the stripped data model.\n2. I took a look at the `strip_unused_lib.py` file, unable to be imported when running `strip_unused`, on github and saw it's created first on Aug. 7:\n   https://github.com/tensorflow/tensorflow/commits/master/tensorflow/python/tools/strip_unused_lib.py\n3. I also looked at a similar issue: https://github.com/tensorflow/tensorflow/issues/3881 but it doesn't solve my problem.\n4. I tried to add . to PYTHONPATH before running `bazel-bin/tensorflow/python/tools/strip_unused` and still got the same error. \n5. I did a fresh new install of TensorFlow 0.10.0rc0 on another Mac OS X El Capitan, and it's still the same.\n### Logs or other output that would be helpful\n\n```\nTraceback (most recent call last):\n  File \"/Users/zero2one/tensorflow-src/latest/tensorflow/bazel-bin/tensorflow/python/tools/strip_unused.runfiles/tensorflow/python/tools/strip_unused.py\", line 46, in <module>\n    from tensorflow.python.tools import strip_unused_lib\nImportError: No module named tools\n```\n", "comments": ["With 2 new releases after 0.10, this issue is now obsolete.\r\nSorry for the lack of attention to the issue.\r\nI am closing this now.\r\n\r\nPlease open a new issue if you run into the same problem in our latest release."]}, {"number": 3961, "title": "R0.10", "body": "Cherry picks for the fixes for r0.10 release\n", "comments": []}, {"number": 3960, "title": "Allow using custom initializers for batch norm params", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "closing due to lack of CLA, comment if you can sign\n", "Hi @vrv , I've signed now via the corporate CLA.\n", "Hi @vrv , just checking in to see if we can merge this?\n", "Can one of the admins verify this patch?\n", "Hey, yeah I now see Clarifai in the corp CLA --is your email part of the google group that is associated with the CLA?   I think that's what can trigger the googlebot to flip the bit\n", "Adding Sergio to look at the change to see if it's something we'd be able to accept anyway\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks, could you add a test where you initialize moving_mean and moving_variance with other values?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins Test this please!\n", "I am waiting for the added test before testing, otherwise we'll just have to test again.\n", "@nanddalal Can you add the test that @sguada requested?\n", "Hi, I've added a test case for the custom initializers.\n", "Thanks! @sguada can you review again?\n", "@tensorflow-jenkins Test this please!\n"]}, {"number": 3959, "title": "Delete unused boringssl.BUILD", "body": "No longer used after https://github.com/tensorflow/tensorflow/commit/4af1fb42d1c84d8493561089d710b2f4a059a279.\n", "comments": ["Looks good to me.\n"]}, {"number": 3958, "title": "tensorboard: graph visualization failed, undefined", "body": "In Chrome it says: \"Graph visualization failed: TypeError: Connot read property 'length' of undefined.\n\nIn Firefox it says: \"Graph visualization failed: TypeError: rawNodes is undefined\"\n\nI have attached the events file (appended '.txt' so it Github would accept it.)\n[events.out.tfevents.1471882692.JMugan.local.txt](https://github.com/tensorflow/tensorflow/files/430670/events.out.tfevents.1471882692.JMugan.local.txt)\n\nWhen I run with inspect, I get\n\n```\n======================================================================\nProcessing event files... (this can take a few minutes)\n======================================================================\n\nFound event files in:\n/Users/jmugan/tensorlog\n\nThese tags are in /Users/jmugan/tensorlog:\naudio -\nhistograms -\nimages -\nscalars\n   exp_cost\n   perplexity\n   train_cost\n======================================================================\n\nEvent statistics for /Users/jmugan/tensorlog:\naudio -\ngraph\n   first_step           0\n   last_step            0\n   max_step             0\n   min_step             0\n   num_steps            1\n   outoforder_steps     []\nhistograms -\nimages -\nscalars\n   first_step           23210\n   last_step            23230\n   max_step             23230\n   min_step             23210\n   num_steps            3\n   outoforder_steps     []\nsessionlog:checkpoint -\nsessionlog:start -\nsessionlog:stop -\n======================================================================\n```\n", "comments": ["Please can you provide the information requested in the TensorFlow issues reporting template.\n", "Operating System: Mac OS, Ubuntu\nInstalled version of CUDA and cuDNN: CPU\n1. Which pip package you installed.\n   https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py2-none-any.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0rc0\n\n### Steps to reproduce\n1. start tensorboard\n2. go to \"graphs\" tab.\n\n### What have you tried?\n1. Other graphs work. There is just something about this model.\n2. I tried different browsers (Firefox and Chrome) and different OS (Ubuntu)\n", "@jmugan I am getting the same error. Did you find a workaround?\n", "@3rd3 No, still an issue. Mine uses dynamic RNNs, so that might have something to do with the problem. I thought I saw an open issue related to that.\n", "I make use of higher-order functions (scan and map) and I think dynamic_rnn does so too.\n", "Acknowledged, we're looking into it.\n", "For the `data/graph` endpoint, the event accumulator returns a very trivial graph:\n\n```\nversions {\n  producer: 10\n}\n```\n\nLooking into why.\n", "Actually, events.out.tfevents.1471882692.JMugan.local seems to lack a graph. The graph_def is trivial (15 bytes):\nhttps://gist.github.com/chihuahua/bab7d4ac3c3bd7cf52955540dd5b9eb4\n\nCould you please provide info on how the file was generated? For starters, the graph is being passed to the summary writer, right?\n\nIf that's the case, I wonder why the summary writer is stubbing away the graph ...\n", "I do it like this. Is this what you are asking?\n\n```\nwith tf.Session() as sess:\n    summary_writer = tf.train.SummaryWriter(summary_path, sess.graph)\n```\n", "Maybe you don't have a graph defined on the session at that point? Did you already construct your tensorflow graph at the time you invoke that line of code?\n\nI think we'll need you to post a reproduction if we are going to debug this further. Preliminarily, it looks like it's an issue with your instantiation of the summary writer, not with the graph visualizer.\n", "Ah, you may be right. I'll take a look.\n", "@danmane My problem was that the graph was not yet defined, sorry. Perhaps it would be worthwhile to note in the error message in TensorBoard that the graph passed to SummaryWriter needs to be fully defined. What I expected was that one could simply pass `sess.graph` before building the graph and it would be written with the next `add_summary`. Or, perhaps, completely remove the writing operation from the constructor, because such side effects in a constructor are somehow counter-intuitive (but that's perhaps just me).\n", "Oh, wow, do I feel like a fool. That fixed it for me. Thanks! Yeah, I guess I didn't realize the order of operations was important there. Sorry for the trouble.\n", "I think @3rd3 is right that this is really a counter-intuitive API, rather than a user error.\nI'm working on re-writing the API for writing summaries and events in advance of the tensorflow 1.0 release, and I'll keep this issue in mind.\n", "![Capture](https://user-images.githubusercontent.com/31464781/119702060-aba1f800-be72-11eb-8fc6-b14c8af65270.PNG)\r\nGetting this error while visualizing the .pb model.\r\nI have created \"events.out.tfevents.1621934261.6e85c43ac415\" file from following code.\r\n```\r\nmodel_filename = 'model.pb'\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\nwith tf.Session() as sess:\r\n    with gfile.FastGFile(model_filename, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        g_in = tf.import_graph_def(graph_def)\r\nLOGDIR='op'\r\ntrain_writer = tf.summary.FileWriter(LOGDIR)\r\ntrain_writer.add_graph(sess.graph)\r\n```\r\n"]}, {"number": 3957, "title": "Optimizer + tf.reduce_prod() + GPU = Cannot assign a device to node 'gradients/Prod_grad/ListDiff'", "body": "### Environment info\n\nOperating System: _Ubuntu 14.4.5 LTS_\n\nInstalled version of CUDA and cuDNN: \n_CUDA 7.5.18_ and _cuDNN 4.0.7_\n\n```\n-rw-r--r-- 1 root root 322936 Aug 15  2015 libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jul 12 08:24 libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jul 12 08:24 libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug 15  2015 libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192 Aug 15  2015 libcudart_static.a\n\n-rw-r--r-- 1 sauterme tumuser        0 Aug 22 18:29 libcudart.so.7.5\n-rw-r--r-- 1 sauterme tumuser        0 Aug 22 18:29 libcudart.so.7.5.18\nlrwxrwxrwx 1 sauterme tumuser       13 Feb  9  2016 libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 sauterme tumuser       17 Feb  9  2016 libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 sauterme tumuser 61453024 Feb  8  2016 libcudnn.so.4.0.7\n-rw-r--r-- 1 sauterme tumuser 62025862 Feb  8  2016 libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed:\n   pip package version 0.10 for Python 2.7 with GPU support:\n   _https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl_\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   _0.10.0rc0_\n### Steps to reproduce\n1. Use a system with GPU\n2. Use this code snippet\n   \n   ```\n   import tensorflow as tf\n   \n   g = tf.Graph()\n   with g.as_default():\n       with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n   \n           with tf.device('/gpu:0'):\n   \n               a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n               b = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], name='b')\n   \n               c = tf.matmul(a, b)\n               res = tf.reduce_prod(c)\n   \n               #with tf.device('/cpu:0'): # <<< adding this solves the issue!\n               train_op = tf.train.AdagradOptimizer(1e-3).minimize(res)\n   \n           sess.run(tf.initialize_all_variables())    \n   \n           print sess.run(res)\n           print sess.run(train_op)\n   \n   ```\n3. You will get an InvalidArgumentError:\n   _InvalidArgumentError: Cannot assign a device to node 'gradients/Prod_grad/ListDiff': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\n    [[Node: gradients/Prod_grad/ListDiff = ListDiff[T=DT_INT32, _device=\"/device:GPU:0\"](gradients/Prod_grad/range_1, Const)]]\n   Caused by op u'gradients/Prod_grad/ListDiff', defined at:\n   File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n     \"__main__\", fname, loader, pkg_name)\n   File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n     exec code in run_globals_ \n4. Cause of the issue: tf.reduce_prod() when running the train_op. So, the problem takes place while back-propagation through this op.\n### What have you tried?\n1. Wrapping the tf.reduce_prod() with tf.device('/cpu:0'):\n   \n   Result: still getting this error!\n2. Exchangeing tf.reduce_prod() with other reduce_*() ops:\n   \n   Result: no error!\n3. Wrapping the Optimizer with  tf.device('/cpu:0'):\n   \n   **Result: Solves the error!**\n", "comments": ["@ibab @girving Possibly related to #3351 ?\n", "@vrv We were discussing a similar thing recently.  Is the right solution here just to tell people to leave the optimizer out of a location?  That seems heavy handed, but I don't think we can guarantee that gradients use no CPU-only ops.\n", "Well, the answer that I personally don't like is to tell people to use soft_placement in the config options.  A better solution would be to implement the GPU versions of these ops when possible, and if not, to take the ops that are not implemented and wrap them in a with device(cpu:0) block.\n", "(the latter approach is an implementation detail of the gradient that we can change).\n", "@bsautermeister Does this commit fix your problem?  https://github.com/girving/tensorflow/commit/957fe73546f25c7f88232ed560ed285c1c97067d\n", "@girving Indeed, it does! Thank you very much!\n\nAnd (obviously) it's much faster than wrapping the Optimizer with _tf.device('/cpu:0')_.\n", "We shouldn't close it  until I actually submit the fix.\n", "My model's performance degraded seriously because of this. tf version is 1.9, hoping the fix could be considered officially"]}, {"number": 3956, "title": "wide_n_deep_tutorial.py won't work on Python 3", "body": "@tiagonj notes that wide_n_deep_tutorial.py will not work on Python 3 because it calls urllib.urlretrieve directly, and suggests using the maybe_download function from tensorflow.contrib.learn.python.learn.datasets.base.\n", "comments": ["Fixed by #4183.\n"]}, {"number": 3955, "title": "Change app.py() to pass through unparsed flags.", "body": "This is so that a client that is using our flags library can pass their own flags as well.\n", "comments": ["LGTM, just one or two little questions.\n\nWe could also tweak `app_test.py` to use the `unittest` module, but I actually feel like it's a little nicer to see that it's getting actual command-line args routed through correctly.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3954, "title": "Linux Shell does not appear when installing Tensorflow in Dockers (Win7)", "body": "Following these instructions:\nhttp://www.netinstructions.com/how-to-install-and-run-tensorflow-on-a-windows-pc/`\n\nexecuting the last command in cmd shell (Win7):\n`docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow`\n\ncmd is downloading, pulling, then Jupyiter notebook services gets started..., but then I get warnings and the linux shell does not appear:\n\n`C:\\Users\\user>docker` run -it b.gcr.io/tensorflow/tensorflow\nUnable to find image 'b.gcr.io/tensorflow/tensorflow:latest' locally\nlatest: Pulling from tensorflow/tensorflow\na64038a0eeaa: Pull complete\n2ec6e7edf8a8: Pull complete\n0a5fb6c3c94b: Pull complete\na3ed95caeb02: Pull complete\n067b43ae9d67: Pull complete\n3d200b674fb1: Pull complete\n88dd2043900b: Pull complete\n3ea4a014aa1c: Pull complete\ned48f1cb940c: Pull complete\n34a0da2387dd: Pull complete\naa5acc667d66: Pull complete\nDigest: sha256:d320d2da9c958ee42e61f810f296de602c674319b7a87b163a42329ac903a12c\nStatus: Downloaded newer image for b.gcr.io/tensorflow/tensorflow:latest\n[I 15:44:31.744 NotebookApp] Writing notebook server cookie secret to /root/.loc\nal/share/jupyter/runtime/notebook_cookie_secret\n[W 15:44:31.790 NotebookApp] WARNING: The notebook server is listening on all IP\n addresses and not using encryption. This is not recommended.\n[W 15:44:31.791 NotebookApp] WARNING: The notebook server is listening on all IP\n addresses and not using authentication. This is highly insecure and not recomme\nnded.\n[I 15:44:31.799 NotebookApp] Serving notebooks from local directory: /notebooks\n[I 15:44:31.800 NotebookApp] 0 active kernels\n[I 15:44:31.801 NotebookApp] The Jupyter Notebook is running at: http://[all ip\naddresses on your system]:8888/\n[I 15:44:31.801 NotebookApp] Use Control-C to stop this server and shut down all\n kernels (twice to skip `confirmation).`\n\nWhat do I need to do to get the Linux Shell?\n", "comments": ["You can connect to the jupiter notebook by doing http://localhost:888\nTo get to the shell you need to find the process # of the running docker container\n\n```\ndocker ps \n```\n\n(this will give a list of running docker instances. Get the sha code (first column) and use it when running\n\n```\ndocker exec  -t -i <hash> bash\n```\n\nThat should give you a bash shell.\n(this is not a tensorflow specific answer)\n\nLet me know if you need additional help.\n", "thank you. \n\nfinally this code line worked for me:\n`docker run -it b.gcr.io/tensorflow/tensorflow:latest-devel`\nfor reference:\nhttp://stackoverflow.com/questions/35582875/unable-to-start-tensorflow-within-docker-on-windows\n", "You can install stuff from the shell using apt-get install (including\neditors). If you want this to be done more persistently you can derive your\nown sub dockerfile that is derived from the tensorflow one (in fact\ntensorflow is derived from the ubuntu image one).\n\n-A\n\nOn Mon, Aug 22, 2016 at 2:59 PM Roboball notifications@github.com wrote:\n\n> thank you. just one final question. Can I use some comfortable editor like\n> spyder from anaconda?? or do I have to stick to bash in cmd prompt?\n> \n> finally this code line worked for me:\n> docker run -it b.gcr.io/tensorflow/tensorflow:latest-devel``\n> http://b.gcr.io/tensorflow/tensorflow:latest-devel\n> for reference:\n> \n> http://stackoverflow.com/questions/35582875/unable-to-start-tensorflow-within-docker-on-windows\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3954#issuecomment-241564515,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAT52r8AoJkvGxx6XJelPMFT4jZ7zfMVks5qihu3gaJpZM4JqCYZ\n> .\n", "im getting this error im in docker shell in ubuntu \r\n`root@1e94bee0135a:/tensorflow# docker ps\r\nbash: docker: command not found\r\nroot@1e94bee0135a:/tensorflow# docker ps \r\nbash: docker: command not found\r\n`\r\n\r\n"]}, {"number": 3953, "title": "error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//'", "body": "Operating System: Scientific Linux release 7.2 (Nitrogen)\nCUDA 7.5\n\nI'm trying to install tensorflow from source (with debug options in order to solve another issue), from GItHub branch r0.10 . \n\nUsing bazel 0.3.1-2016-08-22\n\nApparently related to #3437 \n\nAfter configuring tensorflow, I ran `bazel build -c dbg --config=cuda //tensorflow/cc:tutorials_example_trainer` and got the following errors:\n\n```\nERROR: /hltsrv0/rocha/deps/tensorflow/tensorflow/cc/BUILD:199:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Error cloning repository: https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf: cannot open git-upload-pack caused by Connection timed out github.com and referenced by '//tensorflow/cc:tutorials_example_trainer'.\nERROR: /hltsrv0/rocha/deps/tensorflow/tensorflow/cc/BUILD:199:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Error cloning repository: https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf: cannot open git-upload-pack caused by Connection timed out github.com and referenced by '//tensorflow/cc:tutorials_example_trainer'.\nERROR: /hltsrv0/rocha/deps/tensorflow/tensorflow/cc/BUILD:199:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Error cloning repository: https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf: cannot open git-upload-pack caused by Connection timed out github.com and referenced by '//tensorflow/cc:tutorials_example_trainer'.\nERROR: /hltsrv0/rocha/deps/tensorflow/tensorflow/cc/BUILD:199:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Error cloning repository: https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf: cannot open git-upload-pack caused by Connection timed out github.com and referenced by '//tensorflow/cc:tutorials_example_trainer'.\nERROR: /hltsrv0/rocha/deps/tensorflow/tensorflow/cc/BUILD:199:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Error cloning repository: https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf: cannot open git-upload-pack caused by Connection timed out github.com and referenced by '//tensorflow/cc:tutorials_example_trainer'.\nERROR: /hltsrv0/rocha/deps/tensorflow/tensorflow/cc/BUILD:199:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Error cloning repository: https://github.com/google/protobuf: cannot open git-upload-pack caused by https://github.com/google/protobuf: cannot open git-upload-pack caused by Connection timed out github.com and referenced by '//tensorflow/cc:tutorials_example_trainer'.\nERROR: Analysis of target '//tensorflow/cc:tutorials_example_trainer' failed; build aborted.\n```\n\nI'm using a server where I have no root privileges and there's no internet access. \n\nI understand the building process is trying to clone a repository, so, would it be possible to clone it and download any other necessary files in another machine and then copy them to the server? If so, what exactly do I need to copy and where?\n", "comments": ["I'm sorry, but this is not a supported configuration.  You might try posting this question on StackOverflow.\n", "I would like to ask how the final solution to this problem?\n"]}, {"number": 3952, "title": "Poor results with tensorflow DNNClassifier and cross_val_score", "body": "I am using python 3.5, tensorflow 0.10 and its DNNClassifier. If I perform a single training and testing stage, as below, the test result is decent: accuracy = 0.9333\n\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\nfrom sklearn.cross_validation import cross_val_score, ShuffleSplit, train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import datasets, cross_validation\n\niris = datasets.load_iris()\n\nfeature_columns = learn.infer_real_valued_columns_from_input(iris.data)\n\nx_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.20, random_state = 20)\n\nmodel = learn.DNNClassifier(hidden_units=[5], \n                             n_classes=3, \n                             feature_columns=feature_columns, \n                            )\n\nmodel.fit(x_train, y_train, steps=1000)\npredicted = model.predict(x_test)\n\nprint('Accuracy on test set: %f' % accuracy_score(y_test, predicted))\nIf I use sklearn's cross_val_score, then the final results is much poorer, about 0.33 accuracy:\n\nmodel = learn.DNNClassifier(hidden_units=[5], \n                             n_classes=3, \n                             feature_columns=feature_columns, \n                            )\n\nscores = cross_val_score(estimator=model, \n                         X=iris.data, \n                         y=iris.target, \n                         scoring = 'accuracy',\n                         cv=5,\n                         fit_params={'steps': 1000},\n                        )\n\nprint(scores)\nprint(np.mean(scores))\nThe scores ad their mean are:\n\n[ 0.          0.33333333  1.          0.33333333  0.        ]\n0.333333333333\nWhat's wrong with my code in cross validation estimation?\n", "comments": ["This is a general usage question more suitable for StackOverflow. Please can you re-ask it there.\n", "Thanks for suggestion, done. \n"]}, {"number": 3951, "title": "when I try to install tensorflow from source by bazel, I got this new problem:", "body": "/home/scw4150/Documents/tensorflow/tensorflow/tensorflow/stream_executor/BUILD:5:1: C++ compilation of rule '//tensorflow/stream_executor:stream_executor' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 112 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionFwdAlgo_t perftools::gputools::cuda::{anonymous}::ToConvForwardAlgo(perftools::gputools::dnn::AlgorithmType)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:266:10: error: 'CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING' was not declared in this scope\n     case CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING:\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In function 'cudnnConvolutionBwdDataAlgo_t perftools::gputools::cuda::{anonymous}::ToConvBackwardDataAlgo(perftools::gputools::dnn::AlgorithmType)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:284:10: error: 'CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING' was not declared in this scope\n     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING:\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ntensorflow/stream_executor/cuda/cuda_dnn.cc: In member function 'virtual bool perftools::gputools::cuda::CudnnSupport::GetConvolveAlgorithms(std::vector<long long int>*)':\ntensorflow/stream_executor/cuda/cuda_dnn.cc:942:7: error: 'CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING' was not declared in this scope\n       CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING,\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ntensorflow/stream_executor/cuda/cuda_dnn.cc:947:4: error: no matching function for call to 'std::vector<long long int>::assign(<brace-enclosed initializer list>)'\n   });\n\nany suggestions?\n", "comments": ["Which version of cuDNN and Cuda toolkit are you using? Gcc version? OS version?\n", "@hholst80 \ncuDNN v4 \nCuda toolkit 7.0\ngcc version 6.1.0\nCentOS Linux release 7.2.1511 (Core)\n", "I would double check your cuda and cudnn install, I think you have older versions, despite what you think.   Those symbols are available in cudnn v4 and above, so this suggests you have an earlier version.\n\nMaybe remove the existing cuda and cudnn versions and install them from scratch.\n\nI would also recommend using:\n- cudnn v4, unless you have GTX1080, in which case use cudnn v5\n- cuda toolkit 7.5, unless you have GTX1080, in which case use cuda 8.0 RC\n", "thanks,I will try to update cuda and cudnn version.\n", "(I should clarify, you can use cudnn v5 regardless of what GPU you have).\n\nI'm going to close this since I suspect this is a cuda problem, not a TF one (yet).\n"]}, {"number": 3950, "title": "docs(distributed): Fixed environment variable", "body": "The environment variable for setting container cluster should be `TF_DIST_CONTAINER_CLUSTER` instead of `CONTAINER_CLUSTER`\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 3949, "title": "Standard input pipeline logs errors", "body": "The following code:\n\n```\n    init_op = tf.group(tf.initialize_all_variables(),\n                       tf.initialize_local_variables())\n    sess = tf.Session()\n    sess.run(init_op)\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    try:\n        HERE COMES THE OP READING FROM THE INPUT PIPELINE\n    except tf.errors.OutOfRangeError:\n        print('Epoch limit reached')\n    finally:\n        coord.request_stop()\n    coord.join(threads)\n    sess.close()\n```\n\nFinishes without an exception, but generates two log messages marked as errors. \n\n```\nE tensorflow/core/client/tensor_c_api.cc:485] Reached limit of 2\n     [[Node: input_producer/limit_epochs/CountUpTo = CountUpTo[T=DT_INT64, _class=[\"loc:@input_producer/limit_epochs/epochs\"], limit=2, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer/limit_epochs/epochs)]]\nE tensorflow/core/client/tensor_c_api.cc:485] FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)\n     [[Node: ReaderRead = ReaderRead[_class=[\"loc:@TextLineReader\", \"loc:@input_producer\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](TextLineReader, input_producer)]]\nEpoch limit reached\n```\n\nShouldn't these be warnings or simply hidden?\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: \n8.0.27 and 5.0.5\n\nInstalled from source: \nCommit hash: 3cb39956e622b322e43547cf2b6e337020643f21 (v0.10.0rc0)\n\nBazel:\nBuild label: 0.3.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jun 10 11:38:23 2016 (1465558703)\nBuild timestamp: 1465558703\nBuild timestamp as int: 1465558703\n", "comments": ["@davidsoergel Looks like input handling ops return an error status on EOF which causes session.run to abort graph execution and log an error?\n\nI don't see that this would be easy to change?\n", "Is this something that we still want to do?", "Closing due to inactivity. Feel free to re-open if you would like us to look again."]}]