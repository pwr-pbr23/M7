[{"number": 18034, "title": "Add link to raspberry pi demo", "body": "", "comments": []}, {"number": 18033, "title": "Fixed the bug in mkl_input_conversion", "body": "This PR fixed the failed unit test //tensorflow/python:image_ops_test  for MKL configuration.\r\n\r\nmkl_input_conversion operator compares the shapes of two inputs to decide the input tensors need be converted to TensorFlow format or not. In the bug I saw in TFDO-251, two input tensors are both in MKL format, their total number of elements are the same, but some dimensions are different, for example, one dimension is [2][1][15][3], and another one is [1][2][15][3]. However, the following code to compare the shape is actually comparing MKL shape, not the original TensorFlow shape:\r\n\r\nbool tf_shapes_are_same = context->input(0).shape() == context->input(1).shape();\r\n\r\nI changed it to compare the original TensorFlow shape.\r\n\r\nThis PR also includes some code cleanup.", "comments": ["@shengfuintel Thanks for the fix. Will review a little later.", "@shengfuintel Thanks for the fix."]}, {"number": 18032, "title": "Update tensorboard dependency to 1.7.0+", "body": "TensorBoard 1.7.0 has been released to PyPI: https://pypi.python.org/pypi/tensorboard/1.7.0", "comments": []}, {"number": 18031, "title": "Refix spelling error reverted in #18015", "body": "", "comments": []}, {"number": 18030, "title": "Update RELEASE.md with 1.4.2 change notes", "body": null, "comments": []}, {"number": 18029, "title": "Revert \"Fix minor typos in contrib files\"", "body": "Reverts tensorflow/tensorflow#18015\r\n\r\nThe files in bayesflow are getting deleted, unblock @xiejw ", "comments": []}, {"number": 18028, "title": "Remove conflicting file from contrib.bayesflow to unblock push PR", "body": "", "comments": []}, {"number": 18027, "title": "Fix issue caused by None in batch dimension for tf.layers.conv3d", "body": "This fix tries to address the issue raised in #15655 where error returns when the batch dimension for tf.layers.conv3d is None with \"channels_first\" format.\r\n\r\nThis fix cast None to `-1` to address the issue\r\n\r\nThis fix fixes #15655.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@fchollet Thanks or the quick review."]}, {"number": 18026, "title": "What are the expected performance gains when using model quantization? \\ Is there support for keras models?", "body": "[this page](https://www.tensorflow.org/performance/quantization) describes tensorflow model quantization \r\n\r\n* Is there a similar tool for keras models?\r\n* are there some examples of the performance gain I can expected? ()it's probably model dependent, but some examples would be usefull", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18025, "title": "Trying to fix libtensorflow GPU build.", "body": "CUDNN path error. \r\nInvalid path to cuDNN 7 toolkit. None of the following files can be found:\r\nC:/tools/cuda\\lib/x64/cudnn.lib\r\nC:/tools/cuda\\lib/x64/cudnn.lib", "comments": ["This looks OK to me. Do our installation instructions say anything about specifying this variable?", "Where is the CUDNN_INSTALL_PATH being set? I didn't see it being set in the platform configuration docs. Anyways, lgtm", "@case540 I just sent a change to you for review regarding that."]}, {"number": 18024, "title": "Validate axis in shape function of tf.reverse", "body": "tf.reverse requires the axis to be in the range of `[-rank(tensor), rank(tensor))`. Previously the validation is only done in runtime though it is possible to validate axis inside the shape function if the shape of the input tensor is already known.\r\n\r\nThis fix add the validation in the shape function.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18023, "title": "Remove warnings for initialize_variables", "body": "The `initialize_variables` has been deprecated and replaced with\r\n`tf.variables_initializer`.\r\n\r\nThis fix makes the change and fixes the following warning in array_ops_test.py:\r\n```\r\nWARNING:tensorflow:From /private/var/tmp/_bazel_ytang/48f7de64c479bcefe5e55c65866b55a6/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/kernel_tests/array_ops_test.runfiles/org_tensorflow/tensorflow/python/util/tf_should_use.py:118: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.variables_initializer` instead.\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang thanks for the cleanup."]}, {"number": 18022, "title": " Fix the math equation format issue in tf.matrix_solve_ls", "body": "This PR is to fix #17663.\r\n\r\nAs described in the above issue, there exist a math equation formatting issue in the tf.matrix_solve_ls as below:\r\n![image](https://user-images.githubusercontent.com/1680977/37984072-d382a43a-3227-11e8-9d60-bfe6500b316d.png)\r\n\r\nThis PR is to fix this math equation issue according to the [Math in markdown guideline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md#math-in-markdown). That is, when using \"\\\\( \\\\) \" to surround math equation within text, we needs to make sure it should not break new line.\r\nAfter fix:\r\n![image](https://user-images.githubusercontent.com/1680977/37984261-6544074c-3228-11e8-93cc-be48342e20fe.png)\r\n", "comments": []}, {"number": 18021, "title": "Fix the contrib losses function bullet lists rendering", "body": "This PR is the fix the rendering of function lists bullet in [Losses (contrib)](https://www.tensorflow.org/api_guides/python/contrib.losses#Loss_operations_for_use_in_neural_networks_).\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1680977/37982795-257966c4-3224-11e8-9d74-35723e31153f.png)\r\n\r\nTo keep consistent with all other [api guides](https://www.tensorflow.org/api_guides/python/contrib.linalg), it's better to list functions as bullet for easier reading, this PR is fixing this with add \"* \" in the begining of each function.\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1680977/37982774-142378ce-3224-11e8-967a-a9ee8713cb88.png)\r\n", "comments": []}, {"number": 18020, "title": "[Intel MKL] Adding support for MKL to docker CI infrastructure.", "body": "This PR adds support for the MKL to the docker CI infrastructure. Non-development containers are supported via the publicly available pip whls at https://anaconda.org/intel/tensorflow/. Development containers are supported as well. \r\n\r\nThis PR also adds support for the following parameters:\r\n\r\n- CI_DOCKER_EXTRA_BUILD_PARAMS: allows callers to pass parameters to the docker build command (e.g. --build-arg)\r\n- TF_BAZEL_BUILD_OPTIONS: allows callers to pass parameters to the bazel build command (e.g. \"--copt='mavx2'\")\r\n\r\nThe MKL Dockerfiles have been renamed per the convention in parameterized_docker_build.sh. They have also been modified to work better with the sed commands in the build scripts. ", "comments": ["@jhseu could you take a look, please?", "The changes look good to me, but I'm switching reviewers to @gunan since there are test infrastructure changes going on, so I don't know if this is impacted.", "All of the bash files under here are on their way out. @case540 is working on removing those.\r\nIf you like to check in CI scripts, I recommend creating standalone scripts that do not need any flags under `tools/ci_build/linux/mkl`", "@case540 Can you give me some idea of what you are woring on  so I can help add MKL support to it?", "Yeah, working on rewriting these bash scripts in python to make the code more organized/readable.  Anything you add to the current .sh scripts will probably be deleted (or reimplemented in Python). \r\n\r\nNot much you can currently do to help except by changing the current build shell scripts as little as possible.", "@case540 how about we merge this one (since python are still in the making) and we'll rewrite shells to .py as soon as you're ready?", "That would be fine with me.", "@case540 could you please review and approve this, if you are OK with merging it?", "@case540 & @claynerobison what is the status here? Can we proceed?\r\n@claynerobison would you please rebase?", "Closing because the rebase was a disaster. See new PR #18745 "]}, {"number": 18019, "title": "Branch 190618988", "body": "", "comments": []}, {"number": 18018, "title": "Fix the incorect rendering of math equation in monte_carlo api guides", "body": "This PR is to fix the incorrect rendering of math equation in [BayesFlow Monte Carlo (contrib)](https://www.tensorflow.org/api_guides/python/contrib.bayesflow.monte_carlo).\r\n\r\nThis PR fixed the incorrect math equation rendering in above api guides according to the\r\n[Math in markdown](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md#math-in-markdown) guideline. That is using $$, \\\\( and \\\\) to surround the math equation.\r\n\r\nTake the [math equation](https://www.tensorflow.org/api_guides/python/contrib.bayesflow.monte_carlo#Background) as an example to show the fixing result: \r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1680977/37979991-8e71cb42-321c-11e8-97a3-eff59c65f5ff.png)\r\n\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1680977/37979965-82a6941e-321c-11e8-9335-589c4a375600.png)\r\n\r\n", "comments": []}, {"number": 18017, "title": "the mnist dataset in cnn_mnist.py", "body": "I can't see how to build the dataset 'mnist' as in the [tutorials](https://www.tensorflow.org/tutorials/layers#top_of_page) below:\r\n![image](https://user-images.githubusercontent.com/20028780/37976700-a88dffda-3214-11e8-8a22-8c45cecbb81e.png)\r\n```\r\ndef main(unused_argv):\r\n  # Load training and eval data\r\n  mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\r\n  train_data = mnist.train.images # Returns np.array\r\n  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\r\n  eval_data = mnist.test.images # Returns np.array\r\n  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\r\n```\r\nMy codes don't work of course. And I have downloaded the following dataset in the  [MNIST](http://yann.lecun.com/exdb/mnist/). So what can I do? \r\n![image](https://user-images.githubusercontent.com/20028780/37976920-32b61b0c-3215-11e8-8ef0-67b749daf448.png)\r\nThank you a lot.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: N\r\nOS Platform and Distribution:\r\nTensorFlow installed from: Anaconda\r\nTensorFlow version: 1.6.0\r\nBazel version\r\nCUDA/cuDNN version\r\nGPU model and memory: N\r\nExact command to reproduce", "Hi WendyMin,\r\n\r\nI think you are confused between MNIST dataset provided by the TF bundle and the one which you have downloaded from http://yann.lecun.com/exdb/mnist/. \r\nTF MNIST Data are parsed and converted into arrays of image pixels, whereas http://yann.lecun.com/exdb/mnist/ has files into IDX format which is fairly simple format to extract your image pixels for each files.\r\n\r\nPlease refer \"THE IDX FILE FORMAT\" on same page http://yann.lecun.com/exdb/mnist/  to understand how you can easily convert downloaded IDX files into your array/tfRecord which TF can accept as input. \r\n\r\nYou may want to refer https://www.tensorflow.org/programmers_guide/datasets\r\n\r\nThanks,\r\nJinay", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18016, "title": "Feature: evaluating multiple datasets in tf.estimator.Estimator.evaluate without reloading checkpoint", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora release 25 (Twenty Five)\r\n- **TensorFlow installed from (source or binary)**: binary for CPU\r\n- **TensorFlow version (use command below)**: v1.6.0-0-gd2e24b6039\r\n- **Python version**: 3.5.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nWe have several datasets on which we want to track performance during training, corresponding to multiple data splits or different types of data that are processed by the model. To do this we run `Estimator.evaluate` for each dataset after each epoch of training.\r\n\r\nThe problem with this is that `Estimator.evaluate` reconstructs the graph and loads the variables from checkpoint each time that it is called. In our case, reconstructing the large graph takes longer than doing the evaluation on the relatively small datasets. I'd propose a feature to allow evaluating on multiple datasets without reloading the graph.\r\n\r\nIdeas:\r\n- Add an `Estimator.evaluate_multiple` method that takes a mapping from name to input_fn.\r\n- Add functionality to explicitly start an evaluation session for an estimator and allow passing this as input to `Estimator.evaluate`\r\n\r\n### Source code / logs\r\n```\r\nmodel = Estimator(my_model_fn)\r\nfor epoch in range(n_epochs):\r\n  estimator.train(train_input_fn, steps=n_steps_per_epoch)\r\n  for name, eval_input_fn in eval_datasets:\r\n    # eval_input_fn create one shot iterators\r\n    estimator.evaluate(eval_input_fn, name=name)\r\n```\r\n\r\n", "comments": ["Is this related to https://github.com/tensorflow/tensorflow/issues/16087?", "I'd say related but not the same. If this gets implemented, then adding it to `tf.estimator.train_and_evaluate` would probably be trivial and more efficient than having `train_and_evaluate` calling the `evaluate` method multiple times in a loop like I do in the example above.\r\n\r\nHowever, since `train_and_evaluate` evaluates only once I think that the efficiency loss for that making multiple calls to `evaluate` is not a big deal. This efficiency loss is more important if you have many datasets that you want to evaluate during training.", "Yeah, I agree that this would be useful, but it's probably unlikely to be added (added Mustafa to comment). There's an upcoming API that won't recreate the session and graph that will be more useful for this use-case.", "by design estimator.train, .evaluate, .predict recreates graph and initialize from checkpoint.\r\nwe don't have any plan in Estimator which breaks this decision. ", "@jhseu Can you hint what is the name or when will be available this API that does not recreate graph at each train/eval call?"]}, {"number": 18015, "title": "Fix minor typos in contrib files", "body": "This PR is to fix several typos in contib related files.", "comments": []}, {"number": 18014, "title": "Creating a Dataset within a while_loop does not work", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nyes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nbinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n```bash\r\n$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.7.0-rc0-12-g98c955ee73', '1.7.0-rc1')\r\n```\r\n\r\n- **Python version**: \r\n\r\n```bash\r\n$ python --version\r\nPython 2.7.12\r\n```\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\nN/A\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\n\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\n\r\nCUDA 9.1, cuDNN 7.1\r\n\r\n- **GPU model and memory**:\r\n\r\n```bash\r\n2018-03-27 14:21:38.224538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\n```\r\n\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    zero = tf.constant(0, dtype=tf.int64)\r\n    ten = tf.constant(10, dtype=tf.int64)\r\n    one = tf.constant(1, dtype=tf.int64)\r\n\r\n    def condition(i, n):\r\n        return tf.less(i, ten)\r\n\r\n    def body(i, n):\r\n        value = ten + i\r\n        ds = tf.data.Dataset.from_tensors(value)\r\n        it = ds.make_one_shot_iterator()\r\n\r\n        return i + one, n + it.get_next()\r\n\r\n    loop = tf.while_loop(condition, body, [zero,  zero])\r\n\r\n    global_init_op = tf.global_variables_initializer()\r\n\r\nwith tf.Session(graph=graph) as S:\r\n    S.run(global_init_op)\r\n    print S.run(loop)\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nThe above example is trivial, but I would expect a new Dataset and Iterator to be created within each iteration of the `while_loop`. Instead it seems that only a single instance of the  Dataset/Iterator is created resulting in an `OutOfRange` error (see log in next section)\r\n\r\n### Source code / logs\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"test_tf_dataset_in_graph.py\", line 24, in <module>\r\n    print S.run(loop)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1140, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    run_metadata)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\r\n\t [[Node: while/IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](while/OneShotIterator)]]\r\n\t [[Node: while/IteratorGetNext/_17 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_23_while/IteratorGetNext\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](^_cloopwhile/add_2/_6)]]\r\n\r\nCaused by op u'while/IteratorGetNext', defined at:\r\n  File \"test_tf_dataset_in_graph.py\", line 18, in <module>\r\n    loop = tf.while_loop(condition, body, [zero,  zero])\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3202, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2940, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2877, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"test_tf_dataset_in_graph.py\", line 16, in body\r\n    return i + one, n + it.get_next()\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 366, in get_next\r\n    name=name)), self._output_types,\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1455, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nOutOfRangeError (see above for traceback): End of sequence\r\n\t [[Node: while/IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](while/OneShotIterator)]]\r\n\t [[Node: while/IteratorGetNext/_17 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_23_while/IteratorGetNext\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](^_cloopwhile/add_2/_6)]]\r\n\r\n```", "comments": ["The problem is not in creating the `Dataset`, but rather that the \"one-shot\" iterator is exhausted after the first iteration. You can either switch to using an \"initializable\" iterator that is initialized each time round the loop, or (easier in this case) use `tf.contrib.data.get_single_element()` to get the element from the dataset in the loop body without creating an iterator:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    zero = tf.constant(0, dtype=tf.int64)\r\n    ten = tf.constant(10, dtype=tf.int64)\r\n    one = tf.constant(1, dtype=tf.int64)\r\n\r\n    def condition(i, n):\r\n        return tf.less(i, ten)\r\n\r\n    def body(i, n):\r\n        value = ten + i\r\n        ds = tf.data.Dataset.from_tensors(value)\r\n        return i + one, n + tf.contrib.data.get_single_element(ds)\r\n\r\n    loop = tf.while_loop(condition, body, [zero,  zero])\r\n\r\nwith tf.Session(graph=graph) as S:\r\n    print S.run(loop)  # ==> \"(10, 145)\"\r\n```", "@mrry Great! Thanks for the response."]}, {"number": 18013, "title": "TensorFlow 1.5.1 binaries use AVX instructions like 1.6.0 but unlike 1.5.0", "body": "### System information\r\nLinux, Python 3.6\r\n\r\n### Describe the problem\r\nFor TensorFlow 1.6.0 and later it was announced that the official binaries on PyPI will use AVX instructions and therefore it doesn't run on old GPUs. TensorFlow 1.5.0 did not need AVX instruction support. Unfortunately, the bug fix release 1.5.1 was built with AVX instructions.\r\n\r\nThe 1.5.1 binary should be rebuild without AVX instructions such that upgrading from 1.5.0 to 1.5.1 is always possible.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "- **Have I written custom code**: N/A\r\n- **OS Platform and Distribution**: Linux\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.5.1\r\n- **Python version**: 3.6\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A", "@av8ramit : Seems like we built 1.5.1 with AVX?", "Yes, @jonasrauber we made a mistake. I'm not sure we can delete images off of pypi, but I'll rebuild without AVX and get those binaries on GCS ASAP. Thanks for filing this issue.", "Can you give these Python 3.6 [CPU](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.5.1-cp36-cp36m-linux_x86_64.whl) or [GPU](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.5.1-cp36-cp36m-linux_x86_64.whl) binaries a try?", "Checked the CPU version and it works.\r\nThe link to the GPU binary is broken.", "Sorry, fixed!", "Updated the release notes. Closing this for now."]}, {"number": 18012, "title": "Error: absl.flags._exception:IllegalFlagValueError: flag --input=: cannot convert string to float in tensorflow serving", "body": "I have written a simple program for tensorflow serving to deploy and check how it is working. I followed many tutorials on how to deploy these models using tensorflow serving inside docker environment.\r\n\r\n```\r\nsess = tf.InteractiveSession()\r\n# define the tensorflow network and do some trains\r\nx = tf.placeholder(\"float\", name=\"x\")\r\nw = tf.Variable(2.0, name=\"w\")\r\nb = tf.Variable(0.0, name=\"bias\")\r\nh = tf.multiply(x, w)\r\n\r\nsess.run(tf.global_variables_initializer())\r\ny = tf.add(h, b, name=\"y\")\r\n\r\n\r\nexport_path_base = FLAGS.work_dir\r\nexport_path = os.path.join(tf.compat.as_bytes(export_path_base),\r\n  tf.compat.as_bytes(str(FLAGS.model_version)))\r\nprint('Exporting trained model to', export_path)\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_path)\r\n\r\ntensor_info_x = tf.saved_model.utils.build_tensor_info(x)\r\ntensor_info_y = tf.saved_model.utils.build_tensor_info(y)\r\n\r\nprediction_signature = (\r\n  tf.saved_model.signature_def_utils.build_signature_def(\r\n  inputs={'input': tensor_info_x},\r\n  outputs={'output': tensor_info_y},\r\n  method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\r\n\r\nlegacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\r\n\r\nbuilder.add_meta_graph_and_variables(\r\n  sess, [tf.saved_model.tag_constants.SERVING],\r\n  signature_def_map={\r\n  'prediction':\r\n  prediction_signature,\r\n  },\r\n  legacy_init_op=legacy_init_op)\r\n\r\nbuilder.save()\r\n```\r\nThis saved_model of above programs is currently running inside the docker.I now want to create a client.py file to take input and produce output. I want to give a single number as input to my client file and not to declare inside . I mean\r\n\r\ni want to give input like this\r\n\r\n> python client.py --server=localhost:9000 --input=3\r\n\r\nso i created a client file with input as tf.app.flags.float('input','','input for the model')\r\n\r\n```\r\n\r\nfrom grpc.beta import implementations\r\nimport numpy\r\nimport tensorflow as tf\r\nimport sys \r\nfrom datetime import datetime\r\nfrom tensorflow_serving.apis import predict_pb2\r\nfrom tensorflow_serving.apis import prediction_service_pb2\r\n\r\ntf.app.flags.DEFINE_string('server', 'localhost:9000', 'PredictionService host:port')\r\ntf.app.flags.DEFINE_float('input','', 'input for the model')\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\ndef do_inference(hostport,args):\r\n  \"\"\"Tests PredictionService with concurrent requests.\r\n  Args:\r\n  hostport: Host:port address of the Prediction Service.\r\n  Returns:\r\n  pred values, ground truth label\r\n  \"\"\"\r\n  # create connection\r\n  host, port = hostport.split(':')\r\n  channel = implementations.insecure_channel(host, int(port))\r\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n\r\n  # initialize a request\r\n  data = args\r\n  request = predict_pb2.PredictRequest()\r\n  request.model_spec.name = 'example_model'\r\n  request.model_spec.signature_name = 'prediction'\r\n\r\n  request.inputs['input'].CopyFrom(tf.contrib.util.make_tensor_proto(data))\r\n  # predict\r\n  result = stub.Predict(request, 5.0) # 5 seconds\r\n  return result\r\n\r\ndef main(_):\r\n    if not FLAGS.server:\r\n        print('please specify server host:port')\r\n    return\r\n\r\n    result = do_inference(FLAGS.server,FLAGS.input)\r\n    print('Result is: ', result)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\nThis gives error , \r\nCan someone tell me what i did wrong here please? How i send a float number as input to the client file?\r\n\r\n![capture](https://user-images.githubusercontent.com/26268279/37956646-87ae7df8-31c9-11e8-99e2-11de878f1608.JPG)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code  - yes \r\nOS Platform and Distribution - Using Docker toolbox in windows with VM linux 2.6 64-bit.\r\nTensorFlow installed from - docker \r\nTensorFlow version - tensorflow serving api version 1.3.0\r\nBazel version - bazel version 0.5.4\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\n", "changed the line from tf.app.flags.DEFINE_float('input','', 'input for the model') to tf.app.flags.DEFINE_float('input',0.0, 'input for the model') worked for me."]}, {"number": 18011, "title": "Would tensorflow lite provide python API ?", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.3 LTS\r\nTensorFlow installed from (source or binary): pip\r\nTensorFlow version (use command below): 1.6.0\r\nPython version: 3.6.1\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\nDescribe the problem\r\nCurrently, tensorflow lite provide C++/Java API , I would know about any plans for these operations  can be made available in the Python API ?", "comments": ["Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This is now provided, but it is only accessible in bazel built projects rihgt now. I have a change that will make it available through \"import tensorflow as tf\" pip route. Stay tuned.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/python/interpreter.py\r\n", "Thank you for the reply !", "Sorry to comment on the closed thread. Is it now supported in v2.0.0-alpha0 release? Or is the only way to try tf lite using `bazel` as instructed here?https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/python/label_image.md"]}, {"number": 18010, "title": "Seq2seq minorsp", "body": "can squash to be more like #17815 if desired, let me know!", "comments": ["@brettkoonce thanks for the fixes!"]}, {"number": 18009, "title": "Update tb-nightly dep to >= 1.8.0a0, < 1.9.0a0", "body": "Synchronize tf-nightly dep on current tb-nightly.", "comments": []}, {"number": 18008, "title": "Disabling the state_management_test.", "body": "For non-pip builds also. Failing release tests.", "comments": ["This is going to sync to master and then disable it internally too. It shouldn't be manual internally.", "Changed to no_oss."]}, {"number": 18007, "title": "Computing gradients with tf.cond bug ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0 , CUDNN 7.0.5\r\n- **GPU model and memory**:  GTX 1060 (6 GiB Memory, 6.1 Compute Power)\r\n- **Exact command to reproduce**: python test_cond_gradients.py (or whatever you call the file with source code)\r\n\r\nThe file test_cond_gradients.py can be found  in the source code below\r\n\r\n### Describe the problem\r\nI am using a tf.cond to stop gradients from being computed through one part of the graph - depending on which train cross entropy (TCE) is smaller. According to the time however, even with the tf.cond, gradients is still being computed through both parts of the graph which should not be the case.\r\nIn the source code, there are three different losses tested. The first and second losses (baseline and loss1) are computed in similar time as shown if you run the script. The third loss tested (loss2) is slower than the first two, even though gradients should only be computed through one part of the graph (depending on the cond condition). *The second loss is also using a tf.cond but the true_fn and false_fn passed to it are identical.* The third should take about the same time to compute as the first two. This problem is amplified more when the network is more complex and thus back-prop takes more time but I have just created a simple example to run.  \r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\ndef run_test():\r\n    x1 = tf.placeholder(tf.float32, [64,32,32,3])\r\n    x2 = tf.placeholder(tf.float32, [64,32,32,3])\r\n\r\n    y = tf.placeholder(tf.int64, [64])\r\n\r\n    logits1 = simple_network(x1)\r\n    logits2 = simple_network(x2, reuse=True)\r\n\r\n    # TCEs\r\n    train_cross_entropy1 = tf.reduce_mean(\r\n            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits1))\r\n    train_cross_entropy2 = tf.reduce_mean(\r\n            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits2))\r\n\r\n    # Losses\r\n    loss_baseline = train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2)\r\n\r\n    loss1 = tf.cond(train_cross_entropy1 < train_cross_entropy2, lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2), \r\n                                                                 lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2))\r\n\r\n    loss2 = tf.cond(train_cross_entropy1 < train_cross_entropy2, lambda: train_cross_entropy1 + tf.stop_gradient(train_cross_entropy2), \r\n                                                                 lambda: tf.stop_gradient(train_cross_entropy1) + train_cross_entropy2)\r\n    # Train Step\r\n    train_step = tf.train.AdamOptimizer()\r\n\r\n    apply_gradient_op_baseline = train_step.minimize(loss_baseline)\r\n\r\n    apply_gradient_op1 = train_step.minimize(loss1)\r\n\r\n    apply_gradient_op2 = train_step.minimize(loss2)\r\n    \r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        ## Baseline\r\n        baseline_s = time.time()\r\n        for i in range(1000):\r\n            sess.run(apply_gradient_op_baseline, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\r\n        print(\"Baseline time: \", time.time() - baseline_s)\r\n\r\n        ## Cond 1\r\n        cond1_s = time.time()\r\n        for i in range(1000):\r\n            sess.run(apply_gradient_op1, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\r\n        print(\"Cond1 time: \", time.time() - cond1_s)\r\n\r\n        ## Cond 2 (Should be about the same time as Cond 1)\r\n        cond2_s = time.time()\r\n        for i in range(1000):\r\n            sess.run(apply_gradient_op2, feed_dict={x1: np.random.rand(64,32,32,3), x2: np.random.rand(64,32,32,3), y: np.random.randint(0,11,(64))})\r\n        print(\"Cond2 time: \", time.time() - cond2_s)   \r\n\r\ndef simple_network(Y, reuse=False):\r\n    with tf.variable_scope(\"simple_network\", reuse=reuse):\r\n        K1 = tf.get_variable(\"K1\", shape=[3,3,3,16], initializer=tf.initializers.random_normal)\r\n        Y = tf.nn.conv2d(Y, K1, strides=[1,1,1,1], padding='SAME')\r\n\r\n        K2 = tf.get_variable(\"K2\", shape=[3,3,16,64], initializer=tf.initializers.random_normal)\r\n        Y = tf.nn.conv2d(Y, K2, strides=[1,1,1,1], padding='SAME')\r\n\r\n        Y = tf.reduce_mean(Y, [1,2])\r\n\r\n        FC_W = tf.get_variable(\"FC_W\", shape=[64, 10], initializer=tf.initializers.random_normal)\r\n        FC_B = tf.get_variable(\"FC_B\", shape=[10], initializer=tf.initializers.random_normal)\r\n\r\n        logits = tf.nn.xw_plus_b(Y, FC_W, FC_B)\r\n\r\n        return logits\r\n\r\nrun_test()\r\n```\r\nThanks for any advice or work-arounds of this bug \r\n", "comments": ["The documentation of `tf.cond` has said:\r\n> Note that the conditional execution applies only to the operations defined in true_fn and false_fn. Consider the following simple program:\r\nz = tf.multiply(a, b)\r\nresult = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))\r\nIf x < y, the tf.add operation will be executed and tf.square operation will not be executed. Since z is needed for at least one branch of the cond, the tf.multiply operation is always executed, unconditionally. Although this behavior is consistent with the dataflow model of TensorFlow, it has occasionally surprised some users who expected a lazier semantics.\r\n\r\nI'm not sure, but the story is probably the same for gradients: as long as something is needed by at least one branch, it will be executed.", "@ppwwyyxx  But aren't the only things needed by the branches train_cross_entropy1 and train_cross_entropy2? Which I want to execute anyways. I'm not sure but I would think that the gradients are not needed by either branch as they are computed later? ", "I fixed it by adding the gradient calculation inside the functions  and using tf.gradients like so:\r\n```\r\ndef f1():\r\n    return tf.gradients(train_cross_entropy1, var_list, stop_gradients=[train_cross_entropy2])\r\n```\r\n\r\nThen applying the gradients after the tf.cond. \r\nClosing ", "I have the same problem but when using optimizer with tf.cond. Here my simplified code:\r\n\r\n    def optimize(loss):\r\n        \r\n        with tf.name_scope('Optimizer'):\r\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        \r\n            with tf.control_dependencies(update_ops):\r\n                optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n                    def prev_grads_are_None():\r\n                \r\n                        # This will compute the local gradient given the loss function (taking into account all trainable variables)\r\n                        train_step = optimizer.minimize(loss)\r\n                        grads_wrt_initial_state = tf.gradients(loss, initial_state, grad_ys=None)\r\n                \r\n                        return train_step, grads_wrt_initial_state\r\n            \r\n                train_step, grads_wrt_initial_state = tf.cond(this_is_last_batch, prev_grads_are_None, prev_grads_are_None)\r\n                return train_step, grads_wrt_initial_state\r\n\r\nTherefore I would like to know how to solve this problem. In my case, the optimizer is not running, the the kernel always shows busy. Please note that I am training a 2 layer GRU"]}, {"number": 18006, "title": "[Intel MKL] MKL allocator fix", "body": "reverting mkl allocator inline modifier from #17396. leads to build failures with older compilers (e.g., gcc6.3). ", "comments": ["@anton-matosov we might have to revert the 'inline' modifier temporarily until we find a solution that works for both Mac and linux builds. Assuming, you added the modifier to work around some issue on Mac.", "hi @jbobba, I have added when this line was inside the class and there was no cpp file.\r\nnow the code looks different and should work without inline.", "The change looks good. ", "@jbobba Thanks for the fix."]}, {"number": 18005, "title": "Update GPU package installation instructions", "body": "Updating GPU package installation instructions.", "comments": ["Tagging @gunan @MarkDaoust. Sorry for the previous PR, accidentally submitted under wrong account."]}]